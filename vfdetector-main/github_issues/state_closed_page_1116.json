[{"number": 19762, "title": "TF dataset padded_batch: support for batching non-padable component", "body": "Current` tf.data.dataset `class only support `padded_batch` method which would pad every component into same shape, but there are cases that some component may not be padable, e.g., in object detection input pipeline, the ground truth calsses and bounding boxes are different for different image, so they can not be padded to the same shape as images, so how to batch these data in this case?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Seems like this is a duplicate of #19710, so closing this. Feel free to reopen if I'm mistaken.", "@asimshankar Hi! it is not a duplicate, internal TF dataset API do not have a good support for batching things for object detection model, e.g., different image can be resized/padded into same shape, but may have different number of groundtruth boxes, in object detection API, they batch padded groundtruth boxes tensors, and manually recover them latter, so the question is better implementation in the batching stage with tf.data.dataset may bring more efficiency.", "Indeed, sorry, I only made a cursory read.\r\nThat said though, could you elaborate on what you'd like the behavior to be? Are you suggesting that the \"batches\" contain tensors of irregular shapes?  ", "Yes, I that may be a bit irregular for tensors and not implementable, but can we add feature to batch tensors into list of different length tensors, so the job can be down, without post recovering of the shapes", "Can you use [`group_by_window`](https://www.tensorflow.org/api_docs/python/tf/contrib/data/group_by_window) to bin the different length tensors?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "no, group_by_window can be used to batch similar shaped tensors, like similar aspect ratio tensors, but the question is in object detection, we may first use group_by_window to group similar aspect ratio images, then we have to deal with batching ground truth boxes and cls labels, which has to be padded to same shape within the whole batch to form a tensor. Then we have to recover the actual shape of ground truth boxes and cls labels, so the question is in bathing stage, can we add this support instead of force pad to same shape.", "@jsimsa @mrry may have some guidance here, such as suggestions on if and how someone could contribute to this feature.", "Hello @MrWanter, take a look at the recently introduced tf.data API for windowing and batching of windows  (https://github.com/tensorflow/tensorflow/commit/a6471888cc9dfe9c18d121149bc0516a3f423fbb). Among other things, the newly introduced API makes it possible to use different batching logic for different components.", "Hi. I have the same question.  Is there any answer to this issue?", "same question here", "See https://github.com/tensorflow/community/pull/5/files?short_path=df3302a#diff-df3302ad45d4c94d919eefaefa059780."]}, {"number": 19761, "title": "[bug]Function \"NumHyperthreadsPerCore\" may be forgotten to add into windows/port.cc", "body": "### System information\r\n- **I did not alter any code**:\r\n- **Windows 10 1803 (Ver. 17134.81)**:\r\n- **TensorFlow installed from source**:\r\n- **TensorFlow version: 1.8.0**:\r\n- **Python version: 3.6.4**: \r\n- **No Bazel**:\r\n- **Microsoft Visual Studio 2015**:\r\n- **CUDA 9.2.88 / cuDNN 7.1.4**:\r\n- **NVIDIA GTX 1070 8GB**:\r\n- **Build project \"tf_python_build_pip_package\" in Release configuration**:\r\n\r\n### Describe the problem\r\nThe project \"pywrap_tensorflow_internal\" failed to build, and error was:\r\n\r\n> threadpool_device.obj : error LNK2019: unresolved external symbol \"int __cdecl tensorflow::port::NumHyperthreadsPerCore(void)\" (?NumHyperthreadsPerCore@port@tensorflow@@YAHXZ) in function \"public: __cdecl tensorflow::ThreadPoolDevice::ThreadPoolDevice(struct tensorflow::SessionOptions const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class tensorflow::gtl::IntType<struct tensorflow::Bytes_tag_,__int64>,class tensorflow::DeviceLocality const &,class tensorflow::Allocator *)\" (??0ThreadPoolDevice@tensorflow@@QEAA@AEBUSessionOptions@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$IntType@UBytes_tag_@tensorflow@@_J@gtl@1@AEBVDeviceLocality@1@PEAVAllocator@1@@Z)\r\n\r\nFinally I found that function `NumHyperthreadsPerCore` was not implemented in _**tensorflow/core/platform/windows/port.cc**_, while it was implemented in _**tensorflow/core/platform/posix/port.cc**_. Then I copy this function from **_posix/port.cc_** to **_windows/port.cc_**, it worked.\r\n\r\n### Source code\r\n**_tensorflow/core/platform/posix/port.cc_** line 77-80:\r\n```\r\nint NumHyperthreadsPerCore() {\r\n    static const int ht_per_core = tensorflow::port::CPUIDNumSMT();\r\n    return (ht_per_core > 0) ? ht_per_core : 1;\r\n}\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "I'm experiencing the exact same issue, and also the same solution!!\r\n\r\n> Have I written custom code\r\n\r\nNO\r\n\r\n> OS Platform and Distribution\r\n\r\nWindows 10 Build 17134.167\r\n\r\n> Bazel version\r\n\r\nN/A. Using CMake.\r\n\r\n> CUDA/cuDNN version\r\nGPU model and memory\r\n\r\nN/A. Compiling CPU-only version.\r\n\r\n> Exact command to reproduce\r\n\r\nGenerate VS 2017 projects using CMake and build.\r\n"]}, {"number": 19760, "title": "tflitecamerademo error", "body": "hi,\r\n    I test tflitecamerademo in tensorflow\\contrib\\lite\\java\\demo\r\n\r\nPrompt me to download the file every day, and then compile it after downloading. Do these library files need to be updated every day? :\r\nError:A problem occurred configuring project ':app'.\r\n> Could not resolve all dependencies for configuration ':app:_debugApkCopy'.\r\n   > Could not resolve org.tensorflow:tensorflow-lite:+.\r\n     Required by:\r\n         project :app\r\n      > Could not resolve org.tensorflow:tensorflow-lite:+.\r\n         > Failed to list versions for org.tensorflow:tensorflow-lite.\r\n            > Unable to load Maven meta-data from https://jcenter.bintray.com/org/tensorflow/tensorflow-lite/maven-metadata.xml.\r\n               > Could not GET 'https://jcenter.bintray.com/org/tensorflow/tensorflow-lite/maven-metadata.xml'.\r\n                  > jcenter.bintray.com\r\n      > Could not resolve org.tensorflow:tensorflow-lite:+.\r\n         > Failed to list versions for org.tensorflow:tensorflow-lite.\r\n            > Unable to load Maven meta-data from https://google.bintray.com/tensorflow/org/tensorflow/tensorflow-lite/maven-metadata.xml.\r\n               > Could not GET 'https://google.bintray.com/tensorflow/org/tensorflow/tensorflow-lite/maven-metadata.xml'.\r\n                  > google.bintray.com\r\n\r\n------------------------------------------------------------------------------------------------------\r\nAnother question, how to compile tflite.so by yourself instead of using downloaded tflite.so\r\n\r\nthanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@bigbao9494 I would suggest to use: 'org.tensorflow:tensorflow-lite:0.1.7'. instead of  'org.tensorflow:tensorflow-lite:+'\r\nwhich pins the tensorflow lite version to use. This should not attempt to download a new version. ", "Is there a clear way to solve this problem?"]}, {"number": 19759, "title": "Cherrypicks", "body": "", "comments": []}, {"number": 19758, "title": "CUDA", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["No information seems to be filled out."]}, {"number": 19757, "title": "Branch 199194260", "body": "", "comments": ["Thanks!"]}, {"number": 19756, "title": "Android demo bug ", "body": "Have I written custom code: no\r\nOS Platform and Distribution: MacOs, Android 8.0\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.8.0\r\nBazel version: None\r\nCUDA/cuDNN version: None, cpu version\r\nGPU model and memory: None\r\nExact command to reproduce: None\r\n\r\nI have installed the android demo on my device but i thinks there might be a bug on object detection.\r\nWhen an object is detected the rectangle around the object, the label and the precision appear on the screen; but if I frame something that doesn't present anything to be detected after a detection then the rectangle and everything still remain on the screen.\r\n\r\nDoes anyone know how to fix it?\r\nThanks.", "comments": ["@achowdhery , could you ptal? Thanks!", "Please try with the latest version:\r\nhttps://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193", "This step is not working\r\n`bazel build -c opt --config=android_arm{,64} --cxxopt='--std=c++11' \\\r\n//tensorflow/contrib/lite/examples/android:tflite_demo`\r\n\r\nEven trying different flags i get this error\r\n\r\n`ERROR: No default_toolchain found for cpu 'arm64-v8a'. Valid cpus are: [\r\n  k8,\r\n  local,\r\n  armeabi-v7a,\r\n  x64_windows,\r\n  x64_windows_msvc,\r\n  x64_windows_msys,\r\n  s390x,\r\n  ios_x86_64,\r\n]\r\nINFO: Elapsed time: 0.153s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n`", "Try this flag:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android/app\r\n\r\nbazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  //tensorflow/contrib/lite/examples/android:tflite_demo", "@edob95 Is this still an open issue?\r\n", "Nagging Assignee @achowdhery: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@edob95 Is this still an open issue?"]}, {"number": 19755, "title": "error in rnn with multiple inputs and one output?", "body": "", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19754, "title": "INTEL-MKL:  MKL primitive reuse for conv2d fwd op - refactoring per PR suggestion", "body": "This PR updates the implementation of MKL primitive reuse for conv2d forward op, based on \r\n  1. Rasmus's initial PR review suggestion: moves private members of  MklConv2DFwdPrimitive into a structure. \r\n  2. Rename related implementation classes, for instance\r\n       ConvFwdDimensions  -->  MklConvFwdParams  \r\n  to make them more explicit/generic and to avoid any confusion with prefix \"Mkl\" since \r\n  they are under \"tensorflow\" namespace. \r\n\r\nI have temporarily close 4 related new MKL primitive reuse PRs (for conv bwd, relu, batchnorm, pooling)\r\nsince they are pending on this one. \r\n\r\n", "comments": ["Hi Ramesh, \r\nThank you very much for your valuable suggestions. \r\nI will do similar code refactoring on the remaining 4 MKL primitive reuse PRs (conv2d bwd, relu, batchnorm and pooling) before re-opening them for your review.\r\n--GZ", "Hi Rasmus,\r\nI am sorry, \"Hi Ramesh\" should be \"Hi Rasmus\" in my previous comment. I was confused \r\nwith my team leader's name. \r\nGZ"]}, {"number": 19753, "title": "Expose `tf.broadcast_to` op", "body": "This fix is a follow up of #15243 to expose `tf.broadcast_to`. Previously the op was exposed as `tf.contrib.framework.broadcast_to`.\r\n\r\nThis fix unhides the BroadcastTo so that it is exposed in `tf.broadcast_to`, and also removes `tf.contrib.framework.broadcast_to`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["/cc  @martinwicke @Hoeze", "Seems unrelated but worrisome.\r\n\r\n```\r\n==================== Test output for //tensorflow/core/kernels/batching_util:serial_device_batch_scheduler_test:\r\nRunning test tensorflow/core/kernels/batching_util/serial_device_batch_scheduler_test on GPU 0\r\nRunning main() from test_main.cc\r\n[==========] Running 7 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 7 tests from SerialDeviceBatchSchedulerTest\r\n[ RUN      ] SerialDeviceBatchSchedulerTest.BadOptions\r\n[       OK ] SerialDeviceBatchSchedulerTest.BadOptions (0 ms)\r\n[ RUN      ] SerialDeviceBatchSchedulerTest.InFlightBatchesLimit\r\n[       OK ] SerialDeviceBatchSchedulerTest.InFlightBatchesLimit (3 ms)\r\n[ RUN      ] SerialDeviceBatchSchedulerTest.PendingOnSerialDevice\r\n2018-06-04 17:05:44.712986: F tensorflow/core/kernels/batching_util/serial_device_batch_scheduler_test.cc:171] Check failed: scheduler->in_flight_batches_limit() == 1 (2 vs. 1)\r\n*** Received signal 6 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n2018-06-04 17:05:44.720854: F tensorflow/core/kernels/batching_util/serial_device_batch_scheduler_test.cc:179] Check failed: scheduler->in_flight_batches_limit() == 1 (2 vs. 1)\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/kernels/batching_util/serial_device_batch_scheduler_test.runfiles/org_tensorflow/tensorflow/tools/ci_build/gpu_build/parallel_gpu_execute: line 40: 12858 Aborted                 (core dumped) $@\r\n================================================================================\r\n```"]}, {"number": 19752, "title": "publish 'broadcast_to' op", "body": "See https://github.com/tensorflow/tensorflow/pull/15243#discussion_r178889285", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@Hoeze I think you may want to create the PR against master branch?", "sure, but I just saw that I have to sign some CLA.\r\nDo I really have to create a google account for that?\r\n\r\nMaybe it would be easier if someone else wants to pull-request this change...", "@Hoeze Sure I can create the PR for it.", "OK, then I'll close this pull request again :)"]}, {"number": 19751, "title": "No module named '_pywrap_tensorflow_internal' on Linux", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from **: a Rasa dependencies\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nBoth the script and the command provided for system information resulted in this error: `no module named '_pywrap_tensorflow_internal'`\r\n\r\n### Describe the problem\r\n\r\nTrying to develop a chatbot, anytime I call to tensorflow with a virtual environment I get this error: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"/usr/lib/python3.5/imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n```\r\n\r\nI have read [this closed topic](https://github.com/tensorflow/tensorflow/issues/11571) but it was about Windows and without a working solution.\r\n\r\nThe full error was:\r\n\r\n```\r\n(myFlaskAppenv) mike@mike-thinks:~/Programing/Rasa/myflaskapp$ python train_online.py \r\nINFO:rasa_nlu.components:Added 'nlp_spacy' to component cache. Key 'nlp_spacy-en'.\r\n/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/rasa_nlu/extractors/entity_synonyms.py:85: UserWarning: Failed to load synonyms file from './models/nlu/default/moodnlu/entity_synonyms.json'\r\n  \"\".format(entity_synonyms_file))\r\n/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"/usr/lib/python3.5/imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_online.py\", line 38, in <module>\r\n    run_weather_online(ConsoleInputChannel(), nlu_interpreter)\r\n  File \"train_online.py\", line 22, in run_weather_online\r\n    policies=[MemoizationPolicy(), KerasPolicy()],\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/rasa_core/policies/keras_policy.py\", line 28, in __init__\r\n    import keras\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/keras/utils/__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/keras/backend/__init__.py\", line 84, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"/usr/lib/python3.5/imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/mike/Programing/Rasa/myflaskapp/myFlaskAppenv/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n```\r\n\r\n\r\n\r\n### Source code / logs\r\nYou may be able to reproduce this error by calling for `python train_online.py` from [this repository](https://github.com/antoinecomp/myflaskapp/tree/master/myFlaskAppenv).\r\n", "comments": ["I saved the problem by reinstalling tensorflow:\r\n\r\n` pip install tensorflow --upgrade --force-reinstall`"]}, {"number": 19750, "title": "Numerical errors in symmetric tensors lead to eventual instability", "body": "I use `B = tf.matmul(A, A, transpose_b=True)` throughout my model, but I keep getting output tensors that are not symmetric. At first this is and unnoticeable amount, but after sufficient optimization steps calculations like `tf.linalg.logdet` and `tf.linalg.inv` blow up - not invertable or no cholesky decomposition.\r\n\r\nAfter many steps of testing, I noticed that this happened because the input tensors were not symmetric, when they should be.\r\n\r\nTo test this out, I tried `tf.reduce_sum(tf.cast(B != tf.transpose(B), tf.int16))` to check at what point the data stops being symmetric, and this happens right at the first step.\r\n\r\nIs this a known issue? Any recommendations?", "comments": ["It's a known issue that floating point computations can produce small errors on modern hardware, and that small errors can blow up into big errors over time of training. For instance, SSE optimized-code can lead to [slightly different results](http://blog.nag.com/2011/02/wandering-precision.html) coming out of the same computation, depending on how full the memory is.\r\n\r\nThe ultimate solution is to make your computations robust to small errors/non-determinism. But also, if you narrow down the problem, it might be possible the fix the errors. IE, non-determinism in reduce_sum has been fixed on GPU recently.\r\n\r\nIn your case, loss of symmetry implies that result of `a b'` and `a' b` are not the same. Can isolate a specific `a` and `b` that produce this?\r\n", "OK, so I created a dummy script to reproduce the issue. And I apologize, I don't think the issue ever arises with `tf.matmul(A, A, transpose_b=True)` applied as such. Rather, it happens when `tf.matmul` is applied iteratively on a positive-definite matrix - i.e. `tf.matmul(tf.matmul(c, A), c, transpose_b=True)`.\r\n\r\nHere is an example:\r\n\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    dim = 3\r\n    dimN = 5\r\n\r\n    # We will load a positive definite matrix for K\r\n    K_init = tf.placeholder(tf.float32, [dim, dim])\r\n    K_ = tf.Variable(K_init, trainable=False, collections=[])\r\n\r\n    C = tf.Variable(tf.ones([dimN, dim]), dtype=tf.float32)\r\n    a = tf.Variable(tf.ones([dimN]), dtype=tf.float32)\r\n\r\n    # We build a Recurrent Network for K\r\n    K = [K_]\r\n    # Kca should be symmetric: C*K*C' is symmetric, and A is diagonal; so their sum is symmetric \r\n    Kca = tf.expand_dims(tf.matmul(tf.matmul(C, K[0]), C, transpose_b=True) + tf.diag(a), 0)\r\n    # invKca should be symmetric as the inverse of a Kca\r\n    invKca = tf.expand_dims(tf.linalg.inv(Kca[0]), 0)\r\n    # K[1] should be symmetric since, again C'*invKca*C is symmetric.\r\n    K = tf.concat([K, tf.expand_dims(tf.matmul(tf.matmul(C, invKca[0], transpose_a=True), C), 0)], 0)\r\n\r\n    # Some step cost function\r\n    costN = tf.expand_dims(tf.linalg.logdet(Kca[0]) + tf.matmul(tf.matmul([a], invKca[0]), [a], transpose_b=True)[0][0], 0)\r\n\r\n    # The recursion\r\n    for n in range(1, 10):\r\n        Kca = tf.concat([Kca, tf.expand_dims(tf.matmul(tf.matmul(C, K[n]), C, transpose_b=True) + tf.diag(a), 0)], 0)\r\n        invKca = tf.concat([invKca, tf.expand_dims(tf.linalg.inv(Kca[n]), 0)], 0)\r\n        K = tf.concat([K, tf.expand_dims(tf.matmul(tf.matmul(C, invKca[n], transpose_a=True), C), 0)], 0)\r\n\r\n        costN = tf.concat([costN, tf.expand_dims(tf.linalg.logdet(Kca[n]) + tf.matmul(tf.matmul([a], invKca[n]), [a], transpose_b=True)[0][0], 0)], 0)\r\n\r\n    cost = tf.reduce_sum(costN)\r\n    # If this example blows up, decrease the learning rate.\r\n    step = tf.train.AdamOptimizer(0.001).minimize(cost)\r\n\r\n    sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n\r\n    # Positive definite matrix K0\r\n    L0 = np.array([[1.0, 0.0, 0.0], \\\r\n        [3.5, 2.5, 0.0], \\\r\n        [-2.0, -1.2, 3.0]])\r\n    K0 = np.matmul(L0, L0.transpose())\r\n\r\n    sess.run(K_.initializer, feed_dict={K_init:K0})\r\n    for n in range(100):\r\n        sess.run(step)\r\n        print(\"Cost: \" + str(sess.run(cost)))\r\n\r\n    # Lo and Behold, K[10] is not symmetric\r\n    print(sess.run(K[10]) == sess.run(K[10]).transpose())\r\n    print(sess.run(K[10]))\r\n    print(\"Done\")\r\n\r\nNow I can apply a manual fix using an intermediate step:\r\n`B = tf.matmul(tf.matmul(c, A), c, transpose_b=True)`\r\n`C = 0.5(B + tf.transpose(B))`\r\n\r\nBut this type of process is so common - i.e. estimates of the second moments - that I would expect a more direct implementation.  Suffice it to say that for long recursions and many dimensions, having to add this intermediate step before every `logdet` and `inv` adds a significantly noticeable difference in step time.", "Also, the following variant using a custom `tensor_product` and using `tf.tensordot` suffers the same problem (this version would be necessary to work with higher moments):\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    #  Tensor Product for Ease\r\n    def tensor_product(*e):\r\n        \"\"\" Tensor product of elements \"\"\"\r\n        if len(e) == 1:\r\n            return e\r\n        elif len(e) == 2:\r\n            a, b = e\r\n            r_a = len(a.get_shape().as_list())\r\n            r_b = len(b.get_shape().as_list())\r\n            s_a = tf.concat([tf.shape(a), tf.constant([1] * r_b)], axis=0)\r\n            s_b = tf.concat([tf.constant([1] * r_a), tf.shape(b)], axis=0)\r\n            a_reshaped = tf.reshape(a, s_a)\r\n            b_reshaped = tf.reshape(b, s_b)\r\n            return a_reshaped * b_reshaped\r\n        prod = e[0]\r\n        for tensor in e[1:]:\r\n            prod = tensor_product(prod, tensor)\r\n        return prod\r\n\r\n    dim = 3\r\n    dimN = 5\r\n\r\n    # We will load a positive definite matrix for K\r\n    K_init = tf.placeholder(tf.float32, [dim, dim])\r\n    K_ = tf.Variable(K_init, trainable=False, collections=[])\r\n\r\n    C = tf.Variable(tf.ones([dimN, dim]), dtype=tf.float32)\r\n    a = tf.Variable(tf.ones([dimN]), dtype=tf.float32)\r\n\r\n    CC = tensor_product(C, C)\r\n    aa = tensor_product(a, a)\r\n    # We build a Recurrent Network for K\r\n    K = [K_]\r\n    # Kca should be symmetric: C*K*C' is symmetric, and A is diagonal; so their sum is symmetric \r\n    Kca = tf.expand_dims(tf.tensordot(CC, K[0], [[1, 3], [0, 1]]) + tf.diag(a), 0)\r\n    # invKca should be symmetric as the inverse of a Kca\r\n    invKca = tf.expand_dims(tf.linalg.inv(Kca[0]), 0)\r\n    # K[1] should be symmetric since, again C'*invKca*C is symmetric.\r\n    K = tf.concat([K, tf.expand_dims(tf.tensordot(CC, invKca[0], [[0, 2], [0, 1]]), 0)], 0)\r\n\r\n    # Some step cost function\r\n    costN = tf.expand_dims(tf.linalg.logdet(Kca[0]) + tf.matmul(tf.matmul([a], invKca[0]), [a], transpose_b=True)[0][0], 0)\r\n\r\n    # The recursion\r\n    for n in range(1, 10):\r\n        Kca = tf.concat([Kca, tf.expand_dims(tf.tensordot(CC, K[n], [[1, 3], [0, 1]]) + tf.diag(a), 0)], 0)\r\n        invKca = tf.concat([invKca, tf.expand_dims(tf.linalg.inv(Kca[n]), 0)], 0)\r\n        K = tf.concat([K, tf.expand_dims(tf.tensordot(CC, invKca[n], [[0, 2], [0, 1]]), 0)], 0)\r\n\r\n        costN = tf.concat([costN, tf.expand_dims(tf.linalg.logdet(Kca[n]) + tf.matmul(tf.matmul([a], invKca[n]), [a], transpose_b=True)[0][0], 0)], 0)\r\n\r\n    cost = tf.reduce_sum(costN)\r\n    # If this example blows up, decrease the learning rate.\r\n    step = tf.train.AdamOptimizer(0.001).minimize(cost)\r\n\r\n    sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n\r\n    # Positive definite matrix K0\r\n    L0 = np.array([[1.0, 0.0, 0.0], \\\r\n        [3.5, 2.5, 0.0], \\\r\n        [-2.0, -1.2, 3.0]])\r\n    K0 = np.matmul(L0, L0.transpose())\r\n\r\n    sess.run(K_.initializer, feed_dict={K_init:K0})\r\n    for n in range(100):\r\n        sess.run(step)\r\n        print(\"Cost: \" + str(sess.run(cost)))\r\n\r\n    # Lo and Behold, K[10] is not symmetric\r\n    print(sess.run(K[10]) == sess.run(K[10]).transpose())\r\n    print(sess.run(K[10]))\r\n    print(\"Done\")", "Not clear that this is a bug since some floating point imprecision is expected. IMHO, further isolation is required.", "This is expected behavior as Yaroslav mentioned. So I'm not sure whether there is anything to be done here."]}, {"number": 19749, "title": "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory", "body": "ImportError: Traceback (most recent call last):\r\n  File \"/home/amartya/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/amartya/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/amartya/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n\r\n(From the error message, it seems that the version of TensorFlow and CUDA you have are not compatible. For example, TensorFlow 1.8 binary releases required CUDA 9.0 - https://www.tensorflow.org/install/install_linux#nvidia_requirements_to_run_tensorflow_with_gpu_support.)\r\n\r\n"]}, {"number": 19747, "title": "MyDatasetReaderOp crashes on assertion in tensorflow::core::RefCounted::~RefCounted()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: Not relevant\r\n- **GCC/Compiler version (if compiling from source)**:g++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9.0/7.0.5\r\n- **GPU model and memory**:GTX 1080Ti\r\n- **Exact command to reproduce**:\r\n\r\n```bash\r\n/usr/bin/c++   -Dexample_EXPORTS  -I/opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/include -I/opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/include/external/nsync/public -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC   -std=gnu++11 -o CMakeFiles/example.dir/my-reader-dataset-op.cc.o -c /opt/ssd/ilya/temp/my-reader-dataset-op.cc\r\n/usr/bin/c++  -fPIC -I/opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/include -I/opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/include/external/nsync/public -D_GLIBCXX_USE_CXX11_ABI=0  -shared -Wl,-soname,libexample.so -o libexample.so CMakeFiles/example.dir/my-reader-dataset-op.cc.o -L/opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow -ltensorflow_framework \r\n```\r\n\r\n`my-reader-dataset-op.cc` and `my.py` are copied from https://www.tensorflow.org/extend/new_data_formats#writing_a_dataset_for_a_file_format with two small changes in the Python code:\r\n1) The dataset is wrapped into `batch` Dataset (any other wrapper that calls `Unref` in its destructor would work too, e.g. `cache`).\r\n2) The iterator is initialized twice. The crash happens on the re-initialization attempt.\r\n\r\n```python3\r\nif __name__ == \"__main__\":\r\n  # Create a MyReaderDataset and print its elements.\r\n  with tf.Session() as sess:\r\n    dataset = MyReaderDataset()\r\n    dataset = dataset.batch(1)\r\n    iterator = dataset.make_initializable_iterator()\r\n    sess.run([iterator.initializer])\r\n    print(\"First init OK\")\r\n    sess.run([iterator.initializer])\r\n    print(\"Reinit OK\")\r\n    next_element = iterator.get_next()\r\n    try:\r\n      while True:\r\n        print(sess.run(next_element))  # Prints \"MyReader!\" ten times.\r\n    except tf.errors.OutOfRangeError:\r\n      pass\r\n```\r\n\r\n```gdb\r\nFirst init OK\r\n2018-06-04 14:52:16.062227: F /opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/lib/core/refcount.h:79] Check failed: ref_.load() == 0 (1 vs. 0)\r\n\r\nThread 59 \"python3\" received signal SIGABRT, Aborted.\r\n[Switching to Thread 0x7ff95bfff700 (LWP 24856)]\r\n0x00007ffff7825428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n54\t../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n(gdb) bt\r\n#0  0x00007ffff7825428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n#1  0x00007ffff782702a in __GI_abort () at abort.c:89\r\n#2  0x00007fffa62a1174 in tensorflow::internal::LogMessageFatal::~LogMessageFatal() ()\r\n   from /opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fffe6cf34e4 in tensorflow::core::RefCounted::~RefCounted() () from ./libexample.so\r\n#4  0x00007fffe6cf4670 in tensorflow::DatasetBase::~DatasetBase() () from ./libexample.so\r\n#5  0x00007fffe6cf4a7c in tensorflow::GraphDatasetBase::~GraphDatasetBase() () from ./libexample.so\r\n#6  0x00007fffe6cf25fc in tensorflow::(anonymous namespace)::MyReaderDatasetOp::Dataset::~Dataset() ()\r\n   from ./libexample.so\r\n#7  0x00007fffe6cf262c in tensorflow::(anonymous namespace)::MyReaderDatasetOp::Dataset::~Dataset() ()\r\n   from ./libexample.so\r\n#8  0x00007fffa941529f in tensorflow::(anonymous namespace)::BatchDatasetOp::Dataset::~Dataset() ()\r\n   from /opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fffa9414f3b in tensorflow::(anonymous namespace)::BatchDatasetOp::Dataset::Iterator::~Iterator() ()\r\n   from /opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007fffa84387b9 in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release() ()\r\n   from /opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007fffa945e6b6 in tensorflow::(anonymous namespace)::MakeIteratorOp::Compute(tensorflow::OpKernelContext*) ()\r\n   from /opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007fffa664150c in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n   from /opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#13 0x00007fffa660724d in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()\r\n   from /opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#14 0x00007fffa65f5fd0 in std::_Function_handler<void (), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) ()\r\n   from /opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#15 0x00007fffa626d6aa in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#16 0x00007fffa626c752 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /opt/ssd/ilya/temp-environment-for-bugreport/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#17 0x00007fff9d36fc80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#18 0x00007ffff7bc16ba in start_thread (arg=0x7ff95bfff700) at pthread_create.c:333\r\n#19 0x00007ffff78f741d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n(gdb) \r\n```\r\n\r\nIt looks like `Unref` in https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/core/kernels/data/batch_dataset_op.cc#L62 observes ref-count `1` and causes the destructor `MyReaderDatasetOp::Dataset::~Dataset()` to be called via `delete this`: https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/core/lib/core/refcount.h#L93. However, later in destructor unfolding non-zero count is observed for the same object, which causes assertion failure.\r\n\r\nThe error doesn't reproduce with built-in datasets, e.g. TextLineReader or RangeDataset. However, if I copy RangeDataset code and rebuild it as MyRangeDataset, it reproduces with exactly the same symptoms.", "comments": ["See https://github.com/tensorflow/tensorflow/issues/17316", "@sjperkins Simon: thanks, that makes perfect sense.\r\n\r\n(What happens is: the BatchDatasetOp, compiled with NDEBUG in the stock tensorflow package, doesn't actually decrement the counter when it is equal to 1 and just calls the destructor. MyDatasetOp, compiled without NDEBUG, fails on assertion because the check is not cut out.)"]}, {"number": 19746, "title": "FLOP calculation by tf.profiler might be wrong", "body": "Have I written custom code: Yes\r\nOS Platform and Distribution: macOS 10.11.13\r\nTensorFlow installed from: pip \r\nTensorFlow version: 1.6.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: see code below\r\n\r\n```\r\nimport tensorflow as tf\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    m, p, q = 25, 16, 9\r\n    A = tf.Variable(tf.zeros([m, p]))\r\n    B = tf.Variable(tf.zeros([p, q]))\r\n    C = tf.matmul(A,B)\r\n\r\n    flops = tf.profiler.profile(g, options = tf.profiler.ProfileOptionBuilder.float_operation())\r\n    if flops is not None:\r\n        print('FLOP should be', m * q * (2 * p - 1))\r\n        print('Calculated FLOP', flops.total_float_ops)\r\n```\r\n\r\n\r\nGiven two matrices `A` of shape `(m, p)` and `B` of shape `(p, q)`, the number of floating point operations (FLOP) should be `mq(2p-1)`.\r\n\r\nCalculating the number of FLOP using tensorflow's profiler gives `2mqp` instead of `mq(2p-1)`.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have you read the caveats on [floating point profiling](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md#profile-model-float-operations)? It looks like some of these may apply to your case?", "Yes, I've read the caveats.\r\nIn my example, all the shapes are well defined and there's not ambiguity (no `tf.while`). I took one of the simplest case possible: one matrix multiplication.", "As the docs note, contributions here would be welcome. Are you interested in looking into this issue?", "I'll try to find some time and update this issue accordingly.", "https://github.com/tensorflow/tensorflow/blob/397f04acb1faeff451691d7fdc0f754eeb547cc1/tensorflow/python/ops/math_ops.py#L2023-L2036\r\n", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to staleness. Please use the latest version for TensorFlow and test again. Feel free to open a new issue if it still persists. Thanks!", "@benmyara I came across the same issue and I think the issue is related to the fact that you need to store the results somewhere, at least this is the explanation that I came up with. \r\nYou initialize a result matrix filled with zeros and then you add to each entry the result of the row x column multiplication. This way, one entry of the result matrix requires p multiplications, which are summed using p-1 summations. By adding it to the result matrix, you obtain one additional flop, yielding\r\n ```\r\nflops per entry = p + p - 1 + 1 = 2 p\r\n```\r\nGiven the shape of the result matrix, you obtain a total of 2mpq flops."]}, {"number": 19745, "title": "Jupyter Example: ImportError: Could not find 'cudart64_90.dll'", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pip install --upgrade tensorflow-gpu\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: Python 3.5.5 \r\n- **Bazel version (if compiling from source)**: no\r\n- **GCC/Compiler version (if compiling from source)**: wasn't used\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN v7.0.5 (Dec 5, 2017), for CUDA 9.0, Library for Windows 10\r\n- **GPU model and memory**: GeForce 940MX 2GB\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport numpy as np\r\nimport os\r\nimport six.moves.urllib as urllib\r\nimport sys\r\nimport tarfile\r\nimport tensorflow as tf\r\nimport zipfile\r\n\r\nfrom collections import defaultdict\r\nfrom io import StringIO\r\nfrom matplotlib import pyplot as plt\r\nfrom PIL import Image\r\n\r\n# This is needed since the notebook is stored in the object_detection folder.\r\nsys.path.append(\"..\")\r\nfrom object_detection.utils import ops as utils_ops\r\n\r\nif tf.__version__ < '1.4.0':\r\n  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\r\n\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\nd:\\programfiles\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py in preload_check()\r\n     74         try:\r\n---> 75           ctypes.WinDLL(build_info.cudart_dll_name)\r\n     76         except OSError:\r\n\r\nd:\\programfiles\\anaconda3\\envs\\tensorflow1\\lib\\ctypes\\__init__.py in __init__(self, name, mode, handle, use_errno, use_last_error)\r\n    350         if handle is None:\r\n--> 351             self._handle = _dlopen(self._name, mode)\r\n    352         else:\r\n\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-4a3efc4cbb3c> in <module>()\r\n      4 import sys\r\n      5 import tarfile\r\n----> 6 import tensorflow as tf\r\n      7 import zipfile\r\n      8 \r\n\r\nd:\\programfiles\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 # pylint: disable=wildcard-import\r\n     26 from tensorflow.tools.api.generator.api import *  # pylint: disable=redefined-builtin\r\n\r\nd:\\programfiles\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nd:\\programfiles\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     28 # Perform pre-load sanity checks in order to produce a more actionable error\r\n     29 # than we get from an error during SWIG import.\r\n---> 30 self_check.preload_check()\r\n     31 \r\n     32 # pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nd:\\programfiles\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py in preload_check()\r\n     80               \"environment variable. Download and install CUDA %s from \"\r\n     81               \"this URL: https://developer.nvidia.com/cuda-toolkit\"\r\n---> 82               % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\n     83 \r\n     84       if hasattr(build_info, \"cudnn_dll_name\") and hasattr(\r\n\r\nImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit\r\n\r\n```\r\n\r\nI installed **CUDA 9.0** and extracted **cudnn-9.0-windows10-x64-v7.zip** to `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0`\r\n\r\n![screenshot_10](https://user-images.githubusercontent.com/8851301/40907717-5a3e947a-67ed-11e8-9fea-44b2d4f856cf.png)\r\n\r\nI have next environment variables:\r\n\r\n![screenshot_11](https://user-images.githubusercontent.com/8851301/40907722-5e4bc4e8-67ed-11e8-86b3-8df26821ab7e.png)\r\n\r\n![screenshot_12](https://user-images.githubusercontent.com/8851301/40907724-5f8c666e-67ed-11e8-92d6-8d735abb6dc8.png)\r\n\r\nSo I don't understand what the problem is\r\nI have this file, I have needed variables (paths) but it still isn't working\r\n\r\n![screenshot_13](https://user-images.githubusercontent.com/8851301/40907826-ab3372a6-67ed-11e8-8a2e-c745bbd130e0.png)\r\n\r\n![screenshot_14](https://user-images.githubusercontent.com/8851301/40907959-0db26fb8-67ee-11e8-93fd-3316bc60819c.png)", "comments": ["I had the same problem. But mine was happend with CUDA9.2, TensorFlow 1.8.0\r\nI think it beacause the tensowflow1.8 wasn't support cuda92.", "@onebits  your problem is easy, use cuda9.0, because of course 9.2 doesn't have cudart64_90, it has cudart64_92... or something similar\r\n\r\nmy issue is more difficult, it still isn't solved ", "@anonym24  Yes, I know that. But I don't want to install CUDA9.0 Although they can coexist(9.2 and 9.0).\r\nI fix it.\r\nI download the tensorflow with whl file from:\r\nhttps://github.com/fo40225/tensorflow-windows-wheel\r\n\r\nThere is alot of tensorflow's wheel files by someone rebuild for support any CUDA version.\r\n\r\nand last, Thank you for answer and so quick....", "I met the same problem, I restart my computer ,so I solve my problem , of couse, I use the cuda9.2, I install the tensorflow-gpu with tensorflow_gpu-1.8.0-cp36-cp36m-win_amd64.whl.", "Nagging Assignee @tatatodd: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please try updating your PATH variable too. \r\nAlso, this question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19744, "title": "Warm start with distribute.MirroredStrategy not working", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 16.04\r\n- **TensorFlow installed from (source or binary)**: source \r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.11\r\n- **GCC/Compiler version (if compiling from source)**: gcc 5.4.0\r\n- **CUDA/cuDNN version**: 9.0 - 7.1\r\n- **GPU model and memory**: Titan X\r\n- **Exact command to reproduce**: N/A\r\n\r\nWe are trying to use distribute.MirroredStrategy to train a model on multiple GPUs on a single machine. We have a working implementation without warm start (training from scratch) and now want to initialise to model using checkpoint from imagenet before the training. \r\nOur first attempt is to add `init_from_checkpoint` in our model function:\r\n\r\n```\r\ndef model_fn(features, labels, mode, params):\r\n\r\n    .....\r\n\r\n   tf.train.init_from_checkpoint(params['resnet_checkpoint'], {'/': 'resnet50/'})\r\n\r\n   ....\r\n```\r\n\r\nBut, this gives us the following error\r\n\r\n```\r\n.../tensorflow/contrib/distribute/python/values.py\", line 285, in _get_update_device\r\n    \"Use DistributionStrategy.update() to modify a MirroredVariable.\")\r\n```\r\n\r\nIs there an example of how to warm start a training with an existing checkpoint ?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Another attempt to warm start the training with MirroredStrategy is to use `tf.estimator.WarmStartSetting` as follow :\r\n\r\n```\r\nws = tf.estimator.WarmStartSettings(\r\n        ckpt_to_initialize_from=params['resnet_checkpoint'],\r\n        vars_to_warm_start='resnet50.*',\r\n        var_name_to_prev_var_name=var_name_to_prev_var_name\r\n    )\r\n\r\nsession_config = tf.ConfigProto(allow_soft_placement=True)\r\n\r\nif FLAGS.num_gpus == 0:\r\n        distribution = tf.contrib.distribute.OneDeviceStrategy('device:CPU:0')\r\nelif FLAGS.num_gpus == 1:\r\n        distribution = tf.contrib.distribute.OneDeviceStrategy('device:GPU:0')\r\nelse:\r\n        distribution = tf.contrib.distribute.MirroredStrategy(\r\n            num_gpus=FLAGS.num_gpus\r\n        )\r\nrun_config = tf.estimator.RunConfig(train_distribute=distribution,\r\n                                        session_config=session_config)\r\n\r\nestimator = tf.estimator.Estimator(\r\n        model_fn=model_function,\r\n        params=params,\r\n        config=run_config,\r\n        model_dir=FLAGS.model_dir,\r\n        warm_start_from=ws\r\n    )\r\n```\r\n\r\nBut this fails as well, with the following error:\r\n`TypeError: var MUST be one of the following: a Variable, list of Variable or PartitionedVariable, but is <class 'tensorflow.contrib.distribute.python.values.MirroredVariable'>`", "Same here, initializing with `tf.train.init_from_checkpoint` inside `model_fn` gives following stacktrace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 87, in <module>\r\n    tf.app.run(main)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 64, in main\r\n    estimator.train(train_input_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 368, in train\r\n    return self\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 295, in _mode\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 366, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1117, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1253, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5285, in get_controller\r\n    yield g\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5093, in get_controller\r\n    yield default\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5285, in get_controller\r\n    yield g\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 295, in _mode\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5285, in get_controller\r\n    yield g\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1253, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 318, in __exit__\r\n    self._var_creator_scope.__exit__(exception_type, exception_value, traceback)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2316, in variable_creator_scope\r\n    yield\r\nFile \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2961, in _variable_creator_scope\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2316, in variable_creator_scope\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1160, in _train_model_distributed\r\n    self.config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 794, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 269, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/alexandr/.local/lib/python3.5/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 480, in run\r\n    self.done = True\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 295, in _mode\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 480, in run\r\n    self.done = True\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 514, in device_policy\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 480, in run\r\n    self.done = True\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 50, in _enter_graph\r\n    yield\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5285, in get_controller\r\n    yield g\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5093, in get_controller\r\n    yield default\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5285, in get_controller\r\n    yield g\r\nFile \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 295, in _mode\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5285, in get_controller\r\n    yield g\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 50, in _enter_graph\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 480, in run\r\n    self.done = True\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4371, in device\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 480, in run\r\n    self.done = True\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 6017, in __exit__\r\n    self._name_scope.__exit__(type_arg, value_arg, traceback_arg)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4141, in name_scope\r\n    yield \"\" if new_stack is None else new_stack + \"/\"\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 480, in run\r\n    self.done = True\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2130, in __exit__\r\n    self._graph_context_manager.__exit__(type_arg, value_arg, traceback_arg)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5285, in get_controller\r\n    yield g\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5093, in get_controller\r\n    yield default\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5285, in get_controller\r\n    yield g\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 295, in _mode\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5285, in get_controller\r\n    yield g\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 480, in run\r\n    self.done = True\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\nFile \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2316, in variable_creator_scope\r\n    yield\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2961, in _variable_creator_scope\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2316, in variable_creator_scope\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 479, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1107, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/alexandr/Code/PAN_segmentation/model_builder.py\", line 160, in model_fn\r\n    {'resnet_v2_101/': 'resnet_v2_101/'})\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpoint_utils.py\", line 263, in init_from_checkpoint\r\n    _set_variable_or_list_initializer(var, ckpt_file, full_tensor_name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpoint_utils.py\", line 337, in _set_variable_or_list_initializer\r\n    _set_checkpoint_initializer(variable_or_list, ckpt_file, tensor_name, \"\")\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpoint_utils.py\", line 306, in _set_checkpoint_initializer\r\n    variable._initial_value = restore_op  # pylint:disable=protected-access\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4371, in device\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpoint_utils.py\", line 306, in _set_checkpoint_initializer\r\n    variable._initial_value = restore_op  # pylint:disable=protected-access\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4371, in device\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpoint_utils.py\", line 303, in _set_checkpoint_initializer\r\n    init_op = state_ops.assign(variable, restore_op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 220, in assign\r\n    return ref.assign(value, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 316, in assign\r\n    return self.get(device=_get_update_device()).assign(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 285, in _get_update_device\r\n    \"Use DistributionStrategy.update() to modify a MirroredVariable.\")\r\nRuntimeError: Use DistributionStrategy.update() to modify a MirroredVariable.\r\nException ignored in: <generator object get_controller at 0x7f1456390410>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5287, in get_controller\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 136, in pop\r\nIndexError: pop from empty list\r\n```", "@guptapriya Do you have an idea of the root of (and solution to) this problem?", "@jppgks yeah the root cause is that we haven't yet added support for mirrored variables (used by mirrored strategy) in all paths that can be used for restoring from checkpoints. See this duplicate bug for more details:\r\nhttps://github.com/tensorflow/tensorflow/issues/19958\r\n", "Please follow the other bug https://github.com/tensorflow/tensorflow/issues/19958 for progress, thanks! ", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The fix has been merged: https://github.com/tensorflow/tensorflow/commit/f1de0ddd55dcae6237ea7d21ccddcc6467a6cf8b\r\n\r\nYou should be able to test this out in a nightly build soon. \r\n\r\n"]}, {"number": 19743, "title": "error\uff1aLibrary objects cannot exceed 65,536\uff0cwin10 + TensorFlowGPU ", "body": "**System information:**\r\n\u00b7 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No\r\n\r\n\u00b7 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Win10-x64\r\n\r\n\u00b7 TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow.git\r\n\r\n\u00b7 TensorFlow version (use command below):  r1.4, r1.5, r1.6  r1.7 and r1.8,  command : \r\nv1.8's command : git checkout -b v1.8 -f origin/r1.8\r\nv1.7's command : git checkout -b v1.7 -f origin/r1.7\r\nv1.6's command : git checkout -b v1.6 -f origin/r1.6\r\nand so on include v1.4 and v1.5\r\n\r\n\u00b7 Python version:  Anaconda3 - python3.6\r\n\r\n\u00b7 Bazel version (if compiling from source):  I used CMAKE 3.11.1\r\n\r\n\u00b7 GCC/Compiler version (if compiling from source):  Visual Studio 2015( VS15' MSBuild)\r\n\r\n\u00b7 CUDA/cuDNN version:  CUDA9.0,   cudnn-9.0-win10-7.1\r\n\r\n\u00b7 GPU model and memory: GTX-860m with 2Gb Memory\r\n\r\n\r\n\r\nExact command to reproduce: \r\n\"\r\nD:\\ProgramData\\tensorflow\\tensorflow\\contrib\\cmake\\build> cmake .. -A x64 -DCMAKE_BUILD_TYPE=Debug -DSWIG_EXECUTABLE=D:/soft/TensorflowSoft/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/ProgramData/Anaconda3/python.exe -DPYTHON_LIBRARIES=D:/ProgramData/Anaconda3/libs/python36.lib -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"D:\\soft\\TensorflowSoft\\cudnn\" -G \"Visual Studio 14 2015\"\r\n\r\nD:\\ProgramData\\tensorflow\\tensorflow\\contrib\\cmake\\build> set PreferredToolArchitecture=x64 \r\n\r\nD:\\ProgramData\\tensorflow\\tensorflow\\contrib\\cmake\\build> MSBuild /p:Configuration=Release ALL_BUILD.vcxproj\r\n\"\r\n\r\n**Describe the problem** : \r\nI build TensorFlow-CPU-r1.8 successfully (win10-x64 + Anaconda3-python3.6 + **VS2017**(not VS2015) +  tensorflow1.8 + cmake3.11.1 + SwigWin3.0.12).\r\n \r\nBut when I build tensorflow-**GPU** version(both tensorflow-r1.7 and r1.8), the only error has occurred: The library objects cannot exceed 65,536. (If I use VS2017, it shows error: \" the compiler is not supported for CUDA 9.0\" , so I switch Visual Studio version to VS2015, and CMAKE command is work successfully.)\r\n\r\nAfter that, I switched to lower versions TensorFlow-r1.6 to TensorFlow-r1.5, but Link error occurred\uff1a\r\n\r\n\u201clibprotobufd.lib(text_format.obj) : error LNK2019: unresolved external symbol __std_reverse_trivially_swappable_8 referenced in function \"void _\r\n_cdecl std::_Reverse_unchecked1<class google::protobuf::Message const * *>(class google::protobuf::Message const * * const,class google::protobuf:\r\n:Message const * * const,struct std::integral_constant<unsigned __int64,8>)\" (??$_Reverse_unchecked1@PEAPEBVMessage@protobuf@google@@@std@@YAXQEAP\r\nEBVMessage@protobuf@google@@0U?$integral_constant@_K$07@0@@Z) [D:\\tf\\tensorflowGPU\\tensorflow\\contrib\\cmake\\build\\proto_text.vcxproj]\r\n  libprotobufd.lib(wire_format.obj) : error LNK2001: unresolved external symbol __std_reverse_trivially_swappable_8 [D:\\tf\\tensorflowGPU\\tensorflo\r\nw\\contrib\\cmake\\build\\proto_text.vcxproj]\r\n  D:\\tf\\tensorflowGPU\\tensorflow\\contrib\\cmake\\build\\Debug\\proto_text.exe : fatal error LNK1120: 1 unresolved externals [D:\\tf\\tensorflowGPU\\tenso\r\nrflow\\contrib\\cmake\\build\\proto_text.vcxproj]\u201d\r\n\r\nI don't know how to solve those errors...", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have updated my issue, @tensorflowbutler , and thank you very much for helping me !", "Marking this as community support, because we don't support debug builds on Windows.\r\n\r\n/CC @gunan, who was looking at the symbol limit recently, and may have some suggestions."]}, {"number": 19742, "title": "Attr 'Tshape' not found in Python2 but found in Python3", "body": "\r\n### System information\r\n- Working off extension to mnist demo [here](https://github.com/uTensor/utensor-mnist-demo):\r\n- **macOS High Sierra 10.13.4**:\r\n- **TensorFlow installed from pip**:\r\n- **v1.8.0-0-g93bc2e2072 1.8.0**:\r\n- **Python 3.6.4, Python 2.7.14**: \r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhile trying to quantize a frozen TF graph I get the following error in Python2, *but not in Python3*.\r\n\r\n```\r\nFile \"/Users/micbar02/anaconda2/lib/python2.7/site-packages/utensor_cgen/code_generator.py\", line 114, in _transform_graph\r\n    transforms=[\"quantize_weights\", \"quantize_nodes\"])\r\n  File \"/Users/micbar02/anaconda2/lib/python2.7/site-packages/tensorflow/tools/graph_transforms/__init__.py\", line 51, in TransformGraph\r\n    transforms_string, status)\r\n  File \"/Users/micbar02/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'Tshape' in NodeDef:\r\n\t [[Node: MatMul_eightbit/x/reshape = Reshape[T=DT_FLOAT](x, MatMul_eightbit/x/reshape_dims)]]\r\n```\r\n\r\nFYI, we have a Cloud9 setup as well but we don't experience this issue there. It seems to be Mac only\r\n\r\n### Source code / logs\r\nHere is an example to recreate the issue using a model generated by this script https://github.com/uTensor/utensor-mnist-demo/blob/master/tensorflow-models/deep_mlp.ipynb:\r\n\r\n```\r\ndef _transform_graph(self, pb_file):\r\n  with tf.gfile.FastGFile(pb_file, 'rb') as fid:\r\n    graph_def = GraphDef()\r\n    graph_def.ParseFromString(fid.read())\r\n    # Fails on the following line\r\n    quant_graph_def = TransformGraph(input_graph_def=graph_def,\r\n                                     inputs=[],\r\n                                     outputs=self.output_nodes,\r\n                                     transforms=[\"quantize_weights\", \"quantize_nodes\"])\r\n    temp_file.write(quant_graph_def.SerializeToString())\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19741, "title": "Issue on using custom Op with TensorRT", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI am using tensor2tensor library.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nTensorflow docker image with \"1.7.1-devel-gpu\" tag\r\n- **TensorFlow installed from (source or binary)**:\r\nSource(From the docker image, nothing changed), with TensorRT enabled\r\n- **TensorFlow version (use command below)**:\r\n1.7.1\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.11.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0\r\n- **GPU model and memory**:\r\nGTX 1080 Ti 11 G\r\n- **Exact command to reproduce**:\r\nSee problem description.\r\n\r\n### Describe the problem\r\nI am trying to use TensorRT feature to optimize the inference performance of Tensor2Tensor.  But got Op type not registered issue. \r\n\r\n### Source code / logs\r\n```python\r\ngraph = tf.get_default_graph().as_graph_def()\r\nfrozen_graph = tf.graph_util.remove_training_nodes(graph)\r\ntrt_graph = trt.create_inference_graph(\r\n    input_graph_def = frozen_graph, \r\n    outputs=[model_output],\r\n    max_batch_size=10)\r\n```\r\nbelow error shows when create_inference_graph is called. \r\n\r\n>tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'convert_gradient_to_tensor_cc661786' in binary running on cffae59e0618. Makre sure the Op and Kernal are registered in the binary running in this process. \r\n\r\nconvert_gradient_to_tensor is defined in tensor2tensor in below way. \r\n\r\n```python\r\n@function.Defun(\r\n    python_grad_func=lambda x, dy: tf.convert_to_tensor(dy),\r\n    shape_func=lambda op: [op.inputs[0].get_shape()])\r\ndef convert_gradient_to_tensor(x):\r\n    ...\r\n```\r\n", "comments": ["remove this line, it works. frozen_graph = tf.graph_util.remove_training_nodes(graph)"]}, {"number": 19740, "title": "tflitecamerademo numThreads", "body": "hi,\r\n    I run tflitecamerademo on my device.\r\n    I change some parameter:\r\n    1.  tflite = new Interpreter(loadModelFile(activity),1);    ----> model inference: 250 ms\r\n    2.  tflite = new Interpreter(loadModelFile(activity),4);    ----> model inference: 510 ms\r\n    What is the role of this parameter(numThreads)\uff1a\r\n    Interpreter(@NonNull MappedByteBuffer mappedByteBuffer, int numThreads)\r\nWhy 4 threads are slower than 1 thread\uff0cOnly set to 1 thread, model prediction is fastest\r\nNo matter how you set it, the CPU usage has not improved (checked with the top command)\r\n\r\nthanks.\r\n", "comments": ["    // TODO(ahentz): find a way to avoid this. It causes gemmlowp and eigen to                                                                                                                              \r\n    // be required in order to compile the framework.                                                                                                                                                       \r\n    gemm_support::SetNumThreads(&context_, num_threads);                                                                                                                                                    \r\n    eigen_support::SetNumThreads(&context_, num_threads); \r\n", "@zhongleiwang thanks, but \"gemm_support::SetNumThreads\" is C++ interface\r\nthere is no JAVA interface in \"Interpreter.class\"\r\nI work on Android", "@jdduke, could you ptal? Thanks!", "@bigbao9494: Can you share the exact model and Android version of your phone?\r\n\r\nIncreasing the number of threads doesn't necessarily improve performance for all models on all phones. In the demo's UI you should be able to adjust the number of threads and observe the performance change in real-time. For example, on a Pixel XL, increasing from 1 to 4 threads yields a steady increase in performance, but at 5 threads and beyond performance actually degrades. We also recently fixed an issue where the number of threads for convolution had been hardcoded to 4.", "Friendly ping, @bigbao9494 would you mind sharing more details about the make/model/Android version of your phone?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19739, "title": "Run label_image.py for all images in a folder", "body": "I tried to run label_image.py for all images in a folder as below but it seems that I cannot pass variable to the --image parameter of the label_image.py in the codelab https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#5\r\n\r\nHow can I run label_image.py through all images?\r\n\r\nos.chdir(\"D:/Tensorflow/tf_files/classroom_pixel_test/Empty\")\r\nfor filename in os.listdir(os.getcwd()):\r\n    if filename.endswith(\".jpg\"): \r\n        display(Image(filename))\r\n        imagefile1=os.getcwd()+\"\\\\\"+filename\r\n        imagefile2=\"D:/Tensorflow/tf_files/classroom_pixel_test/Empty/\"+filename\r\n        imagefile3=\"D:/Tensorflow/tf_files/classroom_pixel_test/Empty/29.jpg\"\r\n        print(imagefile1)\r\n        print(imagefile2)\r\n        print(os.getcwd())\r\n        print(filename)\r\n        #%run D:/Tensorflow/label_image --graph=D:/Tensorflow/tf_files/retrained_graph_classroom_pixel_LR_0.5.pb --labels=D:/Tensorflow/tf_files/retrained_labels_classroom_pixel_LR_0.5.txt --image=ph_classroom_pixel_LR_0.5.pb --labels=D:/Tensorflow/tf_files/retrained_labels_classroom_pixel_LR_0.5.txt --image=ph_classroom_pixel_LR_0.5.pb --labels=D:/Tensorflow/tf_files/retrained_labels_classroom_pixel_LR_0.5.txt --image=D:/Tensorflow/tf_files/classroom_pixel_test/Empty/29.jpg --output_layer=final_result --input_layer=Placeholder\r\n        %run D:/Tensorflow/label_image --graph=D:/Tensorflow/tf_files/retrained_graph_classroom_pixel_LR_0.5.pb --labels=D:/Tensorflow/tf_files/retrained_labels_classroom_pixel_LR_0.5.txt --image=ph_classroom_pixel_LR_0.5.pb --labels=D:/Tensorflow/tf_files/retrained_labels_classroom_pixel_LR_0.5.txt --image=ph_classroom_pixel_LR_0.5.pb --labels=D:/Tensorflow/tf_files/retrained_labels_classroom_pixel_LR_0.5.txt --image=imagefile3 --output_layer=final_result --input_layer=Placeholder\r\n        continue                        \r\n    else:\r\n        continue", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: Yes\r\n OS Platform and Distribution: Windows 10\r\n TensorFlow installed from: conda\r\n TensorFlow version: 1.8.0\r\n Bazel version: NIL\r\n CUDA/cuDNN version: NIL\r\n GPU model and memory: NIL\r\n Exact command to reproduce: as above", "Is it possible to make label_image.py acceptable to pass variable to the --image parameter?", "Make sure that the category names don't have escape characters.\r\nYou can create a shell file that iterates over all images as follows:\r\n```\r\nimport os\r\nl=os.listdir(os.getcwd()+'/data') # Assuming data has is the parent directory of images\r\ndef dotremove(poi):  \r\n    for x in poi:\r\n        if x.startswith('.'):\r\n            poi.remove(x)\r\ndotremove(l) #To remove hidden files\r\nfor x in l:\r\n    txt=str(x)+'.sh'\r\n    file=open(txt,\"w\")\r\n    p=os.listdir(os.getcwd()+'/data/'+x)\r\n    dotremove(p)\r\n    for j in p:\r\n        print('python label_image.py --image=/data/'+x+'/'+j+' --input_height=224 --input_width=224 --graph=retrained_graph.pb --labels=labels.txt  --input_layer=\"input\" --output_layer=\"final_result\"',file=open(txt,\"a\"))\r\n```\r\nYou'll have a `.sh` script for each category. Run these `.sh` scripts.", "Thanks duplex143.\r\n\r\nI tried to run it in Jupyter notebook and found this line doesn't run. How to modify it to show the result?\r\nprint('python scripts/label_image.py --image=/tf_files/classroom_photos_test/'+x+'/'+j+' --input_height=224 --input_width=224 --graph=tf_files/retrained_graph_classroom_LR_0.005.pb --labels=tf_files/retrained_labels_classroom_LR_0.005.txt  --input_layer=\"input\" --output_layer=\"final_result\"',file=open(txt,\"a\"))\r\n\r\nIn fact, I want to write the result back to database like MySQL. Any reference for doing that?", "What error are you getting for that line? Try using python3.\r\nYou can use [Psycopg](http://initd.org/psycopg/docs/) to read results from DB.\r\n\r\n", "Hi duplex143\r\nI am using python3 but don't get any output or error in Jupyter notebook. What is the normal output look?Is there anything wrong in --image parameter? \r\nWhat the file=open(txt,\"a\") do? Is the result output to a text file?\r\n", "You'll not get any output in the notebook. `file=open(txt,\"a\")` redirects all the output to the file named `txt`. So, search for `txt` named file. That will be a shell file.\r\nThe output will look like this:\r\n```\r\npython label_image.py --image=9733.jpg --input_height=299 --input_width=299 --graph=retrained_graph.pb --labels=retrained_labels.txt  --input_layer=\"Mul\" --output_layer=\"final_result\"\r\npython label_image.py --image=14147.jpg --input_height=299 --input_width=299 --graph=retrained_graph.pb --labels=retrained_labels.txt  --input_layer=\"Mul\" --output_layer=\"final_result\"\r\npython label_image.py --image=63.jpg --input_height=299 --input_width=299 --graph=retrained_graph.pb --labels=retrained_labels.txt  --input_layer=\"Mul\" --output_layer=\"final_result\"\r\npython label_image.py --image=6400.jpg --input_height=299 --input_width=299 --graph=retrained_graph.pb --labels=retrained_labels.txt  --input_layer=\"Mul\" --output_layer=\"final_result\"\r\npython label_image.py --image=34297.jpg --input_height=299 --input_width=299 --graph=retrained_graph.pb --labels=retrained_labels.txt  --input_layer=\"Mul\" --output_layer=\"final_result\"\r\n```", "Thanks. I was able to run the .sh file generated in terminal to get the batch classification results. I tried to run the .sh file in Jupyter with the following error. Any way to modify the syntax to make it run in Jupyter?  \r\n\r\n%run Empty.sh\r\nFile \"/mnt2/cyrret/Tensorflow/Empty.sh\", line 1\r\n    python scripts/label_image.py --image=tf_files/classroom_photos_test/Empty/30.jpg --input_height=224 --input_width=224 --graph=tf_files/retrained_graph_classroom_LR_0.005.pb --labels=tf_files/retrained_labels_classroom_LR_0.005.txt\r\n                 ^\r\nSyntaxError: invalid syntax\r\n\r\nAlso, how can I get variables to hold the classification results as below for writing to DB?\r\n\r\nempty (score=0.99997)\r\ntwenty (score=0.00003)\r\nfifty (score=0.00000)\r\neighty (score=0.00000)", "For executing a command in python, you can use `os.system(cmd)`\r\nAs for DB schema, it's up to you.\r\nModify the output of the .sh file so that it's something like this:\r\n```\r\n1.jpg,empty (score=0.99997),twenty (score=0.00003),fifty (score=0.00000),eighty (score=0.00000)\r\n2.jpg, twenty (score=0.99996), empty (score=0.00003),fifty (score=0.00001),eighty (score=0.00000)\r\n```\r\n```\r\nCREATE TABLE \"Classification\" (\r\n    \t\t\"Img_name\" varchar(100),\r\n\t\t    \"Class_1\" varchar(100),\r\n\t\t    \"Class_2\" varchar(100),\r\n                   \"Class_3\" varchar(100),\r\n                    \"Class_4\" varchar(100)\r\n\t)\r\ncopy \"Classification\" FROM 'results.csv' DELIMITER ',' CSV HEADER;\r\n```", "@duplex143 \r\nI was able to output the following to csv but not change to the format as you described. How to modify the output of the .sh file?\r\n\r\nEvaluation time (1-image): 0.118s\r\n\r\ntwenty (score=0.99479)\r\nfifty (score=0.00512)\r\neighty (score=0.00006)\r\nempty (score=0.00003)", "Nagging Assignee @MarkDaoust: It has been 126 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi guys.\r\n\r\nThanks for helping out @duplex143.\r\n\r\nThe TensorFlow code in that tutorial is ancient. At this point we're either going to delete or completely-replace that tutorial. We know we have to do this.\r\n\r\nSo I'm not sure it helps to keep this issue open."]}, {"number": 19738, "title": "Android Picture improves image clarity and cannot be used", "body": "![image](https://user-images.githubusercontent.com/19525589/40902335-464f0a12-6806-11e8-933c-bb668e56affa.png)\r\n\r\nI got a picture of the current TF input and saw that the resolution is not high. I tried to change to an HD picture, but\r\n\r\n![image](https://user-images.githubusercontent.com/19525589/40902543-e17d1b28-6806-11e8-8d3c-4b84a09d0f7d.png)\r\nAfter the modification, I could not identify it. I would like to ask about the definition of the image at the time of design. Can't modify and use the high-resolution graphics?", "comments": ["![image](https://user-images.githubusercontent.com/19525589/40902890-1e69e9f2-6808-11e8-91f1-2e0c76ac8a61.png)\r\nafter edited bitmap code", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19737, "title": "Fixed sign-compare build error for interpreter.h", "body": "Fixed sign-compare build error for  interpreter.h.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I have signed the cla", "CLAs look good, thanks!\n\n<!-- ok -->", "@aselle this is a one-liner.", "What is the breakage caused by this? Is this a compiler warning you are trying to fix. One issue with this change is it creates a backward incompatible change to the c++ api. So I am inclined to suggets a cast in the interior of the function.", "Yes, its a fix for compiler warning.\r\nWe are trying to use the same header and it causes build break for our code .\r\n\r\n> \r\n\r\n> [   71s] /usr/include/tensorflow/tensorflow/contrib/lite/interpreter.h: In member function 'const std::pair<TfLiteNode, _TfLiteRegistration>* tflite::Interpreter::node_and_registration(int) const':\r\n[   71s] /usr/include/tensorflow/tensorflow/contrib/lite/interpreter.h:212:20: **error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]**\r\n[   71s]      if (node_index >= nodes_and_registration_.size() || node_index < 0)\r\n[   71s]\r\n\r\ncast in the interior function is an option.\r\nbut there are other places in the same file where the datatype is changed instead of a cast . Please check the below patch.\r\n\r\n> FYI:   **Convert int -> size_t so that implicit conversion doesn't lose integer precision**\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/f0d5d2047833c7221ce3be1690689ca1c6658add#diff-188a1b7e9814e7c3f77a5b4ec998b631\r\n", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "For backward compatibility the signature of the functions cannot change. So adding casts is really the only option.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to lack of recent activity."]}, {"number": 19736, "title": "[tflite] label_image for tflite in Python", "body": "With model (mobilenet_v1_1.0_224_quant.tflite), input image\r\n(grace_hooper.bmp), and labels file (labels.txt) in /tmp.\r\nRun\r\n\r\n```\r\nbazel run --config opt //tensorflow/contrib/lite/examples/python:label_image\r\n```\r\n\r\nWe can get results like\r\n\r\n```\r\n0.470588: military uniform\r\n0.337255: Windsor tie\r\n0.047059: bow tie\r\n0.031373: mortarboard\r\n0.019608: suit\r\n```", "comments": ["Nagging Reviewer : It has been 20 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer : It has been 38 days with no activity and the `awaiting review` label was assigned. Can you please take a look?"]}, {"number": 19735, "title": "How to quantize MobileNetV2 for deeplabV3+ ?", "body": "**System information**\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:Source\r\n- **TensorFlow version (use command below)**:1.8.0rc0\r\n- **Python version**:2.7.12\r\n- **Bazel version (if compiling from source)**:0.12.0\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:cuda-9.0/7.0\r\n- **GPU model and memory**:GeForce GTX 1080/8105MiB\r\n- **Phone**:xiaomi5 (Snapdragon 820)\r\n- **Exact command to reproduce**:\r\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --\r\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb\r\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite\r\n--inference_type=QUANTIZED_UINT8\r\n--input_shape=1,513,513,3\r\n--input_array=sub_7\r\n--output_array=logits/semantic/BiasAdd\r\n\r\n**Describe the problem**\r\nI have tried to quantize MobileNetV2 for deeplabV3+ with TFlite. But I fail to convert the model.\r\nFrom the following issue, I saw that the operations were not supported for the option of quantization.\r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md\r\nCheckpoint name: mobilenetv2_coco_voc_trainaug\r\n\r\nWho can explain and support to resolve the issue?\r\n\r\n**Source code / logs**\r\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco -- \r\n --input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb \r\n --output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite \r\n --inference_type=QUANTIZED_UINT8 \r\n --input_shape=1,513,513,3 \r\n --input_array=sub_7 \r\n --output_array=logits/semantic/BiasAdd\r\n\r\nUnimplemented: this graph contains an operator of type SpaceToBatchND for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "@tensorflowbutler @skye\r\n\r\n\"Exact command to reproduce\" was updated. Could you check out that?", "@suharshs\r\n\r\n\"Exact command to reproduce\" was updated. Could you check out that?", "Hi @kanul ,\r\n\r\nIt seems that the error is that TOCO doesn't yet support quantized SpaceToBatchND. When i look at the kernel implementations, it seems that the kernel does support quantization but the transformation missing that. I will work on a fix soon.\r\n\r\nThanks!", "@suharshs\r\n\r\nAnd, can you support not only SpaceToBatchND but also ResizeBilinear?", "ResizeBilinear needs a quantized kernel implementation, I believe this is in the works and should be ready within a couple of weeks.", "Hi @suharshs,\r\nCan you also point out the kernel implementations you are looking for the SpaceToBatchND ?\r\nWould be good understanding pointer. \r\nThank you ", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @suharshs,\r\n\r\nCan you share for me your status for implement of ResizeBilinear?", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi, @suharshs,\r\nAre there any updates about this problem ? \r\n\r\n", "@PrinceP here is a pointer to the SpaceToBatchND kernel implementation https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h#L3282 I made a recent change to add support for quantization for it.\r\n\r\nI believe ResizeBilinear is supported now but we still need support for dilated depthwise conv. Thus i will mark this a duplicate of https://github.com/tensorflow/tensorflow/issues/20684."]}, {"number": 19734, "title": "[bug] Regularizers do not allow integer scales.", "body": "Location:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/c9de294d0a0980b1636f76757c175afbf4f58ea8/tensorflow/contrib/layers/python/layers/regularizers.py#L92\r\n\r\nExpected behavior: I was expecting layers to work with integer scale regularizers.\r\n\r\nActual behavior: program throws error and quits on me when I set scale=10 but not when I set scale=10.0\r\n\r\nThis seems to be vestigial of a misunderstanding: earlier commits limit regularizer scale to a floating point number between 0.0 and 1.0, disallowing integers. While the upperbound of 1.0 has been removed, the no-integers limitation is left over.", "comments": ["wow very good point. ", "Hi @renxida! Moving this issue to closed status as it has been resolved in the [2.8 version](https://colab.sandbox.google.com/gist/mohantym/8f48d03be51e23592f5f4bae9fea070d/github_19734.ipynb) through [float casting](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/regularizers.py#:~:text=_check_penalty_number(l2)-,self.l2%20%3D%20backend.cast_to_floatx(l2),-def%20__call__(self)). Thanks!"]}, {"number": 19733, "title": "TensorArray performance on GPU is poor.", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 'v1.8.0-7-g3b85959' 1.8.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.13.1\r\n- **GCC/Compiler version (if compiling from source)**: 4.9.3\r\n- **CUDA/cuDNN version**: 9.0/7\r\n- **GPU model and memory**: TITAN X (Pascal) 12GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nHi, I've found `TensorArrayScatterV3` op to be the bottleneck of my model, and it seems like TensorArray.scatter is really slow on GPU.\r\nI compared 'tf.TensorArray.scatter' with `tf.scatter_update` on CPU/GPU, with the following code:\r\n```\r\nimport time\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/gpu:0'):\r\n    t = tf.Variable(tf.random_normal([100000, 100]))\r\n    ta = tf.TensorArray(tf.float32, size=100000, element_shape=[100])\r\n    perm = tf.random_shuffle(tf.range(100000, dtype=tf.int32))\r\n    for i in range(1000):\r\n        idx = perm[i*100: (i+1)*100]\r\n        v = tf.random_normal([100, 100])\r\n        t = tf.scatter_update(t, idx, v)\r\n        ta = ta.scatter(idx, v)\r\n    o1 = tf.gather(t, idx[:10])\r\n    o2 = ta.gather(idx[:10])\r\n\r\nsess_config = tf.ConfigProto()\r\nsess_config.allow_soft_placement = True\r\nsess = tf.Session(config=sess_config)\r\nsess.run([tf.global_variables_initializer()])\r\n\r\ntotal_time_tensor_scatter = 0.\r\ntotal_time_tensorarray_scatter = 0.\r\nfor i in range(100):\r\n    start_time = time.time()\r\n    sess.run([o1])\r\n    total_time_tensor_scatter += time.time() - start_time\r\n    start_time = time.time()\r\n    sess.run([o2])\r\n    total_time_tensorarray_scatter += time.time() - start_time\r\nprint('total_time_tensor_scatter', total_time_tensor_scatter)\r\nprint('total_time_tensorarray_scatter', total_time_tensorarray_scatter)\r\n```\r\nThe results are like:\r\nOn CPU:\r\n```\r\ntotal_time_tensor_scatter 8.333731889724731\r\ntotal_time_tensorarray_scatter 19.28065252304077\r\n```\r\nOn GPU:\r\n```\r\ntotal_time_tensor_scatter 8.216091632843018\r\ntotal_time_tensorarray_scatter 82.21394562721252\r\n```\r\nIt seems like TensorArray scatter is slower than tensor scatter on CPU, and it is much slower on GPU.\r\n\r\nBy the way, the timeline on GPU is like:\r\n![image](https://user-images.githubusercontent.com/7380587/40897557-4f5f2e2e-67ee-11e8-82ba-298c6c74241d.png)\r\n\r\nIs it intrinsic for TensorArray or not?\r\nCan it be optimized further?", "comments": ["@alextp, could you ptal? Thanks!", "@ebrevdo , any idea what's going on here? I don't think this is intended behavior.\r\n\r\n", "@alextp hi!\r\n\r\nI checked tensor array ops' code, and found some very expensive operations, like:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/81161f9d9987a8eb70793d95048c20be34292859/tensorflow/core/kernels/tensor_array_ops.cc#L1329-L1331\r\n\r\nI modify the code and let the split act as trivial, then the total running time reduces to about 30s with the same device.\r\n\r\nMaybe a fused_split can help with performance.", "The split operation there is needed to ensure that the split tensors don't\nend up out of eigen alignment, I believe.\n\nOn Tue, Jul 17, 2018 at 10:17 PM Yingkai Luan <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> hi!\n>\n> I checked tensor array ops' code, and found some very expensive operation,\n> like:\n>\n>\n> https://github.com/tensorflow/tensorflow/blob/81161f9d9987a8eb70793d95048c20be34292859/tensorflow/core/kernels/tensor_array_ops.cc#L1329-L1331\n>\n> I modify the code and let the split act as trivial, then the total running\n> time reduce to about 30s with the same device.\n>\n> I also notice that tensorarray's scatter has some mutex operation, so it\n> should be a little bit slower. Maybe a fused_split can help with\n> performance.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19733#issuecomment-405811189>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxcotdWY-1S23d_KZV9sE8XThoeDaks5uHsTLgaJpZM4UYafN>\n> .\n>\n\n\n-- \n - Alex\n", "Alex is right. You can't do a simple split.\n\nThe TensorArray split, concat, stack, and unstack ops aren't optimized.\nSadly.  This requires someone with time and GPU expertise.  Look at the\ncode for TF.concat I think for an example.\n\nOn Wed, Jul 18, 2018, 8:42 AM Alexandre Passos <notifications@github.com>\nwrote:\n\n> The split operation there is needed to ensure that the split tensors don't\n> end up out of eigen alignment, I believe.\n>\n> On Tue, Jul 17, 2018 at 10:17 PM Yingkai Luan <notifications@github.com>\n> wrote:\n>\n> > @alextp <https://github.com/alextp> hi!\n> >\n> > I checked tensor array ops' code, and found some very expensive\n> operation,\n> > like:\n> >\n> >\n> >\n> https://github.com/tensorflow/tensorflow/blob/81161f9d9987a8eb70793d95048c20be34292859/tensorflow/core/kernels/tensor_array_ops.cc#L1329-L1331\n> >\n> > I modify the code and let the split act as trivial, then the total\n> running\n> > time reduce to about 30s with the same device.\n> >\n> > I also notice that tensorarray's scatter has some mutex operation, so it\n> > should be a little bit slower. Maybe a fused_split can help with\n> > performance.\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/19733#issuecomment-405811189\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AAATxcotdWY-1S23d_KZV9sE8XThoeDaks5uHsTLgaJpZM4UYafN\n> >\n> > .\n> >\n>\n>\n> --\n> - Alex\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19733#issuecomment-405976932>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxOEfCz00AaGy5fPXqgd1EwmnFhGks5uH1dygaJpZM4UYafN>\n> .\n>\n"]}, {"number": 19732, "title": "mobilenetv2", "body": " i just need a  Mobilenet V2 model for android test that is quantized, but there is only  un-quantized version here (https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)  ,who can offer me a quantized version?\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}]