[{"number": 40531, "title": "[tflite] make label_image build on linux and macOS", "body": "label_image doesn't build on Linux and macOS platforms\r\n\r\n```\r\nbazel build --config opt //tensorflow/lite/examples/label_image:label_image\r\n```\r\nshows something like\r\n```\r\nERROR: /home/freedom/work/tensorflow/tensorflow/lite/examples/label_image/BUILD:15:1: undeclared inclusion(s) in rule '//tensorflow/lite/examples/label_image:label_image':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/lite/examples/label_image/label_image.cc':\r\n  'external/com_google_absl/absl/strings/string_view.h'\r\n  'external/com_google_absl/absl/base/internal/throw_delegate.h'\r\n\r\n```\r\n\r\nAdd `\"@com_google_absl//absl/strings\"` to deps", "comments": []}, {"number": 40530, "title": "Attribute Error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: v11\r\n- GPU model and memory: \r\n\r\nI am trying to run the following code\r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Avaiable: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\n\r\nI get the error\r\n\r\n> AttributeError: module 'tensorflow' has no attribute 'config'\r\n\r\n\r\nI tried the following \r\n\r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\nprint(\"Num GPUs Avaiable: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n\r\n```\r\nI got the error\r\n\r\n> ModuleNotFoundError: No module named 'tensorflow.compat'\r\n\r\n\r\nEdit1: After uninstalling and reinstalling tensorflow, now i'm getting the error:\r\n\r\n> The Kernal appears to have died. It will restart automatically\r\n\r\n\r\nI cannot understand what I'm doing wrong, I'm using jupyter notebook\r\n\r\nfor reference, if there is an issue with the versions\r\n`pip list`  \r\n\r\n> Package                            Version\r\n> ---------------------------------- -------------------\r\n> absl-py                            0.9.0\r\n> alabaster                          0.7.12\r\n> anaconda-client                    1.7.2\r\n> anaconda-navigator                 1.9.12\r\n> anaconda-project                   0.8.3\r\n> argh                               0.26.2\r\n> asn1crypto                         1.3.0\r\n> astroid                            2.4.1\r\n> astropy                            4.0.1.post1\r\n> astunparse                         1.6.3\r\n> atomicwrites                       1.4.0\r\n> attrs                              19.3.0\r\n> autopep8                           1.4.4\r\n> Babel                              2.8.0\r\n> backcall                           0.1.0\r\n> backports.functools-lru-cache      1.6.1\r\n> backports.shutil-get-terminal-size 1.0.0\r\n> backports.tempfile                 1.0\r\n> backports.weakref                  1.0.post1\r\n> bcrypt                             3.1.7\r\n> beautifulsoup4                     4.9.0\r\n> bitarray                           1.2.2\r\n> bkcharts                           0.2\r\n> bleach                             1.5.0\r\n> bokeh                              2.0.2\r\n> boto                               2.49.0\r\n> Bottleneck                         1.3.2\r\n> cachetools                         4.1.0\r\n> certifi                            2020.4.5.1\r\n> cffi                               1.14.0\r\n> chardet                            3.0.4\r\n> click                              7.1.2\r\n> cloudpickle                        1.4.1\r\n> clyent                             1.2.2\r\n> colorama                           0.4.3\r\n> comtypes                           1.1.7\r\n> conda                              4.8.3\r\n> conda-build                        3.10.5\r\n> conda-package-handling             1.7.0\r\n> conda-verify                       3.4.2\r\n> contextlib2                        0.6.0.post1\r\n> contextvars                        2.4\r\n> cryptography                       2.9.2\r\n> cycler                             0.10.0\r\n> Cython                             0.29.17\r\n> cytoolz                            0.10.1\r\n> dask                               2.17.2\r\n> decorator                          4.4.2\r\n> defusedxml                         0.6.0\r\n> diff-match-patch                   20181111\r\n> distributed                        2.17.0\r\n> docutils                           0.16\r\n> entrypoints                        0.3\r\n> et-xmlfile                         1.0.1\r\n> fastcache                          1.1.0\r\n> filelock                           3.0.12\r\n> flake8                             3.8.2\r\n> Flask                              1.1.2\r\n> fsspec                             0.7.4\r\n> future                             0.18.2\r\n> gast                               0.3.3\r\n> gevent                             1.4.0\r\n> glob2                              0.7\r\n> google-auth                        1.16.1\r\n> google-auth-oauthlib               0.4.1\r\n> google-pasta                       0.2.0\r\n> greenlet                           0.4.15\r\n> grpcio                             1.29.0\r\n> h5py                               2.10.0\r\n> HeapDict                           1.0.1\r\n> html5lib                           0.9999999\r\n> hypothesis                         5.11.0\r\n> idna                               2.9\r\n> imageio                            2.8.0\r\n> imagesize                          1.2.0\r\n> immutables                         0.11\r\n> importlib-metadata                 1.6.1\r\n> intervaltree                       3.0.2\r\n> ipykernel                          5.1.4\r\n> ipython                            7.13.0\r\n> ipython-genutils                   0.2.0\r\n> ipywidgets                         7.5.1\r\n> isort                              4.3.21\r\n> itsdangerous                       1.1.0\r\n> jdcal                              1.4.1\r\n> jedi                               0.15.2\r\n> Jinja2                             2.11.2\r\n> joblib                             0.15.1\r\n> json5                              0.9.5\r\n> jsonschema                         3.2.0\r\n> jupyter                            1.0.0\r\n> jupyter-client                     6.1.3\r\n> jupyter-console                    6.1.0\r\n> jupyter-core                       4.6.3\r\n> jupyterlab                         1.2.6\r\n> jupyterlab-server                  1.1.4\r\n> Keras-Preprocessing                1.1.2\r\n> keyring                            21.1.1\r\n> kiwisolver                         1.2.0\r\n> lazy-object-proxy                  1.4.3\r\n> libarchive-c                       2.9\r\n> llvmlite                           0.32.1\r\n> locket                             0.2.0\r\n> lxml                               4.5.0\r\n> Markdown                           3.2.2\r\n> MarkupSafe                         1.1.1\r\n> matplotlib                         3.1.3\r\n> mccabe                             0.6.1\r\n> menuinst                           1.4.16\r\n> mistune                            0.8.4\r\n> mkl-fft                            1.0.15\r\n> mkl-random                         1.1.1\r\n> mkl-service                        2.3.0\r\n> mock                               4.0.2\r\n> more-itertools                     8.3.0\r\n> mpmath                             1.1.0\r\n> msgpack                            1.0.0\r\n> multipledispatch                   0.6.0\r\n> navigator-updater                  0.2.1\r\n> nbconvert                          5.6.1\r\n> nbformat                           5.0.6\r\n> networkx                           2.4\r\n> nltk                               3.4.5\r\n> nose                               1.3.7\r\n> notebook                           6.0.3\r\n> numba                              0.49.1\r\n> numexpr                            2.7.1\r\n> numpy                              1.18.5\r\n> numpydoc                           0.9.2\r\n> oauthlib                           3.1.0\r\n> olefile                            0.46\r\n> openpyxl                           3.0.3\r\n> opt-einsum                         3.2.1\r\n> packaging                          20.3\r\n> pandas                             1.0.3\r\n> pandocfilters                      1.4.2\r\n> paramiko                           2.7.1\r\n> parso                              0.5.2\r\n> partd                              1.1.0\r\n> path                               13.1.0\r\n> pathlib2                           2.3.5\r\n> pathtools                          0.1.2\r\n> patsy                              0.5.1\r\n> pep8                               1.7.1\r\n> pexpect                            4.8.0\r\n> pickleshare                        0.7.5\r\n> Pillow                             7.1.2\r\n> pip                                20.1.1\r\n> pkginfo                            1.5.0.1\r\n> pluggy                             0.13.1\r\n> ply                                3.11\r\n> prometheus-client                  0.7.1\r\n> prompt-toolkit                     3.0.5\r\n> protobuf                           3.12.2\r\n> psutil                             5.7.0\r\n> py                                 1.8.1\r\n> pyasn1                             0.4.8\r\n> pyasn1-modules                     0.2.8\r\n> pycodestyle                        2.6.0\r\n> pycosat                            0.6.3\r\n> pycparser                          2.20\r\n> pycrypto                           2.6.1\r\n> pycurl                             7.43.0.5\r\n> pydocstyle                         4.0.1\r\n> pyflakes                           2.2.0\r\n> Pygments                           2.6.1\r\n> pylint                             2.5.2\r\n> PyNaCl                             1.3.0\r\n> pyodbc                             4.0.0-unsupported\r\n> pyOpenSSL                          19.1.0\r\n> pyparsing                          2.4.7\r\n> pyreadline                         2.1\r\n> pyrsistent                         0.16.0\r\n> PySocks                            1.7.1\r\n> pytest                             5.4.2\r\n> pytest-arraydiff                   0.3\r\n> pytest-astropy                     0.8.0\r\n> pytest-astropy-header              0.1.2\r\n> pytest-doctestplus                 0.7.0\r\n> pytest-openfiles                   0.5.0\r\n> pytest-remotedata                  0.3.2\r\n> python-dateutil                    2.8.1\r\n> python-jsonrpc-server              0.3.4\r\n> python-language-server             0.31.9\r\n> pytz                               2020.1\r\n> PyWavelets                         1.1.1\r\n> pywin32                            227\r\n> pywin32-ctypes                     0.2.0\r\n> pywinpty                           0.5.7\r\n> PyYAML                             5.3.1\r\n> pyzmq                              18.1.1\r\n> QDarkStyle                         2.8.1\r\n> QtAwesome                          0.7.0\r\n> qtconsole                          4.7.4\r\n> QtPy                               1.9.0\r\n> requests                           2.23.0\r\n> requests-oauthlib                  1.3.0\r\n> rope                               0.17.0\r\n> rsa                                4.0\r\n> Rtree                              0.9.4\r\n> ruamel-yaml                        0.15.87\r\n> scikit-image                       0.16.2\r\n> scikit-learn                       0.22.1\r\n> scipy                              1.4.1\r\n> seaborn                            0.10.1\r\n> Send2Trash                         1.5.0\r\n> setuptools                         47.1.1.post20200604\r\n> simplegeneric                      0.8.1\r\n> singledispatch                     3.4.0.3\r\n> six                                1.15.0\r\n> snowballstemmer                    2.0.0\r\n> sortedcollections                  1.1.2\r\n> sortedcontainers                   2.1.0\r\n> soupsieve                          2.0.1\r\n> Sphinx                             3.0.4\r\n> sphinxcontrib-applehelp            1.0.2\r\n> sphinxcontrib-devhelp              1.0.2\r\n> sphinxcontrib-htmlhelp             1.0.3\r\n> sphinxcontrib-jsmath               1.0.1\r\n> sphinxcontrib-qthelp               1.0.3\r\n> sphinxcontrib-serializinghtml      1.1.4\r\n> sphinxcontrib-websupport           1.2.1\r\n> spyder                             4.1.3\r\n> spyder-kernels                     1.9.1\r\n> SQLAlchemy                         1.3.17\r\n> statsmodels                        0.11.1\r\n> sympy                              1.5.1\r\n> tables                             3.5.1\r\n> tblib                              1.6.0\r\n> tensorboard                        2.2.2\r\n> tensorboard-plugin-wit             1.6.0.post3\r\n> tensorflow                         2.2.0\r\n> tensorflow-estimator               2.2.0\r\n> termcolor                          1.1.0\r\n> terminado                          0.8.3\r\n> testpath                           0.4.4\r\n> toml                               0.10.0\r\n> toolz                              0.10.0\r\n> tornado                            6.0.4\r\n> tqdm                               4.46.0\r\n> traitlets                          4.3.3\r\n> typed-ast                          1.4.1\r\n> typing-extensions                  3.7.4.1\r\n> ujson                              1.35\r\n> unicodecsv                         0.14.1\r\n> urllib3                            1.25.8\r\n> watchdog                           0.10.2\r\n> wcwidth                            0.1.9\r\n> webencodings                       0.5.1\r\n> Werkzeug                           1.0.1\r\n> wheel                              0.34.2\r\n> widgetsnbextension                 3.5.1\r\n> win-inet-pton                      1.1.0\r\n> win-unicode-console                0.5\r\n> wincertstore                       0.2\r\n> wrapt                              1.12.1\r\n> xlrd                               1.2.0\r\n> XlsxWriter                         1.2.9\r\n> xlwings                            0.19.4\r\n> xlwt                               1.3.0\r\n> xmltodict                          0.12.0\r\n> yapf                               0.28.0\r\n> zict                               2.0.0\r\n> zipp                               3.1.0\r\n> ", "comments": ["@thepontiacbandit,\r\nI was able to run the code without any issues. Could you please create a virtual environment and check if you are facing the same error.\r\n\r\nAlso, just to confirm are you running the code within Anaconda environment? Thanks!", "Yes, i was running in an anaconda virtual environment. \r\nThe isssue was with CUDA  v10.2, \r\nrunning the same code after installation of v10.1, worked, \r\nClosing the issue, Thanks. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40530\">No</a>\n"]}, {"number": 40529, "title": "buenas", "body": "Improving README.md syntaxis and grammar", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40529) for more info**.\n\n<!-- need_sender_cla -->", "@fhso3i Can you please sign CLA. Thanks!"]}, {"number": 40528, "title": "C get temp file name", "body": "@mihaimaruseac \r\nThis PR add a function to getTempFileName for C API.\r\nI have to use `std::string` here since there is not effective way to store the result. Since we don't know we should use `new` or `malloc` to allocate a `const char*` the result", "comments": ["@mihaimaruseac \r\nI add a commit to use `char**`. We don't need `TF_Status` anymore since `LOG(FATAL)` will abort"]}, {"number": 40527, "title": "Can't load tf 2.1 model trained on ai platform", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Ubuntu 18.04 and MacOSX 10.15.5\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.1.0 and 2.2.0\r\n- Python version: 3.7 and 3.6\r\n\r\n**Describe the current behavior**\r\nCan't load a model locally with tensorflow version 2.1. The model was trained on ai platform with [runtime version 2.1](https://cloud.google.com/ai-platform/training/docs/runtime-version-list#2.1) and saved using `tf.keras.callbacks.ModelCheckpoint(..., save_weights_only=False)`. It is possible to load the model locally with tensorflow version 2.2.\r\nIt is also possible to build the model locally and use `load_weights`.\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n2.2.0\r\n>>> tf.keras.models.load_model('/Users/fjp/projects/keras-job-dir/model_checkpoint/model.32-0.00/')\r\n2020-06-17 00:59:39.651511: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-17 00:59:39.699145: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7faa9f631160 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-17 00:59:39.699171: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n<tensorflow.python.keras.engine.training.Model object at 0x12e9ceb10>\r\n```\r\n\r\nUsing tf version 2.1 results in `KeyError: 0`:\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n2.1.0\r\n>>> tf.keras.models.load_model('/Users/fjp/projects/keras-job-dir/model_checkpoint/model.32-0.00/')\r\n2020-06-17 01:30:15.614749: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-17 01:30:15.628231: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc502c878b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-17 01:30:15.628254: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/fjp/projects/venvtf/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 150, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"/Users/fjp/projects/venvtf/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 89, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"/Users/fjp/projects/venvtf/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 552, in load_internal\r\n    export_dir)\r\n  File \"/Users/fjp/projects/venvtf/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 118, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"/Users/fjp/projects/venvtf/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 121, in __init__\r\n    self._load_all()\r\n  File \"/Users/fjp/projects/venvtf/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 239, in _load_all\r\n    node, setter = self._recreate(proto)\r\n  File \"/Users/fjp/projects/venvtf/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 322, in _recreate\r\n    return factory[kind]()\r\n  File \"/Users/fjp/projects/venvtf/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 309, in <lambda>\r\n    \"user_object\": lambda: self._recreate_user_object(proto.user_object),\r\n  File \"/Users/fjp/projects/venvtf/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 328, in _recreate_user_object\r\n    return self._recreate_base_user_object(proto)\r\n  File \"/Users/fjp/projects/venvtf/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 216, in _recreate_base_user_object\r\n    return revived_cls._init_from_metadata(metadata)  # pylint: disable=protected-access\r\n  File \"/Users/fjp/projects/venvtf/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 297, in _init_from_metadata\r\n    revived_obj = cls(**init_args)\r\n  File \"/Users/fjp/projects/venvtf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_layer.py\", line 86, in __init__\r\n    batch_size = batch_input_shape[0]\r\nKeyError: 0\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nLoading a model trained on ai platform with runtime version 2.1 should be possible to load locally with tensorflow version 2.1 and its corresponding tf.keras version. tf version 2.2 shouldn't be required to load the model.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nHere is the [link](https://drive.google.com/drive/folders/1CZwwfUV0uI3bGqi2b5NAUw5RlUoA_ZLV?usp=sharing) to the model trained on ai platform (runtime version 2.1). For me it is possible to load it locally with tensorflow 2.2 but not with 2.1. \r\n\r\n\r\n\r\n", "comments": ["@fjp \r\nPlease provide with complete code for us to replicate the issue faced or if possible share a colab gist for us to analyse.", "@Saduf2019 thanks looking into this. I've created a [Colab notebook](https://colab.research.google.com/drive/1nG5ziQ-bDF35CCrguzFFkRKvXr257z2k?usp=sharing) where you can see that loading the model works with tf 2.2 but not with 2.1 (`KeyError: 0`). \r\n\r\nPlease remember that the model was trained on and downloaded from google cloud ai platform which currently supports only [tensorflow 2.1 runtime version](https://cloud.google.com/ai-platform/training/docs/runtime-version-list#2.1). ", "I have a guess why loading the model is only possible with tf 2.2 and not 2.1: From the job log on AI Platform it seems that another package (provided with `--packages` option), which depends on `tensorflow==2.2.0` (not `tensorflow-gpu`), triggered the installation. According to the logs the package was first installed with `pip3 --no-deps` but then again without `--no-deps` which installed tf 2.2. \r\n\r\nI wasn't aware that [newer versions of tensorflow, other than the runtime versions](https://cloud.google.com/ai-platform/training/docs/versioning#custom-tf-versions), could be installed and that this can be done with a package dependency. \r\n\r\nHowever, also the GPU was used while training. Shouldn't this only be possible when `tensorflow-gpu` is installed? And is it possible to inspect a SavedModel to see which version of tensorflow was used while saving it?", "Thanks for sharing your investigation. Version dependency is causing the issue.\r\n`tensorflow==2.2.0` installs both cpu and gpu packages.\r\nStarting TF2 cpu and gpu packages are combined. \r\n`tensorflow-gpu==2.2.0` will install only gpu package.\r\nAlso there is no way to identify tensorflow version from the SavedModel.\r\n", "Thanks for your help and the clarification.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40527\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40527\">No</a>\n"]}, {"number": 40526, "title": "Added FuzzedDataProvider to split fuzzer data", "body": "Switched manual data splicing to FuzzedDataProvider @mihaimaruseac", "comments": []}, {"number": 40525, "title": "SavedModel generates different probabilities than Estimator model It is exported from for RNNClassifier", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina 10.15.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): b'v1.13.2-5-g04256c89d8' 1.13.2\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nFor a tf.contrib.estimator.RNNClassifier estimator model, the estimator produces different probabilities when making predictions than the same model when exported to a SavedModel and making predictions (both using Serving and locally). When using the same code, except the estimator is changed to a DNNClassifier (and tf.contrib.feature_column.sequence_categorical_column_with_identity is changed to tf.feature_column.categorical_column_with_identity for feature_column), the SavedModel produces the same probabilities as expected. \r\n\r\n**Describe the expected behavior**\r\nThe SavedModel should generate the same probability as the estimator model it is generated from for any sample. \r\n\r\n**Standalone code to reproduce the issue**\r\nJupyter notebooks + small example datasets are provided in this repo: https://github.com/linbrian/tf_1.13_rnn_classifier_bug/tree/master\r\n\r\n\r\n", "comments": ["I have tried in colab with TF version 1.13.2, 1.15 .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e1c42b10092adb5b5d4cec1ba49c3c4c/untitled37.ipynb).Is this the expected behavior?.Thanks!", "The gist shows the behavior I saw, in which the probabilities are different, but the expected behavior is that the probabilities are the same. ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40525\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40525\">No</a>\n"]}, {"number": 40524, "title": "Building binaries for Arduino fails, fatal error: PDM.h", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Ubuntu 20.04 x86_64\r\n- TensorFlow installed from source:\r\n- Tensorflow version : 389405a77946410400ed410246e4cc7257802dde\r\n- Target platform: Arduino Nano 33\r\n\r\nDocumentation [here](https://www.tensorflow.org/lite/microcontrollers/library) under Build binaries heading suggest that a binary build can be initiated with a call as follows:\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=<target> <project_name>_bin\r\n```\r\nTherefore a simple call as follows should work or need specific architecture to run to success.\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=arduino micro_speech_bin\r\n```\r\n\r\nHowever, as the required `PDM.h` Arduino library is not in the source tree, when compiling micro_speech for Arduino, this fails with the error.\r\n\r\n```\r\ntensorflow/lite/micro/examples/micro_speech/arduino/audio_provider.cc:39:10: fatal error: PDM.h: No such file or directory\r\n   39 | #include \"PDM.h\"\r\n      |          ^~~~~~~\r\n```\r\nSupported targets and target architectures are not documented anywhere I have found.\r\n\r\nFurthermore, the `Makefile.inc` in the arduino folder `tensorflow/lite/micro/examples/micro_speech/arduino` contains a reference to 'sparkfun_edge' instead of 'arduino' the following:\r\n\r\n```\r\nifeq ($(TARGET),$(filter $(TARGET),arduino))\r\n\r\nMICRO_SPEECH_SRCS += \\\r\n\ttensorflow/lite/micro/examples/micro_speech/sparkfun_edge/audio_provider.cc \\\r\n\ttensorflow/lite/micro/examples/micro_speech/sparkfun_edge/command_responder.cc\r\n\r\nendif\r\n\r\n```", "comments": ["@victorromeo Could you please let us know if the issue still persists ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40524\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40524\">No</a>\n"]}, {"number": 40523, "title": "High memory use / leak in model.fit", "body": "I have an issue with a high memory uses, from this simple code sample. (link [Test.py](https://gist.github.com/davsklaus/6764bb3cf90841ce57d3658f83e14fa7)\r\n\r\n````\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nimport time, os, gc\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom tensorflow.keras.layers import Dense, Input\r\nfrom tensorflow.keras.optimizers import SGD, Adam\r\nimport numpy as np\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\nprint(tf.version.VERSION)\r\n\r\nX = np.random.random((1119440, 100, 12)).astype(dtype=np.float32)\r\nY = np.random.random((1119440,)).astype(dtype=np.float32)\r\nX_val = np.zeros( ( 300000, 100, 12), dtype=np.float32)\r\nY_val = np.zeros( (X_val.shape[0]), dtype=np.float32)\r\n\r\ninputs = Input((100,12))\r\noutputs = layers.Dense(1)(inputs)\r\nmodel = Model(inputs=inputs, outputs=outputs, name='my_model')\r\n\r\nmodel.compile(optimizer='SGD', loss = 'MSLE', metrics=['MSE'])\r\nprint(model.summary())\r\n\r\nmodel.fit(X,Y, batch_size=32, verbose=1, shuffle=True, epochs=20, validation_data=(X_val,Y_val))\r\n\r\n````\r\n**For each epoch the the memory consumption inc with 1GB**, And after 20 epoch my system run out of memory.\r\nI don't know if it a true memory leak, because gc.collect are able to collect a large portion of it.\r\n\r\nI have localized the issue to be related to \"validation_data=(X_val,Y_val)\" \r\nThe memory uses are stable if i replace the line with \"model.fit(X,Y, batch_size=32, verbose=1, shuffle=True, epochs=20)\"\r\n\r\nI can see there are reported other issues with use of generators, but in this case X_val and Y_val are simple numpy arrary.\r\n\r\nTested with this docker file  \r\n    FROM tensorflow/tensorflow:2.2.0-jupyter \r\n    COPY test.py /tmp/test.py \r\n    CMD python3 /tmp/test.py \r\n\r\n\r\nSame result if I uses tensorflow/tensorflow:lastest\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 20.04):\r\n- TensorFlow installed from (source or binary): docker tensorflow/tensorflow:2.2.0-jupyter\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3\r\n- Tested on a PC with 16GB of physical mem and 10GB of swap space \r\n", "comments": ["@davsklaus \r\nCould you please provide with complete code such that we could replicate the issue faced or please share a colab gist withe error for us to analyse.", "I have a similar problem although I'm not using any validation data:\r\n```python\r\nfrom tensorflow.keras import *\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.optimizers import *\r\nimport tensorflow as tf\r\n\r\nmodel = Sequential([\r\n    Embedding(input_dim = 11,\r\n              output_dim = 100,\r\n              mask_zero = True,\r\n              batch_input_shape = [None, None]),\r\n    LSTM(1024, stateful = False,\r\n         return_sequences = True),\r\n    TimeDistributed(Dense(11, activation = 'softmax'))])\r\nX, Y = [], []\r\nfor _ in range(10000):\r\n    v1 = tf.random.uniform((50,), minval = 1, maxval = 11,\r\n                           dtype = tf.int32)\r\n    v2 = tf.zeros((10,), dtype = tf.int32)\r\n    sample = tf.concat((v1, v2), axis = 0)\r\n    X.append(sample[:-1])\r\n    Y.append(sample[1:])\r\noptimizer = RMSprop(learning_rate = 0.01)\r\nmodel.compile(\r\n    optimizer = optimizer,\r\n    loss = 'sparse_categorical_crossentropy',\r\n    metrics = ['sparse_categorical_accuracy'])\r\nprint('fitting')\r\nmodel.fit(X, Y, epochs = 30)\r\n```\r\nTensorFlow stalls for a long while at the model.fit call. Then its memory usage slowly starts increasing until it has consumed over 4gb and then my oom killer kills it. The dataset isn't that large and should readily fit in a few mb's of memory.\r\n\r\nI've tested on both tf 2.2.0rc4 and 2.3.0-dev20200616 and it's the same result.", "@Saduf2019 \r\n\r\nI have update the issue and added a link to the source code used to provoke the isssue.\r\nI have also try the code on Colab - i had to reduce the size of the train data to get it to fit. But on colab the memory use was stable.  Afterwards i try the modified code in the docker image, but here i still run out of memory.\r\n\r\nBut afterward i fund that i can get it to run, if I change the garbage threshold - by setting \"gc.threshold(15,2,1)\"\r\n", "@davsklaus So, is the issue resolved right now as you were able to run  it? Thanks!", "Yes - i got it to run by tweaking the garbage collection setting. \r\nBut it seems a bit wrong - wonder if framework are doing some strange things. Where it generate some complicated memory reference that make it hard for the garbage collection to cleanup. ", "@davsklaus On what OS are you running into this error? Can you be more clear about the configuration of your PC? Thanks!", "My machine are Ubuntu 20.04 with kernel version 5.4.0\r\n\r\nBut the error was seen running in a docker image - using tensorflow/tensorflow:2.2.0-jupyter and  tensorflow/tensorflow:lastest\r\n", "@davsklaus,\r\nSorry for the delayed response. Your code could be run without any error in **`Tensorflow Version 2.5`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/c8cd506e3718d723c2cf3581af1100f2/gh_40523.ipynb) of the working code.\r\n\r\nIf possible, can you please check if the issue is resolved, when run in [the Docker Image](https://github.com/tensorflow/tensorflow/issues/40523#issuecomment-655624912) as well? Thanks!", "Hi thanks for your response.\r\n\r\nIt was newer an issue in colab - the memory use was stable there.\r\nThe issue was seen using the docker image tensorflow/tensorflow:2.2.0-jupyter - but also in other version of tensorflow 2.2 \r\n\r\nI have retested the script on newer versions (2.3.0, 2.4.0 & 2.5.0)  and there are the memory use stable - so it look like the issue has been fix since the issue was created. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40523\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40523\">No</a>\n"]}, {"number": 40522, "title": "[Intel MKL] Fixing build issue on Windows", "body": "", "comments": []}, {"number": 40521, "title": "TensorFlowLiteSwift nightly build running 40 times slower than stable release build (on iPhone)", "body": "**System information**\r\n   - Using TensorFlowLiteSwift iOS ObjectDetection example project\r\n   - OS Platform and Distribution: iOS 13.5.1\r\n   - Mobile device: iPhone 11 and other older iPhones\r\n   - TensorFlow installed from binary\r\n   - TensorFlow version: TensorFlowLiteSwift 0.0.1-nightly20200611\r\n\r\n**Describe the current behavior**\r\n   - With nightly build, we see 3330ms inference times\r\n\r\n**Describe the expected behavior**\r\n   - With stable release build, we see 80ms inference times\r\n\r\n**Steps to reproduce the issue**\r\n   1. Clone the TensorFlow Lite ObjectDetection iOS example repo\r\n   2. Run and see inferences averaging 80ms on iPhone 11 (up to 250ms on older iPhones)\r\n   3. Edit podfile, changing:\r\n         pod 'TensorFlowLiteSwift'\r\n      to\r\n         pod 'TensorFlowLiteSwift', '-> 0.0.1-nightly'\r\n   4. pod update\r\n   5. Run and see inferences averaging 3330ms on iPhone 11 (up to 9000ms on older iPhones)\r\n", "comments": ["@unarmedman Thanks for reporting. I can see that happening on the exact version you mentioned (`0.0.1-nightly.20200611`), but when I tried a more recent nightly build (`0.0.1-nightly.20200621`) the it seems to be fixed.\r\n\r\nCan you try `pod update --repo-update` and see if it resolves the issue?", "I can confirm that there was a performance regression issue exactly on 6/11, which got fixed the next day, so let me close this issue. If you are still seeing the issue after `pod update`, feel free to reopen.", "thx yyoon.  I guess I was just unlucky trying it that day.  works now :)"]}, {"number": 40520, "title": "change readme", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40520) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 40519, "title": "Tensorflow lite v1 converter not correctly quantizing model (tf 2.2.0)", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04.4 LTS\r\n- TensorFlow installed using pip, version 2.2.0\r\n\r\n**Python Code**\r\n\r\n```\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n        \"mobilenet_v2_quantized/tflite_graph.pb\", ['normalized_input_image_tensor'],\r\n        ['TFLite_Detection_PostProcess:0','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'],\r\n        input_shapes={'normalized_input_image_tensor': [1, 300, 300, 3]})\r\nconverter.inference_type = tf.compat.v1.lite.constants.QUANTIZED_UINT8\r\nconverter.quantized_input_stats = {'normalized_input_image_tensor' : (128, 128)}\r\nconverter.change_concat_input_ranges = False\r\nconverter.allow_custom_ops = True\r\nmodel = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(model)```\r\n```\r\n\r\n# Issue:\r\n\r\nThis code successfully produces a tflite flatbuffer, but the flatbuffer does not appear to be quantized correctly. When I attempt to compile the resulting model using Google's edgetpu_compiler tool, I encounter the following error:\r\n\r\n```\r\nEdge TPU Compiler version 2.1.302470888\r\nInvalid model: converted_model.tflite\r\nModel not quantized\r\n```\r\nThe input model is SSD_mobilenet_v2_quantized, available [here](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz).\r\n", "comments": ["@melhabr,\r\nOn running the given code I am facing an error stating \r\n```\r\nException: Op type not registered 'TFLite_Detection_PostProcess' in binary running on 933ed180d2c1. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n``` \r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/97503b8d532545c2d168efc8379d72f5/40519.ipynb). Thanks!", "It seems the zero output index is causing issues in some versions, this code should work correctly:\r\n```\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n        \"mobilenet_v2_quantized/tflite_graph.pb\", ['normalized_input_image_tensor'],\r\n        ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'],\r\n        input_shapes={'normalized_input_image_tensor': [1, 300, 300, 3]})\r\nconverter.inference_type = tf.compat.v1.lite.constants.QUANTIZED_UINT8\r\nconverter.quantized_input_stats = {'normalized_input_image_tensor' : (128, 128)}\r\nconverter.change_concat_input_ranges = False\r\nconverter.allow_custom_ops = True\r\nmodel = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(model)\r\n```\r\nI tried this code in a clean virtual environment ~~and it successfully produced a quantized model. There seems to be some issue with my installation, despite it being version 2.2.0. A full reinstall should fix it.~~\r\n\r\nEDIT: It turned out that my virtual environment had installed tensorflow 1.14 by default. The issue still persists as described with tensorflow 2.2.0. ", "@melhabr,\r\nThe issue seems to be fixed with the latest TF-nightly. \r\n\r\nI was able to compile the model with the edgetpu-compiler tool, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/19e793d8d1e9b9c412c3854610861eaf/40519.ipynb). Thanks!", "Thank you!"]}, {"number": 40518, "title": "Performance difference of TFlite in Android and iOS (model with quantization)", "body": "I've trained a model to detect custom objects to be used in mobile devices (Android and iOS), my code is based in the tensorflow's examples for [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/ios) and [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android). During my tests in pyshical devices (iPad 6th gen and Xiaomi RedMi Note 7) I've been noticing a difference of performance on Android app and iOS app.\r\n\r\nSome examples of performance (number of objects detected):\r\nIMG - iOS - Android\r\nimg1 - 57 - 74\r\nimg2 - 9 - 33\r\nimg3 - 43 - 78\r\nimg4 - 17 - 25\r\n\r\nThe real number of objects is a bit more than Android's result.\r\n\r\nI'm using the **ssd_mobilenet_v2_quantized_coco** from the [tensorflow model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) and **tensorflow-v1.14**.\r\n\r\nI'm using [this class](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java) in the Android version, and [this equivalent one](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/ios/ObjectDetection/ModelDataHandler/ModelDataHandler.swift) in the iOS version, both without any modification from the ones provided by Tensorflow.\r\n\r\n**My question is**: What should I investigate to know the reason of the performance difference and fix it? My model should give the same result for the customer in both mobile platforms.\r\n\r\nIf it's something unclear please let me know, any help would be great. Thanks!", "comments": ["@zampnrs \r\nPlease share simple stand alone code to replicate the performance issue faced.", "@Saduf2019 \r\n**In Android I did it:**\r\n`private Classifier detector; //tflite`\r\n`private Bitmap sbit; //bitmap scaled for 300x300`\r\n`List<Classifier.Recognition> results = detector.recognizeImage(sbit);`\r\n\r\nThe `Classifier` comes from this [interface](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/Classifier.java) and the `recognizeImage` is a method from [this class](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java), both files are provided by tensorflow and I used them without any modification.\r\n\r\n**In iOS I did it:**\r\n`@State private var result: Result?`\r\n`private var modelDataHandler: ModelDataHandler? =\r\n        ModelDataHandler(modelFileInfo: MobileNetSSD.modelInfo,\r\n                         labelsFileInfo: MobileNetSSD.labelsInfo)`\r\n`let buffer: CVPixelBuffer = self.getBuffer(from: self.image!)!`\r\n`self.result = self.modelDataHandler?.runModel(onFrame: buffer)`\r\n\r\nThe `result` is where I store my results, `modelDataHandler` is the variable with the tflite and txt files. The `buffer` I get from my `image` (`UIImage`) through the `getBuffer()` bellow:\r\n\r\n`func getBuffer(from image: UIImage) -> CVPixelBuffer? {\r\n        \r\n        let attrs = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue, kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue] as CFDictionary\r\n\r\n        var pixelBuffer : CVPixelBuffer?\r\n\r\n        let status = CVPixelBufferCreate(kCFAllocatorDefault, Int(image.size.width), Int(image.size.height), kCVPixelFormatType_32BGRA, attrs, &pixelBuffer)\r\n\r\n        guard (status == kCVReturnSuccess) else {\r\n          return nil\r\n        }\r\n\r\n        CVPixelBufferLockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0))\r\n        let pixelData = CVPixelBufferGetBaseAddress(pixelBuffer!)\r\n    \r\n        let rgbColorSpace = CGColorSpaceCreateDeviceRGB()\r\n        let context = CGContext(data: pixelData, width: Int(image.size.width), height:  Int(image.size.height), bitsPerComponent: 8, bytesPerRow:   CVPixelBufferGetBytesPerRow(pixelBuffer!), space: rgbColorSpace, bitmapInfo:    CGImageAlphaInfo.noneSkipFirst.rawValue)\r\n    \r\n        context?.translateBy(x: 0, y: image.size.height)\r\n        context?.scaleBy(x: 1.0, y: -1.0)\r\n    \r\n        UIGraphicsPushContext(context!)\r\n        image.draw(in: CGRect(x: 0, y: 0, width: image.size.width, height: image.size.height))\r\n        UIGraphicsPopContext()\r\n        CVPixelBufferUnlockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0))\r\n    \r\n        return pixelBuffer\r\n    }`", "Hi @zampnrs. When converting your trained model into tflite format, did you use quantization, or just the float model? If you were using a float model, #40442 might have been the cause. If you were using a quantized model, we may need to investigate further to find the root cause.", "Hi @yyoon. Yes, I'm using quantization. I'll check this other issue, it seems similar with I'm facing. Thanks! Any update I post here.", "Well, if you're using a quantized model, then it's probably a different issue from the one I linked above.\r\n\r\nAnother possibility is that this is caused by the difference in the way image is preprocessed before fed into the model. IIUC, the image is cropped and then resized in the Android app, whereas the iOS app does not properly crop the image.\r\n\r\nDo you think this could be the cause?", "You're right, it's a different issue.\r\n\r\nMaybe not. As far as I could see, the image is cropped in Android to make the real time detection faster, It's visible the velocity detection difference in both apps, in iOS it's slower. Since I'm using a single image, in my Android app I get the image as `Bitmap`, resize it to 300 x 300 and call the inference. In iOS I get the image as `UIImage`, convert it to `CVPixelBuffer` and call the inference, the `modelDataHandler.swift` resizes the pixel buffer after it. I tried to modify the iOS class to use the Android cropping and resizing approach but the result is equal.\r\n\r\nI'm not a big expert in iOS dev but I think it's something with the pre-processing logic, especially in the method `rgbDataFromBuffer()`. Comparing to Android app, there is coding differences beyond to syntax, some parameters are defined different in both apps when the model is quantized or not, like `numBytesPerChannel` in Android and `inputChannels` in iOS, in Android this parameter is 1 for quantized models and 4 for non-quantized models, in iOS this parameter is always 4. I've been testing this and other differences today but I still couldn't figure out anything.", "> I convert my UIImage to CVPixelBuffer and then call the inference\r\n> I tried to modify the iOS class to use the Android cropping and resizing approach but the result is equal.\r\n\r\nCan you share the actual code doing this?\r\n\r\n> in iOS this parameter is always 4\r\n\r\nI don't see any issues (yet) with the constants in the code.\r\n* The `inputChannels` is always 3 (not 4), and this represents the number of image channels expected by the model input, which is correct.\r\n* The `imageChannels` value is always 4, which is probably meant to represent the number of image channels in the image taken from the camera, but it doesn't seem to be used. (Well, we should fix that..)", "Yes, sure. The code to convert UIImage to CVPixelBuffer is in my reply above to Saduf2019. \r\n\r\nI've already undone my change to the `modelDataHandler.swift` since I couldn't get any difference in the results, but the changes were:\r\n- Take out these lines:\r\n` // Crops the image to the biggest square in the center and scales it down to model dimensions.\r\n\r\n    let scaledSize = CGSize(width: inputWidth, height: inputHeight)\r\n    guard let scaledPixelBuffer = pixelBuffer.resized(to: scaledSize) else {\r\n        print(\"Failed to scale pixel buffer\")\r\n      return nil\r\n    }`\r\n\r\n- And then change the `scaledPixelBuffer` to `pixelBuffer`, the original one.\r\n` guard let rgbData = rgbDataFromBuffer(\r\n        scaledPixelBuffer,\r\n        byteCount: batchSize * inputWidth * inputHeight * inputChannels * imageChannels,\r\n        isModelQuantized: inputTensor.dataType == .uInt8\r\n      ) else {\r\n        print(\"Failed to convert the image buffer to RGB data.\")\r\n        return nil\r\n      }\r\n`\r\n\r\n- When I get my bounding box coordinates, I plot them rescaling from 300 x 300 coordinate space to myImage coordinate space.\r\n\r\nSorry, the parameter I was talking about is the `imageChannels`, not the other one.\r\n\r\nWell, If the parameters are not wrong, I sincerally have no idea right now of what is the problem. I'll try to investigate it further", "Hi @zampnrs  in your code\r\n\r\n\"getBuffer: \r\nlet status = CVPixelBufferCreate(kCFAllocatorDefault, Int(image.size.width), Int(image.size.height), kCVPixelFormatType_32BGRA, attrs, &pixelBuffer)\"\r\n\r\nyou're assuming input image is kCVPixelFormatType_32BGRA, which is not always true, sometimes you may got a kCVPixelFormatType_32ARGB image, suggest to debug from here. \r\n\r\nThanks,", "and you need a small change in lite/examples/object_detection/ios/ObjectDetection/Extensions/CVPixelBufferExtension.swift to support selecting ARGB image from iPhone gallery, which i created a pull request here - \r\n\r\nhttps://github.com/tensorflow/examples/pull/227\r\n\r\nThanks,", "@Jamesweng thanks a lot! I didn't know that images from iOS gallery come in ARGB, I've changed to this format in the line you suggested and the number of detected objects has increased a lot in iOS. \r\n\r\nThere is still a small difference between the performance in both platform, differences usually of 1 to 3 detected objects. Do you think this difference could be due to the different way that each platform deal with images (e. g. Bitmaps, UIImage, CVPixelBuffer etc.) ?\r\n\r\nAnother question: if the image from the iOS gallery was not originally taken by an Apple device (e. g. downloaded from the web), will it still be ARGB? Or should I check the pixel format for every image?", "for #1 i would suggest you log input pixel values in both platforms to check if there're any differences from model input. image preprocessing operator(e.g., how image resize is implemented) may cause differences\r\n\r\nfor #2 you need to check pixel format for safe handling", "Humm... Ok, I'll try it. Thank a lot @Jamesweng and also @yyoon for your time to help me. Any updates I post here.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 40517, "title": "for loop in map_fn within tf.function is very slow compared to using while_loop counterpart", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10 2004 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1, 7,6\r\n- GPU model and memory: 1650Ti\r\n\r\n**Describe the current behavior**\r\n\r\nCode 1 runs much slow than Code 2 (71s seconds vs 11s), but from my understanding in Tensorflow documentation, Code 1 and Code 2 should be effectively the same (tf.function will turn the for loop into `while_loop` if the loop use Tensor but apparently not the case here). Moreover Code 2 will run much faster (~1s) on CPU (9750H, 1650Ti), not sure if that is expected tho. Code 3 using for loop and for loop to replace `map_fn` completed in 164s which is the slowest among the three which is expected.\r\n\r\n*Update:* This behavior is not observed in tf2.1.0, code 1/2/3 completed in similar amount of time (11ish seconds)\r\n\r\n**Describe the expected behavior**\r\n\r\nCode 1 and Code 2 runs as fast as each other.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nCode 1:\r\n```python3\r\nimport time\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef odeint_external(tensor):\r\n    finished_user_t_ii = 0\r\n    x = tensor[0]\r\n    t = tensor[1]\r\n    \r\n    # array to store the result\r\n    result = tf.TensorArray(dtype=tf.float32, size=t.shape[0])\r\n    result = result.write(0, x)\r\n\r\n    i = tf.constant(0, dtype=tf.int32)\r\n    for _t in t[1:]:\r\n        i = i+1\r\n        result = result.write(i, tf.stack([tf.cos(_t), tf.sin(_t)]))\r\n\r\n    return result.stack(), t\r\n\r\n\r\n@tf.function\r\ndef parallelized_map_fn(tensor):\r\n    return tf.map_fn(odeint_external, tensor)\r\n\r\n\r\n# warm up\r\nresult = parallelized_map_fn((tf.random.normal((50, 2), 0, 1), tf.random.normal((50, 10000), 0, 1)))\r\n\r\nstart_t = time.time()\r\nresult = parallelized_map_fn((tf.random.normal((50, 2), 0, 1), tf.random.normal((50, 10000), 0, 1)))\r\nprint(time.time()-start_t)\r\n```\r\n\r\nCode 2:\r\n```python3\r\nimport time\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef odeint_external(tensor):\r\n    finished_user_t_ii = 0\r\n    x = tensor[0]\r\n    t = tensor[1]\r\n    \r\n    # array to store the result\r\n    result = tf.TensorArray(dtype=tf.float32, size=t.shape[0])\r\n    result = result.write(0, x)\r\n\r\n    i = tf.constant(1, dtype=tf.int32)\r\n    cond = lambda i, _t, r: i < 9999\r\n    body = lambda i, _t, r: (i+1, _t, r.write(i, tf.stack([tf.cos(_t[i]), tf.sin(_t[i])])))\r\n    result = tf.while_loop(cond=cond, body=body, loop_vars=(i, t[1:], result))\r\n    \r\n    return result[2].stack(), t\r\n\r\n@tf.function\r\ndef parallelized_map_fn(tensor):\r\n    return tf.map_fn(odeint_external, tensor, parallel_iterations=1)\r\n\r\n# warm up\r\nresult = parallelized_map_fn((tf.random.normal((50, 2), 0, 1), tf.random.normal((50, 10000), 0, 1)))\r\n\r\nstart_t = time.time()\r\nresult = parallelized_map_fn((tf.random.normal((50, 2), 0, 1), tf.random.normal((50, 10000), 0, 1)))\r\nprint(time.time()-start_t)\r\n```\r\n\r\nCode 3:\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef odeint_external(tensor):\r\n    finished_user_t_ii = 0\r\n    x = tensor[0]\r\n    t = tensor[1]\r\n    \r\n    # array to store the result\r\n    result = tf.TensorArray(dtype=tf.float32, size=t.shape[0])\r\n    result = result.write(0, x)\r\n\r\n    i = tf.constant(0, dtype=tf.int32)\r\n    for _t in t[1:]:\r\n        i = i+1\r\n        result = result.write(i, tf.stack([tf.cos(_t), tf.sin(_t)]))\r\n    \r\n    return result.stack(), t\r\n\r\nx = tf.random.normal((50, 2), 0, 1)\r\nt = tf.random.normal((50, 10000), 0, 1)\r\n\r\n# warm up\r\nresult = [odeint_external((_x, _t)) for _x, _t in zip(x, t)]\r\n\r\nstart_t = time.time()\r\nresult = [odeint_external((_x, _t)) for _x, _t in zip(x, t)]\r\nprint(time.time()-start_t)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nN/A", "comments": ["I have tried in colab with TF version 2.2, nightly versions and was able to reproduce the issue .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/51b9c8e10a6bb308a3336943defb457d/untitled35.ipynb).Thanks!", "This looks like a regression - certain ops that the for loop uses don't seem to be optimized properly. We'll look into that.\r\n\r\nFor a temporary workaround, writing the loop like so should avoid the slow ops:\r\n\r\n```\r\n    i = tf.constant(1, dtype=tf.int32)\r\n    while i < 9999:\r\n        result = result.write(i, tf.stack([tf.cos(t[i]), tf.sin(t[i])]))\r\n        i = i+1\r\n```", "Looks like the workaround only fixes the difference partially - there is still a pair of logical_and/logical_or ops which are slow on GPU and hurt performance [1]. This will be fixed in TF 2.4.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4e73f5f852941754f1f4a2880bf92d5d2b26e51b/tensorflow/python/autograph/operators/control_flow.py#L525\r\n"]}, {"number": 40516, "title": "Tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.", "body": "\r\n**System information**\r\n- Tf 2.2.0\r\n- Windows10\r\n- Python version: 5.7.6\r\n- git version v2.2.0-rc4-8-g2b96f3662b\r\n\r\n\r\n\r\nWhen I pass class_weight to model.fit() this error comes and when I do not pass it goes? any idea how to solve it?\r\n\r\n`\r\nTensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  indices[84] = 31 is not in [0, 31)\r\n\t [[{{node GatherV2}}]]\r\n\t [[IteratorGetNext]]\r\n\t [[ReverseSequence_1/_502]]\r\n  (1) Invalid argument:  indices[84] = 31 is not in [0, 31)\r\n\t [[{{node GatherV2}}]]\r\n\t [[IteratorGetNext]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_28314]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n`\r\n\r\n`       model_input = tf.keras.layers.Input(shape=(timesteps,), dtype='int32')\r\n        self.createBertLayer()\r\n\r\n        bert = self.bert_layer(model_input)\r\n\r\n\r\n        self.d = tf.keras.layers.Dense(num_hiddens, activation=tf.nn.relu)(bert)\r\n        h = tf.keras.layers.Dropout(0.5)(self.d)\r\n        self.y1 = tf.keras.layers.Dense(classes, activation=tf.nn.softmax)(h)\r\n\r\n\r\n        self.crf = CRF()\r\n        output = self.crf(self.y1)\r\n\r\n        model = Model(model_input, output)\r\n        model.build(input_shape=(None, timesteps))\r\n        self.bert_layer.apply_adapter_freeze()\r\n        self.bert_layer.trainable = False\r\n\r\n        model.compile(loss=self.crf.loss, optimizer=tf.keras.optimizers.Adam(0.001), metrics=[self.crf.accuracy])\r\n\r\n        history = self.model.fit(\r\n            training_set,\r\n            training_label,\r\n            class_weight=class_weights,\r\n            epochs=n_epochs,\r\n            # validation_data=(testing_set, one_hotted2),\r\n            validation_split=0.1,\r\n            verbose=1,\r\n            batch_size=256\r\n            # callbacks=[cp_callback]\r\n        )\r\n`\r\n\r\nI checked class_weight keys and there was no problem with it.", "comments": ["@mahdiabdollahpour,\r\nOn running the code, I'm facing an error stating `NameError: name 'timesteps' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/340241daddb4646b07b9788dd009f549/40516.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40515, "title": "Keras callback stops receiving some params with tensorflow 2.2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 31\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: Python 3.7.5\r\n- CUDA/cuDNN version: No GPU\r\n- GPU model and memory: No GPU\r\n\r\n**Describe the current behavior**\r\n\r\nWhen defining a custom Keras callback, the `set_params` method receives a subset of parameters in tensorflow 2.2.0 in comparison to what it used to receive in tensorflow 2.1.1.\r\n\r\nIn tensorflow 2.2.0, it receives:\r\n\r\n```\r\n{'verbose': 1, 'epochs': 2, 'steps': 1}\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect to get the same params as with tensorflow 2.1.1, which are:\r\n\r\n```\r\n{'batch_size': 4, 'epochs': 2, 'steps': None, 'samples': 4, 'verbose': 1, 'do_validation': False, 'metrics': ['loss']}\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nI used the following script to test the behavior with the two version of tensorflows and also trying with `keras` versus `tf.keras`:\r\n\r\n```\r\nimport numpy as np\r\n# import keras\r\nimport tensorflow.keras as keras\r\n\r\ndef build_xor_data():\r\n    x_train = [np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=float)]\r\n    y_train = [np.array([[0], [1], [1], [0]], dtype=float)]\r\n\r\n    return x_train, y_train\r\n\r\n\r\ndef build_xor_model_keras():\r\n    input_layer = keras.layers.Input(shape=(2,))\r\n    hidden_layer = keras.layers.Dense(2, activation=\"sigmoid\")(input_layer)\r\n    output_layer = keras.layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\r\n    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\r\n    model.compile(loss=\"mse\", optimizer=\"adam\")\r\n    return model\r\n\r\nx_train, y_train = build_xor_data()\r\n\r\nmodel = build_xor_model_keras()\r\n\r\nclass DebugCallback(keras.callbacks.Callback):\r\n\r\n    def set_params(self, params):\r\n        print(\"SET PARAMS\", locals())\r\n\r\nprint(\"KERAS VERSION\", keras.__version__)\r\nmodel.fit(x_train, y_train, batch_size=4, epochs=2, callbacks=[DebugCallback()])\r\n```\r\n\r\nI got the following outputs:\r\n\r\n|          \t| keras                                                                                                                  \t| tf.keras                                                                                                            \t|\r\n|----------\t|------------------------------------------------------------------------------------------------------------------------\t|---------------------------------------------------------------------------------------------------------------------\t|\r\n| tf 2.1.1 \t| {'batch_size': 4, 'epochs': 2, 'steps': None, 'samples': 4, 'verbose': 1, 'do_validation': False, 'metrics': ['loss']} \t| {'batch_size': 4, 'epochs': 2, 'steps': 1, 'samples': 4, 'verbose': 0, 'do_validation': False, 'metrics': ['loss']} \t|\r\n| tf 2.2.0 \t| {'batch_size': 4, 'epochs': 2, 'steps': None, 'samples': 4, 'verbose': 1, 'do_validation': False, 'metrics': ['loss']} \t| {'verbose': 1, 'epochs': 2, 'steps': 1}                                                                             \t|\r\n\r\nIn tensorflow 2.2.0, we don't get anymore the following parameters:\r\n* batch_size\r\n* samples\r\n* do_validation\r\n* metrics\r\n\r\nAfter reading the release note, section `Breaking changes` https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0, I could expect to not receive `metrics` anymore. Are the other missing params also linked to that breaking change?", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/7277eb9f69678632d428fb4df62bfb2f/untitled231.ipynb).", "Let me know if you need more information from my side.", "Was able to reproduce the issue using TF [2.1](https://colab.research.google.com/gist/saikumarchalla/7f581eec506edfcf4e9887905bcc657a/untitled231.ipynb) and [2.5](https://colab.research.google.com/gist/saikumarchalla/10b710439c0d2301b45591b87d776e36/untitled84.ipynb). Please find the attached gist.Thanks!", "@Lothiraldan This issue is still replicating in  TF v2.7.0. Please find the  [gist](https://colab.research.google.com/gist/sushreebarsa/cdb73d0c2ce1138dcf0970aae9b6c338/untitled84.ipynb) for reference .Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40515\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40515\">No</a>\n"]}, {"number": 40514, "title": "TFBertForSequenceClassification: Non-deterministic when training on GPU, even with seeds fixed and TF_DETERMINISTIC_OPS=\"1\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): used on Google Colab\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla K80\r\n\r\n\r\n**Describe the current behavior**\r\n[Colab Link with a minimal example](https://colab.research.google.com/drive/1VSU8lYFD0E1HKZrIL1MvyIRwAktlSF_t#scrollTo=9NM47IRsy_v4)\r\n\r\nTraining a [transformers](huggingface/transformers/) `TFBertForSequenceClassification` model ([model code direkt link](https://github.com/huggingface/transformers/blob/3d495c61efbd2ca8a17827ff3103f7c820f0e9da/src/transformers/modeling_tf_bert.py#L910)) with keras/tf2.2.0 on the GPU is non-deterministic, even when setting all relevant random seeds and following the best practice guide in [these gputechconf slides](https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9911-determinism-in-deep-learning.pdf). \r\nAs clearly documented in [this table](https://github.com/NVIDIA/tensorflow-determinism#confirmed-current-gpu-specific-sources-of-non-determinism-with-solutions), training on the GPU can be expected to be deterministic for TF 2.2.0 when setting environment variable `TF_DETERMINISTIC_OPS` to `\"1\"`. \r\nIn spite of me setting this variable in my code (prior to importing tensorflow), this is not the case in my example.\r\n\r\nHaving seen previous issues (#38185) documented for `CrossEntropyLoss`, I also used the [suggested workaround for those](https://github.com/tensorflow/tensorflow/issues/38185#issuecomment-643014439). \r\nNonetheless, my code remains non-deterministic on the GPU. \r\n\r\nThe following table summarizes my results **after only training for five steps** , numbers were computed using [this function suggested to verify training reproducibility](https://github.com/NVIDIA/tensorflow-determinism/issues/2#issuecomment-548210203):\r\n\r\n| | Device | Before training | After training |\r\n| ------------- | ------------- | ------------- | ------------- |\r\n| Run 1  | GPU | -641227.5609667897224  | -641237.442 **`5159916282`** |\r\n| Run 2  | GPU | -641227.5609667897224  | -641237.442 **`3093758523`** |\r\n| | |  | |\r\n| Run 1 | CPU | -641227.5609667301178 | -641238.1506845243275 |\r\n| Run 2 | CPU | -641227.5609667301178 | -641238.1506845243275 |\r\n\r\n\r\n**Describe the expected behavior**\r\nFor TF 2.2.0, training on the GPU is expected to be deterministic when fixing random seeds and using: \r\n```python\r\nos.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\r\n```\r\nThus, my expectation is that the example code provided in my colab notebook should be deterministic on the colab GPU runtime as well.\r\n\r\n**Standalone code to reproduce the issue**\r\nColab Link, reproducible on CPU runtime, non-deterministic on GPU runtime: https://colab.research.google.com/drive/1VSU8lYFD0E1HKZrIL1MvyIRwAktlSF_t#scrollTo=9NM47IRsy_v4\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n", "comments": ["I have tried reproducing issue in TF2.2 -[CPU ](https://colab.research.google.com/gist/ravikyram/baaff5ec5a89755b7bb210925995d0b7/untitled34.ipynb) and [GPU](https://colab.research.google.com/gist/ravikyram/644e4ada0911e91dba7072ca7844c592/untitled33.ipynb).Is this the expected behavior?.Thanks!", "Note that this issue is being addressed as an [issue in the tensorflow-determinism repo](https://github.com/NVIDIA/tensorflow-determinism/issues/19).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40514\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40514\">No</a>\n"]}, {"number": 40513, "title": "Docker images with tags '1.15.3' are missing on Docker Hub", "body": "We would like to update our images to release 1.15.3 for the security fixes it contains. I'm not seeing any 1.15.3 images on dockerhub though.\r\n\r\nhttps://hub.docker.com/r/tensorflow/tensorflow/tags?name=1.15.3", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40513\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40513\">No</a>\n"]}, {"number": 40512, "title": "fix: convolutional padding argument valid and same explanation", "body": "Fixed convolutional padding argument \"valid\" and \"same\" explanation\r\n\r\nThis helps other developers understand how to use the padding correctly, as currently there is no explanations for what padding \"valid\" or \"same\" mean, leading to confusion.\r\n\r\nPlease do comment on how I can get this merged,\r\n\r\nAny feedback is most welcome", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40512) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40512) for more info**.\n\n<!-- ok -->", "Thank you, I have resolved the suggested change, this should be good :)", "Let me know if there is anything else I can do on this ? Otherwise should be good :)", "> Let me know if there is anything else I can do on this ? Otherwise should be good :)\r\n\r\nI don't think my previous comments were addressed?", "> > Let me know if there is anything else I can do on this ? Otherwise should be good :)\r\n> \r\n> I don't think my previous comments were addressed?\r\n\r\nI did, it seems that `Conv2D` and `Conv3D` do not support the `\"casual\"` padding parameter, therefore I only added the padding definitions where the `\"casual\"` padding parameter was already defined as an option (i.e. valid). Let me know if I am incorrect and I will update the padding options in all the associated places accordingly. However @kyscg seems to think the same as well.", "I agree, it is possible that we've misunderstood @tanzhenyu 's suggestions. Further clarification would be very useful.", "> > > Let me know if there is anything else I can do on this ? Otherwise should be good :)\r\n> > \r\n> > \r\n> > I don't think my previous comments were addressed?\r\n> \r\n> I did, it seems that `Conv2D` and `Conv3D` do not support the `\"casual\"` padding parameter, therefore I only added the padding definitions where the `\"casual\"` padding parameter was already defined as an option (i.e. valid). Let me know if I am incorrect and I will update the padding options in all the associated places accordingly. However @kyscg seems to think the same as well.\r\n\r\nThat makes sense."]}, {"number": 40511, "title": "fix LaunchDepthwiseConvBackpropFilterOp", "body": "fix this issue with depthwise conv https://github.com/tensorflow/tensorflow/issues/27285", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40511) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40511) for more info**.\n\n<!-- ok -->", "Excellent catch! Thanks for the fix.", "@reedwm thanks! Is the ci failure relevant to this commit?", "Failure looks unrelated. This should be merged soon."]}, {"number": 40510, "title": "[RNN] LSTM and Bidir layers can't be converted in a TFLite model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 \r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (or github SHA if from source): 2.2.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf absolutely needed I will upload some code on a Colab, please request it.\r\n\r\n```python\r\n#representative dataset \r\ninput_ds = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(X_train, dtype=np.float32))\r\n\r\ndef representative_data_gen():\r\n      for input_value in input_ds.take(1000).batch(1):\r\n            yield [input_value]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(m[\"k_model\"])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.representative_dataset = representative_data_gen\r\n\r\nprint(\"quantization model conversion started\")\r\n%time m[\"k_model_tflite\"] = converter.convert()\r\nprint(\"quantization model conversion completed\")\r\n\r\ntflite_model_file = 'current_converted_model.tflite'\r\nf = open(tflite_model_file, 'wb')\r\nf.write(m[\"k_model_tflite\"])\r\nf.close()\r\nprint(\"quantization model saved on file\")\r\n\r\n```\r\noutput: \r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\nRuntimeError: Only models with a single subgraph are supported, model had 9 subgraphs\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nSystemError                               Traceback (most recent call last)\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py in __init__(self, model_content)\r\n     50       self._calibrator = (_calibration_wrapper.CalibrationWrapper\r\n---> 51                           .CreateWrapperCPPFromBuffer(model_content))\r\n     52     except Exception as e:\r\n\r\nSystemError: <built-in function CalibrationWrapper_CreateWrapperCPPFromBuffer> returned a result with an error set\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    520     if self._is_calibration_quantize():\r\n    521       result = self._calibrate_quantize_model(\r\n--> 522           result, constants.FLOAT, constants.FLOAT)\r\n    523 \r\n    524     return result\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type)\r\n    259                                 inference_output_type):\r\n    260     allow_float = not self._is_int8_target_required()\r\n--> 261     calibrate_quantize = _calibrator.Calibrator(result)\r\n    262     if self._experimental_calibrate_only:\r\n    263       return calibrate_quantize.calibrate(self.representative_dataset.input_gen)\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py in __init__(self, model_content)\r\n     51                           .CreateWrapperCPPFromBuffer(model_content))\r\n     52     except Exception as e:\r\n---> 53       raise ValueError(\"Failed to parse the model: %s.\" % e)\r\n     54     if not self._calibrator:\r\n     55       raise ValueError(\"Failed to parse the model.\")\r\n\r\nValueError: Failed to parse the model: <built-in function CalibrationWrapper_CreateWrapperCPPFromBuffer> returned a result with an error set.\r\n\r\n```\r\nmodel is quite large and not included please indicated if is needed.\r\n\r\n**Failure details**\r\nWhen the Bidir and LSTM layers are removed the conversion works without error. \r\nWhen the Bidir and LSTM layers are present the above error is presented.\r\n\r\naccording to #36219 is seems that this type of error coudl be raised when using TFLite for microcontroller, this is not the intention here. Is there something to do to ensure that it is not the version for microcontroller which is used. \r\nFor context, the target is the coral dev board (edge-tpu on linux platform).\r\n\r\n**Any other info / logs**\r\nmodel summary that I tried to convert.\r\n```\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninputs (InputLayer)             [(None, 250, 1)]     0                                            \r\n__________________________________________________________________________________________________\r\nbidirectional (Bidirectional)   (None, 250, 100)     20800       inputs[0][0]                     \r\n__________________________________________________________________________________________________\r\nflatten (Flatten)               (None, 25000)        0           bidirectional[0][0]              \r\n__________________________________________________________________________________________________\r\nflatten_1 (Flatten)             (None, 250)          0           inputs[0][0]                     \r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 25250)        0           flatten[0][0]                    \r\n                                                                 flatten_1[0][0]                  \r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 2048)         51714048    concatenate[0][0]                \r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 1024)         2098176     dense[0][0]                      \r\n__________________________________________________________________________________________________\r\ndense_2 (Dense)                 (None, 512)          524800      dense_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_7 (Dense)                 (None, 2048)         51714048    concatenate[0][0]                \r\n__________________________________________________________________________________________________\r\ndense_3 (Dense)                 (None, 256)          131328      dense_2[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_8 (Dense)                 (None, 1024)         2098176     dense_7[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_4 (Dense)                 (None, 128)          32896       dense_3[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_9 (Dense)                 (None, 512)          524800      dense_8[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_5 (Dense)                 (None, 50)           6450        dense_4[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_10 (Dense)                (None, 256)          131328      dense_9[0][0]                    \r\n__________________________________________________________________________________________________\r\nreshape (Reshape)               (None, 50, 1)        0           dense_5[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_11 (Dense)                (None, 128)          32896       dense_10[0][0]                   \r\n__________________________________________________________________________________________________\r\ntime_distributed (TimeDistribut (None, 50, 1)        2           reshape[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_12 (Dense)                (None, 50)           6450        dense_11[0][0]                   \r\n__________________________________________________________________________________________________\r\nom (Reshape)                    (None, 50, 1)        0           time_distributed[0][0]           \r\n__________________________________________________________________________________________________\r\nreshape_1 (Reshape)             (None, 50, 1)        0           dense_12[0][0]                   \r\n__________________________________________________________________________________________________\r\nconcatenate_1 (Concatenate)     (None, 50, 2)        0           om[0][0]                         \r\n                                                                 reshape_1[0][0]                  \r\n__________________________________________________________________________________________________\r\nflatten_2 (Flatten)             (None, 100)          0           concatenate_1[0][0]              \r\n__________________________________________________________________________________________________\r\ndense_13 (Dense)                (None, 100)          10100       flatten_2[0][0]                  \r\n__________________________________________________________________________________________________\r\ndense_14 (Dense)                (None, 100)          10100       dense_13[0][0]                   \r\n__________________________________________________________________________________________________\r\ndense_15 (Dense)                (None, 50)           5050        dense_14[0][0]                   \r\n__________________________________________________________________________________________________\r\ndense_16 (Dense)                (None, 50)           2550        dense_15[0][0]                   \r\n__________________________________________________________________________________________________\r\nreshape_2 (Reshape)             (None, 50, 1)        0           dense_16[0][0]                   \r\n__________________________________________________________________________________________________\r\ntime_distributed_1 (TimeDistrib (None, 50, 1)        2           reshape_2[0][0]                  \r\n__________________________________________________________________________________________________\r\nof (Reshape)                    (None, 50, 1)        0           time_distributed_1[0][0]         \r\n==================================================================================================\r\nTotal params: 109,064,000\r\nTrainable params: 109,064,000\r\nNon-trainable params: 0\r\n```\r\n\r\n", "comments": ["Hi,\r\nI have the same issue trying to qunatize the DeepSpeech2 model:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nX (InputLayer)               [(None, 199, 160)]        0         \r\n_________________________________________________________________\r\nlambda (Lambda)              (None, 199, 160, 1)       0         \r\n_________________________________________________________________\r\nconv_1 (Conv2D)              (None, 100, 80, 32)       14432     \r\n_________________________________________________________________\r\nconv_1_bn (BatchNormalizatio (None, 100, 80, 32)       128       \r\n_________________________________________________________________\r\nconv_1_relu (ReLU)           (None, 100, 80, 32)       0         \r\n_________________________________________________________________\r\nconv_2 (Conv2D)              (None, 100, 40, 32)       236544    \r\n_________________________________________________________________\r\nconv_2_bn (BatchNormalizatio (None, 100, 40, 32)       128       \r\n_________________________________________________________________\r\nconv_2_relu (ReLU)           (None, 100, 40, 32)       0         \r\n_________________________________________________________________\r\nreshape (Reshape)            (None, None, 1280)        0         \r\n_________________________________________________________________\r\nbidirectional_1 (Bidirection (None, None, 1600)        9993600   \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, None, 1600)        0         \r\n_________________________________________________________________\r\nbidirectional_2 (Bidirection (None, None, 1600)        11529600  \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, None, 1600)        0         \r\n_________________________________________________________________\r\nbidirectional_3 (Bidirection (None, None, 1600)        11529600  \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, None, 1600)        0         \r\n_________________________________________________________________\r\nbidirectional_4 (Bidirection (None, None, 1600)        11529600  \r\n_________________________________________________________________\r\ndropout_3 (Dropout)          (None, None, 1600)        0         \r\n_________________________________________________________________\r\nbidirectional_5 (Bidirection (None, None, 1600)        11529600  \r\n_________________________________________________________________\r\ndense_1 (TimeDistributed)    (None, None, 1600)        2561600   \r\n_________________________________________________________________\r\ndense_1_relu (ReLU)          (None, None, 1600)        0         \r\n_________________________________________________________________\r\ndropout_4 (Dropout)          (None, None, 1600)        0         \r\n_________________________________________________________________\r\ndense_2 (TimeDistributed)    (None, None, 29)          46429     \r\n=================================================================\r\nTotal params: 58,971,261\r\nTrainable params: 58,971,133\r\nNon-trainable params: 128\r\n_________________________________________________________________\r\n```\r\nthe only difference in my script is following parameter:\r\n```python\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n```\r\nI would like to know what is the solution as well. Thanks", "Hi,\r\nthe integer-only uni_di_lstm is not done yet. :(\r\n", "Hi @renjie-liu \r\nThanks for that, do you mean bidirectional_sequence or unidirectional? is it possible to make a temporary solution on my side until the feature is implemented?\r\nWhat is the tentative/aspirational timeline for this feature?\r\n", "both are not done yet. @jianlijianli is working on that.\r\n\r\nalternative is you can use dynamic ranged quantization: simply remove the representative dataset.", "Dear @renjie-liu \r\n\r\nHere is what I obtained while following your advice of removing the representative datasets: this leads to the following error message:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    489 \r\n    490     self._validate_quantization()\r\n--> 491     self._validate_representative_dataset()\r\n    492     if self._trackable_obj is None:\r\n    493       self._debug_info = _get_debug_info(\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in _validate_representative_dataset(self)\r\n    212             \"Provide an input generator for representative_dataset\")\r\n    213     elif self._is_int8_target_required():\r\n--> 214       raise ValueError(\"representative_dataset is required when specifying \"\r\n    215                        \"TFLITE_BUILTINS_INT8 or INT8 supported types.\")\r\n    216 \r\n\r\nValueError: representative_dataset is required when specifying TFLITE_BUILTINS_INT8 or INT8 supported types.\r\n```\r\nthen removing ```TFLITE_BUILTINS_INT8```\r\n\r\ngives:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    516         input_tensors=input_tensors,\r\n    517         output_tensors=output_tensors,\r\n--> 518         **converter_kwargs)\r\n    519 \r\n    520     if self._is_calibration_quantize():\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    494       input_data.SerializeToString(),\r\n    495       debug_info_str=debug_info_str,\r\n--> 496       enable_mlir_converter=enable_mlir_converter)\r\n    497   return data\r\n    498 \r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    225       stdout = _try_convert_to_unicode(stdout)\r\n    226       stderr = _try_convert_to_unicode(stderr)\r\n--> 227       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    228   finally:\r\n    229     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2020-06-17 14:09:31.823791: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-17 14:09:35.468192: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.\r\n2020-06-17 14:09:35.468735: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.\r\n2020-06-17 14:09:36.269006: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-06-17 14:09:36.282839: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2047d693bf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-17 14:09:36.283682: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-17 14:09:36.286522: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-06-17 14:09:36.730579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:17:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-06-17 14:09:36.732395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \r\npciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-06-17 14:09:36.733588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-17 14:09:36.742022: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-06-17 14:09:36.749464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-06-17 14:09:36.751902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-06-17 14:09:36.761434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-06-17 14:09:36.766820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-06-17 14:09:36.782061: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-17 14:09:36.784529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-06-17 14:09:37.724046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-17 14:09:37.724489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 \r\n2020-06-17 14:09:37.724957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N N \r\n2020-06-17 14:09:37.725329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   N N \r\n2020-06-17 14:09:37.728213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8685 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2020-06-17 14:09:37.732221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8684 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2020-06-17 14:09:37.740626: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x204e6ffad70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-17 14:09:37.741528: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-06-17 14:09:37.742259: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-06-17 14:09:44.071294: E tensorflow/lite/tools/optimize/quantize_weights.cc:351] Quantize weights tool only supports tflite models with one subgraph.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Eric\\virtualenvs\\venvGPU\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: Quantize weights transformation failed.\r\n```\r\nthe console doesn't give much more:\r\n```\r\n2020-06-17 14:09:27.279730: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-06-17 14:09:27.286798: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 234 nodes (-132), 294 edges (-178), time = 10287.6396ms.\r\n2020-06-17 14:09:27.298610: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 234 nodes (0), 294 edges (0), time = 888.156ms.\r\n2020-06-17 14:09:27.309362: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: while_body_16883_frozen\r\n2020-06-17 14:09:27.315685: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 53 nodes (-1), 53 edges (0), time = 1.028ms.\r\n2020-06-17 14:09:27.324240: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 53 nodes (0), 53 edges (0), time = 0.486ms.\r\n2020-06-17 14:09:27.330354: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: while_body_16439_frozen\r\n2020-06-17 14:09:27.334471: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 53 nodes (-1), 53 edges (0), time = 0.949ms.\r\n2020-06-17 14:09:27.340945: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 53 nodes (0), 53 edges (0), time = 0.475ms.\r\n2020-06-17 14:09:27.345857: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: while_cond_16438_frozen\r\n2020-06-17 14:09:27.349608: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.195ms.\r\n2020-06-17 14:09:27.356077: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.109ms.\r\n2020-06-17 14:09:27.361009: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: while_cond_16882_frozen\r\n2020-06-17 14:09:27.364708: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.186ms.\r\n2020-06-17 14:09:27.371446: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.109ms.\r\n```\r\n\r\nthen removing/commenting the other mentions of INT8 \r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(m[\"k_model\"])\r\n# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n# converter.inference_input_type = tf.uint8\r\n# converter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n# converter.representative_dataset = representative_data_gen\r\n%time m[\"k_model_tflite\"] = converter.convert()\r\n\r\n```\r\n\r\ngives a similar error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    516         input_tensors=input_tensors,\r\n    517         output_tensors=output_tensors,\r\n--> 518         **converter_kwargs)\r\n    519 \r\n    520     if self._is_calibration_quantize():\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    494       input_data.SerializeToString(),\r\n    495       debug_info_str=debug_info_str,\r\n--> 496       enable_mlir_converter=enable_mlir_converter)\r\n    497   return data\r\n    498 \r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    225       stdout = _try_convert_to_unicode(stdout)\r\n    226       stderr = _try_convert_to_unicode(stderr)\r\n--> 227       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    228   finally:\r\n    229     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2020-06-17 14:13:18.338700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-17 14:13:20.777185: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.\r\n2020-06-17 14:13:20.777406: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.\r\n2020-06-17 14:13:21.511767: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-06-17 14:13:21.521765: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ec800b0cd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-17 14:13:21.522623: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-17 14:13:21.526087: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-06-17 14:13:21.947281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:17:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-06-17 14:13:21.948085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \r\npciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-06-17 14:13:21.948693: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-17 14:13:21.951717: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-06-17 14:13:21.954220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-06-17 14:13:21.955078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-06-17 14:13:21.958552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-06-17 14:13:21.960268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-06-17 14:13:21.965218: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-17 14:13:21.966207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-06-17 14:13:22.970868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-17 14:13:22.971142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 \r\n2020-06-17 14:13:22.971334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N N \r\n2020-06-17 14:13:22.971532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   N N \r\n2020-06-17 14:13:22.972496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8685 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2020-06-17 14:13:22.974683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8684 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2020-06-17 14:13:22.979327: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ecf26f33f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-17 14:13:22.979742: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-06-17 14:13:22.980063: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-06-17 14:13:28.134958: E tensorflow/lite/tools/optimize/quantize_weights.cc:351] Quantize weights tool only supports tflite models with one subgraph.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Eric\\virtualenvs\\venvGPU\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: Quantize weights transformation failed.\r\n\r\n```\r\n\r\nDid I do something wrongly?\r\n\r\n\r\n\r\n", "can you try set uint8? thanks!", "Dear @renjie-liu, \r\n\r\nSorry I was a bit confused with your last comment, I am not sure how I should set unit8. I tried the following 4 ways, if it is not one of them please let me know how to do it.\r\n\r\nA)\r\n```python\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n```\r\nthis gives **ValueError** see **Output A**\r\n\r\nB)\r\n```python\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n```\r\nthis gives **ConverterError** see **Output B**\r\n\r\nC)\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(m[\"k_model\"])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n# converter.inference_input_type = tf.uint8\r\n# converter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n```\r\nthis gives **ConverterError** see **Output C**\r\n\r\nD)\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(m[\"k_model\"])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n# converter.inference_input_type = tf.uint8\r\n# converter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n```\r\nThis gives **ValueError** see **Output D**\r\n\r\n### Output A\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    489 \r\n    490     self._validate_quantization()\r\n--> 491     self._validate_representative_dataset()\r\n    492     if self._trackable_obj is None:\r\n    493       self._debug_info = _get_debug_info(\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in _validate_representative_dataset(self)\r\n    212             \"Provide an input generator for representative_dataset\")\r\n    213     elif self._is_int8_target_required():\r\n--> 214       raise ValueError(\"representative_dataset is required when specifying \"\r\n    215                        \"TFLITE_BUILTINS_INT8 or INT8 supported types.\")\r\n    216 \r\n\r\nValueError: representative_dataset is required when specifying TFLITE_BUILTINS_INT8 or INT8 supported types.\r\n```\r\n### Output B\r\n```\r\nConverterError                            Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    516         input_tensors=input_tensors,\r\n    517         output_tensors=output_tensors,\r\n--> 518         **converter_kwargs)\r\n    519 \r\n    520     if self._is_calibration_quantize():\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    494       input_data.SerializeToString(),\r\n    495       debug_info_str=debug_info_str,\r\n--> 496       enable_mlir_converter=enable_mlir_converter)\r\n    497   return data\r\n    498 \r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    225       stdout = _try_convert_to_unicode(stdout)\r\n    226       stderr = _try_convert_to_unicode(stderr)\r\n--> 227       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    228   finally:\r\n    229     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2020-06-17 18:46:36.145653: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-17 18:46:38.618161: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.\r\n2020-06-17 18:46:38.618432: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.\r\n2020-06-17 18:46:39.356324: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-06-17 18:46:39.364369: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d1e927b430 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-17 18:46:39.365063: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-17 18:46:39.367997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-06-17 18:46:39.805154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:17:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-06-17 18:46:39.805932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \r\npciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-06-17 18:46:39.806514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-17 18:46:39.809411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-06-17 18:46:39.811840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-06-17 18:46:39.812680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-06-17 18:46:39.823032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-06-17 18:46:39.826299: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-06-17 18:46:39.831259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-17 18:46:39.832233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-06-17 18:46:40.799158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-17 18:46:40.799646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 \r\n2020-06-17 18:46:40.800115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N N \r\n2020-06-17 18:46:40.800535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   N N \r\n2020-06-17 18:46:40.803384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8685 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2020-06-17 18:46:40.807501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8684 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2020-06-17 18:46:40.815406: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d273f657e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-17 18:46:40.816328: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-06-17 18:46:40.816984: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-06-17 18:46:45.919665: E tensorflow/lite/tools/optimize/quantize_weights.cc:351] Quantize weights tool only supports tflite models with one subgraph.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Eric\\virtualenvs\\venvGPU\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: Quantize weights transformation failed.\r\n\r\n```\r\n\r\n### Output C\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    516         input_tensors=input_tensors,\r\n    517         output_tensors=output_tensors,\r\n--> 518         **converter_kwargs)\r\n    519 \r\n    520     if self._is_calibration_quantize():\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    494       input_data.SerializeToString(),\r\n    495       debug_info_str=debug_info_str,\r\n--> 496       enable_mlir_converter=enable_mlir_converter)\r\n    497   return data\r\n    498 \r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    225       stdout = _try_convert_to_unicode(stdout)\r\n    226       stderr = _try_convert_to_unicode(stderr)\r\n--> 227       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    228   finally:\r\n    229     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2020-06-17 18:48:23.209132: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-17 18:48:25.639476: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.\r\n2020-06-17 18:48:25.639701: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.\r\n2020-06-17 18:48:26.327563: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-06-17 18:48:26.335602: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1a94038c640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-17 18:48:26.336395: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-17 18:48:26.341342: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-06-17 18:48:26.760572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:17:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-06-17 18:48:26.761337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \r\npciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-06-17 18:48:26.761945: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-17 18:48:26.768485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-06-17 18:48:26.775958: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-06-17 18:48:26.778456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-06-17 18:48:26.789050: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-06-17 18:48:26.794500: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-06-17 18:48:26.811420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-17 18:48:26.814332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-06-17 18:48:27.777258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-17 18:48:27.777745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 \r\n2020-06-17 18:48:27.778170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N N \r\n2020-06-17 18:48:27.778563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   N N \r\n2020-06-17 18:48:27.781459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8685 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2020-06-17 18:48:27.785213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8684 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2020-06-17 18:48:27.793258: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1a9f28eeca0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-17 18:48:27.794118: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-06-17 18:48:27.794847: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-06-17 18:48:32.986552: E tensorflow/lite/tools/optimize/quantize_weights.cc:351] Quantize weights tool only supports tflite models with one subgraph.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Eric\\virtualenvs\\venvGPU\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"c:\\users\\eric\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: Quantize weights transformation failed.\r\n\r\n```\r\n\r\n### Output B\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    489 \r\n    490     self._validate_quantization()\r\n--> 491     self._validate_representative_dataset()\r\n    492     if self._trackable_obj is None:\r\n    493       self._debug_info = _get_debug_info(\r\n\r\nc:\\users\\eric\\virtualenvs\\venvgpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in _validate_representative_dataset(self)\r\n    212             \"Provide an input generator for representative_dataset\")\r\n    213     elif self._is_int8_target_required():\r\n--> 214       raise ValueError(\"representative_dataset is required when specifying \"\r\n    215                        \"TFLITE_BUILTINS_INT8 or INT8 supported types.\")\r\n    216 \r\n\r\nValueError: representative_dataset is required when specifying TFLITE_BUILTINS_INT8 or INT8 supported types.\r\n```\r\n\r\n\r\n\r\n", "sorry, I was suggesting B)\r\n\r\nI think the error you got is because lstm is not fused. (so we will have a while loop)\r\n\r\nLet's do a two steps then:\r\n\r\n1) pure float, this step, let's make sure the model is correctly fused:\r\ncan you follow the codelab like here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb\r\n\r\n2) we can use netron to visualize the graph, once we make sure the lstms are fused, we can try the above with dynamic ranged quantization.\r\n\r\nthanks", "Dear Renjie-liu, \r\n\r\nThank you for that, in order to limit extra information I reduced the network (it still exhibits the same error when trying to quantize with the B option). \r\n\r\nSummary\r\n```\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninputs (InputLayer)          [(None, 250, 1)]          0         \r\n_________________________________________________________________\r\nbidirectional_3 (Bidirection (None, 250, 100)          20800     \r\n_________________________________________________________________\r\nflatten_5 (Flatten)          (None, 25000)             0         \r\n_________________________________________________________________\r\ndense_9 (Dense)              (None, 50)                1250050   \r\n_________________________________________________________________\r\nreshape_3 (Reshape)          (None, 50, 1)             0         \r\n_________________________________________________________________\r\ntime_distributed_3 (TimeDist (None, 50, 1)             2         \r\n_________________________________________________________________\r\nom (Reshape)                 (None, 50, 1)             0         \r\n=================================================================\r\nTotal params: 1,270,852\r\nTrainable params: 1,270,852\r\nNon-trainable params: 0\r\n```\r\nthen as in the colab, I do a conversion without any option:\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(m[\"k_model\"])\r\n%time m[\"k_model_float_tflite\"] = converter.convert()\r\n```\r\n\r\nI then saved the resulting tflite, and displayed it with netron (showing attributes, initialisers and names) see below. I am not sure if the lstm is fused or not please advise the next step.\r\n\r\n![current_float_model tflite](https://user-images.githubusercontent.com/13007637/85074048-320e7e00-b1bc-11ea-8171-07d1923bf26a.png)\r\n\r\n", "Looks like it's not fused, can you try upgrade to tf-nightly and use saved model to export (and set concret_func)\r\n\r\nbasically follow the instructions here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb\r\n\r\nthanks", "Hi @renjie-liu, \r\n\r\nIt appears that setting the concrete_func is what makes it possible to save the model (when set it it works in tf-nightly and the release version 2.2.0).\r\nthe resulting converted TFLite is below:\r\n\r\n![current_float_support_model tflite](https://user-images.githubusercontent.com/13007637/85134518-ecde6080-b23c-11ea-9004-3c2939ee77a2.png)\r\nAm I assuming correctly that this is fused?\r\n\r\nI then try to quantize with the previous example B (with tf-nightly):\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR) # the saved model has the concrete_func set\r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n\r\nqtflite_model = converter.convert()\r\n```\r\n\r\nwhich gives the following error: \r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-22-f991b26a4f82> in <module>()\r\n      6 converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n      7 \r\n----> 8 qtflite_model = converter.convert()\r\n      9 \r\n     10 qtflite_model_file = 'test_supportq.tflite'\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    670     return super(TFLiteSavedModelConverterV2,\r\n    671                  self).convert(meta_graph.graph_def, input_tensors,\r\n--> 672                                output_tensors)\r\n    673 \r\n    674 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n    550                                   self.representative_dataset, graph_def)\r\n    551 \r\n--> 552     self._validate_inference_input_output_types(quant_mode)\r\n    553 \r\n    554     if not self._is_unknown_shapes_allowed():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in _validate_inference_input_output_types(self, quant_mode)\r\n    525     elif self.inference_input_type not in default_types or \\\r\n    526         self.inference_output_type not in default_types:\r\n--> 527       raise ValueError(\"The inference_input_type and inference_output_type \"\r\n    528                        \"must be tf.float32.\")\r\n    529 \r\n\r\nValueError: The inference_input_type and inference_output_type must be tf.float32.\r\n```\r\nI am a bit puzzled that tf.float32 are expected (instead of tf.uint8); please advise.\r\n", "that's pretty strange.\r\n\r\nAre you using the keras lstm?", "Yes, at least that is my intention.\r\n\r\nYou can find the code that I pushed on the Cola [https://github.com/ericqu/tf-support/blob/master/fusedunfused.ipynb](https://github.com/ericqu/tf-support/blob/master/fusedunfused.ipynb), maybe you can spot an issue.\r\n\r\n", "can you also try make sure you set the enable_v2_behavior at the beginning like here? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb\r\n\r\nthanks!\r\n", "Dear @renjie-liu , \r\n\r\nIt is done, however it gives the same error I updated the [colab](https://github.com/ericqu/tf-support/blob/master/fusedunfused.ipynb) if you want to check it out.\r\n", "Hi Eric,\r\n\r\nCan you the model and see if it has a fused unidirectional_sequence_lstm op?\r\n\r\nWe should make sure the fusion works first.\r\n\r\nthanks!", "Hi @renjie-liu, \r\n\r\nCertainly, please find below the model details following this code:\r\n```python\r\nrun_model = tf.function(lambda x: m[\"k_model\"](x))\r\n# This is important, let's fix the input size.\r\nBATCH_SIZE = cfg_batches_size\r\nSTEPS = 250\r\nINPUT_SIZE = 1\r\nconcrete_func = run_model.get_concrete_function(\r\n    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], m[\"k_model\"].inputs[0].dtype))\r\n\r\nMODEL_DIR = \"support40510\"\r\nm[\"k_model\"].save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\ntflite_model = converter.convert()\r\n\r\ntflite_model_file = 'current_float_support_model.tflite'\r\nf = open(tflite_model_file, 'wb')\r\nf.write(tflite_model)\r\nf.close()\r\n```\r\n\r\n![current_float_support_model tflite](https://user-images.githubusercontent.com/13007637/85202421-9dfcfd80-b306-11ea-8fce-ff0d8f4b8e39.png)\r\n\r\nHow do you distinguish between fused and unfused?\r\n\r\nPlease let me know what you think we should do next.\r\n\r\nThanks for your continued support.\r\n", "Great!\r\n\r\nWe made good progress so far.\r\n\r\nLike I said before, integer-only lstm is not done yet, so we can only do dynamic ranged quantization. (and fused kernel permits you to do so).\r\n\r\ndynamic ranged quantization will requires you to have float inputs/outputs. (the weights will be quantized) you will still have 1/4 the model size and some acceleration.", "Hi @renjie-liu \r\n\r\nThank you for that, and yes I agree that we progressed. \r\nEventually, I need to have the model quantized with integer because that is needed by the edge-tpu compiler: \r\n```\r\nEdge TPU Compiler version 2.1.302470888\r\nInvalid model: current_float_support_model.tflite\r\nModel not quantized\r\n```\r\n\r\nNow I am trying to figure out how to pragmatically move forward.\r\nI am assuming that integer quantization will first be done in tf-nightly. Then I will continue to train my network with the current release (2.2.0), and use the tf-nightly to convert the trained model to TFlite when it is possible.\r\nIs that a reasonable approach, or should I instead try to have everything on tf-nightly?\r\n\r\n", "I see, then you probably need to wait for edge tpu to support quantized lstm.\r\n\r\n", "@ericqu \r\nPlease update as per above comment.", "@Saduf2019 I am not sure I understand what you mean.\r\nMy understanding is that the integer-only lstm quantization is not done yet, hence the issue is open. Do you have any other information to add? ", "@ericqu\r\nPlease confirm if you want us to keep the issue open until then.", "Yes, please keep it open until then. Thanks ", "Hi @jianlijianli\r\nAny updates on this issue? There's still no support for quantized LSTM on the EdgeTPU?\r\n", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Hi Butler, your message is an error. I recommend you read the problem statement at the beginning of the thread (or the label attached to this thread). Otherwise, your message will be automatically disregarded 77 minutes from now.\r\n\r\nI did not see any news that LSTM is getting fully supported on TensorFlow or coral. Did I miss anything?\r\n", "@ericqu , That was wrongly triggered message, please ignore.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40510\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40510\">No</a>\n", "The butler went crazy, again.", "@ericqu Reopening this issue .Thanks !", "@ericqu The latest compiler release supports LSTM(Unidirectional only). Please check supported operations at https://coral.ai/docs/edgetpu/models-intro/#supported-operations for more information .Let us know if it helps? Thank you !", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40510\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40510\">No</a>\n"]}, {"number": 40509, "title": "GatherV2 bug from pb->tflite", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.16.0\r\n- GCC/Compiler version (if compiling from source):7.4.0\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: RTX 2080TI\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv2.1.0-rc2-17-ge5bf8de 2.1.0\r\n\r\n**Describe the current behavior**\r\n![Screenshot from 2020-06-16 17-29-43](https://user-images.githubusercontent.com/17722852/84787868-26be2580-aff7-11ea-98b0-4b29c446c3ce.png)\r\n\r\nI am trying to create a tflite from this pb, but it fails to convert this GatherV2 because\r\n`2020-06-16 17:32:12.974945: F tensorflow/lite/toco/graph_transformations/resolve_constant_gather.cc:65] Check failed: stride * coords_shape.dims(0) == output_data.size() (131072 vs. 1310720)\r\n`\r\nThe code is from resolve_constant_gather.cc\r\n`  CHECK_EQ(stride * coords_shape.dims(0), output_data.size());\r\n`\r\nIs this desired? I think this is a bug.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@paulgheorghecristian TOCO converter is now deprecated. Could you try conversion with MLIR converter from the recent TF version?", "@abattery Thank you for your response! Are you referring to the tflite_convert tool? If not, do you guys have a tutorial for this MLIR?", "The recent version of TF uses MLIR converter by default. You can reuse your conversion code.", "I'm sorry, I am not sure what you are referring to. If I use tflite_convert, which supposedly uses `    converter = lite.TFLiteConverterV2.from_saved_model(flags.saved_model_dir)\r\n`, I still get an TOCO error:\r\n`2020-06-16 18:01:51.768228: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:118] Check failed: dim_x == dim_y (2048 vs. 256)Dimensions must match\r\n`\r\n\r\nShould I try with TF 2.2?\r\n\r\nUpdate:\r\nWith TF 2.2, I get:\r\n`loc(\"transformer-wmt14-seq10-d512-head8-1592317046/decoder/dec_0/attn/mul_1\"): error: 'tfl.mul' op result type '2048x10x10' not broadcast compatible with broadcasted operands's shapes '256x1x10'\r\nTraceback (most recent call last):\r\n  File \"/home/paul/Work/BERT/transformer-tensorflow/BERT_ENV/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/paul/Work/BERT/transformer-tensorflow/BERT_ENV/lib/python3.5/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/paul/Work/BERT/transformer-tensorflow/BERT_ENV/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/paul/Work/BERT/transformer-tensorflow/BERT_ENV/lib/python3.5/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/paul/Work/BERT/transformer-tensorflow/BERT_ENV/lib/python3.5/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/paul/Work/BERT/transformer-tensorflow/BERT_ENV/lib/python3.5/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: loc(\"transformer-wmt14-seq10-d512-head8-1592317046/decoder/dec_0/attn/mul_1\"): 'tfl.mul' op result type '2048x10x10' not broadcast compatible with broadcasted operands's shapes '256x1x10'\r\n`\r\nI will investigate further.", "I think in your model, one Mul op has an unsupported shape like the last error message. Mul op supports broadcasting but it can no handle invalid broadcasting inputs. The converter can catch that. Could you verify your model code with this behavior? The hint is in the location information \"transformer-wmt14-seq10-d512-head8-1592317046/decoder/dec_0/attn/mul_1.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40509\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40509\">No</a>\n"]}, {"number": 40508, "title": "AttributeError: 'Tensor' object has no attribute 'numpy'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6\r\n- GPU model and memory: NVIDIA 4GB gmx150\r\n\r\n**currently my code giving me the error 'AttributeError: 'Tensor' object has no attribute 'numpy'' in line 11**\r\n\r\n**i wanted to print the error in custom loss function**\r\n\r\n**Here is my code**\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport keras.backend as K\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPool2D, Input\r\n\r\ndef loss(yT, yP):\r\n    err = yT - yP\r\n    print(err.numpy())\r\n    squaredError = tf.square(err)\r\n    meanError = tf.reduce_mean(squaredError)\r\n    return meanError\r\n\r\n(xTrain, yTrain), (xTest, yTest) = mnist.load_data()\r\nprint(xTrain.shape, yTrain.shape, type(xTest))\r\n\r\nxTrain, xTest = xTrain / 255, xTest / 255\r\nyTrain, yTest = to_categorical(yTrain, num_classes=10), to_categorical(yTest, num_classes=10)\r\n\r\nmodel = Sequential()\r\nmodel.add(Flatten(input_shape=(28, 28)))\r\nmodel.add(Dense(100, activation='relu'))\r\nmodel.add(Dense(10, activation='softmax'))\r\nmodel.summary()\r\nmodel.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\r\nmodel.fit(xTrain, yTrain, epochs=4, verbose=1)\r\n\r\nAlso tf.executing_eagerly is True\r\ni don't know why this error is coming\r\n", "comments": ["Hi @devspartan I suspect this is because you are calling numpy() within model.fit, which uses @tf.function under the hood automatically. If you add the argument run_eagerly=True to your model.compile, you should no longer get the error. However, then your model will train very slowly. \r\n\r\nYou might want to investigate [adding a callback](https://www.tensorflow.org/guide/keras/custom_callback) for a more scalable solution.", "Thanks. For sure it did work. But why is the training so slow?", "The training is slow because you are no longer leveraging @tf.function, and instead training eagerly. You could try using a callback or implementing a custom metric, instead of printing directly within the loss function.", "Thanks for explanation and quick response."]}, {"number": 40507, "title": "Conversion error when trying to convert model using BeamSearchDecoder from tensorflow-addons [RNN]", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC OS 10.15.5\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.2\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS\r\n    ]\r\ntflite_quantized_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-06-16 12:24:25.843841: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-06-16 12:24:25.843958: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-16 12:24:25.913996: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-06-16 12:24:25.914107: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 460 nodes (66), 546 edges (111), time = 11.364ms.\r\n2020-06-16 12:24:25.914119: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 460 nodes (0), 546 edges (0), time = 9.66ms.\r\n2020-06-16 12:24:25.914126: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: while_body_5784\r\n2020-06-16 12:24:25.914133: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2020-06-16 12:24:25.914140: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-16 12:24:25.914151: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: TensorArrayV2Write_1_cond_true_6956\r\n2020-06-16 12:24:25.914160: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-16 12:24:25.914166: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-16 12:24:25.914171: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: TensorArrayV2Write_2_cond_true_6974\r\n2020-06-16 12:24:25.914177: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-16 12:24:25.914183: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-16 12:24:25.914193: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: while_cond_5783\r\n2020-06-16 12:24:25.914199: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-16 12:24:25.914205: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-16 12:24:25.914210: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: TensorArrayV2Write_cond_true_6938\r\n2020-06-16 12:24:25.914215: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-16 12:24:25.914228: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-16 12:24:25.914235: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: model_predictive_typing_addons_beam_search_decoder_decoder_while_body_6435\r\n2020-06-16 12:24:25.914240: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 521 nodes (0), 597 edges (0), time = 4.097ms.\r\n2020-06-16 12:24:25.914244: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 521 nodes (0), 597 edges (0), time = 4.171ms.\r\n2020-06-16 12:24:25.914248: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: BeamSearchDecoderStep_cond_true_6915\r\n2020-06-16 12:24:25.914253: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-16 12:24:25.914258: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-16 12:24:25.914264: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: BeamSearchDecoderStep_cond_false_6916\r\n2020-06-16 12:24:25.914270: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-16 12:24:25.914276: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-16 12:24:25.914279: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: TensorArrayV2Write_2_cond_false_6975\r\n2020-06-16 12:24:25.914283: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-16 12:24:25.914289: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-16 12:24:25.914294: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: TensorArrayV2Write_1_cond_false_6957\r\n2020-06-16 12:24:25.914298: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-16 12:24:25.914302: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-16 12:24:25.914307: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: model_predictive_typing_addons_beam_search_decoder_decoder_while_cond_6434\r\n2020-06-16 12:24:25.914313: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-16 12:24:25.914319: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-16 12:24:25.914323: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: TensorArrayV2Write_cond_false_6939\r\n2020-06-16 12:24:25.914328: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-16 12:24:25.914333: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\nTraceback (most recent call last):\r\n  File \"lite_conversion.py\", line 60, in <module>\r\n    main()\r\n  File \"lite_conversion.py\", line 53, in main\r\n    tflite_quantized_model = converter.convert()\r\n  File \"/home/gc/miniconda3/envs/grammatica_tf2-gpu/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 459, in convert\r\n    self._funcs[0], lower_control_flow=False))\r\n  File \"/home/gc/miniconda3/envs/grammatica_tf2-gpu/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 706, in convert_variables_to_constants_v2_as_graph\r\n    func, lower_control_flow, aggressive_inlining)\r\n  File \"/home/gc/miniconda3/envs/grammatica_tf2-gpu/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 461, in _convert_variables_to_constants_v2_impl\r\n    node_defs, tensor_data, name_to_node)\r\n  File \"/home/gc/miniconda3/envs/grammatica_tf2-gpu/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 286, in _get_control_flow_function_data\r\n    arg_types[idx] = get_resource_type(node.input[idx + 1])\r\n  File \"/home/gc/miniconda3/envs/grammatica_tf2-gpu/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 259, in get_resource_type\r\n    node_name = get_source_node_name_through_identities(node_name)\r\n  File \"/home/gc/miniconda3/envs/grammatica_tf2-gpu/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 254, in get_source_node_name_through_identities\r\n    while name_to_node[node_name].op == \"Identity\":\r\nKeyError: 'beamsearchdecoderstep_cond_input_1_0'\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nI am trying to convert a Seq2Seq model to TF-Lite, however I am facing some issues with the BeamSearchDecoder from tensorflow-addons. My model works fine in Python and the source code looks like this: \r\n\r\n```\r\nclass MySeq2SeqModel(tf.keras.models.Model):\r\n    def __init__(self, vocab_size: int, input_len: int, output_len: int,\r\n                 batch_size,\r\n                 rnn_units: int = 64, dense_units: int = 64, embedding_dim: int = 256, **kwargs):\r\n        super(MySeq2SeqModel, self).__init__(**kwargs)\r\n\r\n        # Base Attributes\r\n        self.vocab_size = vocab_size\r\n        self.input_len = input_len\r\n        self.output_len = output_len\r\n        self.rnn_units = rnn_units\r\n        self.dense_units = dense_units\r\n        self.embedding_dim = embedding_dim\r\n        self.batch_size = batch_size\r\n\r\n        # Beam search attributes\r\n        self.beam_width = 3\r\n\r\n        # Encoder\r\n        self.encoder_embedding = layers.Embedding(vocab_size, embedding_dim, input_length=input_len)\r\n        self.encoder_rnn = layers.LSTM(rnn_units, return_sequences=True, return_state=True)\r\n\r\n        # Decoder\r\n        self.decoder_embedding = layers.Embedding(vocab_size, embedding_dim, input_length=output_len)\r\n        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\r\n\r\n        # Attention\r\n        self.attention_mechanism = tfa.seq2seq.LuongAttention(dense_units)\r\n        self.rnn_cell = self.build_rnn_cell(batch_size=batch_size)\r\n\r\n        # Output\r\n        self.dense_layer = tf.keras.layers.Dense(vocab_size)\r\n\r\n        self.inference_decoder = BeamSearchDecoder(cell=self.rnn_cell,\r\n                                                   beam_width=self.beam_width,\r\n                                                   output_layer=self.dense_layer,\r\n                                                   # As tf.nn.embedding_lookup is not supported by tflite\r\n                                                   embedding_fn=lambda ids: tf.gather(tf.identity(\r\n                                                       self.decoder_embedding.variables[0]), ids),\r\n                                                   coverage_penalty_weight=0.0, dynamic=False, parallel_iterations=1,\r\n                                                   maximum_iterations=output_len\r\n                                                   )\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        # Encoder\r\n        encoder = self.encoder_embedding(inputs[0])\r\n        encoder_outputs, state_h, state_c = self.encoder_rnn(encoder)\r\n        decoder_emb = self.decoder_embedding(inputs[1])\r\n\r\n        tiled_a = tfa.seq2seq.tile_batch(encoder_outputs, multiplier=self.beam_width)\r\n        tiled_a_tx = tfa.seq2seq.tile_batch(state_h, multiplier=self.beam_width)\r\n        tiled_c_tx = tfa.seq2seq.tile_batch(state_c, multiplier=self.beam_width)\r\n        start_tokens = tf.fill([1], START_ID)\r\n\r\n        self.attention_mechanism.setup_memory(tiled_a)\r\n\r\n        final_output, final_state, _ = self.inference_decoder(embedding=None,\r\n                                                              start_tokens=start_tokens,\r\n                                                              end_token=EOS_ID,\r\n                                                              initial_state=self.build_decoder_initial_state(\r\n                                                                  size=1 * self.beam_width,\r\n                                                                  encoder_state=[tiled_a_tx, tiled_c_tx],\r\n                                                                  Dtype=tf.float32))\r\n\r\n        return final_output.predicted_ids\r\n\r\n```\r\n", "comments": ["@gcuder could you create a TF model with control flow v2 ops? https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2", "@gcuder \r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Unfortunately that did not work. I believe the function `dynamic_decode` in [beam_search_decoder.py](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/beam_search_decoder.py) is the reason for this issue. ", "Sorry. There are no other ways to workaround the problems if you are using control flow ops.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40507\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40507\">No</a>\n"]}, {"number": 40506, "title": " from tensorflow.python.framework import versions   File \"C:\\Users\\Akanksha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\versions.py\", line 24, in <module>__version__ = pywrap_tensorflow.__version__ AttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute '__version__'", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@Akanksha521 \r\n\r\nRequest you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40505, "title": "Want to contribute but build takes time", "body": "I am new to TensorFlow's contribution community. I want to add some features and write test some scripts for it, but as it is taking so much time to build. I want to import TensorFlow from the source without re-building every time. \r\nPlease let me know if it's possible or if there is another way of doing that.", "comments": ["After the first compile the others should be cached and don't take that much of time. Unless you sync to master again, an action which would likely invalidate the cache again.\r\n\r\nUnfortunately, we don't have too much engineering time dedicated to streamlining the build. There is some effort in this line but we can benefit from open source community contributions too.", "@ratansingh98,\r\nIf you want to install TensorFlow from pre-built packages, please check this [guide](https://www.tensorflow.org/install/pip).\r\n\r\nOr if you have built TensorFlow from source, the `whl` file is stored in the `/tmp/tensorflow_pkg/` directory. You can use this `whl` file to install TensorFlow every time instead of re-building from source.\r\n\r\nPlease check [this](https://www.tensorflow.org/install/source#build_the_package) installation guide for more information. Thanks!", "> After the first compile the others should be cached and don't take that much of time. Unless you sync to master again, an action which would likely invalidate the cache again.\r\n> \r\n> Unfortunately, we don't have too much engineering time dedicated to streamlining the build. There is some effort in this line but we can benefit from open source community contributions too.\r\n\r\nThanks for the info. As for now, the cache can be very useful for me. If possible, I will look into streamlining the build.", "> @ratansingh98,\r\n> If you want to install TensorFlow from pre-built packages, please check this [guide](https://www.tensorflow.org/install/pip).\r\n> \r\n> Or if you have built TensorFlow from source, the `whl` file is stored in the `/tmp/tensorflow_pkg/` directory. You can use this `whl` file to install TensorFlow every time instead of re-building from source.\r\n> \r\n> Please check [this](https://www.tensorflow.org/install/source#build_the_package) installation guide for more information. Thanks!\r\n\r\nThanks @amahendrakar , but that's not exactly was my question. I wanted to reduce that build time or a handy way to import TensorFlow. In my case, I wanted to generate a new **whl** file every time I build TensorFlow from my modified source.\r\nAs @mihaimaruseac told, the building is not streamlined enough. I have to rely on the cache.\r\nThanks for your guidance though.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40505\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40505\">No</a>\n", "@mihaimaruseac Hey, i was actually having the same doubt i built tf from source the day before yesterday and today after pulling the new changes from tf repo i ran bazel build again, it is still taking a lot of time. How much time do you think is reasonable after the initial build? And how are other contributors updating the build everytime new commits are merged? Thanks", "It depends on your operating system and hardware. You should watch memory consumption during build to make sure you are not using swap, since that would definitely slow down builds. There are bazel flags to limit how much RAM should be used by the compilation process and how many processes to run in parallel. These should help with contention issues.\r\n\r\nThen, you should make sure you use the bazel cache between builds so that you don't always rebuild from scratch.\r\n\r\nFinally, you don't need to always build everything. The way I do it is first I build a pip package from scratch, identify the issue, identify a smaller test showcasing the issue or writing one if it does not exist. Then, as I work on fixing the issue, I only run that test and only when done do I build the entire package again", "@mihaimaruseac Thanks for the reply.\r\n\r\n> Then, you should make sure you use the bazel cache between builds so that you don't always rebuild from scratch.\r\n\r\nI didn't quite understand what you meant by using the bazel cache as I'm fairly new to bazel and open source, this being my first project.\r\n\r\n> The way I do it is first I build a pip package from scratch, identify the issue, identify a smaller test showcasing the issue or writing one if it does not exist. Then, as I work on fixing the issue, I only run that test and only when done do I build the entire package again\r\n\r\nIt would be great if you could provide a little insight. ", "https://docs.bazel.build/versions/master/remote-caching.html", "You can follow our effort to optimize and continuously monitoring the contributor experience with our official docker developer image at https://github.com/tensorflow/tensorflow/pull/48421 /cc @theadactyl "]}, {"number": 40504, "title": "RuntimeError: tensorflow/lite/kernels/range.cc:39 (start > limit && delta < 0) || (start < limit && delta > 0) was not true.Node number 3 (RANGE) failed   to invoke. Node number 393 (WHILE) failed to invoke. current error :RuntimeError: tensorflow/lite/kernels/reshape.cc:55 stretch_dim != -1 (0 != -1)Node number 83 (RESHAPE) failed to prepare.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow_tts.processor import LJSpeechProcessor\r\n\r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"fastspeech.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nprint(\"input_details: \", input_details)\r\nprint(\"output_details\", output_details)\r\n\r\nprint(input_details[0])\r\nprint(input_details[1])\r\nprint(input_details[2])\r\n# fastspeech inference\r\nattention_mask = interpreter.tensor(interpreter.get_input_details()[0][\"index\"])()\r\nspeaker_id = interpreter.tensor(interpreter.get_input_details()[1][\"index\"])()\r\ninput_id = interpreter.tensor(interpreter.get_input_details()[2][\"index\"])()\r\n\r\ninput_id = tf.convert_to_tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], tf.int32)\r\nattention_mask = tf.convert_to_tensor([[True, True, True, True, True, True, True, True, True, True]], tf.bool)\r\nspeaker_id = tf.convert_to_tensor([0], tf.int32)\r\n\r\n#out_p = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])\r\ninterpreter.invoke()\r\ninterpreter.invoke()\r\ninterpreter.invoke()\r\nprint(\"done\")\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\nmasked_mel_before = interpreter.get_tensor(output_details[2]['index'])\r\nprint(masked_mel_before)\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\ninput_details:  [{'name': 'attention_mask', 'index': 0, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.bool_'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'input_ids', 'index': 1, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'speaker_ids', 'index': 2, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\noutput_details [{'name': 'Identity', 'index': 585, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_1', 'index': 621, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_2', 'index': 537, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n{'name': 'attention_mask', 'index': 0, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.bool_'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n{'name': 'input_ids', 'index': 1, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n{'name': 'speaker_ids', 'index': 2, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n2020-06-16 06:20:21.460754: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-06-16 06:20:21.460788: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-06-16 06:20:21.460819: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\r\n2020-06-16 06:20:21.461131: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-16 06:20:21.468935: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2593990000 Hz\r\n2020-06-16 06:20:21.469795: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0cfc000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-16 06:20:21.469821: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"test_tflite.py\", line 28, in <module>\r\n    interpreter.invoke()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py\", line 511, in invoke\r\n    self._interpreter.Invoke()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 113, in Invoke\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)\r\nRuntimeError: tensorflow/lite/kernels/range.cc:39 (start > limit && delta < 0) || (start < limit && delta > 0) was not true.Node number 3 (RANGE) failed to invoke.\r\nNode number 393 (WHILE) failed to invoke.\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nsaved_model\r\nhttps://drive.google.com/file/d/136KmfVwBT2htxPDZeYw4-TXmxPYe7Vsa/view?usp=sharing\r\nfastspeech.tflite\r\nhttps://drive.google.com/file/d/1QYyc5cUZbmbv7SwQMDTdM622ZiCFG2cp/view?usp=sharing\r\n```\r\n\r\n**Failure details**\r\nconversion is successful, but there is runtime error,\r\nstate what is wrong:\r\ninterpreter.invoke() failing\r\n\r\n**Any other info / logs**\r\npb conversion\r\n```\r\nimport yaml\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport yaml\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow_tts.configs import FastSpeechConfig\r\nfrom tensorflow_tts.models import TFFastSpeech\r\n\r\nwith open('examples/fastspeech/conf/fastspeech.v3.yaml') as f:\r\n    config = yaml.load(f, Loader=yaml.Loader)\r\n\r\nconfig = FastSpeechConfig(**config[\"fastspeech_params\"])\r\n\r\nfastspeech = TFFastSpeech(config=config, name=\"fastspeech\")\r\nfastspeech._build()\r\n\r\nfastspeech.load_weights(\"examples/fastspeech/pretrained/model-150000.h5\",by_name=True, skip_mismatch=True)\r\ntf.saved_model.save(fastspeech, \"./test_saved\")\r\n```\r\nfastspeech model code\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef get_initializer(initializer_range=0.02):\r\n    \"\"\"Creates a `tf.initializers.truncated_normal` with the given range.\r\n\r\n    Args:\r\n        initializer_range: float, initializer range for stddev.\r\n\r\n    Returns:\r\n        TruncatedNormal initializer with stddev = `initializer_range`.\r\n\r\n    \"\"\"\r\n    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)\r\n\r\n\r\ndef gelu(x):\r\n    \"\"\"Gaussian Error Linear unit.\"\"\"\r\n    cdf = 0.5 * (1.0 + tf.math.erf(x / tf.math.sqrt(2.0)))\r\n    return x * cdf\r\n\r\n\r\ndef gelu_new(x):\r\n    \"\"\"Smoother gaussian Error Linear Unit.\"\"\"\r\n    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\r\n    return x * cdf\r\n\r\n\r\ndef swish(x):\r\n    \"\"\"Swish activation function.\"\"\"\r\n    return x * tf.sigmoid(x)\r\n\r\n\r\ndef mish(x):\r\n    return x * tf.math.tanh(tf.math.softplus(x))\r\n\r\n\r\nACT2FN = {\r\n    \"identity\": tf.keras.layers.Activation('linear'),\r\n    \"tanh\": tf.keras.layers.Activation('tanh'),\r\n    \"gelu\": tf.keras.layers.Activation(gelu),\r\n    \"relu\": tf.keras.activations.relu,\r\n    \"swish\": tf.keras.layers.Activation(swish),\r\n    \"gelu_new\": tf.keras.layers.Activation(gelu_new),\r\n    \"mish\": tf.keras.layers.Activation(mish)\r\n}\r\n\r\n\r\nclass TFFastSpeechEmbeddings(tf.keras.layers.Layer):\r\n    \"\"\"Construct charactor/phoneme/positional/speaker embeddings.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init variables.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.vocab_size = config.vocab_size\r\n        self.hidden_size = config.hidden_size\r\n        self.initializer_range = config.initializer_range\r\n        self.config = config\r\n\r\n        self.position_embeddings = tf.keras.layers.Embedding(\r\n            config.max_position_embeddings + 1,\r\n            config.hidden_size,\r\n            weights=[self._sincos_embedding()],\r\n            name=\"position_embeddings\",\r\n            trainable=False,\r\n        )\r\n\r\n        if config.n_speakers > 1:\r\n            self.encoder_speaker_embeddings = tf.keras.layers.Embedding(\r\n                config.n_speakers,\r\n                config.hidden_size,\r\n                embeddings_initializer=get_initializer(self.initializer_range),\r\n                name=\"speaker_embeddings\"\r\n            )\r\n            self.speaker_fc = tf.keras.layers.Dense(units=config.hidden_size, name='speaker_fc') \r\n    def build(self, input_shape):\r\n        \"\"\"Build shared charactor/phoneme embedding layers.\"\"\"\r\n        with tf.name_scope(\"charactor_embeddings\"):\r\n            self.charactor_embeddings = self.add_weight(\r\n                \"weight\",\r\n                shape=[self.vocab_size, self.hidden_size],\r\n                initializer=get_initializer(self.initializer_range),\r\n            )\r\n        super().build(input_shape)\r\n    #@tf.function(experimental_relax_shapes=True)\r\n    def call(self, inputs, training=False):\r\n        \"\"\"Get charactor embeddings of inputs.\r\n\r\n        Args:\r\n            1. charactor, Tensor (int32) shape [batch_size, length].\r\n            2. speaker_id, Tensor (int32) shape [batch_size]\r\n        Returns:\r\n            Tensor (float32) shape [batch_size, length, embedding_size].\r\n\r\n        \"\"\"\r\n        return self._embedding(inputs, training=training)\r\n    def _embedding(self, inputs, training=False):\r\n        \"\"\"Applies embedding based on inputs tensor.\"\"\"\r\n        input_ids, speaker_ids = inputs\r\n\r\n        input_shape = tf.shape(input_ids)\r\n        seq_length = input_shape[1]\r\n\r\n        position_ids = tf.range(1, seq_length + 1, dtype=tf.int32)[tf.newaxis, :]\r\n\r\n        # create embeddings\r\n        inputs_embeds = tf.gather(self.charactor_embeddings, input_ids)\r\n        position_embeddings = self.position_embeddings(position_ids)\r\n\r\n        # sum embedding\r\n        embeddings = inputs_embeds + position_embeddings\r\n        if self.config.n_speakers > 1:\r\n            speaker_embeddings = self.encoder_speaker_embeddings(speaker_ids)\r\n            speaker_features = tf.math.softplus(self.speaker_fc(speaker_embeddings))\r\n            # extended speaker embeddings\r\n            extended_speaker_features = speaker_features[:, tf.newaxis, :]\r\n            embeddings += extended_speaker_features\r\n\r\n        return embeddings\r\n    def _sincos_embedding(self):\r\n        position_enc = np.array([\r\n            [pos / np.power(10000, 2.0 * (i // 2) / self.hidden_size) for i in range(self.hidden_size)]\r\n            for pos in range(self.config.max_position_embeddings + 1)\r\n        ])\r\n\r\n        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])\r\n        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])\r\n\r\n        # pad embedding.\r\n        position_enc[0] = 0.0\r\n\r\n        return position_enc\r\n\r\n\r\nclass TFFastSpeechSelfAttention(tf.keras.layers.Layer):\r\n    \"\"\"Self attention module for fastspeech.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init variables.\"\"\"\r\n        super().__init__(**kwargs)\r\n        if config.hidden_size % config.num_attention_heads != 0:\r\n            raise ValueError(\r\n                \"The hidden size (%d) is not a multiple of the number of attention \"\r\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\r\n            )\r\n        self.output_attentions = config.output_attentions\r\n        self.num_attention_heads = config.num_attention_heads\r\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\r\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\r\n\r\n        self.query = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\r\n        )\r\n        self.key = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\r\n        )\r\n        self.value = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\r\n        )\r\n\r\n        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\r\n\r\n    def transpose_for_scores(self, x, batch_size):\r\n        \"\"\"Transpose to calculate attention scores.\"\"\"\r\n        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\r\n        return tf.transpose(x, perm=[0, 2, 1, 3])\r\n\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def __call__(self, inputs, training=False):\r\n        \"\"\"Call logic.\"\"\"\r\n        hidden_states, attention_mask = inputs\r\n\r\n        batch_size = tf.shape(hidden_states)[0]\r\n        mixed_query_layer = self.query(hidden_states)\r\n        mixed_key_layer = self.key(hidden_states)\r\n        mixed_value_layer = self.value(hidden_states)\r\n\r\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\r\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\r\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\r\n\r\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\r\n        dk = tf.cast(tf.shape(key_layer)[-1], tf.float32)  # scale attention_scores\r\n        attention_scores = attention_scores / tf.math.sqrt(dk)\r\n\r\n        if attention_mask is not None:\r\n            # extended_attention_masks for self attention encoder.\r\n            extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\r\n            extended_attention_mask = tf.cast(extended_attention_mask, tf.float32)\r\n            extended_attention_mask = (1.0 - extended_attention_mask) * -1e9\r\n            attention_scores = attention_scores + extended_attention_mask\r\n\r\n        # Normalize the attention scores to probabilities.\r\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\r\n        attention_probs = self.dropout(attention_probs, training=training)\r\n\r\n        context_layer = tf.matmul(attention_probs, value_layer)\r\n        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\r\n        context_layer = tf.reshape(\r\n            context_layer, (batch_size, -1, self.all_head_size)\r\n        )\r\n\r\n        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\r\n        return outputs\r\n\r\n\r\nclass TFFastSpeechSelfOutput(tf.keras.layers.Layer):\r\n    \"\"\"Fastspeech output of self attention module.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init variables.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\r\n        )\r\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\r\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def __call__(self, inputs, training=False):\r\n        \"\"\"Call logic.\"\"\"\r\n        hidden_states, input_tensor = inputs\r\n\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states, training=training)\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\n\r\nclass TFFastSpeechAttention(tf.keras.layers.Layer):\r\n    \"\"\"Fastspeech attention module.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init variables.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.self_attention = TFFastSpeechSelfAttention(config, name=\"self\")\r\n        self.dense_output = TFFastSpeechSelfOutput(config, name=\"output\")\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def __call__(self, inputs, training=False):\r\n        input_tensor, attention_mask = inputs\r\n\r\n        self_outputs = self.self_attention([input_tensor, attention_mask], training=training)\r\n        attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)\r\n        masked_attention_output = attention_output * tf.cast(tf.expand_dims(attention_mask, 2), dtype=tf.float32)\r\n        outputs = (masked_attention_output,) + self_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\n\r\nclass TFFastSpeechIntermediate(tf.keras.layers.Layer):\r\n    \"\"\"Intermediate representation module.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init variables.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.conv1d_1 = tf.keras.layers.Conv1D(\r\n            config.intermediate_size,\r\n            kernel_size=config.intermediate_kernel_size,\r\n            kernel_initializer=get_initializer(config.initializer_range),\r\n            padding='same',\r\n            name=\"conv1d_1\"\r\n        )\r\n        self.conv1d_2 = tf.keras.layers.Conv1D(\r\n            config.hidden_size,\r\n            kernel_size=config.intermediate_kernel_size,\r\n            kernel_initializer=get_initializer(config.initializer_range),\r\n            padding='same',\r\n            name=\"conv1d_2\"\r\n        )\r\n        if isinstance(config.hidden_act, str):\r\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\r\n        else:\r\n            self.intermediate_act_fn = config.hidden_act\r\n\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def __call__(self, inputs, training=False):\r\n        \"\"\"Call logic.\"\"\"\r\n        hidden_states, attention_mask = inputs\r\n\r\n        hidden_states = self.conv1d_1(hidden_states)\r\n        hidden_states = self.intermediate_act_fn(hidden_states)\r\n        hidden_states = self.conv1d_2(hidden_states)\r\n\r\n        masked_hidden_states = hidden_states * tf.cast(tf.expand_dims(attention_mask, 2), dtype=tf.float32)\r\n        return masked_hidden_states\r\n\r\n\r\nclass TFFastSpeechOutput(tf.keras.layers.Layer):\r\n    \"\"\"Output module.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init variables.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\r\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def __call__(self, inputs, training=False):\r\n        \"\"\"Call logic.\"\"\"\r\n        hidden_states, input_tensor = inputs\r\n\r\n        hidden_states = self.dropout(hidden_states, training=training)\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\n\r\nclass TFFastSpeechLayer(tf.keras.layers.Layer):\r\n    \"\"\"Fastspeech module (FFT module on the paper).\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init variables.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.attention = TFFastSpeechAttention(config, name=\"attention\")\r\n        self.intermediate = TFFastSpeechIntermediate(config, name=\"intermediate\")\r\n        self.bert_output = TFFastSpeechOutput(config, name=\"output\")\r\n\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def __call__(self, inputs, training=False):\r\n        \"\"\"Call logic.\"\"\"\r\n        hidden_states, attention_mask = inputs\r\n\r\n        attention_outputs = self.attention([hidden_states, attention_mask], training=training)\r\n        attention_output = attention_outputs[0]\r\n        intermediate_output = self.intermediate([attention_output, attention_mask], training=training)\r\n        layer_output = self.bert_output([intermediate_output, attention_output], training=training)\r\n        masked_layer_output = layer_output * tf.cast(tf.expand_dims(attention_mask, 2), dtype=tf.float32)\r\n        outputs = (masked_layer_output,) + attention_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\n\r\nclass TFFastSpeechEncoder(tf.keras.layers.Layer):\r\n    \"\"\"Fast Speech encoder module.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init variables.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.output_attentions = config.output_attentions\r\n        self.output_hidden_states = config.output_hidden_states\r\n        self.layer = [TFFastSpeechLayer(config, name=\"layer_._{}\".format(i)) for i in range(config.num_hidden_layers)]\r\n    \r\n    @tf.function(experimental_relax_shapes=True)\r\n    def __call__(self, inputs, training=False):\r\n        \"\"\"Call logic.\"\"\"\r\n        hidden_states, attention_mask = inputs\r\n\r\n        all_hidden_states = ()\r\n        all_attentions = ()\r\n        for _, layer_module in enumerate(self.layer):\r\n            if self.output_hidden_states:\r\n                all_hidden_states = all_hidden_states + (hidden_states,)\r\n\r\n            layer_outputs = layer_module([hidden_states, attention_mask], training=training)\r\n            hidden_states = layer_outputs[0]\r\n\r\n            if self.output_attentions:\r\n                all_attentions = all_attentions + (layer_outputs[1],)\r\n\r\n        # Add last layer\r\n        if self.output_hidden_states:\r\n            all_hidden_states = all_hidden_states + (hidden_states,)\r\n\r\n        outputs = (hidden_states,)\r\n        if self.output_hidden_states:\r\n            outputs = outputs + (all_hidden_states,)\r\n        if self.output_attentions:\r\n            outputs = outputs + (all_attentions,)\r\n        return outputs  # outputs, (hidden states), (attentions)\r\n\r\n\r\nclass TFFastSpeechDecoder(TFFastSpeechEncoder):\r\n    \"\"\"Fast Speech decoder module.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(config, **kwargs)\r\n        self.config = config\r\n\r\n        # create decoder positional embedding\r\n        self.decoder_positional_embeddings = tf.keras.layers.Embedding(\r\n            config.max_position_embeddings + 1,\r\n            config.hidden_size,\r\n            weights=[self._sincos_embedding()],\r\n            name=\"position_embeddings\",\r\n            trainable=False\r\n        )\r\n\r\n        if config.n_speakers > 1:\r\n            self.decoder_speaker_embeddings = tf.keras.layers.Embedding(\r\n                config.n_speakers,\r\n                config.hidden_size,\r\n                embeddings_initializer=get_initializer(config.initializer_range),\r\n                name=\"speaker_embeddings\"\r\n            )\r\n            self.speaker_fc = tf.keras.layers.Dense(units=config.hidden_size, name='speaker_fc')\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def __call__(self, inputs, training=False):\r\n        hidden_states, speaker_ids, encoder_mask, decoder_pos = inputs\r\n\r\n        # calculate new hidden states.\r\n        hidden_states = hidden_states + self.decoder_positional_embeddings(decoder_pos)\r\n\r\n        if self.config.n_speakers > 1:\r\n            speaker_embeddings = self.decoder_speaker_embeddings(speaker_ids)\r\n            speaker_features = tf.math.softplus(self.speaker_fc(speaker_embeddings))\r\n            # extended speaker embeddings\r\n            extended_speaker_features = speaker_features[:, tf.newaxis, :]\r\n            hidden_states += extended_speaker_features\r\n\r\n        return super().call([hidden_states, encoder_mask], training=training)\r\n\r\n    def _sincos_embedding(self):\r\n        position_enc = np.array([\r\n            [pos / np.power(10000, 2.0 * (i // 2) / self.config.hidden_size) for i in range(self.config.hidden_size)]\r\n            for pos in range(self.config.max_position_embeddings + 1)\r\n        ])\r\n\r\n        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])\r\n        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])\r\n\r\n        # pad embedding.\r\n        position_enc[0] = 0.0\r\n\r\n        return position_enc\r\n\r\n\r\nclass TFTacotronPostnet(tf.keras.layers.Layer):\r\n    \"\"\"Tacotron-2 postnet.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init variables.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.conv_batch_norm = []\r\n        for i in range(config.n_conv_postnet):\r\n            conv = tf.keras.layers.Conv1D(\r\n                filters=config.postnet_conv_filters if i < config.n_conv_postnet - 1 else config.num_mels,\r\n                kernel_size=config.postnet_conv_kernel_sizes,\r\n                padding='same',\r\n                name='conv_._{}'.format(i)\r\n            )\r\n            batch_norm = tf.keras.layers.BatchNormalization(name='batch_norm_._{}'.format(i))\r\n            self.conv_batch_norm.append((conv, batch_norm))\r\n        self.dropout = tf.keras.layers.Dropout(rate=config.postnet_dropout_rate, name='dropout')\r\n        self.activation = [tf.nn.tanh] * (config.n_conv_postnet - 1) + [tf.identity]\r\n\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def __call__(self, inputs, training=False):\r\n        \"\"\"Call logic.\"\"\"\r\n        outputs, mask = inputs\r\n        extended_mask = tf.cast(tf.expand_dims(mask, axis=2), tf.float32)\r\n        for i, (conv, bn) in enumerate(self.conv_batch_norm):\r\n            outputs = conv(outputs)\r\n            outputs = bn(outputs)\r\n            outputs = self.activation[i](outputs)\r\n            outputs = self.dropout(outputs, training=training)\r\n        return outputs * extended_mask\r\n\r\n\r\nclass TFFastSpeechDurationPredictor(tf.keras.layers.Layer):\r\n    \"\"\"FastSpeech duration predictor module.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init variables.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.conv_layers = []\r\n        for i in range(config.num_duration_conv_layers):\r\n            self.conv_layers.append(\r\n                tf.keras.layers.Conv1D(\r\n                    config.duration_predictor_filters,\r\n                    config.duration_predictor_kernel_sizes,\r\n                    padding='same',\r\n                    name='conv_._{}'.format(i)\r\n                )\r\n            )\r\n            self.conv_layers.append(\r\n                tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm_._{}\".format(i))\r\n            )\r\n            self.conv_layers.append(\r\n                tf.keras.layers.Activation(tf.nn.relu6)\r\n            )\r\n            self.conv_layers.append(\r\n                tf.keras.layers.Dropout(config.duration_predictor_dropout_probs)\r\n            )\r\n        self.conv_layers_sequence = tf.keras.Sequential(self.conv_layers)\r\n        self.output_layer = tf.keras.layers.Dense(1)\r\n\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def __call__(self, inputs, training=False):\r\n        \"\"\"Call logic.\"\"\"\r\n        encoder_hidden_states, attention_mask = inputs\r\n        attention_mask = tf.cast(tf.expand_dims(attention_mask, 2), tf.float32)\r\n\r\n        # mask encoder hidden states\r\n        masked_encoder_hidden_states = encoder_hidden_states * attention_mask\r\n\r\n        # pass though first layer\r\n        outputs = self.conv_layers_sequence(masked_encoder_hidden_states)\r\n        outputs = self.output_layer(outputs)\r\n        masked_outputs = outputs * attention_mask\r\n        return tf.squeeze(tf.nn.relu6(masked_outputs), -1)  # make sure positive value.\r\n\r\n\r\nclass TFFastSpeechLengthRegulator(tf.keras.layers.Layer):\r\n    \"\"\"FastSpeech lengthregulator module.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init variables.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.config = config\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def __call__(self, inputs, training=False):\r\n        \"\"\"Call logic.\r\n\r\n        Args:\r\n            1. encoder_hidden_states, Tensor (float32) shape [batch_size, length, hidden_size]\r\n            2. durations_gt, Tensor (float32/int32) shape [batch_size, length]\r\n        \"\"\"\r\n        encoder_hidden_states, durations_gt = inputs\r\n        outputs, encoder_masks = self._length_regulator(encoder_hidden_states, durations_gt)\r\n        return outputs, encoder_masks\r\n\r\n    def _length_regulator(self, encoder_hidden_states, durations_gt):\r\n        \"\"\"Length regulator logic.\"\"\"\r\n        sum_durations = tf.reduce_sum(durations_gt, axis=-1)  # [batch_size]\r\n        max_durations = tf.reduce_max(sum_durations)\r\n\r\n        input_shape = tf.shape(encoder_hidden_states)\r\n        batch_size = input_shape[0]\r\n        hidden_size = input_shape[-1]\r\n\r\n        # initialize output hidden states and encoder masking.\r\n        outputs = tf.zeros(shape=[0, max_durations, hidden_size], dtype=tf.float32)\r\n        encoder_masks = tf.zeros(shape=[0, max_durations], dtype=tf.int32)\r\n\r\n        def condition(i,\r\n                      batch_size,\r\n                      outputs,\r\n                      encoder_masks,\r\n                      encoder_hidden_states,\r\n                      durations_gt,\r\n                      max_durations):\r\n            return tf.less(i, batch_size)\r\n\r\n        def body(i,\r\n                 batch_size,\r\n                 outputs,\r\n                 encoder_masks,\r\n                 encoder_hidden_states,\r\n                 durations_gt,\r\n                 max_durations):\r\n            repeats = durations_gt[i]\r\n            real_length = tf.reduce_sum(repeats)\r\n            pad_size = max_durations - real_length\r\n            masks = tf.sequence_mask([real_length], max_durations, dtype=tf.int32)\r\n            repeat_encoder_hidden_states = tf.repeat(\r\n                encoder_hidden_states[i],\r\n                repeats=repeats,\r\n                axis=0\r\n            )\r\n            repeat_encoder_hidden_states = tf.expand_dims(\r\n                tf.pad(\r\n                    repeat_encoder_hidden_states, [[0, pad_size], [0, 0]]\r\n                ),\r\n                0)  # [1, max_durations, hidden_size]\r\n            outputs = tf.concat([outputs, repeat_encoder_hidden_states], axis=0)\r\n            encoder_masks = tf.concat([encoder_masks, masks], axis=0)\r\n            return [i + 1, batch_size, outputs, encoder_masks,\r\n                    encoder_hidden_states, durations_gt, max_durations]\r\n\r\n        # initialize iteration i.\r\n        i = tf.constant(0, dtype=tf.int32)\r\n        _, _, outputs, encoder_masks, _, _, _, = tf.while_loop(\r\n            condition,\r\n            body,\r\n            [i, batch_size, outputs, encoder_masks, encoder_hidden_states, durations_gt, max_durations],\r\n            shape_invariants=[i.get_shape(),\r\n                              batch_size.get_shape(),\r\n                              tf.TensorShape([None, None, self.config.hidden_size]),\r\n                              tf.TensorShape([None, None]),\r\n                              encoder_hidden_states.get_shape(),\r\n                              durations_gt.get_shape(),\r\n                              max_durations.get_shape()]\r\n        )\r\n\r\n        return outputs, encoder_masks\r\n\r\n\r\nclass TFFastSpeech(tf.keras.Model):\r\n    \"\"\"TF Fastspeech module.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init layers for fastspeech.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.embeddings = TFFastSpeechEmbeddings(config, name='embeddings')\r\n        self.encoder = TFFastSpeechEncoder(config, name='encoder')\r\n        self.duration_predictor = TFFastSpeechDurationPredictor(config, name='duration_predictor')\r\n        self.length_regulator = TFFastSpeechLengthRegulator(config, name='length_regulator')\r\n        self.decoder = TFFastSpeechDecoder(config, name='decoder')\r\n        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, name='mel_before')\r\n        self.postnet = TFTacotronPostnet(config=config, name='postnet')\r\n\r\n    def _build(self):\r\n        \"\"\"Dummy input for building model.\"\"\"\r\n        # fake inputs\r\n        input_ids = tf.convert_to_tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], tf.int32)\r\n        attention_mask = tf.convert_to_tensor([[True, True, True, True, True, True, True, True, True, True]], tf.bool)\r\n        speaker_ids = tf.convert_to_tensor([0], tf.int32)\r\n        self(input_ids, attention_mask, speaker_ids)\r\n\r\n    @tf.function(experimental_relax_shapes=True,\r\n                 input_signature=[tf.TensorSpec(shape=[None, 10], dtype=tf.int32),\r\n                                  tf.TensorSpec(shape=[None, 10], dtype=tf.bool),\r\n                                  tf.TensorSpec(shape=[None, ], dtype=tf.int32)])\r\n    def __call__(self,\r\n             input_ids,\r\n             attention_mask,\r\n             speaker_ids,\r\n             training=False):\r\n        \"\"\"Call logic.\"\"\"\r\n        embedding_output = self.embeddings([input_ids, speaker_ids], training=training)\r\n        encoder_output = self.encoder([embedding_output, attention_mask], training=training)\r\n        last_encoder_hidden_states = encoder_output[0]\r\n\r\n        # duration predictor, here use last_encoder_hidden_states, u can use more hidden_states layers\r\n        # rather than just use last_hidden_states of encoder for duration_predictor.\r\n        duration_outputs = self.duration_predictor([last_encoder_hidden_states, attention_mask])  # [batch_size, length]\r\n        speed_ratios = tf.convert_to_tensor(np.array([1.0]), dtype=tf.float32)\r\n\r\n        duration_gts = tf.cast(tf.math.round(duration_outputs), tf.int32)\r\n\r\n        length_regulator_outputs, encoder_masks = self.length_regulator([\r\n            last_encoder_hidden_states, duration_gts], training=training)\r\n\r\n        # create decoder positional embedding\r\n        decoder_pos = tf.range(1, tf.shape(length_regulator_outputs)[1] + 1, dtype=tf.int32)\r\n        masked_decoder_pos = tf.expand_dims(decoder_pos, 0) * encoder_masks\r\n\r\n        decoder_output = self.decoder(\r\n            [length_regulator_outputs, speaker_ids, encoder_masks, masked_decoder_pos], training=training)\r\n        last_decoder_hidden_states = decoder_output[0]\r\n\r\n        # here u can use sum or concat more than 1 hidden states layers from decoder.\r\n        mel_before = self.mel_dense(last_decoder_hidden_states)\r\n        mel_after = self.postnet([mel_before, encoder_masks], training=training) + mel_before\r\n        outputs = (mel_before, mel_after, duration_outputs)\r\n        #model10 = keras.models.Model(inputs=[input_ids,attention_mask,speaker_ids], output=outputs)\r\n        return outputs\r\n\r\n```", "comments": ["@manmay-nakhashi could you confirm that when you feed the exact same input data to the TF graph, it generates a correct result?", "@manmay-nakhashi I can reproduce your errors. However, the model is kind of complicated one. I think the problematic portion is `TFFastSpeechLengthRegulator`. Could you help creating a small tflite that reproduces the same error and only contains the above keras layer? That \r\n will make us easier to debug.", "@abattery i am able to load and generate correct result in pb file but i have to modify a code little bit, if with same code i am trying to convert it to tflite then i am getting this error\r\n```\r\ntraceback (most recent call last):\r\n  File \"load_model.py\", line 12, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 483, in convert\r\n    _get_tensor_name(tensor), shape_list))\r\nValueError: None is only supported in the 1st dimension. Tensor 'attention_mask' has invalid shape '[None, None]'.\r\n```\r\nif i am giving fixed length dimensions  while creating model , in signature then i am getting this error \r\nwhich is from TFFastSpeechEmbeddings layer\r\n```\r\nTraceback (most recent call last):\r\n  File \"load_model.py\", line 11, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 518, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py\", line 496, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py\", line 227, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-06-16 15:46:16.544808: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.\r\n2020-06-16 15:46:16.544861: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: Failed to find function '__inference___call___372'. The imported TensorFlow GraphDef is ill-formed.\r\n```\r\nso i commented @tf.functions in embedding and i am getting above error ", "> @manmay-nakhashi I can reproduce your errors. However, the model is kind of complicated one. I think the problematic portion is `TFFastSpeechLengthRegulator`. Could you help creating a small tflite that reproduces the same error and only contains the above keras layer? That\r\n> will make us easier to debug.\r\n\r\ni'll do that", "@manmay-nakhashi Thanks for your support :)", "@abattery   getting this error while converting from TFFastSpeechLengthRegulator  as you suspected  it is able to convert in tf.while_cond and and while_body after that it is crashing\r\n```2020-06-18 12:36:46.320909: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-06-18 12:36:46.320955: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 1031 nodes (-666), 1893 edges (-909), time = 731.028ms.\r\n2020-06-18 12:36:46.320968: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 1031 nodes (0), 1893 edges (0), time = 60.799ms.\r\n2020-06-18 12:36:46.320978: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: __inference_while_body_3091_3082_frozen\r\n2020-06-18 12:36:46.320988: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 117 nodes (5), 129 edges (12), time = 3.444ms.\r\n2020-06-18 12:36:46.320997: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 117 nodes (0), 129 edges (0), time = 1.798ms.\r\n2020-06-18 12:36:46.321006: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: __inference_while_cond_3090_2763_frozen\r\n2020-06-18 12:36:46.321016: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 12 nodes (0), 4 edges (0), time = 0.241ms.\r\n2020-06-18 12:36:46.321025: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 12 nodes (0), 4 edges (0), time = 0.138ms.\r\nTraceback (most recent call last):\r\n  File \"load_model.py\", line 11, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 518, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py\", line 496, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py\", line 227, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-06-18 12:36:48.585792: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.\r\n2020-06-18 12:36:48.585849: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: Failed to find function '__inference___call___8538'. The imported TensorFlow GraphDef is ill-formed\r\n```", "@manmay-nakhashi Can you share the converted tflite that contains only TFFastSpeechLengthRegulator?", "If the conversion is halted, can you share a minimal reproducible step?", "> @manmay-nakhashi Can you share the converted tflite that contains only TFFastSpeechLengthRegulator?\r\n\r\nok", "@abattery  length_regulator.py , i have changes output to return single element just to make it simple, i am not able to build model with input signature because of tf.while_loop \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow_tts.configs import FastSpeechConfig\r\nimport yaml\r\nclass TFFastSpeechLengthRegulator(tf.keras.layers.Layer):\r\n    \"\"\"FastSpeech lengthregulator module.\"\"\"\r\n\r\n    def __init__(self, config, **kwargs):\r\n        \"\"\"Init variables.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.config = config\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def __call__(self, inputs, training=False):\r\n        \"\"\"Call logic.\r\n        Args:\r\n            1. encoder_hidden_states, Tensor (float32) shape [batch_size, length, hidden_size]\r\n            2. durations_gt, Tensor (float32/int32) shape [batch_size, length]\r\n        \"\"\"\r\n        encoder_hidden_states, durations_gt = inputs\r\n        \"\"\"Length regulator logic.\"\"\"\r\n        sum_durations = tf.reduce_sum(durations_gt, axis=-1)  # [batch_size]\r\n        max_durations = tf.reduce_max(sum_durations)\r\n\r\n        input_shape = tf.shape(encoder_hidden_states)\r\n        batch_size = input_shape[0]\r\n        hidden_size = input_shape[-1]\r\n\r\n        # initialize output hidden states and encoder masking.\r\n        outputs = tf.zeros(shape=[0, max_durations, hidden_size], dtype=tf.float32)\r\n        encoder_masks = tf.zeros(shape=[0, max_durations], dtype=tf.int32)\r\n\r\n        def condition(i,\r\n                      batch_size,\r\n                      outputs,\r\n                      encoder_masks,\r\n                      encoder_hidden_states,\r\n                      durations_gt,\r\n                      max_durations):\r\n            return tf.less(i, batch_size)\r\n\r\n        def body(i,\r\n                 batch_size,\r\n                 outputs,\r\n                 encoder_masks,\r\n                 encoder_hidden_states,\r\n                 durations_gt,\r\n                 max_durations):\r\n            repeats = durations_gt[i]\r\n            real_length = tf.reduce_sum(repeats)\r\n            pad_size = max_durations - real_length\r\n            masks = tf.sequence_mask([real_length], max_durations, dtype=tf.int32)\r\n            repeat_encoder_hidden_states = tf.repeat(\r\n                encoder_hidden_states[i],\r\n                repeats=repeats,\r\n                axis=0\r\n            )\r\n            repeat_encoder_hidden_states = tf.expand_dims(\r\n                tf.pad(\r\n                    repeat_encoder_hidden_states, [[0, pad_size], [0, 0]]\r\n                ),\r\n                0)  # [1, max_durations, hidden_size]\r\n            outputs = tf.concat([outputs, repeat_encoder_hidden_states], axis=0)\r\n            encoder_masks = tf.concat([encoder_masks, masks], axis=0)\r\n            return [i + 1, batch_size, outputs, encoder_masks,\r\n                    encoder_hidden_states, durations_gt, max_durations]\r\n\r\n        # initialize iteration i.\r\n        i = tf.constant(0, dtype=tf.int32)\r\n        _, _, outputs, encoder_masks, _, _, _, = tf.while_loop(\r\n            condition,\r\n            body,\r\n            [i, batch_size, outputs, encoder_masks, encoder_hidden_states, durations_gt, max_durations],\r\n            shape_invariants=[i.get_shape(),\r\n                              batch_size.get_shape(),\r\n                              tf.TensorShape([None, None, self.config.hidden_size]),\r\n                              tf.TensorShape([None, None]),\r\n                              encoder_hidden_states.get_shape(),\r\n                              durations_gt.get_shape(),\r\n                              max_durations.get_shape()]\r\n        )\r\n\r\n        return encoder_masks\r\n\r\n#last_encoder_hidden_states = np.array(np.random.random_sample((16,10,384)), dtype=np.float32) #tf.keras.Input((None,10,64))\r\n#duration_gts = np.array(np.random.random_sample((16,10)), dtype=np.int32) #tf.keras.Input((None,10), dtype=tf.int32)\r\nwith open('examples/fastspeech/conf/fastspeech.v3.yaml') as f:\r\n    config = yaml.load(f, Loader=yaml.Loader)\r\nconfig = FastSpeechConfig(**config[\"fastspeech_params\"])\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(TFFastSpeechLengthRegulator(config, name='fastspeech_length_regulator'))\r\n\r\ntf.saved_model.save(model, \"./test_lr\")\r\n```", "change FastSpeechConfig and fastspeech.v3.yaml accordingly\r\n[supported_files.zip](https://github.com/tensorflow/tensorflow/files/4799490/supported_files.zip)\r\n", "@manmay-nakhashi Sorry, again. Could you share the saved_model in the test_lr directory to me? :)", "@abattery  [test_lr.zip](https://github.com/tensorflow/tensorflow/files/4802539/test_lr.zip)\r\n", "Could you try again with the following code snippet?\r\n\r\n```\r\n  interpreter = tf.lite.Interpreter(model_path=\"fastspeech.tflite\")\r\n  interpreter.allocate_tensors()\r\n  input_details = interpreter.get_input_details()\r\n  output_details = interpreter.get_output_details()\r\n\r\n  test_attention_mask = np.array([[True, True, True, True, True, True, True, True, True, True]], dtype=np.bool)\r\n  test_input_id = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype=np.int32)\r\n  test_speaker_id = np.array([0], dtype=np.int32)\r\n\r\n  interpreter.set_tensor(input_details[0]['index'], test_attention_mask)\r\n  interpreter.set_tensor(input_details[1]['index'], test_input_id)\r\n  interpreter.set_tensor(input_details[2]['index'], test_speaker_id)\r\n\r\n  interpreter.invoke()\r\n```", "> Could you try again with the following code snippet?\r\n> \r\n> ```\r\n>   interpreter = tf.lite.Interpreter(model_path=\"fastspeech.tflite\")\r\n>   interpreter.allocate_tensors()\r\n>   input_details = interpreter.get_input_details()\r\n>   output_details = interpreter.get_output_details()\r\n> \r\n>   test_attention_mask = np.array([[True, True, True, True, True, True, True, True, True, True]], dtype=np.bool)\r\n>   test_input_id = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype=np.int32)\r\n>   test_speaker_id = np.array([0], dtype=np.int32)\r\n> \r\n>   interpreter.set_tensor(input_details[0]['index'], test_attention_mask)\r\n>   interpreter.set_tensor(input_details[1]['index'], test_input_id)\r\n>   interpreter.set_tensor(input_details[2]['index'], test_speaker_id)\r\n> \r\n>   interpreter.invoke()\r\n> ```\r\n\r\n@abattery  Segmentation fault (core dumped) without any error", "any update on this ?", "Hi, I am Jae from TensorFlow Lite team, jaesung's colleague. We investigated your FastSpeech model and conclude that we need some modifications on your model. Jaesung and I could convert the modified version of FastSpeech with `tf-nightly` version.\r\n\r\nThe first error like `[None, None]` is not supported will be gone in the `tf-nightly`. However, we saw a little subtle problem inside the model. The preferred remedy is to fix the `batch_size = 1` during inference. By setting `[1, None]` the error will be gone.\r\n\r\nIn addition, the real problem was the input tensor_spec mismatch inside of `tf.while_loop()`. the 4th and 5th input tensors are growing (outputs, encoder_masks). this kind of dynamic size (not dynamic shape, please be careful of words. they are different) is not well recommended in TFLite converter. So we also fixed the size. The details can be found in the PR that I will open soon.", "Since the model conversion is done, and the changes are pushed into the new PRs by request from the owner of TensorflowTTS, I close this issue and leave the link to the PR. \r\n\r\nhttps://github.com/TensorSpeech/TensorflowTTS/pull/84", "@jaeyoo @abattery thank you so much for the support.  ", "@jaeyoo  android build is crashing in latest nightly build\r\n```\r\nW/System.err: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reshape.cc:55 stretch_dim != -1 (0 != -1)\r\n    Node number 83 (RESHAPE) failed to prepare.\r\n```", "@manmay-nakhashi Could you print the shapes related to Reshape?\r\nReshape can only stretch one dimension, Looks like you have two dimensions = -1.", "@thaink sure ", "@thaink \r\ninputs:\r\n[{'name': 'input_ids', 'index': 0, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'speaker_ids', 'index': 1, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'speed_ratios', 'index': 2, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'f0_ratios', 'index': 3, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'energy_ratios', 'index': 4, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\noutputs:\r\n[{'name': 'Identity', 'index': 1117, 'shape': array([ 1,  1, 80], dtype=int32), 'shape_signature': array([-1, -1, 80], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_1', 'index': 1138, 'shape': array([ 1,  1, 80], dtype=int32), 'shape_signature': array([-1, -1, 80], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_2', 'index': 721, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_3', 'index': 698, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_4', 'index': 649, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]", "@thaink model doesn't have any reshapes which is changing 2 dimensions of reshapes(same model was working before ) \r\ntesting on tf-nightly-2.4.0.dev20200811\r\n[fastspeech2_quant.zip](https://github.com/tensorflow/tensorflow/files/5062025/fastspeech2_quant.zip)\r\n", "@thaink any update on this ?", "two files from different tf-lite versions\r\n[fastspeech_2.3.txt](https://github.com/tensorflow/tensorflow/files/5096964/fastspeech_2.3.txt)\r\n[fastspeech_2.4.0_19_8_nightly.txt](https://github.com/tensorflow/tensorflow/files/5096975/fastspeech_2.4.0_19_8_nightly.txt)\r\n", "FYI, we fixed one bug in Range op recently. https://github.com/tensorflow/tensorflow/commit/fce766941e6864414b80d7b6471f5ae3db53aaf7", "@abattery thanks for the update , but this doesn't resolve my issue , it is crashing on below node does it look okay to you ?\r\n```\r\n{'name': 'encoder/layer_._2/attention/self/Reshape_3/shape/2',\r\n 'index': 81, 'shape': array([], dtype=int32),\r\n 'shape_signature': array([], dtype=int32), \r\n 'dtype': <class 'numpy.int32'>, \r\n 'quantization': (0.0, 0),\r\n 'quantization_parameters': {'scales': array([], dtype=float32), \r\n 'zero_points': array([], dtype=int32),\r\n 'quantized_dimension': 0},\r\n 'sparsity_parameters': {}}\r\n```", "@manmay-nakhashi According to @thaink 's comment, when you create a model, please make sure the input shape is the explicitly given, for example, batch size.", "@abattery i am giving batch size as 1 for tflite \r\n[{'name': 'input_ids', 'index': 0, 'shape': array([1, 1], dtype=int32), 'shape_signature': array(**[ 1, -1],** dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'speaker_ids', 'index': 1, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'speed_ratios', 'index': 2, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'f0_ratios', 'index': 3, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'energy_ratios', 'index': 4, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]", "@jaeyoo could you take a look? @manmay-nakhashi is experiencing an unknown dimension problem with the fast speech model.", "Hi @manmay-nakhashi, I've already converted fast speech model in TensorFlowTTS repo. Could you check it? @dathudeptrai\r\n", "@manmay-nakhashi what is ur problem ?, the real android example is provided here (https://github.com/TensorSpeech/TensorFlowTTS/tree/master/examples/android) which used FastSpeech2 already, why don't you follow this example ? everything is fine so far.", "@dathudeptrai @jaeyoo  tflite conversion works on 2.3.0 but when ever i try to convert it using latest build it is crashing in runtime. problem is coming when converting model with latest build and same model when i run it in latest build. problem with 2.3.0 version is that it reduces quality of speech significantly for me. **inference also crashes in @jaeyoo 's colab notebook**", "I am getting this error:\r\n\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 925 nodes with 1 partitions.\r\n\r\n{'name': 'input_ids', 'index': 0, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n{'name': 'speaker_ids', 'index': 1, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n{'name': 'speed_ratios', 'index': 2, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n{'name': 'f0_ratios', 'index': 3, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n{'name': 'energy_ratios', 'index': 4, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n{'name': 'Identity', 'index': 1117, 'shape': array([ 1,  1, 80], dtype=int32), 'shape_signature': array([-1, -1, 80], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n{'name': 'Identity_1', 'index': 1138, 'shape': array([ 1,  1, 80], dtype=int32), 'shape_signature': array([-1, -1, 80], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n{'name': 'Identity_2', 'index': 721, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n{'name': 'Identity_3', 'index': 698, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n{'name': 'Identity_4', 'index': 649, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\nTraceback (most recent call last):\r\nin <module>\r\n    decoder_output_tflite, mel_output_tflite = infer(input_text)\r\nin infer\r\n    interpreter.invoke()\r\n in  invoke   self._interpreter.Invoke()\r\nRuntimeError: tensorflow/lite/kernels/reshape.cc:55 stretch_dim != -1 (0 != -1)Node number 83 (RESHAPE) failed to prepare.\r\n\r\n", "@jaeyoo @abattery any update on this issue ? , should i open a new issue ? or is this error in same scope ? ", "@manmay-nakhashi Sorry for the late reply. I think it would be better to file a separate issue since this one is already closed and it is not easy for readers to capture the last problem.", "@abattery sure", "@abattery @thaink @jaeyoo Hi, i can reproduce the reshape bug in tf-nightly, but in `tf-nightly==2.4.0-dev20200630` everything is ok.\r\n\r\nthis is a colab (https://colab.research.google.com/drive/1HudLLpT9CQdh2k04c06bHUwLubhGTWxA?usp=sharing)", "@dathudeptrai Can you upload the saved model? ", "> @dathudeptrai Can you upload the saved model?\r\n\r\ntflite model or .pb ? ", "@dathudeptrai .pb or you could save it to a saved_model dir", "\r\n\r\n\r\n> @dathudeptrai .pb or you could save it to a saved_model dir\r\n\r\nit's here (https://drive.google.com/file/d/1wxxgOQ5u4vL3XZQYRGUlN_Fb5DlJYZHc/view?usp=sharing)", "@dathudeptrai I got an error while trying to convert it.\r\ngoogle3.third_party.tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"length_regulator/while@__inference__inference_6859\"): 'tf.While' op body result type tensor<1x?x?xf32> is incompatible with result type tensor<0x?x?xf32> at index 4\r\nAre you able to converting with the saved model?", "@thaink this is my minimal code to reproduce the bug (https://colab.research.google.com/drive/1IJbr7Nu7cSAmEJivApR5w_ymvnsR5_YH?usp=sharing). The bug is because conv1d ", "> @thaink this is my minimal code to reproduce the bug (https://colab.research.google.com/drive/1IJbr7Nu7cSAmEJivApR5w_ymvnsR5_YH?usp=sharing). The bug is because conv1d\r\n\r\nthen how to fix that?\r\n any workaround?", "@dathudeptrai @Zak-SA \r\nA fix is to make the batch_size specific in:\r\n@tf.function(\r\n      input_signature=[tf.TensorSpec(shape=[1, None, 384], dtype=tf.float32)])\r\nChange None to 1.\r\n", "@thaink what do you mean ? the input shape is [B, T, F] so i explicit it by [1, None, 384]. ", "@dathudeptrai could you make your keras model have a fixed batch size? Since keras model usually have an unspecified size in batch size and you also have an unspecified size in the above input_signature. TF and TFLite both does not support two unspecified sizes in the one shape array.", "@abattery the input signature is [1, None, 384] already in my colab.\r\n![Screenshot from 2020-09-23 14-52-22](https://user-images.githubusercontent.com/43868663/93983218-79546e00-fdac-11ea-8b92-3053902155f5.png)\r\n", "@dathudeptrai  Can you make your model with a fixed input signature? The none value will be propagated to a reshape op. and that reshape op has a wrong shape. Could you revisit your logic to avoid this behavior?", "> @dathudeptrai Can you make your model with a fixed input signature? The none value will be propagated to a reshape op. and that reshape op has a wrong shape. Could you revisit your logic to avoid this behavior?\r\n\r\nHi, this is NLP/Speech task so i need dynamic time dimention. The old tf-nightly don't have this bug (tf.2.3 is also ok.)", "@dathudeptrai I see. Do you remember which version of tf-nightly does not have this issue before? That helps us find the root cause.", "@abattery tf-nightly==2.4.0-dev20200630 is the previous nightly version i used. Maybe we need iterate install some version to find the version that cause this bug :)", "Thank you for reporting the regression. I will take a look at this issue.", "> Thank you for reporting the regression. I will take a look at this issue.\r\n\r\nHi, tf-nightly==2.4.0-dev20200730 ->> bug, tf-nightly==2.4.0-dev20200729 ->> ok. So the commit cause this problem may in 30/07/2020 ", "@dathudeptrai Thank you for conducting your binary search! This information is very helpful for debugging. \ud83d\udc4d ", "I tried converting a FastSpeech2 model to TensorFlow lite and I seem to be getting the same error, but changing the version tf-nightly does not seem to fix the issue for me.\r\n\r\n\r\nUsing tensorflow nightly version 2.4.0-dev20200925\r\n\r\n>     java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reshape.cc:55 stretch_dim != -1 (0 != -1)\r\n>     Node number 81 (RESHAPE) failed to prepare.\r\n\r\nUsing tensorflow nightly version 2.4.0-dev20200716\r\n\r\n>     java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/range.cc:45 (start > limit && delta < 0) || (start < limit && delta > 0) was not true.\r\n>     Node number 524 (RANGE) failed to invoke.\r\n\r\nUsing tensorflow version 2.3.0\r\n\r\n>     java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/range.cc:45 (start > limit && delta < 0) || (start < limit && delta > 0) was not true.\r\n>     Node number 524 (RANGE) failed to invoke.\r\n\r\nI can't find much information on the issue, and I'm not really sure why I've been getting the bug.  If it makes any difference, I'm using a machine without GPU to convert to TF-lite.", "@gcervantes8 can you try any build between nightly 2.4.0-dev20200630 - 2.4.0-dev20200729 ?", "I tried version tf-nightly 2.4.0-dev20200713, I got the same error.\r\n\r\n> java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/range.cc:45 (start > limit && delta < 0) || (start < limit && delta > 0) was not true.\r\n>     Node number 524 (RANGE) failed to invoke.", "@gcervantes8 are you using enable_tflite_convertible flag? ", "Yes! When I reload the model using FastSpeech2Config, I give the parameter:\r\nenable_tflite_convertible=True", "> > Thank you for reporting the regression. I will take a look at this issue.\r\n> \r\n> Hi, tf-nightly==2.4.0-dev20200730 ->> bug, tf-nightly==2.4.0-dev20200729 ->> ok. So the commit cause this problem may in 30/07/2020\r\n\r\nI also encounter this bug in the conv1d layer, your solution really helps!", "I have a working fix for this. It will be fixed soon.", "> I have a working fix for this. It will be fixed soon.\r\n\r\nthank you so much :D. ", "the tf.repeat function can also cause errors when I use tf-nightly==2.5.0-dev20201029\r\n\r\ntensorflow.lite.python.convert.ConverterError: ...../lib/python3.6/site-packages/tensorflow/python/saved_model/load.py:890:0: error: 'tf.Reshape' op requires 'shape' to have at most one dynamic dimension, but got multiple dynamic dimensions at indices 1 and 2\r\n\r\n........\r\n\r\nException ignored in: <bound method Buckets.__del__ of <tensorflow.python.eager.monitoring.ExponentialBuckets object at 0x7f6e852aa4c8>>\r\nTraceback (most recent call last):\r\n  File \"......./lib/python3.6/site-packages/tensorflow/python/eager/monitoring.py\", line 407, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'\r\n"]}, {"number": 40503, "title": "Android-PoseNet is not accurate when person is laying down", "body": "I am running PoseNet on mobile and saw that when the person is laying down the model can't detect the legs of the person, does this is something that is happening only to me or is a general problem. If so is there any way I can improve the accuracy of PoseNet ?\r\n\r\n- Running on mobile device: Xiaomi MI A2 (the speed was around 100ms, but I also tried on another device with Qualcomm 835 which had better speed but not improvement on accuracy)\r\n\r\n-Also why its so unstable, I mean I stay in one place and points keep changing places \r\n\r\n-Can it work better on black and white frames ?", "comments": ["Hi,\r\n\r\nI'm facing the same issue as well. What's a possible solution for this. \r\n\r\nThank you!", "Hi, Same problem here, any solution found?? @ArtanBerisha1 ??", "Hi, not yet on PoseNet, but I also tried the Google ML Kit Pose Detection model and is better than PoseNet.\r\nI hope it helps @mariem1992 ", "@ArtanBerisha1 Could you please let us know if this is still an issue ?Thanks!", "Hi @sushreebarsa , \r\nI didn't replace the .tflite model so I guess this is still an issue, if there is newer .tflite model let me know so I can give it a try and test it.\r\n\r\nThank you!", "@ArtanBerisha1 Could you please refer to the [examples here](https://www.tensorflow.org/lite/examples/pose_estimation/overview) ,[TF blog](https://blog.tensorflow.org/2019/08/track-human-poses-in-real-time-on-android-tensorflow-lite.html) and try with the latest TF v2.7.0 ? Please let us know if it helps?  Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @sushreebarsa ,\r\nThank you, I was already trying those models but I found them in TF Hub page searching for skeleton models (didn't see that the pose estimation page was updated ). These models are more accurate, but I have a question, can we train these models even more or not ? ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40503\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40503\">No</a>\n"]}, {"number": 40502, "title": "Tensorflow's FixedLengthRecordDataset reads binary file significantly slower than pure Python", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: CPU Only\r\n- GPU model and memory: CPU Only\r\n\r\n**Describe the current behavior**\r\nI'm wondering why Tensorflow's FixedLengthRecordDataset is so slow when reading records from a binary file (without processing the records). My implementation in plain Python (see code below) appears to be two order of magnitude faster. \r\n\r\n**Describe the expected behavior**\r\nI would expect that reading a file with FixedLengthRecordDataset would perform comparably or faster than the pure Python implementation. \r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nwith open('some_file', 'wb') as f:\r\n    f.write(np.ones(10000, dtype=np.int32).tobytes())\r\n\r\n\r\ndef time_tf():\r\n    start_time = time.time()\r\n    for record in tf.data.FixedLengthRecordDataset('some_file', record_bytes = 4):\r\n        _ = record\r\n    print(f'FixedLengthRecordDataset: {time.time() - start_time}')\r\n\r\ndef time_py():\r\n    start_time = time.time()\r\n    with open('some_file', 'rb') as f:\r\n        b = f.read(4)\r\n        while b:\r\n            b = f.read(4)\r\n    print(f'Plain Python: {time.time() - start_time}')\r\n\r\ntime_tf()\r\ntime_py()\r\n```\r\n\r\n```\r\nFixedLengthRecordDataset: 0.6247196197509766\r\nPlain Python: 0.001483917236328125\r\n```\r\n\r\n**Other info / logs** \r\nNone, but happy to provide on request. \r\n", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/d27936b334109100a63e1f41af3250cc/40502.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/2a3b0437187fd5ee7e79ce3d4c303415/40502-tf-nightly.ipynb). Please find the attached gist. Thanks!", "To add to this, the problem seem to lie with the Dataset itself rather than IO:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\na = np.ones(100000, dtype=np.float32)\r\n\r\nstart_time = time.time()\r\nfor x in a:\r\n    pass\r\nprint(time.time() - start_time)\r\n\r\nstart_time = time.time()\r\nfor x in tf.data.Dataset.from_tensor_slices(a):\r\n    pass\r\nprint(time.time() - start_time)\r\n```\r\n\r\n```\r\n0.05548405647277832\r\n5.67711615562439\r\n```", "@mhorlacher For a more apples to apples comparison, we should append `skip(10000)` to the dataset, so that the iteration happens in C++ instead of python. We should also make the benchmark run longer, to reduce the impact on constant overheads. With those changes, the difference is between 3x and 4x. This isn't concerning, since real use cases don't fetch tens of thousands of elements per second from `tf.data`. The performance advantages of using `tf.data` come out when doing non-trivial input processing to train a model.", "Thanks for your response @aaudiber . I'm fitting a large number (billions) of short sequences to a shallow network so in my case the above issue becomes a critical performance bottleneck, as both CPU and GPU are at very low utilization when using `tf.data` as described above. Is this a hopeless case or can you provide me with / point me towards any information to optimize performance for that task? ", "@mhorlacher Is the data stored in a single file? If you split the data into multiple files, you can use `Dataset.interleave` to read from many files in parallel to speed up the input. If you share the dataset you are using, I can give recommendations", "Thanks for looking into this @aaudiber  . Unfortunately, I didn't get any significant speed-up using `Data.interleave` or FixedLengthRecordDataset's `num_num_parallel_reads`. I'm unsure how to share my dataset properly, but I can provide you with some simulated data. \r\n\r\nBriefly, my inputs are sequences of length 24 over the alphabet {A, C, G, T} and they are paired with corresponding counts which I aim to predict. To reduce the size of the dataset, sequences are encoded as integers, since enumerating all possible sequences of length 24 fits into a 64-bit integer. In fact, I only need 6 bytes to encode a sequence of 24 and corresponding counts fit into a 32-bit integer, leading to 10 bytes per sample. Samples are stored across dozens of files (in fact, file-of-origin is another model input but irrelevant here). \r\n\r\nGenerating some data:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom pathlib import Path\r\n\r\nn_files = 4\r\nn_samples = 10000\r\n\r\nout_dir = '.'\r\n\r\nfor i in range(0, n_files):\r\n    with open(Path(out_dir).joinpath(f'file_{i}.gerbil'), 'wb') as f:\r\n        for j in range(0, n_samples):\r\n            mer_bytes = np.random.randint(0, 4 ** 24 - 1, size=None, dtype=np.uint64).tobytes()[:6]\r\n            count_bytes = np.random.randint(1, 2 ** 32 - 1, size=None, dtype=np.uint32).tobytes()\r\n            f.write(count_bytes + mer_bytes)\r\n```\r\n\r\nLoading raw byte records:\r\n```\r\nfilepaths = [str(fp) for fp in Path(out_dir).glob('*.gerbil')]\r\n\r\ndataset = tf.data.FixedLengthRecordDataset(filepaths, record_bytes=10)\r\nprint(dataset.element_spec)\r\n```\r\n\r\nParsing raw records, i.e. decoding bytes and one-hot encoding sequence:\r\n```\r\ndef _decode_mer_base(mer_int64, i):\r\n    mer_int64_rs = tf.bitwise.right_shift(mer_int64, 2*i)\r\n    return tf.math.floormod(mer_int64_rs, tf.constant(4, tf.int64))\r\n\r\ndef _decode_mer(mer_int64, k):\r\n    \"\"\"Decode an int64-encoded kmer to a k*4 one-hot matrix.\"\"\"\r\n    vec = tf.vectorized_map(lambda i: _decode_mer_base(mer_int64, i), tf.range(k, dtype=tf.int64))\r\n    return tf.one_hot(vec, 4, dtype=tf.dtypes.float16)\r\n\r\ndef _parse_raw_record(raw_record, k):\r\n    # decode mer count\r\n    count = tf.io.decode_raw(tf.strings.substr(raw_record, 0, 4), out_type=tf.int32, little_endian=True)[0]\r\n\r\n    # decode mer\r\n    mer_int64 = tf.io.decode_raw(tf.strings.substr(raw_record, 4, 6) + b'\\x00\\x00', out_type=tf.int64, little_endian=False)[0]\r\n    mer_int64 = tf.bitwise.right_shift(mer_int64, (32-k)*2)\r\n    mer_onehot = _decode_mer(mer_int64, k)\r\n\r\n    return mer_onehot, count\r\n\r\ndataset = dataset.map(lambda x: _parse_raw_record(x, k=24))\r\nprint(dataset.element_spec)\r\n```\r\n\r\nThe last code block is mostly for completeness and to demonstrate that the CPU is not fully utilized when decoding and preparing the raw input. The sequences are fed into a shallow CNN to detect short sequence motifs. \r\n\r\n\r\n", "Thanks for the detailed explanation. \r\n\r\n- Would it be possible for the model to train on batches of examples instead of single examples? That could reduce the overhead and get better utilization from the CPU.\r\n- Try setting the `num_parallel_reads` option of `FixedLengthRecordDataset`, so that multiple files are read in parallel.\r\n- To fully understand the performance and what is causing the bottleneck, use https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras#overview to generate a trace view that shows how much time is being spent in each dataset.", "Dear @aaudiber - thanks for the suggestions, I did some testing. \r\n\r\nFirst, I repeat parts of the code above, but with a larger synthetic data sample. \r\n\r\n```\r\nimport numpy as np\r\nfrom pathlib import Path\r\n\r\nimport tensorflow as tf\r\ntf.__version__\r\n\r\nn_files = 4\r\nn_samples = 400000\r\n\r\nout_dir = '.'\r\n\r\nfor i in range(0, n_files):\r\n    with open(Path(out_dir).joinpath(f'file_{i}.gerbil'), 'wb') as f:\r\n        for j in range(0, n_samples):\r\n            mer_bytes = np.random.randint(0, 4 ** 24 - 1, size=None, dtype=np.uint64).tobytes()[:6]\r\n            count_bytes = np.random.randint(1, 2 ** 32 - 1, size=None, dtype=np.uint32).tobytes()\r\n            f.write(count_bytes + mer_bytes)\r\n\r\nfilepaths = [str(fp) for fp in Path('.').glob('*.gerbil')]\r\n```\r\n\r\n1. Testing the influence of `num_parallel_reads` on runtime:\r\n\r\n```\r\nimport time\r\n\r\ndef time_ParallelReads_1():\r\n    dataset = tf.data.FixedLengthRecordDataset(filepaths, record_bytes=10, num_parallel_reads = 1)\r\n    dataset = dataset.skip(10000)\r\n\r\n    start_time = time.time()\r\n    for r in dataset:\r\n        _ = r\r\n    print(f'time_ParallelReads_1: {time.time() - start_time}')\r\n\r\ndef time_ParallelReads_4():\r\n    dataset = tf.data.FixedLengthRecordDataset(filepaths, record_bytes=10, num_parallel_reads = 4)\r\n    dataset = dataset.skip(10000)\r\n\r\n    start_time = time.time()\r\n    for r in dataset:\r\n        _ = r\r\n    print(f'time_ParallelReads_4: {time.time() - start_time}')\r\n\r\ndef time_ParallelReads_AUTOTUNE():\r\n    dataset = tf.data.FixedLengthRecordDataset(filepaths, record_bytes=10, num_parallel_reads = tf.data.experimental.AUTOTUNE)\r\n    dataset = dataset.skip(10000)\r\n\r\n    start_time = time.time()\r\n    for r in dataset:\r\n        _ = r\r\n    print(f'time_ParallelReads_4: {time.time() - start_time}')\r\n\r\ntime_ParallelReads_1()\r\ntime_ParallelReads_4()\r\ntime_ParallelReads_AUTOTUNE()\r\n```\r\n```\r\ntime_ParallelReads_1: 148.9105041027069\r\ntime_ParallelReads_4: 153.37963438034058\r\ntime_ParallelReads_4: 158.80391359329224\r\n```\r\n\r\nIt seems like increasing the number of parallel reads has no effect, indicating that the overhead of iterating over the tf.data.Dataset is indeed the problem. \r\n\r\n2. Batching the data before iterating\r\n```\r\nimport time\r\n\r\ndef time_Batch_64():\r\n    dataset = tf.data.FixedLengthRecordDataset(filepaths, record_bytes=10)\r\n    dataset = dataset.skip(10000)\r\n    dataset = dataset.batch(64)\r\n\r\n    start_time = time.time()\r\n    for r in dataset:\r\n        _ = r\r\n    print(f'time_Batch_64: {time.time() - start_time}')\r\n\r\ndef time_Batch_512():\r\n    dataset = tf.data.FixedLengthRecordDataset(filepaths, record_bytes=10)\r\n    dataset = dataset.skip(10000)\r\n    dataset = dataset.batch(512)\r\n\r\n    start_time = time.time()\r\n    for r in dataset:\r\n        _ = r\r\n    print(f'time_Batch_512: {time.time() - start_time}')\r\n\r\ndef time_Batch_4096():\r\n    dataset = tf.data.FixedLengthRecordDataset(filepaths, record_bytes=10)\r\n    dataset = dataset.skip(10000)\r\n    dataset = dataset.batch(4096)\r\n\r\n    start_time = time.time()\r\n    for r in dataset:\r\n        _ = r\r\n    print(f'time_Batch_4096: {time.time() - start_time}')\r\n\r\ntime_Batch_64()\r\ntime_Batch_512()\r\ntime_Batch_4096()\r\n```\r\n```\r\ntime_Batch_64: 2.4835407733917236\r\ntime_Batch_512: 1.2220275402069092\r\ntime_Batch_4096: 1.0555901527404785\r\n```\r\n\r\nIterating over batches instead of individual samples leads to impressive runtime improvements. I was wondering whether bringing down the number of iteration over the dataset through other means (while keeping amount of data constant) will effect the runtime as well, so I tried to increase the number of bytes I read of the files. \r\n\r\n```\r\nimport time\r\n\r\ndef time_RecordBytes_40():\r\n    dataset = tf.data.FixedLengthRecordDataset(filepaths, record_bytes=40)\r\n    dataset = dataset.skip(10000)\r\n\r\n    start_time = time.time()\r\n    for r in dataset:\r\n        _ = r\r\n    print(f'time_RecordBytes_40: {time.time() - start_time}')\r\n\r\ndef time_RecordBytes_400():\r\n    dataset = tf.data.FixedLengthRecordDataset(filepaths, record_bytes=400)\r\n    dataset = dataset.skip(10000)\r\n\r\n    start_time = time.time()\r\n    for r in dataset:\r\n        _ = r\r\n    print(f'time_RecordBytes_400: {time.time() - start_time}')\r\n\r\ndef time_RecordBytes_4000():\r\n    dataset = tf.data.FixedLengthRecordDataset(filepaths, record_bytes=4000)\r\n    dataset = dataset.skip(10000)\r\n\r\n    start_time = time.time()\r\n    for r in dataset:\r\n        _ = r\r\n    print(f'time_RecordBytes_4000: {time.time() - start_time}')\r\n\r\ntime_RecordBytes_40()\r\ntime_RecordBytes_400()\r\ntime_RecordBytes_4000()\r\n```\r\n```\r\ntime_RecordBytes_40: 18.031224489212036\r\ntime_RecordBytes_400: 1.4269778728485107\r\ntime_RecordBytes_4000: 0.010393857955932617\r\n```\r\n\r\nIndeed, reading multiple samples at once by increasing the number of record bytes reduces iteration time drastically. \r\n\r\n---\r\n\r\nContinuing further I decided to preprocess and fit data in batches. \r\n\r\nFirst, I redefine the above processing functions for a single sample:\r\n\r\n```\r\ndef _decode_mer_base(mer_int64, i):\r\n    mer_int64_rs = tf.bitwise.right_shift(mer_int64, 2*i)\r\n    return tf.math.floormod(mer_int64_rs, tf.constant(4, tf.int64))\r\n\r\ndef _decode_mer(mer_int64, k):\r\n    \"\"\"Decode an int64-encoded kmer to a k*4 one-hot matrix.\"\"\"\r\n    vec = tf.vectorized_map(lambda i: _decode_mer_base(mer_int64, i), tf.range(k, dtype=tf.int64))\r\n    return tf.one_hot(vec, 4, dtype=tf.dtypes.float16)\r\n\r\ndef _parse_raw_record(raw_record, k):\r\n    # decode mer count\r\n    count = tf.io.decode_raw(tf.strings.substr(raw_record, 0, 4), out_type=tf.int32, little_endian=True)[0]\r\n\r\n    # decode mer\r\n    mer_int64 = tf.io.decode_raw(tf.strings.substr(raw_record, 4, 6) + b'\\x00\\x00', out_type=tf.int64, little_endian=False)[0]\r\n    mer_int64 = tf.bitwise.right_shift(mer_int64, (32-k)*2)\r\n    mer_onehot = _decode_mer(mer_int64, k)\r\n\r\n    return mer_onehot, count\r\n```\r\n\r\nThen, I use `map_fn` to map the preprocessing function over the entire batch and pass the dataset into the model. \r\n\r\n```\r\ndataset = tf.data.FixedLengthRecordDataset(filepaths, record_bytes=10)\r\ndataset = dataset.skip(10000)\r\ndataset = dataset.batch(512).prefetch(tf.data.experimental.AUTOTUNE)\r\ndataset = dataset.map(lambda x: tf.map_fn(lambda y: _parse_raw_record(y, k=24), x, dtype=(tf.float16, tf.int32)), num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\ndataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\ndataset.element_spec\r\n```\r\n\r\nThis brings GPU usage on my server from essentially idle to around 8-10%. Only when I `.cache` the preprocessed dataset I archive close to maximum GPU utilization (unfortunately this is not possible as my data does not fit into RAM). \r\n\r\nI think from here I need to work on optimizing the preprocessing for further improvements. If you have any immediate recommendations on that, please let me know. Otherwise I think I'll close the issue soon as I now understand the source of the initial problem better.\r\n\r\nThanks and best regards\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"]}]