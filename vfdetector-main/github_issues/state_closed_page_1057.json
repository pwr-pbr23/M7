[{"number": 21578, "title": "Useless sampleRateList?", "body": "In the Tensorflow Mobile Android simple speech recognition sample, why feed the inferenceInterface with a sampleRateList which seems useless? The sampleRateList is just initialized without meaningful values.\r\n\r\n[Code line 311](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/SpeechActivity.java#L311)", "comments": ["It's one of the two input nodes?"]}, {"number": 21577, "title": "Add Pop!_OS section to main page for", "body": "installing TensorFlow on Pop!_OS.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "As this is not a first party owned package, we cannot add it as is.\r\nBut we may be able to link to your instructions in README.md at the root, under Community supported builds?", "The community supported builds section is more for hardware architecture, not distro formats.\r\n\r\nThere isn't currently a good place to put this on tensorflow.org and would be difficult to maintain a list for non-first parties. But might be worth a discussion on the tensorflow discuss list or the SIG build list: https://www.tensorflow.org/community/lists"]}, {"number": 21576, "title": "Install pop os", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 21575, "title": "the tensorflow-CPU version can support X86 in window??", "body": "Now i want to compile tensorflow-CPU by cmake+vs2015 ,which is to get tensorflow.dll and tensorflow.lib\uff0cbut now I can't sure tensorf-CPU can support X86 in window platform\uff1f\r\nthanks\r\n\r\n", "comments": ["I am guessing what you want to do \r\nis to compile tensorflow from sources in Windows....Follow the below links:\r\nhttps://joe-antognini.github.io/machine-learning/build-windows-tf\r\nhttps://github.com/cjweeks/tensorflow-cmake\r\nhttps://medium.com/@shiweili/building-tensorflow-c-shared-library-on-windows-e79c90e23e6e\r\n\r\nIf I am guessing wrong, please correct me..!!", "Yes it does support in x86 platform in windows.", "thanks.\r\nif compile tensorflow from sources in windows , and it can support  win-x86??  No 64", "Yes it might support on x64-win too.Altough not much sure about it.", "thanks", "> Now i want to compile tensorflow-CPU by cmake+vs2015 ,which is to get tensorflow.dll and tensorflow.lib\uff0cbut now I can't sure tensorf-CPU can support X86 in window platform\uff1f\r\n> thanks\r\n\r\ndidyou  got the x86 dll?"]}, {"number": 21574, "title": "Tensorflow Lite, python API does not work ", "body": "### System information\r\n- **TensorFlow version:  1.9.0**\r\n- **Python version:  3.5**\r\n\r\n### Describe the problem\r\nI am try run TFlite model file with Python API (like in example: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md), but I get an error: \r\n**ImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKaiiS2_PKfiPfi**\r\n\r\n### Source code / logs\r\nMy code:\r\n```\r\nimport tensorflow as tf\r\n\r\nif __name__ == \"__main__\":\r\n \r\n   # Load TFLite model and allocate tensors.\r\n   interpreter = tf.contrib.lite.Interpreter(model_path=\"./mobilenet_v1_0.25_128_quant.tflite\")\r\n \r\n   interpreter.allocate_tensors()\r\n \r\n   #Get input and output tensors.\r\n   input_details = interpreter.get_input_details()\r\n   output_details = interpreter.get_output_details()\r\n\r\n   print(input_details)\r\n   print(output_details)\r\n```\r\n\r\nLog output:\r\n```\r\nTraceback (most recent call last):\r\n  File \"tflite_test.py\", line 12, in <module>\r\n    interpreter = tf.contrib.lite.Interpreter(model_path=\"/home/pi/test/mobilenet_v1_0.25_128_quant/mobilenet_v1_0.25_128_quant.tflite\")\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter.py\", line 50, in __init__\r\n    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.5/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 673, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 693, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKaiiS2_PKfiPfi\r\n```", "comments": ["Could you provide more info:\r\n\r\nWhat platform is this? Looks like a raspberry pi? What version? What board? What raspbian version?\r\n", "I ran into the same problem w/ either Python 2.7 or 3.5 on RPI 3 B running latest Raspbian. See my comment https://github.com/tensorflow/tensorflow/pull/21109#issuecomment-412413091", "@aselle , @petewarden , I try run this code on **Raspberry Pi 3** board with **Raspbian 9.0** system on architecture **arm7l**. Also tried on **Artik 710s** board with **Ubuntu 16.04.5** system on architecture **aarm64** and have was the same error.", "I'm having the same problem with Raspbian 9.0, Tensorflow 1.9.0 on a Raspberry Pi 3, with Python 2.7.3 and 3.5.3 ", "I managed to build pip package from master branch on a Raspbian Pi 3 B running Raspbian and ran the [label_image using Python TF Lite binding](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/python/label_image.md) example without problem. So, this should be some kind of compilation flag problem when these pip packages were built.", "From the error message we ran into,\r\n\r\n```\r\n> c++filt _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKfiiS2_iPfi\r\ntflite::tensor_utils::NeonMatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*, int)\r\n```\r\nIt seems that \r\n```\r\ntflite::tensor_utils::NeonMatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*, int)\r\n```\r\nwhich is located in [tensorflow/contrib/lite/kernels/internal/optimized/neon_tensor_utils.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/internal/optimized/neon_tensor_utils.cc#L48-L85)  is not compiled in because `USE_NEON` [is not defined](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/internal/optimized/neon_tensor_utils.cc#L25). Or, it just simply was not linked into the .so?", "Even i'm facing the same issue. I'm using Raspberry Pi 3 Model B, Raspbian 9.0, Tensorflow 1.9.0, Python 3.5.3. @freedomtan how did you build a pip package?", "@himanshurawlani \r\n\r\n0. patience, it's painfully slow to build it natively on RPI 3\r\n1. prepare requirements as described in [TensorFlow's doc](https://www.tensorflow.org/install/install_sources#prepare_environment_for_linux)\r\n2. build and install bazel from source, you may wanna prepare paging space and increase the maximum size of the memory allocation pool before build bazel as described in [this doc](https://gist.github.com/EKami/9869ae6347f68c592c5b5cd181a3b205) to .\r\n3. ./configure\r\n4. build it with bazel, note that I can build it successfully without modifying source with the following command\r\n\r\n```\r\nbazel build --config opt --local_resources 1024.0,0.5,0.5 \\\r\n--copt=-mfpu=neon-vfpv4 \\\r\n--copt=-ftree-vectorize \\\r\n--copt=-funsafe-math-optimizations \\\r\n--copt=-ftree-loop-vectorize \\\r\n--copt=-fomit-frame-pointer \\\r\n--copt=-DRASPBERRY_PI \\\r\n--host_copt=-DRASPBERRY_PI \\\r\n//tensorflow/tools/pip_package:build_pip_package\r\n```", "I had a same problem.\r\nsudo pip install --upgrade \"tensorflow==1.10.*\" solved the problem", "when I was working on updating https://github.com/tensorflow/tensorflow/pull/16175, it came to me that, for the original problem, mostly `//tensorflow/contrib/lite/kernels/internal:neon_tensor_utils` was not linked in.", "Did using the nightly or 1.10 solve your problem @Mykheievskyi?\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I have the same issue with raspbian 9.0, tensorflow 1.9.0, python 3.5.3 on raspberry pi 3B. I tried to solve it installing tensorflow 1.10.1 and even though i did it, it's still showing the same problem. did anyone solve it? ", "I could resolve this issue by building the tf :1.10 from source natively in raspberrypi.\r\n\r\nPython version 3.5.3\r\nBuild Command is below.\r\nbazel build --config opt --local_resources 1024.0,0.5,0.5 \\\r\n--copt=-mfpu=neon-vfpv4 \\\r\n--copt=-ftree-vectorize \\\r\n--copt=-funsafe-math-optimizations \\\r\n--copt=-ftree-loop-vectorize \\\r\n--copt=-fomit-frame-pointer \\\r\n--copt=-DRASPBERRY_PI \\\r\n--copt=-D__ARM_NEON__\\\r\n--copt=-D__ARM_NEON\\\r\n--host_copt=-DRASPBERRY_PI \\\r\n//tensorflow/tools/pip_package:build_pip_package\r\n\r\nBut I face an assertion error\r\n\r\n\r\n/usr/lib/python3/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"usage_test.py\", line 5, in <module>\r\n    interpreter = tf.contrib.lite.Interpreter(model_path=\"mobilenet_v1_1.0_224.tflite\")\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.5/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 673, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py\", line 69, in <module>\r\n    from tensorflow.contrib import periodic_resample\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/periodic_resample/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.periodic_resample.python.ops.periodic_resample_op import periodic_resample\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/periodic_resample/python/ops/periodic_resample_op.py\", line 32, in <module>\r\n    resource_loader.get_path_to_datafile('_periodic_resample_op.so'))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/load_library.py\", line 73, in load_op_library\r\n    exec(wrappers, module.__dict__)\r\n  File \"<string>\", line 317, in <module>\r\n  File \"<string>\", line 229, in _InitOpDefLibrary\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_registry.py\", line 36, in register_op_list\r\n    assert _registered_ops[op_def.name] == op_def\r\nAssertionError\r\n\r\nDid anyone face this issue before?", "I tried to install Tensorflow **using PIP install** on my Raspberry Pi3 B+ (Raspbian Stretch - June 2018 Version) and when I tried to run the sample label_image.py example I faced the same error. i.e.\r\n\r\n`(ImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKaiiS2_PKfiPfi)`\r\n\r\n\r\nSo I tried another way and build the **Cross Compile Package** using latest Tensorflow code from master branch and after installing the package on pi I am facing this error - \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"label_image.py\", line 37, in <module>\r\n    interpreter = tf.contrib.lite.Interpreter(model_path=args.model_file)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.4/importlib/__init__.py\", line 109, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 2254, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 2237, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 2226, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1200, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1129, in _exec\r\n  File \"<frozen importlib._bootstrap>\", line 1471, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 321, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/__init__.py\", line 48, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/distribute/__init__.py\", line 34, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 27, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/tpu/__init__.py\", line 69, in <module>\r\n    from tensorflow.contrib.tpu.python.ops.tpu_ops import *\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py\", line 39, in <module>\r\n    resource_loader.get_path_to_datafile(\"_tpu_ops.so\"))\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/load_library.py\", line 60, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name: \r\n\r\nAn op that loads optimization parameters into HBM for embedding. Must be\r\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\r\nembedding table configuration. For example, this op is used to install\r\nparameters that are loaded from a checkpoint before a training loop is\r\nexecuted.\r\n```\r\n\r\n", "I also getting the same error, updating to TF-1.10.* or TF-1.11 does not resolve the problem.\r\nI am using Raspberry Pi3 with Raspbian Stretch and Python3.5\r\n\r\npi@raspberrypi:~/tflite_exp $ python3 tf_lite_cam.py \r\nPATH_TO_LABELS= object_detection/data/mscoco_label_map.pbtxt\r\nTraceback (most recent call last):\r\n  File \"tf_lite_cam.py\", line 48, in <module>\r\n    interpreter = tf.contrib.lite.Interpreter(model_path=TF_MODEL)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/interpreter.py\", line 51, in __init__\r\n    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.5/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 673, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 693, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKfiiS2_iPfi\r\n\r\nSomeone tried the miniconda version of tensorflow for ARM ?? https://towardsdatascience.com/stop-installing-tensorflow-using-pip-for-performance-sake-5854f9d9eb0c\r\n\r\nIf someone is able to generate the .whl with the correct compilations, plz share the .whl\r\n\r\n", "@saurabh-kachhia @stanlee321 yes, it seems that the problem is still there in the official 1.11 pip wheel. I tested with both python 2.7 and 3.5.", "Even i am facing the same issue on Raspberry PI 3. Been looking around for some solution. \r\nAlso tried cross-compile but in vain. Now trying to compile on pi3.\r\n@freedomtan please let us know is there is a fast fix to it. Eagerly waiting for your response.\r\n\r\nThanks in advance :)", "@sahilparekh for 1.9.x to 1.11.x, what I posted in Aug,\r\n```\r\nbazel build --config opt --local_resources 1024.0,0.5,0.5 \\\r\n--copt=-mfpu=neon-vfpv4 \\\r\n--copt=-ftree-vectorize \\\r\n--copt=-funsafe-math-optimizations \\\r\n--copt=-ftree-loop-vectorize \\\r\n--copt=-fomit-frame-pointer \\\r\n--copt=-DRASPBERRY_PI \\\r\n--host_copt=-DRASPBERRY_PI \\\r\n//tensorflow/tools/pip_package:build_pip_package\r\n```\r\nshould work. For master branch, some modification for building AWS SDK may be needed. AWS SDK problem may need something like https://github.com/tensorflow/tensorflow/pull/22856\r\n", "It turns out that target \"tensor_utils\" may have missed neon deps. To correct it, make sure that  `tensorflow/contrib/lite/kernels/internal:tensor_utils` have `neon_tensor_utils` deps. \r\nFor aarch64 cross-compile configuration, do below modification can solve the problem:\r\n```\r\ndiff --git a/tensorflow/contrib/lite/kernels/internal/BUILD b/tensorflow/contrib/lite/kernels/internal/BUILD\r\nindex 464163b..d1f4a0e 100644\r\n--- a/tensorflow/contrib/lite/kernels/internal/BUILD\r\n+++ b/tensorflow/contrib/lite/kernels/internal/BUILD\r\n@@ -53,6 +53,13 @@ config_setting(\r\n )\r\n \r\n config_setting(\r\n+    name = \"aarch64\",\r\n+    values = {\r\n+        \"cpu\": \"aarch64\",\r\n+    },\r\n+)\r\n+\r\n+config_setting(\r\n     name = \"arm64-v8a\",\r\n     values = {\r\n         \"cpu\": \"arm64-v8a\",\r\n@@ -448,6 +455,9 @@ cc_library(\r\n         \":arm\": [\r\n             \":neon_tensor_utils\",\r\n         ],\r\n+        \":aarch64\": [\r\n+            \":neon_tensor_utils\",\r\n+        ],\r\n         \":arm64-v8a\": [\r\n             \":neon_tensor_utils\",\r\n         ],\r\n```", "@zhewang95 Your suggestion doesn't help me. I am still facing the same issue.\r\n\r\n", "@saurabh-kachhia @zhewang95 surely it doesn\u2019t work. RPI 3 does have ARMv8/AArch64 cores, but the Raspbian runs 32-bit kernel.", "SOLUTION FOR THIS ERROR!\r\n\r\nSource Code:\r\ninterpreter = tf.contrib.lite.Interpreter(model_path=\"optimized_graph.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\nImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKaiiS2_PKfiPfi\r\n\r\n\r\njust install tensorflow 1.11.0 following the next steps:\r\n\r\n\r\n$ sudo apt-get install python-pip python3-pip\r\n$ sudo pip3 uninstall tensorflow\r\n$ git clone https://github.com/PINTO0309/Tensorflow-bin.git\r\n$ cd Tensorflow-bin\r\n$ sudo pip3 install tensorflow-1.11.0-cp35-cp35m-linux_armv7l.whl\r\n\r\nif it doesn\u00b4t work, try to re-format the sd card and do it again \r\n", "@EmilioMezaE Thank you so much. It works well.\r\n I tested py27 and py35. I was able to load tflite file. \r\nCan i ask how do you find this solution ? I want to know what was the problem. ", "Thanks @EmilioMezaE  for the solution.\r\nUninstalling the old tensorflow does not work with pip, just force delete it:\r\n```\r\nrm -rf  /home/pi/.local/lib/python3.5/site-packages/tensorflow\r\n```\r\nCurrently the performance of 'Lite' seems to be **2-3x slower** compared to 'regular tensorflow', this is for quantized model. Lite uses only 1 CPU...maybe this is to blame.", "@gasparka\r\nMy solution is to disable \"jemalloc\".\r\n**https://github.com/PINTO0309/Tensorflow-bin.git**\r\nAlthough I have not tried it yet, enabling \"jemalloc\" may improve performance.", "You're welcome @rky0930 ! I' sorry but i don't know the reason of this problem, i just saw this page https://github.com/PINTO0309/Tensorflow-bin  and followed the process ", "@rky0930 , @EmilioMezaE see my previous comments for the reason and build instructions", "I am referring to the suggestion of @freedomtan completely.\r\nThank you, freedomtan.\r\nI found that activating MPI is meaningful for performance improvement, so now I started recompiling.\r\nIt will take about 3 days.", "@PINTO0309 let us know how much faster it gets :)", "@gasparka \r\n\r\nI tried installing the rebuilt binary with \"jemalloc\" and \"MPI\" enabled.\r\nUnfortunately, I did not get faster as I expected.\r\n\"MPI\" seems to be a mechanism to speed up by distributed processing at learning.\r\n\r\n\u3010My ENet\u3011 Pure Tensorflow v1.11.0 10.2 sec ---> 9.5 sec\r\n\u3010My UNet\u3011 Tensorflow Lite v1.11.0 11.5 sec ---> 12.1 sec\r\n\r\n**https://github.com/PINTO0309/Tensorflow-bin.git**\r\n**tensorflow-1.11.0-cp35-cp35m-linux_armv7l_jemalloc_mpi.whl**\r\n\r\nNext I will try to validate \"XLA JIT\" and I will try to verify whether to speed up.\r\nI hope it will work...", "@PINTO0309 \r\nHave you experimented with the thread count? I see that Lite is stuck on one thread, there is a C++ API for this but nothing in Python.\r\n\r\nCould try to hardcode the thread count to 4:\r\nhttps://github.com/tensorflow/tensorflow/blob/1084594657a5d139102ac794f84d1427a710e39a/tensorflow/contrib/lite/interpreter.cc#L127\r\n\r\n\r\n\r\n", "Thank you @gasparka.\r\n\r\nBefore enabling \"XLA JIT\",\r\nI will try to change the hard code\r\n**`context_.recommended_num_threads = -1;`**\r\nto\r\n**`context_.recommended_num_threads = 4;`**\r\n", "@gasparka \r\nI tried rebuilding with MultiThread enabled.\r\nHowever, it seems that Python's Wrapper does not refer to \"Thread Count\", and the processing speed has not changed.\r\nIf it is a C++ program, 4 threads will be used.\r\nSince I do not have the skills to write C++ programs, can you try? ---> gasparka\r\n**https://github.com/PINTO0309/Tensorflow-bin.git**\r\n**tensorflow-1.11.0-cp35-cp35m-linux_armv7l_jemalloc_mpi_multithread.whl**\r\n\r\nResults of Python program.\r\n\u3010My ENet\u3011 Pure Tensorflow v1.11.0 9.5 sec ---> 9.5 sec\r\n\u3010My UNet\u3011 Tensorflow Lite v1.11.0 12.1 sec ---> 12.5 sec\r\n\r\nNext I will try to validate \"XLA JIT\" and I will try to verify whether to speed up.\r\n", "I will try to take a look at the C++ stuff this week.", "@PINTO0309 \r\nUnfortunately i have no time to look into this :(\r\n", "Tensorflow v1.12.0 rc0, Stand alone installer.\r\nhttps://github.com/tensorflow/tensorflow/issues/23082#issuecomment-439071863", "> SOLUTION FOR THIS ERROR!\r\n> \r\n> Source Code:\r\n> interpreter = tf.contrib.lite.Interpreter(model_path=\"optimized_graph.tflite\")\r\n> interpreter.allocate_tensors()\r\n> \r\n> ImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKaiiS2_PKfiPfi\r\n> \r\n> just install tensorflow 1.11.0 following the next steps:\r\n> \r\n> $ sudo apt-get install python-pip python3-pip\r\n> $ sudo pip3 uninstall tensorflow\r\n> $ git clone https://github.com/PINTO0309/Tensorflow-bin.git\r\n> $ cd Tensorflow-bin\r\n> $ sudo pip3 install tensorflow-1.11.0-cp35-cp35m-linux_armv7l.whl\r\n> \r\n> if it doesn\u00b4t work, try to re-format the sd card and do it again\r\n\r\nThis was the only thing that work for me, something i did wrong was that i kept trying to install the multi-threaded version but that doesnt work on the pi. you need to install the one on the instrucitons.\r\nafter that it works just fine i am still testing to see if its better than the regular tensorflow.", "We got ours to work by updating interpreter.py to include contrib in the path as follows:\r\n_interpreter_wrapper = LazyLoader(\r\n    \"_interpreter_wrapper\", globals(),\r\n    \"tensorflow.**contrib**.lite.python.interpreter_wrapper.\"\r\n    \"tensorflow_wrap_interpreter_wrapper\")\r\n# pylint: enable=g-inconsistent-quotes\r\n", "for multi-threading stuff, I sent a PR https://github.com/tensorflow/tensorflow/pull/25748", "Thank you for always doing great work, @freedomtan.\r\nI succeeded in building Tensorflow Lite, incorporating your suggestion.\r\nhttps://github.com/tensorflow/tensorflow/issues/25120#issuecomment-464401990", "I tried implementing MultiThread with Tensorflow Lite v1.11.0.\r\nIt gained 2.5 times the performance.\r\n\r\n**https://github.com/PINTO0309/Tensorflow-bin/blob/master/tensorflow-1.11.0-cp35-cp35m-linux_armv7l_jemalloc_multithread.whl**\r\n\r\n```bash\r\n$ sudo apt-get install -y libhdf5-dev libc-ares-dev libeigen3-dev\r\n$ sudo pip3 install keras_applications==1.0.7 --no-deps\r\n$ sudo pip3 install keras_preprocessing==1.0.9 --no-deps\r\n$ sudo pip3 install h5py==2.9.0\r\n$ sudo apt-get install -y openmpi-bin libopenmpi-dev\r\n$ sudo pip3 uninstall tensorflow\r\n$ wget -O tensorflow-1.11.0-cp35-cp35m-linux_armv7l.whl https://github.com/PINTO0309/Tensorflow-bin/raw/master/tensorflow-1.11.0-cp35-cp35m-linux_armv7l_jemalloc_multithread.whl\r\n$ sudo pip3 install tensorflow-1.11.0-cp35-cp35m-linux_armv7l.whl\r\n\r\n\u3010Required\u3011 Restart the terminal.\r\n```\r\n\r\nCustomize \"tensorflow/contrib/lite/examples/python/label_image.py\".\r\n```python\r\nimport argparse\r\nimport numpy as np\r\nimport time\r\n\r\nfrom PIL import Image\r\n\r\nfrom tensorflow.contrib.lite.python import interpreter as interpreter_wrapper\r\ndef load_labels(filename):\r\n  my_labels = []\r\n  input_file = open(filename, 'r')\r\n  for l in input_file:\r\n    my_labels.append(l.strip())\r\n  return my_labels\r\nif __name__ == \"__main__\":\r\n  floating_model = False\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument(\"-i\", \"--image\", default=\"/tmp/grace_hopper.bmp\", \\\r\n    help=\"image to be classified\")\r\n  parser.add_argument(\"-m\", \"--model_file\", \\\r\n    default=\"/tmp/mobilenet_v1_1.0_224_quant.tflite\", \\\r\n    help=\".tflite model to be executed\")\r\n  parser.add_argument(\"-l\", \"--label_file\", default=\"/tmp/labels.txt\", \\\r\n    help=\"name of file containing labels\")\r\n  parser.add_argument(\"--input_mean\", default=127.5, help=\"input_mean\")\r\n  parser.add_argument(\"--input_std\", default=127.5, \\\r\n    help=\"input standard deviation\")\r\n  parser.add_argument(\"--num_threads\", default=1, help=\"number of threads\")\r\n  args = parser.parse_args()\r\n\r\n  interpreter = interpreter_wrapper.Interpreter(model_path=args.model_file)\r\n  interpreter.allocate_tensors()\r\n  input_details = interpreter.get_input_details()\r\n  output_details = interpreter.get_output_details()\r\n  # check the type of the input tensor\r\n  if input_details[0]['dtype'] == np.float32:\r\n    floating_model = True\r\n  # NxHxWxC, H:1, W:2\r\n  height = input_details[0]['shape'][1]\r\n  width = input_details[0]['shape'][2]\r\n  img = Image.open(args.image)\r\n  img = img.resize((width, height))\r\n  # add N dim\r\n  input_data = np.expand_dims(img, axis=0)\r\n  if floating_model:\r\n    input_data = (np.float32(input_data) - args.input_mean) / args.input_std\r\n\r\n  interpreter.set_num_threads(int(args.num_threads))\r\n  interpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\n  start_time = time.time()\r\n  interpreter.invoke()\r\n  stop_time = time.time()\r\n\r\n  output_data = interpreter.get_tensor(output_details[0]['index'])\r\n  results = np.squeeze(output_data)\r\n  top_k = results.argsort()[-5:][::-1]\r\n  labels = load_labels(args.label_file)\r\n  for i in top_k:\r\n    if floating_model:\r\n      print('{0:08.6f}'.format(float(results[i]))+\":\", labels[i])\r\n    else:\r\n      print('{0:08.6f}'.format(float(results[i]/255.0))+\":\", labels[i])\r\n\r\n  print(\"time: \", stop_time - start_time)\r\n```\r\n\r\nEnvironment Preparation for MobileNet v1.\r\n```bash\r\n$ cd ~;mkdir test\r\n$ curl https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp > ~/test/grace_hopper.bmp\r\n$ curl https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz | tar xzv -C ~/test mobilenet_v1_1.0_224/labels.txt\r\n$ mv ~/test/mobilenet_v1_1.0_224/labels.txt ~/test/\r\n$ curl http://download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224_quant.tgz | tar xzv -C ~/test\r\n$ cp tensorflow/tensorflow/contrib/lite/examples/python/label_image.py ~/test\r\n```\r\n\r\nResult of x1 Thread.\r\n```bash\r\n$ cd ~/test\r\n$ python3 label_image.py \\\r\n--num_threads 1 \\\r\n--image grace_hopper.bmp \\\r\n--model_file mobilenet_v1_1.0_224_quant.tflite \\\r\n--label_file labels.txt\r\n\r\n0.415686: 653:military uniform\r\n0.352941: 907:Windsor tie\r\n0.058824: 668:mortarboard\r\n0.035294: 458:bow tie, bow-tie, bowtie\r\n0.035294: 835:suit, suit of clothes\r\ntime:  0.4152982234954834\r\n```\r\nResult of x4 Thread.\r\n```bash\r\n$ cd ~/test\r\n$ python3 label_image.py \\\r\n--num_threads 4 \\\r\n--image grace_hopper.bmp \\\r\n--model_file mobilenet_v1_1.0_224_quant.tflite \\\r\n--label_file labels.txt\r\n\r\n0.415686: 653:military uniform\r\n0.352941: 907:Windsor tie\r\n0.058824: 668:mortarboard\r\n0.035294: 458:bow tie, bow-tie, bowtie\r\n0.035294: 835:suit, suit of clothes\r\ntime:  0.1647195816040039\r\n```", "Did you ran this on the RaspberryPI 3B?\r\nSo no difference? no performace gain? i dont understand the title saying 2.5 but the results are the same?\r\nWhat about the PI resources was it taking over 90% or still at 25%-30%?\r\n\r\n", "@masterchop \r\n\r\n>Did you ran this on the RaspberryPI 3B?\r\n\r\nYes. The above performance measurement result is based on RaspberryPi3.\r\n\r\n>What about the PI resources was it taking over 90% or still at 25%-30%?\r\n\r\n25%-30%\r\n\r\nIt seems that you are misunderstanding something.\r\n\"freedomtan\" and my implementation is **\"MultiThread\"**.\r\nIt is not **\"MultiProcess\"**.\r\nPerformance will never improve more than 4 times.\r\n4 Core is never used in full. \r\n**http://www.dabeaz.com/python/UnderstandingGIL.pdf**\r\n**https://qiita.com/pumbaacave/items/942f86269b2c56313c15**\r\n\r\nIf you need implementation with 4 cores, implement it yourself.\r\nI am sorry, I do not have the technology of implementation by \"C++ and MultiProcess\".", "@PINTO0309 and @masterchop as far as I can remember only the convolution kernel is multithreaded, so you hit [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law)", "> @himanshurawlani\r\n> \r\n> 1. patience, it's painfully slow to build it natively on RPI 3\r\n> 2. prepare requirements as described in [TensorFlow's doc](https://www.tensorflow.org/install/install_sources#prepare_environment_for_linux)\r\n> 3. build and install bazel from source, you may wanna prepare paging space and increase the maximum size of the memory allocation pool before build bazel as described in [this doc](https://gist.github.com/EKami/9869ae6347f68c592c5b5cd181a3b205) to .\r\n> 4. ./configure\r\n> 5. build it with bazel, note that I can build it successfully without modifying source with the following command\r\n> \r\n> ```\r\n> bazel build --config opt --local_resources 1024.0,0.5,0.5 \\\r\n> --copt=-mfpu=neon-vfpv4 \\\r\n> --copt=-ftree-vectorize \\\r\n> --copt=-funsafe-math-optimizations \\\r\n> --copt=-ftree-loop-vectorize \\\r\n> --copt=-fomit-frame-pointer \\\r\n> --copt=-DRASPBERRY_PI \\\r\n> --host_copt=-DRASPBERRY_PI \\\r\n> //tensorflow/tools/pip_package:build_pip_package\r\n> ```\r\n\r\n@freedomtan Thanks. Can I ask what the options `--copt=-DRASPBERRY_PI` and `host_copt=-DRASPBERRY_PI` do?", "@hoonkai dunno if it's still needed. It was used to build `//tensorflow/lite/kernels/internal:neon_tensor_utils` (was `//tensorflow/contrib/lite/kernels/internal:neon_tensor_utils` with neon support). As mentioned in my comment several month ago, you need `-DRASPBERRY_PI` (or `-DARM_NON_MOBILE`) to enabled it", "@freedomtan : I ran the following command based on the suggestions above: \r\n\r\n```\r\nbazel --host_jvm_args=-Xmx3g --host_jvm_args=-Xms512m \\\r\nbuild --jobs 4 --local_resources 3000,0.7,0.7 \\\r\n--config=opt --copt=\"-funsafe-math-optimizations\" \\\r\n--copt=\"-ftree-vectorize\" \\\r\n--copt=\"-fomit-frame-pointer\" \\\r\n--copt=-DARM_NON_MOBILE \\\r\n--host_copt=-DARM_NON_MOBILE \\\r\n--verbose_failures \\\r\ntensorflow/tools/pip_package:build_pip_package\r\n```\r\nThe package build successfully however I still get the undefined symbol error. \r\n**Can you please elaborate what exactly you have done to remove that error?**\r\n\r\nI am compiling tensorflow 1.12.2 with python3.6 on arm64 / aarch64 with Ubuntu 18.04\r\nI had the same result when compiling tensorflow 1.10.0\r\n\r\n@PINTO0309 : I've checked yout your repository however there is no description what exactly you have edited to make it work. The binaries you provide are only for armv7, and the one for armv8 is not good for me because I don't have CUDA / Nvidia support. **Could you maybe elaborate?**", " my bad, I've found the right patches:\r\n\r\n1. [tensorflow/lite/kernels/internal/BUILD](https://github.com/tensorflow/tensorflow/pull/16175/files)\r\nIn my case it has been _tensorflow/contrib/lite/kernels/internal/BUILD\r\n\r\n2. third_party/aws/BUILD.bazel and tensorflow/BUILD [here](https://github.com/tensorflow/tensorflow/pull/22856/commits/3f88ddb71ba49d343a5db1304c296e78ddeb2575)\r\nThe first file has been  _third_party/aws.BUILD_\r\n\r\n@freedomtan @PINTO0309 : Keep up the good work !", "I have exactly the same issue with TensorFlow 1.13.1 on Raspberry Pi running Python 3.6.8. Here is the output: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tflite.py\", line 20, in <module>\r\n    interpreter = tf.lite.Interpreter(MODEL_PATH)\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 54, in __init__\r\n    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 61, in __getattr__\r\n    module = self._load()\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 44, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 684, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils24NeonVectorScalarMultiplyEPKaifPf\r\n```\r\nAny ideas on how to sort this out?", "> I have exactly the same issue with TensorFlow 1.13.1 on Raspberry Pi running Python 3.6.8. Here is the output:\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   File \"tflite.py\", line 20, in <module>\r\n>     interpreter = tf.lite.Interpreter(MODEL_PATH)\r\n>   File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 54, in __init__\r\n>     _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n>   File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 61, in __getattr__\r\n>     module = self._load()\r\n>   File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 44, in _load\r\n>     module = importlib.import_module(self.__name__)\r\n>   File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n>   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n>   File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n>     _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n>   File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\n>   File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/imp.py\", line 243, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n>     return _load(spec)\r\n>   File \"<frozen importlib._bootstrap>\", line 684, in _load\r\n>   File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n>   File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n>   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n> ImportError: /home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils24NeonVectorScalarMultiplyEPKaifPf\r\n> ```\r\n> \r\n> Any ideas on how to sort this out?\r\n\r\nDid you solve it ?\r\n\r\nI have the exact same issue (same error, same versions).\r\n\r\nThanks", "@gaetanbahl : I've applied this two patches:\r\nhttps://github.com/tensorflow/tensorflow/pull/16175/files \r\nhttps://github.com/tensorflow/tensorflow/pull/22856/files\r\n\r\nI think the first one is the one which is solving the error.", "> We got ours to work by updating interpreter.py to include contrib in the path as follows:\r\n> _interpreter_wrapper = LazyLoader(\r\n> \"_interpreter_wrapper\", globals(),\r\n> \"tensorflow.**contrib**.lite.python.interpreter_wrapper.\"\r\n> \"tensorflow_wrap_interpreter_wrapper\")\r\n> \r\n> # pylint: enable=g-inconsistent-quotes\r\n\r\n## This worked for me. Thanks\r\n----------------------------------"]}, {"number": 21573, "title": "Question: Scatter_nd_op order of operations on GPU ", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc4.8\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: AMD R9 Nano\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nHi,\r\nWe are working on a SYCL implementation of TensorFlow. When implementing the backend for the scatter_nd operation we were wondering what is the behavior of concurrent updates.\r\nThe test in [scatter_nd_ops_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/scatter_nd_ops_test.py#L231) assumes that the operations are applied in a non-deterministic order, which matches what the [documentation](https://www.tensorflow.org/api_docs/python/tf/scatter_nd) says. This corner case is costly to handle on the GPU and it seems that the [CUDA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#L81) implementation does not handle it, neither does ours.\r\n\r\n\r\nThe documentation says: \r\n\r\n> The order in which updates are applied is nondeterministic, so the output will be nondeterministic if indices contains duplicates.\r\n\r\nThe test checks for Add and Sub of two concurrent indices to result in the total summation (or subtraction) of both updates to apply (order of additions is nondeterministic but the result is). On a replace operation the output would be nondeterministic (hence the test not checking for it).\r\nWith a non blocking GPU implementation the second update applied to an index may (and will probably) overwrite the first update from a different thread.\r\n\r\nOur question then is:\r\nIs the test wrong to test that concurrent summation & subtraction is deterministic (in which case we would be happy to write a PR to correct that)?\r\nOtherwise, is the CUDA implementation wrong? In which case the documentation should probably be more descriptive on what happens with concurrent add/sub compared to replace.\r\n\r\nThanks,\r\n", "comments": ["@drpngx could you help us to find the best person to talk to about this?", "@ekelsen any opinion on this perhaps?", "@zheng-xq @reedwm ", "/CC @ebrevdo", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think it's nondeterministic on CPU (have to check).  On GPU we use [atomic operations](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#L47).  So while the order of adds is not consistent, which may lead to numerical differences, at least we aren't overwriting values.", "Hey, thanks for the answer!\r\n\r\nWe checked and the operation is deterministic on CPU.\r\nIt still seems that the [documentation](https://www.tensorflow.org/api_docs/python/tf/scatter_nd_update) of scatter_nd_update should reflect the fact that replacing multiple times the same index is non deterministic on GPU.\r\n\r\nSomething similar to scatter_nd : \"WARNING: The order in which updates are applied is nondeterministic, so the output will be nondeterministic if indices contains duplicates.\"\r\n\r\nThanks again for your answer!"]}, {"number": 21572, "title": "tf.estimator.train_and_evaluate got wrong Evaluation accuray and loss", "body": "I use tf.estimator.train_and_evaluate to train and evaluate my model. This is my code:\r\n\r\n```\r\ndef model_fn(features, labels, mode):\r\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n    with slim.arg_scope(resnet_v2.resnet_arg_scope()):\r\n        logits, endpoints = resnet_v2.resnet_v2_50(features, num_classes=cls_num,\r\n                is_training=is_training)\r\n    logits = tf.squeeze(logits, [1, 2])\r\n    preds = tf.argmax(logits, 1)\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n    accuracy = tf.metrics.accuracy(labels=labels, predictions=preds)\r\n    metrics = {'accuracy': accuracy}\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\r\n    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\ndef train_input_fn():\r\n    dataset = tf.data.Dataset.from_tensor_slices((np.array(img_train), label_train))\r\n    dataset = dataset.repeat(5).batch(8)\r\n    return dataset\r\n\r\ndef eval_input_fn():\r\n    dataset = tf.data.Dataset.from_tensor_slices((np.array(img_test), label_test))\r\n    dataset = dataset.repeat(1).batch(8)\r\n    return dataset\r\n\r\nestimator = tf.estimator.Estimator(model_fn=model_fn, model_dir='logs')\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\r\neval_specs = tf.estimator.EvalSpec(input_fn=eval_input_fn)\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_specs)\r\n```\r\nThe training step is ok and the loss became very small (about 0.001), but the evaluation result is wrong (the following is the evaluaiton log):\r\n\r\n```\r\n...\r\nINFO:tensorflow:Saving dict for global step 625: accuracy = 0.5, global_step = 625, loss = 1330830600000.0\r\n...\r\n```\r\nThe task is very simple, just a binaray classfication. I don not think it is overfitting. Is there something wrong for my evaluation code?", "comments": ["Try decreasing the learning rate in the step,\r\noptimizer = tf.train.AdagradOptimizer(learning_rate=0.1) to\r\n \r\noptimizer = tf.train.AdagradOptimizer(learning_rate=0.001) (or smaller value)\r\n\r\nand try again. \r\n\r\nThis might help you.", "@aravind3134 decreasing the learning rate to 0.001 seems not work:\r\n```\r\nINFO:tensorflow:Saving dict for global step 625: accuracy = 0.5, global_step = 625, loss = 7749.7485\r\n```\r\nThe loss is smaller but the accuracy is also 0.5.", "Can you let me know the data-set size and can I see more logs in the console, if you have any.\r\n\r\nYou have them probably above this line:\r\nINFO:tensorflow:Saving dict for global step 625: accuracy = 0.5, global_step = 625, loss = 7749.7485\r\n\r\nIf you don't find any, you can edit line \r\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)   to\r\n\r\nlogging_hook = tf.train.LoggingTensorHook (tensors=tensors_to_log, every_n_iter=50)\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=1000, hook = [logging_hook])\r\n\r\nThis should give you the summary of the training you are doing. \r\n\r\nSee if your loss during your training is fluctuating. If it is, you might have to look for a better learning rate, may be by performing a grid search on the interval [0.00001, 0.001]. \r\n\r\nThe loss seems to have decreased a lot, but nothing can be judged with one line. Let me know more about the logs.\r\n\r\nHope this helps.", "OK, this is my training log:\r\n\r\n```\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fefaadf1ed0>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'logs', '_train_distribute': None, '_save_summary_steps': 100}\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-08-16 09:47:59.546073: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-08-16 09:48:01.441117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 22.38GiB freeMemory: 22.21GiB\r\n2018-08-16 09:48:01.441186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-16 09:48:01.811401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-16 09:48:01.811463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-16 09:48:01.811477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-16 09:48:01.812022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21544 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into logs/model.ckpt.\r\nINFO:tensorflow:loss = 0.7005657, step = 0\r\nINFO:tensorflow:global_step/sec: 13.5045\r\nINFO:tensorflow:loss = 0.3813903, step = 100 (7.405 sec)\r\nINFO:tensorflow:global_step/sec: 15.0218\r\nINFO:tensorflow:loss = 0.5762321, step = 200 (6.657 sec)\r\nINFO:tensorflow:global_step/sec: 15.0521\r\nINFO:tensorflow:loss = 0.15642221, step = 300 (6.644 sec)\r\nINFO:tensorflow:global_step/sec: 15.0839\r\nINFO:tensorflow:loss = 0.19744259, step = 400 (6.630 sec)\r\nINFO:tensorflow:global_step/sec: 15.0664\r\nINFO:tensorflow:loss = 0.110218376, step = 500 (6.638 sec)\r\nINFO:tensorflow:global_step/sec: 15.0758\r\nINFO:tensorflow:loss = 0.04312685, step = 600 (6.633 sec)\r\nINFO:tensorflow:Saving checkpoints for 625 into logs/model.ckpt.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2018-08-16-01:50:14\r\nINFO:tensorflow:Graph was finalized.\r\n2018-08-16 09:50:15.665224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-16 09:50:15.665298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-16 09:50:15.665314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-16 09:50:15.665324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-16 09:50:15.665545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21544 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Restoring parameters from logs/model.ckpt-625\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Evaluation [10/100]\r\nINFO:tensorflow:Evaluation [20/100]\r\nINFO:tensorflow:Evaluation [30/100]\r\nINFO:tensorflow:Evaluation [40/100]\r\nINFO:tensorflow:Evaluation [50/100]\r\nINFO:tensorflow:Evaluation [60/100]\r\nINFO:tensorflow:Evaluation [70/100]\r\nINFO:tensorflow:Finished evaluation at 2018-08-16-01:50:19\r\nINFO:tensorflow:Saving dict for global step 625: accuracy = 0.5, global_step = 625, loss = 7912.4326\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 625: logs/model.ckpt-625\r\nINFO:tensorflow:Loss for final step: 0.028676154.\r\n```\r\n...", "One thing to look out for is making sure the shapes of the predictions and labels being fed into the accuracy metric are the same. There could be some broadcasting at times that can lead to odd values for the accuracy number. Use tf.shape. tf.Print to print out for a few steps and see whats going on. ", "@honeytidy did you solve your problem?", "I experience a similar problem where the evaluation loss reported during training (using `estimator.train_and_evaluate()`)at a given step is much higher than when I evaluate the checkpoint at this step using `estimator.evaluate()`. The datasets and `EvalSpec` are exactly the same. I already changed the `steps` paramter in the `EvalSpec` to `None`. Is this behavior expected? ", "I had a similar problem and it turned out to be related to the batch normalization module in my code that the fact that I needed to update the moving average and variance each time before training. basically adding the following commands would fix this issue. \r\n\r\n```\r\n   optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\r\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\r\n        train_op = optimizer.minimize(loss_fn, var_list=tf.trainable_variables())\r\n```\r\n\r\n\r\n\r\n"]}, {"number": 21571, "title": "Custom CUDA op kernel launch dimensions", "body": "Are there any kind of restrictions on the grid dimensions for custom op CUDA kernels?\r\n\r\nThanks", "comments": ["These are standard CUDA kernels, hence normal CUDA restrictions apply:\r\nhttps://developer.download.nvidia.com/compute/cuda/CUDA_Occupancy_calculator.xls\r\n\r\nAnd Table H.1\r\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications\r\n\r\n> Maximum ... of a grid of threadblocks\r\n\r\n"]}, {"number": 21570, "title": "[TensorFlow Android Camera Demo] NOTHING DETECTED with SSD FPN model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Window 10 64bit\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Samsung Galaxy S7 Edge, RAM 4GB\r\n- **TensorFlow installed from (source or binary)**: binary with pip: pip install tensorflow\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: Python 3.6.4 |Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\n### Describe the problem\r\nI retrained the model using [SSD FPN](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config). The training process was ok, total loss was acceptable at 0.5 after 1.7K steps.\r\nThe problem: I imported to [TensorFlow Android Camera Demo](https://github.com/tensorflow/tensorflow/tree/r1.10/tensorflow/examples/android), at DetectorActivity, I modified:\r\n```\r\nprivate static final String TF_OD_API_MODEL_FILE = \"file:///android_asset/frozen_inference_graph.pb\"; private static final String TF_OD_API_LABELS_FILE = \"file:///android_asset/my_list.txt\";\r\nNOTHING DETECTED. \r\n```\r\nJust to confirm that, the app was working with [SSD Mobilenet V1](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config) model (re-trained with my own data)\r\n\r\nI also changed:\r\n`private static final int TF_OD_API_INPUT_SIZE = 640; ` at DetectorActivity.java because SSD FPN uses input-size 640x640, but not working", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21569, "title": "tfgan.gan_train_ops is incompatible with DistibutionStrategy", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 7.0\r\n- **GPU model and memory**: GTX 980M(4G memory)\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\ntfgan.gan_train_ops is incompatible with DistibutionStrategy.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e7475504094b3018973740df470d2c9ee73a4fd5/tensorflow/contrib/gan/python/train.py#L972 will cause \"You must specify an aggregation method to update a MirroredVariable in Tower Context.\" error.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce\nMobile device", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 21568, "title": "doc: fix typo in api_guides/cc/guide.md", "body": "When I was learning the cc api from official site, I found the calculation result  and the comment doesn't match, so I send this PR.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I have made the pr for the docs project, so this pr can be  closed.", "closed"]}, {"number": 21567, "title": "[question] [tflite] How to serialize Interpreter?", "body": "Sorry for question, but could you, please, give me hint? I want to construct tflite model with C++ API (not with toco converter) and serialize it. If I use 'Interpreter' class for model creation, can I then convert it to FlatBufferModel somehow? Or the only opportunity is to create model via FlatBufferBuilder API?", "comments": ["I'm working on an experimental feature for that right now. I'll keep you posted. It will probably show up in tensorflow/contrib/lite/experimental/writer \r\n", "aselle@ has submitted the files here:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/experimental/writer\r\n\r\nPlease reopen if any issues arise. Thanks!"]}, {"number": 21566, "title": "optimize_for_inference output broken graph", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNo.\r\n- **TensorFlow installed from (source or binary)**:\r\nSource.\r\n- **TensorFlow version (use command below)**:\r\n1.10.0\r\n- **Python version**:\r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.15.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n7.3.0\r\n- **CUDA/cuDNN version**:\r\n9.2/7.1.4.18\r\n- **GPU model and memory**:\r\nGTX 960 / 4G\r\n- **Exact command to reproduce**:\r\nI trained a ssdlite_mobilenet_v2 model and run the following command to optimize the frozen graph.\r\n\r\npython -m tensorflow.python.tools.optimize_for_inference \\\r\n    --input=frozen_inference_graph.pb \\\r\n    --output=optimized_frozen_inference_graph.pb  \\\r\n    --input_names=image_tensor --placeholder_type_enum=4 \\\r\n    --output_names=num_detections,detection_classes,detection_scores,detection_boxes\r\n\r\nIt gave a lot of the following warnings but succeeded:\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm'\r\n\r\nHowever, the outputted graph is broken:\r\nbenchmark_model --graph=optimized_frozen_inference_graph.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=1,300,300,3 --output_layer=detection_boxes,detection_scores,detection_classes,num_detections\r\n\r\n2018-08-13 08:05:27.077759: E tensorflow/core/framework/types.cc:102] Unrecognized DataType enum value 161403164\r\n2018-08-13 08:05:27.080527: E tensorflow/tools/benchmark/benchmark_model.cc:275] Could not create TensorFlow Session: Invalid argument: Input 0 of node Preprocessor/map/while/add/y was passed int32 from Preprocessor/map/while/Switch:1 incompatible with expected unknown dtype enum (161403164)_ref.\r\n", "comments": ["I encounter the same problem ...", "same problem using quantize_graph or transformer_graph for quantize, and run on x86. tf version 1.9", "I just tested the latest 1.11.0-rc2 release. The problem is still there. Is anything going to take a look at this?", "Unfortunately optimize_for_inference has become outdated over the last couple of years, and needs to be deprecated. We have an alternative in the Graph Transform Tool (though this is also getting a bit out of date): https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#optimizing-for-deployment", "Hi @petewarden, shall we use then Graph Transform Tool in place of optimize_for_inference wherever it is used? Is there a newer alternative since you say it's becoming out of date? Thank you.", "You're correct, I do suggest the GTT in place of this script, since it should be more reliable."]}, {"number": 21565, "title": "Manually call right-operand version of binary ops to preserve error message", "body": "#12454\r\n\r\n```python\r\nimport tensorflow as tf\r\nx = tf.get_variable('asdfds', shape=[10], dtype=tf.int32)\r\nx*0.5\r\n```\r\nused to print:\r\n```\r\nTypeError: unsupported operand type(s) for *: 'Variable' and 'float' \r\n```\r\n\r\nIt now prints:\r\n```\r\nTypeError: Expected int32, got 0.5 of type 'float' instead.\r\n```", "comments": ["@ppwwyyxx Could you add more test cases?", "Nagging Reviewer @skye: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 46 days with no activity and the `awaiting review` label has been applied.", "@skye Could you PTAL and approve.", "I agree with @wt-huang , can you add a test case or two demonstrating the new behavior?", "This issue was fixed in 2da8e8c60343d9e833f1c591c54ee2a27c240842 (though not yet made into any major release)."]}, {"number": 21564, "title": "Tensorflow code is not running on GPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0/7.1.4\r\n- **GPU model and memory**: K80\r\n- **Exact command to reproduce**: \r\n```\r\nimport tensorflow as tf\r\nwith tf.device('/GPU:0'):\r\n  w = tf.constant(2.0,shape=[3,3,3,3,1])\r\n  inpt = tf.constant(2.0,shape=[1,5,5,5,3])\r\n  conv = tf.nn.conv3d(inpt,w,[1,1,1,1,1],'SAME')\r\n  init_op = tf.global_variables_initializer()\r\n  with tf.Session() as sess:\r\n    sess.run(init_op)\r\n    print('Convolution Output')\r\n    print( sess.run(conv))\r\n```\r\n\r\nI have installed tensorflow from sources and modified(just some print statements in both CPU kernel and GPU kernel) inside '**conv_ops_3d.cc**'. And built from sources, but when I am running on GPU it is always calling CPU code.\r\nSimilarly I checked for backprop using '**tf.test.compute_gradient_error**', by keeping some print statements in '**conv_grad_ops_3d.cc**' in CPU kernels and GPU kernels. But this time, it is calling GPU code only (from 'conv_grad_ops_3d.cc' to 'conv_ops_3d.cc'). May be I am missing something small when I am installing from sources..! From the attached file, you can see the yellow colored lines which I changed which are just print statements.\r\n[Conv_ops_3d.pdf](https://github.com/tensorflow/tensorflow/files/2281807/Conv_ops_3d.pdf)\r\n", "comments": ["Try replacing all constants with `tf.Variable`s. TensorFlow does constant propagation on the CPU even if your ops are GPU ops. Using `tf.Variable`s instead of constants prevents constant propagation from occurring.", "Thank you...Got it...!!"]}, {"number": 21563, "title": "fix documentation for SECURITY.md", "body": "- grammatical fixes in examples of sandboxing tools (remove an extra comma)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "To be consistent with the rest of our usage, which generally favors the comma after both \"i.e.\" and \"e.g.\", I'm inclined to close this. \r\n\r\nThere's no consensus on what the \"correct\" way to do this is, see, e.g., [here](https://www.dailywritingtips.com/comma-after-i-e-and-e-g/)."]}, {"number": 21562, "title": "tf.estimator.DNNClassifier TypeError: __init__() got an unexpected keyword argument 'batch_norm'", "body": "Hello ,\r\n          I've run into the following error and could not find any relevant post on stack overflow. I'm trying to create a DNN classifier using the tf.estimator.DNNClassifier. My  tf.GIT_VERSION is  v1.9.0-0-g25c197e023  and  tf.VERSION  is 1.9.0\r\n\r\n### Problem\r\n\r\n**TypeError: __init__() got an unexpected keyword argument 'batch_norm'**\r\n\r\n\r\nIt doesn't seem like batch_norm is one of the params\r\nUpdating tensorflow to 1.10.0 causes another issue saying bitwise cannot be imported\r\n\r\n\r\n### Source code / logs\r\n\r\n**inspect.getargspec(tf.estimator.DNNClassifier).args** \r\n\r\ngives \r\n\r\n<class 'list'>: ['self', 'hidden_units', 'feature_columns', 'model_dir', 'n_classes', 'weight_column', 'label_vocabulary', 'optimizer', 'activation_fn', 'dropout', 'input_layer_partitioner', 'config', 'warm_start_from', 'loss_reduction']\r\n\r\nIt has self as a param and the estimator.params dictionary is always None and doesn't give information about the values\r\n\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Could you give a minimal example for reproducing the exception? Thanks.", "You can try running this\r\n\r\nimport tensorflow as tf\r\nimport inspect\r\nprint(inspect.getargspec(tf.estimator.DNNClassifier).args)\r\n\r\nwhich gives\r\n\r\n<class 'list'>: ['self', 'hidden_units', 'feature_columns', 'model_dir', 'n_classes', 'weight_column', 'label_vocabulary', 'optimizer', 'activation_fn', 'dropout', 'input_layer_partitioner', 'config', 'warm_start_from', 'loss_reduction']\r\n\r\nThis is missing batch_norm but the documentation says it is supported. If i try to give in a batch_norm parameter i get the error that __init__() doesn't have attribute batch_norm", "oh, my bad. So we need update document or support the argument, right? ", "Yup. I think the argument needs to be supported. It is defined and there is code which gets executed when this boolean value is True. It's not being read properly somewhere. I didn't debug where exactly. I can try that\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/estimator/canned/dnn.py", "cc @ispirmustafa ", "The `batch_norm` parameter was added in 1.10, which is why you get the error in 1.9. So I'm closing this issue.\r\n\r\nAs for the \"bitwise cannot be imported\" issue, can you file a new issue with the issue template filled out? Thank you."]}, {"number": 21561, "title": "Sorry for the code smell.", "body": "I don't have time to maintain this codebase anymore, but word got to me through the grapevine that my framework was competing with yours. I just hope you can look at what's good in TensorBox and learn from it.", "comments": ["It'll be hard for us to sift through this without some clearer direction. If you have specific features you found are missing in TensorFlow it would be worthwhile to send more targeted PRs.\r\n\r\nI will close this one. It'll be here for inspiration and reference. Thank you!"]}, {"number": 21560, "title": "Fix typo: compatbility => compatibility", "body": "Fixes a typo that creeps up in many places: compatbility => compatibility\r\nThis makes some documentation pages look weird, such as: https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\r\n", "comments": ["Thanks, @ageron !", "Hi @caisq ,\r\nI'm not sure why two checks were not successful (GPU Python 3 and Ubuntu Python3 PIP), I can't access the logs.  I suspect it's unrelated to this PR since all I changed were typos in documentation, right?", "@ageron I think those are due to some flaky tools failure. Our infrastructure will test and sync the PR in google code base, and then merge this PR automatically.", "Thanks for your feedback and for merging @caisq. Cheers!"]}, {"number": 21559, "title": "Update Nesterov implementation docs", "body": " Clarification that this is a modified version of the algorithm which is only correct under certain conditions.\r\n\r\nFixes #19899", "comments": ["@accraze could you make sure that the lines are hard-wrapped to 80 chars and that there are no trailing spaces? Thanks!", "@martinwicke I fixed the line lengths, but it seems to have dismissed your review."]}, {"number": 21558, "title": "contrib/quantize vs Graph Transform Tool", "body": "Hi,\r\n\r\nI am wondering if the quantize_weights and quantize_nodes transforms in Graph Transform Tool use the same method as contrib/quantizei in this paper: https://arxiv.org/abs/1712.05877", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Hi @jonkoi \r\n\r\nThe contrib/quantize tool instruments the graph with FakeQuant operations and is the exact technique described in the paper you have attached. You can then convert the model to TensorFlow Lite to run full quantized inference.\r\n\r\nThe instructions to do this are here: https://www.tensorflow.org/performance/quantization and at the contrib/quantize docs (https://github.com/tensorflow/tensorflow/tree/r1.10/tensorflow/contrib/quantize)\r\n\r\nThe Graph Transform Tool doesn't support this. Further, quantization efforts are being spent more on TensorFlow Lite right now so we recommend using TFLite quantization tools (we will be releasing more as well).\r\n\r\nThanks!\r\n"]}, {"number": 21557, "title": "In DNNClassifier evaluation results, the 'loss' is not always = 'average_loss' * batch_size", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: yes\r\n- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34 1.10.0\r\n- **Python version**: Python 3.6.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nThe evaluation results of a `DNNClassifier` include a `'loss'` and an `'average_loss`'.  It seems that the `'average_loss'` is correct (same as when I compute it manually), while the `'loss'` is weird. \r\n It seems that it's roughly equal to the `'average_loss'` times the batch size, but not always. This seems like a bug to me.\r\n\r\nI wrote a test (see the code below), and here is its output with different batch sizes (notice that the loss/average_loss ratio is often equal or close to the batch size, but not always, and sometimes it is exactly one off, or simply far from it):\r\n\r\n```\r\nBatch size: 1 loss: 0.75871724 average_loss: 0.75871724 Ratio: 1.0\r\nBatch size: 11 loss: 8.33756 average_loss: 0.7587179 Ratio: 10.989012\r\nBatch size: 21 loss: 15.806628 average_loss: 0.75871813 Ratio: 20.833334\r\nBatch size: 31 loss: 22.991457 average_loss: 0.7587181 Ratio: 30.30303\r\nBatch size: 41 loss: 30.348719 average_loss: 0.75871795 Ratio: 40.0\r\nBatch size: 51 loss: 37.935898 average_loss: 0.75871795 Ratio: 50.0\r\nBatch size: 61 loss: 44.63047 average_loss: 0.758718 Ratio: 58.82353\r\nBatch size: 71 loss: 50.581203 average_loss: 0.7587181 Ratio: 66.666664\r\nBatch size: 81 loss: 58.36292 average_loss: 0.75871795 Ratio: 76.92307\r\nBatch size: 91 loss: 68.974365 average_loss: 0.758718 Ratio: 90.90909\r\nBatch size: 101 loss: 75.8718 average_loss: 0.758718 Ratio: 100.0\r\nBatch size: 111 loss: 75.871796 average_loss: 0.75871795 Ratio: 100.0\r\nBatch size: 121 loss: 84.302 average_loss: 0.758718 Ratio: 111.111115\r\nBatch size: 131 loss: 94.83975 average_loss: 0.758718 Ratio: 125.0\r\nBatch size: 141 loss: 94.83975 average_loss: 0.758718 Ratio: 125.0\r\n```\r\n\r\n### Source code / logs\r\n```python\r\nfrom __future__ import division, print_function, unicode_literals\r\n\r\nimport logging\r\nlogging.basicConfig(level=logging.WARNING) # or else it's too verbose\r\n\r\nimport tensorflow as tf\r\n#tf.enable_eager_execution()  # same exact output if you activate eager execution\r\nimport numpy as np\r\n\r\n# make the code reproducible (see https://youtu.be/Ys8ofBeR2kA ;-) )\r\nnp.random.seed(1234)\r\nconfig = tf.estimator.RunConfig(tf_random_seed=1234)\r\n\r\nnum_col = tf.feature_column.numeric_column(\"X\", shape=[10])\r\ndnn_clf = tf.estimator.DNNClassifier(hidden_units=[200, 100], n_classes=3,\r\n                                     feature_columns=[num_col], config=config)\r\n\r\n# some random data\r\nX = np.random.randn(1000, 10)\r\ny = np.random.randint(3, size=1000)\r\n\r\n# train the model\r\ninput_fn = tf.estimator.inputs.numpy_input_fn(\r\n    x={\"X\": X}, y=y, num_epochs=10, batch_size=32, shuffle=False)\r\ndnn_clf.train(input_fn=input_fn)\r\n\r\n# evaluate the model with different batch sizes\r\nfor batch_size in range(1, 151, 10):\r\n    input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"X\": X}, y=y, batch_size=batch_size, shuffle=False)\r\n    eval_results = dnn_clf.evaluate(input_fn=input_fn)\r\n    print(\"Batch size:\", batch_size, end=\" \")\r\n    print(\"loss:\", eval_results[\"loss\"], end=\" \")\r\n    print(\"average_loss:\", eval_results[\"average_loss\"], end=\" \")\r\n    print(\"Ratio:\", eval_results[\"loss\"] / eval_results[\"average_loss\"])\r\n```\r\n", "comments": ["average_loss is loss per example count. and loss is loss per mini batch. As you said, the ratio is roughly `batch_size`. It should be exactly `batch_size` if all `batch_size`s are equal for all mini batches. The given batch sizes in your test, make the last batch not a full one. That's the reason for difference.", "Oh okay, that's clear, thanks @ispirmustafa , I'll see if I can document this somewhere.  Cheers."]}, {"number": 21556, "title": "Fix url in automatic differentiation from eager notebook", "body": "Correct url for a link in automatic differentiation notebook.", "comments": []}, {"number": 21555, "title": "Better  tf.contrib.estimator.early_stopping (Feature request)", "body": "\r\n### System information\r\nN/A\r\n\r\n### Describe the problem\r\nThe current early stopping code doesn't take into account any trends in the loss, and therefore can be thrown off by a single low loss. It would be better to use an exponential moving average of the loss to compute the lowest loss, as that will be less likely to be thrown off by a single very low loss. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code: Yes\r\nOS Platform and Distribution: Linux \r\nTensorFlow installed from: pip, \r\nTensorFlow version: 1.8.0\r\nBazel version: 0.11.1\r\nCUDA/cuDNN version: 9.0\r\nGPU model and memory: GTX 950 M\r\nExact command to reproduce:N/A\r\nMobile deviceN/A\r\n\r\nIn tensorflows stop if no increase hook, https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/stop_if_no_increase_hook, I find it gives inconsistent results. It currently works by stopping if loss hasn't decreased for a certain number of steps. If there is a single very low loss, then the training will stop too early. What I found worked better was to maintain a running average of the loss, with and stop if that didn't increase beyond a certain number of steps, because that also takes into account trends in the loss. ", "We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!\r\n", "No, I don't have time to work on this now.\n\nOn Fri, Sep 14, 2018, 3:47 PM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21555#issuecomment-421345917>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AG0BV1XMJ-mFM4bvd4SWRg94hvarkQcqks5ua6VcgaJpZM4V5jvx>\n> .\n>\n", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "Hi, I would be happy to try implementing this.\r\nIs this still relevant? @facaiy @bignamehyp ", "It is still relevent. I would love if you would try implementing it.\n\nOn Sat, Nov 3, 2018, 7:23 PM Eyal Zakkay <notifications@github.com> wrote:\n\n> Hi, I would be happy to try implementing this.\n> Is this still relevant? @facaiy <https://github.com/facaiy> @bignamehyp\n> <https://github.com/bignamehyp>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21555#issuecomment-435605234>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AG0BV0dgoe0o2hdVAA224ls14ozTsHDmks5urdD7gaJpZM4V5jvx>\n> .\n>\n", "I would be happy to. However, I want to wait for a response from one of the TF members so I will know I'm not working on something irrelevant in the project's perspective...", "Contrib has been depreciated here in Tensorflow repo and moved to tensorflow/addons , please reopen the issue in https://github.com/tensorflow/addons/issues if this exists in latest version. Thank you"]}, {"number": 21554, "title": "Kernel Restarting The kernel appears to have died. It will restart automatically. importing tensorflow", "body": "hey comrades. am a new ubuntu user, i read most articles on how to install tensorflow in anaconda so that i can run it in the notebook.\r\ni am however facing this challenge which is disturbing having thought i've succeeded. and it's nagging me.\r\nthe moment i do \"import tensorflow as tf\", and upon running, i receive the following;\r\n_**Kernel Restarting\r\nThe kernel appears to have died. It will restart automatically.**_\r\ni am totally beaten. what should i do? any help is highly regarded.\r\nNote that my kernel works fine with other import of numpy, pandas,... etc. Issues are only available with import of tensorflow.\r\nthank you.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 18 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@yoasaine please do fill out the system information. We can't reproduce without it. \r\n\r\nAlso, does #9829 give any hints? ", "I got the same problem after installation, so I just updated my packages and it's now working.\r\nIf you are using anaconda you can simply do:  `conda update anaconda --yes`\r\nHope it helps.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I resolved this issue by upgrading numpy and importing numpy before tensorflow.  my numpy version was 1.15.4. Upgraded it  to  1.16.2.  "]}, {"number": 21553, "title": "Replacing tf.contrib.data.batch_and_drop_remainder by batch(..., drop\u2026", "body": "Replacing `tf.contrib.data.batch_and_drop_remainder` by `batch(..., drop_remainder=True)`. Also checkpointing at `(epoch + 1) % x` while saving the model to consider the last epoch's variables.", "comments": ["@allenlavoie @drpngx @yifeif ", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@drpngx Do you mind pulling this in? The variables are not being restired correctly. Issues will start cropping up \ud83d\ude05"]}, {"number": 21552, "title": "Adam: Fix latex text (docs).", "body": "Escape special character and add text section for comments.", "comments": ["Looks like #18168 messed up the formula a bit.", "Fixed sqrt in lr formula, and updated various files with adam docs.", "Should the variable names correspond to the ones used in the API?\r\nE.g. `beta_1`should be `beta1`?", "@MarkDaoust , @martinwicke care to review/merge?", "Thanks @MarkDaoust !"]}, {"number": 21551, "title": "Remove the extra clip_by_value created by keras.layers.ReLU", "body": "When `max_value` of `tf.keras.layers.ReLU.__init__` is None, [`self.max_value = cast_to_floatx(max_value)` will set `self.max_value` to `nan`](https://github.com/tensorflow/tensorflow/blob/0cf2c612e5e6ff8c5026011e8186056801def747/tensorflow/python/keras/layers/advanced_activations.py#L318). This `nan self.max_value` results in an extra `tf.clip_by_value(..., clip_value_max=nan)` following `tf.nn.relu`.\r\n", "comments": ["This has been fixed independently in a separate commit. Thanks for the PR regardless, it was a useful fix."]}, {"number": 21550, "title": "Horrible", "body": "Really a waste of time. Installing all kind of s**t, winpython, anaconda, then, finally, checking that tensorflow doesnt work with 32bit systems. Why cant tensorflow write:\r\n\"Only 64 bit supported!!\"", "comments": ["This is mentioned on both https://www.tensorflow.org/install/install_windows and https://www.tensorflow.org/install/install_sources\r\n\r\nIf you have a suggestion to make that more prominent, we'd be happy to hear. "]}, {"number": 21549, "title": "tf.contrib.ffmpeg.decode_video Error", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**:  2.7\r\n- **GCC/Compiler version (if compiling from source)**: 4.8\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: Titan 1080 Ti\r\n\r\n### Describe the problem\r\nI try to use tf.contrib.ffmpeg.decode_video to decode *avi video, but i got errors when processing some videos.\r\n\r\n### Source code / logs\r\n\r\nSource code:\r\n\r\nwith tf.Session() as sess:\r\n    for pth in pths:\r\n        print(pth)\r\n        vids = tf.contrib.ffmpeg.decode_video(tf.read_file(pth))\r\n        np_frames = sess.run(vids)\r\n\r\n\r\nLog Information:\r\n\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-3-525c5b7a8b81> in <module>()\r\n      5         print(pth)\r\n      6         vids = tf.contrib.ffmpeg.decode_video(tf.read_file(pth))\r\n----> 7         np_frames = sess.run(vids)\r\n      8         im_shapes += [np_frames.shape]\r\n\r\n/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    875     try:\r\n    876       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 877                          run_metadata_ptr)\r\n    878       if run_metadata:\r\n    879         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1098     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1099       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1100                              feed_dict_tensor, options, run_metadata)\r\n   1101     else:\r\n   1102       results = []\r\n\r\n/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1270     if handle is None:\r\n   1271       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1272                            run_metadata)\r\n   1273     else:\r\n   1274       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1289         except KeyError:\r\n   1290           pass\r\n-> 1291       raise type(e)(node_def, op, message)\r\n   1292 \r\n   1293   def _extend_graph(self):\r\n\r\nUnknownError: Output created by FFmpeg [967449600] does not match description [100, 608, 1360, 3]\r\n\t [[Node: DecodeVideo_5 = DecodeVideo[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReadFile_5)]]\r\n\r\nCaused by op u'DecodeVideo_5', defined at:\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\r\n    self.io_loop.start()\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tornado/ioloop.py\", line 1064, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-525c5b7a8b81>\", line 6, in <module>\r\n    vids = tf.contrib.ffmpeg.decode_video(tf.read_file(pth))\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/contrib/ffmpeg/ffmpeg_ops.py\", line 108, in decode_video\r\n    return gen_decode_video_op_py.decode_video(contents)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/contrib/ffmpeg/ops/gen_decode_video_op_py.py\", line 46, in decode_video\r\n    \"DecodeVideo\", contents=contents, name=name)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\r\n    op_def=op_def)\r\n  File \"/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nUnknownError (see above for traceback): Output created by FFmpeg [967449600] does not match description [100, 608, 1360, 3]\r\n\t [[Node: DecodeVideo_5 = DecodeVideo[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReadFile_5)]]\r\n\r\n", "comments": ["@Flowerfan Would you mind to provide one video samples that trigger the issue described?", "\r\n[VIRAT_S_000000_0023.avi.zip](https://github.com/tensorflow/tensorflow/files/2280711/VIRAT_S_000000_0023.avi.zip)\r\n@yongtang  I uploaded the video.\r\n\r\nBelow is the video info using ffprobe\r\nffprobe version 2.8.14-0ubuntu0.16.04.1 Copyright (c) 2007-2018 the FFmpeg developers\r\n  built with gcc 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.9) 20160609\r\n  configuration: --prefix=/usr --extra-version=0ubuntu0.16.04.1 --build-suffix=-ffmpeg --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --cc=cc --cxx=g++ --enable-gpl --enable-shared --disable-stripping --disable-decoder=libopenjpeg --disable-decoder=libschroedinger --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzvbi --enable-openal --enable-opengl --enable-x11grab --enable-libdc1394 --enable-libiec61883 --enable-libzmq --enable-frei0r --enable-libx264 --enable-libopencv\r\n  libavutil      54. 31.100 / 54. 31.100\r\n  libavcodec     56. 60.100 / 56. 60.100\r\n  libavformat    56. 40.101 / 56. 40.101\r\n  libavdevice    56.  4.100 / 56.  4.100\r\n  libavfilter     5. 40.101 /  5. 40.101\r\n  libavresample   2.  1.  0 /  2.  1.  0\r\n  libswscale      3.  1.101 /  3.  1.101\r\n  libswresample   1.  2.101 /  1.  2.101\r\n  libpostproc    53.  3.100 / 53.  3.100\r\nInput #0, avi, from '/home/flowerfan/data/cubes/gt_train/VIRAT_S_000000/VIRAT_S_000000_0023.avi':\r\n  Metadata:\r\n    encoder         : Lavf56.40.101\r\n  Duration: 00:00:13.00, start: 0.000000, bitrate: 1000 kb/s\r\n    Stream #0:0: Video: h264 (High) (H264 / 0x34363248), yuv420p, 1360x608, 993 kb/s, 30 fps, 30 tbr, 30 tbn, 60 tbc\r\n", "@Flowerfan The tf.contrib.ffmpeg in tensorflow repo has been deprecated and will be removed in 2.0 (soon to be released). The video decoding functionality support is now in `tensorflow/io` repo:\r\n\r\nhttps://github.com/tensorflow/io\r\n\r\nAlso google group discussion: https://groups.google.com/a/tensorflow.org/forum/#!forum/io\r\n\r\nI tried your sample video with `tensorflow-io`, everything seems to be fine.\r\n\r\nI will close this issue for now. But if you continue to encounter issues with the video clip when using `tensorflow-io`, you can create an issue in https://github.com/tensorflow/io"]}]