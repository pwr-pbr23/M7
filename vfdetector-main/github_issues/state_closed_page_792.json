[{"number": 29783, "title": "Lazy ExponentialMovingAverage for embedding matrix with sparse gradients", "body": "Currently ExponentialMovingAverage is only designed for small Variables without sparse gradient(https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/training/moving_averages.py#L89). \r\n\r\nDirectly calling \"ema.apply(trainable_vars)\" for embedding matrix is significantly slow. Would anyone provide a solution for sparse gradient in somewhat lazy manner?\r\n\r\nSimilar questions: https://github.com/pytorch/pytorch/issues/1285\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 29782, "title": "How to build Dockerfile.ubuntu  for tensorflow-gpu", "body": "How to build Dockerfile.ubuntu  for tensorflow-gpu\r\nubuntu16.04\r\ntensorflow-gpu1.12.0\r\n\r\nI have  nvidia-docker  Dockerfile.ubuntu", "comments": ["@mk123qwe Please find instructions in this [link](https://www.tensorflow.org/install/docker) which help you to install Tensorflow from docker. Let us know if that solve your problem. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29781, "title": "[TF 2.0] tf.hessians", "body": "**System information**\r\n- TensorFlow version: '2.0.0-dev20190612'\r\n- Python version: 3.6.7\r\n\r\n**Describe the current behavior**\r\nWhen you run tf.hessians you get a not supported when eager execution is enabled. Use tf.GradientTape instead Error but Hessians is not implemented in GradientTape.\r\n\r\n**Describe the expected behavior**\r\nI am linking the TF 2.0 API doc issue which was opened to fix the API doc to specify you can't use the tf.hessians with eager mode. (https://github.com/tensorflow/tensorflow/issues/29271). So just wanted to make sure someone is working on a Hessians attribute for GradientTape.\r\n\r\n**Code to reproduce the issue**\r\nIn Eager Mode:\r\n```\r\nf = 100*(y - x**2)**2 + (1 - x)**2\r\nx = tf.Variable([1., 1.])\r\nhessian_matrix = tf.hessians(f, x)\r\n\r\nRuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\r\n```\r\n\r\nTrying with GradientTape:\r\n\r\n```\r\nwith tf.GradientTape() as t:\r\n    out = f\r\ndy_dx = t.hessians(out, x)\r\n\r\nAttributeError: 'GradientTape' object has no attribute 'hessians'\r\n```\r\n\r\n", "comments": ["Is there anyone looking at this? Please support `tf.hessains` in eager mode or add `.hessians` to `tf.GradientTape`", "Have tried on Colab with TF 2.0 and was able to get mentioned error in both the cases.", "@mukeshmithrakumar @JelleAalbers\r\nA temp workaround is \r\n```\r\nimport tensorflow as tf\r\n\r\nv = tf.Variable(tf.random.normal((2,)))\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    y = tf.sin(v[0] + tf.cos(v[1]))\r\n    grads = tape.gradient(y, [v])\r\n\r\nhessians = tape.gradient(grads[0], [v])\r\n```\r\nlook forward to an official version of `tf.GradientTape.hessians`", "thanks @zakizhou, you can also get the gradient of the Jacobian, would give the same answer as above", "@agarwal-ashish can we add .hessians and .batch_hessians to the tape, implemented in terms of .jacobians and .batch_jacobians?", "Probably makes sense to finish eager-friendly forward-mode autodiff and then do forward-over-reverse for Hessians, which will be more efficient for most networks. I'm working on that at the moment, but I'll keep this in mind as a use-case.", "@zakizhou with your code I get a (2,1) tensor for `hessians = tape.gradient(grads[0], [v])`. Were you getting a (2,2) square?\r\n\r\nI looked into it further and it looks like it's returning the row-wise sum of the actual hessian:\r\n\r\n```\r\ntf.random.set_seed(0)\r\nv = tf.Variable(tf.random.normal((2,)))\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    y = tf.sin(v[0] + tf.cos(v[1]))\r\n    grad = tape.gradient(y, v)\r\n    hessian = tape.gradient(grad, v)\r\n    hessian0 = tape.gradient(grad[0], v)\r\n    hessian1 = tape.gradient(grad[1], v)\r\n\r\nprint(\"hessian:\", hessian)\r\nprint(\"hessian0:\", hessian0)\r\nprint(\"hessian1:\", hessian1)\r\nprint(\"hessian0 + hessian1:\", hessian0 + hessian1)\r\n```\r\nwhich prints\r\n```\r\nhessian: tf.Tensor([-0.38815078  0.8456935 ], shape=(2,), dtype=float32)\r\nhessian0: tf.Tensor([-0.65835893  0.27020815], shape=(2,), dtype=float32)\r\nhessian1: tf.Tensor([0.27020815 0.5754854 ], shape=(2,), dtype=float32)\r\nhessian0 + hessian1: tf.Tensor([-0.38815078  0.8456936 ], shape=(2,), dtype=float32)\r\n```\r\nas you can see `hessian` == `hessian0 + hessian1`\r\n\r\nany tips on how to get the full (2, 2) result without this indexing? should i be reshaping my `Variable` somehow?\r\n\r\n\r\nedit: \r\n```\r\nPython 3.7.2\r\ntensorflow==2.0.0\r\ntensorflow-estimator==2.0.0\r\ntensorflow-probability==0.8.0\r\n```\r\n", "This works for me:\r\n\r\n```\r\ndef get_my_hessian(f, x):\r\n    with tf.GradientTape(persistent=True) as hess_tape:\r\n        with tf.GradientTape() as grad_tape:\r\n            y = f(x)\r\n        grad = grad_tape.gradient(y, x)\r\n        grad_grads = [hess_tape.gradient(g, x) for g in grad]\r\n    hess_rows = [gg[tf.newaxis, ...] for gg in grad_grads]\r\n    hessian = tf.concat(hess_rows, axis=0)\r\n    return hessian\r\n\r\ntf.random.set_seed(0)\r\nx = tf.Variable(tf.random.normal((2,)))\r\n\r\nprint(get_my_hessian(f, x))\r\n```\r\n```\r\ntf.Tensor(\r\n[[-0.65835893  0.27020815]\r\n [ 0.27020815  0.5754854 ]], shape=(2, 2), dtype=float32)\r\n```\r\nit redundantly computes the non-diagonals, but i think it's the hessian!", "> @mukeshmithrakumar @JelleAalbers\r\n> A temp workaround is\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> \r\n> v = tf.Variable(tf.random.normal((2,)))\r\n> with tf.GradientTape(persistent=True) as tape:\r\n>     y = tf.sin(v[0] + tf.cos(v[1]))\r\n>     grads = tape.gradient(y, [v])\r\n> \r\n> hessians = tape.gradient(grads[0], [v])\r\n> ```\r\n> \r\n> look forward to an official version of `tf.GradientTape.hessians`\r\n\r\nhessians = tape.jacobian(grads[0], v) gives you the full hessian and will be faster than the iterative version by virtue of using vectorized_map for vectorization.\r\n\r\n", "@agarwal-ashish thank you! that's exactly what i need. i tried using the jacobian earlier, per @mukesh104 , but must have missed something (maybe `persistent=True`). thanks again.\r\n\r\nfor those like me who could use a refresher, the hessian of f is the jacobian of the gradient of f, transposed. if the function is twice continuously differentiable, then the hessian is symmetric and therefore the transpose can be dropped. (source, https://en.wikipedia.org/wiki/Hessian_matrix#Definitions_and_properties via https://math.stackexchange.com/a/2437889/124047)\r\n", "hey sorry @grisaitis , I should have been more clear, the gradient of the Jacobian will give you a row-wise sum and yeah to get the correct hessian matrix you will have to take the transpose of the resulting matrix.", "Are there any news on this?", "I wanted to use this for models where model.trainable_variables is a list of Variable Tensors. How do I modify this workaround?\r\n\r\n```\r\nimport tensorflow as tf\r\nv = tf.Variable([0],dtype=\"float32\")\r\nw = tf.Variable([0],dtype=\"float32\")\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    y = tf.sin(v + tf.cos(w))\r\n    grads = tape.gradient(y, [v,w])\r\n\r\nhessian0 = tape.jacobian(grads[0], [v,w])\r\nhessian1 = tape.jacobian(grads[1], [v,w])\r\n```\r\n\r\nI get the correct values but for this seems not the best solution for big Neural Networks. How does one typically do this?", "tape.batch_jacobian(grads, [v, w])?\n\nOn Wed, Apr 1, 2020 at 10:50 AM Korbinian Kottmann <notifications@github.com>\nwrote:\n\n> I wanted to use this for models where model.trainable_variables is a list\n> of Variable Tensors. How do I modify this workaround?\n>\n> import tensorflow as tf\n> v = tf.Variable([0],dtype=\"float32\")\n> w = tf.Variable([0],dtype=\"float32\")\n>\n> with tf.GradientTape(persistent=True) as tape:\n>     y = tf.sin(v + tf.cos(w))\n>     grads = tape.gradient(y, [v,w])\n>\n> hessian0 = tape.jacobian(grads[0], [v,w])\n> hessian1 = tape.jacobian(grads[1], [v,w])\n>\n> I get the correct values but for this seems not the best solution for big\n> Neural Networks. How does one typically do this?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/29781#issuecomment-607397600>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJXEXQBU35Y6Q3KGVLRKN5GRANCNFSM4HYEVW3A>\n> .\n>\n\n\n-- \n - Alex\n", "Isnt batch_jacobian for when target_i and input_j are independent? (which is not the case in the example, right?)\r\nAnyway, this produces:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-1a760699feac> in <module>\r\n      8     grads = tape.gradient(y, [v,w])\r\n      9 \r\n---> 10 hessian = tape.batch_jacobian(grads, [v,w])\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in batch_jacobian(self, target, source, unconnected_gradients, parallel_iterations, experimental_use_pfor)\r\n   1198         dimension of `target` and `source` do not match.\r\n   1199     \"\"\"\r\n-> 1200     target_shape = target.shape\r\n   1201     if target_shape.rank is None:\r\n   1202       dim = tensor_shape.Dimension(None)\r\n\r\nAttributeError: 'list' object has no attribute 'shape'\r\n```", "How about tape.batch_jacobian(tf.concat(grads, axis=1), [v, w]) ? \r\n", "```\r\nimport tensorflow as tf\r\n\r\nv = tf.Variable([0],dtype=\"float32\")\r\nw = tf.Variable([0],dtype=\"float32\")\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    y = tf.sin(v + tf.cos(w))\r\n    grads = tape.gradient(y, [v,w])\r\n\r\nhessian = tape.batch_jacobian(tf.concat(grads, axis=0), [v, w])\r\n```\r\nproduces \r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-15-5f54f273b24b> in <module>\r\n      8     grads = tape.gradient(y, [v,w])\r\n      9 \r\n---> 10 hessian = tape.batch_jacobian(tf.concat(grads, axis=0), [v, w])\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in batch_jacobian(self, target, source, unconnected_gradients, parallel_iterations, experimental_use_pfor)\r\n   1203     else:\r\n   1204       dim = target_shape.dims[0]\r\n-> 1205     if not (target_shape.with_rank_at_least(2) and\r\n   1206             source.shape.with_rank_at_least(2) and\r\n   1207             dim.is_compatible_with(source.shape[0])):\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py in with_rank_at_least(self, rank)\r\n   1025     \"\"\"\r\n   1026     if self.rank is not None and self.rank < rank:\r\n-> 1027       raise ValueError(\"Shape %s must have rank at least %d\" % (self, rank))\r\n   1028     else:\r\n   1029       return self\r\n\r\nValueError: Shape (2,) must have rank at least 2\r\n```\r\n\r\nand `tape.batch_jacobian(tf.concat(grads, axis=1), [v, w])` is expecting axis in [-1,1).\r\n\r\n`grads` in this example code is a list of two (1,) Tensor.", "Can you use tf.stack instead of concat ?", "Workaround (but I wouldnt believe this was intended like this)\r\n```\r\nimport tensorflow as tf\r\n\r\nv = tf.Variable([0],dtype=\"float32\")\r\nw = tf.Variable([0],dtype=\"float32\")\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    y = tf.sin(v + tf.cos(w))\r\n    grads = tape.gradient(y, [v,w])\r\n\r\nhessian = [tape.jacobian(grad, [v, w]) for grad in grads]\r\n```\r\n\r\nBecause no matter which one, both tf.concat or tf.stack erase the context information of the grad objects (not sure if this is correct terminology, but tape is not active anymore afterwards)\r\n`hessian = [tape.jacobian(grad, [v, w]) for grad in grads]` produces [None,None]\r\n\r\nMaybe worth another ticket: When using this for fully connected models it works, for models with convolutional layers I have to put `jacobian(.., experimental_use_pfor=False)` which seems to be less optimal. Should I open a ticket?\r\n\r\nIs there any news on the original issue, tf.hessian implementation within tf.GradientTape?", "Sorry, the stacking or concating has to happen within the tape scope for it\nto work. Your solution is good though.\n\nOn Thu, Apr 9, 2020 at 5:46 AM Korbinian Kottmann <notifications@github.com>\nwrote:\n\n> Workaround (but I wouldnt believe this was intended like this)\n>\n> import tensorflow as tf\n>\n> v = tf.Variable([0],dtype=\"float32\")\n> w = tf.Variable([0],dtype=\"float32\")\n>\n> with tf.GradientTape(persistent=True) as tape:\n>     y = tf.sin(v + tf.cos(w))\n>     grads = tape.gradient(y, [v,w])\n>\n> hessian = [tape.jacobian(grad, [v, w]) for grad in grads]\n>\n> Because no matter which one, both tf.concat or tf.stack erase the context\n> information of the grad objects (not sure if this is correct terminology,\n> but tape is not active anymore afterwards)\n> hessian = [tape.jacobian(grad, [v, w]) for grad in grads] produces\n> [None,None]\n>\n> Maybe worth another ticket: When using this for fully connected models it\n> works, for models with convolutional layers I have to put jacobian(..,\n> experimental_use_pfor=False) which seems to be less optimal. Should I\n> open a ticket?\n>\n> Is there any news on the original issue, tf.hessian implementation within\n> tf.GradientTape?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/29781#issuecomment-611508273>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPU3VE6NKA3I7PZA4TRLW7UDANCNFSM4HYEVW3A>\n> .\n>\n\n\n-- \n - Alex\n", "That creates a new tape in every loop iteration, and so only computes the\nhessian wrt the last iteration. Instead put the whole loop inside a\nsingle tf.gradienttape.\n\nOn Mon, May 4, 2020 at 3:46 AM Korbinian Kottmann <notifications@github.com>\nwrote:\n\n> If my forward pass is too big to be done in one pass but needs to be\n> serialized in batches (as is the case for big training sets), can I put the\n> GradientTape inside a (for) loop like this?\n>\n> import tensorflow as tf\n>\n> v = tf.Variable([1,1],dtype=\"float32\")\n> ws = tf.constant([[0,0],[1,1],[2,2]], dtype=\"float32\")\n> grad = tf.cast(0,dtype=\"float32\")\n>\n> for w in ws:\n>     with tf.GradientTape(persistent=True) as tape:\n>         tape.watch(v)\n>         y = tf.sin(tf.tensordot(ws,v,axes=1))\n>         grad += tape.gradient(y, v)\n>\n> hessian = tape.jacobian(grad, v)\n>\n> Or is it that in each iteration in the loop the previous tape is\n> overwritten?\n> Reversing for- and with- statements makes it very slow.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/29781#issuecomment-623392052>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPPFQNXJGVBMJTIVATRP2MI3ANCNFSM4HYEVW3A>\n> .\n>\n\n\n-- \n - Alex\n", "Here is a workaround function based on the ones above. If you give it a model, the input data, and a loss function, it will compute the Hessian of the model with respect to the weights - w x w, where w is the total number of model weights.\r\n\r\n```\r\ndef get_hessian(model, x, y, loss_fn):\r\n    with tf.GradientTape(persistent=True) as tape:\r\n        preds = model(x)\r\n        loss = loss_fn(y, preds)\r\n        grads = tape.gradient(loss, model.trainable_weights)\r\n        flattened_grads = tf.concat([tf.reshape(grad, [-1]) for grad in grads], axis=0)\r\n    hessians = tape.jacobian(flattened_grads, model.trainable_weights)\r\n    flattened_hessians = tf.concat([tf.reshape(hess, [hess.shape[0], -1]) for hess in hessians], 1)\r\n    return flattened_hessians\r\n```", "@mukeshmithrakumar I support the question. For the most of cases, we will compute gradient and hessian together. Considering that we are using bfgs algorithm to iterate, we have to pass jacobian and hessian simultaneously. GradientTape api is not handy to do that. ", "Do the code snippets for hessian, as illustrated on this thread, not work for your use case ? ", "@mukeshmithrakumar,\r\nCan you please respond to the above comment? Thanks! ", "Hey @agarwal-ashish and @rmothukuru sorry about the late reply, the illustration works for my use case, my use case was just a simple example of how to calculate a hessian matrix but I agree with @yiakwy-enterprise-roborock , for certain use cases GradientTape api is not convenient", "Closing the issue as it has been resolved. Thanks!"]}, {"number": 29780, "title": "a4b3c41 breaks 2019-06-13 nightly build", "body": "The following part of CL a4b3c41775b52d5a2d3d10c8f2f46f645e6f9a5d:\r\n\r\n```\r\ndiff --git a/tensorflow/compiler/xla/debug_options_flags.cc b/tensorflow/compiler/xla/debug_options_flags.cc\r\nindex 5238b01075..8212c1e9b4 100644\r\n--- a/tensorflow/compiler/xla/debug_options_flags.cc\r\n+++ b/tensorflow/compiler/xla/debug_options_flags.cc\r\n@@ -17,6 +17,10 @@ limitations under the License.\r\n\r\n #include <mutex>  // NOLINT(build/c++11): only using std::call_once, not mutex.\r\n #include <vector>\r\n+\r\n+#include \"absl/container/flat_hash_map.h\"\r\n+#include \"absl/container/node_hash_map.h\"\r\n+#include \"absl/strings/str_format.h\"\r\n #include \"absl/strings/str_split.h\"\r\n #include \"tensorflow/compiler/xla/debug_options_parsers.h\"\r\n #include \"tensorflow/compiler/xla/parse_flags_from_env.h\"\r\n@@ -58,9 +62,42 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\r\n   return opts;\r\n }\r\n\r\n+static std::once_flag flags_init;\r\n static DebugOptions* flag_values;\r\n static std::vector<tensorflow::Flag>* flag_objects;\r\n-static std::once_flag flags_init;\r\n+\r\n+// Maps pass -> initial fuel values (parsed when AllocateFlags was run).\r\n+static absl::flat_hash_map<string, int64>* initial_fuel;\r\n+\r\n+// Maps pass -> whether fuel was ever consumed for that pass.\r\n+static absl::node_hash_map<string, std::atomic<bool>>* fuel_ever_consumed;\r\n+\r\n+// Maps pass -> remaining fuel.\r\n+//\r\n+// All threads start off using this global fuel pool, but ResetThreadLocalFuel()\r\n+// switches them to a thread-local fuel pool.\r\n+static absl::node_hash_map<string, std::atomic<int64>>* global_fuel;\r\n+\r\n+// If we're using thread-local fuel, this stores it.\r\n+static thread_local absl::optional<\r\n+    absl::node_hash_map<string, std::atomic<int64>>>\r\n+    thread_fuel;  // NOLINT (global variable with nontrivial destructor)\r\n+\r\n+// Logs a warning if a pass's fuel was never consumed, on the theory that this\r\n+// may be a typo in the flag value.  Called atexit.\r\n+static void WarnIfFuelWasNeverConsumed() {\r\n+  CHECK(fuel_ever_consumed != nullptr);\r\n+  for (const auto& kv : *fuel_ever_consumed) {\r\n+    absl::string_view pass = kv.first;\r\n+    bool was_consumed = kv.second;\r\n+    if (!was_consumed) {\r\n+      LOG(ERROR) << absl::StreamFormat(\r\n+          \"Compiler fuel for \\\"%s\\\" was never consumed. This may be a typo in \"\r\n+          \"the --xla_fuel flag you passed.\",\r\n+          pass);\r\n+    }\r\n+  }\r\n+}\r\n\r\n // Allocates flag_values and flag_objects; this function must not be called more\r\n // than once - its call done via call_once.\r\n```\r\n\r\nBreaks nightly build as `std::atomic<int64>` cannot be used in `absl::flat_hash_map`:\r\n\r\n```\r\nERROR: /tensorflow_src/tensorflow/compiler/xla/BUILD:840:1: C++ compilation of rule '//tensorflow/compiler/xla:debug_options_flags' failed (Exit 1)\r\nIn file included from tensorflow/compiler/xla/debug_options_flags.cc:25:0:\r\n./tensorflow/compiler/xla/debug_options_parsers.h: In function 'bool xla::parse_xla_reduce_precision_option(xla::HloReducePrecisionOptions*, std::__cxx11::string)':\r\n./tensorflow/compiler/xla/debug_options_parsers.h:108:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < HloOpcodeCount(); i++) {\r\n                     ~~^~~~~~~~~~~~~~~~~~\r\n./tensorflow/compiler/xla/debug_options_parsers.h:115:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       for (int i = 0; i < HloOpcodeCount(); i++) {\r\n                       ~~^~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/include/x86_64-linux-gnu/c++/6/bits/c++allocator.h:33:0,\r\n                 from /usr/include/c++/6/bits/allocator.h:46,\r\n                 from /usr/include/c++/6/vector:61,\r\n                 from ./tensorflow/compiler/xla/debug_options_flags.h:19,\r\n                 from tensorflow/compiler/xla/debug_options_flags.cc:16:\r\n/usr/include/c++/6/ext/new_allocator.h: In instantiation of 'void __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> >; _Args = {const std::pair<const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::atomic<long long int> >&}; _Tp = std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> >]':\r\nexternal/com_google_absl/absl/memory/memory.h:574:5:   required from 'static decltype (a.construct((forward<Args>)(absl::allocator_traits::construct_impl::args)...)) absl::allocator_traits<Alloc>::construct_impl(int, A&, Args&& ...) [with A = std::allocator<std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> > >; Args = {std::pair<const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::atomic<long long int> >*&, const std::pair<const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::atomic<long long int> >&}; Alloc = std::allocator<std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> > >; decltype (a.construct((forward<Args>)(absl::allocator_traits::construct_impl::args)...)) = void]'\r\nexternal/com_google_absl/absl/memory/memory.h:536:19:   required from 'static void absl::allocator_traits<Alloc>::construct(Alloc&, T*, Args&& ...) [with T = std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> >; Args = {const std::pair<const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::atomic<long long int> >&}; Alloc = std::allocator<std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> > >]'\r\nexternal/com_google_absl/absl/container/node_hash_map.h:540:49:   required from 'static absl::container_internal::NodeHashMapPolicy<Key, Value>::value_type* absl::container_internal::NodeHashMapPolicy<Key, Value>::new_element(Allocator*, Args&& ...) [with Allocator = std::allocator<std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> > >; Args = {const std::pair<const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::atomic<long long int> >&}; Key = std::__cxx11::basic_string<char>; Value = std::atomic<long long int>; absl::container_internal::NodeHashMapPolicy<Key, Value>::value_type = std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> >]'\r\nexternal/com_google_absl/absl/container/internal/node_hash_policy.h:54:32:   required from 'static void absl::container_internal::node_hash_policy<Reference, Policy>::construct(Alloc*, typename std::remove_cv<typename std::remove_reference<_From>::type>::type**, Args&& ...) [with Alloc = std::allocator<std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> > >; Args = {const std::pair<const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::atomic<long long int> >&}; Reference = std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> >&; Policy = absl::container_internal::NodeHashMapPolicy<std::__cxx11::basic_string<char>, std::atomic<long long int> >; absl::container_internal::node_hash_policy<Reference, Policy>::slot_type = std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> >*; typename std::remove_cv<typename std::remove_reference<_From>::type>::type = std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> >]'\r\nexternal/com_google_absl/absl/container/internal/hash_policy_traits.h:76:22:   required from 'static void absl::container_internal::hash_policy_traits<Policy, <template-parameter-1-2> >::construct(Alloc*, absl::container_internal::hash_policy_traits<Policy, <template-parameter-1-2> >::slot_type*, Args&& ...) [with Alloc = std::allocator<std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> > >; Args = {const std::pair<const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::atomic<long long int> >&}; Policy = absl::container_internal::NodeHashMapPolicy<std::__cxx11::basic_string<char>, std::atomic<long long int> >; <template-parameter-1-2> = void; absl::container_internal::hash_policy_traits<Policy, <template-parameter-1-2> >::slot_type = std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> >*]'\r\nexternal/com_google_absl/absl/container/internal/raw_hash_set.h:1698:28:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\nexternal/com_google_absl/absl/container/internal/raw_hash_set.h:815:49:   required from 'absl::container_internal::raw_hash_set<Policy, Hash, Eq, Alloc>::raw_hash_set(const absl::container_internal::raw_hash_set<Policy, Hash, Eq, Alloc>&) [with Policy = absl::container_internal::NodeHashMapPolicy<std::__cxx11::basic_string<char>, std::atomic<long long int> >; Hash = absl::container_internal::StringHash; Eq = absl::container_internal::StringHashEq::Eq; Alloc = std::allocator<std::pair<const std::__cxx11::basic_string<char>, std::atomic<long long int> > >]'\r\nexternal/com_google_absl/absl/container/internal/raw_hash_map.h:29:7:   required from 'struct std::is_trivially_copy_constructible<absl::node_hash_map<std::__cxx11::basic_string<char>, std::atomic<long long int> > >'\r\nexternal/com_google_absl/absl/meta/type_traits.h:366:54:   required from 'constexpr const bool absl::is_trivially_copy_constructible<absl::node_hash_map<std::__cxx11::basic_string<char>, std::atomic<long long int> > >::compliant'\r\nexternal/com_google_absl/absl/meta/type_traits.h:368:27:   required from 'struct absl::is_trivially_copy_constructible<absl::node_hash_map<std::__cxx11::basic_string<char>, std::atomic<long long int> > >'\r\nexternal/com_google_absl/absl/types/internal/optional.h:173:72:   required from 'class absl::optional<absl::node_hash_map<std::__cxx11::basic_string<char>, std::atomic<long long int> > >'\r\ntensorflow/compiler/xla/debug_options_flags.cc:84:5:   required from here\r\n/usr/include/c++/6/ext/new_allocator.h:120:4: error: use of deleted function 'constexpr std::pair<_T1, _T2>::pair(const std::pair<_T1, _T2>&) [with _T1 = const std::__cxx11::basic_string<char>; _T2 = std::atomic<long long int>]'\r\n  { ::new((void *)__p) _Up(std::forward<_Args>(__args)...); }\r\n    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/include/c++/6/bits/stl_algobase.h:64:0,\r\n                 from /usr/include/c++/6/vector:60,\r\n                 from ./tensorflow/compiler/xla/debug_options_flags.h:19,\r\n                 from tensorflow/compiler/xla/debug_options_flags.cc:16:\r\n/usr/include/c++/6/bits/stl_pair.h:288:17: note: 'constexpr std::pair<_T1, _T2>::pair(const std::pair<_T1, _T2>&) [with _T1 = const std::__cxx11::basic_string<char>; _T2 = std::atomic<long long int>]' is implicitly deleted because the default definition would be ill-formed:\r\n       constexpr pair(const pair&) = default;\r\n                 ^~~~\r\n/usr/include/c++/6/bits/stl_pair.h:288:17: error: use of deleted function 'std::atomic<long long int>::atomic(const std::atomic<long long int>&)'\r\nIn file included from external/protobuf_archive/src/google/protobuf/io/coded_stream.h:113:0,\r\n                 from bazel-out/host/bin/tensorflow/compiler/xla/xla.pb.h:23,\r\n                 from ./tensorflow/compiler/xla/debug_options_flags.h:22,\r\n                 from tensorflow/compiler/xla/debug_options_flags.cc:16:\r\n/usr/include/c++/6/atomic:692:7: note: declared here\r\n       atomic(const atomic&) = delete;\r\n       ^~~~~~\r\n```\r\n\r\nI am sure someone in the team notices this; I just need a marker so I will know when it is fixed.", "comments": ["Gently ping @jlebar ", "Also, ping @CJ-Johnson to confirm. Maybe it's related to my specific compiler version (GCC 6.3) though.", "> I am sure someone in the team notices this\r\n\r\nI am not so sure!  This is passing all of our tests.\r\n\r\nLike you, I suspect it is related to your compiler version (specifically your standard library version).\r\n\r\nIf there's a fix that isn't too ugly I'd probably accept it.  You're probably going to have to write it, though, because I cannot test it myself.  Also just as a general thing, patch authors cannot be responsible for supporting every C++ compiler/stdlib that someone else might try to use.  (Of course TF doesn't have a written policy on what we *do* support.  But that's a hill to die on for another day...)\r\n\r\nAnother option would be to compile with clang; you're given an option when you run configure.py.  That should work, and if it does not, I am probably willing and able to fix that or rope in someone who can.", "Oh, I may be confused.  You're saying it breaks the (official?) nightly build, but below you're saying that this may be related to *your own* compiler version?", "> Oh, I may be confused. You're saying it breaks the (official?) nightly build, but below you're saying that this may be related to _your own_ compiler version?\r\n\r\nIt breaks my in-house nightly build. I do not know if this breaks the official nightly build, as they are built at test-passing commits only (which I am not sure this CL is included or not). Ping @gunan to double check.", "> If there's a fix that isn't too ugly I'd probably accept it. You're probably going to have to write it, though, because I cannot test it myself. Also just as a general thing, patch authors cannot be responsible for supporting every C++ compiler/stdlib that someone else might try to use. (Of course TF doesn't have a written policy on what we _do_ support. But that's a hill to die on for another day...)\r\n> \r\n> Another option would be to compile with clang; you're given an option when you run configure.py. That should work, and if it does not, I am probably willing and able to fix that or rope in someone who can.\r\n\r\nIt's alright; I just submit this issue so other users could know when they hit the same problem.\r\n", "I've also seen this. For now I have disabled XLA in my builds to avoid this. \r\n\r\nI'm using gcc 5.4.0 , the default compiler with Ubuntu 16.04", "Adding @revan to see if official nightlies are broken or not.", "@chsigg has a CL to fix this, which I am currently attempting to autosubmit on his behalf.", "Should be fixed by https://github.com/tensorflow/tensorflow/commit/a8e873ce4924aeb2c22e867097a4fa43c598b1f7"]}, {"number": 29779, "title": "Compiler dependent error for TFLite micro's FastInt32ToBufferLeft", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 x64\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):1.14.0-rc1\r\n- Python version:not used\r\n- Bazel version (if compiling from source):not used\r\n- GCC/Compiler version (if compiling from source):Visual C++ 14 or higher.\r\n\r\n**Describe the current behavior**\r\nCan't build all TFLite micro's samples on Windows or some embedded systems (requires Visual C++).\r\n\r\n**Describe the expected behavior**\r\n\r\n[TFLite micro's FastInt32ToBufferLeft](https://github.com/tensorflow/tensorflow/blob/4053e17dbb7a1be85553e5173c2e08fb9653d33f/tensorflow/lite/experimental/micro/debug_log_numbers.cc#L80) causes Error C4146 `unary minus operator applied to unsigned type, result still unsigned` on Visual C++. Meanwhile, tensoflow's implementaion has no problem.\r\nThis issue can be solved by rewriting `u = -u;` to `u = 0 - u;` as follows.\r\n\r\n* [TFLite micro's FastInt32ToBufferLeft](https://github.com/tensorflow/tensorflow/blob/4053e17dbb7a1be85553e5173c2e08fb9653d33f/tensorflow/lite/experimental/micro/debug_log_numbers.cc#L80)\r\n```cpp\r\n// Populates the provided buffer with an ASCII representation of the number.\r\nchar* FastInt32ToBufferLeft(int32_t i, char* buffer) {\r\n  uint32_t u = i;\r\n  if (i < 0) {\r\n    *buffer++ = '-';\r\n    u = -u;\r\n  }\r\n  return FastUInt32ToBufferLeft(u, buffer, 10);\r\n}\r\n```\r\n\r\n* [Tensorflow's FastInt32ToBufferLeft](https://github.com/tensorflow/tensorflow/blob/4213d5c1bd921f8d5b7b2dc4bbf1eea78d0b5258/tensorflow/core/lib/strings/numbers.cc#L131)\r\n```cpp\r\nsize_t FastInt32ToBufferLeft(int32 i, char* buffer) {\r\n  uint32 u = i;\r\n  size_t length = 0;\r\n  if (i < 0) {\r\n    *buffer++ = '-';\r\n    ++length;\r\n    // We need to do the negation in modular (i.e., \"unsigned\")\r\n    // arithmetic; MSVC++ apparently warns for plain \"-u\", so\r\n    // we write the equivalent expression \"0 - u\" instead.\r\n    u = 0 - u;\r\n  }\r\n  length += FastUInt32ToBufferLeft(u, buffer);\r\n  return length;\r\n}\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```cpp\r\n#include \"tensorflow/lite/experimental/micro/debug_log_numbers.h\"\r\nint main()\r\n{\r\n    char s[80];\r\n    FastInt32ToBufferLeft(-1, s);\r\n    return 0;\r\n}\r\n```\r\nThis issue is critical for us.\r\nWould you like to modify the code?", "comments": ["This makes a lot of sense. Do you want to kindly provide a PR?\r\n\r\nThanks,", "@stakemura \r\nIs this still an issue", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29778, "title": "GPU usage increases after dozens of steps", "body": "Could you support gpu trace tools for us.@tensorflow\r\nI cant find cause of this bug. \r\n```\r\n# @tf.function\r\ndef train_step(inputs):\r\n    with tf.GradientTape() as tape:\r\n        losses = model(inputs)\r\n    grads = tape.gradient(losses[\"total_loss\"], model.variables)   \r\n    optimizer.apply_gradients(zip(grads, model.variables),\r\n                              global_step=tf.train.get_or_create_global_step())\r\n    return losses\r\n\r\n\r\nfor epoch in range(10):\r\n    for (batch, inputs) in enumerate(train_dataset):\r\n        # ckpt.step.assign_add(1)\r\n        # print (ckpt.step.numpy())\r\n        losses = train_step(inputs)\r\n```\r\nOOM error after nearly 100 steps. batch_size=1, gpu usage from 11G to 23G+. GPU usage increase after each twenty-odd steps.\r\ntrain_dataset reader is tf.data. \r\n\r\ntensorflow 1.13 \r\nGPU titan rtx 24G\r\ncuda 10.0 \r\n", "comments": []}, {"number": 29777, "title": "7e1deb1 breaks 2019-06-13 nightly build", "body": "The following part in CL 7e1deb16c6f10a477003b5f2caadbe3e6fb48af4:\r\n\r\n```\r\ndiff --git a/tensorflow/python/BUILD b/tensorflow/python/BUILD\r\nindex 6145b06a9c..a56a3fd488 100644\r\n--- a/tensorflow/python/BUILD\r\n+++ b/tensorflow/python/BUILD\r\n@@ -36,13 +36,10 @@ load(\"//tensorflow:tensorflow.bzl\", \"py_tests\")\r\n load(\"//tensorflow:tensorflow.bzl\", \"tf_py_build_info_genrule\")\r\n load(\"//tensorflow:tensorflow.bzl\", \"tf_py_wrap_cc\")\r\n load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_shared_object\")\r\n-load(\"//tensorflow:tensorflow.bzl\", \"tf_native_cc_binary\")\r\n-load(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library_additional_deps_impl\")\r\n load(\"//tensorflow:tensorflow.bzl\", \"cuda_py_test\")\r\n load(\"//tensorflow:tensorflow.bzl\", \"cuda_py_tests\")\r\n load(\"//tensorflow/core:platform/default/build_config.bzl\", \"pyx_library\")\r\n load(\"//tensorflow/core:platform/default/build_config.bzl\", \"tf_proto_library\")\r\n-load(\"//tensorflow/core:platform/default/build_config.bzl\", \"tf_proto_library_py\")\r\n load(\"//tensorflow/core:platform/default/build_config.bzl\", \"tf_additional_lib_deps\")\r\n load(\"//tensorflow/core:platform/default/build_config.bzl\", \"tf_additional_all_protos\")\r\n load(\"//tensorflow/core:platform/default/build_config.bzl\", \"tf_protos_grappler\")\r\n```\r\n\r\nCauses nightly build break:\r\n\r\n```\r\nERROR: /tensorflow_src/tensorflow/python/BUILD:4563:1: name 'tf_proto_library_py' is not defined (did you mean 'tf_proto_library'?)\r\n```\r\n\r\nI am sure someone in the team notices this; I just need a marker so I will know when it is fixed.", "comments": ["Gently ping @dmjlm  ", "Hi, apologies for the inconvenience. The offending change has been rolled back internally and should make its way out soon.\r\n\r\nBest,\r\nJacob", "Thanks, Jacob! I will leave this issue open while waiting for resubmission of your CL. Hope it lands soon :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29777\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29777\">No</a>\n"]}, {"number": 29776, "title": "Please add a flag to prevent app.run from terminating with  _sys.exit(main(argv))", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12.x, for now\r\n- Are you willing to contribute it (Yes/No): No. Simple fix but easy to depart from tf naming conventions, and also to not propagate it properly through the code and doc.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrent behavior is for tf to call _sys.exit at the end of app.run completion:\r\n\r\ntensorflow/tensorflow/python/platform/app.py, line 125: \r\n```\r\n # Call the main function, passing through any arguments\r\n  # to the final program.\r\n  _sys.exit(main(argv))\r\n```\r\nIn python applications where tf.app.run is to be invoked multiple times, it would be useful to suppress the _sys.exit. The current workaround is to call out to an external script, only for the purpose of continued processing after the app.run call.\r\n\r\nWHat's needed is a flag - something like _prevent_sys_exit, or whatever is consistent with tf flag naming conventions.\r\n \r\n**Will this change the current api? How?**\r\nProbably not, but would add another flag to externally specifiable set of tf flags.\r\n**Who will benefit with this feature?**\r\nAnyone with a need to not have a python sys.exit call immediately after running a tf app.\r\n\r\n A specific benefit is to aid both logging and debugging, which are much more difficult with call-outs to external scripts.\r\n\r\n**Any Other info.**\r\n", "comments": ["@mfeblowitz Sorry for the delay in my response. Can you please check with `TF1.15`? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29775, "title": "Speed up fold batch norm", "body": "", "comments": ["@petewarden  Can you please review this PR ?", "> Can you add a comment here explaining that the form used here is because the simpler approach was causing performance issues, and explain what it's doing? It only needs to be a couple of lines, thanks.\r\n\r\nWhat is implemented in the original code is to process the original matrix (scaled_weights) bit by bit, let them multiply by a certain number, but because the loop is used to process the whole matrix, so the efficiency is very low, I built a sum matrix (scaled_weights) does not wait for the size of the new matrix, using numpy's own broadcast function to achieve its position-by-position multiplication, thus achieving the acceleration effect.\r\n", "Can one of the admins verify this patch?", "@petewarden Can you please take a look on this PR? Thanks!", "@petewarden Can you please take a look on this PR? Thanks!", "Can you please add tests/benchmark results?", "@SmokerX Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@SmokerX, Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 29774, "title": "Added examples to tf.clip_by_value", "body": "", "comments": ["@jvishnuvardhan Can you please check Ubuntu Sanity errors? Thanks!"]}, {"number": 29773, "title": "added examples for math.argmax", "body": "", "comments": []}, {"number": 29772, "title": "Bug for calling end() of SessionRunHook", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Linux Ubuntu 18.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6\r\n- GPU model and memory: 2080Ti\r\n\r\n**Describe the current behavior**\r\nI define a hook class which is inherited from SessionRunHook, and add an assign operation in the begin() function. Then, I run the assign operation in the end() function. Although the value is changed, the results do not store into the checkpoint. So, If I restore the model, I get the model which is not changed. \r\n\r\n**Describe the expected behavior**\r\nSince we only know the end() is called before closing the session. I think the changed result should be store in the chekpoint. Otherwise, the document should provide the ordering of executing all hooks. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nclass NetworkPruningHook(tf.train.SessionRunHook):\r\n    def __init__(self):\r\n\r\n    def begin(self):\r\n      self.mask_var = []\r\n      ### collect the masked variables\r\n      for v in tf.global_variables():\r\n        if v.name.find(\"mask:0\") != -1:\r\n          self.mask_var.append(v)\r\n\r\n      self.check_mask = tf.assign(self.mask_var[0], np.zeros(shape=self.mask_var[0].shape))\r\n\r\n    def end(self, session):\r\n      session.run(self.check_mask) # assign 0 to the variable \r\n      print(session.run(self.mask_var[0])) # get [0, ..., 0]\r\n      \r\n      ### However, if restoring the checkpoint, the self.mask_var[0] is not [0, ..., 0]\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a full code snippet to reproduce the issue reported here. Thanks!", "\r\n    class NetworkPruningHook(tf.train.SessionRunHook):\r\n     def __init__(self):\r\n        super(NetworkPruningHook, self).__init__()\r\n        self.sparsity = 0\r\n        self.total_step = 0\r\n        self.step = 0\r\n        self.hasInitialize = False\r\n\r\n    def initialize(self, sparsity, total_step):\r\n      self.step = 0\r\n      self.total_step = (int)(total_step)\r\n      self.sparsity = sparsity\r\n      self.hasInitialize = True\r\n\r\n    def begin(self):\r\n      tf.logging.info(\"[INFO] sparsity: %d\", self.sparsity)\r\n      self.mask_var = []\r\n      self.masked_output = []\r\n      self.assign_ops = []\r\n      self.placeholders = []\r\n      self.scores = []\r\n      self.loss = None\r\n      assert(self.hasInitialize == True)\r\n\r\n      ### collect the masked variables\r\n      for v in tf.global_variables():\r\n        if v.name.find(\"mask:0\") != -1:\r\n          self.mask_var.append(v)\r\n          p = tf.placeholder(tf.float32, shape=v.shape)\r\n          self.placeholders.append(p)\r\n          self.assign_ops.append(tf.assign(v, p))\r\n          self.scores.append(np.zeros(v.shape))\r\n\r\n      ### collect the outputs of masked layers and loss layer\r\n      for op in tf.get_default_graph().get_operations():\r\n        for t in op.values():\r\n          if t.name.find(\"mask-output\") != -1:\r\n            self.masked_output.append(t)\r\n          elif t.name.find(\"loss/Mean:0\") != -1:\r\n          \tself.loss = t\r\n\r\n      self.var_grad = tf.gradients(self.loss, self.mask_var)\r\n\r\n    def before_run(self, run_context):\r\n      return tf.train.SessionRunArgs([self.var_grad, self.masked_output])\r\n\r\n    def after_run(self, run_context, run_values):\r\n      for i in range(len(self.mask_var)):\r\n        score = abs(np.sum(run_values.results[0][i] * run_values.results[1][i], axis=0))\r\n        score_sum = np.sum(score)\r\n        if(score_sum) != 0:\r\n          score = score / score_sum\r\n        self.scores[i] = self.scores[i] + score\r\n\r\n      self.step += 1\r\n      if self.step >= self.total_step:\r\n        ### check the zero_density\r\n        zero_density = [np.sum(x)*1. / np.shape(x)[0] for x in run_context.session.run(self.mask_var)]\r\n        average = np.average(zero_density)\r\n        tf.logging.info(\"zero_density of all layers before pruning: \")\r\n        # for i in range(len(zero_density)):\r\n        #   tf.logging.info(\"  zero_density of layer %d: %f\", i, zero_density[i])\r\n        tf.logging.info(\"The average zero_density of network: %f\", average)\r\n\r\n        ### pruning\r\n        threshold = np.percentile([y for x in self.scores for y in x], self.sparsity)\r\n        for i in range(len(self.mask_var)):\r\n          mask = (self.scores[i] > threshold)\r\n          run_context.session.run(self.assign_ops[i], feed_dict={self.placeholders[i]: mask})\r\n\r\n        ### check the zero_density\r\n        zero_density = [np.sum(x)*1. / np.shape(x)[0] for x in run_context.session.run(self.mask_var)]\r\n        average = np.average(zero_density)\r\n        tf.logging.info(\"zero_density of all layers: \")\r\n        # for i in range(len(zero_density)):\r\n        #   tf.logging.info(\"  zero_density of layer %d: %f\", i, zero_density[i])\r\n        tf.logging.info(\"The average zero_density of network: %f\", average)\r\n\r\n    def end(self, session):\r\n    \tself.hasInitialize = False\r\n\r\n\r\nAbove is my hook, and I add a masked layer by tf.Variable with name \"xxx-mask\" on the fully-connected layer. If I move the assign operation from begin() to end(), then the assign results will not be written in checkpoint file after training. \r\n", "I tried reproducing the issue. But i am not able to get saved checkpoints.Please,help us to give  more information to reproduce the issue.Thanks!", "I use the code of https://github.com/google-research/bert directly, I think the estimator will save checkpoints every some steps and when the training is end", "Will it be possible to send me the full minimal code snippet to reproduce the issue for faster resolution.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29771, "title": "Multi GPU training - inconstant results between virtual GPUs and non Virtual GPUs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n    - Modified from the stock example\r\n    - https://www.tensorflow.org/beta/tutorials/distribute/keras\r\n    - Working example is attached to this bug report\r\n- OS Platform and Distribution\r\n    - Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n    - No\r\n- TensorFlow installed from (source or binary):\r\n    - Binary\r\n- TensorFlow version (use command below):\r\n    - 2.0.0-beta0\r\n- Python version:\r\n    - Python 3.5.2\r\n- Bazel version (if compiling from source):\r\n    - N/A (0.23.2)\r\n- GCC/Compiler version (if compiling from source):\r\n    - N/A (5.4.0 20160609)\r\n- CUDA/cuDNN version:\r\n    - 10.0.130/7.4.1\r\n- GPU model and memory:\r\n    - 4x Titan Xp (12Gb) (Standalong, no SLI connections)\r\n\r\n**Describe the current behavior**\r\nThe model accuracy is far off between virtual GPUs and physical GPUs. The default tf.distribute.MirroredStrategy() is used for multi GPU training.\r\n\r\n\r\n**Describe the expected behavior**\r\nThe trained model should achieve similar level of accuracy regardless of virtual devices (0.9938) or physical devices (0.9698).\r\n\r\n**Code to reproduce the issue**\r\n1. The script attached is used to train a model using multiple GPUs.\r\n  * The code is modified from the stock example for quick experiment.\r\n    * https://www.tensorflow.org/beta/tutorials/distribute/keras\r\n  * Traing using two VirtualDevices after 10 epoches\r\n    * when only one GPU is detected, two virtual devices will be created. (init_gpu())\r\n    * Use the following command to run the script\r\n```\r\nexport CUDA_VISIBLE_DEVICES=\"0\"\r\npython3 mnist_muti_gpus.py\r\n```\r\n  * results after 10 epoches\r\n```\r\nEpoch 10/10\r\n465/469 [============================>.] - ETA: 0s - loss: 0.0250 - accuracy: 0.9937\r\nLearning rate for epoch 10 is 0.00001\r\n469/469 [==============================] - 4s 9ms/step - loss: 0.0249 - accuracy: 0.9938\r\n\r\n...\r\n\r\nEpoch 20/20\r\n463/469 [============================>.] - ETA: 0s - loss: 0.0240 - accuracy: 0.9939\r\nLearning rate for epoch 20 is 0.00001\r\n469/469 [==============================] - 4s 8ms/step - loss: 0.0239 - accuracy: 0.9940\r\n\r\n```\r\n\r\n  * Traing using two GPUs (non virtual) after 10 epoches\r\n    * If more than one GPU is visiable, no virtual device will be created. (init_gpu())\r\n    * Use the following command to run the script\r\n```\r\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\r\npython3 mnist_muti_gpus.py\r\n```\r\n  * results after 10 epoches\r\n```\r\nEpoch 10/10\r\n464/469 [============================>.] - ETA: 0s - loss: 0.1421 - accuracy: 0.9697\r\nLearning rate for epoch 10 is 0.00001\r\n469/469 [==============================] - 4s 8ms/step - loss: 0.1421 - accuracy: 0.9698\r\n\r\n...\r\n\r\nEpoch 20/20\r\n465/469 [============================>.] - ETA: 0s - loss: 0.1157 - accuracy: 0.9728\r\nLearning rate for epoch 20 is 0.00001\r\n469/469 [==============================] - 4s 9ms/step - loss: 0.1157 - accuracy: 0.9728\r\n\r\n```\r\n\r\n**Other info / logs**\r\n\r\n\r\n```python\r\n\"\"\"\r\nThis script is extended from the following stock example:\r\nhttps://www.tensorflow.org/beta/tutorials/distribute/keras\r\n\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom __future__ import unicode_literals\r\n\r\nfrom tensorflow.keras.callbacks import TensorBoard\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Dense, Flatten, BatchNormalization\r\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D\r\nfrom tensorflow.keras import optimizers\r\nfrom tensorflow.keras import losses\r\nfrom tensorflow.keras import initializers\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras.layers import Activation\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport math\r\nimport os\r\n\r\n# Initialize GPUs\r\n# This function must be called first\r\n# Setting up GPU memory ussage\r\n# https://www.tensorflow.org/beta/guide/distribute_strategy\r\n# https://www.tensorflow.org/beta/guide/using_gpu\r\ndef init_gpus(soft_device_placement=True, log_device_placement=False, create_virtual_devices=False, memory_limit=4096):\r\n\r\n    tf.config.set_soft_device_placement(soft_device_placement)    \r\n    tf.debugging.set_log_device_placement(log_device_placement)\r\n\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    if gpus:\r\n        # If there is only one GPU, create two logical virtual devices for developing\r\n        # on a machine with only one GPU installed\r\n        try:\r\n            # Create 2 virtual GPUs on each physical GPU with the given memory_limit GPU memory\r\n            if create_virtual_devices and len(gpus) == 1:\r\n                for gpu in gpus:\r\n                    tf.config.experimental.set_virtual_device_configuration(\r\n                        gpu,\r\n                        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096),\r\n                         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]\r\n                    )\r\n\r\n            else:\r\n                # Currently, memory growth needs to be the same across GPUs\r\n                for gpu in gpus:\r\n                    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n        except RuntimeError as e:\r\n            # Memory growth must be set before GPUs have been initialized\r\n            print(e)\r\n\r\n        # print out physical and logical GPUs\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n\r\n    else:\r\n        print(\"No visible GPU is detected...\")\r\n\r\ndef scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n\r\n# Callback for printing the LR at the end of each epoch.\r\nclass PrintLR(tf.keras.callbacks.Callback):\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        print('\\nLearning rate for epoch %d is %.5f' %(epoch + 1, self.model.optimizer.lr.numpy()))\r\n\r\n# Function for decaying the learning rate.\r\n# You can define any decay function you need.\r\ndef decay(epoch):\r\n    if epoch < 3:\r\n        return 1e-3\r\n    elif epoch >= 3 and epoch < 7:\r\n        return 1e-4\r\n    else:\r\n        return 1e-5\r\n\r\ndef run():\r\n    ### get data\r\n    init_gpus(\r\n        log_device_placement=False,\r\n        create_virtual_devices=True\r\n    )\r\n\r\n    # \r\n    # strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())\r\n    # strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\n    # strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce())\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n\r\n    # Setting with_info to True includes the metadata for the entire dataset, \r\n    # which is being saved here to info. Among other things, this metadata object\r\n    # includes the number of train and test examples.\r\n    datasets, info = tfds.load(\r\n        name='mnist',\r\n        with_info=True,\r\n        as_supervised=True\r\n    )\r\n    mnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\n    # You can also do info.splits.total_num_examples to get the total\r\n    # number of examples in the dataset.\r\n    num_train_examples = info.splits['train'].num_examples\r\n    num_test_examples = info.splits['test'].num_examples\r\n\r\n    #\r\n    print(\"num_train_examples = %d, num_test_examples = %d\" %(num_train_examples, num_test_examples))\r\n\r\n    BUFFER_SIZE = num_train_examples\r\n    BATCH_SIZE_PER_REPLICA = 64\r\n    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n    EPOCHS = 20\r\n\r\n    # train_dataset = mnist_train.map(scale).shuffle(num_train_examples).repeat().batch(BATCH_SIZE)\r\n    train_dataset = mnist_train.shuffle(num_train_examples).repeat().map(scale).batch(BATCH_SIZE)\r\n    eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\r\n\r\n    with strategy.scope():\r\n        ### compile\r\n        ## Seperate the activation layer so that BatchNormalization can be added later on.\r\n        model = tf.keras.Sequential([\r\n            tf.keras.layers.Conv2D(32, 3, input_shape=(28, 28, 1)),\r\n            tf.keras.layers.Activation(activation='relu'),\r\n            tf.keras.layers.MaxPooling2D(),\r\n            tf.keras.layers.Flatten(),\r\n            tf.keras.layers.Dense(64),\r\n            tf.keras.layers.Activation(activation='relu'),\r\n            tf.keras.layers.Dense(10, activation='softmax')\r\n        ])\r\n\r\n        model.compile(\r\n            loss='sparse_categorical_crossentropy',\r\n            optimizer=tf.keras.optimizers.Adam(),\r\n            metrics=['accuracy']\r\n        )\r\n        model.summary()\r\n\r\n    # print out physical and logical GPUs\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n\r\n    # Define the checkpoint directory to store the checkpoints\r\n    checkpoint_dir = './training_checkpoints'\r\n    # Name of the checkpoint files\r\n    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n\r\n    callbacks = [\r\n        tf.keras.callbacks.TensorBoard(log_dir='./logs'),\r\n        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True),\r\n        tf.keras.callbacks.LearningRateScheduler(decay),\r\n        PrintLR()\r\n    ]\r\n\r\n    steps_per_epoch = math.ceil(num_train_examples / BATCH_SIZE)\r\n    model.fit(\r\n        train_dataset,\r\n        steps_per_epoch=steps_per_epoch,\r\n        epochs=EPOCHS,\r\n        callbacks=callbacks\r\n    )\r\n\r\n    # validation\r\n    eval_loss, eval_acc = model.evaluate(eval_dataset)\r\n\r\n    K.clear_session()\r\n    return\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n\r\n```", "comments": ["Gently ping @aaroey", "@scotthong, I believed virtual gpu is an experimental feature, it has been tested in inference environment where there are no data dependency between the graph on different virtual GPUs. It has not been tested in training environment. ", " The problem is that why\u00a0tf.distribute.MirroredStrategy() does not work on two physical GPUs.\u00a0\r\n\u00a0\u00a0\u00a0\u00a01. the results are similar between one GPU and two Virtual GPUs.\u00a0\u00a0 \u00a0 \r\n    2. the results are very different between\u00a0one\u00a0GPU and two physical GPUs.\u00a0\r\n\r\n", "@scotthong,\r\nI did not observe much difference in the accuracy on running the code with TensorFlow v2.4.\r\n\r\n```\r\n#with CUDA_VISIBLE_DEVICES=\"0\"\r\nLearning rate for epoch 10 is 0.00001\r\nEpoch 11/20\r\n469/469 [==============================] - 4s 9ms/step - loss: 0.0268 - accuracy: 0.9935\r\n.\r\n.\r\nLearning rate for epoch 20 is 0.00001\r\n2021-01-18 15:38:00.249082: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\r\n79/79 [==============================] - 2s 10ms/step - loss: 0.0434 - accuracy: 0.9848\r\n```\r\n\r\n\r\n\r\n```\r\n#with CUDA_VISIBLE_DEVICES=\"0,1\"\r\nLearning rate for epoch 10 is 0.00001\r\nEpoch 11/20\r\n469/469 [==============================] - 4s 9ms/step - loss: 0.0286 - accuracy: 0.9927\r\n.\r\n.\r\nLearning rate for epoch 20 is 0.00001\r\n2021-01-18 15:40:42.211088: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\r\n79/79 [==============================] - 2s 10ms/step - loss: 0.0468 - accuracy: 0.9853\r\n```\r\n\r\nCould you please update TensorFlow to the latest stable version and check if you are still facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29771\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29771\">No</a>\n"]}, {"number": 29770, "title": "tf.keras: predict, fit, predict = old result", "body": "Windows 10 / Python 3.7 / TF 1.13\r\n\r\n```python\r\nm = model.predict(inputs)\r\nprint(model.fit(inputs, outputs, batch_size=inputs.shape[0], verbose=1)) # OK: loss: 16.0302\r\nprint(model.predict(inputs) - m) # all = 0\r\n```\r\nSynchronization of the weights for the last call predict() does not work.", "comments": ["Keras is ported with errors.\r\n\r\ntf.keras:\r\n```python\r\nfrom tensorflow import keras\r\n\r\nmodel.compile(optimizer=keras.optimizers.Adam(lr=1e-2),\r\n\t\t\t\tloss=keras.losses.binary_crossentropy)\r\n\r\nmodel.fit(inputs, outputs, batch_size=inputs.shape[0], verbose=1, epochs=5)\r\n```\r\n```\r\nTrain on 60 samples\r\nEpoch 1/5\r\n\r\n60/60 [==============================] - 1s 15ms/sample - loss: 15.3791\r\nEpoch 2/5\r\n\r\n60/60 [==============================] - 0s 0s/sample - loss: 15.3791\r\nEpoch 3/5\r\n\r\n60/60 [==============================] - 0s 0s/sample - loss: 15.3791\r\nEpoch 4/5\r\n\r\n60/60 [==============================] - 0s 260us/sample - loss: 15.3791\r\nEpoch 5/5\r\n\r\n60/60 [==============================] - 0s 0s/sample - loss: 15.3791\r\n```\r\n\r\nKeras:\r\n```python\r\nimport keras\r\n\r\nmodel.compile(optimizer=keras.optimizers.Adam(lr=1e-2),\r\n\t\t\t\tloss=keras.losses.binary_crossentropy)\r\n\r\nmodel.fit(inputs, outputs, batch_size=inputs.shape[0], verbose=1, epochs=5)\r\n```\r\n```\r\nEpoch 1/5\r\n\r\n60/60 [==============================] - 1s 15ms/step - loss: 6.1551\r\nEpoch 2/5\r\n\r\n60/60 [==============================] - 0s 261us/step - loss: 1.0960e-07\r\nEpoch 3/5\r\n\r\n60/60 [==============================] - 0s 0us/step - loss: 1.0960e-07\r\nEpoch 4/5\r\n\r\n60/60 [==============================] - 0s 260us/step - loss: 1.0960e-07\r\nEpoch 5/5\r\n\r\n60/60 [==============================] - 0s 261us/step - loss: 1.0960e-07\r\n```\r\n", "@Roffild Looks, code snippet is incomplete, provide us the complete code snippet to reproduce the issue. Thanks!", "https://drive.google.com/file/d/1xg54ll-4DVyoQnGA89bjbvKgDHtuIxly/view?usp=sharing", "@Roffild I am able to reproduce the issue on my system with Tensorflow 1.13.1. Thanks!", "I checked on Ubuntu 18, TF 1.14\r\nThe numbers are different, but the error is the same.\r\n\r\nI tried to use the TF API, but the error repeated.\r\n\r\n```python\r\n    def compile(self, optimizer, loss):\r\n        if self.build:\r\n            return\r\n        with self.session.graph.as_default():\r\n            with tf.variable_scope(\"train\", reuse=False, dtype=tf.float64) as scope:\r\n                self.target = tf.placeholder(self.outputs.dtype, self.outputs.shape, name=\"target\")\r\n                self.loss = tf.keras.losses.binary_crossentropy(self.target, self.outputs)\r\n                self.train = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(self.loss)\r\n                self.session.run(tf.variables_initializer(tf.global_variables(scope.original_name_scope)), options=self.run_options, run_metadata=self.run_metadata)\r\n        self.build = True\r\n    def fit(self, inputs, outputs, batch_size=0, verbose=0):\r\n        _, l = self.session.run([self.train, self.loss], {self.target: outputs, self.inputs: inputs}, options=self.run_options, run_metadata=self.run_metadata)\r\n        return l\r\n```", "@gadagashwini do you have a colab repro for this?", "@pavithrasv, Please take a look at the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/8cb0d2da742a303dda1e3e06bcab59c7/untitled178.ipynb). Thanks!", "Is this still an issue? i do not see any error or predict calls in the colab.", "Closing the issue as there is not enough information to repro the issue, please feel free to re-open if required.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29770\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29770\">No</a>\n"]}, {"number": 29769, "title": "Use `PyLong_AsVoidPtr`, `PyLong_FromVoidPtr` for conversion", "body": "Use PyLong_AsVoidPtr, PyLong_FromVoidPtr to convert the pointer. Use of\r\nPyInt_AsLong returns a long which will truncate the pointer on a LLP64\r\ntarget (i.e. Windows). Using PyLong_AsVoidPtr will return a void * for\r\nthe Python integer or long integer. Define the corresponding \"out\" map\r\nwhich specifies the conversion to wrap the return value of a function\r\nreturning a TfLiteDelegate *. This ensures that the conversion is always\r\nvalid as the value is constructed fro the PyLong_FromVoidPtr.", "comments": ["CC: @jdduke @saeta ", "@saeta - I don't think that this is sufficient to restore the Windows build.  I'm still trying to get tensorflow to build on Windows - seems that the version info gen rule is failing for some reason.  I suspect that the changes for bazel will be needed for that"]}, {"number": 29768, "title": "Fix the failed test case in ReduceDatasetOpTest", "body": "This PR fixes the failed test case in `ReduceDatasetOpTest`. \r\n\r\n@jsimsa ", "comments": ["@jsimsa For the reduce function with the same number of inputs and outputs, it will do the aggregation on `initial_state` but the input dataset will not be used by the function. Does that mean this kind of reduce function will be invalid?  If yes, do we need to add a check for this case?"]}, {"number": 29767, "title": "Support for distributed custom input pipeline", "body": "Feature Request\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04 and 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNo\r\n- **TensorFlow installed from (source or binary)**:\r\nDocker hub image using: FROM tensorflow/tensorflow:2.0.0b0-gpu-py3\r\n- **TensorFlow version (use command below)**:\r\nv1.12.1-3259-gf59745a381 2.0.0-beta0\r\n- **Python version**:\r\nPython 3.6.8\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nSee prebuilt docker image\r\n- **GPU model and memory**:\r\nNividia GeForce GTX TITAN-X (Maxwell)\r\n- **Exact command to reproduce**:\r\nNone, since this is a feature request\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nThis is a feature request:\r\n---------------------------------\r\nIn the guide for TensorFlow 2.0 at https://www.tensorflow.org/beta/guide/distribute_strategy\r\nthere is an example for using tf.distribute.Strategy with custom training loops. However this guide assumes that one uses tf.data.Dataset for the input pipeline.\r\n\r\nI have the need for a very dynamic input pipeline since I do inference-based batch mining for triplet training very much like what is described in an article from Google found here:\r\nFaceNet: A Unified Embedding for Face Recognition and Clustering\r\nhttps://arxiv.org/pdf/1503.03832.pdf\r\n\r\nI believe such capability is fundamental for having flexible customization capability while being able to distribute training. Fixed input pipeline can be very limiting.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThe above mentioned tutorial assumes you can do something like this with the input pipeline:\r\n```python\r\nwith mirrored_strategy.scope():\r\n  dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(\r\n      global_batch_size)\r\n  dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\r\n\r\nwith mirrored_strategy.scope():\r\n  for inputs in dist_dataset:\r\n    print(train_step(inputs))\r\n```\r\nWhereas my example of custom inference-based dataset reader looks more like this:\r\n```python\r\n[anchor_inputs, positive_inputs, negative_inputs] = get_dynamic_triplet_batches_pytorch(args, model, dataset_handler)\r\n```\r\nThe dataset_handler is entirely custom and is used to provide candidate triplets. Then inference is run on them and if the loss is moderate (as described in the FaceNet paper from Google) the triplet is added to a final training batch. In the end that batch is returned to the training loop for the full backprop pass. However, the need to distribute a custom input pipeline is generic and not limited to this use-case.\r\n\r\nThe example is taken from code I wrote for PyTorch where the distribution strategy available is very flexible, so this input pipeline works just out-of-the-box (for distributing on GPUs on a single machine). I would like to see this be possible in Tensorflow as well (but here for all distribution strategies of course). This would make Tensorflow more attractive for more advanced use-cases for training. Right now I don't see how it is possible to set up a custom training input pipeline like this with the existing distribution strategies.\r\n\r\nBR,\r\n/Niclas\r\n\r\n\r\n                                                                                                    ", "comments": ["We have recently added a new API that should allow you to construct customized input and feed into distribution strategy `run` method: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/distribute_lib.py#L1318\r\n\r\nWould something like that help in your use case?", "@niclasdn-axis   Closing this issue as of now. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29766, "title": "[TF-TRT] Support constant params input for gather", "body": "Duplicate of #28344 since there was a problem with the CLA.", "comments": []}, {"number": 29765, "title": "ModuleNotFoundError: No module named 'tensorflow.python.saved_model.model_utils", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO\r\n- OS Platform and Distribution :Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):conda install tensorflow-gpu=1.13.1\r\n- Python version:3.6\r\n\r\n\r\nI have installed tf=2.0.0 before on another conda env ,but after uninstalling tf=2.0.0 the whole tensorflow on every conda env has been ruined. Trying different envs/python-version doesn't work at all. \r\n`pip uninstall/install tensorflow_estimator` in/out of conda envs doesn't work as well\r\n\r\n", "comments": ["@Charlie4zc \r\nHow did you solve it?\r\nI am facing same problem while working with Tensorlfow object detection API."]}, {"number": 29764, "title": "Set 1.14 forward compatibility date to 9/10", "body": "Sets it before 9/14 so the gated feature in cl/252672618 isn't triggered.", "comments": ["Instead of changing the horizon which I did in this PR, the 9/14 date in cl/252672618 should be changed with one after Nov 1 so that it is disabled."]}, {"number": 29763, "title": "Batched tf.RaggedTensor with tf.data.Dataset returns invalid ragged tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): public colab instance\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0-rc1\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nIterating on a batched dataset made from tensor slices of ragged tensors produces `tf.RaggedTensor` that seem to be improperly formatted. Specifically, calling `RaggedTensor.value_rowids` return the row index for every element in `RaggedTensor.values`. This is inconsistent with the output obtained from calling `RaggedTensor.value_rowids` directly on the original tensor (or slice of the original), and also inconsistent with the format expected by `tf.RaggedTensor. from_value_rowids`.\r\n \r\n**Describe the expected behavior**\r\nGiven a batched ragged tensor `x`, manually building a new one using `tf.RaggedTensor. from_value_rowids(x.values, x.value_rowids())` should work. Currently this raise an exception at runtime (not during graph construction).\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndata_values = tf.RaggedTensor.from_row_splits(\r\n    np.arange(8).reshape((-1, 2)),\r\n    np.array([0, 2, 3, 4]),\r\n)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(data_values)\r\ndataset = dataset.repeat()\r\ndataset = dataset.batch(3)\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nbatch = iterator.get_next()\r\n\r\na = tf.RaggedTensor.from_value_rowids(batch.values, batch.value_rowids())\r\nwith tf.Session() as sess:\r\n  sess.run(a)  # this fails due to invalid shapes\r\n  assert np.allclose(sess.run(batch.value_rowids()),  # this will also fail\r\n                     sess.run(data_values.value_rowids()))\r\n  \r\n```\r\n", "comments": ["This appears to be fixed in tf-nightly (version `1.14.1-dev20190619`). This issue is probably safe to close.", "Thank you, it seems to work!\r\nI have a related problem, but maybe I should open it as another issue:\r\nI have a dataset from generator, containing uneven tensors. After batching I would expect it to return dataset with RaggedTensors. At present it fails. \r\nHere is a minimal example:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.data import Dataset\r\n\r\ndef get_iterator():\r\n    return map(lambda x: np.ones([x, 1]), range(4))\r\n    \r\nds = Dataset.from_generator(get_iterator, output_types= tf.float32,\r\n                            output_shapes=[None, 1])\r\n\r\nfor x in ds:\r\n    print(x)    \r\n\r\nprint(\"Batched:\")\r\n\r\nfor x in ds.batch(2):\r\n    print(x) \r\n```", "Sorry, I found probably a more related issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/27653\r\nand I repated my above comment there."]}, {"number": 29762, "title": "Accuracy Discrepancy Between Eager and Graph Mode Tensorflow Alpha 2.0.0", "body": "**System information**\r\n- Custom and Stock Code\r\n- TensorFlow installed from binary:\r\n- [tf_env.txt](https://github.com/tensorflow/tensorflow/files/3288005/tf_env.txt)\r\n- CUDA/cuDNN version: \r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:04_Central_Daylight_Time_2018\r\nCuda compilation tools, release 10.0, V10.0.130\r\n- GPU model and memory:\r\n  - Name: NVIDIA GeForce GTX 1060 with Max-Q Design\r\n  - Memory: 8124 MB\r\n\r\n**Describe the current behavior**\r\nThere is a large accuracy difference between running tensorflow in Eager versus Graph mode \r\n(i.e. using @tf.function and not using @tf.function) for the same model, number of batches and epochs on the same data.\r\n\r\nThree different tests are run to measure performance on both a GPU and CPU if\r\navailable. Each test uses tensorflow alpha 2.0, and trains using the MNIST dataset provided/bundled by tensorflow. Each test runs for 1 epoch comprised of 1000 mini-batches. A mini-batch contains 32 samples. All tests use Adam optimizer, and sparse categorical loss.\r\n\r\nTest descriptions:\r\n- Test 1: Uses the Keras 'compile' and 'fit' function\r\n- Test 2: Uses tf.GradientTape and does not use the @tf.function decorator\r\n- Test 3: Uses tf.GradientTape and the @tf.function decorator. \r\n    The @tf.function decorator is applied to the function training \r\n    on MNIST mini-batches\r\n\r\n**Describe the expected behavior**\r\nI would expect roughly the same training accuracy after 1000 steps for each test. When using the @tf.function decorator a much greater accuracy is achieved for the same model type, number of epochs, number of batches, and dataset compared to a training run not using @tf.function. \r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.python.client import device_lib\r\nimport time\r\n\r\n# Uncomment this to observe the device used to \r\n# run tensorflow operations\r\n#tf.debugging.set_log_device_placement(True)\r\n''' \r\nDescription:\r\nRuns three different tests to measure performance on both a GPU and CPU if\r\navailable. Each test uses tensorflow\r\nalpha 2.0, and trains using the MNIST dataset provided/bundled by tensorflow.\r\nEach test runs for 1 epoch comprised of 1000 mini-batches. A mini-batch contains\r\n32 samples. All tests use Adam optimizer, and sparse categorical loss.\r\n\r\nTest descriptions:\r\n- Test 1: Uses the Keras 'compile' and 'fit' function\r\n- Test 2: Uses tf.GradientTape and does not use the @tf.function decorator\r\n- Test 3: Uses tf.GradientTape and the @tf.function decorator. \r\n    The @tf.function decorator is applied to the function training \r\n    on MNIST mini-batches\r\n\r\nIssue Observed:\r\n(example below for GPU for example)\r\nTest 1 provides a benchmark accuracy of ~70% (6.4 sec runtime) on the training data after 1000 \r\nsteps. Test 3 achieves roughly the same accuracy in a similar time (even \r\na little better ~80% with 3.75 sec runtime). Test 2 achieves ~20-40% accuracy (14-16 sec runtime) \r\nafter training for number of batches. The only difference\r\nbetween Test 2 and Test 3 is no utilization of the tf.function decorator in Test 2. \r\n\r\nThe slow down in Test 2 is expected when not using the tf.function decorator.\r\nHowever, I would expect roughly the same training accuracy after 1000 steps for each \r\ntest.\r\n'''\r\n\r\n# ------------------- Create Several Helper Functions and variables \r\n# ------------------- used for training\r\ndef print_info():\r\n    print(tf.__version__)\r\n    print(tf.executing_eagerly())\r\n    print(device_lib.list_local_devices())\r\n    print()\r\n\r\ndef get_available_device_names():\r\n    local_devices = device_lib.list_local_devices()\r\n    names = []\r\n    for device in local_devices:\r\n        names.append(device.name)\r\n    return names\r\n\r\ndef get_and_prepare_data():\r\n    data, info = tfds.load(\r\n        'mnist',with_info=True, as_supervised=True)\r\n\r\n    train_data, test_data = data['train'], data['test']\r\n\r\n    batch_size = 32\r\n    prep_train_data = train_data.batch(batch_size).prefetch(1).repeat()\r\n    return prep_train_data, info\r\n\r\ndef create_model():\r\n    img_shape = info.features['image'].shape\r\n    model = keras.Sequential([\r\n        keras.layers.Flatten(input_shape=img_shape),\r\n        keras.layers.Dense(128, activation='relu'),\r\n        keras.layers.Dropout(0.2),\r\n        keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    return model\r\n\r\ndef train_batch(model, images, label, loss_fn, optimizer, train_loss, train_accuracy):\r\n    with tf.GradientTape() as tape:\r\n        pred = model(images)\r\n        loss = loss_fn(label, pred)\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    grads_and_vars = zip(gradients, model.trainable_variables)\r\n    optimizer.apply_gradients(grads_and_vars)\r\n\r\n    train_loss(loss)\r\n    train_accuracy(label, pred)\r\n    \r\n    return loss, pred\r\n\r\ndef run_batch(batch_train_func, steps_per_epoch=1000, epochs=1):\r\n    train_loss = keras.metrics.Mean()\r\n    train_accuracy = keras.metrics.SparseCategoricalAccuracy()\r\n    for e in range(epochs):\r\n        batch = 0\r\n        for (data, label) in prep_train_data:\r\n            loss, pred = batch_train_func(data, label, train_loss, train_accuracy)\r\n            \r\n            if batch % 500 == 0:\r\n                print(\"Batch {} accuracy {} loss {}\\n\".format(\r\n                    batch, train_accuracy.result(), train_loss.result()))\r\n            if batch >= steps_per_epoch:\r\n                break\r\n            batch = batch + 1\r\n\r\n# Basic info\r\nprint_info()\r\navail_device_names = get_available_device_names()\r\nprint(avail_device_names)\r\n\r\nsteps_per_epoch = 1000\r\nepochs = 1\r\ndevices = ['/device:CPU:0', '/device:GPU:0']\r\n\r\nfor device in devices:\r\n    if device not in avail_device_names:\r\n        print(\"Device {} not available\".format(device))\r\n        continue # Device not available\r\n\r\n    print(\"\\n-------------------------------------------\")\r\n    print(\"-------------------------------------------\")\r\n    print(\"Starting test for device {}\".format(device))\r\n    print(\"-------------------------------------------\")\r\n\r\n    # Tests record accuracy, loss, batch number and test type\r\n    # ----------------------------- TEST 1 --------------------------- #\r\n    # ------------ Test Using Keras compile and fit ------------------ #\r\n\r\n    print(\"Begin Benchmarking Test 1\")\r\n    print(\"Test uses keras functions compile and fit\")\r\n\r\n    # Data prep\r\n    prep_train_data, info = get_and_prepare_data()\r\n\r\n    t1_start = time.time()\r\n\r\n    with tf.device(device):\r\n        keras_fit_model = create_model()\r\n\r\n        keras_fit_model.compile(optimizer='adam', \r\n                        loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n\r\n        keras_fit_model.fit(prep_train_data, epochs=epochs, steps_per_epoch=steps_per_epoch,verbose=2)\r\n\r\n    t1_end = time.time()\r\n    print(\"Test 1 Time Elapsed {}\\n\".format(t1_end-t1_start))\r\n\r\n    # ----------------------------- TEST 2 ----------------------------- #\r\n    # --------- Test Using Gradient Tape without tf.function ----------- #\r\n    print(\"Begin Benchmarking Test 2\")\r\n    print(\"Test uses tf.GradientTape() (does not use tf.function decorator)\")\r\n\r\n    # Data prep\r\n    prep_train_data, info = get_and_prepare_data()\r\n\r\n    t2_start = time.time()\r\n\r\n    with tf.device(device):\r\n        eager_model = create_model()\r\n        eager_optimizer = tf.keras.optimizers.Adam()\r\n        eager_loss_func = keras.losses.SparseCategoricalCrossentropy()\r\n\r\n        def eager_train_batch(data, label, train_loss, train_accuracy):\r\n            return train_batch(eager_model, data, label, eager_loss_func, eager_optimizer, train_loss, train_accuracy)\r\n\r\n        run_batch(eager_train_batch, steps_per_epoch, epochs)\r\n\r\n    t2_end = time.time()\r\n    print(\"Test 2 Time Elapsed {}\\n\".format(t2_end-t2_start))\r\n\r\n\r\n    # ----------------------------- TEST 3 ----------------------------------- #\r\n    # --------- Test Using Gradient Tape with tf.function decorator----------- #\r\n    print(\"Begin Benchmarking Test 3\")\r\n    print(\"Test uses tf.GradientTape() (uses tf.function decorator)\")\r\n\r\n    # Data prep\r\n    prep_train_data, info = get_and_prepare_data()\r\n\r\n    t3_start = time.time()\r\n\r\n    with tf.device(device):\r\n        graph_model = create_model()\r\n        graph_optimizer = tf.keras.optimizers.Adam()\r\n        graph_loss_func = keras.losses.SparseCategoricalCrossentropy()\r\n\r\n        @tf.function\r\n        def graph_train_batch(data, label, train_loss, train_accuracy):\r\n            return train_batch(graph_model, data, label, graph_loss_func, graph_optimizer, train_loss, train_accuracy)\r\n\r\n        run_batch(graph_train_batch, steps_per_epoch, epochs)\r\n\r\n    t3_end = time.time()\r\n    print(\"Test 3 Time Elapsed {}\\n\".format(t3_end-t3_start))\r\n```\r\n\r\n**Other info / logs**\r\n[perf_test_tf_2_0.log](https://github.com/tensorflow/tensorflow/files/3287965/perf_test_tf_2_0.log)\r\n\r\n", "comments": ["I have reproduced the issue with Tensorflow Alpha 2.0.0 version on both CPU and GPU.", "@robieta can you triage this?", "Hello @alextp , @robieta ,\r\nI faced the same issue - i have replicated in a simpler scenario of model.fit() - not even GradientTape.\r\nI have created a [colab notebook](https://colab.research.google.com/drive/1PR3u4BaVnwBCoBoGSgK0kRNX_pMxzSep) to showcase this", "Do you get the same difference with TF2 beta1? (pip install tensorflow==2.0.0-beta1)", "@yashk2810 Yes my test was on beta1", "I saw two things in your code.\r\n\r\n1. You were not passing the training flag to the dropout layer in the model.\r\n2. You don't need .repeat() and steps_per_epoch anymore. (Though using it should be fine).\r\n\r\nI tried it out and I am getting consistent results (https://colab.sandbox.google.com/drive/1rzDpHynUY7geq1ivb8wvlki6SeLbqioP#scrollTo=CH_9oxXER3iC). \r\n\r\nFYI: You don;t need the tf.function decorator on your model since fit() executes inside a tf.function anyways.", "@yashk2810 Awesome! Many thanks! Much grateful.\r\nBoth (1) and (2) above works perfectly.\r\nCan you explain a bit on your statement on why tf.function is not needed? Anything to do with subclassed model?  When would t.function be needed then? \r\nAny pointers to documentation on this?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29762\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29762\">No</a>\n"]}, {"number": 29761, "title": "tf.train.Saver() problem when running session: Cannot assign a device for operation save/SaveV2: Could not satisfy explicit device specification '/device:GPU:1' because no supported kernel for GPU devices is available. Registered kernels", "body": "Having an issue when specificying graph/device placement and saving the model.\r\n\r\ngraph_model = tf.Graph()\r\n with graph_model.device(self.device):\r\n        with graph_model.as_default():\r\n         ...\r\n         ...\r\n         saver = tf.train.Saver()\r\n config = tf.ConfigProto()\r\n config.gpu_options.allow_growth = True\r\n with tf.Session(graph=graph_model,config=config) as sess:\r\n       sess.run(tf.global_variables_initializer())\r\n\r\n\r\nand i get this error..\r\n\r\nCaused by op 'save/SaveV2', defined at:\r\n  File \"/usr/local/pycharm-2019.1.1/helpers/pydev/pydevd.py\", line 1758, in <module>\r\n    main()\r\n  File \"/usr/local/pycharm-2019.1.1/helpers/pydev/pydevd.py\", line 1752, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/usr/local/pycharm-2019.1.1/helpers/pydev/pydevd.py\", line 1147, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/usr/local/pycharm-2019.1.1/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/jsidhom1/DeepTCR/ancillary_analysis/supervised/Supervised_Repertoire_Human.py\", line 119, in <module>\r\n    gcn=True,batch_size=10)\r\n  File \"/home/jsidhom1/DeepTCR/DeepTCR/DeepTCR.py\", line 3643, in Monte_Carlo_CrossVal\r\n    embedding_dim_genes=embedding_dim_genes,embedding_dim_hla=embedding_dim_hla)\r\n  File \"/home/jsidhom1/DeepTCR/DeepTCR/DeepTCR.py\", line 3400, in Train\r\n    GO.saver = tf.train.Saver()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 832, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 844, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 881, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 510, in _build_internal\r\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 210, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 124, in save_op\r\n    tensors)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1807, in save_v2\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation save/SaveV2: Could not satisfy explicit device specification '/device:GPU:1' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'\r\n\t [[node save/SaveV2 (defined at /home/jsidhom1/DeepTCR/DeepTCR/DeepTCR.py:3400) ]]\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29760, "title": "Updated categorical_hinge loss doc", "body": "This is just another minor update of the keras documentation.\r\nPlease feel free to make suggestions.", "comments": ["@pavithrasv Should be fine now."]}, {"number": 29759, "title": "Only check memory growth for visible GPUs", "body": "This was discovered in Issue #26460.\r\n\r\nPiperOrigin-RevId: 253082055", "comments": []}, {"number": 29758, "title": "TFlite object detection example contains unimplemented TFlite op", "body": "**System information**\r\n- OS Platform and Distribution Linux ubuntu : Ubuntu 16.04.4 LTS\r\n- TensorFlow installed from pip:\r\n- TensorFlow version 1.13.1:\r\n- TensorFlow repo commit version: 2b4245af999d33a7bdf31cf8b6de2db9c1d0afe5\r\n\r\nI followed the directions here, with the change that I did not quanitze (I hope this is not the problem)\r\n\r\nhttps://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193 \r\n\r\n\r\nThe command I ran\r\n```\r\nbazel run -c opt tensorflow/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --mean_values=128 --std_values=128 --change_concat_input_ranges=false0\r\n```\r\n\r\nproduced the following error and a zero byte file\r\n```-rw-rw-r--   1 ubuntu ubuntu        0 Jun 13 18:30 detect.tflite```\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\n```\r\n\r\n**Any other info / logs**\r\n\r\n```bazel run -c opt tensorflow/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --mean_values=128 --std_values=128 --change_concat_input_ranges=false\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=104\r\nINFO: Reading rc options for 'run' from /home/ubuntu/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Analyzed target //tensorflow/lite/toco:toco (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/lite/toco:toco up-to-date:\r\n  bazel-bin/tensorflow/lite/toco/toco\r\nINFO: Elapsed time: 0.390s, Critical Path: 0.00s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Running command line: bazel-bin/tensorflow/lite/toco/toco '--input_file=/tmp/detection_export_tflite_62750_eMdgL5/tflite_graph.pb' '--output_file=/tmp/detection_export_tflite_62750_eMdgL5/detect.tflite' '--input_shapes=1,300,300,3' '--input_arrays=normalized_input_image_tensor' '--output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostPrINFO: Build completed successfully, 1 total action\r\n2019-06-13 18:30:23.047578: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2019-06-13 18:30:23.070489: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 500 operators, 754 arrays (0 quantized)\r\n2019-06-13 18:30:23.090507: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 500 operators, 754 arrays (0 quantized)\r\n2019-06-13 18:30:23.120462: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 64 operators, 176 arrays (0 quantized)\r\n2019-06-13 18:30:23.122651: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 64 operators, 176 arrays (0 quantized)\r\n2019-06-13 18:30:23.123979: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 64 operators, 176 arrays (0 quantized)\r\n2019-06-13 18:30:23.126908: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 8640000 bytes, theoretical optimal value: 6480000 bytes.\r\n2019-06-13 18:30:23.127414: I tensorflow/lite/toco/toco_tooling.cc:434] Estimated count of arithmetic ops: 1.31037 billion (note that a multiply-add is counted as 2 ops).\r\n2019-06-13 18:30:23.128028: E tensorflow/lite/toco/toco_tooling.cc:462] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\nWe are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\n```\r\n", "comments": ["Found the answer\r\n\r\nThe flag `--allow_custom_ops` is needed for it to build TFLite_Detection_PostProcess.\r\n\r\nhere is my final command\r\n```\r\nbazel run -c opt tensorflow/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops\r\n```"]}, {"number": 29757, "title": "Update doc for categorical hinge", "body": "Hey, this is just a minor doc update. \r\nPlease feel free to suggest improvements.\r\n\r\nBtw. what am I doing wrong that my older commits always appear when I create a new pull request?\r\nI just create a new branch on my tf fork.", "comments": ["#29760"]}, {"number": 29756, "title": "Added couple of examples for tf.zeros_like", "body": "", "comments": []}, {"number": 29755, "title": "tf.Print was not rendered correctly in the website", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/Print\r\n\r\n## Description of issue (what needs changing):\r\n\r\n[tf.Print](https://www.tensorflow.org/api_docs/python/tf/Print) was not rendered correctly in the website. I understand that it was deprecated in TF2.0, but it should be rendered correctly in the TF website.\r\n\r\n### Correct links\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/Print\r\n\r\n\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@jvishnuvardhan Mind if I resolve this?", "@RonLek Please go ahead and resolve through PR. Thanks!", "That file is here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/logging_ops.py\r\n\r\nThere is a code block sample in the warning note. The deprecation notice should be short and any code examples should move under `def Print`.\r\n\r\nThanks for taking a look", "@jvishnuvardhan  @lamberta Please let me know if it works fine. Thanks!"]}, {"number": 29754, "title": "[Intel MKL] Adding support for rewriting Eager ops", "body": "This PR adds a mechanism to support an Eager op rewrite. This can be used by MKL or other optimizers.", "comments": ["Hi @akshaym,\r\n\r\nThank you for looking at this. The main motivation of this change is to enable the use of MKL_DNN optimized ops in eager mode if running in MKL-enabled version of Tensorflow. For example, if MKL is enabled, we want Conv2D to be re-written to _MKLConv2D to run the optimized version of the op.", "@alextp  - What do you think of the general mechanism?", "I don't understand what's the advantage of defining \"rewrites\" when there's not a graph. Why not just register MKL kernel implementations for the ops we want to accelerate?", "Thanks @akshaym. I have addressed your comments. ", "Thanks @alextp for your comments. I have addressed them.", "I fixed a small merge conflict. Can you please re-approve?", "@rthadur looks like the failing test is a flaky one. I tested locally and it is passing. I also saw it failing in other PRs.", "Hi @rthadur, any update on this?", "> Hi @rthadur, any update on this?\r\n\r\nIt is failing some internal tests , we are working on it."]}]