[{"number": 6237, "title": "Embedding projector: clicking on points doesn't select them.", "body": "In Chrome, clicking on points (regardless of dataset, PCA or T-SNE, 2D or 3D) will not select them. Win10, Chrome Version 55.0.2883.87 m", "comments": ["Hi @PAK90,\r\n\r\nHaving more information will help us debug your problem.\r\n\r\n- Are you referring to the standalone website (projector.tensorflow.org) or the tab in TensorBoard?\r\n- We used anti-aliasing but noticed that some users experienced the same problem as you, so we disabled it. If you saw the problem at projector.tensorflow.org, can you visit the website again and see if it works?\r\n- How many points were you using?\r\n- Is your zoom level 100%? If not, can you test if clicking on points at 100% zoom works?\r\n\r\nThanks for reporting this!\r\n\r\n", "This was using the standalone website, with the default provided MNIST dataset of 10000 points. After rebooting my pc it returned to working as normal. If I encounter it again I'll check zoom levels.", "Good to hear. I'll close the issue. If the problem appears again, feel free to reopen.", "Hi @dsmilkov @michaelisard , I know this is a very old thread but I was wondering if you ever figured out why some users experienced this problem? I am using some embedding projector code in an app and a few users are experiencing this."]}, {"number": 6236, "title": "Disable saved_model_test from cmake builds.", "body": "this target depends on half_plus_two, which is due for deletion.", "comments": []}, {"number": 6235, "title": "[Windows] Couldn't open CUDA library cupti64_80.dll", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n[This issue ](https://github.com/tensorflow/tensorflow/issues/5968) is not applicable for me as CuDNN is getting loaded. I checked the CuDNN folder manually, there is no such dll.\r\n\r\n### Environment info\r\nOperating System: Windows 10 Pro\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n```\r\nD:\\TensorFlow>nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Sat_Sep__3_19:05:48_CDT_2016\r\nCuda compilation tools, release 8.0, V8.0.44\r\n```\r\n\r\n```\r\ncudnn-8.0-windows10-x64-v5.1.zip\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: Sorry, I cant remember which package pip fetched. I just ran this command.\r\n```\r\nD:\\TensorFlow>pip3.5 install --upgrade tensorflow-gpu\r\nRequirement already up-to-date: tensorflow-gpu in c:\\users\\windows\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\r\nRequirement already up-to-date: wheel>=0.26 in c:\\users\\windows\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: six>=1.10.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: numpy>=1.11.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: protobuf==3.1.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: setuptools in c:\\users\\windows\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from protobuf==3.1.0->tensorflow-gpu)\r\n```\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n```\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally\r\n0.12.0-rc1\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport random\r\n\r\nvLength = 8\r\n\r\nnodeCount = [vLength,8,8,vLength]\r\n\r\nSUMMARY_DIR = 'D:/Summary/'\r\n\r\nBATCH_SIZE = 10\r\nTRAIN_SIZE = 100\r\n\r\ndef model(x_t):    \r\n    layerCount = len(nodeCount)\r\n    layer = [None for _ in range(layerCount)]\r\n    weights = [None for _ in range(layerCount - 1)]\r\n    biases = [None for _ in range(layerCount - 1)]\r\n    layer[0] = x_t\r\n    for i in range(layerCount-1):\r\n        weights[i] = tf.Variable(tf.random_normal([nodeCount[i],nodeCount[i+1]]))\r\n        biases[i] = tf.Variable(tf.random_normal([nodeCount[i+1]]))\r\n          \r\n        layer[i+1] = tf.add(tf.matmul(layer[i],weights[i]),biases[i])\r\n        if(i != layerCount-2):\r\n            layer[i+1] = tf.nn.tanh(layer[i+1])\r\n        else:\r\n            layer[i+1] = tf.nn.softmax(layer[i+1])\r\n\r\n    return layer[i+1]\r\n\r\ndef getNextBatch():\r\n    v = [0.0 for _ in range(vLength)]\r\n    v[0] = 1.0\r\n\r\n    r = [None for _ in range(BATCH_SIZE)]\r\n    for i in range(BATCH_SIZE):\r\n        random.shuffle(v)\r\n        r[i] = v.copy()\r\n    return r,r\r\n    \r\nm = None\r\ndef trainNN():\r\n    x_t = tf.placeholder(tf.float32,[None,vLength],'input')\r\n    y_t = tf.placeholder(tf.float32, [None, vLength],'actual')\r\n    \r\n    m = model(x_t)    \r\n\r\n    with tf.name_scope('Cost_Function'):\r\n        cost = tf.reduce_mean(-tf.reduce_sum(y_t * tf.log(m)))\r\n        \r\n    with tf.name_scope('Learning_Rate'):\r\n        learning_rate = tf.Variable(0.5,dtype=tf.float32)\r\n\r\n    with tf.name_scope('Optimizer'):\r\n        optimizer = tf.train.AdamOptimizer().minimize(cost)\r\n\r\n    with tf.name_scope('testing'):\r\n        correct = tf.equal(tf.argmax(y_t,1),tf.argmax(m,1))\r\n        accuracy = tf.reduce_mean(tf.cast(correct,'float'))\r\n        tf.summary.scalar('accuracy',accuracy)\r\n\r\n    epochs = 10\r\n\r\n    with tf.Session() as sess:\r\n        merged = tf.summary.merge_all()\r\n        \r\n        sess.run(tf.global_variables_initializer())\r\n        \r\n        for epoch in range(epochs):            \r\n            tw = tf.summary.FileWriter(SUMMARY_DIR+'/epoch'+str(epoch),sess.graph)\r\n            epochLoss = 0\r\n            c = 0\r\n            for i in range(TRAIN_SIZE):\r\n                bx,by = getNextBatch()    \r\n                fd = {x_t:bx,y_t:by}                \r\n                run_metadata = tf.RunMetadata()\r\n                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n                summary,c,_ = sess.run([merged,cost,optimizer],\r\n                              feed_dict=fd,\r\n                              options=run_options,\r\n                              run_metadata=run_metadata)\r\n                if(i%BATCH_SIZE == 0):\r\n                    tw.add_summary(summary,i)\r\n                    tw.add_run_metadata(run_metadata,'step%d'%i)\r\n                \r\n                epochLoss += c       \r\n            \r\n            tx,ty = getNextBatch()\r\n            print('Epoch ',epoch,'/',epochs,':',epochLoss/TRAIN_SIZE,' ',accuracy.eval({x_t:tx,y_t:ty}))\r\n       \r\n        tw.close()            \r\n\r\ntrainNN()\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nI googled the said dll file. No, results.\r\n\r\nCould PTI stand for Parameter Tuning Interface?\r\nI found [this](http://www.socsci.uci.edu/~jkrichma/CARLsim/) link, but I am hesitant to installed anything that is not prescribed officially.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\nD:\\TensorFlow>D:\\TensorFlow\\identity_dnn_bare.py\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce 930MX\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.0195\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.66GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce 930MX, pci bus id: 0000:01:00.0)\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cupti64_80.dll\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\default\\gpu\\cupti_wrapper.cc:59] Check failed: ::tensorflow::Status::OK() == (::tensorflow::Env::Default()->GetSymbolFromLibrary( GetDsoHandle(), kName, &f)) (OK vs. Not found: cuptiActivityRegisterCallbacks not found)could not find cuptiActivityRegisterCallbacksin libcupti DSO\r\n```\r\n", "comments": ["I have encountered this issue as well. It looks like on Windows, the cupti DLL is not on PATH by default, so you will have to add its location manually. Potentially, this can be added to TF installation instructions for Windows.", "I googled the dll, I am doubtful it is in the standard cuda or cudnn distribution.  Also I ran both the MNIST examples from the getting started, they ran fine. Several of my old code ran just fine. I am almost certain that the problem is somewhere with the summary writer.", "I have encountered this problem before. When you use CUDA 8.0,the file cupti64_80.dll lies in  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\CUPTI\\libx64. I just fixed the problem by copying the dll into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin, and the file cupti.lib in the same location into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64. And it works!", "@Ghost---Shadow does this work for you?", "@menggangmark Your solution worked perfectly for me. This resolved an issue I was having with running the basic [tensorboard demo](https://www.tensorflow.org/versions/r0.12/how_tos/summaries_and_tensorboard/index.html). Cheers!", "@michaelisard , yes it works in Windows 7 for tensorflow r0.12 and CUDA 8.0.", "Added ```C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\CUPTI\\libx64``` to ```PATH```. It works. Cheers.", "Great. Closing the issue.", "I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cublas64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:2294] Unable to load cuBLAS DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cufft64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_fft.cc:344] Unable to load cuFFT DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library curand64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_rng.cc:338] Unable to load cuRAND DSO.\r\n\r\ncuda8.0 cudnn 5.0 ", "@xia342560937  Is your `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin` folder in path?", "Thank you, the problem has been resolved, I re-installed again tensorflow, before the first installation of the anaconda, may be the wrong place, I will not English, you can understand these words, haha", "Hello\uff0cThere is no such file named CUPTI in 'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras',and only have file named visual_studio_integration.", "Here is my problem\r\n>>> import tensorflow as tf\r\nI d:\\build\\tensorflow\\tensorflow_gpu-r0.12\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cublas64_80.dll\r\nI d:\\build\\tensorflow\\tensorflow_gpu-r0.12\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:2294] Unable to load cuBLAS DSO.\r\nI d:\\build\\tensorflow\\tensorflow_gpu-r0.12\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally\r\nI d:\\build\\tensorflow\\tensorflow_gpu-r0.12\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cufft64_80.dll\r\nI d:\\build\\tensorflow\\tensorflow_gpu-r0.12\\tensorflow\\stream_executor\\cuda\\cuda_fft.cc:344] Unable to load cuFFT DSO.\r\nI d:\\build\\tensorflow\\tensorflow_gpu-r0.12\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\nI d:\\build\\tensorflow\\tensorflow_gpu-r0.12\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library curand64_80.dll\r\nI d:\\build\\tensorflow\\tensorflow_gpu-r0.12\\tensorflow\\stream_executor\\cuda\\cuda_rng.cc:338] Unable to load cuRAND DSO.\r\n>>>", "still not working\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library cublas64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:2294] Unable to load cuBLAS DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library cufft64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_fft.cc:344] Unable to load cuFFT DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library curand64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_rng.cc:338] Unable to load cuRAND DSO.", "![image](https://cloud.githubusercontent.com/assets/16426261/23336750/c114420e-fc13-11e6-882c-90c901a61be1.png)\r\n I can successfully open CUDA library cupti64_80.dll locally , but it just stop here  and can't forward. What should I do? help~~ ", "Because of this problem. Waste lots of days. Why I have to add the path, C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\CUPTI\\libx64 manually?\r\nEven on tensorflow 1.2.1", "> I have encountered this problem before. When you use CUDA 8.0,the file cupti64_80.dll lies in C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\CUPTI\\libx64. I just fixed the problem by copying the dll into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin, and the file cupti.lib in the same location into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64. And it works!\r\n\r\nThanks for sharing this. I had the same issue. I followed this method and it worked! \r\n\r\n```\r\nTensorflow v2.0 alpha release\r\nCuda v10.0\r\nCudnn v7.4.2\r\n```", "```\r\n> be me\r\n> revisiting tensorflow after years\r\n> install tensorflow 2.0.0 because its the future or something\r\n> google the error messages\r\n> MFW my own post shows up\r\n> MFW the solution works\r\n```\r\n"]}, {"number": 6234, "title": "Moving example models from github.com/tensorflow/tensorflow to github.com/tensorflow/models", "body": "See https://github.com/tensorflow/models/pull/731", "comments": ["Looks like we need internal changes to sync first before this can go through.", "Earlier today there was one sync. \r\nWhat time do the changes you need from?", "Also, do we need to clean up any tutorials or documentation?", "Deleted and recreated the branch because that was easier than rebasing. It looks like I'll have to make a new pull request though.\r\n\r\nFor your questions, it looks like the sync that went in should be good. Tutorials and docs have already been updated in CLs that I've submitted. I don't know how long it takes to update to tensorflow.org (which currently still has the old version), but I just checked and the site appears to successfully link to the r0.12 code, so we should be good to go on this."]}, {"number": 6233, "title": "Import tensorflow in ipython  Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH", "body": "I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: zsx-All-Series\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.57  Mon Oct  3 20:37:01 PDT 2016\r\nGCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) \r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 367.57.0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1080] LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1081] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libnvidia-fatbinaryloader.so.367.57: cannot open shared object file: No such file or directory\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n", "comments": ["A few weeks ago I installed and used successfully\uff0cBut yesterday, when used, out of this problem\u3002", "I have two PC graphics card, I just looked at the NVIDA X SERVER Setting. Set the Do not use integrated graphics after just fine.", "Is libcuda.so in your path? Your path is mentioned in the log:\r\nLD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:\r\n", "@zsxgb based on your last comment, you seemed to have resolved the issue?", "I have the exact same problem. Everything loads except `libcuda.so.1`\r\n\r\n`Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: $LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/lib64`\r\n\r\nI did notice that the file was missing in that path which is absurd cause i literally purged all Nvidia and CUDA stuff and did a fresh deb(network) install for Ubuntu 64-bit 16.04 from [here](https://developer.nvidia.com/cuda-downloads) . \r\n\r\nI looked into `/usr/lib/x86_64-linux-gnu/` and found that `libcuda.so.1` is a symbolic link to a file in that folder. I tried creating a symbolic link to that file in the `/usr/local/cuda/lib64` directory and tried to import tensorflow, it still dint work.\r\n\r\nIf someone who has CUDA 8.0 installed can tell us what `libcuda.so.1` links to I think this issue might get solved.", "in my system, it is under:\r\n`/usr/lib/x86_64-linux-gnu/` and it points to `libcuda.so.370.28`. `370.28` comes from my driver version.\r\nWhen you use APT to install cuda, libcuda ends up here.\r\n\r\nAlso:\r\n```\r\n$ echo $LD_LIBRARY_PATH\r\n/usr/local/cuda:/usr/local/cuda/lib64:\r\n```\r\n\r\nI see that your LD_LIBRARY_PATH has two paths that are exactly the same. maybe thats the problem?\r\n", "Okay I purged everything and installed nvidia-375 (with DKMS) and the cuda deb(network install) .\r\n\r\nAlthough I have 2 nvidia drivers now. The problem inst there.", "Did you fix your LD_LIBRARY_PATH?\r\nand do you mean the problem is not there?", "Yes the problem is gone now. I really just reinstalled everything dint try changing the LD_LIBRARY_PATH. I guess the cuda installer took care of it.\r\n\r\nThis is what I see now :\r\n\r\n    $ echo $LD_LIBRARY_PATH\r\n    :/usr/local/cuda/lib64\r\n", "@DollarAkshay \r\n\r\nit works for me, thanks!"]}, {"number": 6232, "title": "Format Error on the Website", "body": "Hi, on tensorflow.org there is part of documentation accidentally formatted into code. It's not a serious problem.  It's about initializer on r0.11. I didn't check if other versions have the same problem. Just  want to post it here for your information.\r\n\r\n![screen shot 2016-12-09 at 20 41 19](https://cloud.githubusercontent.com/assets/16264284/21070208/e29a628e-be4f-11e6-973e-e5a936a5f794.png)\r\n\r\n[link: https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#random_normal_initializer](https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#random_normal_initializer) (this actually won't lead you right to the location because of the error ).", "comments": ["This seems fixed at head. @xmbrst is this worth fixing in earlier versions?", "@tfboyd PTAL", "looks OK in the new website. closing."]}, {"number": 6231, "title": "Update installation instructions for tensorflow-gpu.", "body": "", "comments": []}, {"number": 6229, "title": "tf.contrib.learn.LinearClassifier.evaluate reports incorrect labels/prediction_mean", "body": "Relatively new to machine learning but this appears to be a reproducible bug. Have not been able to find related issues on web searches.\r\n\r\nOS is MacOS,  no CUDA. Installed using pip package:\r\n# Mac OS X, CPU only, Python 2.7:\r\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0rc0-py2-none-any.whl\r\ntensorflow version: 0.12.0-rc0\r\n\r\nI first noted discrepancy between on my data using a tf.contrib.learn.LinearClassifier. The evaluate function returned a 'labels/actual_label_mean' that was accurate but a 'labels/prediction_mean' that was too low. I was able to reproduce similar results (i.e. 'labels/prediction_mean' too low) using the linearclassifier tutorial at:\r\nhttps://www.tensorflow.org/versions/r0.12/tutorials/wide/index.html\r\n\r\nFollowing the tutorial, I get the following from `m.evaluate(input_fn=eval_input_fn, steps=1)`\r\n\r\n{'accuracy': 0.83582091,\r\n 'accuracy/baseline_label_mean': 0.23622628,\r\n 'accuracy/threshold_0.500000_mean': 0.83582091,\r\n 'auc': 0.88367212,\r\n 'global_step': 400,\r\n 'labels/actual_label_mean': 0.23622628,\r\n 'labels/prediction_mean': 0.23983434,\r\n 'loss': 0.35221672,\r\n 'precision/positive_threshold_0.500000_mean': 0.70658684,\r\n 'recall/positive_threshold_0.500000_mean': 0.52158087}\r\n\r\nThe  'labels/actual_label_mean' value exactly matches `sum(df_test.label)/len(df_test.label)`\r\n\r\nIf I actually use the predict using \r\n\r\n```\r\npred = m.predict(input_fn = lambda:input_fn(df_test))\r\npredictions = []\r\nfor i in range( df_test.shape[0] ):\r\n    predictions.append(pred.next())\r\n### I realize above code looks ugly but I can't get it to work in a less kludgy way, e.g.\r\n### predictions = list(pred) will hang indefinitely until control-c\r\n### Is this a separate bug?\r\n\r\nsum(predictions) -> 2839\r\nlen(predictions) -> 16281\r\n```\r\nThe ratio is 0.1743750 which is much lower than the reported value for 'labels/prediction_mean', 0.23983434.\r\n\r\nI think that this is a serious bug as predicting/guessing the more likely label artificially inflates the accuracy. As an extreme example, predicting 'no meteor today' will give you a 99.99_% accuracy.\r\n\r\nIf this is not a bug but rather a misunderstanding of how prediction/evaluation work, pardon my error.", "comments": ["@ispirmustafa can you comment?", "One other thing I forgot to mention regarding prediction: computed accuracy closely matches the results reported by the evaluate() function.\r\n\r\n```\r\nnumCorrect=0\r\nfor i in range(len( predictions) ):\r\n    if (predictions[i] == df_test.label[i]):\r\n        numCorrect = numCorrect +1\r\n\r\nnumCorrect --> 13641\r\nlen( predictions) --> 16281\r\n```\r\nThe ratio is 0.837847798046803 which matches 'accuracy' returned by evaluate() to 7 decimal places.\r\n\r\nThe mix of correct and incorrect results makes me wonder where the discrepancy lies: error in evaluate() or me. Once again, thank you in advance for your consideration and time.", "labels/prediction_mean is based on probability predictions.\r\nest.predict returns predicted classes not the probabilities.\r\nTo compare them, you can call est.predict_proba"]}, {"number": 6228, "title": "Add concat_v2 to array_ops.py. Also, add gradient support for concat_\u2026", "body": "\u2026v2 op.\r\n\r\nChange: 139839646\r\n\r\nWe just need this for final r0.12, too late for rc1.", "comments": ["DONOTMERGE until r0.12 is released.", "I mean, rc1\r\n", "This change is good to go. Please approve and merge whenever you have time.\r\nJenkins, test this please.", "rc1 is out, should this be merged now?", "yes please. we can go ahead as all the rc1 branch is merged back into master\n\n\nOn Mon, Dec 12, 2016 at 3:43 PM, Martin Wicke <notifications@github.com>\nwrote:\n\n> rc1 is out, should this be merged now?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6228#issuecomment-266589821>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOay4ilCYPKdcO0yL5xnBdXvFPcUvks5rHdwegaJpZM4LJdUf>\n> .\n>\n", "Do we have a tracker to follow up undoing these?\nI am 90% sure that we will forget undoing these without a tracker.\n\nOn Mon, Dec 12, 2016 at 5:18 PM, Martin Wicke <notifications@github.com>\nwrote:\n\n> Merged #6228 <https://github.com/tensorflow/tensorflow/pull/6228>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6228#event-891315530>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOT7FX0LQNSouOTFELmyoNaoRfw1yks5rHfJwgaJpZM4LJdUf>\n> .\n>\n", "@aselle Do you? We have a list, but I wouldn't call it a tracker.", "What aspect do we want to undo? You mean remove the _v2 names?"]}, {"number": 6227, "title": "Fix split_v docs", "body": "Fixes some inconsistencies and copy/paste typos in split_v's code.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it", "This is really just some typo fixes. I don't think it should take that long to approve.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "I signed it with my other email as well now.", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "I think, this has been fixed internally. This is same as my PR #6208. :)", "ok great. I'll close this then.", "@martinwicke Has the batch norm comment in split_v benchmark also been updated?", "yes, but thank you for noticing and submitting a PR.  I hope we can accept your next one."]}, {"number": 6226, "title": "Feature request to specify substitutions in embedding_lookup for invalid indices ", "body": "In the current master code, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gather_op.cc it says that the attribute _validate_indices_ is not supported anymore:\r\n` // We used to grab the validate_indices attribute here, but now we\r\n    // always validate indices since the speed difference was only 1.5%.\r\n    // TODO(irving): Remove the validate_indices attribute once we have\r\n    // support for removing attrs in a backwards compatible way.`\r\n\r\nI actually do want an option to specify validate_indices as false. I am using _embedding_lookup_ and it would be useful for out of vocabulary words that are not in the embedding matrix.  \r\n\r\nIf it's not possible to support _validate_indices_, can you suggest a way to get all zeros (instead of an exception) for indices that are not in the embedding matrix.", "comments": ["I have 1.5 hours left before parental leave, so I'm not going to get to this.  However, this bug isn't about \"bringing back validate indices\"; it's about doing something entirely different where invalid indices are replaced with something else.", "Yes. If there's a way to specify substitutions for invalid indices, that would be great! \r\n\r\n(PS: congrats!)", "I think the request was to bring back validate_indices as a preferred course of action, and if not, to suggest the workaround that would do something else entirely. Who is a good person to look at this?", "\"Bringing back validate indices\" doesn't make any sense, since the default today is check for invalid indices.  If we allowed turning that off, it would mean bringing back the ability for tf.gather to segfault.  You might get lucky and have the segfault do what you what, but it's not likely.\r\n\r\n@ebrevdo is a good person to ask.", "Replace invalid indices with 0 via tf.where/tf.select and after the embedding lookup, replace the output rows you got from the invalid indices with tf.zeros, again using broadcasting tf.where/tf.select"]}, {"number": 6225, "title": "Don't fetch sample Android app deps in configure.", "body": "`bazel fetch` runs the Bazel loading phase on the given targets. At the\r\nmoment, the loading phase of an android_binary succeeds even if an\r\nandroid_sdk_repository is not set up in the WORKSPACE. However, this is\r\ndeceptive as the purpose of the loading phase is to ensure that all\r\nfiles needed for the build are present. Without an android_sdk_repository\r\nset up, this is not the case. In the future, Bazel will likely not allow\r\nthe loading phase to succeed for android_binary without an\r\nandroid_sdk_repository.\r\n\r\nUnfortunately, `bazel fetch` does net support the target substitution\r\nsyntax (see https://github.com/bazelbuild/bazel/issues/2220) so I've\r\nadded a TODO to remove the necessary nested `bazel query` once that\r\nissue is resolved.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins Test this, please.", "I guess I should add some motivation for this. I'm planning to make a change in Bazel shortly that will break Tensorflow's `configure` script without this change.\r\n\r\nI tried to submit this change a while ago in https://github.com/bazelbuild/bazel/commit/af8ebca2b88a0ee1129b72b4ce825f6489c31e67, which inadvertently broke this `configure` script (see https://github.com/bazelbuild/bazel/issues/1880).", "Is there anything else I need to do for this PR, or is \"andrewharp approved these changes\" sufficient to merge it?", "Can someone merge this please?", "Thanks! :)"]}, {"number": 6224, "title": "Extremely inefficient cuda event polling", "body": "I am running tensorflow with this application: https://github.com/openai/pixel-cnn. I am running on 4 NVIDIA K80 cards and CUDA 7.5. For my experiment, I modified the code to only do 100 training iterations.\r\n\r\nDuring the 100 iterations, tensorflow destroys 2662 cuda events. But how many times it polls event status? 40669120 times. This means on average it polls ~15,000 times for 1 destroy. Isn't it extremely inefficient?\r\n\r\nPer my understanding of tensorflow source code, it uses one dedicated thread to poll event status and destroy an event once it's complete. That's the only place where the event status is polled.", "comments": ["@zheng-xq do you want to comment?", "Adding poxvoculi@\r\n\r\nIn general, TensorFlow polls the event to make sure the kernel actually finishes. It effectively synchronizes between host and device while respecting the execution model. I'll leave it up to poxvoculi@ to answer whether 15,000:1 is the right ballpark between polling and event count. ", "@ZhengBitFusion: I don't think your implied measure of efficiency is most useful in this case.  \r\n\r\nIf your measure is to poll the minimum number of times per queued event, then the optimal strategy would be to poll at an extremely long interval, say once per second or slower.  However, that would result in extremely slow overall program execution.\r\n\r\nThe kind of efficiency most people care more about is getting the program to execute as quickly as possible, which is most easily accomplished by polling much more frequently, so that any potential computation waiting on completion of an event can begin as soon as possible.\r\n\r\nMore generally, there is some conflict between increased polling frequency and overall computation speed, but only to the extent that use of one thread for polling blocks other necessary work that might otherwise use that thread.  And the extra gain provided by the part of that one thread not used for polling must be greater than the loss from polling less frequently.  If your TF program is mostly executing on GPUs, and you're using a modern CPU with many cores and hyperthreading, this seems unlikely.  \r\n\r\nIf you look at the polling logic source code in\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc#L123\r\nyou can see the tradeoff explained, and that we try to achieve a good balance by polling at two different frequencies, depending on whether the last poll found the event queue empty.\r\n\r\nAt the slower polling frequency,  it should poll about 1000/sec, which should consume only a small fraction of one core.  At the higher frequency it should poll about 100k/sec, which still probably doesn't consume a full core on a good processor.  You didn't say how long your 100 iterations takes, but it sounds like your program always had events queued, and hence always polled at full speed.\r\n\r\nWe arrived at this polling strategy by experimentation with some test programs.  It's possible that a different strategy would result in slightly faster execution for your program.  Somewhat more likely, it's possible that a different strategy could yield equivalent performance but poll a little less often, if that matters to you.  \r\n\r\nIn any case, if you devise an alternative polling strategy that consistently yields better performance in practice, that would make a nice contribution.", "Can you please briefly explain why the lower polling frequency could potentially slow down the overall program execution? @poxvoculi \r\n\r\nFrom your reply, looks like it may affect any computation that is waiting on completion of event. But looks to me the polling would only lead to the destroy of events once these events are completed.\r\n\r\nIf in any case the computation would wait on polling of events to proceed, then the execution model is not efficient. Events should automatically unblock computation with CUDA API like cuStreamWaitEvent, which I believe is in fact used by tensorflow in this regard.\r\n\r\nSo it only makes sense to me that polling of events is used to release event resources once the events are complete. Then polling frequency is a matter that you want to have as lower frequency as possible but still make sure the GPUs are not out of resources for event allocation.\r\n", "The events are used to queue the freeing of memory that must stay live\nuntil completion.  If memory is not freed promptly it can cause the max\nmemory in use to exceed that available.  In that circumstance either the\nprogram fails or it slows down to wait for memory to be freed.\n\nYou could try varying the polling to see what happens.\n\n\nOn Mon, Dec 12, 2016 at 12:38 PM, Zheng <notifications@github.com> wrote:\n\n> Can you please briefly explain why the lower polling frequency would slow\n> down the overall program execution?\n>\n> From your reply, looks like it may affect any computation that is waiting\n> on completion of event. But looks to me the polling would only lead to the\n> destroy of events once these events are completed.\n>\n> If in any case the computation would wait on polling of events to proceed,\n> then the execution model is not efficient. Events should automatically\n> unblock computation with CUDA API like cuStreamWaitEvent, which I believe\n> is in fact used by tensorflow in this regard.\n>\n> So it only makes sense to me that polling of events is used to release\n> event resources once the events are complete. Then polling frequency is a\n> matter that you want to have as lower frequency as possible but still make\n> sure the GPUs are not out of resources for event allocation.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6224#issuecomment-266545533>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AO818Rgi848mm3NfnyIDsWR70ivzpyJtks5rHbDFgaJpZM4LJWUJ>\n> .\n>\n", "Another common use of the events is to enable operations across device boundaries. For example,  after a GPU-to-CPU memcpy, enable CPU operations that depend on the copied results. \r\n", "@zheng-xq I assume you want to use event polling to enable the CPU operations? That's not the correct way of using events here. Correct thing to use in this scenario is callback.", "I don't think there is incorrectness here, unorthodox maybe. Cuda intentionally encourages different use patterns in this area. At the time when this code was written, active polling was more efficient than Cuda callback, since it betters matches the remaining TF threading model.\r\n\r\nAfter a few generations of Cuda drivers, this might have changed. Anyone is welcome to give a try and see whether there is improvement. The concern is to really performance in complicated models. ", "@poxvoculi I tested with 1/1000 query frequency using this application https://github.com/openai/pixel-cnn. It's actually 1% faster on native CUDA driver, and 10% faster on a remote CUDA service, meaning that the CUDA APIs are forwarded to a remote system.\r\n\r\nSince it is to protect against out-of-resource, why to use so many resources in the first place? Could the events be reused? Or, can the events be self-destroyed? A naive approach would be that once the event is recorded, insert a callback immediately after to destroy it, if you only want to record the event for the last time. Or, can the query frequency be adjustable at runtime?", "@ZhengBitFusion It's interesting that you're seeing a 10% speedup with a remote CUDA service.  That's not a configuration that we tried or considered during development.  This part of TensorFlow was developed and tuned some time ago, before we fully settled some other details, and not reconsidered since.  And of course the GPU environment has continued to evolve.  It would be nice to drive a reexamination of the EventMgr design with a benchmark suite that could be tested on multiple system configurations.  Unfortunately I don't know of anything especially suited to this purpose.  My recollection is that we tuned using a data-parallel version of inception.\r\n\r\nIn the short term I think it would be good to replace the timing constants kPollingDelayUsecs and kPollingSuspendMsecs with values that can be user-set in GPUOptions \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L14\r\nThis will allow you to immediately improve your remote CUDA case.\r\n\r\nIn the longer term, we should gather or develop an approprate benchmark suite and try using the cuda callback mechanism.", "Sounds good. Thanks.", "FYI. From our experiments (on CUDA 7.5 at least), callback is not as efficient. Inserting callbacks into compute streams hurts the performance dramatically. So scratch my idea of using callbacks to signal event completion instead of polling."]}, {"number": 6223, "title": "Branch 141580230", "body": "Pushing internal changes", "comments": ["The half_plus_two rules are referenced also from:\r\ntensorflow/cc/saved_model/BUILD  loader_test rule\r\ntensorflow/python/saved_model/BUILD  saved_model_test\r\ntensorflow/contrib/session_bundle/BUILD  bundle_shim_py_test session_bundle_test session_bundle_py_test rules.", "@andrewharp the current test failures can be seen in your branch.\r\n`extract` function is defined twice in `tensorflow/contrib/learn/python/learn/estimators/estimator_test.py`\r\nmaybe a merge conflist resolution issue?", "Once the tests pass, good to go", "The build \"Linux CPU Tests\" failed because several tutorials in tensorflow/models have been removed. This is not a big deal - we can fix it easily by removing them from test_tutorials.sh.\r\n\r\nThe \"Linux GPU\" build failed due to two timeouts:\r\n//tensorflow/python/kernel_tests:fft_ops_test\r\n//tensorflow/python/kernel_tests:shape_ops_test\r\n\r\nThe first one looks like an existing flaky timeout. The second one may be related to this push? In any case, I think we should move ahead and merge this pushing PR. @gunan @andrewharp ", "If I had to guess, the Windows cmake failure is also due to half_plus_two_data being missing: https://github.com/tensorflow/tensorflow/pull/6223/files#diff-79d31bc9f4560bd3b40a78be48d3a228"]}, {"number": 6222, "title": "slice_input_producer output tensor of wrong shape under certain conditions", "body": "From the verison\r\ntensorflow-0.12.0rc0-cp27-cp27mu-manylinux1_x86_64.whl\r\n\r\nWhen using  tf.train.slice_input_producer with a tensor_list containing only a single tensor, the output from the tf.train.batch has incorrect dimension.\r\n\r\nCode to reproduce it\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndata=tf.random_uniform(shape=(50, 50))\r\nlabels=tf.ones(50)\r\nsample, _=tf.train.slice_input_producer([data, labels])\r\nbatch=tf.train.batch([sample], batch_size=8)\r\n\r\nprint batch\r\n\r\nsample=tf.train.slice_input_producer([data])\r\nbatch=tf.train.batch([sample], batch_size=8)\r\n\r\nprint batch\r\n\r\n```\r\n\r\nthe output is\r\n\r\n```\r\nTensor(\"batch:0\", shape=(8, 50), dtype=float32)\r\nTensor(\"batch_1:0\", shape=(8, 1, 50), dtype=float32)\r\n\r\n```\r\n\r\nI think the two output should be the same? Simple workaround like reshaping the batch data is feasible. However, I think this is a bug.", "comments": ["@ebrevdo is this WAI?", "Good question. I'm ooo for 2 weeks.  Ispir may know better.", "slice_input_producer returns a list.\r\nFirst call expands the list due to double assignment.\r\nSecond call doesn't expand it.\r\nsecond call of batch should be as follow:\r\nbatch=tf.train.batch(sample, batch_size=8)\r\n"]}, {"number": 6221, "title": "Delete zombie code", "body": "- seq2seq_test.py was deleted in cl/141309717 but added back by a\r\n  GitHub sync due to a bug where files deleted in perforce are not\r\n  deleted in the GitHub repository.\r\n\r\n- status_helper.i was not referenced by any rules and appears to be\r\n  dead code in the truest sense.\r\n\r\n- jpeg.BUILD was moved to third_party/jpeg.BUILD a while back.\r\n\r\nCC: @xiejw ", "comments": ["Test failures appear to be unrelated. PTAL @martinwicke. This should be a safe change. Nothing references that other file. Nothing.", "LGTM. "]}, {"number": 6220, "title": "Upgrade from r11 to r12 prodeuces \"Variables not defined\" when using any optimizer but GradientDescentOptimizer", "body": "After a recent upgrade to the latest version of tensorflow in github, several things stop working. I found out that all the optimizers, such as Adam or Adagrad are now producing an error related to variable scope that I have not managed to solve yet. However, GradientDescentOptimizer works fine.\r\n\r\nIt may be related to the issue: https://github.com/tensorflow/tensorflow/issues/5652\r\n\r\nThe error looks like this:\r\n```\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 651, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable filter/Adadelta/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\r\n```\r\n\r\nIt works fine with tensorflow r11\r\n\r\nOperating System: Ubuntu 16 and Ubuntu 14\r\nInstalled version of CUDA and cuDNN: cuda 8.0, cuda 5.1\r\n[cuda.txt](https://github.com/tensorflow/tensorflow/files/642888/cuda.txt)\r\n The commit hash  6dc8deaed8d8bd9cc6d52a03474d0b82891c8b86\r\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\r\nBuild timestamp: 1478109254\r\nBuild timestamp as int: 1478109254\r\n\r\nFind below a minimal version that causes the error:\r\n```\r\nimport tensorflow as tf\r\nimport pdb\r\n\r\ndef main():\r\n\r\n    ## !!! change this to test the different behaviors !!!\r\n    #optimizer = tf.train.GradientDescentOptimizer(1e-3)                 # This one is working\r\n    optimizer = tf.train.AdamOptimizer(1e-3, beta1=0.9, beta2=0.999999) # This one is not working\r\n    #optimizer = tf.train.AdagradOptimizer(1e-3)                         # This one is not working\r\n    #optimizer = tf.train.AdadeltaOptimizer(1e-3)                        # This one is not working\r\n\t\r\n    list_grads = []\r\n    for i in xrange(2):\r\n        with tf.device('/gpu:%d' % i):\r\n            with tf.name_scope('%d' % i) as scope:\r\n                W = tf.get_variable(name=\"filter\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\r\n                X = tf.get_variable(name=\"data\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\r\n                Y_ = tf.get_variable(name=\"out\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\r\n                Y = W+X\r\n                loss =tf.reduce_mean(Y-Y_)\r\n                grad = optimizer.compute_gradients(loss)\r\n                list_grads.append(grad)\r\n\r\n                tf.get_variable_scope().reuse_variables()\t\r\n    \r\n    grads = list_grads[0] + list_grads[1]\r\n    #pdb.set_trace()\r\n\r\n    op_train = optimizer.apply_gradients(grads)\r\n\r\n    init_global = tf.global_variables_initializer()\r\n    init_local =  tf.local_variables_initializer()\r\n\r\n    sess = tf.Session()\r\n    sess.run([init_global, init_local])\r\n\r\n    _, sol = sess.run([op_train, loss])\r\n    print(str(sol))\r\n\r\nif (__name__ == '__main__'):\r\n\tmain()\r\n```\r\n", "comments": ["Comment and uncomment the different optimizers to see the behavior. As explained, the only work working is GradientDescentOptimizer. This behavior does not occur in version r11 and it does not happen either if the averaging is not performed. Any clue?", "In particular, this commit causes the problem: 0fc86dd.", "Please, tell me if I can help. ", "You can revert the changes to slot_creater.py or fix the changes and send a PR. Thanks.", "Sorry sherry -- the current behaviour is correct. Your code is leaking reuse -- it just wasn't checked before. It could cause all other troubles, and I think we should correct the leaky reuse cases, not revert the slot change. I'll write more on the test cases, closing this.", "Then, just to be clear. How do we get the desired results? Does the reuse need to be done in a different way? This has been directly taken from the cifar10 multi-gpu example.\r\n\r\nThanks", "To clarify, we just need to put a scope around the model-construction part.\r\n\r\n```\r\nwith tf.variable_scope(tf.get_variable_scope()) as scope:\r\n  for i in xrange(2):\r\n    ... code as before until ...reuse_varables() ....\r\n\r\ngrads = list_grads[0] + list_grads[1]\r\n... rest of code as before ...\r\n```\r\n\r\nHope that helps!", "Thanks lukaszkaiser. It works perfectly fine now!", "@lukaszkaiser Hello, I found that your workaround to put a variable_scope which wraps the outermost num_gpus loop, but I am still confused why it does eliminate the error. \r\n\r\n```python\r\nwith tf.variable_scope(tf.get_variable_scope()) as vscope:\r\n  for i in xrange(FLAGS.num_gpus):\r\n    with tf.device('/gpu:%d' % i):\r\n      with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:\r\n        loss = tower_loss(scope)\r\n        tf.get_variable_scope().reuse_variables()     # HERE\r\n```\r\n\r\nIs it just because that the `tf.get_variable_scope()` (which is identical to `vscope`) is explicitly created than the implicit default? Then, what do these two `VariableScope` objects differ in?\r\n\r\nWhat do you mean by \"leaky reuse\"? Could you please clarify me? \r\n/cc @cesc-park ", "Sure, let me try to clarify.\r\n\r\nWhen you do `tf.get_variable_scope().reuse_variables()` you set the current scope to reuse variables. If you call the optimizer in such scope, it's trying to reuse slot variables, which it cannot find, so it throws an error. If you put a scope around, the `tf.get_variable_scope().reuse_variables()` only affects that scope, so when you exit it, you're back in the non-reusing mode, the one you want.\r\n\r\nHope that helps, let me know if I should clarify more.", "Ah, great. Your explanation is clear and helpful. Thanks!\r\n\r\nTo sum, a thing to remember is that **where the (Adam-like) optimizer acts**, i.e. `opt.apply_gradients(...)` (which is where the error is thrown from) **should** lie in the scope with **`reuse=False`** in order to properly create the slot variables.", "@wagonhelm  hello,when use Adam,how do you solve it?please tell  me the details,thank you", "Find below a minimal version that causes the error:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nclass SimpleModel:\r\n    def __init__(self):\r\n        self.loss = self.calc_loss()\r\n        self.train = self.train_model(self.loss)\r\n    def calc_loss(self):\r\n        W = tf.get_variable(\"w\", [1])\r\n        b = tf.Variable(tf.zeros([1]))\r\n        y = W * x_data + b\r\n        return tf.reduce_mean(tf.square(y - y_data))\r\n    def train_model(self, loss):\r\n        return tf.train.AdamOptimizer(0.5).minimize(loss)\r\n        # return tf.train.GradientDescentOptimizer(0.5)\r\nx_data = np.random.rand(100).astype(np.float32)\r\ny_data = x_data * 0.1 + 0.3\r\ns1 = SimpleModel()\r\ntf.get_variable_scope().reuse_variables()\r\ns2 = SimpleModel()\r\n```\r\nThe error looks like this:\r\n```\r\n File \"D:\\MyProgram\\Install\\Anaconda3\\envs\\tensorflow121\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 682, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable embeddings/Adam_2/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n```\r\ntensorflow version\r\n```\r\n1.2.1\r\n```", "@lukaszkaiser Hi, I've confronted with this problem when using AdamOptimizer, I've tried your suggestion but it still doesn't work. Could you please help me change the code?\r\nBesides, I'm not very familiar with TF, wish you can help me point out anything not appropriate in this code. Thank you!\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport datetime\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data/')\r\nos.environ['CUDA_VISIBLE_DEVICES']='4'\r\n\r\nsample_image = mnist.train.next_batch(1)[0]\r\nprint (sample_image.shape)\r\n\r\nsample_image = sample_image.reshape([28,28])\r\nplt.imshow(sample_image,cmap='Greys')\r\n\r\ndef discriminator(images,reuse=False,):\r\n    if(reuse):\r\n        tf.get_variable_scope().reuse_variables()\r\n\r\n    with tf.variable_scope('D_conv1'):\r\n        d_w1 = tf.get_variable('d_w1',[5,5,1,32],initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n        d_b1 = tf.get_variable('d_b1',[32],initializer=tf.constant_initializer(0))\r\n        d1 = tf.nn.conv2d(input=images,filter=d_w1,strides=[1,1,1,1],padding='SAME')+d_b1\r\n        d1 = tf.nn.relu(d1)\r\n        d1 = tf.nn.avg_pool(d1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\r\n\r\n    with tf.variable_scope('D_conv2'):\r\n        d_w2 = tf.get_variable('d_w2',[5,5,32,64],initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n        d_b2 = tf.get_variable('d_b2',[64],initializer=tf.constant_initializer(0))\r\n        d2 = tf.nn.conv2d(input=d1,filter=d_w2,strides=[1,1,1,1],padding='SAME')+d_b2\r\n        d2 = tf.nn.relu(d2)\r\n        d2 = tf.nn.avg_pool(d2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\r\n\r\n    with tf.variable_scope('D_fcn3'):\r\n        d_w3 = tf.get_variable('d_w3',[7*7*64,1024],initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n        d_b3 = tf.get_variable('d_b3',[1024],initializer=tf.constant_initializer(0))\r\n        d3 = tf.matmul(tf.reshape(d2,[-1,7*7*64]),d_w3)+d_b3\r\n        d3 = tf.nn.relu(d3)\r\n\r\n    with tf.variable_scope('D_fc4'):\r\n        d_w4 = tf.get_variable('d_w4',[1024,1],initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n        d_b4 = tf.get_variable('d_b4',[1],initializer=tf.constant_initializer(0))\r\n        d4 = tf.matmul(d3,d_w4)+d_b4\r\n        d4 = tf.nn.sigmoid(d4,name='d4')\r\n\r\n    return d4\r\n\r\ndef generator(z,batch_size,z_dim):\r\n    g_w1 = tf.get_variable('g_w1',[z_dim,56*56],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b1 = tf.get_variable('g_b1',[56*56],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g1 = tf.matmul(z,g_w1)+g_b1\r\n    g1 = tf.reshape(g1,[-1,56,56,1])\r\n    g1 = tf.contrib.layers.batch_norm(g1,epsilon=1e-5,scope='bn1')\r\n    g1 = tf.nn.relu(g1)\r\n\r\n    g_w2 = tf.get_variable('g_w2',[3,3,1,z_dim/2],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b2 = tf.get_variable('g_b2',[z_dim/2],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g2 = tf.nn.conv2d(g1,filter=g_w2,strides=[1,1,1,1],padding='SAME')+g_b2\r\n    g2 = tf.contrib.layers.batch_norm(g2,epsilon=1e-5,scope='bn2')\r\n    g2 = tf.nn.relu(g2)\r\n    g2 = tf.image.resize_images(g2,[56,56])\r\n\r\n    g_w3 = tf.get_variable('g_w3',[3,3,z_dim/2,z_dim/4],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b3 = tf.get_variable('g_b3',[z_dim/4],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g3 = tf.nn.conv2d(g2,filter=g_w3,strides=[1,1,1,1],padding='SAME')+g_b3\r\n    g3 = tf.contrib.layers.batch_norm(g3,epsilon=1e-5,scope='bn3')\r\n    g3 = tf.nn.relu(g3)\r\n    g3 = tf.image.resize_images(g3,[56,56])\r\n\r\n    g_w4 = tf.get_variable('g_w4',[1,1,z_dim/4,1],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b4 = tf.get_variable('g_b4',[1],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g4 = tf.nn.conv2d(g3,filter=g_w4,strides=[1,2,2,1],padding='SAME')+g_b4\r\n    g4 = tf.nn.sigmoid(g4)\r\n\r\n    return g4\r\n\r\ntf.reset_default_graph()\r\nbatch_size =100\r\nz_dimension = 100\r\n\r\nz_placeholder = tf.placeholder(tf.float32,[None,z_dimension],name='z_placeholder')\r\nx_placeholder = tf.placeholder(tf.float32,[None,28,28,1],name='x_placeholder')\r\n\r\nwith tf.variable_scope(tf.get_variable_scope()):\r\n    Gz = generator(z_placeholder,batch_size,z_dimension)\r\n    Dx = discriminator(x_placeholder)\r\n    Dg = discriminator(Gz,reuse=True)\r\n\r\nd_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(Dx),logits=Dx))\r\nd_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(Dg),logits=Dg))\r\ng_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(Dg),logits=Dg))\r\n\r\ntvars = tf.trainable_variables()\r\nd_vars = [var for var in tvars if 'd_' in var.name]\r\ng_vars = [var for var in tvars if 'g_' in var.name]\r\n\r\nprint ([v.name for v in d_vars])\r\nprint ([v.name for v in g_vars])\r\n'''\r\nd_trainer_real = tf.train.GradientDescentOptimizer(0.0003).minimize(d_loss_real,var_list=d_vars)\r\nd_trainer_fake = tf.train.GradientDescentOptimizer(0.003).minimize(d_loss_fake,var_list=d_vars)\r\ng_trainer = tf.train.GradientDescentOptimizer(0.001).minimize(g_loss,var_list=g_vars)\r\n'''\r\nwith tf.variable_scope(tf.get_variable_scope()):\r\n    d_trainer_real = tf.train.AdamOptimizer(0.0003).minimize(d_loss_real, var_list=d_vars)\r\n    tf.get_variable_scope().reuse_variables()\r\n    d_trainer_fake = tf.train.AdamOptimizer(0.0003).minimize(d_loss_fake, var_list=d_vars)\r\n    g_trainer = tf.train.AdamOptimizer(0.0001).minimize(g_loss, var_list=g_vars)\r\n\r\ntf.summary.scalar('Discriminator_loss_real',d_loss_real)\r\ntf.summary.scalar('Discriminator_loss_fake',d_loss_fake)\r\ntf.summary.scalar('Generator_loss',g_loss)\r\n\r\nimages_for_tensorboard = generator(z_placeholder,batch_size,z_dimension)\r\ntf.summary.image('Generated_images',images_for_tensorboard,5)\r\nmerged = tf.summary.merge_all()\r\nlogdir = 'Tensorboard/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S') + '/'\r\nwriter = tf.summary.FileWriter(logdir,sess.graph)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\n#Pre-train discriminator\r\nfor i in range(3000):\r\n    z_batch = np.random.normal(0, 1, size=[batch_size,z_dimension])\r\n    real_image_batch = mnist.train.next_batch(batch_size)[0].reshape([batch_size,28,28,1])\r\n    _,__,dLossReal,dLossFake = sess.run([d_trainer_real,d_trainer_fake,d_loss_real,d_loss_fake],\r\n                                       feed_dict={x_placeholder:real_image_batch,z_placeholder:z_batch})\r\n    if(i%100==0):\r\n        print ('dLossReal: ',dLossReal,'dLossFake: ',dLossFake)\r\n\r\n#Train discriminator and generator together\r\nfor i in range(100000):\r\n    real_image_batch = mnist.train.next_batch(batch_size)[0].reshape([batch_size,28,28,1])\r\n    z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\r\n\r\n    _,__,dLossReal,dLossFake = sess.run([d_trainer_real,d_trainer_fake,d_loss_real,d_loss_fake],\r\n                                        feed_dict={x_placeholder:real_image_batch,z_placeholder:z_batch})\r\n    z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\r\n    _ = sess.run(g_trainer,feed_dict={z_placeholder:z_batch})\r\n\r\n    if i%10 ==0:\r\n        z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\r\n        summary = sess.run(merged,feed_dict={x_placeholder:real_image_batch,z_placeholder:z_batch})\r\n        writer.add_summary(summary,i)\r\n\r\n    if i%100==0:\r\n        print ('Iteration:',i,'at',datetime.datetime.now())\r\n        z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\r\n        generated_image = generator(z_placeholder,1,z_dimension)\r\n        images = sess.run(generated_image,feed_dict={z_placeholder:z_batch})\r\n        plt.imshow(images[0].reshape([28,28]),cmap='Greys')\r\n        plt.savefig('Generated_images/'+str(i)+'.jpg')\r\n\r\n        img = images[0].reshape([1,28,28,1])\r\n        result = discriminator(x_placeholder)\r\n        estimate = sess.run(result,feed_dict={x_placeholder:img})\r\n        print ('Estimate:',estimate)\r\n\r\n```", "@Huayra007\r\nif you remove the \r\n```\r\nimages_for_tensorboard = generator(z_placeholder,batch_size,z_dimension)\r\ntf.summary.image('Generated_images',images_for_tensorboard,5)\r\n```\r\nshould be able to run it. You are calling 2 time your generator. So or you remove the snippet or you add reuse to your generator code as such:\r\n\r\n```\r\n\r\ndef generator(z,batch_size,z_dim,reuse=False):\r\n    if (reuse):\r\n        tf.get_variable_scope().reuse_variables()\r\n\r\n    g_w1 = tf.get_variable('g_w1',[z_dim,56*56],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b1 = tf.get_variable('g_b1',[56*56],dtype=tf.float32,initializ\r\n\r\n\r\n\r\n```\r\n\r\nand when you call it for the tensorboard as such:\r\n```\r\nwith tf.variable_scope(tf.get_variable_scope()) as scope:\r\n    images_for_tensorboard = generator(z_placeholder, batch_size, z_dimensions,reuse=True)\r\n    tf.summary.image('Generated_images', images_for_tensorboard, 5)\r\n```\r\n\r\nI hope this helps. Good luck with your GANs ;-)", "i am a student,i am not very familiar with tensorflow, i just  follow @lukaszkaiser \r\nand use with ' tf.variable_scope(tf.get_variable_scope(),reuse=tf.AUTO_REUSE) as scope:'\r\nand delete the 'tf.get_variable_scope().reuse_variables()'  my code is work .\r\ni am runing the code of ROLO.", "I'm new in TF, I tried to use:\r\nwith tf.variable_scope(tf.get_variable_scope()) as scope:\r\nbut it didn't work, and I changed it to default, can you help me to change it in a right way?\r\nI attached the python code :\r\n[exprgan.py.gz](https://github.com/tensorflow/tensorflow/files/2407575/exprgan.py.gz)\r\nthis 4 method has tf.get_variable_scope().reuse_variables():\r\nencoder, generator, discriminator_z and discriminator_img\r\n", "@lukaszkaiser Hello I need your help :( , in the code i have the same error :ValueError: Variable G_fc/w does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\r\nBut i didn't understand haw i can change the code (it works with GradientDescent)\r\n\r\nthis is the function \r\n\r\n    def generator(self, z, y, gender=None, reuse_variables=False, enable_tile_label=True, tile_ratio=1.0):\r\n        if reuse_variables:\r\n            tf.get_variable_scope().reuse_variables()\r\n        num_layers = int(np.log2(self.size_image)) - int(self.size_kernel / 2)\r\n        if enable_tile_label:\r\n            duplicate = int(self.num_z_channels * tile_ratio / self.y_dim)\r\n        else:\r\n            duplicate = 1\r\n        z = concat_label(z, y, duplicate=duplicate)\r\n        if enable_tile_label:\r\n            duplicate = int(self.num_z_channels * tile_ratio / 2)\r\n        else:\r\n            duplicate = 1\r\n\r\n        size_mini_map = int(self.size_image / 2 ** num_layers)\r\n\r\n        name = 'G_fc'\r\n        current = fc(\r\n            input_vector=z,\r\n            num_output_length=self.num_gen_channels * size_mini_map * size_mini_map,\r\n            name=name,\r\n             # reuse = reuse_variables\r\n        )\r\n\r\n        current = tf.reshape(current, [-1, size_mini_map, size_mini_map, self.num_gen_channels])\r\n        current = tf.nn.relu(current)\r\n        current = concat_label(current, y)\r\n\r\n        for i in range(num_layers):\r\n            name = 'G_deconv' + str(i)\r\n            current = tf.image.resize_nearest_neighbor(current, [size_mini_map * 2 ** (i + 1), size_mini_map * 2 ** (i + 1)])\r\n            current = custom_conv2d(input_map=current, num_output_channels=int(self.num_gen_channels / 2 ** (i + 1)), name=name)\r\n            current = tf.nn.relu(current)\r\n            current = concat_label(current, y)\r\n\r\n        name = 'G_deconv' + str(i + 1)\r\n        current = tf.image.resize_nearest_neighbor(current, [self.size_image, self.size_image])\r\n        current = custom_conv2d(input_map=current, num_output_channels=int(self.num_gen_channels / 2 ** (i + 2)), name=name)\r\n        current = tf.nn.relu(current)\r\n        current = concat_label(current, y)\r\n\r\n        name = 'G_deconv' + str(i + 2)\r\n        current = custom_conv2d(input_map=current, num_output_channels=self.num_input_channels, name=name)\r\n\r\n        return tf.nn.tanh(current)\r\n\r\n  **********************************************************************************************\r\nAnd this is the G_fc function \r\n\r\n\r\ndef fc(input_vector, num_output_length, name='fc'):\r\n    with tf.variable_scope(name):\r\n        stddev = np.sqrt(1.0 / (np.sqrt(input_vector.get_shape()[-1].value * num_output_length)))\r\n        w = tf.get_variable(\r\n            name='w',\r\n            shape=[input_vector.get_shape()[1], num_output_length],\r\n            dtype=tf.float32,\r\n            initializer=tf.random_normal_initializer(stddev=stddev)\r\n        )\r\n        b = tf.get_variable(\r\n            name='b',\r\n            shape=[num_output_length],\r\n            dtype=tf.float32,\r\n            initializer=tf.constant_initializer(0.0)\r\n        )\r\n        return tf.matmul(input_vector, w) + b  ", "i got the same error in multi-gpu training script, even when i include all the model define inside a `with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE) as scope:` and manually added `tf.get_variable_scope().reuse_variables()` in each gpu. would like to know what happened, any information or suggestion is appreciated", "> i got the same error in multi-gpu training script, even when i include all the model define inside a `with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE) as scope:` and manually added `tf.get_variable_scope().reuse_variables()` in each gpu. would like to know what happened, any information or suggestion is appreciated\r\n\r\nJust do as @wookayin suggested", "@Traeyee thanks,  fixed the problem after many attempts of changing tf.name_scope and tf.variable_scope."]}, {"number": 6218, "title": "Add missing documentation of Nesterov attribute.", "body": "The doc did not explain the parameter.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 6217, "title": "process_bounding_boxes.py does not ouput .csv file", "body": "\r\nhttps://github.com/tensorflow/models/blob/master/inception/inception/data/process_bounding_boxes.py\r\n\r\n**process_bounding_boxes.py** does not output any files after processing .xml files.\r\n\r\nit says: The script dumps out a CSV text file in which each line contains an entry. but if go through the python code, there is no write() function is called.\r\n\r\ni can only see:\r\n\r\n**Finished processing 544546 XML files.\r\nSkipped 0 XML files not in ImageNet Challenge.\r\nSkipped 0 bounding boxes not in ImageNet Challenge.\r\nWrote 615299 bounding boxes from 544546 annotated images.\r\nFinished.**\r\n\r\n\r\n\r\n", "comments": ["The `process_bounding_boxes.py` script does *not* dump out a CSV file. Instead the script the bounding box annotations to STDOUT here:\r\n\r\nhttps://github.com/tensorflow/models/blob/master/inception/inception/data/process_bounding_boxes.py#L225\r\n\r\nYou can redirect the bounding box annotations to a file using standard shell script redirection as was done in this wrapping script:\r\n\r\nhttps://github.com/tensorflow/models/blob/master/inception/inception/data/download_and_preprocess_imagenet.sh#L86\r\n\r\nA [README.md](https://github.com/tensorflow/models/blob/master/inception/README.md) contains instructions for running these scripts.\r\n\r\n", "Please feel free to reopen if this does not answer your questions."]}, {"number": 6216, "title": "task assignment in tensorflow distributed process", "body": "I'm confused about the distributed training process in tensorflow.\r\nI think the tensorflow feed a batch_size of data to a worker and then the worker update the ps server,is this right? \r\nBut when training , I noticed that the step number in the log may strange.\r\nIf I have only 2 workers , I thinks the right process should be some thing like\r\n[worker1] step 0 xxxxxxx\r\n[worker2] step 100 xxxxxxx\r\n[worker1] step 200 xxxxxxx\r\n[worker2] step 300 xxxxxxx\r\n.....\r\nevery worker should print different step to log.\r\n\r\nActually , the log are as below:\r\n[worker1] step 0 xxxxxxx\r\n[worker2] step 100 xxxxxxx\r\n[worker1] step 100 xxxxxxx\r\n[worker2] step 200 xxxxxxx\r\n[worker1] step 300 xxxxxxx\r\n...\r\nWhy the worker1 dosn't print step 200? \r\n\r\nI am confused about the job assign .... a step to a worker? or every worker run every step?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nthere are nothing about the tensorflow distributed process in these websites\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\ndef main(_):\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n  # Create a cluster from the parameter server and worker hosts.\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n  # Create and start a server for the local task.\r\n  server = tf.train.Server(cluster,\r\n                           job_name=FLAGS.job_name,\r\n                           task_index=FLAGS.task_index)\r\n\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n  elif FLAGS.job_name == \"worker\":\r\n\r\n    # Assigns ops to the local worker by default.\r\n    with tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n        cluster=cluster)):\r\n\r\n      # Variables of the hidden layer\r\n      hid_w = tf.Variable(\r\n          tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, FLAGS.hidden_units],\r\n                              stddev=1.0 / IMAGE_PIXELS), name=\"hid_w\")\r\n      hid_b = tf.Variable(tf.zeros([FLAGS.hidden_units]), name=\"hid_b\")\r\n\r\n      # Variables of the softmax layer\r\n      sm_w = tf.Variable(\r\n          tf.truncated_normal([FLAGS.hidden_units, 10],\r\n                              stddev=1.0 / math.sqrt(FLAGS.hidden_units)),\r\n          name=\"sm_w\")\r\n      sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\r\n\r\n      x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\r\n      y_ = tf.placeholder(tf.float32, [None, 10])\r\n\r\n      hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\r\n      hid = tf.nn.relu(hid_lin)\r\n\r\n      y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\r\n      loss = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\r\n\r\n      global_step = tf.Variable(0)\r\n\r\n      train_op = tf.train.AdagradOptimizer(0.01).minimize(\r\n          loss, global_step=global_step)\r\n\r\n      saver = tf.train.Saver()\r\n      summary_op = tf.merge_all_summaries()\r\n      init_op = tf.initialize_all_variables()\r\n\r\n    # Create a \"supervisor\", which oversees the training process.\r\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\r\n                             logdir=\"/tmp/train_logs\",\r\n                             init_op=init_op,\r\n                             summary_op=summary_op,\r\n                             saver=saver,\r\n                             global_step=global_step,\r\n                             save_model_secs=600)\r\n\r\n    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\r\n\r\n    # The supervisor takes care of session initialization, restoring from\r\n    # a checkpoint, and closing when done or an error occurs.\r\n    with sv.managed_session(server.target) as sess:\r\n      # Loop until the supervisor shuts down or 1000000 steps have completed.\r\n      step = 0\r\n      while not sv.should_stop() and step < 1000000:\r\n        # Run a training step asynchronously.\r\n        # See `tf.train.SyncReplicasOptimizer` for additional details on how to\r\n        # perform *synchronous* training.\r\n\r\n        batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\r\n        train_feed = {x: batch_xs, y_: batch_ys}\r\n\r\n        _, step = sess.run([train_op, global_step], feed_dict=train_feed)\r\n        if step % 100 == 0: \r\n            print \"Done step %d\" % step\r\n\r\n    # Ask for all the services to stop.\r\n    sv.stop()\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\n[0:00:17.115814] Task: 0, Step: 74600, loss: 0.303285002708, accuracy: 0.910000026226, auc: 0.946377456188\r\n[0:00:03.804889] Task: 1, Step: 74700, loss: 0.287385582924, accuracy: 0.879999995232, auc: 0.946395516396\r\n[0:00:03.778589] Task: 0, Step: 74800, loss: 0.247096762061, accuracy: 0.860000014305, auc: 0.946370542049\r\n[0:00:03.772320] Task: 1, Step: 74900, loss: 0.264987647533, accuracy: 0.899999976158, auc: 0.946406364441\r\n[0:00:03.795459] Task: 0, Step: 75000, loss: 0.228719010949, accuracy: 0.899999976158, auc: 0.946437120438\r\n[0:00:01.902293] Task: 1, Step: 75000, loss: 0.217391207814, accuracy: 0.910000026226, auc: 0.946473121643\r\n[0:00:01.942055] Task: 1, Step: 75100, loss: 0.284583866596, accuracy: 0.889999985695, auc: 0.946496844292\r\n[0:00:03.860608] Task: 0, Step: 75200, loss: 0.273199081421, accuracy: 0.850000023842, auc: 0.946503221989\r\n[0:00:03.800881] Task: 1, Step: 75300, loss: 0.189931258559, accuracy: 0.930000007153, auc: 0.946559965611\r\n```", "comments": ["Your example doesn't have print statement (ie, I can't see who prints \"Task: 0 ..\"). Also github is generally for bugs in TensorFlow, and these kinds of questions get better answers on stackoverflow (tag question with \"tensorflow\")", "thanks @yaroslavvb  .  I recently submit a lot issue to tensorflow , many of them replied \"ask on stackoverflow\", but currently,  a question about tensorflow in the stackoverflow barely be answered :( , so .....\r\n\r\nwhere to find some docs about the inner process about distributed tensorflow? I have read the tutorial , but not help.\r\n\r\nthe example a provide is not the current version in my code. I updated \r\n```\r\n    with sv.managed_session(server.target) as sess:\r\n\r\n        if mode == \"train\" or mode == \"train_from_scratch\":\r\n\r\n            while not sv.should_stop():\r\n                # Get coordinator and run queues to read data\r\n                coord = tf.train.Coordinator()\r\n                threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\n                start_time = datetime.datetime.now()\r\n\r\n                try:\r\n                    while not coord.should_stop():\r\n                        _, loss_value, step = sess.run([train_op, loss, global_step])\r\n                        if step % steps_to_validate == 0:\r\n                            accuracy_value, auc_value, summary_value = sess.run(\r\n                                [accuracy, auc_op, summary_op])\r\n                            end_time = datetime.datetime.now()\r\n                            print(\"[{}] Task: {}, Step: {}, loss: {}, accuracy: {}, auc: {}\".format(\r\n                                end_time - start_time,\r\n                                FLAGS.task_index,\r\n                                step, loss_value, accuracy_value,\r\n                                auc_value))\r\n\r\n                            #if FLAGS.task_index == 0:\r\n                            #  writer.add_summary(summary_value, step)\r\n                            #  saver.save(sess, checkpoint_file, global_step=step)\r\n                            start_time = end_time\r\n                except tf.errors.OutOfRangeError:\r\n                    print(\"Done training after reading all data\")\r\n                finally:\r\n                    coord.request_stop()\r\n                    print(\"coord stopped\")\r\n\r\n                # Wait for threads to exit\r\n                coord.join(threads)\r\n```", "How the tensorflow do the distribution training?\r\nthe chief worker split data to batch_size , then give a batch to a worker then update the ps server?   \r\nOR, every worker will run whole data ,and update the ps server ?\r\n", "I'm pretty sure all basic distributed tensorflow questions have gotten answered on SO recently. If you move your q to stackoverflow, I promise I'll answer it. The problem with putting these questions on github issues, is that issues are meant for developers to truck bugs/features in TensorFlow code, so it would be hard to tell what needs to be fixed if all stackoverflow tf discussions moved here", "@yaroslavvb   thanks \ud83d\udc4d   (the other basic problem I'll try stackoverflow first )\r\nI have move this issue to this link , wish for you help :)\r\nhttp://stackoverflow.com/questions/41067398/task-assignment-in-tensorflow-distributed-process"]}, {"number": 6215, "title": "Embedding Projector hangs when data with string-based features are added", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System: OS X 10.11.6\r\n\r\nAnswer to other prompts: N/A\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nMy issue is on the online tool Embedding Projector (http://projector.tensorflow.org). On both Safari and Chrome, if I upload data via the \"Load Data\" button, I see a window \"Computing PCA...\" that _never goes away_. \r\n\r\nA 10-sample version of my data can be found at [10samples.txt](https://github.com/tensorflow/tensorflow/files/642762/10samples.txt).\r\n\r\nIf I remove the string-based feature at the end\u2014which happens to be the output/label for these samples\u2014the problem goes away.\r\n\r\nThis case should be caught and an informative error message should be returned. \r\n\r\n(Relatedly, I can't understand from the current interface how to add string-based labels for the data. The implication is that these labels are \"metadata\", but that's not clear.)", "comments": ["Keep your first file without the string based feature at the end. \r\nCreate a second file called labels.txt with the following contents\r\n\r\n```\r\nnone\r\nback\r\nfwd\r\nnone\r\nleft\r\nfwd\r\nright\r\nnone\r\nleft\r\nnone\r\n```\r\n\r\nuse that as the metadata file. \r\n\r\nNote that if you make the metadata file have more than 1 variable you have to supply a header column. for example\r\n\r\n```\r\nLabel\tcount \r\nnone\t2\t\r\nback\t4\t\r\nfwd\t4\r\nnone\t34\r\nleft\t3\r\nfwd\t5\r\nright\t34\r\nnone\t23\r\nleft\t2\r\nnone\t4\r\n```", "Thanks, Matt. I eventually figured out how to add data and metadata through trial and error, and reading the documentation confirmed what I learned and matches what you wrote.\r\n\r\nI should clarify that the bug report is primarily about the response of the UI when the wrong format is provided, not about what the right format is. Instead of hanging indefinitely on \"Computing PCA...\", it would be much more helpful if the system fails smoothly (not crashing) and lets the user know that non-numeric features and labels can only be added as metadata.\r\n\r\nThe parenthetical comment (which maybe I should have put in a separate bug report, despite it being very related) is also about a lack of clarity in the UI. The example for adding metadata in the \"Load Data\" window shows a header. If someone follows that example with a single column of metadata, they will get an obfuscated piece of feedback about the system expecting n columns and instead seeing n+1. In my opinion, how to load data that follows this canonical machine learning case of a single label should be clear, not requiring the user to read into the implications of the UI's feedback or read long documentation. The solution could be a (a) preventative: a sentence in the window like \"Single column metadata should not have a header.\" and (b) informative upon failure: when the metadata has one more row than the data and has a single column, the error message should remind/inform the user to \"Please note that single column metadata should not have a header.\"\r\n\r\n(Also, thanks for supplying and/or supporting this great tool!)", "Ah...good point @bradknox. It is indeed super confusing....the error messages are not good, I actually only found this thread because I was trying to upload a tsv file and I kept getting another error \r\n\r\n![screenshot 2016-12-14 13 11 00](https://cloud.githubusercontent.com/assets/1277725/21183218/d77ef7e0-c1fe-11e6-9fbf-e4e375530cbf.png)\r\n\r\nwhich I think happens when the files are not tabbed correctly, but it's hard to tell. \r\n\r\nAgree on the n+1 error too, that will trap a lot of people unless there is a message saying something like \"The number of vectors(500) does not match the number of items of metadata (501). Please not that single column metadata does not need a header\" or something like that. ", "This should be fixed now, I pushed a new version with some better error messages. Please re-open if you still find any of these messages opaque.", "@mrmattwright how did you fix the problem of the Parsing failed?\r\nI am trying to load a dataset from my computer but it says that it found a vector with only one dimension. \r\nCan you please share some data with a correct format, I am a bit lost here.\r\nThank you in advance.", "I have this same issue\r\nPlease how can I load my data?", "Hi. I'm getting the same message trying to load a 11553 x 100 tsv file. (No header, No text, just tab separated floats)", "\r\n> @mrmattwright how did you fix the problem of the Parsing failed?\r\n> I am trying to load a dataset from my computer but it says that it found a vector with only one dimension.\r\n> Can you please share some data with a correct format, I am a bit lost here.\r\n> Thank you in advance.\r\n\r\nInculded a short snippet to generate TSV files.\r\n\r\n```\r\nimport csv\r\n\r\nvectors = [[0,0,1], [0,1,0], [1,0,0], [1,1,1]]       # load your embeddings\r\nmetadata = ['point1', 'point2', 'point3', 'point4']  # load your metadata\r\n\r\nwith open('output.tsv', 'wt') as out_file:\r\n    tsv_writer = csv.writer(out_file, delimiter='\\t')\r\n    for vector in vectors:\r\n      tsv_writer.writerow(vector)\r\n\r\nwith open('metadata.tsv', 'wt') as out_file:\r\n    tsv_writer = csv.writer(out_file, delimiter='\\t')\r\n    for meta in metadata:\r\n      tsv_writer.writerow([meta])\r\n\r\n```\r\nFor TSV file of vectors upload output.tsv and \r\nFor TSV file of metadata upload metadata.csv \r\n"]}, {"number": 6214, "title": "Push tfdbg stepper", "body": "git cherry-pick 938443b && \\\r\n    git cherry-pick 5073769 && \\\r\n    git cherry-pick d1c8a40 && \\\r\n    git cherry-pick f37c468\r\n", "comments": []}, {"number": 6213, "title": "[Android] Put large .pb files outside of Asset-folder? [ERROR]: Check failed: message->ParseFromZeroCopyStream(&lis) ", "body": "Right now, I'm facing a problem which throws the following error:\r\n\r\n```\r\n12-09 14:11:51.635 194-194/? A/DEBUG: Abort message: 'jni_utils.cc:125 Check failed: message->ParseFromZeroCopyStream(&lis) '\r\n12-09 14:11:51.635 194-194/? A/DEBUG:     r0 00000000  r1 000011e8  r2 00000006  r3 9df38978\r\n12-09 14:11:51.636 194-194/? A/DEBUG:     r4 9df38980  r5 9df38930  r6 00000058  r7 0000010c\r\n12-09 14:11:51.636 194-194/? A/DEBUG:     r8 9df37c14  r9 9df37ac0  sl b4d13c90  fp 9df38400\r\n12-09 14:11:51.636 194-194/? A/DEBUG:     ip 00000006  sp 9df37a68  lr b6c95b61  pc b6c97f50  cpsr 400f0010\r\n12-09 14:11:51.645 194-194/? A/DEBUG: backtrace:\r\n12-09 14:11:51.645 194-194/? A/DEBUG:     #00 pc 00041f50  /system/lib/libc.so (tgkill+12)\r\n12-09 14:11:51.645 194-194/? A/DEBUG:     #01 pc 0003fb5d  /system/lib/libc.so (pthread_kill+32)\r\n12-09 14:11:51.646 194-194/? A/DEBUG:     #02 pc 0001c30f  /system/lib/libc.so (raise+10)\r\n12-09 14:11:51.646 194-194/? A/DEBUG:     #03 pc 000194c1  /system/lib/libc.so (__libc_android_abort+34)\r\n12-09 14:11:51.646 194-194/? A/DEBUG:     #04 pc 000174ac  /system/lib/libc.so (abort+4)\r\n12-09 14:11:51.646 194-194/? A/DEBUG:     #05 pc 0058bb74  /data/app/de.-2/lib/arm/libnative_microscope.so\r\n12-09 14:11:51.646 194-194/? A/DEBUG:     #06 pc 0058bcf4  /data/app/de.-2/lib/arm/libnative_microscope.so\r\n12-09 14:11:51.646 194-194/? A/DEBUG:     #07 pc 0058bd10  /data/app/de.-2/lib/arm/libnative_microscope.so\r\n12-09 14:11:51.646 194-194/? A/DEBUG:     #08 pc 00076700  /data/app/de.-2/lib/arm/libnative_microscope.so (_Z15ReadFileToProtoP13AAssetManagerPKcPN6google8protobuf11MessageLiteE+984)\r\n12-09 14:11:51.647 194-194/? A/DEBUG:     #09 pc 0007459c  /data/app/de.-2/lib/arm/libnative_microscope.so (Java_de_beamerscope_nativepart_NativePart_init+772)\r\n12-09 14:11:51.647 194-194/? A/DEBUG:     #10 pc 0000087d  /data/data/de./cache/slice-slice_6-classes.dex (offset 0x3000)\r\n\r\n```\r\n\r\nI think it's coming due to the larg protobuffer file .pb which is loaded from the asset folder. I build the app with Android Studio and tuned the compression in the gradle-file of via \r\n```\r\n    aaptOptions {\r\n        noCompress 'pb'\r\n    }\r\n```\r\nWhen I load a smaller model it's working fine. I think it's still coming from a wrong file-transfer-conversion thing in Android Studio?! Is it possible?\r\n\r\nSo my question. Is it also possible to place large \".pb\"-files (i.e. 130 mb) somewhere else on the SD-card and load it in seperately?\r\n\r\nOr do you think, it's a different problem? I'm training boss models with the same version of TF. \r\n\r\nThank you! :)\r\n\r\n", "comments": ["Or does anybody know how to solve this issue in a more beautiful way? Thanks :)", "@beniroquai Are you using tensorflow/contrib/android/jni/jni_utils.cc? If so, it's strange that the the filesize check in isn't catching this and picking the correct path. Would you mind adding   `LOG(INFO) <<  \"fd: \" << fd << \", length: \" << length;` directly after fd is initialized and reporting what it says?\r\n", "@andrewharp Thanks for your reply. Yes, I use the `jni_utils.cc` file and when I use a pb file with about 35 MB I get the following output:\r\n\r\n`I/native: jni_utils.cc:117 fd: 28, length: 35256478`\r\n\r\nAnother file with 135 MB gives the following output:\r\n\r\n`I/native: jni_utils.cc:117 fd: 28, length: 137619240`\r\n\r\nThe crap crashed all over sudden with the following output:\r\n\r\n```\r\nA/native: jni_utils.cc:127 Check failed: message->ParseFromZeroCopyStream(&lis) \r\nA/libc: Fatal signal 6 (SIGABRT), code -6 in tid 13421 (AsyncTask #1)\r\n12-18 22:23:18.735 199-199/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n12-18 22:23:18.735 199-199/? A/DEBUG: Build fingerprint: 'google/hammerhead/hammerhead:6.0.1/M4B30X/3237893:user/release-keys'\r\n12-18 22:23:18.735 199-199/? A/DEBUG: Revision: '11'\r\n12-18 22:23:18.735 199-199/? A/DEBUG: ABI: 'arm'\r\n12-18 22:23:18.735 199-199/? A/DEBUG: pid: 13346, tid: 13421, name: AsyncTask #1  >>> de.xxx <<<\r\n12-18 22:23:18.735 199-199/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------\r\n12-18 22:23:18.778 199-199/? A/DEBUG: Abort message: 'jni_utils.cc:127 Check failed: message->ParseFromZeroCopyStream(&lis) '\r\n12-18 22:23:18.778 199-199/? A/DEBUG:     r0 00000000  r1 0000346d  r2 00000006  r3 9d7d1978\r\n12-18 22:23:18.778 199-199/? A/DEBUG:     r4 9d7d1980  r5 9d7d1930  r6 00000058  r7 0000010c\r\n12-18 22:23:18.778 199-199/? A/DEBUG:     r8 9d7d0c14  r9 9d7d0ac0  sl 9d7d0c14  fp 9d7d1400\r\n12-18 22:23:18.778 199-199/? A/DEBUG:     ip 00000006  sp 9d7d0a68  lr b6ca0b61  pc b6ca2f50  cpsr 400f0010\r\n12-18 22:23:18.793 199-199/? A/DEBUG: backtrace:\r\n12-18 22:23:18.793 199-199/? A/DEBUG:     #00 pc 00041f50  /system/lib/libc.so (tgkill+12)\r\n12-18 22:23:18.793 199-199/? A/DEBUG:     #01 pc 0003fb5d  /system/lib/libc.so (pthread_kill+32)\r\n12-18 22:23:18.794 199-199/? A/DEBUG:     #02 pc 0001c30f  /system/lib/libc.so (raise+10)\r\n12-18 22:23:18.794 199-199/? A/DEBUG:     #03 pc 000194c1  /system/lib/libc.so (__libc_android_abort+34)\r\n12-18 22:23:18.795 199-199/? A/DEBUG:     #04 pc 000174ac  /system/lib/libc.so (abort+4)\r\n12-18 22:23:18.795 199-199/? A/DEBUG:     #05 pc 0058bbe4  /data/app/de.xxx-1/lib/arm/libnative_microscope.so\r\n12-18 22:23:18.796 199-199/? A/DEBUG:     #06 pc 0058bd64  /data/app/de.xxx-1/lib/arm/libnative_microscope.so\r\n12-18 22:23:18.796 199-199/? A/DEBUG:     #07 pc 0058bd80  /data/app/de.xxx-1/lib/arm/libnative_microscope.so\r\n12-18 22:23:18.796 199-199/? A/DEBUG:     #08 pc 0007675c  /data/app/de.xxx-1/lib/arm/libnative_microscope.so (_Z15ReadFileToProtoP13AAssetManagerPKcPN6google8protobuf11MessageLiteE+1076)\r\n12-18 22:23:18.796 199-199/? A/DEBUG:     #09 pc 0007459c  /data/app/de.xxx-1/lib/arm/libnative_microscope.so (Java_de_xxx_nativepart_NativePart_init+772)\r\n12-18 22:23:18.796 199-199/? A/DEBUG:     #10 pc 0000087d  /data/data/de.xxx/cache/slice-slice_6-classes.dex (offset 0x3000)\r\n\r\n```\r\n\r\nDo I need to do anythin with the file? I've just seen, that there is something like optimizing protobuf files for inference using Bazel. Is that necessary? \r\n\r\nThank you! ", "@beniroquai Yes, you may be able to strip some unnecessary nodes out of the GraphDef with the [optimize_for_inference.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py) script.\r\n\r\nIf there are some other non-size related issues with the pb file that may also expose them, as the ParseFromZeroCopyStream can fail for other reasons as well. I've verified that I can load a 85mb GraphDef file via ParseFromZeroCopyStream so it's not a 64mb limitation, at least.\r\n\r\nIf you want to load directly from the sdcard just `adb push` it to your device and use a regular filepath in ClassifierActivity.java -- i.e. strip off the file:///android_asset/ prefix.", "@beniroquai Have you made any other modifications to the file, though? Your printout is on line 117, while I would assume it should be on line 97 given the current state of `jni_utils.cc`.", "Ups. You're perfectly right! Wow! That was great! I used an older Version of the file `jni_utils.cc`. I updated the file and now it works perfectly fine! I would have never been able to solve this issue. \r\n\r\nThank you so much! :) ", "hey i have trained my own model in tensor flow and saved the model in protobuf file ..kindly tell me how to import it in android ", "Hey, did you have a look at the tensorflow example for android apps?\n\nOn 13 Nov 2017 11:49, \"sarakazmi\" <notifications@github.com> wrote:\n\n> hey i have trained my own model in tensor flow and saved the model in\n> protobuf file ..kindly tell me how to import it in android\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6213#issuecomment-343881601>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEJOuP81TBM74M_LkWt-LcsrKOE_h4t8ks5s2B7DgaJpZM4LI79H>\n> .\n>\n"]}, {"number": 6212, "title": "Determine output shape for SplitV Op", "body": "Fixes #6186 \r\nFixes #5977 \r\n\r\nI couldn't find any tests for SplitV in `array_ops_test.cc`. Is there any other file for SplitV test?", "comments": ["Can one of the admins verify this patch?", "Any review for this? I have used three same for loops to set output shape. Can anyone suggest better way to refactor this? Thanks! :)", "added some comments, please also add a test that verifies the output shapes are indeed correct.", "Good question about tests though -- @ekelsen where do splitV tests live?", "The split_v tests live in kernel_tests/split_op_test.py.", "@ekelsen Updated the PR.. Please check.", "@ekelsen Done!", "We can run the tests now?", "Jenkins, test this please!\r\n", "split_op_test failed, please check.", "@martinwicke The tests are failing because `size_splits` in [these tests](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/split_op_test.py#L69) are numpy array instead of list. If I type cast them into a list, then all the tests are passing. I'm not sure why my changes are causing this error. Do you have any idea about this? Please help. :)\r\n\r\nThis is the output of running tests.\r\n```\r\n............F tensorflow/core/framework/tensor.cc:487] Check failed: dtype() == expected_dtype (9 vs. 3)\r\n```", "I have made the changes and the tests are passing. Can we run the tests again?", "The tests are failing after I rebased the branch. It is failing because of commit - a46b6d211eac423c72d3a57a177daf2f64db8642\r\nLooking into it.", "Done! Please run the tests now.", "Jenkins, test this please!", "All checks have passed :)", "Thanks for the contribution Anish! Should get merged by someone with permission soon."]}, {"number": 6211, "title": "ImportError: No module named nets, r0.12", "body": "I am trying to run `resnet_v2.py`  from provided modles but getting error `ImportError: No module named nets\r\n`.\r\nI have done following \r\nFirst installing slim\r\n`cd $HOME/tensorflow\r\ngit clone https://github.com/tensorflow/models/`\r\n\r\nTo verify that this has worked, execute the following commands; it should run\r\nwithout raising any errors.\r\n\r\n```\r\n`cd $HOME/tensorflow/models/slim\r\npython3 -c \"from nets import cifarnet; mynet = cifarnet.cifarnet\"`\r\n```\r\nI got no error.\r\n\r\nBut when I run following gives error **\"ImportError: No module named nets\"**. I have also tried\r\n`cd $HOME/tensorflow/models/slim/\r\npython3 -c \"from nets import resnet_utils\"`\r\n\r\nGot not error.  Also build  using `bazel build nets` output \r\n`INFO: Found 1 target...\r\nTarget //slim:nets up-to-date (nothing to build)\r\nINFO: Elapsed time: 0.096s, Critical Path: 0.00s\r\n`\r\n\r\nI don't know why this is not working.\r\n", "comments": ["Closing as duplicate of [tensorflow/models #729](https://github.com/tensorflow/models/issues/729)"]}, {"number": 6210, "title": "[Installing TensorFlow offline] $ ./configure fails because no Internet connection is available for my server.", "body": "**My environment without Internet**\r\nMy server can't access the Internet because my company's firewall blocks any external network or the Internet. To install Tensorflow on this server, I have to install TensorFlow offline. In other words, I have to correct tens of errors by myself. Upon this error, I had to submit an issue. \r\n\r\nI moved the template below because it's way too long. (I filled in some information.) I'd appreciate your feedback.\r\n \r\n**Questions**\r\nMy question nails down to two core questions.\r\nQ1. How can I manually fetch all the packages for tensorflow without using bazel command?\r\n      I took a closer look at the \"configure\" file and the following line causes the problem.\r\n       $bazel fetch //tensorflow/...\r\n\r\n      This bazel command fetches all the packages in target \"tensorflow\". \r\n       No Internet connection in my server, so this command fails!. \r\n       I'll have to download all these packages in ANOTHER computer with Internet connection and   \r\n       move them to my server.\r\n \r\n       In another computer with Internet connection, how can I download them without bazel command? Alternative commands can be wget, git, apt-get, pip and so on.\r\n\r\nQ2. Where should I place the downloaded packages?\r\n    Say all the packages for target \"tensorflow\" are downloaded.\r\n    Where should I place them to run the next command successfully?\r\n \r\n     $ bazel build -c opt -config=cuda //tensorflow/cc:tutorials_example_trainer\r\n \r\n    I'll have to move all the package to my server from another computer.\r\n   So I need some information about the directory that the above command refers to.\r\n\r\n(This is all because of the security measure in my company. As a matter of fact, this process is more complex than using another computer, but this gives you an idea about my installation environment without the Internet)\r\n-----------------------------------------------------------------------------------\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n \r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n \r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n \r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\"How to make the ./configure find package in local place #5428\"\r\nGaofengCheng opened this issue on 6 Nov and there're 13 comments, but no solution is presented.\r\n \r\n### Environment info\r\n1. Operating System: Ubuntu Linux 14.04\r\n2. NO INTERNET CONNECTION.\r\n    My company's firewall blocks my server from the Internet connection.  So no Internet connection is available. A comment in Issue #5029 says \"Long story short, [...] a network access is required.\", but a network access on this server is not an option. I'll have to download necessary packages from another computer with Internet connection and upload them to my server via sftp.\r\n \r\n3. CUDA & cuDNN are installed in ~/local/ because of the system admin asked me to do so.\r\n   When I run \" $ ./configure\", CUDA & cuDNN installation was done manually.  \r\n \r\nInstalled version of CUDA and cuDNN:\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n~/local/cuda-8.0/lib64$ ls -l libcud*\r\n-rw-r--r-- 1 root root   558720 Dec  9 01:00 libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Dec  9 01:00 libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Dec  9 01:00 libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Dec  9 01:00 libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Dec  9 01:00 libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Dec  9 01:05 libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       17 Dec  9 01:05 libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxr-xr-x 1 root root 79337624 Dec  9 01:05 libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Dec  9 01:05 libcudnn_static.a\r\n \r\nIf installed from source, provide\r\n1. The commit hash (`git rev-parse HEAD`)\r\n$ git rev-parse HEAD\r\nfatal: Not a git repository (or any parent up to mount point /home)\r\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n \r\n2. The output of `bazel version`\r\n$ bazel version\r\nBuild label: 0.4.0\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\r\nBuild timestamp: 1478109254\r\nBuild timestamp as int: 1478109254\r\n \r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n$ ./configure\r\n[...]\r\nERROR: package contains errors: tensorflow/examples/android.\r\nERROR: error loading package 'tensorflow/examples/android': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Error downloading from http://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz to /home/the.kim/.cache/bazel/_bazel_the.kim/e904ac0e4fbd2c03039cdbaeea674781/external/protobuf: Error downloading http://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz to /home/the.kim/.cache/bazel/_bazel_the.kim/e904ac0e4fbd2c03039cdbaeea674781/external/protobuf/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: Timed out connecting to http://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz : connect timed out.\r\n(tensorflow) $\r\n \r\n(The following line within configure causes my problem.\r\nMy server has no Internet connection, so download fails.)\r\n$ bazel fetch //tensorflow/...\r\nINFO: Downloading from http://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e85\\\r\n5cb16db.tar.gz: 0B\r\n \r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n \r\nthe.kim@clover3:~/tensorflow/downloads/tensorflow$ source ~/tensorflow/bin/activate\r\n(tensorflow) the.kim@clover3:~/tensorflow/downloads/tensorflow$ ls\r\n008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz  eigen.BUILD        LICENSE          tensorflow\r\nACKNOWLEDGMENTS                                  farmhash.BUILD     linenoise.BUILD  third_party\r\nADOPTERS.md                                      gif.BUILD          models.BUILD     tools\r\nAUTHORS                                          gmock.BUILD        nanopb.BUILD     util\r\nbower.BUILD                                      grpc.BUILD         png.BUILD        WORKSPACE\r\nBUILD                                            ISSUE_TEMPLATE.md  README.md        zlib.BUILD\r\nconfigure                                        jpeg.BUILD         RELEASE.md\r\nCONTRIBUTING.md                                  jsoncpp.BUILD      six.BUILD\r\n(tensorflow) the.kim@clover3:~/tensorflow/downloads/tensorflow$ ./configure\r\n~/tensorflow/downloads/tensorflow ~/tensorflow/downloads/tensorflow\r\nPlease specify the location of python. [Default is /home/the.kim/tensorflow/bin/python]: /home/the.kim/tensorflow/bin/python3\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /home/the.kim/tensorflow/lib/python3.4/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/the.kim/tensorflow/lib/python3.4/site-packages]\r\n \r\nUsing python library path: /home/the.kim/tensorflow/lib/python3.4/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] y\r\nOpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /home/the.kim/local/cuda-8.0\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]:\r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /home/the.kim/local/cuda-8.0]:\r\nlibcudnn.so resolves to libcudnn.5\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 3.7\r\nPlease specify which C++ compiler should be used as the host C++ compiler. [Default is /usr/bin/g++]:\r\nPlease specify which C compiler should be used as the host C compiler. [Default is /usr/bin/gcc]:\r\nPlease specify the location where ComputeCpp 1.2 is installed. Refer to README.md for more details. [Default is /usr/local/computecpp]: /home/the.kim/local/ComputeCpp-CE-0.1.1-Linux\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n........\r\nERROR: package contains errors: tensorflow/examples/android.\r\nERROR: error loading package 'tensorflow/examples/android': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Error downloading from http://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz to /home/the.kim/.cache/bazel/_bazel_the.kim/e904ac0e4fbd2c03039cdbaeea674781/external/protobuf: Error downloading http://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz to /home/the.kim/.cache/bazel/_bazel_the.kim/e904ac0e4fbd2c03039cdbaeea674781/external/protobuf/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: Timed out connecting to http://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz : connect timed out.\r\n(tensorflow) the.kim@clover3:~/tensorflow/downloads/tensorflow$\r\n \r\n(Before the error, $ ./configure attempts this.)\r\nINFO: Downloading from http://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e85\\\r\n5cb16db.tar.gz: 0B", "comments": ["@martinwicke can you see an easy way to automate the download of necessary packages before build?", "I would use bazel fetch on another computer and copy the result. \r\n\r\nThe full paths of all of the packages are in the workspace.bzl file, so if you really want you can use wget and get them all. You can then change all the rules in the workspace file to be \"local_repository\" instead, and that will use the previously downloaded files from the paths you provide.", "Dear @martinwicke,\r\nThanks for your feedback. \r\n\r\n> I would use bazel fetch on another computer and copy the result.\r\nYes, this is what I've been doing. The command \"$bazel fetch //tensorflow/...\" fetches the repository and the only way I can install the repo to my server (without Internet) is to copy the result. Anyways it's good to confirm to use another computer.\r\n\r\n> You can then change all the rules in the workspace file to be \"local_repository\" instead, and that > will use the previously downloaded files from the paths you provide.\r\nWell, yes. That's exactly what I'm trying to do. The question is where is the \"local_repository\" located. If the local_repository can be any directory, how can I specify the directory when I build with bazel command?\r\n\r\nI wasn't able to find about local_repository on [Command-Line Reference](https://www.bazel.io/versions/master/docs/command-line-reference.html#build).\r\n\r\n@martinwicke, I'd appreciate if you can tell me a bit more in detail with concrete command. Thanks in advance!", "It's not a command line option, you'll have to edit the workspace.bzl file to replace how bazel finds its dependencies from their current sources (often, http_archive) to [local_repository](https://www.bazel.io/versions/master/docs/be/workspace.html#local_repository).", "@mrtonnet Did you solve your problem?  I'm having the same issue and was wondering if you could share your results.", "```\r\nbazel --output_user_root=/tmp/bazel fetch  //tensorflow/tools/pip_package:build_pip_package \r\n```\r\ndownloads the packages required for the \"bazel build ... //tensorflow/tools/pip_package:build_pip_package\" stage and put the downloaded files in\r\n```\r\n /tmp/bazel/<md5sum hash>/external/\r\n``` \r\nCorrect? \r\n\r\nNow how I am supposed to convert the URL such as \"http://bazel-mirror.storage.googleapis.com/ftp.exim.org/pub/pcre/pcre-8.39.tar.gz\" from ./tensorflow/workspace.bzl into \"file:///tmp/bazel/da36536f347c0e23d2402d33213b5c40/external/pcre/pcre-8.39.tar.gz\" ?\r\nor from the WORKSPACE file : \"https://github.com/polymerelements/paper-slider/archive/v1.0.10.tar.gz\" -> \"/tmp/bazel/da36536f347c0e23d2402d33213b5c40/external/paper_slider/v1.0.10.tar.gz\"", "@mrtonnet @truatpasteurdotfr  Did you solve your problem? I'm having the same issue and was wondering if you could share your results.", "not solved for me :( I ended up building a docker image and build inside.\r\n", "see this\uff1a\r\nhttps://github.com/amutu/tensorflow_offline", "Hi @amutu \r\nI have followed your amazing introduction of how to install offline tensorflow. \r\nBut now I have one more question...\r\nWhen I try to view the data flow graph through tensorboard\r\nIt seems the browser(firefox) keeps trying to download something from website (google?)\r\nAnd after waiting for a long time (~5mins), it still can't show anything from my browser.\r\nI was wondering if there is anyway to run tensorboard with internet connection..\r\nThanks for your help in advance! ", "I think it needs some javascript libs to show the graph, these libs are only available on the public network and not included in the tensorflow pkg. I am sorry I can not help you because I am not a front end guy. You can open an issue to the tensorflow project to ask help.", "Better yet, open an issue against tensorflow/tensorboard. You should be able to run this just fine without an internet connection. ", "@amutu Thank you for your contribution of tensorflow-offline. Everything seems to work for me when not using cuda, but when trying to follow your instructions with cuda I get the following error:\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/ec2-user/tensorflow-1.2.1/third_party/gpus/cuda_configure.bzl\", line 958\r\n\t\t_create_cuda_repository(repository_ctx)\r\n\tFile \"/home/ec2-user/tensorflow-1.2.1/third_party/gpus/cuda_configure.bzl\", line 915, in _create_cuda_repository\r\n\t\t_host_compiler_includes(repository_ctx, cc)\r\n\tFile \"/home/ec2-user/tensorflow-1.2.1/third_party/gpus/cuda_configure.bzl\", line 144, in _host_compiler_includes\r\n\t\tget_cxx_inc_directories(repository_ctx, cc)\r\n\tFile \"/home/ec2-user/tensorflow-1.2.1/third_party/gpus/cuda_configure.bzl\", line 119, in get_cxx_inc_directories\r\n\t\tset(includes_cpp)\r\ndepsets cannot contain mutable items.\r\n\r\n\r\n\r\nAny idea what I can do?", "CUDA is not supported for the offline build now.", "Sorry to bump an old thread.\r\nI have made a [post on building tensorflow on an online computer](https://flanaras.wordpress.com/2018/03/16/build-tensorflow-on-an-offline-computer/) based on @truatpasteurdotfr's idea and I wanted to point to a solution.\r\nThis should work with any kind of configuration (it works at least with CUDA support).", "Ended up creating a twin VM, build there then deploy the libs to the target server. "]}, {"number": 6209, "title": "Bug: while_loop gives invalid gradients (NaN) even when loss is computed using only non NaNs", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/2540 is the only thing that is seemingly related, but it's a different issue.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: This bug occurs even if I only use CPUs.\r\n\r\nCompiled from source. Commit hash: a5074383617a9947f248a0ddd56b94f9fb0f970b\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nOverview: Build a minimalist RNN-like model and compute derivatives with respect to the loss. Allow the ability to specify invalid time steps using NaNs.\r\n\r\nBehavior:\r\n- If no NaNs are included, it works\r\n- If only one invalid time step is included, it still works (odd)\r\n- If two or more invalid time steps are included, it fails.\r\n- If we change `INVALID_FLOAT_SYMBOL` to, say, `0.12345`, then it works as expected.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nINVALID_FLOAT_SYMBOL = np.nan\r\n\r\nw = tf.Variable(1.0)\r\ninputs = tf.placeholder(tf.float32, shape=[None])\r\ntargets = tf.placeholder(tf.float32, shape=[None])\r\nT = tf.size(inputs)\r\n\r\nx_ta = tf.TensorArray(tf.float32, size=T).unpack(inputs)\r\nh_ta = tf.TensorArray(tf.float32, size=T)\r\n\r\ndef cond(t, h, h_ta):\r\n  return tf.less(t, T)\r\n\r\ndef body(t, h, h_ta):\r\n  x = x_ta.read(t)\r\n  h = w * h + x\r\n  h_ta = h_ta.write(t, h)\r\n  return t + 1, h, h_ta\r\n\r\nt = tf.constant(0)\r\nh = tf.constant(0.0)\r\n_, _, h_ta = tf.while_loop(cond, body, [t, h, h_ta])\r\noutputs = h_ta.pack()\r\n\r\ninvalid_mask = tf.is_nan(inputs) if np.isnan(INVALID_FLOAT_SYMBOL) else tf.equal(inputs, INVALID_FLOAT_SYMBOL)\r\nvalid_mask = tf.logical_not(invalid_mask)\r\n\r\nvalid_outputs = tf.boolean_mask(outputs, valid_mask)\r\nvalid_targets = tf.boolean_mask(targets, valid_mask)\r\n\r\nvalid_losses = tf.nn.sigmoid_cross_entropy_with_logits(valid_outputs, valid_targets)\r\nvalid_loss = tf.reduce_sum(valid_losses)\r\n\r\ngrad, = tf.gradients(valid_loss, w)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.initialize_all_variables())\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\n# Case 1: INVALID_FLOAT_SYMBOL = np.nan, but no INVALID symbol is used.\r\nprint(sess.run(grad, feed_dict={inputs: [1.0, 2.0, 3.0],\r\n                                targets: [1.0, 1.0, 1.0]}))\r\n# Correctly prints -0.0573164\r\n\r\n# Case 2: INVALID_FLOAT_SYMBOL = np.nan, and one INVALID symbol is used.\r\nprint(sess.run(grad, feed_dict={inputs: [1.0, 2.0, 3.0, INVALID_FLOAT_SYMBOL],\r\n                                targets: [1.0, 1.0, 1.0, INVALID_FLOAT_SYMBOL]}))\r\n# Correctly prints -0.0573164\r\n\r\n# Case 3: INVALID_FLOAT_SYMBOL = np.nan, and two INVALID symbols are used.\r\nprint(sess.run(grad, feed_dict={inputs: [1.0, 2.0, 3.0, INVALID_FLOAT_SYMBOL, INVALID_FLOAT_SYMBOL],\r\n                                targets: [1.0, 1.0, 1.0, INVALID_FLOAT_SYMBOL, INVALID_FLOAT_SYMBOL]}))\r\n# Incorrectly prints nan\r\n\r\n# Case 4: INVALID_FLOAT_SYMBOL = 0.12345, and two INVALID symbols are used.\r\nprint(sess.run(grad, feed_dict={inputs: [1.0, 2.0, 3.0, INVALID_FLOAT_SYMBOL, INVALID_FLOAT_SYMBOL],\r\n                                targets: [1.0, 1.0, 1.0, INVALID_FLOAT_SYMBOL, INVALID_FLOAT_SYMBOL]}))\r\n# Correctly prints -0.0573164\r\n```", "comments": ["Including a temporary fix for anyone else with custom RNN implementations that indicate specific invalid time steps instead of using the `sequence_length` option provided in TensorFlow. (This makes writing various models (time-step-level regression, sequence-level regression, time-step-level classification, sequence-level classification) simple with no edge cases.)\r\n\r\nIf we pad the inputs with 0.0 instead of NaN, this bug will never occur. We can then still pad targets with NaNs and mask using the targets.", "@yuanbyu I suspect the issue is because of the eager processing of conditionals: can you comment?", "The problem is that backprop uses tensors from forward computation, and some of those tensors are nan because the nans in `inputs`.  Why don't you mask `inputs` instead\r\n\r\n`valid_inputs = tf.boolean_mask(inputs, valid_mask)\r\n`\r\n?", "I don't see why the gradient with respect to time step 5 (for example) should depend in any way on the information at time step 10. Is this actually happening?\r\n\r\nThe logs I include above suggest that this isn't happening: it works if there is only one NaN after the valid data.\r\n\r\nYour suggested workaround to the bug works for this simplified case but doesn't work in the general case. In a real RNN setting, the batch is made up of multiple sequences, each with NaNs at different places, so flattening along batch/time with `tf.boolean_mask` does not work. (We could possibly post process segments independently, but this would a) lead to messy code and b) make us lose the efficiency gains from batch processing.)", "The problem is about the nan steps. In the backprop, after boolean_mask, zeros are used as gradients to continue the back propagation. It could get to an op whose gradient uses some nans produced in the forward.  ", "Hmm. To me it seems like it's a bug, especially because it doesn't lead to NaNs if exactly one NaN input is included after the valid inputs. But in any case if it's deemed too awkward to fix then it's not too big of a deal; since I encountered this I've been marking invalid time steps with 0s.", "I would consider it not a bug. So I will close it for now."]}, {"number": 6208, "title": "Improve  documentation for tf.split_v", "body": "Hi,\r\n\r\nI was reading through [`tf.split_v`](https://www.tensorflow.org/versions/r0.12/api_docs/python/array_ops.html#split_v) documentation. I got confused because `num_split` is not there in the function arguments. To understand better, I read the examples and found that the argument order is not correct. \r\n\r\nSo, this pull request currently fixes the example of `tf.split_v`. If you agree that the documentation is not proper, then I can fix that too. Thanks!", "comments": ["Can one of the admins verify this patch?", "This is fixed internally and will get released in the next cycle, so it probably doesn't make sense to pull, but thanks for catching this."]}, {"number": 6207, "title": "Batch support for cropping operators", "body": "I had a look at the issues and I don't think this has been submitted yet. Sorry if I missed one.\r\n\r\nThe [cropping](https://www.tensorflow.org/versions/r0.12/api_docs/python/image.html#cropping) operators like `tf.image.resize_image_with_crop_or_pad`, `tf.image.crop_to_bounding_box`, etc only support 3-D tensors. It would be useful to support 4-D tensors for those operators, specially when there are used after the [resizing](https://www.tensorflow.org/versions/r0.12/api_docs/python/image.html#resizing) operators, which support 4-D tensor.\r\n\r\nEdit: I just saw there was `extract_glimpse` or `crop_and_resize` which could works for 4-D tensors. It's confusing to have 2 different functions with different name which seems to perform the same operation with just a different input rank\r\n\r\nEdit 2: It's not exactly the same functions. `extract_glimpse` and `crop_and_resize` both require normalized coordinates. `crop_to_bounding_box` use pixel values. It would be good to have a function which allows both pixel based coordinates and 4-D input tensors but anyway, at least there is a way to do what I wanted.", "comments": []}, {"number": 6206, "title": "accelerate crf_log_norm", "body": "the base `crf_log_norm` function looks very cool, but it runs very slow.\r\nI run the experiment on cpu\r\n```\r\nmodel name  : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz\r\n```\r\n\r\nI use the configure\r\n```python\r\nbatch_size=128\r\nmax_seq_len=200\r\n```\r\nit runs `0.5s` per batch\r\n\r\nwhen I optimize the realization way\r\nit  runs `0.25s` per batch", "comments": ["Can one of the admins verify this patch?", "This change assumes that the maximum sequence length is known and fixed, which breaks backwards compatibility. It would make sense to fallback to the `dynamic_rnn` implementation in that scenario.", "Will take a look in early January\n\nOn Dec 12, 2016 3:25 PM, \"Martin Wicke\" <notifications@github.com> wrote:\n\n> Assigned #6206 <https://github.com/tensorflow/tensorflow/pull/6206> to\n> @ebrevdo <https://github.com/ebrevdo>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6206#event-891322354>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimw5FDnAMVPokf5E3wwN6xFcrz4RBks5rHfQhgaJpZM4LImfZ>\n> .\n>\n", "Sounds good.", "@ebrevdo would you have time to review this now?", "We cannot accept this PR because it assumes a fixed length when calculating\nthe log_crf.  The current approach uses dynamic_rnn for the reason that the\nmax_seq_len is probably *not* known at graph build time, and varies from\nstep to step.\n\nOn Mon, Dec 12, 2016 at 5:25 PM, Martin Wicke <notifications@github.com>\nwrote:\n\n> Assigned #6206 <https://github.com/tensorflow/tensorflow/pull/6206> to\n> @ebrevdo <https://github.com/ebrevdo>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6206#event-891322354>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimw5FDnAMVPokf5E3wwN6xFcrz4RBks5rHfQhgaJpZM4LImfZ>\n> .\n>\n", "Closing due to most recent response suggesting that this change is not correct.  Feel free to comment if you feel this is incorrect, thanks!"]}]