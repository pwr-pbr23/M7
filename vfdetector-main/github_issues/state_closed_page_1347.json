[{"number": 12673, "title": "Java load python trained mode by SavedModelBundle predict wrong result", "body": " \r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.3/1.2.1\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n \r\n\r\n### Describe the problem\r\nI trained a simple model using python and save it with SavedModelBuilder. Then I loaded it using java. it's loaded successfully but predict wrong result.\r\n\r\n### Source code / logs\r\nMy python code is in\r\nhttps://gist.github.com/fancyerii/af10e8cc35ffda4c983c6e9cf074b317\r\nand java code https://gist.github.com/fancyerii/c1096f9f8c9bd9705da1b1c7d6ca2138\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\n(Though, from the gist of your Python code, I can see one problem - you're invoking `tf.global_variables_initializer().run()` inside the `save_model` function, which means that you're changing the weights to their initial (as opposed to trained) value before saving the model. As a result, the saved model doesn't have the trained weights so your predictions will be wrong)", "I have the same issue.\r\nfancyerii, did you solved this problem?\r\nMy python code is in\r\nhttps://github.com/joyspark/TensorFlow/blob/master/simpleRegression.py\r\nand java code\r\nhttps://github.com/joyspark/TensorFlow/blob/master/loadPythonModel.java", "@joyspark @fancyerii hi, have you resolved the problem? the SavedModelBundle document is so blurred... I  even cannot load the saved model successfully using SavedModelBundle.load() in java.", "@gypleon : Feel free to file a new issue if you're having trouble loading the saved model - though do include a short example to reproduce the problem.\r\n\r\nHaven't heard back from @fancyerii , but there was certainly a problem wherein the model was being exported with random weights, not the trained weights. Not sure about @joyspark 's issue"]}, {"number": 12672, "title": "Fixing typo in default_name argument for name_scope", "body": "Looks like a typo to me and I think this could lead to unexpected behavior.", "comments": ["@raphael-sch, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo, @adarob and @tensorflower-gardener to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@ebrevdo any opinion here?", "@ebrevdo ping", "Jenkins, test this please."]}, {"number": 12671, "title": "Bundling tensorflow app to desktop", "body": "I have got a tensorflow based app which recognizes the objects state. I use opencv and tensorflow to get this done. The final application has to be a desktop app. How do I bundle all dependencies and export it for desktop?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@asimshankar I have asked the same question in stackoverflow but it has been put on hold since they think that it is too broad"]}, {"number": 12670, "title": "add name param to ctc_ops", "body": "", "comments": ["Can one of the admins verify this patch?", "@afpro, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo, @tensorflower-gardener and @andrewharp to be potential reviewers.", "Jenkins, test this please.\r\n\r\n@caisq @ebrevdo how does the new version look?", "Jenkins, test this please.", "Once we like this change, we should trigger an API review.", "@afpro can you take a look at the failing tests. so that we can move the review forward", "@sb2nov i'll check this after work, cause i'm using a mac for work, linux at home.", "Looks like the API has now changed; so LGTM but we'll need API approvers to describe how you should update the API metadata.", "@martinwicke ptal - do we need to run the api regeneration?", "Yes, the instructions are with the failed test.", "@afpro, could you run the API golden regeneration (must be on Linux, py2), and add the changed files to this PR?", "'name' param is not necessary, close this PR."]}, {"number": 12669, "title": "TensorFlowInferenceInterface Constructor with Graph", "body": "Added a new Constructor with `Graph` object only, it would be very useful for custom graph loading. Combining with #12668, user can even perform custom graph loading in C/C++ via Android NDK as well.\r\n\r\nNew Constructor Signature: `public TensorFlowInferenceInterface(Graph g)`", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 12668, "title": "Make Graph(long) Constructor Public", "body": "It would be very useful when user wants to do custom model loading in C/C++ via JNI then return the handle to Java and use this constructor to wrap/maintain the graph object.\r\n\r\nChanged Graph(long) from `Graph(long)` To `public Graph(long)`.", "comments": ["@resec, thanks for your PR! By analyzing the history of the files in this pull request, we identified @asimshankar, @quaeler and @piyush121 to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for the PR, but  I am a little weary of this change. It makes a very unsafe call part of the public API which makes it very easy to incorrectly use or misuse. For example, something like `new Graph(1)` is going to crash with a segmentation fault bringing the JVM down. This also opens the door to security holes.\r\n\r\nThe use case you've described above does seem a bit esoteric. Can't you create the graph in Java? Or create the graph in native code but provide a serialized form to Java? There must be a better alternative to exposing unsafe calls, particularly those that can fail in mysterious ways to a Java programmer (for example, an invalid value will not raise a Java exception, just crash the JVM).", "@asimshankar First of all thanks for your quick response.\r\n\r\nI am not sure how this change would opens the door to security holes, but I am not a expert in this and actually, the reason I want this change is for security and performance in my case. \r\n\r\nI want to deploy my model to mobile device. and the final model size is around 100MB, and have the model zipped and encrypted and splitted into multiple files as assert before putting into the final APK. So I need to merge and decrypt and unzip the model in C/C++ first. \r\n\r\nAs you can see, moving/copying 100MB across C/C++ and Java runtime for multiple times is not a good choice which leaves much memory footprint.\r\n\r\nNote: in iOS case, I really got things done much more smoothly.......\r\n\r\nBesides, I really not like the way that have the plain serialized graph existed in JVM, which is kind of easy for someone to just copy it. I understand that C/C++ is not perfect safe, but we all want more security especially the case that it would even bring better performance?\r\n\r\nI have been struggling in this for few days, and this is the simplest change I came up to achieve what I want, by opening a kind of low-level API.\r\n\r\nJust to share the situation I am facing, any advice towards a better design/solution will be really very appreciated.\r\n\r\nBesides, I believe its user's responsibility to choice what API to use and take the consequence? in the case anyone wants to be evil, a public or not descritor can hardly stop it. So maybe add some more warning doc for this constructor?\r\n\r\n In short, for better performance and more flexibility on loading graph, while leaving everything else in Java world, this is the right constructor I would like to have.\r\n\r\nWaiting for your feedback, great thanks.", "Regarding your specific use case, it seems that you should be able to provide a `ByteBuffer` (see Java NIO support for `DirectByteBuffers`) from native code to Java, and then use that in `Graph.importGraphDef` instead of creating the `TF_Graph` object in native code. Doing so would also mean that you don't copy 100MB across the boundary. Basically, if you need some data manipulation in native code, then make the boundary between Java and native code the data manipulation instead of making the boundary be TensorFlow objects.\r\n\r\nIf that doesn't work, I'd suggest posting a more detailed description of what you're trying to do on StackOverflow and seek opinions there - there is a larger community on StackOverflow and good ideas are likely to come up.\r\n\r\nFor the PR though, I'm going to close it out. I don't feel comfortable providing a public API that is very easy to use incorrectly and which manifests itself with a JVM segmentation fault instead of a clear error message/exception. It also breaks an abstraction boundary - the fact that the graphs are backed by native long handles is an implementation detail that I do not think should be exposed to the user.\r\n\r\nThanks for your understanding.", "@asimshankar Thanks for your patience, I think for the moment I would stick with reflection to use this constructor. And I will really go trying the `DirectByteBuffers`.\r\n\r\nOnce again, here is just sharing my use case, and actually I was trying my luck with this PR(to see if people can accept the idea). I think \"big model\" is a relatively common case, and Tensorflow Android API is now relying on Java API, \"model security\" problem is also be an important concern in mobile deployment. \r\n\r\nSo I do hope in Android/Java API, Tensorflow will have better support for this in the future.\r\n\r\nI am fine with closing this PR. Once again, thanks."]}, {"number": 12667, "title": "Distributed Training Randomly Stops During the Training Process", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 \r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: 6.0\r\n- **GPU model and memory**:  Tesla K80, 12G\r\n\r\n\r\n### Describe the problem\r\nIn my distributed training program, there are one server and two workers, which all run in separately nvidia-docker container. At the beginning, the cluster works just fine, but running normally after several hours, the two workers just stop.\r\n\r\nMy training process:\r\n1. I create three nvidia-docker containers, one for parameter server, two for workers\r\n2. In every container, I run the `train_replica` function below after defining all necessary parts such as cluster_spec, inference function, data batch and so on.\r\n3. It works correctly at the beginning\r\n4. It stops several hours later\r\n\r\n\r\n### Source code / logs\r\nMy trainer function:\r\n```{python}\r\ndef train_replica(cluster_spec,\r\n                  get_data_batch,\r\n                  inference_fn,\r\n                  get_init_fn,\r\n                  get_learning_rate,\r\n                  get_optimizer,\r\n                  get_train_variables,\r\n                  replica_param,\r\n                  train_param,\r\n                  ):\r\n    job_name = replica_param['job_name']\r\n    task_index = replica_param['task_index']\r\n    sync_replicas = train_param['sync_replicas']\r\n    log_dir = train_param['log_dir']\r\n    assert job_name in ['ps', 'worker']\r\n    server = tf.train.Server(cluster_spec, job_name=job_name,\r\n                             task_index=task_index, config=get_ps_session_config())\r\n    if job_name == 'ps':\r\n        server.join()\r\n    else:\r\n        is_chief = (task_index == 0)\r\n        device_setter = tf.train.replica_device_setter(cluster=cluster_spec)\r\n        with tf.Graph().as_default():\r\n            with tf.device(device_setter):\r\n                global_step = create_global_step()\r\n                learning_rate = get_learning_rate(global_step)\r\n                data_batch = get_data_batch()\r\n                _ = inference_fn(data_batch)\r\n                total_loss, task_loss = get_losses()\r\n                optimizer = get_optimizer(learning_rate)\r\n                if sync_replicas:\r\n                    optimizer = tf.train.SyncReplicasOptimizer(\r\n                        opt=optimizer,\r\n                        replicas_to_aggregate=cluster_spec.num_tasks('worker'),\r\n                        total_num_replicas=cluster_spec.num_tasks('worker'),\r\n                        name='sync_replica_optimizer'\r\n                    )\r\n                train_op = slim.learning.create_train_op(\r\n                    total_loss=total_loss,\r\n                    optimizer=optimizer,\r\n                    global_step=global_step,\r\n                    variables_to_train=get_train_variables(),\r\n                    clip_gradient_norm=train_param['clip_norm'],\r\n                    gradient_multipliers=train_param['gradient_multipliers'],\r\n                )\r\n                init_fn = get_init_fn() if get_init_fn is not None else None\r\n                scaffold = tf.train.Scaffold(\r\n                    init_op=tf.global_variables_initializer())\r\n                scaffold._init_fn = init_fn\r\n                hooks = [tf.train.StopAtStepHook(train_param['train_steps'])]\r\n                if sync_replicas is True:\r\n                    hooks.append(optimizer.make_session_run_hook(is_chief))\r\n                chief_only_hooks = [tf.train.LoggingTensorHook([total_loss, task_loss], 100)]\r\n                step_ind = 0\r\n                with tf.train.MonitoredTrainingSession(\r\n                        master=server.target,\r\n                        is_chief=is_chief,\r\n                        checkpoint_dir=log_dir,\r\n                        scaffold=scaffold,\r\n                        hooks=hooks,\r\n                        chief_only_hooks=chief_only_hooks,\r\n                        config=get_worker_session_config(task_index)) as session:\r\n                    while not session.should_stop():\r\n                        session.run(train_op)\r\n                        step_ind += 1\r\n                        if step_ind % 1000 == 0:\r\n                            tf.logging.debug('Training Step At {s}'.format(s=step_ind))\r\n```\r\n", "comments": ["Is there any chance you could have it generate a Python stack trace, or attach a GDB debugger to the process to get a backtrace of all the threads, so we can have a better idea of where the code is getting stuck at?", "@jart \r\nI have run my codes again, and when the training cluster hung, I used gdb to attach the server and worker processes. The trace back is stored in log files.\r\n[ps.log.txt](https://github.com/tensorflow/tensorflow/files/1272757/ps.log.txt)\r\n[worker1.log.txt](https://github.com/tensorflow/tensorflow/files/1272755/worker1.log.txt)\r\n[worker0.log.txt](https://github.com/tensorflow/tensorflow/files/1272756/worker0.log.txt)\r\n\r\n\r\n", "Some more information, this problem exists among all TF versions. \r\nThe stuck problem only happens in SyncReplicasOptimizer. When tracing the stucked processes, all workers and PSes are waiting on mutex. PSes do not get all the gradients in this round, so the global_step won't be updated to start next round. \r\nAnd the problem is related to the variables size in the model. If the variables are small enough(less data to transfer during iterations), the stuck doesn't happen. Not sure the exact threshold.", "@mrry, could you please take a look?", "There's nothing obviously wrong with the code you've shown, but without a minimal and complete reproduction, there's almost no chance we'll be able to trigger the same bug, which might be due to transient network connectivity issues between your containers. In general, for long-running training, I would recommend adding a watchdog process that monitors whether progress is still being made (e.g. whether checkpoints are still being written), and that restarts the cluster from a checkpoint when no progress is detected for (e.g.) 2x the checkpoint interval.", "@mrry \r\nI will try to provide a minimal and complete reproduction. Can I provide My Docker Image and corresponding script to run the Cluster?\r\n\r\nBesides, when the cluster hung, the checkpoints will not be generated. And if the cluster is restarted, the cluster will work correctly until the next sudden hung.\r\n\r\nAs mentioned by @passerbyd, is there any possibility that it is all the grpc's fault? ", "About the checkpoints.\r\nIf train.supervisor is used, checkpoints will keep being written, as it's on a separated thread. So there is no problem about the whole TF process, but only in the optimizer.\r\nAs mentioned by @lsy643, everything works fine after restarting all processes.", "I am experimenting something quite similar. With TensorFlow 1.4 and python 2.7. I run 2 workers (master + worker) one in each GPU and several parameter servers (I tried with 1 and 4). After a couple of hours, the master hangs and the worker keeps working. I run the master and the worker with the python debugger. It works for the worker but not for the master. It seems master is stopped in the C code somewhere. All processes are using the CPU, but master doesn't use any GPU and there is no progress in the training (nothing written in the log). So it could be master is in a loop somewhere. I have no idea how to debug it more to give you more information.", "This is the back trace where it hangs for me:\r\n\r\n```\r\n#0  0x00007ffff7bc738d in pthread_cond_wait@@GLIBC_2.3.2 () from /usr/lib/libpthread.so.0\r\n#1  0x00007fffb19a248d in __gthread_cond_wait (__mutex=<optimized out>, __cond=<optimized out>)\r\n    at /build/gcc/src/gcc-build/x86_64-pc-linux-gnu/libstdc++-v3/include/x86_64-pc-linux-gnu/bits/gthr-default.h:864\r\n#2  std::condition_variable::wait (this=<optimized out>, __lock=...)\r\n    at /build/gcc/src/gcc/libstdc++-v3/src/c++11/condition_variable.cc:53\r\n#3  0x00007fffbc58c2fb in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fffbc58bbe1 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fffbc589134 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fffbc589645 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fffba294c1b in tensorflow::(anonymous namespace)::WaitForNotification(tensorflow::CallOptions*, long long, tensorflow::Notification*) ()\r\n   from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fffba29573b in tensorflow::LocalMaster::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) ()\r\n   from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fffba27ee09 in tensorflow::GrpcSession::RunProto(tensorflow::CallOptions*, tensorflow::MutableRunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) ()\r\n   from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007fffba27fc81 in tensorflow::GrpcSession::RunHelper(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, std::string const&) ()\r\n   from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007fffba2802fb in tensorflow::GrpcSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()\r\n   from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007fffba5084ea in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) () from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007fffba508824 in TF_Run ()\r\n   from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007fffba22601a in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\r\n   from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#15 0x00007fffba226411 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\r\n   from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#16 0x00007fffba1ea6f1 in _wrap_TF_Run ()\r\n   from /python2env/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#17 0x00007ffff74a4449 in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#18 0x00007ffff7503886 in PyEval_EvalCodeEx () from /usr/lib/libpython2.7.so.1.0\r\n#19 0x00007ffff747c80f in function_call.lto_priv () from /usr/lib/libpython2.7.so.1.0\r\n#20 0x00007ffff74ccd93 in PyObject_Call () from /usr/lib/libpython2.7.so.1.0\r\n#21 0x00007ffff74a76fe in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#22 0x00007ffff7503886 in PyEval_EvalCodeEx () from /usr/lib/libpython2.7.so.1.0\r\n#23 0x00007ffff74aa283 in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#24 0x00007ffff7503886 in PyEval_EvalCodeEx () from /usr/lib/libpython2.7.so.1.0\r\n#25 0x00007ffff74aa283 in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#26 0x00007ffff7503886 in PyEval_EvalCodeEx () from /usr/lib/libpython2.7.so.1.0\r\n#27 0x00007ffff74aa283 in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#28 0x00007ffff7503886 in PyEval_EvalCodeEx () from /usr/lib/libpython2.7.so.1.0\r\n#29 0x00007ffff747c9b8 in function_call.lto_priv () from /usr/lib/libpython2.7.so.1.0\r\n#30 0x00007ffff74ccd93 in PyObject_Call () from /usr/lib/libpython2.7.so.1.0\r\n#31 0x00007ffff74a76fe in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#32 0x00007ffff7503886 in PyEval_EvalCodeEx () from /usr/lib/libpython2.7.so.1.0\r\n#33 0x00007ffff747c9b8 in function_call.lto_priv () from /usr/lib/libpython2.7.so.1.0\r\n#34 0x00007ffff74ccd93 in PyObject_Call () from /usr/lib/libpython2.7.so.1.0\r\n#35 0x00007ffff746144f in instancemethod_call.lto_priv () from /usr/lib/libpython2.7.so.1.0\r\n#36 0x00007ffff74ccd93 in PyObject_Call () from /usr/lib/libpython2.7.so.1.0\r\n#37 0x00007ffff74a9c6e in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#38 0x00007ffff7503886 in PyEval_EvalCodeEx () from /usr/lib/libpython2.7.so.1.0\r\n#39 0x00007ffff747c9b8 in function_call.lto_priv () from /usr/lib/libpython2.7.so.1.0\r\n#40 0x00007ffff74ccd93 in PyObject_Call () from /usr/lib/libpython2.7.so.1.0\r\n#41 0x00007ffff74a76fe in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#42 0x00007ffff7503886 in PyEval_EvalCodeEx () from /usr/lib/libpython2.7.so.1.0\r\n#43 0x00007ffff74aa283 in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#44 0x00007ffff7503886 in PyEval_EvalCodeEx () from /usr/lib/libpython2.7.so.1.0\r\n#45 0x00007ffff74a9d7f in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#46 0x00007ffff7503886 in PyEval_EvalCodeEx () from /usr/lib/libpython2.7.so.1.0\r\n#47 0x00007ffff74a9d7f in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#48 0x00007ffff7503886 in PyEval_EvalCodeEx () from /usr/lib/libpython2.7.so.1.0\r\n#49 0x00007ffff74a9d7f in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#50 0x00007ffff74a4b50 in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#51 0x00007ffff74a4b50 in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#52 0x00007ffff74a4b50 in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#53 0x00007ffff74a4b50 in PyEval_EvalFrameEx () from /usr/lib/libpython2.7.so.1.0\r\n#54 0x00007ffff7503886 in PyEval_EvalCodeEx () from /usr/lib/libpython2.7.so.1.0\r\n#55 0x00007ffff751905a in PyEval_EvalCode () from /usr/lib/libpython2.7.so.1.0\r\n#56 0x00007ffff75207f1 in run_mod () from /usr/lib/libpython2.7.so.1.0\r\n#57 0x00007ffff75220d5 in PyRun_FileExFlags () from /usr/lib/libpython2.7.so.1.0\r\n#58 0x00007ffff75222aa in PyRun_SimpleFileExFlags () from /usr/lib/libpython2.7.so.1.0\r\n#59 0x00007ffff7510863 in Py_Main () from /usr/lib/libpython2.7.so.1.0\r\n#60 0x00007ffff7822f6a in __libc_start_main () from /usr/lib/libc.so.6\r\n#61 0x000055555555478a in _start ()\r\n```", "There's no such problem in \"grpc + verbs\" mode. #5394", "To anyone facing hangs in the distributed mode, there was a bug in the version of gRPC used in TF 1.4 that would cause servers to stop serving after a (non-deterministic) period of time. This has been fixed at HEAD, and TensorFlow now uses a version of gRPC with the fix. I'd recommend trying to reproduce the problem with the `tf-nightly` build to rule out that old bug as the source of the problem.", "@lsy643, @jorgemf, can you confirm that the issue has been fixed for you at HEAD?", "@angersson I have been using a nightly build for the last week and no stop happening anymore. I am not sure about TF 1.4.1", "@angersson I ran my experiment again after updating, so far It works fine and no stop happens", "Great, thanks for confirming!", "@mrry  Is there the same issue with grpc used in tf1.7.0?", "@mrry I have encountered hang issue 3 times since last week in tf1.9.0 on a 8x8 GPUs cluster.\r\nby the way, the GPU bound to the hanging process is forever in 100% usage when it is hanging.\r\n> (gdb) bt\r\n#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38\r\n#1  0x00007fdaea31dfa4 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fdaea31d771 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fdaea31acb4 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fdaea31b1d5 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fdae9fa32cb in tensorflow::DirectSession::WaitForNotification(tensorflow::Notification*, long long) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fdae9fa331b in tensorflow::DirectSession::WaitForNotification(tensorf---Type <return> to continue, or q <return> to quit---\r\nlow::DirectSession::RunState*, tensorflow::CancellationManager*, long long) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fdae9fa6fd8 in tensorflow::DirectSession::RunInternal(long long, tensorflow::RunOptions const&, tensorflow::CallFrameInterface*, tensorflow::DirectSession::ExecutorsAndKeys*, tensorflow::RunMetadata*) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fdae9fae123 in tensorflow::DirectSession::RunCallable(long long, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fdae723745d in tensorflow::(anonymous namespace)::RunCallableHelper(tensorflow::Session*, long, _object*, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007fdae7237ddc in tensorflow::TF_SessionRunCallable(TF_Session*, long, _object*, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*)\r\n    ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorf---Type <return> to continue, or q <return> to quit---\r\nlow_internal.so\r\n#11 0x00007fdae71ec0d2 in _wrap_TF_SessionRunCallable ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#12 0x00000000004ea10f in PyCFunction_Call () at ../Objects/methodobject.c:109\r\n#13 0x0000000000536d94 in call_function (oparg=<optimized out>, \r\n    pp_stack=0x7ffc2ac5b120) at ../Python/ceval.c:4705\r\n#14 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\r\n#15 0x0000000000540b0b in _PyEval_EvalCodeWithName (qualname=0x0, name=0x0, \r\n    closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwcount=0, \r\n    kws=<optimized out>, argcount=<optimized out>, args=<optimized out>, \r\n    locals=<optimized out>, globals=<optimized out>, \r\n    _co=<code at remote 0x7fdb80119300>) at ../Python/ceval.c:4018\r\n#16 PyEval_EvalCodeEx () at ../Python/ceval.c:4039\r\n#17 0x00000000004ec2e3 in function_call.lto_priv ()\r\n    at ../Objects/funcobject.c:627\r\n#18 0x00000000005c20e7 in PyObject_Call () at ../Objects/abstract.c:2165\r\n#19 0x00000000004fbfce in method_call.lto_priv ()\r\n    at ../Objects/classobject.c:330\r\n#20 0x00000000005c20e7 in PyObject_Call () at ../Objects/abstract.c:2165\r\n#21 0x0000000000574db6 in slot_tp_call () at ../Objects/typeobject.c:6053\r\n#22 0x00000000005c20e7 in PyObject_Call () at ../Objects/abstract.c:2165\r\n#23 0x0000000000538cab in ext_do_call (nk=<optimized out>, na=0, \r\n---Type <return> to continue, or q <return> to quit---\r\n    flags=<optimized out>, pp_stack=0x7ffc2ac5b4c8, \r\n    func=<_Callable(_session=<Session(_opened=False, _session=<SwigPyObject at remote 0x7fdaa76e62d0>, _created_with_new_api=True, _current_version=0, _extend_lock=<_thread.lock at remote 0x7fdaa7741968>, _closed=False, _delete_lock=<_thread.lock at remote 0x7fdaa7741a30>, _default_session_context_manager=None, _dead_handles=[], _config=<ConfigProto at remote 0x7fdaa76d17c0>, _default_graph_context_manager=None, _graph=<Graph(_names_in_use={'rcnn_ctcv4/expand_conv1/add_n_1/placeholder_1': 1, 'training/adam/distributedadam_allreduce/horovodallreduce_training_adam_gradients_addn_216_0': 1, 'rcnn_ctcv4/expand_conv1/static_batch_normalization_4/moving_variance/read': 1, 'training/adam/zeros_62/const': 1, 'training/adam/gradients/rcnn_ctcv4/conv_block2/unit2/conv2d_20/weight_regularizer/sum_grad': 1, 'rcnn_ctcv4/conv_block2/unit2/static_batch_normalization_21/moving_variance/local_step/assign': 1, 'rescnn_1/static_batch_normalization_36/strided_slice_1/stack_1': 1, 'rcnn_ctcv4/conv_block2/unit3/static_batch_normalization_2...(truncated)) at ../Python/ceval.c:5034\r\n#24 PyEval_EvalFrameEx () at ../Python/ceval.c:3275\r\n#25 0x000000000053b294 in fast_function (nk=<optimized out>, \r\n    na=<optimized out>, n=<optimized out>, pp_stack=0x7ffc2ac5b5f0, \r\n    func=<optimized out>) at ../Python/ceval.c:4803\r\n#26 call_function (oparg=<optimized out>, pp_stack=0x7ffc2ac5b5f0)\r\n    at ../Python/ceval.c:4730\r\n#27 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\r\n#28 0x0000000000540b0b in _PyEval_EvalCodeWithName (qualname=0x0, name=0x0, \r\n---Type <return> to continue, or q <return> to quit---\r\n    closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwcount=0, \r\n    kws=<optimized out>, argcount=<optimized out>, args=<optimized out>, \r\n    locals=<optimized out>, globals=<optimized out>, \r\n    _co=<code at remote 0x7fdb98aa49c0>) at ../Python/ceval.c:4018\r\n#29 PyEval_EvalCodeEx () at ../Python/ceval.c:4039\r\n#30 0x00000000004ec2e3 in function_call.lto_priv ()\r\n    at ../Objects/funcobject.c:627\r\n#31 0x00000000005c20e7 in PyObject_Call () at ../Objects/abstract.c:2165\r\n#32 0x00000000004fbfce in method_call.lto_priv ()\r\n    at ../Objects/classobject.c:330\r\n#33 0x00000000005c20e7 in PyObject_Call () at ../Objects/abstract.c:2165\r\n#34 0x0000000000574db6 in slot_tp_call () at ../Objects/typeobject.c:6053\r\n#35 0x00000000005c20e7 in PyObject_Call () at ../Objects/abstract.c:2165\r\n#36 0x000000000053b656 in do_call (nk=<optimized out>, na=<optimized out>, \r\n    pp_stack=0x7ffc2ac5b990, func=<optimized out>) at ../Python/ceval.c:4936\r\n#37 call_function (oparg=<optimized out>, pp_stack=0x7ffc2ac5b990)\r\n    at ../Python/ceval.c:4732\r\n#38 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\r\n#39 0x000000000053fc97 in _PyEval_EvalCodeWithName () at ../Python/ceval.c:4018\r\n#40 0x000000000053b83f in fast_function (nk=<optimized out>, \r\n    na=<optimized out>, n=<optimized out>, pp_stack=0x7ffc2ac5bba0, \r\n    func=<optimized out>) at ../Python/ceval.c:4813\r\n#41 call_function (oparg=<optimized out>, pp_stack=0x7ffc2ac5bba0)\r\n---Type <return> to continue, or q <return> to quit---\r\n    at ../Python/ceval.c:4730\r\n#42 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\r\n#43 0x000000000053fc97 in _PyEval_EvalCodeWithName () at ../Python/ceval.c:4018\r\n#44 0x000000000053b83f in fast_function (nk=<optimized out>, \r\n    na=<optimized out>, n=<optimized out>, pp_stack=0x7ffc2ac5bdb0, \r\n    func=<optimized out>) at ../Python/ceval.c:4813\r\n#45 call_function (oparg=<optimized out>, pp_stack=0x7ffc2ac5bdb0)\r\n    at ../Python/ceval.c:4730\r\n#46 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\r\n#47 0x0000000000540b0b in _PyEval_EvalCodeWithName (qualname=0x0, name=0x0, \r\n    closure=0x0, kwdefs=0x0, defcount=12, defs=0x7fdac724a8d0, kwcount=10, \r\n    kws=<optimized out>, argcount=<optimized out>, args=<optimized out>, \r\n    locals=<optimized out>, globals=<optimized out>, \r\n    _co=<code at remote 0x7fdac72aea50>) at ../Python/ceval.c:4018\r\n#48 PyEval_EvalCodeEx () at ../Python/ceval.c:4039\r\n#49 0x00000000004ec3f7 in function_call.lto_priv ()\r\n    at ../Objects/funcobject.c:627\r\n#50 0x00000000005c20e7 in PyObject_Call () at ../Objects/abstract.c:2165\r\n#51 0x0000000000538cab in ext_do_call (nk=<optimized out>, na=0, \r\n    flags=<optimized out>, pp_stack=0x7ffc2ac5c068, \r\n    func=<function at remote 0x7fdac7251ea0>) at ../Python/ceval.c:5034\r\n#52 PyEval_EvalFrameEx () at ../Python/ceval.c:3275\r\n#53 0x00000000005401ef in _PyEval_EvalCodeWithName () at ../Python/ceval.c:4018\r\n---Type <return> to continue, or q <return> to quit---\r\n#54 0x000000000053bc93 in fast_function (nk=<optimized out>, \r\n    na=<optimized out>, n=<optimized out>, pp_stack=0x7ffc2ac5c270, \r\n    func=<optimized out>) at ../Python/ceval.c:4813\r\n#55 call_function (oparg=<optimized out>, pp_stack=0x7ffc2ac5c270)\r\n    at ../Python/ceval.c:4730\r\n#56 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\r\n#57 0x000000000053b294 in fast_function (nk=<optimized out>, \r\n    na=<optimized out>, n=<optimized out>, pp_stack=0x7ffc2ac5c3a0, \r\n    func=<optimized out>) at ../Python/ceval.c:4803\r\n#58 call_function (oparg=<optimized out>, pp_stack=0x7ffc2ac5c3a0)\r\n    at ../Python/ceval.c:4730\r\n#59 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\r\n#60 0x000000000053b294 in fast_function (nk=<optimized out>, \r\n    na=<optimized out>, n=<optimized out>, pp_stack=0x7ffc2ac5c4d0, \r\n    func=<optimized out>) at ../Python/ceval.c:4803\r\n#61 call_function (oparg=<optimized out>, pp_stack=0x7ffc2ac5c4d0)\r\n    at ../Python/ceval.c:4730\r\n#62 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\r\n#63 0x000000000053fc97 in _PyEval_EvalCodeWithName () at ../Python/ceval.c:4018\r\n#64 0x00000000005409bf in PyEval_EvalCodeEx () at ../Python/ceval.c:4039\r\n#65 PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, \r\n    locals=<optimized out>) at ../Python/ceval.c:777\r\n#66 0x000000000060cb42 in run_mod () at ../Python/pythonrun.c:976\r\n---Type <return> to continue, or q <return> to quit---\r\n#67 0x000000000060efea in PyRun_FileExFlags () at ../Python/pythonrun.c:929\r\n#68 0x000000000060f7dc in PyRun_SimpleFileExFlags ()\r\n    at ../Python/pythonrun.c:396\r\n#69 0x0000000000640256 in run_file (p_cf=0x7ffc2ac5c740, \r\n    filename=0x218e2f0 L\"train.py\", fp=0x21f3330) at ../Modules/main.c:318\r\n#70 Py_Main () at ../Modules/main.c:768\r\n#71 0x00000000004d0001 in main () at ../Programs/python.c:65\r\n#72 0x00007fdb9e90f830 in __libc_start_main (main=0x4cff20 <main>, argc=10, \r\n    argv=0x7ffc2ac5c958, init=<optimized out>, fini=<optimized out>, \r\n    rtld_fini=<optimized out>, stack_end=0x7ffc2ac5c948)\r\n    at ../csu/libc-start.c:291\r\n#73 0x00000000005d6999 in _start ()\r\n", "@chenjiasheng, anything new ? i see the same call stack ...", "@zrss My colleague has avoided this hanging issue, or at least reduced the hanging probability, by making an explicit call to mpi barrier at the end of each batch.  We don't know why it works but it just goes that way. ", "@chenjiasheng , oh thx a lot, we use the tf 1.8 with horovod nccl, and ran at 8 * 4 GPU cluster, and it hang at the middle of trainning with all GPUs util 100%; we print the backtrace of threads, it is very similar to your case ...\r\n\r\n", "@zrss I met the same problem, have you solved it? "]}, {"number": 12666, "title": "c++ gradients for mean and sum", "body": "Gradients for the math ops, Mean and Sum. Ported from python:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/86d9171b006a2f4a65055228aa13d83d60916052/tensorflow/python/ops/math_grad.py#L95\r\n\r\nI grouped these together because they share helper functions.\r\n\r\ncc @suharshs\r\n\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Ah unfortunately someone internally just submitted this change yesterday.  Should sync soon. Apologies :(\r\n\r\nFrom now on can you create an issue with your in progress gradients with \"c++ gradient\" in the title. I will ask internal folks to do the same, this way we can avoid stepping on toes due to sync latency.", "@suharshs No problem at all. Getting the functionality in there is the important thing."]}, {"number": 12665, "title": "Conv2DGrad & MaxPoolGradHelper", "body": "Gradients for the nn ops, Conv2D and MaxPool. Ported from python:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/86d9171b006a2f4a65055228aa13d83d60916052/tensorflow/python/ops/nn_grad.py#L457\r\n\r\nThese are simple enough that I grouped them into a single PR. I can split into two, if necessary.\r\n\r\ncc @suharshs @dguerra\r\n\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@bpiel, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @DeNeutoy and @vrv to be potential reviewers.", "should this get merged? Just worried about the branch becoming in conflict with master", "Jenkins, test this please.", "GPU build rerun at http://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/6635", "XLA rerun at http://ci.tensorflow.org/job/tensorflow-pull-requests-xla/2636", "What's the difference between MaxPool and MaxPoolV2? Should it be used one over the other?", "We (almost) never change ops, whenever we have to, we instead introduce a new V2 (or V3, or V4) op. That makes sure that old serialized graphdefs keep working.\r\n\r\nSo in your code, use V2, it's the newest version."]}, {"number": 12664, "title": "Fix markdown syntax mistakes", "body": "Add a space after `##`, `###` and `####`.", "comments": ["Can one of the admins verify this patch?", "@meijun, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener and @lukeiwanski to be potential reviewers."]}, {"number": 12663, "title": "Improve documentation of tf.gfile.GFile vs tf.gfile.FastGFile", "body": "is not clear from the docs what is the difference between the two. \r\nI assumed that one has thread locking (.GFile) and the other does not, but both say the same thing in the docs:\r\n- [https://www.tensorflow.org/api_docs/python/tf/gfile/GFile](https://www.tensorflow.org/api_docs/python/tf/gfile/GFile)\r\n- [https://www.tensorflow.org/api_docs/python/tf/gfile/FastGFile](https://www.tensorflow.org/api_docs/python/tf/gfile/FastGFile)", "comments": ["@ericdnielsen Do you know what the difference is between these two classes?", "Actually, I'm curious to know what the difference between the two is as well. ", "OK so in the open source world, there is no difference.\r\n\r\nPython's file API has mutexes. So FastGFile is the same as GFile. This weird division exists because, internally at Google where TensorFlow first evolved, there exists two absl-py classes that that have these names. They hook into some C++ APIs and do mutexing manually. The constructors for these two classes optionally provide it an RLock or NullLock.\r\n\r\nWe probably don't have the time right now to clean up this API. We should however make FastGFile undocumented, leave a plain old comment saying it should be marked for deletion, and then never delete it, due to our API stability promise.\r\n\r\nIf someone can enlighten me on the benefits of saving nanoseconds in Python when even system calls these days take microseconds, then I'd suggest we implement it, but otherwise seems doubtful.\r\n\r\nI'll mark this contributions welcome for docs cleanup help.", "Forgive me a moment while I correct myself. Remembering that TensorFlow has its own abstract C++ file system implementation under the hood, it might be worth examining this more closely to see if we're doing the right thing with locking.\r\n\r\nIn all likelihood, the contracts of this API are likely safe, but it's possible there's inconsistencies that only relate to Google. Someone who has the cycles might want to take a closer look.", "@Mistobaan, does this answer your question to completion?", "@aselle from my personal tests seems like FastGFile does not have locking, while GFile does. But I did not dig into the C++ internals to figure it out definitely. The documentation does not say anything and is equal to GFile.  Many example online use FastGFile. I think an official clarification would be nice to have :)", "To be clear, as @jart's original analysis mentioned, there is no difference in implementation between GFile and FastGFile in the current codebase:\r\nhttps://github.com/tensorflow/tensorflow/blob/f02efd91973a596469333f562146b6b3c0e5c83a/tensorflow/python/platform/gfile.py#L41", "ok then must be the exact same thing. ", "Fixed in #21490."]}, {"number": 12662, "title": "Adding a decode to support Python 3.5 on Windows.", "body": "Fix this error: python 3.5: TypeError: a bytes-like object is required, not 'str'\r\n\r\n", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener and @wangqr to be potential reviewers.", "Let's move this change internally to discuss the full details."]}, {"number": 12661, "title": "Element wise optimizations", "body": "Added MKL element-wise ops that utilize eigen ops as their back-end. Also added an input-conversion op that ensures that shapes of both input tensors are compatible (same or broadcastable).\r\n\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@vivek-rane, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @tensorflower-gardener and @martinwicke to be potential reviewers.", "Ping @andydavis1 @rmlarsen ", "@tfboyd @andydavis1 @rmlarsen Please merge this one second (after 12654)", "Jenkins, test this please.", "MacOS rerun: http://ci.tensorflow.org/job/tensorflow-pull-requests-mac/6176", "@martinwicke thanks! From the rerun:\r\n\r\n```\r\nExecuted 1193 out of 1193 tests: 1193 tests pass.\r\n\r\nParameterized build ends with SUCCESS at: Tue Sep  5 09:51:01 PDT 2017 (Elapsed time: 2281 s)\r\nFinished: SUCCESS\r\n```", "@tfboyd @andydavis1 @rmlarsen @martinwicke any reason we're blocked on this one?", "Let me get double check.  It looks like Martin had the Mac tests rerun and they are fine.  ", "@yifeif   Can you take a look and get this merged if everything is ok?  Thank you very much.  We are ok with the changes as long as the tests pass.  ", "@yifeif sorry to pester but we have a lot of things waiting on this pull request. Please can you check if merge is possible? (see comment by @tfboyd)", "We are ready to merge now! Thanks for the patience."]}, {"number": 12660, "title": "[Reopened][XLA] Bring XLA Transpose021Tiled up-to-date with reference CUDA implementation.", "body": "Reopening this [PR](https://github.com/tensorflow/tensorflow/pull/12337).\r\nThe commits have been cherry-picked to master branch.\r\n\r\nPing @jpienaar, @jhseu because you were the previous assignee and assigner.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Do you have some performance results to compare the new and current implementations? ", "Hi, I did a manual test on two sets of inputs. One transposes 4 matrices of size 1024x1024, the other transposes 4 matrices of size 512x512. It takes current implementation 549.36us and 152.84us to finish whilst only  283.75us and 75.842us for the new implementation on a K40 GPU.\r\n\r\nI actually do not know how to invoke this transpose method using Python scripts so I used nvprof on test cases I added in xla/tests/copy_test.cc. Also, nvprof automatically aggregates the performance metrics of kernels with the same name so I can only measure two test cases at one run which is quite inconvenient. Would you please tell me what's the best way to benchmark it? It may come in handy in the future as well. \r\n\r\nIn addition, I am not sure how important this transpose kernel is. If it is performance critical then I can try to bring other specializations done in the reference cuda kernels here as well.", "That sounds good thanks! I'll try to add a benchmark for this.\r\n\r\nAs to performance critical, this is more about wanting to supplement the decision to change the implementation with results. It is always nicer if an implementation is changed as it provides better performance or corrects a bug. Those are more measurable benefits :)\r\n\r\nThe folks who are the main GPU backend developers are otherwise engaged this week. If this is not blocking you, I'm going to defer until they are able to review it.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Just to make sure, please do not merge yet, hopefully more commits are to come early next week.", "Hi, can anyone take a look at this [PR](https://github.com/tensorflow/tensorflow/pull/13049) ? For now the CUDA reference implementation seems to be inefficient when dealing with narrow matrices and before I go ahead and implement some specializations for dealing with narrow matrices in XLA, I'd like to make sure that the CUDA version of my idea looks okay.", "Jenkins, test this please.", "Transient failure on Windows. Jenkins, test this please.", "Build timeout on GPU?\r\n\r\nJenkins, test this please.", "Jenkins, test this please."]}, {"number": 12659, "title": "tf.maximum does not return nan when inputs contain nan", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: Tesla K40m, 11439MiB\r\n- **Exact command to reproduce**: python main.py\r\n\r\n### Describe the problem\r\n`tf.maximum(a, b)` should return `nan` when `a` or `b` contain `nan`. However, it does not at some cases.\r\n\r\n### Source code / logs\r\nmain.py\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\na = tf.placeholder(dtype=tf.float32)\r\nmax_a = tf.maximum(a, 1.)\r\nwith tf.Session():\r\n    print max_a.eval(feed_dict={a: np.nan})\r\n```\r\nThe output is:\r\n```\r\n1.0\r\n```", "comments": ["Well that's concerning. What's odd is that if you don't use `feed_dict`, then it actually does return `nan`:\r\n\r\n```pycon\r\n>>> tf.Session().run(tf.maximum(np.nan, 1.))\r\nnan\r\n```\r\n\r\nWhat do you think about this @mrry?", "When I run @meijun's code on my CPU-only workstation, I get the expected result (`nan`). Also, I think @jart's version (which passes `np.nan` as a constant) will end up running on the CPU as part of a constant folding pass.\r\n\r\nI suspect the problem might be in the GPU implementation of `tf.maximum()`. This looks like code that hasn't changed for a long time, and is basically a call into Eigen. Perhaps @benoitsteiner knows a potential cause for the discrepancy?", "I tends to think the issue comes from using `std::max` as the op. As far as I know, `std::max` is implemented as `(a < b) ? b : a` which does not take into consideration if `a` or `b` is `nan`.\r\n\r\nFor example, with the following:\r\n```c++\r\n$ cat max.cpp \r\n#include <algorithm>\r\n\r\nint main(int argc, char *argv[])\r\n{\r\n    printf(\"std::max(1., nan) = %f\\n\", std::max(1.0, std::numeric_limits<double>::quiet_NaN()));\r\n    printf(\"std::max(nan, 1.) = %f\\n\", std::max(std::numeric_limits<double>::quiet_NaN(), 1.0));\r\n\r\n    return 0;\r\n}\r\n$ g++ max.cpp\r\n$ ./a.out \r\nstd::max(1., nan) = 1.000000\r\nstd::max(nan, 1.) = nan\r\n$ \r\n```\r\n\r\nAnd in TensorFlow with CPU version:\r\n```python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> import numpy as np\r\n>>> \r\n>>> print tf.Session().run(tf.maximum(np.nan, 1.))\r\nnan\r\n>>> \r\n>>> print tf.Session().run(tf.maximum(1., np.nan))\r\n1.0\r\n>>> \r\n>>> a = tf.placeholder(dtype=tf.float32)\r\n>>> max_a = tf.maximum(a, 1.)\r\n>>> with tf.Session():\r\n...     print max_a.eval(feed_dict={a: np.nan})\r\n... \r\nnan\r\n>>> a = tf.placeholder(dtype=tf.float32)\r\n>>> max_a = tf.maximum(1., a)\r\n>>> with tf.Session():\r\n...      print max_a.eval(feed_dict={a: np.nan})\r\n... \r\n1.0\r\n>>> \r\n```\r\n", "Depending on what the desired output is, the fix might varies.\r\n\r\nIn C99' `<math.h>` there is `fmax` which covers `nan`. However, the behavior is exactly opposite to `numpy`:\r\nhttp://en.cppreference.com/w/c/numeric/math/fmax\r\n> Error handling\r\n> This function is not subject to any of the error conditions specified in math_errhandling.\r\n> If the implementation supports IEEE floating-point arithmetic (IEC 60559),\r\n> - If one of the two arguments is NaN, the value of the other argument is returned\r\n> - Only if both arguments are NaN, NaN is returned\r\n\r\nTo achieve the same behavior as `numpy` I think `isnan()` might have to be used even though this is less ideal.\r\n\r\n", "The behavior of `numpy`:\r\n```python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> np.max([float('1'), float('nan')])\r\nnan\r\n>>> np.max([float('nan'), float('1')])\r\nnan\r\n>>> \r\n```", "On GPU the issue exists as well:\r\n```python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> import numpy as np\r\n>>> \r\n>>> print tf.Session().run(tf.maximum(np.nan, 1.))\r\n2017-09-15 16:17:47.560456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-15 16:17:47.560956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\r\n2017-09-15 16:17:47.560992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\nnan\r\n>>> \r\n>>> print tf.Session().run(tf.maximum(1., np.nan))\r\n2017-09-15 16:18:14.369889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n1.0\r\n>>> \r\n>>> a = tf.placeholder(dtype=tf.float32)\r\n>>> max_a = tf.maximum(a, 1.)\r\n>>> with tf.Session():\r\n...     print max_a.eval(feed_dict={a: np.nan})\r\n... \r\n2017-09-15 16:18:44.264749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n1.0\r\n>>> a = tf.placeholder(dtype=tf.float32)\r\n>>> max_a = tf.maximum(1., a)\r\n>>> with tf.Session():\r\n...   print max_a.eval(feed_dict={a: np.nan})\r\n... \r\n2017-09-15 16:19:20.710406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n1.0\r\n>>> \r\n```", "@mrry @jart @benoitsteiner I do think the issue may need to be addressed one way or another, either stick with `numpy` or stick with `<math.h>`' `fmax()`.\r\n\r\nThe reason is that `std::max()` is mathematically problematic:\r\n```\r\nstd::max(1., nan) != std::max(nan, 1.)\r\n\r\nstd::max(1., nan) == std::min(1, nan)\r\n```\r\n\r\nI can help with PR if TensorFlow team can provide direction guidance.", "Actually it seems numpy defined `maximum` and `fmax` for different cases already:\r\nhttps://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html\r\n```\r\nSee also\r\nminimum Element-wise minimum of two arrays, propagates NaNs.\r\nfmax    Element-wise maximum of two arrays, ignores NaNs.\r\namax    The maximum value of an array along a given axis, propagates NaNs.\r\nnanmax  The maximum value of an array along a given axis, ignores NaNs.\r\n```", "We are also facing same problem with TF's relu operator (with GPU) test case for NaNs on Linux RHEL ppc64le. \r\nRelu operator computes rectified max which calls Eigen's cwiseMax function. \r\nThe behavior is weird on ppc64le shown by below code snippet-\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n#from tensorflow.python.ops import nn_ops\r\n\r\nwith tf.Session() as sess:\r\n  for i in range(18):\r\n    x = np.zeros(i) + np.nan\r\n    z = tf.nn.relu(tf.constant(x))\r\n#    z = tf.maximum(x, np.zeros(i))\r\n    result = sess.run(z)\r\n    np.isnan(result).all()\r\n    print(\"For i = %d, result is %s\" % (i, result))\r\n```\r\nOutput of this on ppc64le:\r\n```\r\nFor i = 0, result is []\r\nFor i = 1, result is [ nan]\r\nFor i = 2, result is [ 0.  0.]\r\nFor i = 3, result is [  0.   0.  nan]\r\nFor i = 4, result is [ 0.  0.  0.  0.]\r\nFor i = 5, result is [  0.   0.   0.   0.  nan]\r\nFor i = 6, result is [ 0.  0.  0.  0.  0.  0.]\r\nFor i = 7, result is [  0.   0.   0.   0.   0.   0.  nan]\r\nFor i = 8, result is [ 0.  0.  0.  0.  0.  0.  0.  0.]\r\nFor i = 9, result is [  0.   0.   0.   0.   0.   0.   0.   0.  nan]\r\nFor i = 10, result is [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\nFor i = 11, result is [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  nan]\r\nFor i = 12, result is [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\nFor i = 13, result is [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  nan]\r\nFor i = 14, result is [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\nFor i = 15, result is [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  nan]\r\nFor i = 16, result is [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\nFor i = 17, result is [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  nan]\r\n```\r\nFor odd sized array, all elements in output are 0 except last element. And for even sized input array, output contains all zeros. \r\nOn x86, output of same code is\r\n```\r\nFor i = 0, result is []\r\nFor i = 1, result is [ nan]\r\nFor i = 2, result is [ nan  nan]\r\nFor i = 3, result is [ nan  nan  nan]\r\nFor i = 4, result is [ nan  nan  nan  nan]\r\nFor i = 5, result is [ nan  nan  nan  nan  nan]\r\nFor i = 6, result is [ nan  nan  nan  nan  nan  nan]\r\nFor i = 7, result is [ nan  nan  nan  nan  nan  nan  nan]\r\nFor i = 8, result is [ nan  nan  nan  nan  nan  nan  nan  nan]\r\nFor i = 9, result is [ nan  nan  nan  nan  nan  nan  nan  nan  nan]\r\nFor i = 10, result is [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\r\nFor i = 11, result is [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\r\nFor i = 12, result is [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\r\nFor i = 13, result is [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\r\nFor i = 14, result is [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\r\nFor i = 15, result is [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\r\nFor i = 16, result is [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\r\n  nan]\r\nFor i = 17, result is [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\r\n```\r\n\r\nWhich linux architecture this original issue is reported on? And could anyone give any pointers on how to make TF behave similar to numpy. I know underlying implementation of this max operator is in Eigen. I tried modifying Eigen's code too for ppc64le (Altivec/PacketMath.h) but no luck.\r\nPlease refer [our issue](https://github.com/tensorflow/tensorflow/issues/11603) for more details.\r\n", "Request tensorflow core members to provide some guidance on this issue. I'm also discussing this with Eigen community.", "(Assigning @benoitsteiner as the most obvious intersection of the TensorFlow core team and Eigen community.)", "Raised a bug with Eigen Community - http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1494", "Issue #11603 is similar to this and might well be a dup.\r\nThe differences seen in 11603 and here likely come down to the architecture specific implementations of Eigen's PacketMath pmax function.\r\n\r\nPer [IEEE-754R](https://msdn.microsoft.com/en-us/library/windows/desktop/jj218760(v=vs.85).aspx#alpha_754_Deviations) the behavior should be:\r\n\r\nmin(x,QNaN) == min(QNaN,x) == x (same for max)\r\n\r\nIn practice with std::max we see:\r\nstd::max(nan, x) = nan\r\nstd::max(x, nan) = x\r\n\r\nIn Eigen's pmax functions it differs per architecture:\r\n\r\n**x86 Eigen PacketMath pmax**\r\nThrough Eigen [bug 1373](http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1373) the x86 archs under Eigen/src/Core/arch were changed to match std::max's behavior of always returning the first argument when comparing NaNs.  Previous to this it followed the Intel SIMD intrinsics' behavior of always returning the second argument.\r\n\r\n**CUDA Eigen PacketMath pmax**\r\nFloat and double are implemented using [fmaxf](http://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__SINGLE.html#group__CUDA__MATH__SINGLE_1g6e7516db46be25c33fb26e203287f2a3) and [fmax](http://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__DOUBLE.html#group__CUDA__MATH__DOUBLE_1g8f5b0627e6706e432728bd16cb326754) respectively.  Per CUDA's API doc these handle NaNs like this: \"If one argument is NaN, returns the numeric argument.\"\r\n\r\nSo the Eigen CUDA impl is following IEEE-754R in this respect.\r\n\r\nHalf math is a bit different and is using > and < operators on the respective halves.\r\n\r\n**ALTIVEC (used by ppc64le) Eigen PacketMath pmax**\r\n\r\nThe pmax functions are implemented using the vec_max intrinsic which follows IEEE-754R and thus returns numeric for numeric vs NaN comparisons.\r\n\r\nSo to resolve the issue of differing functionality between std::max, x86 Eigen, and CUDA/ALTIVEC Eigen a decision has to be made which behavior is the desired behavior.\r\n\r\nThe changes under Eigen [bug 1373](http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1373) were done because it \"makes the Eigen Tensor library properly propagate NaNs for the Relu operator, which is performance critical for neural net models.\"", "Hi, is this still an issue?", "To my knowledge this is still an issue. The ALTIVEC (ppc64le) pmin/pmax functions in Eigen were changed to match x86 behavior, but there is still differing behavior between CPU and GPU calculations with NaNs in min/max.  The CUDA/GPU implementation in Eigen uses CUDA API calls which will give different results from the x86 and ppc64le CPU call as discussed in my comment above.\r\n\r\nI believe this CPU-GPU discrepancy needs to be addressed so operations give the same result regardless of the device they run on. It would help if TensorFlow weighed in on which behavior it desires in the Eigen functions / tensor operations and then chime in on http://eigen.tuxfamily.org/bz/show_bug.cgi?id=564.\r\n\r\nOnce the desired behavior is chosen the appropriate changes can be pushed into the various architecture implementations in Eigen to make them align.", "@rmlarsen are you a good person to look at this, or can you redirect as necessary? Thanks!", "I believe it's still an issue. \r\n\r\nAfter debugging I found this was source of my problem. My specs:\r\n\r\ntensorflow-gpu - 1.11.0\r\nOS: Windows", "@mrry Could you please look into this issue ?", "@harshini-gadige This is an issue in Eigen, which is not my area of expertise. ", "As I've stated previously, it's not so much an issue with Eigen as it is a lack of direction or choice from TensorFlow as to what TensorFlow's preferred behavior is for the min and max operations when it comes to NaNs. Once that is known, it is relatively simple to change the code to be consistent in Eigen.", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=12659\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=12659\">No</a>\n", "This is fixed with latest version of  TF  version '1.15.0-dev20190821' using cpu as well as gpu accelerator.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\na = tf.placeholder(dtype=tf.float32)\r\nmax_a = tf.maximum(a, 1.)\r\nwith tf.Session():\r\n    print(max_a.eval(feed_dict={a: np.nan}))\r\n#prints \r\nnan\r\n```\r\nFeel free to reopen if still have problems. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=12659\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=12659\">No</a>\n", "The problem still exists. I used the latest tf-nightly-gpu container, tensorflow/tensorflow:nightly-gpu-py3.\r\n\r\n```\r\npip list | grep tf\r\ntf-estimator-nightly 1.14.0.dev2019082801\r\ntf-nightly-gpu       1.15.0.dev20190821  \r\n```\r\n\r\nTest program:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ncpu_device='/cpu:0'\r\ngpu_device='/device:GPU:0'\r\n\r\ndevices = [cpu_device, gpu_device]\r\n\r\nfor device in devices:\r\n    print('Testing device %s' % device)\r\n    a = tf.placeholder(dtype=tf.float32)\r\n    with tf.device(device):\r\n        max_a = tf.maximum(a, 1.)\r\n        max_b = tf.maximum(1., a)\r\n    with tf.Session():\r\n        print('max(nan, 1.): %s' % max_a.eval(feed_dict={a: np.nan}))\r\n        print('max(1., nan): %s' % max_b.eval(feed_dict={a: np.nan}))\r\n```\r\n\r\nOutputs:\r\n```\r\nTesting device /cpu:0\r\nmax(nan, 1.): nan\r\nmax(1., nan): 1.0\r\nTesting device /device:GPU:0\r\nmax(nan, 1.): 1.0\r\nmax(1., nan): 1.0\r\n```\r\n\r\nSo we are still observing the difference which can be described as the CPU implementation following the std::max convention while the GPU op implementation is following IEEE-754R, per my comments here: https://github.com/tensorflow/tensorflow/issues/12659#issuecomment-368129230\r\n\r\nIf we want standardized behavior out of TensorFlow for this operation it's a matter of determining which behavior is desired and then getting the changes in Eigen. Additional discussion on the pros/cons of std::max vs IEEE-754R behavior and how to support both behaviors via API choice has occured in Eigen bug 564 over the past few months.\r\n", "@ymodak I do not have the authority to re-open the issue.", "This also reproduces in TensorFlow 2.0 at commit https://github.com/tensorflow/tensorflow/tree/b8b60ae09afb3b05dde9f183bac587c7dd437c78 from the r2.0 branch:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ncpu_device='/cpu:0'\r\ngpu_device='/device:GPU:0'\r\n\r\ndevices = [cpu_device, gpu_device]\r\n\r\nfor device in devices:\r\n    print('Testing device %s' % device)\r\n    with tf.device(device):\r\n        print('max(nan, 1.): %s' % tf.maximum(np.nan, 1.))\r\n        print('max(1, nan): %s' % tf.maximum(1., np.nan))\r\n```\r\n\r\nOutputs:\r\n```\r\nTesting device /cpu:0\r\nmax(nan, 1.): tf.Tensor(nan, shape=(), dtype=float32)\r\nmax(1, nan): tf.Tensor(1.0, shape=(), dtype=float32)\r\nTesting device /device:GPU:0\r\nmax(nan, 1.): tf.Tensor(1.0, shape=(), dtype=float32)\r\nmax(1, nan): tf.Tensor(1.0, shape=(), dtype=float32)\r\n```", "One further datapoint. On the Intel MKL TensorFlow build the test program, modified to only test the CPU device, outputs:\r\n\r\n```\r\nmax(nan, 1.): nan\r\nmax(1., nan): 1.0\r\n```\r\n\r\n```\r\nconda list | grep tensorflow\r\ntensorflow                1.14.0          mkl_py36h2526735_0    anaconda\r\ntensorflow-base           1.14.0          mkl_py36h7ce6ba3_0    anaconda\r\ntensorflow-estimator      1.14.0                     py_0    anaconda\r\n```", "On 2.2.0-dev20200131 this also happens for sign:\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    print(\"reduce_max:\", tf.reduce_max(tf.fill((1,4,4), float(\"NaN\"))))\r\n    print(\"sign:\", tf.sign(tf.fill((1,4,4), float(\"NaN\"))))\r\n\r\nOutput:\r\n\r\n    reduce_max: tf.Tensor(-inf, shape=(), dtype=float32)\r\n    sign: tf.Tensor(\r\n    [[[0. 0. 0. 0.]\r\n      [0. 0. 0. 0.]\r\n      [0. 0. 0. 0.]\r\n      [0. 0. 0. 0.]]], shape=(1, 4, 4), dtype=float32)\r\n\r\nBut not on numpy:\r\n\r\n    print(\"np.max:\", np.max(tf.fill((1,4,4), float(\"NaN\"))))\r\n    print(\"np.sign:\", np.sign(tf.fill((1,4,4), float(\"NaN\"))))\r\n\r\n    np.max: nan\r\n    np.sign: [[[nan nan nan nan]\r\n      [nan nan nan nan]\r\n      [nan nan nan nan]\r\n      [nan nan nan nan]]]\r\n", "@meijun , The issue has been fixed in the latest Tensorflow 2.5 version for both CPU and GPU, please find the [gist](https://colab.research.google.com/gist/sachinprasadhs/240087877dd9dbc17ff2077bcf8f6239/untitled.ipynb) here and the below output.\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n2.5.0\r\nimport numpy as np\r\nprint(\"1. \",tf.maximum(1.,np.nan))\r\nprint(\"2. \",tf.maximum(np.nan,1.))\r\n\r\n\r\nimport numpy as np\r\nprint(\"1. \",tf.maximum(1.,np.nan))\r\nprint(\"2. \",tf.maximum(np.nan,1.))\r\n\r\n1.  tf.Tensor(nan, shape=(), dtype=float32)\r\n2.  tf.Tensor(nan, shape=(), dtype=float32)\r\n```", "The output above shows `tf.maximum` returning the `nan` regardless of order. This doesn't align with IEEE-754R or the std::max convention see [this comment above](https://github.com/tensorflow/tensorflow/issues/12659#issuecomment-526257969).\r\n\r\nWhat device was used above (CPU or NVIDIA GPU?), if CPU, was Eigen or MKL used?", "@smatzek I have reproduced it in google colab, in both CPU and GPU regardless of nan order the output should return nan for all right as per the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/12659\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/12659\">No</a>\n", "Are we AGREED that the consensus is max and min should always return NaN if either of the inputs to the function is a NaN?\r\nI still see some suggestions in some places that it should in fact return the non-NaN value...", "> Are we AGREED that the consensus is max and min should always return NaN if either of the inputs to the function is a NaN?\r\n> I still see some suggestions in some places that it should in fact return the non-NaN value...\r\n\r\nhttps://numpy.org/doc/stable/reference/generated/numpy.maximum.html\r\nhttps://pytorch.org/docs/stable/generated/torch.maximum.html\r\n\r\nCurrent behavior makes sense if you look at the numpy and pytorch. \r\nIt make sense to me that everybody aligns to numpy."]}, {"number": 12658, "title": "Support CopyFile with streaming", "body": "This fix tries to address the issue raised in #12641 where it was not possible to have CopyFile with streaming. The original implementation copies the whole content of the file to a string\r\nbuffer and write to the file. This could be an issue if the file size is too large (than the memory of the host).\r\n\r\nThis fix streams the CopyFile operation.\r\n\r\n*Also, sendfile is used if the file system is posix*\r\n\r\nThis fix fixes #12641.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @rohan100jain and @josh11b to be potential reviewers.", "Can one of the admins verify this patch?", "/cc @rohan100jain @jart ", "@rohan100jain will this make the 1.4 release?", "Thanks @rohan100jain for the review. The PR has been updated. Please take a look.", "Jenkins, test this please.", "@yongtang can you look into the failing tests", "On Mac sendfile is not defined in `<sys/sendfile.h>`. Though it looks like there is a Darwin version of `sendfile`:\r\nhttps://developer.apple.com/legacy/library/documentation/Darwin/Reference/ManPages/man2/sendfile.2.html\r\n\r\nWill take a look and update the PR.", "@yongtang It seems like there is also an issue with the Linux ARM build:\r\n\r\n```c++\r\n11:44:48 /android/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-g++ --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -O2 --sysroot /android/ndk/platforms/android-21/arch-arm -Wno-narrowing -fomit-frame-pointer -march=armv7-a -mfloat-abi=softfp -mfpu=neon -fPIE -MT /workspace/tensorflow/contrib/makefile/gen/obj/tensorflow/core/platform/posix/posix_file_system.o -MMD -MP -MF /workspace/tensorflow/contrib/makefile/gen/dep//tensorflow/core/platform/posix/posix_file_system.Td -I/android/ndk/sources/android/support/include -I/android/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -I/android/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi/include -I. -I/workspace/tensorflow/contrib/makefile/downloads/ -I/workspace/tensorflow/contrib/makefile/downloads/eigen -I/workspace/tensorflow/contrib/makefile/downloads/gemmlowp -I/workspace/tensorflow/contrib/makefile/downloads/nsync/public -I/workspace/tensorflow/contrib/makefile/downloads/fft2d -I/workspace/tensorflow/contrib/makefile/gen/protobuf/include -I/workspace/tensorflow/contrib/makefile/gen/proto/ -I/workspace/tensorflow/contrib/makefile/gen/proto_text/ -c tensorflow/core/platform/posix/posix_file_system.cc -o \r\n11:44:50 tensorflow/core/platform/posix/posix_file_system.cc: In member function 'virtual tensorflow::Status tensorflow::PosixFileSystem::CopyFile(const string&, const string&)':\r\n11:44:50 tensorflow/core/platform/posix/posix_file_system.cc:307:54: error: cannot convert 'off_t* {aka long int*}' to 'off64_t* {aka long long int*}' for argument '3' to 'ssize_t sendfile64(int, int, off64_t*, size_t)'\r\n11:44:50      rc = sendfile64(target_fd, src_fd, &offset, chunk);\r\n11:44:50                                                       ^\r\n```", "@quaeler Thanks and sorry for the late reply (was out of town). The Android failure could be fixed with `sendfile64` -> `sendfile`. I also updated the Mac OS X implementation so that appropriate sendfile API could be used.", "@drpngx could you kick off a build please", "Jenkins, test this please.", "Jenkins seems troubled again - https://ci.tensorflow.org/job/tensorflow-pull-requests-mac/6766/console\r\n\r\n(Thanks for the kickoff, @drpngx!)", "Thanks @quaeler @drpngx, the PR has been updated with Darwin OS X fix.", "Thanks @yongtang - your branch now builds the libtensorflow.so target fine for me on OS X 10.12.6.\r\n\r\n@drpngx could you pass this on to Jenkins please?", "Jenkins, test this please.", "@rohan100jain are you good with the changes?\r\n\r\nChecking again... Jenkins, test this please.", "@yongtang The file_io_test failure on MacOS does look a little suspicipous. I reran the tests, but could you take a look?\r\n\r\nThanks!", "@yongtang The [Darwin man page for sendfile](https://developer.apple.com/legacy/library/documentation/Darwin/Reference/ManPages/man2/sendfile.2.html) reads differently than the Linux one (aside from method signature) and seems to imply the destination descriptor is not allowed to be a file descriptor, only a socket descriptor; this would jive with the failing tests that complain like:\r\n```\r\nInvalidArgumentError: /var/folders/l_/bvssjjhd2s10ymxdfx1xy12c0000gn/T/exporter_testI_uRih/export_no_shard/00000222-tmp/assets/hello42.txt; Socket operation on non-socket\r\n```", "@quaeler thanks for looking into this. Any recommendation on a fix?", "@quaeler @drpngx Thank for the help. I think it makes sense to disable sendfile in MacOS and use fallback implementation. I don't have a Mac dev env setup at the moment but plan to setup one today to take a look.", "@yongtang @drpngx I concur - disable sendfile usage for MacOS and fallback to read()/write()-isms.\r\n", "Sounds good, waiting for the patch. Thanks!", "The PR has been updated. Tested on my Mac machine and it works. Please take a look.", "Passes for me as well. Thanks @yongtang !", "Jenkins, test this please.", "MacOSX issue has been fixed now though Jenkins build on Windows still fails with linkage issue. I will take a look.", "Thanks @jart for the review. The PR has been updated.\r\n\r\nThe Windows build might fail as there was a linkage error before and I haven't been able to find out the reason. Unfortunately I don't have a Windows dev machine available at the moment. I will see what I could do to resolve the Windows build error.", "@jart The PR has been updated. Now all build passes (I think `Ubuntu contrib` failure is unrelated). Please take a look and let me know if there are any issues.", "@jart good to go?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@jart Can you take a look and see if the PR is ready to go? I think the build `Ubuntu contrib` is unrelated.", "Yup, it's unrelated."]}, {"number": 12657, "title": "Undefined op in speech_commands example", "body": "I am using tensorflow 1.3.0 built from source.\r\n\r\nI run the newly released speech_commands example (its in tensorflow/examples/speech_commands), following all the stuffs and all good. However, after I used the freeze.py script and generated the pb graph, I am unable to load it back. The code I used to load pb graph is:\r\n\r\n```\r\ndef load_graph(frozen_graph_filename):\r\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(\r\n            graph_def, \r\n            input_map=None, \r\n            return_elements=None, \r\n            name=\"imported\", \r\n            op_dict=None, \r\n            producer_op_list=None\r\n        )\r\n    return graph\r\ngraph = load_graph('./test_loading_graph_back.pb')\r\n```\r\n\r\nError:\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 285, in import_graph_def\r\n    raise ValueError('No op named %s in defined operations.' % node.op)\r\nValueError: No op named DecodeWav in defined operations.", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "-  I haven't written customized code\r\n- **OS Platform and Distribution**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: 1.3.0\r\n- **Python version**: 2.7 \r\n- **Bazel version (if compiling from source)**: 0.5.3\r\n- **CUDA:8.0 cuDNN:6.0**:\r\n- **GPU: GTX1080**:\r\n- **Exact command to reproduce**:\r\n\r\n1. built tensorflow from source\r\n\r\n2. run the tensorflow/examples/speech_commands with the default setting to train CNN model;\r\n\r\n3. use the freeze.py script inside the speech_commands dir to obtain frozen_graph.pb\r\n\r\n4. Load the graph back by using the following code:\r\n`def load_graph(frozen_graph_filename):\r\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(\r\n            graph_def, \r\n            input_map=None, \r\n            return_elements=None, \r\n            name=\"imported\", \r\n            op_dict=None, \r\n            producer_op_list=None\r\n        )\r\n    return graph\r\ngraph = load_graph('./test_loading_graph_back.pb')`\r\n\r\nError:\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 285, in import_graph_def\r\nraise ValueError('No op named %s in defined operations.' % node.op)\r\nValueError: No op named DecodeWav in defined operations.\r\n\r\n\r\n", "Gosh is there any chance you're doing something related to mobile? Like Android? Also is it possible that this graph was saved using a different version of TensorFlow? It's just strange because the DecodeWav op is in core.", "I am running into the same issue on 1.4.0-rc0 (compiled from source, bazel v0.7.0). At first I thought it was trying to load the model on a Raspberry Pi that caused this, but it looks like I cannot load the frozen model even on the very workstation it was created on. Any tips on how to deal with this? ", "Is there any chance you could provide a minimal code, with commands, to create the .pb file and load it back, which reproduces the error?\r\n\r\nIt makes sense that a kernel might be missing on an embedded or mobile device. But it seems so strange that a kernel like DecodeWav would be missing on a normal system while using Bazel.", "Sorry for the late reply. I tried again with a freshly compiled TF 1.4, and whatever my problem was, it seems to exist no more. Just a heads up if anybody encounters something similar.\r\n\r\nFor the record, I was using the standard speech commands example, and freezing my models using the included freeze_model script. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "it is resolved in TF 1.4. Thank you!", "I had the same issue on TF 1.4 with the speech_commands example. It turns out that I had absentmindedly removed this from the top of `label_wav.py`, thinking that it was unused:\r\n\r\n```\r\n# pylint: disable=unused-import\r\nfrom tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\r\n# pylint: enable=unused-import\r\n```\r\n\r\nIt turns out having `contrib_audio.decode_wav` in scope is essential to loading the graph.\r\n\r\nI realize this isn't exactly the same as the original issue, but perhaps others who are modifying `label_wav.py` or writing a custom loader will, as I did, find this through google."]}, {"number": 12656, "title": "DNN Classifier producing Blue Screen of Death every time", "body": "Hi,  \r\n\r\nWhen running inputs 9 10 and 11 from the below notebook, I get a blue screen of death every time?  \r\n\r\nhttps://github.com/ageron/handson-ml/blob/master/10_introduction_to_artificial_neural_networks.ipynb\r\n\r\nI'm using: \r\n\r\nPython ver 3.5.3\r\nTensorflow 1.2.1\r\nRunning on a windows 7 pro machine\r\n\r\nAny tips on how to resolve this?\r\n\r\n", "comments": ["Would you please fill out the issue template including details of GPU and CUDA drivers, etc.?", "Windows 7 Pro 64 bit (6.1, build 7601)\r\nTensorflow installed via anaconda\r\nTensorflow version 1.2.1\r\nPython version 3.5.3\r\nAs for Cuda etc this PC was set up some time ago there is a GPU installed is that sufficient or should I install CUDA/cuDNN separately? I don't think I'm currently using the GPU.\r\nGPU: AMD radeon 6900 series 4095MB\r\n\r\nThe problem arises when the last line of block 11 is run,\r\n\r\nhttps://github.com/ageron/handson-ml/blob/master/10_introduction_to_artificial_neural_networks.ipynb\r\n\r\nIt also arises when block 11 of this notebook is run:\r\n\r\nhttps://github.com/ageron/handson-ml/blob/master/10_introduction_to_artificial_neural_networks.ipynb\r\n\r\nIt does not crash immediately it gets to a test accuracy of 92% (so quite close to the end) before crashing and given a blue screen of death.\r\n\r\nAnything else you need just let me know.", "@mrry can you comment on the most helpful additional debugging information to collect to make progress on this?", "I don't think TensorFlow does anything that could cause a BSOD. Are you using the CPU version of TensorFlow? (I'd assume so because you don't have a CUDA-capable GPU.)", "Yes it is the CPU version. All I can say is that the BSOD are produced in the same place every time it does seem to be a bit of a coincidence? I guess it could be hardware overheating but I don't think these practice problems are that challenging?", "I suspect that this is a problem with your hardware or operating system, and perhaps TensorFlow is aggravating it by allocating a large amount of memory. Since Windows 7 is now end-of-life, it's possible that there's a bug in the OS that won't be patched. I'd suggest upgrading to a new operating system to see if that helps the problem.\r\n\r\nUnfortunately that is not something we're able to diagnose, and so I'm going to close this issue.", "Re OS would you recommend switching to Linux?", "I have the same problem i try to reinstalling windows but it doesn\u2019t work when i installing tensorflow using anaconda navigator it shout down the pc and goes to the blue screen       "]}, {"number": 12655, "title": "Add a timeout for tests in cmake.", "body": "", "comments": ["Artifically added a very small timeout to see if this really works.", "Ping me if it works!"]}, {"number": 12654, "title": "Mkl filter optimization", "body": "This pull request optimizes the MKL convolution backpropagation by avoiding extra filter data conversion.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "Can one of the admins verify this patch?", "Hi,\r\n\r\nI have signed Google CLA (from Intel) for this contribution already, and have made several contributions to Tensorflow so far. Not sure why it is showing me that I don't have signed CLA.", "Most of the time it's because the email on your commits doesn't match the email on file (or the one on github). Sadly the github UI makes it really hard to see that email.\r\n\r\nI'll override CLA bot.", "thanks @martinwicke!", "Ping @rmlarsen @andydavis1 ", "@tfboyd @andydavis1 @rmlarsen Please merge this one first (before 12661)", "Jenkins, test this please."]}, {"number": 12653, "title": "Device string in eager API differs from main ops API", "body": "In the Python API device strings are normalized such that `/cpu:0` becomes `/device:CPU:0`. This happens in the `device.py` and more specifically, [here](https://github.com/tensorflow/tensorflow/blob/668db64a5d612d5f96b5d87772ce6ff6531fc035/tensorflow/python/framework/device.py#L192). However, the eager execution API throws an error if provided `/device:CPU:0` for the device. It only works if I set it to `CPU:0`. Why does that inconsistency exist? @asimshankar @alextp \r\n\r\nP.S. I try to tag people that I believe are relevant based on previous issues/discussions. Please let me know if that's annoying and would prefer me not tagging anyone. :)", "comments": ["Please continue tagging us on eager-related issues as that makes the triage\nperson's life easier :-)\n\nOn Mon, Aug 28, 2017 at 8:58 AM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> In the Python API device strings are normalized such that /cpu:0 becomes\n> /device:CPU:0. This happens in the device.py and more specifically, here\n> <https://github.com/tensorflow/tensorflow/blob/668db64a5d612d5f96b5d87772ce6ff6531fc035/tensorflow/python/framework/device.py#L192>.\n> However, the eager execution API throws an error if provided /device:CPU:0\n> for the device. It only works if I set it to CPU:0. Why does that\n> inconsistency exist? @asimshankar <https://github.com/asimshankar> @alextp\n> <https://github.com/alextp>\n>\n> P.S. I try to tag people that I believe are relevant based on previous\n> issues/discussions. Please let me know if that's annoying and would prefer\n> me not tagging anyone. :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12653>, or mute the\n> thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxeUCyNISQA34e28jKuhMOser-bhjks5scuOSgaJpZM4PEtpU>\n> .\n>\n\n\n\n-- \n - Alex\n", "Sounds good! :)", "There is no reason for this inconsistency to exist. However, I'm having trouble reproducing the problem. Could you provide a short Python snippet that reproduces the problem?", "@asimshankar You could try replacing [lines 172-173 in c_api_test.cc](https://github.com/tensorflow/tensorflow/blob/1a0a160cb9993b30abe1d1451dcaeb5d277a11f4/tensorflow/c/eager/c_api_test.cc#L172) with:\r\n```\r\nTFE_TensorHandle* hdevice =\r\n  TFE_TensorHandleCopyToDevice(hcpu, ctx, \"/device:CPU:0\", status.get());\r\n```\r\nThis should give you an error that says:\r\n```\r\nW tensorflow/core/common_runtime/device_mgr.cc:94] Unknown device: /device:CPU:0 all devices: CPU:0, /job:localhost/replica:0/task:0/cpu:0, /job:localhost/replica:0/task:0/device:CPU:0\r\n```", "Ah, hmm...I believe the C API currently expects only fully qualified names (`/job:J/worker:W/...`), so even `\"/cpu:0\"` will not work. Python converts strings to fully qualified names before calling `TFE_TensorHandleCopyToDevice`.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/context.py#L218\r\nand https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/context.py#L225\r\n\r\n", "Hmm that's interesting because in the case of constructing a graph and building symbolic ops (i.e., not eager), the output of [this function](https://github.com/tensorflow/tensorflow/blob/668db64a5d612d5f96b5d87772ce6ff6531fc035/tensorflow/python/framework/device.py#L192) works just fine, without needing to be fully qualified (e.g., with `self.job` being set to `None`).", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks! "]}, {"number": 12652, "title": "delete `get_started/monitors.md`", "body": "This is a copy of  https://github.com/tensorflow/tensorflow/pull/12651 cherrypicked onto master. \r\n\r\nThe monitors in this doc are not compatible with the core Estimators in the other get started docs.\r\n\r\nThis will likely only cause confusion. Since AFAIK monitors are deprecated I see no advantage to preserving this doc in api_guides.\r\n\r\nIf we delete it here I can regenerate the root version of the website without this file.\r\n\r\nI'll mirror these changes into master.", "comments": ["@MarkDaoust, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @dandelionmane and @caisq to be potential reviewers.", "Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 12651, "title": "R1.3 - Delete monitors.md", "body": "The monitors in this doc are not compatible with the core `Estimators` in the other get started docs.\r\n\r\nThis will likely only cause confusion. Since AFAIK monitors are deprecated I see no advantage to preserving this doc in `api_guides`.\r\n\r\nIf we delete it here I can regenerate the root version of the website without this file.\r\n\r\nI'll mirror these changes into master.", "comments": ["Can one of the admins verify this patch?", "I think this is why this [webpage](https://www.tensorflow.org/get_started/monitors) show 404 error?\r\nI just wrote a program that user \"ValidationMonitor \" to implement early stopping, even though this class is in contrib  but it still works fine, why delete the documentation? \r\n\r\n", "The class is deprecated. We would like to not encourage its use.", "@martinwicke \r\n I didn't see any sign or comment said that the class is deprecated(both master and r1.3 branch).  Is there some place that I can check which API is going to deprecated? \r\nAnother question, What's the new API to support early stopping?", "Whenever you instantiate a ValidationMonitor, a deprecation warning is\nlogged, with the text \u200b\n\n\"Monitors are deprecated. Please use tf.train.SessionRunHook.\"\n\nYou can use SessionRunHooks to achieve the same thing, although we have\nnone that does it. The reason for that is that this implementation has\nissues, and we want to provide a better one, which we're working on.\n"]}, {"number": 12650, "title": "TF build fails for simulator architectures on master", "body": "When building TF for iOS from `master` with\r\n\r\n    ./build_all_ios.sh\r\n\r\nThe build fails with\r\n\r\n```\r\nclang++ -x c++ -M -std=c++11 -DNSYNC_USE_CPP11_TIMEPOINT -DNSYNC_ATOMIC_CPP11 -I../../platform/c++11 -I../../platform/gcc_no_tls -I../../platform/macos -I../../platform/posix -pthread -I../../public -I../../internal ../../internal/*.c ../../testing/*.c ../../platform/posix/src/clock_gettime.c ../../platform/c++11/src/nsync_semaphore_mutex.cc ../../platform/posix/src/per_thread_waiter.c ../../platform/c++11/src/yield.cc ../../platform/c++11/src/time_rep_timespec.cc ../../platform/c++11/src/nsync_panic.cc \\\r\n\t\t  ../../platform/c++11/src/start_thread.cc > dependfile\r\nclang++ -DNSYNC_USE_CPP11_TIMEPOINT -DNSYNC_ATOMIC_CPP11 -I../../platform/c++11 -I../../platform/gcc_no_tls -I../../platform/macos -I../../platform/posix -pthread -I../../public -I../../internal -O -arch i386 -fno-exceptions -stdlib=libc++ -fembed-bitcode  -mios-simulator-version-min=8.0 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.0.sdk -fPIC -x c++ -std=c++11 -Werror -Wall -Wextra -pedantic -c ../../internal/common.c\r\nclang: error: no such sysroot directory: '/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.0.sdk' [-Werror,-Wmissing-sysroot]\r\nmake: *** [common.o] Error 1\r\n```\r\n\r\nFor device architectures the build uses `iPhoneOS10.3.sdk`, which is there and correct, but not for the sim architecture (see missing SDK). If I manually symlink the current `iPhoneSimulator.sdk` as `iPhoneSimulator10.0.sdk` I get another error\r\n\r\n```\r\nclang++ -DNSYNC_USE_CPP11_TIMEPOINT -DNSYNC_ATOMIC_CPP11 -I../../platform/c++11 -I../../platform/gcc_no_tls -I../../platform/macos -I../../platform/posix -pthread -I../../public -I../../internal -O -arch i386 -fno-exceptions -stdlib=libc++ -fembed-bitcode  -mios-simulator-version-min=8.0 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.0.sdk -fPIC -x c++ -std=c++11 -Werror -Wall -Wextra -pedantic -c ../../internal/common.c\r\nIn file included from ../../internal/common.c:18:\r\n../../platform/c++11/platform.h:19:10: fatal error: 'string.h' file not found\r\n#include <string.h>\r\n         ^\r\n1 error generated.\r\nmake: *** [common.o] Error 1\r\n```\r\n\r\nMacOS Sierra 10.12.6 (16G29)\r\nXcode Version 8.3.3 (8E3004b)\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin", "comments": ["Seems `PhoneSimulator10.0.sdk` is hardcoded in `compile_nsync.sh`", "Would be nice if there was a CI that runs mobile builds. Those seem to break quite frequently.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'll close this issue. Please reopen if this is still an issue. We do have some internal tests for mobile builds now, but it's hard to find a good option that we can publicly expose. \r\n\r\n@petewarden @angersson FYI."]}, {"number": 12649, "title": "Android buffer overflow exception when running only a certain model above a certain image resolution?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android Lollipop\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.2\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 4.5\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: Running app on android studio\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI have met with a very peculiar problem that seem to show that a certain model, Inception V3, which I got from the TF-slim library, seem to consume more memory than usual and cause a bufferoverflow problem like in here:\r\n\r\n```\r\n08-28 21:23:40.121 1094-1094/com.example.android.androidevaluateimagenet D/AndroidRuntime: Shutting down VM\r\n08-28 21:23:40.122 1094-1094/com.example.android.androidevaluateimagenet E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                                                           Process: com.example.android.androidevaluateimagenet, PID: 1094\r\n                                                                                           java.nio.BufferOverflowException\r\n                                                                                               at java.nio.FloatBuffer.put(FloatBuffer.java:444)\r\n                                                                                               at org.tensorflow.Tensor.writeTo(Tensor.java:390)\r\n                                                                                               at org.tensorflow.contrib.android.TensorFlowInferenceInterface.fetch(TensorFlowInferenceInterface.java:338)\r\n                                                                                               at org.tensorflow.contrib.android.TensorFlowInferenceInterface.fetch(TensorFlowInferenceInterface.java:301)\r\n                                                                                               at com.example.android.androidevaluateimagenet.TensorFlowImageClassifier.recognizeImage(TensorFlowImageClassifier.java:149)\r\n                                                                                               at com.example.android.androidevaluateimagenet.MainActivity.getInferenceTime(MainActivity.java:267)\r\n                                                                                               at com.example.android.androidevaluateimagenet.MainActivity$2.onClick(MainActivity.java:345)\r\n                                                                                               at android.view.View.performClick(View.java:4763)\r\n                                                                                               at android.view.View$PerformClick.run(View.java:19821)\r\n                                                                                               at android.os.Handler.handleCallback(Handler.java:739)\r\n                                                                                               at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                                                               at android.os.Looper.loop(Looper.java:135)\r\n                                                                                               at android.app.ActivityThread.main(ActivityThread.java:5272)\r\n                                                                                               at java.lang.reflect.Method.invoke(Native Method)\r\n                                                                                               at java.lang.reflect.Method.invoke(Method.java:372)\r\n                                                                                               at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:909)\r\n                                                                                               at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:704)\r\n```\r\n\r\nSpecifically, if you try an image resolution higher than 360x360 in the model, it crashes with the error. The interesting thing is that this doesn't happen for a significantly larger model, the Inception Resnet V2, although it supposedly consumes a lot more memory. With that larger inception resnet v2 model, I can use a resolution of over 400 with no issues. I have rebooted my device, switched it and run it on another identical device and another brand of device, but still there's this problem. I can't exactly locate the issue, but specifically here is what I have traced in the error:\r\n\r\n\r\nSpecific error stack trace:\r\n\r\n**TensorFlowImageClassifier.java:**\r\n\r\n        inferenceInterface.fetch(outputName, outputs);\r\n\r\n**TensorFlowInferenceInterace.java:**\r\n\r\n    public void fetch(String var1, float[] var2) {\r\n        this.fetch(var1, FloatBuffer.wrap(var2));\r\n    }\r\n\r\n**Tensor.java:**\r\n\r\n    public void writeTo(FloatBuffer var1) {\r\n        if(this.dtype != DataType.FLOAT) {\r\n            throw incompatibleBuffer(var1, this.dtype);\r\n        } else {\r\n            ByteBuffer var2 = this.buffer();\r\n            var1.put(var2.asFloatBuffer());\r\n        }\r\n    }\r\n\r\n**FloatBuffer.java:**\r\n\r\n    public FloatBuffer put(FloatBuffer src) {\r\n        if (src == this)\r\n            throw new IllegalArgumentException();\r\n        int n = src.remaining();\r\n        if (n > remaining())\r\n            throw new BufferOverflowException();\r\n        for (int i = 0; i < n; i++)\r\n            put(src.get());\r\n        return this;\r\n    }\r\n\r\nI am not sure why this happens, as everything else works. In fact, if I try a resolution below 360, the Inception V3 model works perfectly fine. Note that I got the checkpoint model from TF-slim and froze it. I believe the method I used to freeze it works well, since there is no problem for all other models except Inception V3. So I can only conclude the problem lies within the layers. But I am not exactly sure how I can find out which layer is causing the problem, or even if it is because of the layers, I'm not sure how to fix it. I have included the layers of Inception V3 and Inception Resnet V2 in order here:\r\n\r\nInception V3 Layers: https://gist.github.com/kwotsin/82016b003057cdcffec1bc9d1ea1e02b\r\nInception Resnet V2 Layers: https://gist.github.com/kwotsin/893e11fe171af426091b89645d6a86d3\r\n\r\nIf it is really the problem within the model, then I thought it could be because of a faulty implementation of a certain operation that is causing overly huge memory consumed.\r\n\r\nAs an alternative fix, is there a way to check and raise the limit of the buffer size for the app to run successfully?", "comments": ["@andrewharp `BufferOverflowException` happens in `TensorFlowImageClassifier` with image resolution \u2265360.", "@kwotsin: I'm not aware that Inception v3 is even supposed to work at an input resolution other than 299x299 -- do you have a reason to believe that it should? I'm not 100% certain it won't either, so just checking. It could be possible that it merely seems to work up to a point, but really the image internally has striding errors or something until finally you run into a mismatched allocation issue. In any case I doubt you'll get much better performance than the resolution it was trained at, and will most likely just be needlessly be wasting CPU time.\r\n\r\nThe error seems to be due to mismatched buffer sizes, not necessarily OOM or anything. I'd put a printout in Tensor.writeTo() to figure out how var1 and var2 relate to each other size-wize as you change the input size.", "@andrewharp I have tried Inception V3 at varying resolutions that is not 299x299 on my workstation, and it could still work. I think what has to be changed is just the first input node shape and some modification to its auxiliary operations. This is also true for the android implementation, and I've tried till 360x360, beyond which it gives a buffer overflow error. As for the image data, I think generally all the images have around the same range of pixel height and width, and I don't think there are any outlier images (using the imagenet data). In fact, because I feed my image sequentially in order, without shuffling, I've checked the first image doesn't have weird sizes. Also, the model fails at the first image already so it didn't stop halfway while running.\r\n\r\nI've also tried for a significantly larger model, which took around 2x more memory but could still work at higher resolutions - this is the part I'm unsure about. From my experience, increasing the image resolution does improve the results for some models, just as decreasing the resolution would worsen the performance. \r\n\r\nActually I'd like to print the sizes of var1 and var2 at `TensorFlowInferenceInterface.class` like you mentioned, but it's compiled and so not accessible directly. Is there a direct way to get the sizes of var1 and var or increase the buffer size?\r\n\r\nBefore the `inferenceInterface.fetch(outputName, outputs);` call happens, the values of the arguments are:\r\n\r\n1. **outputName:** InceptionV3/Predictions/Softmax\r\n2. **outputs:** [F@c9bf470\r\n\r\nwhich are output node name and output float array (which is empty at first).\r\n\r\n\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm going to take this issue out of triage. Ideally what I'd like to see happen is someone who was involved in creating that model come along and let us know that configuring shapes in this manner is a must-have requirement, which would make this bug.\r\n\r\nI'll also note I think it's a little peculiar that fails with buffer overflow. But it seems like Java is doing a good job keeping us safe here. What would scare me is if this was happening in the unsafe C++ code that lurks beneath the Java. Furnish evidence of that and this becomes a much different story.", "I am using FasterRcnn, I've solved same issue by changing value of MAX_RESULTS from 100 to 500 in TensorFlowObjectDetectionAPIModel.java (TensorFlow android example app).", "@UsamaIslam Good trick!\r\n", "> I am using FasterRcnn, I've solved same issue by changing value of MAX_RESULTS from 100 to 500 in TensorFlowObjectDetectionAPIModel.java (TensorFlow android example app).\r\n\r\nthank you, I'm also trying to use FasterRcnn and it finally worked!"]}, {"number": 12648, "title": "Variable in Dataset map function is not stably initialized", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:macos\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.3\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:0.5.3-homebrew\r\n- **CUDA/cuDNN version**:NO\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nunable to init variable in map, flatmap, fillter function of Dataset API\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as  tf\r\nfrom  tensorflow.contrib.data import Dataset\r\n\r\ndataset = Dataset.range(100)\r\n\r\ndef mapf(v):\r\n    temp = tf.Variable(\r\n            tf.random_uniform(\r\n                    [1],\r\n                    minval=0,\r\n                    maxval=10,\r\n                    dtype=tf.int32,\r\n            ),\r\n            trainable=False,\r\n            collections=[tf.GraphKeys.LOCAL_VARIABLES],\r\n            name=\"my_var\"\r\n    )\r\n\r\n    with tf.control_dependencies([tf.variables_initializer([temp])]):\r\n        temp = temp + 1\r\n\r\n    return temp\r\n\r\n\r\ndataset = dataset.map(mapf)\r\ndataset = dataset.batch(1)\r\n\r\niterator = dataset.make_one_shot_iterator()\r\n\r\nnext_element = iterator.get_next()\r\nlocal_init = tf.local_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(local_init)\r\n    try:\r\n        while True:\r\n            print(sess.run([next_element]))\r\n    except tf.errors.OutOfRangeError as e:\r\n        print(\"ending\")\r\n```\r\noutput is not stable:\r\n```\r\n2017-08-28 18:09:31.817313: W tensorflow/core/framework/op_kernel.cc:1192] Failed precondition: Attempting to use uninitialized value my_var\r\n         [[Node: my_var/read = Identity[T=DT_INT32, _class=[\"loc:@my_var\"]](my_var)]]\r\nTraceback (most recent call last):\r\n  File \"/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1306, in _run_fn\r\n    status, run_metadata)\r\n  File \"/Users/liyanan/anaconda3/lib/python3.6/contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value my_var\r\n         [[Node: my_var/read = Identity[T=DT_INT32, _class=[\"loc:@my_var\"]](my_var)]]\r\n         [[Node: IteratorGetNext_2 = IteratorGetNext[output_shapes=[[?,1]], output_types=[DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](OneShotIterator)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"dataset.py\", line 90, in <module>\r\n    print(sess.run([next_element]))\r\n  File \"/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value my_var\r\n         [[Node: my_var/read = Identity[T=DT_INT32, _class=[\"loc:@my_var\"]](my_var)]]\r\n         [[Node: IteratorGetNext_2 = IteratorGetNext[output_shapes=[[?,1]], output_types=[DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](OneShotIterator)]]\r\n\r\n\r\n```\r\n\r\nOR \r\n\r\n```\r\n[array([[2]], dtype=int32)]\r\n[array([[9]], dtype=int32)]\r\n[array([[1]], dtype=int32)]\r\n[array([[5]], dtype=int32)]\r\n[array([[9]], dtype=int32)]\r\n[array([[9]], dtype=int32)]\r\n.....\r\nending\r\n```", "comments": ["Using `tf.Variable` in a TensorFlow function is not supported. To make this work, you'll need to use `tf.get_variable(..., use_resource=True)` *before* defining the `mapf()` function to declare the variable in the outer scope, then simply capture the variable in your `mapf()` function. Since you're defining a stateful input pipeline, you'll also need to use an *initializable* iterator rather than a one-shot iterator.\r\n\r\nThe following code illustrates how to do this:\r\n\r\n```python\r\nimport tensorflow as  tf\r\nfrom  tensorflow.contrib.data import Dataset\r\n\r\ndataset = Dataset.range(100)\r\n\r\ntemp = tf.get_variable(\r\n    \"my_var\",\r\n    initializer=tf.random_uniform([1], minval=0, maxval=10, dtype=tf.int32),\r\n    trainable=False,\r\n    collections=[tf.GraphKeys.LOCAL_VARIABLES],\r\n    use_resource=True)\r\n\r\ndef mapf(v):\r\n  # NOTE(mrry): I assume you didn't really intend to reinitialize the\r\n  # variable in each call to `mapf()`, and were instead trying to\r\n  # increment it with the `temp = temp + 1` in the original code.\r\n  with tf.control_dependencies([temp.assign_add([1]).op]):\r\n    return temp\r\n\r\ndataset = dataset.map(mapf)\r\ndataset = dataset.batch(1)\r\n\r\niterator = dataset.make_initializable_iterator()\r\nnext_element = iterator.get_next()\r\nlocal_init = tf.local_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(local_init)\r\n  sess.run(iterator.initializer)\r\n  try:\r\n    while True:\r\n      print(sess.run([next_element]))\r\n  except tf.errors.OutOfRangeError as e:\r\n    print(\"ending\")\r\n```", "@mrry tks for help, it bother me for a week, may be we should add this to the doc"]}, {"number": 12647, "title": "tensorflow1.3 bazel-bin/tensorflow/examples/label_image/label_image failed to run.", "body": "step 1:python tensorflow/examples/image_retraining/retrain.py    --architecture=inception_v3    --output_graph=inception_v3.pb  ....  \r\nstep2: bazel build tensorflow/examples/label_image\r\nstep3: bazel-bin/tensorflow/examples/label_image/label_image\r\nthen get the following error:\r\ntensorflow/examples/label_image/main.cc:349] Running model failed: Not found: FeedInputs: unable to find feed output input.\r\nAny step wrong? Could you give me some hint?", "comments": ["What were the exact command line arguments you performed?\r\n\r\nThe [label_image code expects](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc#L281) the graph model to be named specifically `inception_v3_2016_08_28_frozen.pb` and [the labels file to be named](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc#L283) `imagenet_slim_labels.txt` unless you specify differently via command line flags.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12646, "title": "building tensorflow from source / general issue", "body": "Dear Tensorflow team,\r\n\r\nI was facing a problem described here:\r\nhttps://stackoverflow.com/questions/45859371/error-building-tensorflow-on-centos-7\r\n\r\nIn short words: In the documentation there is no any single word about bazel version need for specific version of Tensorflow. In particular, I could not build a Python package with the newest version of Bazel. Experimentally I found out that that the master version (r1.3) can be built successfully with Bazel not newer than 0.5.1. (with very old versions it also doesn't work). \r\nSo I am kindly asking to mention a Bazel version (as well as other dependencies, such as gcc, etc) to have successful build for the next releases.\r\n\r\nI spent a lot of time to find a solution. Probably it will be useful experience for others.\r\n\r\nCheers,\r\nSergey", "comments": ["gunan@ is our bazel version check not working again? \r\n\r\n@srvoin If the check is working properly, it should tell you something like \"you need at least bazel X.X.X\".\r\n\r\nI think otherwise this is an unfortunate one-off, since the 0.5.3 bazel release was broken.\r\n\r\nLeaving open only to check whether our check works.", "@yifeif are you aware of any issues with bazel version check?", "configure checks bazel against a minimum version of 0.4.5.\r\n\r\nafaik Bazel 0.5.3 issue was resolved with #11871. I haven't seen the swig error mentioned in the SO link.\r\n\r\n", "@martinwicke as @yifeif said it were checking against a minimum version and nothing about maximum. In my case it didn't work with any version higher than 0.5.1 (or 0.5.2)\r\nIt looks like there are some differences in environment settings within bazel scope... could be this a problem: \r\n```\r\nRelease 0.5.3\r\nIncompatible changes:\r\n\r\ndeprecate --python2_path and --python3_path flags for the\r\nnew --python_path flag; introduce --python_top flag for using\r\nthe new py_runtime rule.\r\n....\r\n```\r\nI can't reproduce test with version 0.5.2 at the moment, but 0.5.3 and 0.5.4 don't work for python package generation.", "Maybe we should fix the bazel version for releases, since it's breaking relatively quickly. \r\n\r\nThis should be fixed at head though. @srvoin can you verify that building head with the latest bazel works for you?", "@srvoin we added a check to switch to --python_path for bazel >=0.5.3 at head, but it was not in 1.3 branch.\r\nhttps://github.com/tensorflow/tensorflow/commit/6252d296040e8a2ff0c6c8668db55abe3e6f1416#diff-ade1d3e4b7c35655f854151d899df62b \r\n\r\nNot sure if the python_path change is the cause of the issue you are seeing, but might be good to try the configure from master.\r\n\r\n", "I am able to build clean the pip wheel target from head of master (as of 9.30a (pacific) today) via `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` - no non-default values in the build configuration setup.\r\n\r\nI'm using 0.5.3 Bazel:\r\n```\r\nautogenic:tensorflow loki$ bazel version\r\nBuild label: 0.5.3-homebrew\r\nBuild target: bazel-out/darwin_x86_64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Sun Aug 6 12:54:10 2017 (1502024050)\r\nBuild timestamp: 1502024050\r\nBuild timestamp as int: 1502024050\r\nautogenic:tensorflow loki$\r\n", "Thanks @quaeler. I will close this issue, since it will be resolved in 1.4 then."]}, {"number": 12645, "title": "y can be a dict for numpy_input_fn", "body": "The PR is opened for #12610.\r\n\r\n### What changes were proposed in this pull request?\r\n\r\ndict is accept for `y` as labels.\r\n\r\n### How was this patch tested?\r\n\r\n+ [x] add unit test\r\n+ [ ] pass all tests.\r\n\r\n", "comments": ["@facaiy, thanks for your PR! By analyzing the history of the files in this pull request, we identified @xiejw and @tensorflower-gardener to be potential reviewers.", "Can one of the admins verify this patch?", "Marking as WIP until you have resolved test failures (it's not checked off the list yet, I assume it's not done).", "Jenkins, test this please.", "Never mind, I'm running the tests.", "Hi, test runs timeout. I am not sure whether the error is introduced by the PR.\r\n\r\n```python\r\n//tensorflow/python/kernel_tests:matrix_solve_ls_op_test                TIMEOUT in 65.0s\r\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/tensorflow/python/kernel_tests/matrix_solve_ls_op_test/test.log\r\n\r\nExecuted 1205 out of 1205 tests: 1204 tests pass and 1 fails locally.\r\n```", "Hi, could you retest the PR? I cannot reproduce the failure.\r\n\r\n```bash\r\nTarget //tensorflow/python/kernel_tests:matrix_solve_ls_op_test up-to-date:\r\n  bazel-bin/tensorflow/python/kernel_tests/matrix_solve_ls_op_test\r\nINFO: Elapsed time: 402.251s, Critical Path: 206.79s\r\n//tensorflow/python/kernel_tests:matrix_solve_ls_op_test                 PASSED in 2.1s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\n```", "Sure. Jenkins, test this please.", "@xiejw feel free to wait until it passes or review now.", "I cannot reproduce the failure on mac test:\r\n\r\n```bash\r\n(py27) ~/W/g/tensorflow \u276f\u276f\u276f bazel test -c opt //tensorflow/python:basic_session_run_hooks_test\r\nWARNING: /Users/facai/Workshop/github/tensorflow/tensorflow/core/BUILD:1634:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /Users/facai/Workshop/github/tensorflow/tensorflow/tensorflow.bzl:911:30.\r\nINFO: Found 1 test target...\r\nTarget //tensorflow/python:basic_session_run_hooks_test up-to-date:\r\n  bazel-bin/tensorflow/python/basic_session_run_hooks_test\r\nINFO: Elapsed time: 19.859s, Critical Path: 17.47s\r\n//tensorflow/python:basic_session_run_hooks_test                         PASSED in 17.5s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\n```\r\n\r\nsystem info:\r\n```bash\r\n(py27) ~/W/g/tensorflow \u276f\u276f\u276f uname -a                                    ENH/numpy_input_fn_support_dict \u2731\r\nDarwin bj-m-203050a.local 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64\r\n\r\n(py27) ~/W/g/tensorflow \u276f\u276f\u276f python --version                            ENH/numpy_input_fn_support_dict \u2731\r\nPython 2.7.13 :: Anaconda 4.3.1 (x86_64)\r\n```", "This test failure is unrelated (and flaky, I believe).", "Hi, I revise code and explain the invalid cases as below:\r\n\r\n+ `x = {}`, raise a ValueError\r\n+ `y = {}`, raise a ValueError, and suggest user to use `None` instead.\r\n+ `y = None`, leave a todo comment, but don't change the behavior now for compatibility.\r\n\r\nBy the way, more tests have been added to cover those cases above.\r\n", "Moreover, `shuffle` seems an optional argument (default value is `None`), but it requires a boolean value. \r\n\r\n```python\r\ndef numpy_input_fn(x,\r\n                    y=None,\r\n                    batch_size=128,\r\n                    num_epochs=1,\r\n                    shuffle=None,\r\n                    queue_capacity=1000,\r\n                    num_threads=1):\r\n```\r\n\r\nReally odd, if `shuffle` is required, why to give it a default value?  If it is optional, why to give it an invalid value by default?", "Hi, quite a long time, might we test the PR? Thanks.", "Is this going to be merged?", "@facaiy did you address @martinwicke comments?", "I think so, \r\n\r\n> Hi, I revise code and explain the invalid cases as below:\r\n> x = {}, raise a ValueError\r\n> y = {}, raise a ValueError, and suggest user to use None instead.\r\n> y = None, leave a todo comment, but don't change the behavior now for compatibility.\r\n> By the way, more tests have been added to cover those cases above.", "Hi, @martinwicke @xiejw . What do you think about the behavior changed ? Thanks.", "Jenkins, test this please."]}, {"number": 12644, "title": "compile tensorlfow c++ object class as /clr project to make the dll  fail !!", "body": "------------------------\r\n\r\n### System information\r\n- **windows10**:\r\n- **install tensorflow**: from source by vs2015 \r\n- **TensorFlow version**: r1.2\r\n-**CMake**:  3.8.2 build\r\n\r\n### Error List\r\n1. \r\n>#error instruction:  <condition_variable> is not supported when compiling with /clr or /clr:pure.\tCppWrapper\td:\\Developer\\Microsoft Visual Studio 14.0\\VC\\include\\condition_variable\t17\t\r\n2. \r\n>#error instruction:  <mutex> is not supported when compiling with /clr or /clr:pure.\tCppWrapper\td:\\Developer\\Microsoft Visual Studio 14.0\\VC\\include\\mutex\t8\t\r\n3.\r\n> #error instruction:  <thread> is not supported when compiling with /clr or /clr:pure.\tCppWrapper\td:\\Developer\\Microsoft Visual Studio 14.0\\VC\\include\\thread\t8\t\r\n4. \r\n >e:\\tensor\\tensorflow\\tensorflow\\contrib\\cmake\\build\\external\\eigen_archive\\eigen\\src/Core/ArithmeticSequence.h(205): fatal error C1001: An internal error has occurred in the compiler.\r\n", "comments": ["The TensorFlow codebase is not compatible with the `/clr` flag when building with Visual C++. The MSDN documentation outlines [some of the restrictions](https://msdn.microsoft.com/en-us/library/ffkc918h(v=vs.90).aspx) when you compile using that flag.\r\n\r\nIf there is a compelling reason to build TensorFlow with this flag, and the code changes to make this possible are not too invasive, we might consider accepting a contribution to do so.  Therefore I'll mark this as \"Contributions Welcome\" for now, and we'll keep the issue open if you can give us some more details about why you want to do this.\r\n\r\n(For accessing the TensorFlow API from a managed language, check out [TensorFlowSharp](https://github.com/migueldeicaza/TensorFlowSharp) by @migueldeicaza. As far as I can tell, it uses P/Invoke to invoke TensorFlow API methods that are implemented in an unmanaged DLL.)", "I have build an console exe with tensorflow and opencv libraries for detecting and recognizing human. However, my team built a C# GUI system. They hope me to build the /clr project to make a DLL for their C# project to import and finish human face recognition. But we even don't know tensorflow can't compile with /clr flag. So this is really frustrating. \r\nI right now don't have some ideas about compiling the DLL, but I must do that. Maybe take a long time to find the correct trick or make some changes to Tensorflow source files.", "I'd suggest building your console exe as a native DLL instead, and using P/Invoke to call into the DLL from the C# GUI.", "@mrry suggestion would work"]}]