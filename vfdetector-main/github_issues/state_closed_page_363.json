[{"number": 43169, "title": "DLL load failed: A dynamic link library (DLL) initialization routine failed.", "body": "**System information**\r\nI am using Tensorflow on Windows 10 2004\r\nTensorflow installed from PIP (I don't know if that's what you are asking for, sorry)\r\nTensorflow version is 1.15\r\nPython version is 3.7 (64-bit)\r\n\r\n**Describe the problem**\r\nIt says _\"DLL load failed: A dynamic link library (DLL) initialization routine failed.\"_\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI spent many hours installing necessary packages for [this repository](https://github.com/CorentinJ/Real-Time-Voice-Cloning), plus any extra packages that it asked for.\r\n\r\n**Any other info / logs**\r\n> Traceback (most recent call last):\r\n>   File \"E:\\Python\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"E:\\Python\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"E:\\Python\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"E:\\Python\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"E:\\Python\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"E:\\a\\b\\Real-Time-Voice-Cloning-master\\demo_cli.py\", line 4, in <module>\r\n>     from synthesizer.inference import Synthesizer\r\n>   File \"E:\\a\\b\\Real-Time-Voice-Cloning-master\\synthesizer\\inference.py\", line 1, in <module>\r\n>     from synthesizer.tacotron2 import Tacotron2\r\n>   File \"E:\\a\\b\\Real-Time-Voice-Cloning-master\\synthesizer\\tacotron2.py\", line 3, in <module>\r\n>     from synthesizer.models import create_model\r\n>   File \"E:\\a\\b\\Real-Time-Voice-Cloning-master\\synthesizer\\models\\__init__.py\", line 1, in <module>\r\n>     from .tacotron import Tacotron\r\n>   File \"E:\\a\\b\\Real-Time-Voice-Cloning-master\\synthesizer\\models\\tacotron.py\", line 1, in <module>\r\n>     import tensorflow as tf\r\n>   File \"E:\\Python\\lib\\site-packages\\tensorflow\\__init__.py\", line 99, in <module>\r\n>     from tensorflow_core import *\r\n>   File \"E:\\Python\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 28, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n>   File \"E:\\Python\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"E:\\Python\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"E:\\Python\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"E:\\Python\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"E:\\Python\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"E:\\Python\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"E:\\Python\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"E:\\Python\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"E:\\Python\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"E:\\Python\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.", "comments": ["@ITAC85v2,\r\nYou might be facing this issue because of the following reasons\r\n\r\n- You are running 32-bit Python or 32-bit OS\r\n- You have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) package\r\n- Your CPU does not support AVX instructions. \r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204.\r\n\r\nThanks!", "Do you possibly know away to install Microsoft Visual C++ Redistributable to a removable drive instead of a local hard drive? I've tried to create a symlink but it did not work. Do you have any ideas?\r\n\r\nAlso, I am using Windows 10 2004 x64 with Python 3.7 (64-bit).", "I have done the same steps on a different computer and with Microsoft VC++ and I have received this:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"demo_cli.py\", line 4, in <module>\r\n>     from synthesizer.inference import Synthesizer\r\n>   File \"G:\\a\\b\\Real-Time-Voice-Cloning-master\\synthesizer\\inference.py\", line 1, in <module>\r\n>     from synthesizer.tacotron2 import Tacotron2\r\n>   File \"G:\\a\\b\\Real-Time-Voice-Cloning-master\\synthesizer\\tacotron2.py\", line 3, in <module>\r\n>     from synthesizer.models import create_model\r\n>   File \"G:\\a\\b\\Real-Time-Voice-Cloning-master\\synthesizer\\models\\__init__.py\", line 1, in <module>\r\n>     from .tacotron import Tacotron\r\n>   File \"G:\\a\\b\\Real-Time-Voice-Cloning-master\\synthesizer\\models\\tacotron.py\", line 1, in <module>\r\n>     import tensorflow as tf\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 99, in <module>\r\n>     from tensorflow_core import *\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 28, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\Betty Faye\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n\r\nI do not know if this is the same or not as I did not bother checking.", "> I have done the same steps on a different computer and with Microsoft VC++ and I have received this:\r\n\r\n@ITAC85v2,\r\nCould you please provide the make and model of the CPU on this machine? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43169\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43169\">No</a>\n"]}, {"number": 43168, "title": "Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary: `libtensorflow_jni-cpu-windows-x86_64-1.14.0/tensorflow_jni.dll`\r\n- TensorFlow version: 1.14.0\r\n\r\n**Describe the problem**\r\n2020-09-12 01:46:00.374893: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nModel load took 21ms, TensorFlow version: 1.14.0\r\nSuccessfully loaded model from the input stream\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Go to: https://www.tensorflow.org/install/lang_java and download libtensorflow.jar and Java Native Interface (JNI) file, save it to project root;\r\n2. Load tensorflow through `System.load (\"./tensorflow_jni.dll\")`\r\n", "comments": ["It is ok. It is just an info that the default binary it is not compiled with AVX2 but you have an AVX2 CPU available.", "See also https://github.com/tensorflow/tensorflow/issues/15450#issuecomment-352882680", "Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43168\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43168\">No</a>\n", "Hello,\nI am using tensorflow for deepsignal and I have the same problem and that is: \"Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\".\nI installed tensorflow in a virtual environment along with tombo and python3 as they themselves suggest. The tensorflow version I have is 1.8.0.\nHow can I solve this problem ?", "https://github.com/tensorflow/tensorflow/issues/43168#issuecomment-691523272."]}, {"number": 43167, "title": "[INTEL MKL] Added missed  bfloat16 CPU support for op math.rsqrt", "body": "", "comments": ["This PR has been merged, and I can see the merge information in the log, but the files doesn't have the changes in this PR, was there a merge issue? please check the file in the master:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_rsqrt.cc\r\nwhich doesn't have the changes from this PR, but in the history of this file, I can see there is merge record.\r\n", "@cuixiaom It got rolled back because it broke some of our internal tests that cannot be easily fixed. I'll try to bring it back but it might take some time. Sorry about that!", "@penpornk  thanks for your response! Please let me know if need my help."]}, {"number": 43166, "title": "Support 5D tensors in the binary ops of Grappler Layout Optimizer", "body": "This PR enables the grappler layout optimizer to handle the binary ops when inputs are 5D tensors. Besides, it also enables this feature for the ops in the default layout agnostic list, i.e., the fanin and fanout number is both 1.\r\n\r\nThis can help deal with 3D conv networks, for example, conv3d + nn.batch_normalization (which contains a set of binary/unary prim itives).\r\n\r\n\r\ncc @nluehr ", "comments": ["It looks like this was rolled back in https://github.com/tensorflow/tensorflow/commit/bc3cbad8d8b8d3a8df73beb0686cca9dad62c7fa due to a GPU test failing in sonnet (https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conformance/xla_test.py#L61). I am current investigating what error it is but from the logs I see there are some incompatible shaped inputs going into an Add op.", "After some debugging the error is due to https://github.com/tensorflow/tensorflow/blob/6fd903f770302eaef6ceff63e0e3487c47fb2f13/tensorflow/core/grappler/optimizers/generic_layout_optimizer_transposer.cc#L1134-L1135 not being updated to handle rank 5 tensors. Can you update that function and send out another PR to reland your change?"]}, {"number": 43165, "title": "SyncBatchNormalization has NaN losses with channels-first format", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04 running in Docker container (host is 18.04)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0 and 2.3.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.29.1-1.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Various, e.g. Nvidia TITAN X\r\n\r\n**Describe the current behavior**\r\nWhen using the experimental SyncBatchNormalization layer with the channels-first (NCHW or batch-channel-height-width) format, the output of the layer is NaN.\r\n\r\n**Describe the expected behavior**\r\nThe output of the layer should be valid, as occurs with the default channels-last format, or with regular BatchNormalization.\r\n\r\n**Standalone code to reproduce the issue**\r\nThis [Colab notebook](https://colab.research.google.com/drive/1zeLVlrWwohsmGJnEOQaFsb_gzH6xOFo1?usp=sharing) contains several models that demonstrate the issue. A Keras model with NCHW format, and a Keras model with SyncBatchNorm, both train correctly. However, a Keras model with _both_ NCHW format and SyncBatchNorm immediately fails with NaN loss. Similarly, an Estimator model with NCHW and SyncBatchNorm fails with NaN loss.\r\n\r\n**Other info / logs**\r\nI think the input tensor size seems to be related to the error. I tried a smaller tensor (10x10x10) before and it didn't cause the issue. And a medium-size tensor (I think 30x30x30) trained for a few steps and then encountered this issue. I'd have to double-check on the current code, but I think there's something there.\r\n\r\nIn the example, the loss is NaN. I believe that the output of the layer is already NaN (I'm not sure if every single value is NaN or just some of the values). I had some tests where I printed out the sum of the layer outputs etc., and if I recall correctly, immediately after the first SyncBatchNorm layer they were already NaN. This happened on the second and later iterations, but not the first iteration, so I suspect there may be some issue with backpropagation rather than with the forward step. For example, hypothetically if it happened that backpropagation in the first step caused the moving variance to go to 0 (or infinity), I think we would see behavior like this.\r\n\r\nYou should be able to replicate all these tests fairly easily with some simple print functions etc., but if you need any more information please let me know and I can provide it.\r\n\r\nThe loss function doesn't seem to matter - I can use various loss functions and still trigger the NaN - but the Conv2D at the beginning may matter; it seems like it works fine if I remove that layer or if I change it to channels-last.\r\n\r\nIf I run on Colab without a GPU, I get the following error:\r\n\r\n```\r\nInvalidArgumentError:  Conv2DCustomBackpropFilterOp only supports NHWC.\r\n\t [[node gradient_tape/sequential/conv2d/Conv2D/Conv2DBackpropFilter (defined at <ipython-input-3-11fc99b50ff4>:18) ]] [Op:__inference_train_function_654]\r\n```\r\n\r\nI wonder if this might be related to the problem?", "comments": ["I have tried in colab with TF version 2.3 and was able to reproduce the issue. Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/baf51cdacf5192432098f321e1e3bfb2/untitled341.ipynb) Thanks!", "@yhliang2018 Have you been able to take a look at this?", "Add @anj-s to look into the problem, as she knows more about SyncBatchNormalization layer.", "@anj-s Have you had a chance to take a look at this?", "Was able to reproduce your issue in Tensorflow-gpu 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/eed7bcac822d0d53daea74d517eca509/43165.ipynb). Thanks!", "@MinasTyuru, \r\n\r\nClosing this issue as it is fixed in latest version of TensorFlow. Please refer attached [gist](https://colab.research.google.com/gist/chunduriv/4f59b004ccd37d5e8e55fcffa7d36703/43165.ipynb) and feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43165\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43165\">No</a>\n"]}, {"number": 43164, "title": "Model produces three different outcomes for the same input depending on how I measure it", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.6\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.7\r\n\r\n**Describe the current behavior**\r\nWhen I take the loss from the history object or the result from the evaluate function on the training data or from a numpy calculation of mean squared error I receive different results.\r\nThis is an output of my code below:\r\n```\r\n4.520166488224531 \r\n4.409448146820068 \r\n4.139582633972168\r\n```\r\n\r\n**Describe the expected behavior**\r\nI would receive the same results for all different evaluation calculations.\r\nLike:\r\n```\r\n4.520166488224531 \r\n4.520166488224531 \r\n4.520166488224531 \r\n```\r\n**Standalone code to reproduce the issue**\r\n\r\nYou can find a colab notebook with the issue here: \r\nhttps://colab.research.google.com/gist/ichitaka/c97ce36d57184ba154c8840802a8e01d\r\n\r\n```\r\nfrom tensorflow.keras.layers import Input, Dense, Activation\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.optimizers import RMSprop\r\nimport numpy as np\r\nx = np.random.normal(1, 2, 10)\r\ny = np.random.normal(1, 2, 10)\r\n\r\nmodel = Sequential()\r\nmodel.add(Input(1))\r\nmodel.add(Dense(10))\r\nmodel.add(Activation(\"sigmoid\"))\r\nmodel.add(Dense(1))\r\nmodel.summary()\r\n\r\n\r\nrmsprop = RMSprop(lr=0.01)\r\nmodel.compile(loss='mean_squared_error', optimizer=rmsprop)\r\nhist = model.fit(x=x,y=y)\r\n_ = model.evaluate(x, y, verbose=0)\r\npred = model.predict(x)\r\nprint(np.mean((y-pred)**2), '\\n', hist.history['loss'][0], '\\n', _)\r\n```", "comments": ["So seems like the numpy function had issues with the dimensionality. When I fix that, evaluate and the numpy function return the same. Also adding the training data as validation data returns also the same value as the other two. So it seems, like the loss in history to be a value before the last gradient is applied. So I would consider this to not be a bug, just unexpected behaviour.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43164\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43164\">No</a>\n"]}, {"number": 43163, "title": "Small cleanup to the stm32f4 makefile and CI scripts.", "body": " * Added running the tests with renode behind a flag to disable for CI but keep for direct invocations of test_stm32f4.sh\r\n * Do not excluding kernel tests that pass for stm32f4 (these currently do not have optimized implementations and fall-back to the reference kernels)\r\n * Explicitly excluded kernel tests that do fail (the cmsis-nn implementation will likely need some fixing).\r\n * Removed stale example exclusions\r\n\r\nManually tested that the following command succeeds (and runs all the tests with renode):\r\n```\r\ntensorflow/lite/micro/tools/ci_build/test_stm32f4.sh\r\n```", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@freddan80 @jenselofsson let me know what you think of this change.\r\n\r\nWe'll wait for at least one of you to respond before merging.", "@advaitjain Looks good"]}, {"number": 43162, "title": "custom xception input size broadcasting incorrectly", "body": "## System information:\r\nRed Hat Enterprise Linux Server release 7.7 (Maipo)\r\nTensorflow: tensorflow gpu 2.1\r\nGPU: Nvidia V100\r\nPython: 3.6.10\r\nGCC: 7.3.0\r\nCUDA: 10.1 (I think)\r\nRunning through slurm\r\n\r\n## The Error log:\r\n```\r\nValueError: could not broadcast input array from shape (850,550,3) into shape (850,550,3,3)\r\n```\r\n\r\n## model.summary()\r\nThis model is the stock xception with a custom top (global max, dense layer, and softmax) used for image classification. \r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\ninput_2 (InputLayer)            [(None, 850, 550, 3) 0\r\n__________________________________________________________________________________________________\r\nblock1_conv1 (Conv2D)           (None, 424, 274, 32) 864         input_2[0][0]\r\n__________________________________________________________________________________________________\r\nblock1_conv1_bn (BatchNormaliza (None, 424, 274, 32) 128         block1_conv1[0][0]\r\n__________________________________________________________________________________________________\r\nblock1_conv1_act (Activation)   (None, 424, 274, 32) 0           block1_conv1_bn[0][0]\r\n\r\n...\r\n```\r\n\r\n## Relevant Code\r\n\r\nA runnable colab gist can be found [here](https://colab.research.google.com/gist/auchtopus/3fdb4fdecd4a4754d10951e942ecddd1/issue_43162.ipynb)\r\n```\r\n!git clone https://github.com/auchtopus/flowering_toy_dataset\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom sklearn.model_selection import train_test_split\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.models import load_model\r\nfrom tensorflow.keras.models import Model, Sequential\r\nfrom tensorflow.keras.applications import Xception\r\nfrom tensorflow.keras.preprocessing import image\r\nfrom tensorflow.keras.layers import Flatten, Dense, Input, GlobalAvgPool2D\r\nfrom tensorflow.keras import optimizers\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\r\nimport matplotlib.pyplot as plt\r\nimport random\r\nimport os\r\nimport time\r\nfrom datetime import datetime\r\nfrom IPython.display import SVG\r\nfrom PIL import ImageFile\r\nImageFile.LOAD_TRUNCATED_IMAGES = True\r\n\r\n\r\n# verify gpus\r\nprint(tensorflow.config.list_physical_devices('GPU'))\r\n\r\n\r\nFAST_RUN = False\r\nIMAGE_HEIGHT=850\r\nIMAGE_WIDTH=550\r\nIMAGE_CHANNELS=3\r\nIMAGE_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)\r\n\r\ninput_tensor_def = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))  # unused for now\r\n\r\n\r\nNAME = f\"{datetime.today().strftime('%Y-%m-%d')}-xception_850_flowering_keras_xception\"\r\n\r\n\r\nmodel_core = Xception(weights = None, include_top = False, input_shape = IMAGE_SIZE)\r\n\r\nmodel_head = model_core.output\r\nmodel_head = GlobalAvgPool2D()(model_head)\r\nmodel_head = Flatten()(model_head)\r\nmodel_head = Dense(512, activation = 'relu')(model_head)\r\nmodel_head = Dense(256, activation = 'relu')(model_head)\r\nmodel_head = Dense(2, activation = 'softmax')(model_head)\r\n\r\nmodel = Model(inputs = model_core.input, outputs = model_head)\r\n\r\nmodel.compile(Adam(lr=.00005), loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\nprint(model.summary())\r\n\r\nearlystop = EarlyStopping(patience=20)\r\n\r\n\r\nfilepath=f\"/content/model/model.hdf5\"\r\nif not os.path.isdir(f\"/content/model\"):\r\n  os.makedirs(f\"/content/model\")\r\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\r\n\r\ncallbacks = [earlystop, checkpoint]\r\n\r\n\r\n# hard coded for now, replace later\r\nnb_train_samples = 20\r\n# nb_validation_samples = \r\nbatch_size=4\r\n\r\ntrain_path = '/content/flowering_toy_dataset/images'\r\n# valid_path = '/content/NEVP_phenology_unscored_20191206/images'\r\ntrain_datagen = ImageDataGenerator(\r\n    rotation_range=15,\r\n    rescale=1./255,\r\n    shear_range=0.1,\r\n    zoom_range=0.2,\r\n    horizontal_flip=True,\r\n    width_shift_range=0.1,\r\n    height_shift_range=0.1\r\n)\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    train_path, target_size=IMAGE_SIZE, class_mode='categorical', classes=['flowering', 'not_flowering'], batch_size=batch_size)\r\n\r\n# validation_datagen = ImageDataGenerator(rescale=1./255)\r\n# validation_generator = validation_datagen.flow_from_directory(\r\n#     valid_path, target_size=IMAGE_SIZE, class_mode='categorical', classes=['Flowering', 'Not_Flowering'], batch_size=batch_size)\r\n\r\n# print(nb_validation_samples//batch_size)\r\n\r\nepochs=3 if FAST_RUN else 500\r\nhistory = model.fit_generator(\r\n    train_generator,\r\n    epochs=epochs,\r\n    steps_per_epoch=nb_train_samples//batch_size,\r\n    callbacks=callbacks\r\n)\r\n\r\n```\r\n## My thoughts\r\nI feel like normally with broadcasting errors, it's generally related to the sizes of the images, or it fails to broadcast from a 3-dim input tensor to the 4-dim (batch, height ,width,channels) tensor. However, here, it just seems like the code has forgotten about the existence of the batch dimension and confused height for batches, width for height, and channels for both width and channels. I have double checked my code, but to my (admittedly very limited) knowledge, everything looks okay. ", "comments": ["Can you edit your example to have very minimal but runnable example to reproduce this?", "> Can you edit your example to have very minimal but runnable example to reproduce this?\r\n\r\nUpdated!", "It Is still depending on filesystem input. Can you have something minimal that we could just copy, paste and run?", "@auchtopus \r\nI ran the code shared and face the error shared in the [gist here](https://colab.research.google.com/gist/Saduf2019/dbd8ab4a67543c6216979c49eda7ca92/untitled410.ipynb).\r\nPLease share simple stand alone code such that we could replicate the issue or is possible share a colab gist with the issue reported.", "@Saduf2019 @bhack where can I host toy versions of the requisite image directories? Should I just make a public git repo with 25 images of each category (20 train, 5 test)? Or is there a better solution?", "@Saduf2019 @bhack I've updated the code! \r\nA runnable colab can be found [here](https://colab.research.google.com/drive/16JidHSw5vE2FJ_c_5KbE5mDzpJR8SYRR?usp=sharing)\r\n\r\nThe error log:\r\n```\r\n\r\nFound 20 images belonging to 2 classes.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1799: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\r\n  warnings.warn('`Model.fit_generator` is deprecated and '\r\n\r\n---------------------------------------------------------------------------\r\n\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-5-ea94dcc78bee> in <module>()\r\n     96     epochs=epochs,\r\n     97     steps_per_epoch=nb_train_samples//batch_size,\r\n---> 98     callbacks=callbacks\r\n     99 )\r\n\r\n7 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py in _get_batches_of_transformed_samples(self, index_array)\r\n    238                 x = self.image_data_generator.apply_transform(x, params)\r\n    239                 x = self.image_data_generator.standardize(x)\r\n--> 240             batch_x[i] = x\r\n    241         # optionally save augmented images to disk for debugging purposes\r\n    242         if self.save_to_dir:\r\n\r\nValueError: could not broadcast input array from shape (850,550,3) into shape (850,550,3,3)\r\n\r\n```\r\n\r\nThis error is not localized to 2.1.0 as 2.4.0 is also throwing this error. \r\n", "Your problem is `target_size`: \r\n> target_size: Either None (default to original size) or tuple of ints (img_height, img_width).", "Thanks so much! I forgot to check the data generator, and thought the error was contained entirely in the model specification.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43162\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43162\">No</a>\n"]}, {"number": 43161, "title": "Uses of the new rocm skip test decorator.", "body": "Merge to upstream.\r\n\r\n/cc @cheshire @chsigg ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43161) for more info**.\n\n<!-- need_sender_cla -->", "@rsanthanam-amd  Thank you for your contribution. Can you please sign CLA? Thanks!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43161) for more info**.\n\n<!-- ok -->", "gentle ping", "@rsanthanam-amd can you please check ubuntu sanity build failures ?", "i pushed a fix that should address the Ubuntu sanity failures", "@rsanthanam-amd  Can you please check @allenlavoie's comments and keep us posted ? Thanks!", "For TF API owners: We don't believe this should be made public because the internal tests don't need it. It maybe makes more sense to have a disable_with_predicate(pred) where pred is is_built_with_cuda, is_built_with_rocm etc. ", "@rsanthanam-amd  Can you please resolve conflicts? Thanks!", "Done.", "@rsanthanam-amd Can you please resolve conflicts? Thanks!", "Done.", "@rsanthanam-amd sorry for the delay , can you please check sanity build failures ?", "Done.", "@rthadur ... anything we can do on our end to help get this PR merged?\r\n", "@rsanthanam-amd @deven-amd here are the comments from @fchollet \r\nNit: most skip_messages throughout this PR can be put on a single line\r\nThank you ", "Done.  Changed all but one skip message (that one was not possible without obscuring its meaning) to a single line.", "@rthadur gentle ping.  Please let me know if there is anything else I can do to help get this PR merged.  Thanks!", "@rsanthanam-amd here is the internal error which we are seeing \r\n\r\n`/py/absl/testing/parameterized.py\", line 318, in bound_param_test\r\n   return test_method(self, testcase_params)\r\n File \"tensorflow/python/keras/google/private_tf_api_test/symbol_test.py\", line 84, in test_private_api_symbol_usage\r\n   f.read(), self.transfermation_rules)\r\n File \"tensorflow/python/keras/google/private_tf_api_test/symbol_converter.py\", line 701, in convert\r\n   module = converter.visit(pasta.parse(input_file))\r\n File \"<embedded stdlib>/ast.py\", line 253, in visit\r\n   return visitor(node)\r\n File \"tensorflow/python/keras/google/private_tf_api_test/symbol_converter.py\", line 278, in visit_Module\r\n   self.generic_visit(node)\r\n File \"<embedded stdlib>/ast.py\", line 308, in generic_visit\r\n   value = self.visit(value)\r\n File \"<embedded stdlib>/ast.py\", line 253, in visit\r\n   return visitor(node)\r\n File \"tensorflow/python/keras/google/private_tf_api_test/symbol_converter.py\", line 292, in visit_ClassDef\r\n   self.generic_visit(node)\r\n File \"<embedded stdlib>/ast.py\", line 308, in generic_visit\r\n   value = self.visit(value)\r\n File \"<embedded stdlib>/ast.py\", line 253, in visit\r\n   return visitor(node)\r\n File \"tensorflow/python/keras/google/private_tf_api_test/symbol_converter.py\", line 285, in visit_FunctionDef\r\n   self.generic_visit(node)\r\n File \"<embedded stdlib>/ast.py\", line 308, in generic_visit\r\n   value = self.visit(value)\r\n File \"<embedded stdlib>/ast.py\", line 253, in visit\r\n   return visitor(node)\r\n File \"tensorflow/python/keras/google/private_tf_api_test/symbol_converter.py\", line 496, in visit_Attribute\r\n   symbol = self._get_scoped_symbol(node.value.id)\r\n File \"tensorflow/python/keras/google/private_tf_api_test/symbol_converter.py\", line 198, in _get_scoped_symbol\r\n   raise KeyError('{} does not exist in scope'.format(name))\r\nKeyError: 'test_util does not exist in scope'\r\n`", "and also this error \r\n\r\n`tensorflow/python/keras/layers/lstm_v2_test.py\", line 64, in <module>\r\n    class LSTMV2Test(keras_parameterized.TestCase):\r\n  File \"tensorflow/python/keras/layers/lstm_v2_test.py\", line 330, in LSTMV2Test\r\n    @test_util.run_v2_only\r\nNameError: name 'test_util' is not defined`", "My latest commit should fix those errors.  Please retest.  Thanks!", "@rthadur this is strange.  after my most recent commit, the tests were run again and they all passed.  then they were run a subsequent time and now there are failures.  please let me know what happened and what the new internal google check failures are.\r\n\r\nalso, by inspecting the logs from the tflite micro, windows bazel, and windows bazel gpu, it is not clear to me why those checks are failing.", "@rsanthanam-amd trying to merge this internally , will update once it is gets through", "here are additional logs \r\n\r\n```\r\n 11:01:37.836079    7509 symbol_test.py:97] Found invalid tf symbol usage in /third_party/tensorflow/python/keras/layers/convolutional_recurrent_test.py, symbol = google3.third_party.tensorflow.python.platform.test.disable_with_predicate\r\n 11:01:43.827857    7509 symbol_test.py:97] Found invalid tf symbol usage in /third_party/tensorflow/python/keras/layers/gru_test.py, symbol = google3.third_party.tensorflow.python.platform.test.disable_with_predicate\r\n 11:01:44.657812    7509 symbol_test.py:97] Found invalid tf symbol usage in /third_party/tensorflow/python/keras/layers/gru_v2_test.py, symbol = google3.third_party.tensorflow.python.platform.test.disable_with_predicate\r\n 11:01:47.510349    7509 symbol_test.py:97] Found invalid tf symbol usage in /third_party/tensorflow/python/keras/layers/lstm_test.py, symbol = google3.third_party.tensorflow.python.platform.test.disable_with_predicate\r\n 11:01:48.784457    7509 symbol_test.py:97] Found invalid tf symbol usage in /third_party/tensorflow/python/keras/layers/lstm_v2_test.py, symbol = google3.third_party.tensorflow.python.platform.test.disable_with_predicate\r\n 11:02:16.531931    7509 symbol_test.py:97] Found invalid tf symbol usage in /third_party/tensorflow/python/keras/layers/wrappers_test.py, symbol = google3.third_party.tensorflow.python.platform.test.disable_with_predicate\r\nI0310 11:02:16.538181    7509 test_util.py:2080] time(__main__..test_private_api_symbol_usage10 ('layers')): 42.01s\r\n[  FAILED  ] .test_private_api_symbol_usage10 ('layers')\r\n[ RUN      ] .test_private_api_symbol_usage18 ('tests')\r\nI0310 11:02:23.710425    7509 test_util.py:2080] time(__main__..test_private_api_symbol_usage18 ('tests')): 7.17s\r\n[       OK ] .test_private_api_symbol_usage18 ('tests')\r\n[ RUN      ] .test_private_api_symbol_usage6 ('estimator')\r\nI0310 11:02:24.011263    7509 test_util.py:2080] time(__main__..test_private_api_symbol_usage6 ('estimator')): 0.3s\r\n[       OK ] .test_private_api_symbol_usage6 ('estimator')\r\n======================================================================\r\nFAIL: test_private_api_symbol_usage10 ('layers') (__main__.)\r\n.test_private_api_symbol_usage10 ('layers')\r\ntest_private_api_symbol_usage('layers')\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n File \"/third_party/py/absl/testing/parameterized.py\", line 318, in bound_param_test\r\n   return test_method(self, testcase_params)\r\n File \"/third_party/tensorflow/python/keras/google/private_tf_api_test/symbol_test.py\", line 99, in test_private_api_symbol_usage\r\n   self.fail(\"Found invalid tf symbol usage in Keras. \"\r\nAssertionError: Found invalid tf symbol usage in Keras. See error log for more details.\r\n\r\n----------------------------------------------------------------------\r\nRan 3 tests in 49.481s\r\n\r\nFAILED (failures=1)\r\n-- Forge runner: Test failed with exit code 1 while\r\n```", "I don't understand how to fix the \"Found invalid tf symbol usage...\" errors.  The failing symbol is a new API function.\r\n\r\nI fixed the other errors.", "@allenlavoie @fchollet any idea how to fix above error ? Thank you ", "gentle ping. @allenlavoie do you know how i can fix the \"Found invalid tf symbol usage...\" errors?", "@qlzh727 for the Keras symbol usage tests (I pinged you on the integration change but you apparently missed it).", "Let me patch and fix the PR internally. Sorry for the wait."]}, {"number": 43160, "title": "Increase the number of targets that are built for Bluepill.", "body": "This will help provide some safeguard against issues like #43126\r\n\r\nNote:\r\n * Flash and ram are much larger than the actual hardware.\r\n * The tests are only built, not run because we are currently unable to use renode as part of out continuous integration system.\r\n * Some of the core tests (such as micro_interpreter_test) do not build for the bluepill target and have been explicitly excluded.\r\n * While the kernel_circular_buffer_test builds for bluepill, it does not pass and so had to be excluded.\r\n\r\nManually tested that micro/tools/ci_build/test_bluepill.sh passes on my local machine (i.e. builds and all the tests that are run, pass).", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43159, "title": "[ROCm] Disabling failing unit tests", "body": "This upstreams a few unit test tags for tests which are known to fail with ROCm.", "comments": ["@ekuznetsov139 can you please check ubuntu sanity errors ?", "@ekuznetsov139  Any update on this PR? Please. Thanks!", "I'm trying to land it manually, sorry for the delay."]}, {"number": 43158, "title": "[SE] Include absl/memory/memory.h", "body": "Similar to https://github.com/tensorflow/tensorflow/pull/42942\r\n\r\n```\r\nexternal/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc:184:23: error: 'make_unique' is not a member of 'absl'\r\n     auto lock = absl::make_unique<absl::MutexLock>(&mutex_);\r\n                       ^~~~~~~~~~~\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43158) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43158) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "@veshij Thank you for your contribution. Can you please sign CLA? Thanks!", "@googlebot I fixed it.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43158) for more info**.\n\n<!-- ok -->"]}, {"number": 43157, "title": "`validation_split` support for RaggedTensors", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No):No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCall to tf.keras.Model.fit with validation_split=0.2 produces:\r\n\r\nValueError: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'>]\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n**Who will benefit with this feature?**\r\nUsers of RaggedTensor\r\n**Any Other info.**\r\nThis only occurs when validation_split parameter is being used.", "comments": ["@dcpatton \r\n\r\n Do you have any use case that requires the feature you are interested in? Are you interested to submit a PR ?\r\nThanks!", "I do not have a PR to submit. Is there a template for the Use Case?", "@dcpatton, Sorry for the late response. Is this still an issue for you?\r\n\r\nCan you please share a simple standalone code to reproduce the issue? Also, check your code with the recent TF versions (2.6 and tf-nightly) and let us know whether the issue persists with recent TF versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43156, "title": "Revert renaming of tools to exec_tools", "body": "Reverts part of f827c023906e7d30f0e5f2992b111ab34153310a as that causes trouble due to action_env variables not passed through to any dependent build which breaks builds using TF_SYSTEM_LIBS\r\n\r\nFixes #43019", "comments": ["Let's hold on this one, Bazel folks are investigating at the moment whether we need to use `tools` or `exec_tools`.", "Sure. Just to highlight what I found:\r\n- Using tools instead makes `action_env` be passed to builds triggered further down the dependency chain, with exec_tools they are not\r\n- Some code which was affected by the tools->exec_tools renaming in 2.3 has since been changed to (IIRC) use `_local_genrule`. I'm unsure how to verify that `action_env`s are passed through that.\r\n- There was some discussion in Bazel about introducing `host_env` variables similar to `action_env`. I'm not sure I could follow that distinction as e.g. for us the whole build is executed on the local machine, so in that case those 2 (if there were 2) should be equal. At best by default\r\n- It is absolutely essential to have a way to pass variables like CPATH, LIBRARY_PATH and the like (not speaking of PATH and LD_LIBRARY_PATH which seem be to passed through already) to all invocations without exception or stuff breaks.\r\n\r\nHope that helps in resolving this. In the meantime this patch is used by us to build TF on our HPC systems", "@mihaimaruseac  Any update on this PR? Please. Thanks!", "No update yet from the Bazel team", "Any update yet? As for\r\n\r\n>  whether we need to use tools or exec_tools\r\n\r\nFor the build using system protobuf using `exec_tools` does not work but `tools` does. For the non-system protobuf either work. So IMO this is a step forward and discussion which should be used could be done in a follow-up issue and likely depends on bugfix or feature addition on the Bazel side because at the current state `exec_tools` cannot be the correct choice if it breaks builds.", "Approving though it might be reverted if things fail"]}, {"number": 43155, "title": "[INTEL MKL] MKL DNN 0.x code cleanup -  slice op and mkl_util_test", "body": "DNN 0.x cleanup of MKL Slice op and mkl_util_test:\r\n\r\n (1) Remove all DNN 0.x related code\r\n (2) Replace all DNN 1.x macro usages", "comments": []}, {"number": 43154, "title": "tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018DDF556708> and will run it as-is.", "body": "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018DDF556708> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018DDF556708> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert", "comments": ["Do you have a very minimal but runnable standalone example/colab to reproduce this?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I am getting the same error. What can be done tor resolve this?\r\n\r\n\r\nEpoch 1/10\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A9028F5AF8> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A9028F5AF8> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-17-637040b66b25> in <module>\r\n----> 1 model.fit(x=train_batch, validation_data=valid_batch, epochs=10, verbose=2)\r\n\r\n~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1101                 _r=1):\r\n   1102               callbacks.on_train_batch_begin(step)\r\n-> 1103               tmp_logs = self.train_function(iterator)\r\n   1104               if data_handler.should_sync:\r\n   1105                 context.async_wait()\r\n\r\n~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    782     tracing_count = self.experimental_get_tracing_count()\r\n    783     with trace.Trace(self._name) as tm:\r\n--> 784       result = self._call(*args, **kwds)\r\n    785       compiler = \"xla\" if self._jit_compile else \"nonXla\"\r\n    786       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    842         # Lifting succeeded, so variables are initialized and we can run the\r\n    843         # stateless function.\r\n--> 844         return self._stateless_fn(*args, **kwds)\r\n    845     else:\r\n    846       _, _, _, filtered_flat_args = \\\r\n\r\n~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2970        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n   2971     return graph_function._call_flat(\r\n-> 2972         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n   2973 \r\n   2974   @property\r\n\r\n~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1946       # No tape is watching; skip to running the function.\r\n   1947       return self._build_call_outputs(self._inference_function.call(\r\n-> 1948           ctx, args, cancellation_manager=cancellation_manager))\r\n   1949     forward_backward = self._select_forward_and_backward_functions(\r\n   1950         args,\r\n\r\n~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    559               inputs=args,\r\n    560               attrs=attrs,\r\n--> 561               ctx=ctx)\r\n    562         else:\r\n    563           outputs = execute.execute_with_cancellation(\r\n\r\n~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node sequential/conv2d/Relu (defined at <ipython-input-17-637040b66b25>:1) ]] [Op:__inference_train_function_1271]\r\n\r\nFunction call stack:\r\ntrain_function\r\n", "> I am getting the same error. What can be done tor resolve this?\r\n\r\n@GTK-ARJUN,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "I am facing the same issue. Does anyone know how to fix it??\r\nIt's really urgent."]}, {"number": 43153, "title": "Do not symlink system protobuf headers but only the required .proto files", "body": "Symlinking the system headers has proven to be problematic as newer versions of protobuf add or remove headers which makes having a static array of header files hard to impossible. Turns out the headers don't need to be symlinked at all but only the .proto files used as inputs need to be present.\r\n\r\nExample: In #34792 @cbalint13 removed the `port_def.inc` \"header\". But the workspace.bzl requests protobuf 3.8.0 (https://github.com/tensorflow/tensorflow/blob/610a78b98569e2908809645626b4bd6afd2a22d8/tensorflow/workspace.bzl#L423, now 3.9.2: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L628) which has it: https://github.com/protocolbuffers/protobuf/blob/v3.8.0/src/google/protobuf/port_def.inc\r\nThis leads to build failures due to this missing (although I think if all would be working well it would just take the header from the system if `$INCLUDEDIR` is in the compilers header search path).\r\n\r\nThis partially resolves/helps #37861", "comments": []}, {"number": 43152, "title": "ImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`", "body": "ImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-0f2507b3ca9f> in <module>\r\n     16 \r\n     17 \r\n---> 18 from keras.models import Sequential\r\n     19 from keras.layers import Dense, Dropout, BatchNormalization, Activation\r\n     20 from keras import regularizers\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\__init__.py in <module>\r\n      4 except ImportError:\r\n      5     raise ImportError(\r\n----> 6         'Keras requires TensorFlow 2.2 or higher. '\r\n      7         'Install TensorFlow via `pip install tensorflow`')\r\n      8 \r\n\r\nImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`\r\n\r\n", "comments": ["Please import keras from tensorflow '`from tensorflow import keras`.\r\n\r\nSee https://github.com/keras-team/keras/releases/tag/2.4.0", "@pratikshambharkar \r\nCan you please share the complete stand alone indented code for us to replicate the issue faced or if possible share a colab gist with the error reported.\r\nplease refer to [this link ](https://stackoverflow.com/questions/63006475/how-to-solve-importerror-keras-requires-tensorflow-2-2-or-higher-install-tenso)as per the error reported.\r\nYou may also verify with existing issues containing \"ImportError: DLL load failed\": #42495 #43026 #43003 ", "Also we don't officially support Ananconda. If you reaally need Anaconda with Tensorflow check third_party https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\r\n\r\n/cc @lamberta This is already  quite recurrent issue with users that try to use Tensorflow in a conda setup. I think that the our current notice it is not visible enough as it is only in https://www.tensorflow.org/install/pip#2.-create-a-virtual-environment-recommended. \r\nCan we have a better exposition of this info to minimize the number of tickets like this?", "Correct, Conda is not officially supported and often lags behind the current release.\r\n\r\nAs @bhack noted, there's a mention here: https://www.tensorflow.org/install/pip#conda \u2014but little appetite for further visibility of an out-of-date package. But you're welcome to add a an entry to the [errors.md](https://github.com/tensorflow/docs/blob/master/site/en/install/errors.md) page, if you think that is useful. Thanks\r\n", "@Saduf2019 \r\n\r\nI verified the versions I have installed (with pip) for everything and I have:\r\n\r\nPython 3.7.9\r\nName: tensorflow\r\nVersion: 2.1.0\r\nName: Keras\r\nVersion: 2.4.3\r\n\r\n\r\n\r\ni have tried this also :- https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\r\n< conda create -n tf tensorflow\r\n   conda activate tf\r\n\r\n    conda create -n tf-gpu tensorflow-gpu \r\n    conda activate tf-gpu\r\n>\r\n\r\n`import tensorflow`\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-d6579f534729> in <module>\r\n----> 1 import tensorflow\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 \r\n     42 # pylint: enable=wildcard-import\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in <module>\r\n     33 from tensorflow.core.protobuf import config_pb2\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n     37 from tensorflow.python.client import pywrap_tf_session\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>\r\n     26 \r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n---> 28 from tensorflow.python import pywrap_tensorflow\r\n     29 from tensorflow.python._pywrap_tfe import *\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     81 for some common reasons and solutions.  Include the entire stack trace\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n"]}, {"number": 43151, "title": "Using Hidden layers output in the loss function ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): *\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: *\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.3 / tf-nightly\r\n- Python version: 3.6-3.7-3.8\r\n- Bazel version (if compiling from source): *\r\n- GCC/Compiler version (if compiling from source): *\r\n- CUDA/cuDNN version: *\r\n- GPU model and memory: *\r\n\r\n**Describe the current behavior**\r\n\r\nHidden layers output of the model cannot be accessed outside of the function building code.\r\n \r\n**Describe the expected behavior**\r\n\r\nTo be able to use hidden layers output in my loss function.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1laEpykHax2QbAV4SB-8Srwfh9SvmAt4B?usp=sharing\r\n\r\n**Other info / logs** \r\n\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: dense_6/Relu:0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n_SymbolicException                        Traceback (most recent call last)\r\n9 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     72       raise core._SymbolicException(\r\n     73           \"Inputs to eager execution function cannot be Keras symbolic \"\r\n---> 74           \"tensors, but found {}\".format(keras_symbolic_tensors))\r\n     75     raise e\r\n     76   # pylint: enable=protected-access\r\n\r\n**_SymbolicException:** Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'dense_6/Relu:0' shape=(None, 128) dtype=float32>]\r\n\r\nWhen i used `tf.config.run_functions_eagerly(True)`\r\n\r\nI got my loss being equal to  **0.0000e+00**\r\n", "comments": ["Can you `add_loss` for your case? Check https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_add_loss_method", "Thanks for your answer ! \r\n\r\nI'm actually getting a `ValueError: No gradients provided for any variable` error.\r\n\r\nIn fact, i'm trying to reproduce this model [https://github.com/IAmSuyogJadhav/3d-mri-brain-tumor-segmentation-using-autoencoder-regularization/blob/master/model.py](url)\r\n\r\nUsing 2 model outputs, a dice loss for one ouput ( GT ) and a loss function ( VAE) depending on two layers (z_mean,z_var) of the model.\r\n\r\nI think i've tried everything to get it work (layer output in loss function), so if someone can manage to make the standalone code i've made in the first post work, with ability to generalize for multiple outputs, it will be huge ! Thanks.\r\n\r\n", "@Otakarlp \r\n\r\nI have tried in colab with TF nightly version(`2.4.0-dev20200912`) and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/c16be7ff698abbc656c0aa968c77d6a6/untitled343.ipynb).Please, verify once and close the issue. Thanks!", " @ravikyram It seems to me that you have not reproduced the same user example with nightly cause on your Colab gist you are missing the `+ tf.math.reduce_sum(layer2)` user original case inside `loss2`.\r\n\r\n@Otakarlp As I told you I think the you could follow the documentation example I mentioned with `add_loss`. Other then documentation you could see https://github.com/keras-team/keras/issues/5563#issuecomment-283397931", "First, thank you both for trying to help me.\r\n\r\nSecondly, i want to make my mea culpa here, @bhack was absolutely right, i used `add_loss` and it worked perfectly ( before compiling the model(s) ), thank you again @bhack you are huge.\r\n\r\nFor those who are trying to add intermediate layers to the loss function with respect to a certain path of the model, please use the gist [Here](https://colab.research.google.com/drive/1qGZYgWRZoYXIeMKIEVKLf4hTo24b0CHv?usp=sharing).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43151\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43151\">No</a>\n"]}, {"number": 43150, "title": "Make failed with C++ API used like external library. Error: static assertion failed: std::string is no longer a scalar type, use tensorflow::tstring", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- GPU model and memory:\r\n \r\nI'm trying to use TensorFlow c++ like external library with cmake to load a model trained in python, I use the example of label image https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image. I did it with past version of tensorflow v1.14.0, bazel 0.24.1, protobuf 3.7.x, Eigen 3.3.7, but when I updated the version of Tensorflow 2.3.0 with its dependencies (bazel 3.1.0, protobuf 3.9.2, Eigen 3.3.x (according to workspace.bzl)) there is the next error:\r\n\r\n`[ 50%] Building CXX object CMakeFiles/main.dir/main.cc.o\r\nIn file included from /usr/local/include/tensorflow/cc/framework/ops.h:21:0,\r\n                 from /usr/local/include/tensorflow/cc/ops/const_op.h:19,\r\n                 from /home/nae/ML/LoadModel/src/main.cc:41:\r\n/usr/local/include/tensorflow/core/framework/tensor.h: In instantiation of \u2018typename tensorflow::TTypes<T>::Scalar tensorflow::Tensor::scalar() [with T = std::__cxx11::basic_string<char>; typename tensorflow::TTypes<T>::Scalar = Eigen::TensorMap<Eigen::TensorFixedSize<std::__cxx11::basic_string<char>, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>]\u2019:\r\n/home/nae/ML/LoadModel/src/main.cc:109:26:   required from here\r\n/usr/local/include/tensorflow/core/framework/tensor.h:878:3: error: static assertion failed: std::string is no longer a scalar type, use tensorflow::tstring\r\n   static_assert(\r\n`\r\n\r\n\r\n**Any other info / logs**\r\nCMakeLists.txt:\r\n\r\ncmake_minimum_required(VERSION 3.10)\r\n\r\n\r\nproject(LoadModel)\r\nfind_package(Tensorflow REQUIRED)\r\nfind_package(Protobuf REQUIRED)\r\nfind_package(Eigen3 REQUIRED)\r\n\r\nset(INCLUDE_DIRS ${Tensorflow_INCLUDE_DIRS}\r\n\t\t ${EIGEN3_INCLUDE_DIRS}\r\n                 ${Protobuf_INCLUDE_DIRS}\r\n)\r\nset(MODULE_LIBS ${Tensorflow_LIBRARIES}\r\n\t\t${EIGEN3_LIBRARIES}\r\n                ${Protobuf_LIBRARIES}\r\n)\r\n\r\nadd_executable(main ./main.cc)\r\ntarget_include_directories(main PUBLIC ${INCLUDE_DIRS})\r\ntarget_link_libraries(main ${MODULE_LIBS})\r\n\r\n\r\nExample label images\r\n\r\n https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image", "comments": ["They have started to use `tensorflow::tstring` as you can see from https://github.com/tensorflow/tensorflow/commit/f05a57eefe9f03c9fae83d0fcb727ee07949d963 /cc @gharibian ", "Solved! thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43150\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43150\">No</a>\n"]}, {"number": 43149, "title": "Error while using ModelCheckpoint callback in tf.keras when creating checkpoints.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 20.04 LTS (GNU/Linux 4.4.0-18362-Microsoft x86_64)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version (use command below):2.3.0\r\n- Python version:3.8.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI am trying to create checkpoints while training the model using ModelCheckpoint callback. It is giving the following error\r\n`TypeError: get_config() missing 1 required positional argument: 'self'`\r\n\r\n**Describe the expected behavior**\r\nI tried saving the model using save_model func. It works fine. I think there is some issue with the callback. \r\n\r\n**Standalone code to reproduce the issue**\r\nLink to the gist:\r\n[https://gist.github.com/Gokul-S-Kumar/0a355ca70e44953f3af40693e38bc2b1](url)\r\n\r\n**Other info / logs** \r\nLink to full description of the error, in case useful:\r\n[https://gist.github.com/Gokul-S-Kumar/b478a8fba470626aca51d4e09331388b](url)\r\n\r\nPS:- I may be committing a simple mistake, if yes please help in pointing it out so that I can rectify it. ", "comments": ["Can you try changing the init with `kernel_initializer = keras.initializers.he_normal()`?", "> Can you try changing the init with `kernel_initializer = keras.initializers.he_normal()`?\r\n\r\nThanks @bhack !! It works :). \r\nI have been initializing weights in the previous way all the time. Is it wrong to do so?", "@Gokul-S-Kumar \r\nPlease feel free to move the issue to closed status if resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43149\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43149\">No</a>\n", "TF is not longer backward compatible and posed so much irritations !!!!"]}, {"number": 43148, "title": "[TFLite] Ensure that all int16x8 operators check that the zero-point is null", "body": "Hi,\r\n\r\nThe int16x8 operators should double-check that their int16 inputs or outputs don't use any zero point. Most int16x8 operators already check that but it wasn't the case for a few remaining kernels. This PR fixes that and adapt the tests accordingly to avoid the creation of int16 tensors with a non-null zero-point.\r\n\r\nThe int16 output type check in `PreluPrepare` is also removed as `PreluEval` doesn't support int16x8 quantization yet. I'm not sure why this check was added in commit c01a6d66514886d2d75f9f3d0d46301827d2b7e9.\r\n\r\nThanks,\r\nThibaut", "comments": ["Thanks for your contribution.\r\nWhat is the bug associated with this PR?", "Hi,\r\n\r\nThere isn't any bug associate to it per se as an int16 tensor with a non-null zero-point should not be generated by the quantizer for now. It's mainly to keep coherence between the int16x8 versions of the operators we contributed and to explicitly state that they only need to support int16 tensors with a null zero-point.", "@jianlijianli Could you help to review this PR?"]}, {"number": 43147, "title": "Cannot import name 'image_preprocessing'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.12\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI am currently struggling to proceed due to having an unexpected error message containing 'ImportError: cannot import name 'image_preprocessing''.\r\n\r\nI am doing a project related to Object Detection (credits to: EdjeElectronics). Upon installing Tensorflow, Keras, CUDA, cudnn, Visual Studio Microsoft Redistribution 2019, I tried to do testing 'python ~\\object_detection\\builders\\model_builder_tf2_test.py' to see if everything is correctly installed. However, I keep receiving this error.\r\n\r\n**Describe the expected behavior**\r\n```\r\n(tensor-gpu) PS C:\\Users\\User\\tensorflow11\\models\\research> python object_detection\\builders\\model_builder_tf2_test.py\r\n2020-09-11 12:40:46.813673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nTraceback (most recent call last):\r\n  File \"object_detection\\builders\\model_builder_tf2_test.py\", line 24, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"C:\\Users\\User\\tensorflow11\\models\\research\\object_detection\\builders\\model_builder.py\", line 65, in <module>\r\n    from object_detection.models import ssd_efficientnet_bifpn_feature_extractor as ssd_efficientnet_bifpn\r\n  File \"C:\\Users\\User\\tensorflow11\\models\\research\\object_detection\\models\\ssd_efficientnet_bifpn_feature_extractor.py\", line 33, in <module>\r\n    from official.vision.image_classification.efficientnet import efficientnet_model\r\n  File \"C:\\Users\\User\\tensorflow11\\models\\official\\vision\\image_classification\\efficientnet\\efficientnet_model.py\", line 37, in <module>\r\n    from official.vision.image_classification import preprocessing\r\n  File \"C:\\Users\\User\\tensorflow11\\models\\official\\vision\\image_classification\\preprocessing.py\", line 25, in <module>\r\n    from official.vision.image_classification import augment\r\n  File \"C:\\Users\\User\\tensorflow11\\models\\official\\vision\\image_classification\\augment.py\", line 31, in <module>\r\n    from tensorflow.python.keras.layers.preprocessing import image_preprocessing as image_ops\r\nImportError: cannot import name 'image_preprocessing'\r\n```\r\n\r\nAppreciate if experienced TF developers to state where/what I am doing wrong.\r\n\r\nThank you.", "comments": ["@trystbinx \r\n\r\nImport of image_ops is unfortunately only available with TF 2.2+ .\r\n\r\nPlease, upgrade to TF 2.3 and check if the issue still persists. Thanks!", "> \r\n> \r\n> @trystbinx\r\n> \r\n> Import of image_ops is unfortunately only available with TF 2.2+ .\r\n> \r\n> Please, upgrade to TF 2.3 and check if the issue still persists. Thanks!\r\n\r\nThank you for your reply. Yes, the issues still persists, but a different error output.\r\n\r\nTF version is 2.2.0\r\nKeras is 2.3.1\r\n\r\nInstalled CUDA 10.1, cudnn, 7.6.5, set path for both.\r\n\r\nI am not sure what I am doing wrong. I have been looking for answers on the stackoverflow/github, but has not resolved this issue since a few days ago.\r\n\r\nIs there anything I should do to resolve this issue?\r\n\r\n```\r\n(toogpu) PS C:\\Users\\User\\tensorflow11\\models\\research> python\r\nPython 3.6.12 |Anaconda, Inc.| (default, Sep  9 2020, 00:29:25) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import keras\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\toogpu\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n```", "@trystbinx \r\n\r\nThis is Tensorflow installation error.\r\nCan you try it in fresh virtual environment and try to install. It is mostly due to compatibility issue.\r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download [the latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43147\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43147\">No</a>\n"]}, {"number": 43145, "title": "Data augmentation on TfRecords for multichannel images ", "body": "I am working with multichannel medical Nifti images which I convert to TfRecords and create a tf.dataset to pass later on a multichannel 3D CNN.  The shape of the images is (61,73,61,2) which means the image has 2 channels,  is there a way to do data augmentation on the fly when I read the TfRecords and increase the dataset size?  How can I apply the augmentation across all channels? ", "comments": ["Two years ago we had an official blog post about a biomedical toolkit:\r\nhttps://blog.tensorflow.org/2018/07/an-introduction-to-biomedical-image-analysis-tensorflow-dltk.html", "@quartermaine \r\nPlease update as per above comment.", "Thank you for the suggestion @bhack  somehow  I missed the DLTK toolkit. From my understanding the augmentation according to 04_input_normalisation_and_augmentation.ipynb notebook on tutorials has to be done when writing the TfRecords? ", "I don't think It is maintained anymore.\n\nNew image augmentation pipelines will be landing from the TF gsoc at https://github.com/tanzhenyu/image_augmentation.\nYou can try to ask there for you specific use case needs.", "I see.\r\n Thanks again @bhack I will ask there then.\r\n", "@quartermaine \r\nPlease feel free to move the issue to closed status."]}, {"number": 43144, "title": "Rename libtensorflowlite.a to libtensorflow-lite.a", "body": "LIB_NAME is defined as libtensorflow-lite.a in tensorflow/lite/tools/make/Makefile.", "comments": ["What is the main purpose of this renaming?", "https://github.com/search?q=%22libtensorflow-lite%22&type=Code\r\n\r\nThe library name is already assigned as libtensorflow-lite.a. And libtensorflow-lite.a is already used for many projects.\r\n", "This is fixing not a proposal.", "@terryheo Do we have any specific reasion to name it as tensorflowlite?"]}, {"number": 43143, "title": "Discrepancy between loss when called from `tf.keras.losses.BinaryCrossentropy()(true, pred)` and verbose output from model.fit() or hand-computation", "body": "I'm struggling to understand the behavior of `tf.keras.losses.BinaryCrossentropy()(true, pred)` -- I can't reproduce it's behavior from first principles.  Here's a MWE with a very simple two-output loss:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras import Model, Input\r\n\r\nnp.random.seed(8675309)\r\nX = np.random.normal(size = 1000).reshape(100,10)\r\nB = np.random.normal(size = 20).reshape(10,2)\r\nY = np.sin(X @ B ) @ np.array([1,0,0,-1]).reshape(2,2) >0\r\n\r\ni = Input(10)\r\nl = Dense(5, activation = 'relu')(i)\r\no1 = Dense(1, name = \"one\", activation = 'sigmoid')(l)\r\no2 = Dense(1, name = \"two\", activation = 'sigmoid')(l)\r\nm = Model(i, [o1, o2])\r\n\r\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\r\n\r\nm.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\r\n              loss={'one': tf.keras.losses.BinaryCrossentropy(from_logits=False),\r\n                    'two': tf.keras.losses.BinaryCrossentropy(from_logits=False)})\r\n\r\nm.fit(X, [Y[:,0], Y[:,1]],\r\n      batch_size=1000,\r\n      epochs=3,\r\n      verbose=1)\r\npred = m.predict(X)\r\nloss = loss_object(Y, pred)\r\n```\r\n\r\nwhich gives the following output:\r\n\r\n```\r\nTrain on 100 samples\r\nEpoch 1/3\r\n100/100 [==============================] - 1s 10ms/sample - loss: 1.5362 - one_loss: 0.8195 - two_loss: 0.7167\r\nEpoch 2/3\r\n100/100 [==============================] - 0s 47us/sample - loss: 1.5359 - one_loss: 0.8193 - two_loss: 0.7166\r\nEpoch 3/3\r\n100/100 [==============================] - 0s 35us/sample - loss: 1.5357 - one_loss: 0.8192 - two_loss: 0.7166\r\n\r\n\r\nThe loss from the loss object is 0.7730987071990967\r\n```\r\n\r\nThe losses don't match.  To see what's going on, I'll compute entropy by hand:\r\n\r\n```\r\nP = np.concatenate([pred[0], pred[1]], axis = 1)\r\nYY = np.concatenate([Y, 1-Y], axis = 1)\r\nPP = np.concatenate([P, 1-P], axis = 1)\r\n\r\n(YY*np.log(PP)).sum(axis=1).mean()\r\nOut[47]: -1.5354833577014506\r\n\r\n(YY[:,[0,2]]*np.log(PP[:,[0,2]])).sum(axis=1).mean()\r\nOut[48]: -0.8189752248860895\r\n\r\n(YY[:,[1,3]]*np.log(PP[:,[1,3]])).sum(axis=1).mean()\r\nOut[49]: -0.7165081328153611\r\n```\r\n\r\nThe results match the verbose output, but not the output from the loss object.  (Aside from a fairly large discrepancy in the third-ish decimal place???)\r\n\r\nWould appreciate an explaination of this behavior.  \r\n\r\nI have looked into the loss code on github, and it confuses me further:  \r\n\r\n ```\r\nExample subclass implementation:\r\n  ```python\r\n  class MeanSquaredError(Loss):\r\n    def call(self, y_true, y_pred):\r\n      y_pred = tf.convert_to_tensor_v2(y_pred)\r\n      y_true = tf.cast(y_true, y_pred.dtype)\r\n      return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1)\r\n  ```\r\nConverting a list to a tensor concatenates it along the first dimension, irrespective of the shape of the other input.  The rest of the github page doesn't make it clear how the entropy is actually calculated from that, or at least I can't see it.  binary cross entropy appears to be a super class, with the actual implementation somewhere else.\r\n\r\n```", "comments": ["Generally you can't exactly match loss output on fit and loss on evaluate. See https://github.com/tensorflow/tensorflow/issues/29964#issuecomment-505498333", "> Generally you can't exactly match loss output on fit and loss on evaluate. See [#29964 (comment)](https://github.com/tensorflow/tensorflow/issues/29964#issuecomment-505498333)\r\n\r\nThat explains the discrepancy in the third-ish decimal place: the loss in the verbose output is apparently the penultimate loss before the last gradient update.  \r\n\r\nBut that is not the main issue.  The main problem is that the loss from this:\r\n\r\n`\r\nloss = loss_object(Y, pred)\r\n`\r\n\r\nis VERY DIFFERENT from the loss that I compute by hand:\r\n\r\n```\r\nP = np.concatenate([pred[0], pred[1]], axis = 1)\r\nYY = np.concatenate([Y, 1-Y], axis = 1)\r\nPP = np.concatenate([P, 1-P], axis = 1)\r\n\r\n(YY*np.log(PP)).sum(axis=1).mean()\r\nOut[47]: -1.5354833577014506\r\n\r\n(YY[:,[0,2]]*np.log(PP[:,[0,2]])).sum(axis=1).mean()\r\nOut[48]: -0.8189752248860895\r\n\r\n(YY[:,[1,3]]*np.log(PP[:,[1,3]])).sum(axis=1).mean()\r\nOut[49]: -0.7165081328153611\r\n```\r\n\r\nMoreover, the loss that I compute by hand is similar (but not exactly the same) as the loss from the verbose output.  I guess the small dissimilarity comes from the fact that the verbose output reflects the penultimate loss, from which the gradients were computed.\r\n", "Update:  here are the results of about 50 experiments, using the loss object and using hand-computed entropy.  \r\n\r\n(they are from my actual use-case instead of the dummy MWE in the original post)\r\n\r\n![image](https://user-images.githubusercontent.com/13458532/93025146-937e9700-f5c9-11ea-97b4-8b19bdfba196.png)\r\n\r\n(the line is a unit-slope zero intercept)\r\n\r\nIt looks like there is a roughly 1:1 correspondence between the loss out of the loss object, and the hand-computed loss.  It must be a scaling issue -- mostly.  If it was only scaling, it'd be perfectly flat.  Someone care to shed light on this?\r\n\r\n", "About your example here you can print the evaluation total loss, loss one and, loss two:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras import Model, Input\r\nfrom tensorflow.keras.callbacks import Callback\r\n\r\nnp.random.seed(1)\r\ntf.random.set_seed(1)\r\n\r\nclass CustomMonitoring(Callback):\r\n    def on_train_batch_end(self, batch, logs=None):\r\n        loss = self.model.evaluate(X,  [Y[:,0], Y[:,1]])\r\n        print('For end batch {}, loss is {:7.4f} - one {:7.4f} - two {:7.4f}.'.format(batch, loss[0],loss[1],loss[2]))\r\n\r\nX = np.random.normal(size = 1000).reshape(100,10)\r\nY = np.sin(X @ B ) @ np.array([1,0,0,-1]).reshape(2,2) >0\r\ni = Input(10)\r\nl = Dense(5, activation = 'relu')(i)\r\no1 = Dense(1, name = \"one\", activation = 'sigmoid')(l)\r\no2 = Dense(1, name = \"two\", activation = 'sigmoid')(l)\r\nm = Model(i, [o1, o2])\r\n\r\n\r\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\r\n\r\nm.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\r\n              loss={'one': tf.keras.losses.BinaryCrossentropy(from_logits=False),\r\n                    'two': tf.keras.losses.BinaryCrossentropy(from_logits=False)})\r\nY_0 = Y[:,0]\r\nY_1 = Y[:,1]\r\nm.fit(X, [Y_0, Y_1],\r\n      batch_size=100,\r\n      epochs=3,\r\n      verbose=1,\r\n      shuffle=False,\r\n      callbacks=[CustomMonitoring()])\r\n```\r\n\r\nYou told that the main problem is `loss = loss_object(Y, pred)`. So you can reproduce `For end batch` output losses if you reproduce with your manual code all the fundamental steps in the `evaluate` pipeline (so please double check the flow).\r\n\r\nIf you see the core of the evaluation `test_step` there is relative your manual model `pred`:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L1154\r\n\r\nAs the next line is going to call `self.compiled_loss` internally you can follow the relative `loss_object` call at:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/compile_utils.py#L203\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43142, "title": "keras Sequential model get empty metrics_names", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 0.15.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pip3 install --upgrade tensorflow\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version:\r\n```\r\nPython 3.7.5 (default, Oct 25 2019, 10:52:18)\r\n[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\r\n```\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n**Describe the current behavior**\r\n```python\r\ndef build_model():\r\n    model = keras.Sequential()\r\n    model.add(keras.layers.Embedding(vocab_size, 64))\r\n    model.add(keras.layers.GlobalAveragePooling1D())\r\n    model.add(keras.layers.Dense(128, activation='relu'))\r\n    model.add(keras.layers.Dense(128, activation='relu'))\r\n    model.add(keras.layers.Dense(64, activation='relu'))\r\n    model.add(keras.layers.Dense(1))\r\n    optimizer = tf.keras.optimizers.RMSprop(0.01)\r\n    model.compile(loss='mse',\r\n                  optimizer=optimizer,\r\n                  metrics=['mae', 'mse'])\r\n    return model\r\n\r\n\r\nmodel = build_model()\r\nprint(model.metrics_names)  #  print []\r\n```\r\n\r\n**Describe the expected behavior**\r\nshould print out: ['loss', 'mae', 'mse']\r\ntensorflow 2.1.0 print correct", "comments": ["@zishuaiz,\r\nAs per the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#attributes_1), `metrics_names` are available only after a `keras.Model` has been trained/evaluated on actual data.\r\n\r\nCould you please check if you are facing the same issue after training the model? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 43141, "title": "Tensorflow 2.3.0 TfLite converter adds ExpandDims", "body": "I have a problem with new version of Tensorflow. I try to deploy my model to embedded system. I have trained Keras model and want to convert it to *.tflite format. Tensorflow 2.2.0 generates .tflite model which is accepted by tflite/micro on embedded system. However Tensorflow 2.3.0 adds some ExpandDims layer at the beggining of the network, and ExpandDims can not e resolved on the tf/lite/micro side. \r\nConvertion using TF 2.3.0:\r\n![image](https://user-images.githubusercontent.com/58625554/92916599-f3403b00-f42d-11ea-9562-03e6aca59bb0.png)\r\n\r\nConvertion using TF 2.2.0:\r\n![image](https://user-images.githubusercontent.com/58625554/92916775-1b2f9e80-f42e-11ea-9ba9-672af0d6281f.png)\r\n\r\nWe want to move our project from tf 2.2 to tf 2.3 ut for now it is impossible because of this problem.\r\nHere is the code used for convertion:\r\n`        converter = tf.lite.TFLiteConverter.from_keras_model(LOADED_KERAS_MODELl)\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n        quantized_and_pruned_tflite_model = converter.convert()\r\n\r\n        quantized_and_pruned_tflite_file = 'model_lite_pruned_microchip.tflite'\r\n\r\n        with open(quantized_and_pruned_tflite_file, 'wb') as f:\r\n            f.write(quantized_and_pruned_tflite_model)\r\n        print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)`\r\n\r\nTools versions:\r\n_Python 3.8.5,\r\nTensorflow 2.2.0/2.3.0\r\ntensorflow-model-optimization 0.4.1_\r\n\r\n", "comments": ["See https://github.com/tensorflow/tensorflow/pull/35189 /cc  @njeffrie @sicong-li-arm ", "I'm having this problem as well on Tensorflow 2.3.1. I think it might be due to the network working on 1D data, such as Conv1D layers, which are converted behind the scenes to 2D operations.", "Having the same problem with TensorFlow 2.5.0. I want to use Conv1D layers with dilatation. Is it possible to add this operation to tflite/micro? ", "@Xeratec @Mjonir @DominikStachura I have stumbled upon the same issue with TF 2.4.1 but have found a solution/workaround that might help you as well. I have replaced all 1D operations such as Conv1D or MaxPooling1D with their equivalent 2D counterpart. The process is straightforward:\r\n\r\n- Replace the 1D operations with their 2D counterpart\r\n- Adjust the input shape for your network\r\n- Adjust the `kernel_size` for your Conv2D layers, e.g., `kernel_size=(3,1)` instead of `kernel_size=3`\r\n- Adjust the `pool_size` for the pooling layers, e.g., `pool_size=(2,1)` instead of `pool_size=2`\r\n- Retrain the network\r\n\r\nIt's not ideal but it's a solution for now. Hope that helps!", "@Xeratec @DominikStachura Can anyone of you share a sample ?\r\nWe should be changing ExpandDims to Reshape during conversion as long as the shape is known.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43141\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43141\">No</a>\n"]}, {"number": 43140, "title": "AttributeError: 'int' object has no attribute 'op' in using customized model layers", "body": "Hi there, \r\n\r\nI am trying to build a model with a customized layer in Tensorflow. However, I always got the error when I initialize the customized model.\r\n\r\nThe error message I got is 'AttributeError: 'int' object has no attribute 'op' '.\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\wgw\\.spyder-py3\\Training_601_Model_Subclass_Layers.py\", line 85, in <module>\r\n    my_model = MyModel(64,12,name='my_custom_model')\r\n\r\n  File \"C:\\Users\\wgw\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 167, in __init__\r\n    super(Model, self).__init__(*args, **kwargs)\r\n\r\n  File \"C:\\Users\\wgw\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 173, in __init__\r\n    self._init_graph_network(*args, **kwargs)\r\n\r\n  File \"C:\\Users\\wgw\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 456, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\wgw\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 254, in _init_graph_network\r\n    base_layer_utils.create_keras_history(self._nested_outputs)\r\n\r\n  File \"C:\\Users\\wgw\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_utils.py\", line 186, in create_keras_history\r\n    _, created_layers = _create_keras_history_helper(tensors, set(), [])\r\n\r\n  File \"C:\\Users\\wgw\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_utils.py\", line 212, in _create_keras_history_helper\r\n    op = tensor.op  # The Op that created this Tensor.\r\n\r\nAttributeError: 'int' object has no attribute 'op'\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43140\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43140\">No</a>\n"]}, {"number": 43139, "title": "Add maintainers list for arc_mli optimized kernels and ARC platform", "body": "This pull request adds maintainer lists into readme files of optimized micro/kernels/arc_mli kernels and general micro/tools/make/targets/arc platform description.\r\n\r\nFixes #42933\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}]