[{"number": 29906, "title": "why are there so many  function as parameter that passed into in python code ?", "body": "why are there so many  functions as parameter that be passed into another function in Python code ?\r\nIt's hard to understand it !", "comments": ["Child education is not hard, so go back to the kindergarten please.\r\n\r\n\r\n\r\n", "Can you please elaborate on your query.Thanks!", "Closing as there is a lack of response and the question seems more fitted for StackOverflow."]}, {"number": 29905, "title": "tensorflow-gpu==2.0.0-beta1 AND tf-nightly-gpu-2-0-preview 2.0.0.dev20190607 fail with errors on windows 64 ", "body": "When installing tensorflow on windows 64 Home edition (with GPU enabled for tensorflow ) - I get the following two errors (with both tensorflow beta and the nightly preview )\r\n\r\nERROR: tensorflow-gpu 2.0.0b1 has requirement tb-nightly<1.14.0a20190604,>=1.14.0a20190603, but you'll have tb-nightly 1.15.0a20190617 which is incompatible.\r\nERROR: tfp-nightly 0.8.0.dev20190617 has requirement cloudpickle==1.1.1, but you'll have cloudpickle 1.2.1 which is incompatible.\r\n\r\nERROR: tf-nightly-gpu-2-0-preview 2.0.0.dev20190607 has requirement tb-nightly<1.15.0a0,>=1.14.0a0, but you'll have tb-nightly 1.15.0a20190617 which is incompatible.\r\nERROR: tfp-nightly 0.8.0.dev20190617 has requirement cloudpickle==1.1.1, but you'll have cloudpickle 1.2.1 which is incompatible.\r\n\r\n**System information**\r\n- Windows 10 64 bit Home edition with NVidia GPU\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: tf-nightly-gpu-2.0-preview=tf_nightly_gpu_2.0_preview-2.0.0.dev20190607\r\nAND\r\ntensorflow-gpu==2.0.0-beta1\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip install inside a newly created env using virtualenv\r\n- CUDA/cuDNN version: CUDA - cuda_10.1.168_425.25_win10 , cuDNN - cudnn-10.1-windows10-x64-v7.6.0.64\r\n- GPU model and memory: GEForce GTX 1050 Ti, 8 GB\r\n\r\n**Describe the problem**\r\nGetting 2 errors on trying to install tf-nightly-gpu-2.0-preview / tensorflow-gpu==2.0.0-beta1. \r\n\r\nERROR: tf-nightly-gpu-2-0-preview 2.0.0.dev20190607 has requirement tb-nightly<1.15.0a0,>=1.14.0a0, but you'll have tb-nightly 1.15.0a20190617 which is incompatible.\r\nERROR: tfp-nightly 0.8.0.dev20190617 has requirement cloudpickle==1.1.1, but you'll have cloudpickle 1.2.1 which is incompatible. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI am trying to install tensorflow 2.0 beta / tf-nightly on my windows machine using pip install inside a virtual environment.\r\n\r\nCommands -\r\n1. python -m pip install --upgrade pip setuptools virtualenv\r\n2. virtualenv <python path> env\r\n3. Activate the environment\r\n4. python -m pip install --upgrade -r requirements.txt\r\nrequirements.txt - Contains the following\r\n```\r\n##### Core scientific packages\r\njupyter==1.0.0\r\nmatplotlib==3.0.3\r\nnumpy==1.16.2\r\npandas==0.24.1\r\nscipy==1.1.0\r\n\r\n##### Machine Learning packages\r\nscikit-learn==0.20.3\r\n\r\n# Optional: the XGBoost library is only used in the ensemble learning chapter.\r\nxgboost==0.82\r\n\r\n##### TensorFlow-related packages\r\n\r\n# Replace tensorflow with tensorflow-gpu if you want GPU support. If so,\r\n# you need a GPU card with CUDA Compute Capability 3.5 or higher support, and\r\n# you must install CUDA, cuDNN and more: see tensorflow.org for the detailed\r\n# installation instructions.\r\n\r\n#tf-nightly-2.0-preview\r\n#tensorflow-gpu==2.0.0-beta1\r\ntf-nightly-gpu-2.0-preview\r\n\r\n#tensorboard\r\ntb-nightly\r\n\r\n#tensorflow-datasets\r\ntfds-nightly\r\n\r\ntensorflow-hub\r\n\r\n# Optional: only used in chapter 13.\r\n#tensorflow-transform==0.13.0\r\n\r\n# Optional: only used in chapter 16.\r\n# At the present (April 2019) the TF Addons library is only available on Linux\r\n# So uncomment this line if you are using Linux.\r\n#tensorflow-addons\r\n\r\n# Optional: the TF Agents library is only needed in chapter 18\r\ntf-agents-nightly\r\n\r\n# Optional: the TF Serving API library is just needed for chapter 19.\r\ntensorflow-serving-api\r\n\r\n##### Image manipulation\r\nimageio==2.5.0\r\nPillow==5.4.1\r\nscikit-image==0.14.2\r\n\r\n##### Reinforcement Learning library\r\n\r\n# OpenAI gym is only needed in chapter 18.\r\n# There are a few dependencies you need to install first, check out:\r\n# https://github.com/openai/gym#installing-everything\r\ngym[atari]==0.10.9\r\n\r\n##### Additional utilities\r\n\r\n# Joblib is a set of tools to provide lightweight pipelining\r\njoblib==0.13.2\r\n\r\n# May be useful with Pandas for complex \"where\" clauses (e.g., Pandas\r\n# tutorial).\r\nnumexpr==2.6.9\r\n\r\n# Optional: these libraries can be useful in chapter 3, exercise 4.\r\nnltk==3.4\r\nurlextract==0.9\r\n\r\n# Needed in chapter 19.\r\nrequests==2.22.0\r\n\r\n# Optional: nice utility to diff Jupyter Notebooks.\r\n#nbdime==1.0.5\r\n\r\n# Optional: tqdm displays nice progress bars, ipywidgets for tqdm's notebook support\r\ntqdm==4.31.1\r\nipywidgets==7.4.2\r\n```\r\n\r\n", "comments": ["@vidyabhandary Please find instructions in this [link](https://www.tensorflow.org/install/pip) which help you to install Tensorflow. Let us know how that progresses. Thanks!", "Those are the same commands that are being used. \r\nI have created the virualenv , activated it and then used pip install (via requirements.txt)", "If it helps I get similar error message in the colab environment too - although the version for cloudpickle error is different\r\n\r\nERROR: tf-nightly-2-0-preview 2.0.0.dev20190618 has requirement tb-nightly<1.15.0a0,>=1.14.0a0, but you'll have tb-nightly 1.15.0a20190618 which is incompatible.\r\n\r\nERROR: tfp-nightly 0.8.0.dev20190619 has requirement cloudpickle==1.1.1, but you'll have **cloudpickle 0.6.1** which is incompatible.", "This is an error in your python environment, not TensorFlow.\r\n\r\nYou could `pip install cloudpickle==1.1.1` and see if that works.", "I do not install cloudpickle separately. It is installed as part of tensorflow. And I am using a clean virtual environment every time. Even colab gives similar error when installing from scratch. ", "@vidyabhandary Can you share a gist from colab? Thanks!", "Somehow there are 2 packages in your `requirements.py` requiring two different version of `cloudpickle`\r\n\r\nWill debug this week.", "@mihaimaruseac  - Thanks for the clue. It looks like scikit-image and tfp-nightly both have dependency on cloudpickle.  The other dependency for tb-nightly version still shows an error though. \r\n\r\nERROR: tf-nightly-gpu-2-0-preview 2.0.0.dev20190626 has requirement tb-nightly<1.15.0a0,>=1.14.0a0, but you'll have tb-nightly 1.15.0a20190624 which is incompatible.\r\n\r\n@jvishnuvardhan - I have created a GitHub gist at this path. \r\n\r\nhttps://gist.github.com/vidyabhandary/e9fd7ef120922e90479cd95be05ab889\r\n\r\nI am currently not looking at the errors for ipython 5.5.0, google-colab 1.0.0, datascience 0.10.6 and albumentations since none of them appear in my local environment.", "Assigning to @nfelt for tensorboard versioning changes.", "Actually, @vidyabhandary can you try using `tb_nightly==1.14.0a20190614` in `requirements.py` instead of just `tb-nightly`? Version changed recently as TF 1.14 was launched but the 2.0 branch was not updated (most likely won't be until mid July)", "With that change I no longer get this error - \r\n\r\nERROR: tf-nightly-gpu-2-0-preview 2.0.0.dev20190626 has requirement tb-nightly<1.15.0a0,>=1.14.0a0, but you'll have tb-nightly 1.15.0a20190624 which is incompatible.\r\n\r\nThanks !!", "Thank you for confirming fix. I'm going to close the issue now but feel free to reopen if there's something else to solve here and I missed it"]}, {"number": 29904, "title": "The parameter M is a placeholder type variable. M's dimension is [None, 4]. Because the dimension information is None, it is impossible to use None as a cyclic variable.", "body": "The parameter M is a placeholder type variable. M's dimension is [None, 4]. Because the dimension information is None, it is impossible to use None as a cyclic variable. I need to use None dimension to extract four coordinate information, and then deduct this area from the feature map. What should I do? Thank you very much.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and can provide better and faster help to such issues. Thanks!\r\n"]}, {"number": 29903, "title": "Output of tf.train.piecewise_constant  function returns a TypeError", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0/7.1\r\n- GPU model and memory: Nvidia 2080 8GB\r\n\r\n**Describe the current behavior**\r\nWe are trying to write a multi-step decay function in Tensorflow using tf.train.piecewise_constant() as suggested [here](https://stackoverflow.com/a/47174243/5079359). \r\n\r\nHowever, when we tried running the code, it returned a TypeError. It returns the same error even when lr() is used.\r\n\r\n**Describe the expected behavior**\r\nTensorflow documentation for tf.train.piecewise_constant states that:\r\n\r\n\"When eager execution is enabled, this function returns a function which in turn returns the decayed learning rate Tensor\"\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nimport numpy as np\r\n\r\ndef conv3x3(out_planes, data_format ='channels_last',  stride=1, padding='same', dilation=1, name = None,use_bias = False):\r\n    \"\"\"3x3 convolution with padding\"\"\"\r\n    return  tf.keras.layers.Conv2D(filters = out_planes, kernel_size = 3,data_format= data_format,\r\n                                   strides=(stride, stride), padding='same', use_bias=use_bias,\r\n                                   dilation_rate = (dilation,dilation) , kernel_initializer=tf.initializers.he_normal(),name = name)\r\n\r\n\r\ndef conv1x1(out_planes,data_format ='channels_last', padding = 'same', stride=1):\r\n    \"\"\"1x1 convolution\"\"\"\r\n    return tf.keras.layers.Conv2D(filters = out_planes, kernel_size = 1, strides=(stride, stride),data_format= data_format,\r\n                                  padding=padding, use_bias=False, kernel_initializer=tf.initializers.he_normal())\r\n\r\nclass BasicBlock(tf.keras.Model):\r\n    expansion = 1\r\n\r\n    def __init__(self, planes=1, stride=1, data_format= 'channels_last', downsample=None,  dilation=(1, 1), residual=True, key=None, stage = None):\r\n        super(BasicBlock, self).__init__()\r\n        self.data_format = data_format\r\n        bn_axis = 1 if self.data_format == 'channels_first' else 3\r\n        self.conv1 = conv3x3(out_planes= planes, stride = stride, padding='same' ,\r\n                             data_format = self.data_format, dilation=dilation[0], name = '{}_{}_conv0'.format(key,stage))\r\n\r\n        self.bn1 = tf.keras.layers.BatchNormalization(axis=bn_axis, name = '{}_{}_BN0'.format(key,stage))\r\n\r\n        self.conv2 = conv3x3(out_planes =planes, padding='same',\r\n                             data_format = self.data_format, dilation=dilation[0],name = '{}_{}_conv1'.format(key,stage))\r\n\r\n        self.bn2 = tf.keras.layers.BatchNormalization(axis=bn_axis,name = '{}_{}_BN1'.format(key,stage))\r\n\r\n        self.downsample = downsample\r\n        self.relu = tf.keras.layers.ReLU(name = '{}_{}_Relu'.format(key,stage))\r\n        self.stride = stride\r\n        self.residual = residual\r\n\r\n    def get_config(self):\r\n        base_config = {}\r\n        base_config['conv1'] = self.conv1.get_config()\r\n        base_config['bn1'] = self.bn1.get_config()\r\n        base_config['conv2'] = self.conv2.get_config()\r\n        base_config['bn2'] = self.bn2.get_config()\r\n        if self.downsample is not None:\r\n            base_config['downsample'] = self.downsample.get_config()\r\n        return base_config\r\n\r\n\r\n    def call(self, inputs, training=None):\r\n        residual = inputs\r\n        out = self.conv1(inputs)\r\n        out = self.bn1(out,training = training)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(inputs)\r\n        if self.residual:\r\n            out += residual\r\n        out = self.relu(out)\r\n        return out\r\n\r\n\r\nclass Bottleneck(tf.keras.Model):\r\n    expansion = 4\r\n\r\n    def __init__(self, planes, stride=1, data_format = 'channels_last',downsample=None,dilation=(1, 1)):\r\n        super(Bottleneck, self).__init__()\r\n\r\n        bn_axis = 1 if data_format == 'channels_first' else 3\r\n        self.conv1 = conv1x1(planes, data_format = data_format)\r\n        self.bn1 = tf.keras.layers.BatchNormalization(axis=bn_axis)\r\n        self.relu = tf.keras.layers.ReLU()\r\n        self.conv2 = conv3x3(planes, stride, padding= 'same', bias=False,  data_format = data_format, dilation=dilation[1])\r\n        self.bn2 = tf.keras.layers.BatchNormalization(axis=bn_axis)\r\n        self.conv3 =conv1x1( planes * 4, data_format = data_format, )\r\n        self.bn3 =  tf.keras.layers.BatchNormalization(axis=bn_axis) # nn.BatchNorm2d(planes * self.expansion)\r\n        self.downsample = downsample\r\n        self.stride = stride\r\n\r\n    def get_config(self):\r\n        base_config = {}\r\n        base_config['conv1'] = self.conv1.get_config()\r\n        base_config['bn1'] = self.bn1.get_config()\r\n        base_config['conv2'] = self.conv2.get_config()\r\n        base_config['bn2'] = self.bn2.get_config()\r\n        base_config['conv3'] = self.conv3.get_config()\r\n        base_config['bn3'] = self.bn3.get_config()\r\n        if self.downsample is not None:\r\n            base_config['downsample'] = self.downsample.get_config()\r\n        return base_config\r\n\r\n\r\n\r\n    def call(self, inputs, training=None):\r\n        identity = inputs\r\n        out = self.conv1(inputs)\r\n        out = self.bn1(out,training = training)\r\n        out = self.relu(out)\r\n        out = self.conv2(out)\r\n        out = self.bn2(out,training = training)\r\n        out = tf.nn.relu(out)\r\n        out = self.conv3(out)\r\n        out = self.bn3(out,training = training)\r\n        if self.downsample is not None:\r\n            identity = self.downsample(inputs)\r\n        out += identity\r\n        out = self.relu(out)\r\n        return out\r\n\r\nclass pooling (tf.keras.Model):\r\n    def __init__(self, pool_size, stride = None, data_format='channels_last'):\r\n        super(pooling, self).__init__()\r\n        self.pool_size = pool_size\r\n        self.data_format = data_format\r\n        if stride is None:\r\n            self.stride =self.pool_size\r\n        else:\r\n            self.stride = stride\r\n\r\n\r\n    def call(self, inputs):\r\n        return tf.layers.average_pooling2d(inputs, strides =self.stride, pool_size = self.pool_size, data_format = self.data_format)\r\n\r\n\r\nclass DRN(tf.keras.Model):\r\n    def __init__(self, block, layers, data_format='channels_last', num_classes=7,channels=(16, 32, 64, 128, 256, 512, 512, 512),\r\n                 out_map=False, out_middle=False, pool_size=28, arch='D'):\r\n        super(DRN, self).__init__()\r\n        self.inplanes = channels[0]\r\n        self.out_map = out_map\r\n        self.out_dim = channels[-1]\r\n        self.out_middle = out_middle\r\n        self.arch = arch\r\n        self.poolsize = pool_size\r\n        self.data_format = data_format\r\n        self.bn_axis = 1 if data_format == 'channels_first' else 3\r\n\r\n        self.conv0 = tf.keras.layers.Conv2D(filters=channels[0], kernel_size=7, strides=1,  padding='same',\r\n                                               use_bias=False, data_format = self.data_format, kernel_initializer=tf.initializers.he_normal(), name ='L0_conv0' )\r\n        self.bn0 = tf.keras.layers.BatchNormalization(axis=self.bn_axis,name ='L0_BN0')\r\n        self.relu0 = tf.keras.layers.ReLU(name ='L0_Relu0')\r\n\r\n\r\n        if arch == 'C':\r\n            self.layer1 = self._make_layer(block = BasicBlock, planes = channels[0], blocks = layers[0], stride=1, data_format = self.data_format, key='CL1')\r\n            self.layer2 = self._make_layer(block = BasicBlock, planes =  channels[1], blocks = layers[1], stride=2, data_format = self.data_format, key='CL2')\r\n        elif arch == 'D':\r\n            self.layer1 = self._make_conv_layers(channels = channels[0],convs = layers[0], stride=1, data_format = self.data_format, key='DL1')\r\n            self.layer2 = self._make_conv_layers(channels = channels[1],convs = layers[1], stride=2, data_format = self.data_format, key='DL2')\r\n\r\n\r\n        self.layer3 = self._make_layer(block = block, planes = channels[2], blocks = layers[2], stride=2, data_format = self.data_format, key='L3')\r\n        self.layer4 = self._make_layer(block = block, planes = channels[3], blocks = layers[3], stride=2, data_format = self.data_format, key='L4')\r\n        self.layer5 = self._make_layer(block = block, planes = channels[4], blocks = layers[4], dilation=2, new_level=False, data_format = self.data_format, key='L5')\r\n        self.layer6 = None if layers[5] == 0 else self._make_layer(block, channels[5], layers[5], dilation=4, new_level=False, data_format = self.data_format, key='L6')\r\n\r\n        if arch == 'C':\r\n            self.layer7 = None if layers[6] == 0 else self._make_layer(BasicBlock, channels[6], layers[6], dilation=2, new_level=False, residual=False, data_format = self.data_format, key='CL7')\r\n            self.layer8 = None if layers[7] == 0 else self._make_layer(BasicBlock, channels[7], layers[7], dilation=1, new_level=False, residual=False, data_format = self.data_format, key='CL8')\r\n        elif arch == 'D':\r\n            self.layer7 = None if layers[6] == 0 else self._make_conv_layers(channels[6], layers[6], dilation=2, data_format = self.data_format, key='DL7')\r\n            self.layer8 = None if layers[7] == 0 else self._make_conv_layers(channels[7], layers[7], dilation=1, data_format = self.data_format, key='DL8')\r\n\r\n        if num_classes > 0:\r\n            self.avgpool = tf.keras.layers.GlobalAveragePooling2D(data_format = self.data_format)\r\n            self.fc = tf.keras.layers.Dense(units=num_classes)\r\n\r\n\r\n    def _make_layer(self, block, planes, blocks, stride=1,dilation=1, new_level=True, data_format = 'channels_last', residual=True, key=None):\r\n        assert dilation == 1 or dilation % 2 == 0\r\n        downsample = None\r\n        if stride != 1 or self.inplanes != planes * block.expansion:\r\n            downsample = tf.keras.Sequential([conv1x1(out_planes = planes * block.expansion,stride = stride, data_format = data_format),\r\n                      tf.keras.layers.BatchNormalization(axis=self.bn_axis)], name = 'downsample')\r\n\r\n\r\n        layers = []\r\n        layers.append(block(planes= planes, stride =  stride, downsample = downsample, dilation=(1, 1) if dilation == 1 else (\r\n                dilation // 2 if new_level else dilation, dilation), data_format=data_format, residual=residual, key = key, stage = '0'))\r\n        self.inplanes = planes * block.expansion\r\n        for i in range(1, blocks):\r\n            layers.append(block(planes, residual=residual,dilation=(dilation, dilation), data_format=data_format, key = key, stage = i))\r\n        return tf.keras.Sequential(layers, name = key)\r\n\r\n\r\n    def _make_conv_layers(self, channels, convs, stride=1, dilation=1 ,data_format = 'channels_last', key = None):\r\n        modules = []\r\n        for i in range(convs):\r\n            modules.extend([\r\n                conv3x3(out_planes= channels, stride=stride if i == 0 else 1,\r\n                          padding= 'same' , use_bias=False, dilation=dilation,  data_format = data_format,name ='{}_{}_Conv'.format(key,i)),\r\n                tf.keras.layers.BatchNormalization(axis=self.bn_axis,name ='{}_{}_BN'.format(key,i)),\r\n                tf.keras.layers.ReLU(name ='{}_{}_Relu'.format(key,i))])\r\n            self.inplanes = channels\r\n        return tf.keras.Sequential(modules,name=key)\r\n\r\n\r\n    def call(self, x, training=None):\r\n        x = self.conv0(x)\r\n        x = self.bn0(x,training = training)\r\n        x = self.relu0(x)\r\n        x = self.layer1(x,training = training)\r\n        x = self.layer2(x,training = training)\r\n        x = self.layer3(x,training = training)\r\n        x = self.layer4(x,training = training)\r\n        x = self.layer5(x,training = training)\r\n\r\n        if self.layer6 is not None:\r\n            x = self.layer6(x,training = training)\r\n\r\n        if self.layer7 is not None:\r\n            x = self.layer7(x)\r\n        if self.layer8 is not None:\r\n            x = self.layer8(x)\r\n        if self.out_map:\r\n            x = self.fc(x)\r\n        else:\r\n            x = self.avgpool(x)\r\n            x = self.fc(x)\r\n        return x\r\n\r\ndef loss(logits, labels):\r\n  return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\r\n\r\ndef make_scheduler(policy, init_lr, n_step_epoch, global_step):\r\n    total_steps= n_step_epoch * 10 #10 epochs\r\n    milestones = policy.split('_')\r\n    milestones.pop(0)\r\n    milestones = list(map(lambda x: int(x), milestones))\r\n    boundaries = np.multiply(milestones,n_step_epoch)\r\n    values = [init_lr] + [init_lr/(0.1**-i) for i in  range(1,len(milestones)+1)]\r\n    learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\r\n    return learning_rate\r\n\r\n\r\ndef train(model, optimizer, step_counter ):\r\n  \"\"\"Trains model on `dataset` using `optimizer`.\"\"\"\r\n  \r\n  for (batch, i) in enumerate(range(10)):\r\n      print('Training Loop {}'.format(i))\r\n      images = tf.random.uniform((4, 224, 224,3))\r\n      labels = tf.constant(np.random.randint(4, size=4))\r\n      with tf.contrib.summary.record_summaries_every_n_global_steps(10, global_step=step_counter):\r\n          with tf.GradientTape() as tape:\r\n            logits = model(images, training=True)\r\n            loss_value = loss(logits, labels)\r\n          grads = tape.gradient(loss_value, model.variables)\r\n          optimizer.apply_gradients(zip(grads, model.variables), global_step=step_counter)\r\n\r\n\r\ndef test(model):\r\n  \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\r\n  for  i in (range(10)):\r\n    images = tf.random.uniform((4, 225, 225,3))\r\n    logits = model(images, training=False)\r\n    print(logits)\r\n\r\ndef main():\r\n    model =  DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch='C',num_classes = 4)\r\n    device = '/gpu:0'\r\n    step_counter = tf.train.get_or_create_global_step()\r\n    lr = make_scheduler(policy='multistep_2_5',init_lr=0.1,n_step_epoch = 10,global_step= step_counter)\r\n    optimizer = tf.train.MomentumOptimizer(lr,momentum=0.5)\r\n    \r\n    with tf.device(device):\r\n        for _ in range(10):\r\n           train(model, optimizer,step_counter)\r\n           print(optimizer._lr_t)\r\n           test(model)\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n\r\n```\r\n\r\n**Other info / logs**\r\n\r\n>File \"<ipython-input-1-666c765cdffd>\", line 1, in <module>\r\n    runfile('/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug/stackoverflow.py', wdir='/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug')\r\n\r\n>  File \"/home/srijith/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py\", line 709, in runfile\r\n    execfile(filename, namespace)\r\n\r\n > File \"/home/srijith/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py\", line 108, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n > File \"/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug/stackoverflow.py\", line 311, in <module>\r\n    main()\r\n\r\n > File \"/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug/stackoverflow.py\", line 305, in main\r\n    train(model, optimizer,step_counter)\r\n\r\n>  File \"/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug/stackoverflow.py\", line 284, in train\r\n    optimizer.apply_gradients(zip(grads, model.variables), global_step=step_counter)\r\n\r\n > File \"/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py\", line 598, in apply_gradients\r\n    self._prepare()\r\n\r\n>  File \"/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/momentum.py\", line 87, in _prepare\r\n    learning_rate = learning_rate()\r\n\r\n>  File \"/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/learning_rate_decay_v2.py\", line 171, in decayed_lr\r\n    boundaries = ops.convert_n_to_tensor(boundaries)\r\n\r\n > File \"/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1273, in convert_n_to_tensor\r\n    as_ref=False)\r\n\r\n > File \"/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in internal_convert_n_to_tensor\r\n    raise TypeError(\"values must be a list.\")\r\n\r\n>TypeError: values must be a list.\r\n\r\nThe code works as expected when we provide a constant learning rate. Is there something that we are missing?", "comments": ["I have reproduced the issue with with Tensorflow GPU 1.13.1 on Colab.Thanks", "@srijithrajeev Can you provide simplified standalone code to reproduce the issue? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29903\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29903\">No</a>\n"]}, {"number": 29902, "title": "InvalidArgumentError (see above for traceback): max_x must be larger than min_x", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: **NO**\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: **Ubuntu 14.04**\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NO\r\n- **TensorFlow installed from (source or binary)**: Source \r\n- **TensorFlow version (use command below)**: **r.12**\r\n- **Python version**: **2.7.12**\r\n- **Bazel version (if compiling from source)**: **0.15.0**\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nI have reinforcement learning model which I am trying to Quantize using transform_graph. Transform graph generates quantized graph but this quantized graph returns an error while running.\r\nError: Max_x for QuantizedAdd must be larger than min_x.\r\n\r\nReinforcement Learning model runs OK without quantization. I tried converting to tflite, but tflite doesn't support some of the ops used in the RL model.\r\n\r\nIs there a workaround for max_x and min_x checks. \r\n\r\n### Source code / logs\r\nInvalidArgumentError (see above for traceback): max_x must be larger than min_x.\r\n         [[Node: add_1/eightbit = QuantizedAdd[T1=DT_QUINT8, T2=DT_QUINT8, Toutput=DT_QINT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](add_1_eightbit/truediv/quantize, add_eightbit/add_1/y/quantize, add_1_eightbit/truediv/quantize:1, add_1_eightbit/truediv/quantize:2, add_eightbit/add_1/y/quantize:1, add_eightbit/add_1/y/quantize:2)]]\r\n         [[Node: add_1/_97 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_312_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]] ", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29902\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29902\">No</a>\n"]}, {"number": 29901, "title": "Refactor {TensorSlice, Window, Zip, & Batch} DatasetOps", "body": "This PR refactors `TensorSliceDatasetOp`, `WindowDatasetOp`, `ZipDatasetOp`, and `BatchDatasetOp`.\r\n\r\ncc: @jsimsa ", "comments": ["@jsimsa Thanks for your quick review! The comments are addressed by this commit(https://github.com/tensorflow/tensorflow/pull/29901/commits/1e597ff3b5c8b1fafdeece3f7b4cf6c74fa9e70b). Could you please have a look at the changes when you have time? \r\n\r\n---------\r\nAfter checking `dataset_utils`, it seems better to keep them in `window_dataset.h` and `window_dataset.cc`, so I change them back to be as same as they are before.  Please feel free to let me know if you want to move them anywhere.\r\n", "@jsimsa Thanks for pointing out these issues. Yeah, some APIs in name_utils (e.g. `OpName()` and `DatasetDebugString()`) need to be changed to be more robust and consistent.\r\n\r\nDifferent cases I met are summarized below:\r\n\r\n### 1. Normal case\r\n\r\n- `OpName(\"Range\" ... )` -> `\"RangeDataset\"`\r\n\r\n- `DatasetDebugString(\"Map\" ... )` -> `\"MapDatasetOp::Dataset\"`;\r\n\r\n### 2. Different versions of Ops with the same dataset kernel \r\n\r\nIn this case, there is only one dataset op class and `kDatasetType` will be the same.\r\n\r\n`op_version_` is used internally for different versions of Ops with different inputs/attributes defined in `REGISTER_OP()`.\r\n\r\n- `OpName(\"Batch\" ...)` -> `\"BatchDataset\"` when `op_version == 1`;  `\"BatchDatasetV2\"` when `op_version == 2`;\r\n\r\n- `DatasetDebugString(\"Batch\" ... )` -> `\"BatchDatasetOp::Dataset\"` when `op_version == 1`;  `\"BatchDatasetV2Op::Dataset\"` when `op_version == 2`;\r\n\r\n### 3. Different versions of Ops with different dataset kernels\r\n\r\nIn this case, there are multiple dataset op classes, and their `kDatasetType` will be different. For example `ParallelInterleaveDatasetOp::kDatasetType = \"ParallelInterleaveV2\"`, but the ParallelInterleave dataset op under the experimental directory will be `experimental::ParallelInterleaveDatasetOp::kDatasetType = \"ParallelInterleave\"`. \r\n\r\nDo you think it will be better to make their `kDatasetType` same and add the member `op_version_` to differentiate them, which will be consistent with the second category?\r\n\r\n- `OpName(\"ParallelInterleave\" ... )` -> `\"ParallelInterleaveDataset\" `\r\n\r\n- `OpName(\"ParallelInterleaveV2\" ... )` -> \"ParallelInterleaveDatasetV2\"\r\n\r\n- `DatasetDebugString(\"ParallelInterleave\" ...)` -> \"ParallelInterleaveDatasetOp::Dataset\"\r\n\r\n- `DatasetDebugString(\"ParallelInterleaveV2\" ...)` -> \"ParallelInterleaveDatasetV2Op::Dataset\"\r\n\r\n### 4. Different dataset prefix for DebugString\r\n\r\n- `DatasetDebugString(\"Shuffle\" ...)` -> `\"ShuffleDatasetOp(arg1, arg2,..., argn)::ReshufflingDataset\"` or `\"ShuffleDatasetOp(arg1, arg2,..., argn)::FixedSeedDataset\"` \r\n\r\nFor the above cases, we need to handle: 1) `kDatasetType` may have the version number, e.g. the above third category; 2) `OpName()` needs to support different `op_version_`, e.g. the above second category; 3) `DatasetDebugString()` needs to support the dataset prefix, e.g. the fourth category.\r\n\r\n### The proposed APIs\r\n\r\n1. `OpName(const string& dataset_type, int op_version)`\r\n\r\n-  `OpName(RangeDatasetOp::kDatasetType, 1)` -> `\"RangeDataset\"`\r\n-  `OpName(BatchDatasetOp::kDatasetType, 1)` -> `\"BatchDataset\"`\r\n-  `OpName(BatchDatasetOp::kDatasetType, 2)` -> `\"BatchDatasetV2\"`\r\n-  `OpName(ParallelInterleaveDatasetOp::kDatasetType=\"ParallelInterleave\", 2)` -> `\"ParallelInterleaveDatasetV2\"`\r\n-  `OpName(experimental::ParallelInterleaveDatasetOp::kDatasetType=\"ParallelInterleave\", 1)` -> `\"ParallelInterleaveDataset\"`\r\n\r\nNote that here `ParallelInterleaveDatasetOp::kDatasetType` will be same with `experimental::ParallelInterleaveDatasetOp::kDatasetType`, and `op_version` is used to differentiate them.\r\n\r\n2. `DatasetDebugString(const string& op_name,  \r\n                                        const string& dataset_prefix,\r\n                                        const Args&... args)` -> `strings::StrCat(op_name, kOp, args_str, kDelimiter, dataset_prefix, kDataset)`\r\n\r\n\r\n\r\nPlease feel free to revise/comment!\r\n", "I suggest that we create the following API:\r\n\r\n```\r\nstruct OpNameParams {  \r\n  int op_version = 1;\r\n}\r\n\r\nOpName(const string& dataset_type) {\r\n  OpName(dataset_type, OpNameParams());\r\n}\r\n\r\nOpName(const string& dataset_type, OpNameParams params) {\r\n  ... // the actual implementation\r\n}\r\n\r\nstruct DatasetDebugStringParams {\r\n  std::vector<strings::AlphaNum> args;\r\n  string dataset_prefix = \"\";\r\n  int op_version = 1;\r\n}\r\n\r\nDatasetDebugString(const string& dataset_type) {\r\n  DatasetDebugString(dataset_type, DatasetDebugStringParams());\r\n}\r\n\r\nDatasetDebugString(const string& dataset_type, DatasetDebugStringParams params) {\r\n  ... // the actual implementation\r\n}\r\n```\r\n\r\nCall sites that do not need to override the defaults simply do `name_utils::OpName(kDatasetType)`. Call sites that wish to override the defaults will do:\r\n\r\n```\r\nname_utils::OpNameParams params = name_utils::OpNameParams();\r\nparams.op_version = op_version_;\r\nname_utils::OpName(kDatasetType, std::move(params));\r\n```\r\n\r\nor \r\n\r\n```\r\nname_utils::DatasetDebugStringParams params = name_utils::DatasetDebugStringParams();\r\nparams.op_version = op_version_;\r\nparams.dataset_prefix = kFixedSeed;\r\nparams.args = {\"hello\", 1, 2.0};\r\nname_utils::DatasetDebugString(kDatasetType, std::move(params));\r\n```\r\n\r\nThis will make it possible to evolve the set of optional parameters without having to update existing call site (once this proposal is implemented) or introducing new API endpoints.", "@jsimsa Thanks for the detailed and great suggestions! I will submit another PR to updates the APIs in name_utils.\r\n\r\nFor the value of `ParallelInterleaveDatasetOp::kDatasetType`, which one do you think is better (`ParallelInterleave` or `ParallelInterleaveV2`)?  ", "`ParallelInterleave` and use the params object to set the op version to 2.", "> `ParallelInterleave` and use the params object to set the op version to 2.\r\n\r\nGot it. Will change that as well.", "@jsimsa This PR has been revised with the new APIs of name_utils. Please have a look at the changes (https://github.com/tensorflow/tensorflow/pull/29901/commits/17ad3b8493c0e1b2d2831fdd4776d436d9c1a7f0) when you have time!", "@jsimsa `IteratorPrefixParams` has been revised by removing the `prefix` parameter. Please have another look (https://github.com/tensorflow/tensorflow/pull/29901/commits/483d93a8f403cdaf0d253e3d103ec150d87cdcff)!", "This breaks `//tensorflow/python/data/experimental/kernel_tests/optimization:map_vectorization_test` with:\r\n\r\n`InvalidArgumentError: Asserted Batch transformation at offset 1 but encountered BatchV2 transformation instead.`\r\n\r\nPlease fix the test to reflect the change in the batch iterator prefix. Thanks.", "@jsimsa Thanks for checking the internal tests! The MapVectorization tests have been updated to match the change in the batch iterator prefix. Please take a look at the change (https://github.com/tensorflow/tensorflow/pull/29901/commits/7784088481b7e0ebafc83413d657e1bb9e4af141).", "@jsimsa Sorry that there was a format issue in the last commit (the line length exceeds 80 characters). This commit (https://github.com/tensorflow/tensorflow/pull/29901/commits/4535e116e87294fc6e042e6e01d7444a2978f43a) is submitted to fix it. Could you please take a look?", "@jsimsa Thanks for your help on this PR! As the code has been merged, I close this PR."]}, {"number": 29900, "title": "Support INT32 for calibration plus tests", "body": "", "comments": []}, {"number": 29899, "title": "Update RELEASE.md", "body": "Update RELEASE.md with contributors and relnotes not included in the original due to the staggered release.", "comments": []}, {"number": 29898, "title": "Need example of mixed precision in eager execution mode", "body": "https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/train/experimental/enable_mixed_precision_graph_rewrite\r\n\r\nhttps://www.tensorflow.org/versions/r1.14/api_docs/python/tf/train/experimental/MixedPrecisionLossScaleOptimizer\r\n\r\nhttps://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer\r\n\r\nI have tried all of them. None of them worked for me in eager mode. Could you provide some examples?", "comments": ["Please have a look on #29241. PR for \"enable_mixed_precision_graph_rewrite\" is in queue. Thanks!", "Sorry, my mistake.\r\n\r\nThe **tf.keras.Model.compile** and **tf.keras.Model.fit** is still able to use in Eager Execution Mode.\r\n\r\nClose this issue now."]}, {"number": 29897, "title": "[TF 2.0 API Docs] tf.sets.difference", "body": "## System Information\r\n\r\nTensorFlow version: 2.0\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/sets/difference\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Raises listed and defined\r\n\r\nRaises not listed and defined.\r\nEvery method has a way that it can be mishandled, maybe when a wrong parameter or wrong order of parameters is/are passed in ( e.g, in this case, two sets `a` and `b` in which the last elements don't match) will raise an error.\r\n\r\n### Request visuals, if applicable\r\n\r\nNo visuals\r\n\r\n### Submit a pull request?\r\n\r\nNo\r\n", "comments": ["Added a PR #29938 for the fix."]}, {"number": 29896, "title": "Keras Colab TPU Error when compiling and fitting a pre-trained model in 1.14", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen doing transfer learning using pre-trained keras model (Xception per say) with the imagenet weights and adding a classification layer there is an error when fitting. If you use no weights there is no error during fitting, but if you fine tune the model, re-compile it and fit again the same error pops.\r\n\r\n**Describe the expected behavior**\r\nNo error, it was working in 1.13. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nThe easiest way to reproduce the problem is using the official notebook : \r\nhttps://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb\r\nand re-compiling and fitting a second time \r\n\r\n```\r\nimport os\r\n\r\nresolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.contrib.distribute.initialize_tpu_system(resolver)\r\nstrategy = tf.contrib.distribute.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n  model = create_model()\r\n  model.compile(\r\n      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\r\n      loss='sparse_categorical_crossentropy',\r\n      metrics=['sparse_categorical_accuracy'])\r\n\r\nmodel.fit(\r\n    x_train.astype(np.float32), y_train.astype(np.float32),\r\n    epochs=17,\r\n    steps_per_epoch=60,\r\n    validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),\r\n    validation_freq=17\r\n)\r\n\r\n##This part was added##\r\nprint('Fine tuning')\r\n\r\nwith strategy.scope():\r\n  model.compile(\r\n      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\r\n      loss='sparse_categorical_crossentropy',\r\n      metrics=['sparse_categorical_accuracy'])\r\n\r\nmodel.fit(\r\n    x_train.astype(np.float32), y_train.astype(np.float32),\r\n    epochs=17,\r\n    steps_per_epoch=60,\r\n    validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),\r\n    validation_freq=17\r\n)\r\n\r\n\r\nmodel.save_weights('./fashion_mnist.h5', overwrite=True)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nAnd it leads to this error\r\nW0617 21:41:40.701483 140301503657856 tpu_strategy_util.py:56] TPU system %s has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\r\nEpoch 1/17\r\n60/60 [==============================] - 5s 81ms/step - loss: 1.0900 - sparse_categorical_accuracy: 0.6855\r\nEpoch 2/17\r\n60/60 [==============================] - 1s 24ms/step - loss: 0.5287 - sparse_categorical_accuracy: 0.8202\r\nEpoch 3/17\r\n60/60 [==============================] - 2s 25ms/step - loss: 0.4317 - sparse_categorical_accuracy: 0.8518\r\nEpoch 4/17\r\n60/60 [==============================] - 1s 25ms/step - loss: 0.3728 - sparse_categorical_accuracy: 0.8692\r\nEpoch 5/17\r\n60/60 [==============================] - 1s 25ms/step - loss: 0.3453 - sparse_categorical_accuracy: 0.8776\r\nEpoch 6/17\r\n60/60 [==============================] - 1s 24ms/step - loss: 0.3080 - sparse_categorical_accuracy: 0.8898\r\nEpoch 7/17\r\n60/60 [==============================] - 1s 24ms/step - loss: 0.2892 - sparse_categorical_accuracy: 0.8954\r\nEpoch 8/17\r\n60/60 [==============================] - 1s 24ms/step - loss: 0.2641 - sparse_categorical_accuracy: 0.9044\r\nEpoch 9/17\r\n60/60 [==============================] - 1s 25ms/step - loss: 0.2485 - sparse_categorical_accuracy: 0.9093\r\nEpoch 10/17\r\n60/60 [==============================] - 1s 24ms/step - loss: 0.2337 - sparse_categorical_accuracy: 0.9135\r\nEpoch 11/17\r\n60/60 [==============================] - 1s 25ms/step - loss: 0.2236 - sparse_categorical_accuracy: 0.9170\r\nEpoch 12/17\r\n60/60 [==============================] - 1s 25ms/step - loss: 0.2081 - sparse_categorical_accuracy: 0.9232\r\nEpoch 13/17\r\n60/60 [==============================] - 1s 25ms/step - loss: 0.1962 - sparse_categorical_accuracy: 0.9281\r\nEpoch 14/17\r\n60/60 [==============================] - 2s 25ms/step - loss: 0.1816 - sparse_categorical_accuracy: 0.9318\r\nEpoch 15/17\r\n60/60 [==============================] - 1s 24ms/step - loss: 0.1717 - sparse_categorical_accuracy: 0.9355\r\nEpoch 16/17\r\n60/60 [==============================] - 1s 24ms/step - loss: 0.1666 - sparse_categorical_accuracy: 0.9375\r\nEpoch 17/17\r\n10/10 [==============================] - 7s 720ms/step\r\n10/10 [==============================] - 7s 720ms/step\r\n60/60 [==============================] - 13s 216ms/step - loss: 0.1535 - sparse_categorical_accuracy: 0.9424 - val_loss: 0.2364 - val_sparse_categorical_accuracy: 0.9235\r\nFine tuning\r\nEpoch 1/17\r\n\r\nNotFoundErrorTraceback (most recent call last)\r\n<ipython-input-5-ca82800c4c4f> in <module>()\r\n     33     steps_per_epoch=60,\r\n     34     validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),\r\n---> 35     validation_freq=17\r\n     36 )\r\n     37 \r\n\r\n7 frames\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    647             steps_per_epoch=steps_per_epoch,\r\n    648             validation_steps=validation_steps,\r\n--> 649             validation_freq=validation_freq)\r\n    650 \r\n    651     batch_size = self._validate_or_infer_batch_size(\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.pyc in fit_distributed(model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\r\n    126         steps_per_epoch=steps_per_epoch,\r\n    127         validation_steps=validation_steps,\r\n--> 128         validation_freq=validation_freq)\r\n    129   else:\r\n    130     return training_arrays.fit_loop(\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.pyc in experimental_tpu_fit_loop(model, dataset, epochs, verbose, callbacks, initial_epoch, steps_per_epoch, val_dataset, validation_steps, validation_freq)\r\n    412         prev_step_count = step_count\r\n    413       try:\r\n--> 414         _, outputs = K.batch_get_value([train_op, output_tensors])\r\n    415       except errors.OutOfRangeError:\r\n    416         logging.warning('Your dataset iterator ran out of data; '\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/backend.pyc in batch_get_value(tensors)\r\n   3008     raise RuntimeError('Cannot get value inside Tensorflow graph function.')\r\n   3009   if tensors:\r\n-> 3010     return get_session(tensors).run(tensors)\r\n   3011   else:\r\n   3012     return []\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    948     try:\r\n    949       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 950                          run_metadata_ptr)\r\n    951       if run_metadata:\r\n    952         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1171     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1172       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1173                              feed_dict_tensor, options, run_metadata)\r\n   1174     else:\r\n   1175       results = []\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1348     if handle is None:\r\n   1349       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1350                            run_metadata)\r\n   1351     else:\r\n   1352       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1368           pass\r\n   1369       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1370       raise type(e)(node_def, op, message)\r\n   1371 \r\n   1372   def _extend_graph(self):\r\n\r\nNotFoundError: From /job:worker/replica:0/task:0:\r\nResource worker/batch_normalization_3_1/moving_mean/replica_7/N10tensorflow3VarE does not exist.\r\n\t [[node TPUReplicateMetadata_5 (defined at <ipython-input-5-ca82800c4c4f>:35) ]]\r\n\r\nOriginal stack trace for u'TPUReplicateMetadata_5':\r\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-5-ca82800c4c4f>\", line 35, in <module>\r\n    validation_freq=17\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.py\", line 649, in fit\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 128, in fit_distributed\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 367, in experimental_tpu_fit_loop\r\n    initial_loop_values=initial_loop_values)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1501, in experimental_run_steps_on_iterator\r\n    initial_loop_values)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/tpu_strategy.py\", line 416, in _experimental_run_steps_on_iterator\r\n    replicate_outputs = rewrite_fn()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/tpu_strategy.py\", line 397, in rewrite_fn\r\n    replicate_outputs = tpu.replicate(run_fn, replicate_inputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tpu/tpu.py\", line 592, in replicate\r\n    maximum_shapes=maximum_shapes)[1]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tpu/tpu.py\", line 854, in split_compile_and_replicate\r\n    num_replicas=num_replicas, use_tpu=use_tpu, **metadata_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_tpu_ops.py\", line 6039, in tpu_replicate_metadata\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()", "comments": ["Was able to reproduce the reported issue on Colab with Tensorflow 1.14.0-rc1. Thanks!", "I had a similar issue where when trying to fine-tune a pre-trained Keras model on a Colab TPU (using `model.load_weights()` to load the pre-trained weights), I got an error message saying \"Resource worker/batch_normalization_1/moving_mean/replica_5/N10tensorflow3VarE does not exist\".\r\n\r\n```\r\nwith strategy.scope():\r\n  model = create_model()\r\n  model.load_weights('fashion_mnist.h5')\r\n  model.compile(\r\n      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\r\n      loss='sparse_categorical_crossentropy',\r\n      metrics=['sparse_categorical_accuracy'])\r\n\r\n# Got `Resource worker/batch_normalization_10/moving_mean/replica_7/N10tensorflow3VarE does not exist` error after calling model.fit()\r\n```\r\n\r\nHowever, this issue was fixed when I changed my code to call `model.load_weights()` after `model.compile()` (rather than before), like so:\r\n\r\n```\r\nwith strategy.scope():\r\n  model = create_model()\r\n  model.compile(\r\n      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\r\n      loss='sparse_categorical_crossentropy',\r\n      metrics=['sparse_categorical_accuracy'])\r\n  model.load_weights('fashion_mnist.h5') # Load weights after model.compile()\r\n\r\n# model trains without issue\r\n```\r\n", "@horacejlee Can you share a standalone code to reproduce the issue? Thanks!", "If you run this code below on the official notebook (https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb), you should be able to reproduce the issue:\r\n\r\n```\r\nimport os\r\n\r\nresolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.contrib.distribute.initialize_tpu_system(resolver)\r\nstrategy = tf.contrib.distribute.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n  model = create_model()\r\n  model.compile(\r\n      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\r\n      loss='sparse_categorical_crossentropy',\r\n      metrics=['sparse_categorical_accuracy'])\r\n\r\nmodel.fit(\r\n    x_train.astype(np.float32), y_train.astype(np.float32),\r\n    epochs=17,\r\n    steps_per_epoch=60,\r\n    validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),\r\n    validation_freq=17\r\n)\r\n\r\nmodel.save_weights('./fashion_mnist.h5', overwrite=True)\r\n\r\n# ----- This part was added -----\r\nprint('Fine tuning')\r\n\r\n# If I call model.load_weights() after model.compile() (see below), there should be no error during training.\r\nwith strategy.scope():\r\n  model = create_model()\r\n  model.compile(\r\n      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\r\n      loss='sparse_categorical_crossentropy',\r\n      metrics=['sparse_categorical_accuracy'])\r\n  model.load_weights('fashion_mnist.h5') # Calling model.load_weights() after model.compile()\r\n\r\nmodel.fit(\r\n    x_train.astype(np.float32), y_train.astype(np.float32),\r\n    epochs=17,\r\n    steps_per_epoch=60,\r\n    validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),\r\n    validation_freq=17\r\n)\r\n\r\n\r\nprint('Fine tuning (error version)')\r\n\r\n# If I call model.load_weights() before model.compile(), then\r\n# model.fit() raises an error like this: \"Resource worker/batch_normalization_6/moving_mean/replica_1/N10tensorflow3VarE does not exist\"\r\n\r\nwith strategy.scope():\r\n  model = create_model()\r\n  model.load_weights('fashion_mnist.h5')\r\n  model.compile(\r\n      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\r\n      loss='sparse_categorical_crossentropy',\r\n      metrics=['sparse_categorical_accuracy'])\r\n\r\nmodel.fit(\r\n    x_train.astype(np.float32), y_train.astype(np.float32),\r\n    epochs=17,\r\n    steps_per_epoch=60,\r\n    validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),\r\n    validation_freq=17\r\n)\r\n```\r\n", "I can confirm this issue. In my case I was getting the same error, when trying to specify weights for an Embedding layer, eg:\r\n\r\n```python\r\nwith strategy.scope():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Embedding(..., weights = [embedding_matrix], ...),\r\n        ...\r\n    ])\r\n    model.compile(...)\r\n```\r\n\r\nIf I specified them after compiling the model, the code would run fine, eg:\r\n```python\r\nwith strategy.scope():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Embedding(...),\r\n        ...\r\n    ])\r\n    model.compile(...)\r\n    model.layers[0].set_weights([embedding_matrix])\r\n```\r\n\r\n@jvishnuvardhan are you happy with the example provided above, or do you still want a standalone example to reproduce the issue?", "@dimitry-ishenko , sorry to post this here, but i am not able to get help anywhere, i am getting a different error when i try to run this\r\n\r\n                import pprint\r\n\t\tuse_tpu = True #@param {type:\"boolean\"}\r\n\r\n\t\tif use_tpu:\r\n\t\t    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\r\n\r\n\t\tif 'COLAB_TPU_ADDR' in os.environ:\r\n\t\t  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\r\n\t\telse:\r\n\t\t  TF_MASTER=''\r\n\r\n\t\twith tf.Session(TF_MASTER) as session:\r\n\t\t  print ('List of devices:')\r\n\t\t  pprint.pprint(session.list_devices())\r\n\t\t  \r\n\t\t# Model specific parameters\r\n\r\n\t\t# TPU address\r\n\t\ttpu_address = TF_MASTER\r\n\r\n\t\t# Number of epochs\r\n\t\tepochs = 50\r\n\r\n\t\t# Number of steps_per_epoch\r\n\t\tsteps_per_epoch = 5\r\n\r\n\t\t# NOTE: Total number of training steps = Number of epochs * Number of steps_per_epochs\r\n\t\tresolver = tf.contrib.cluster_resolver.TPUClusterResolver(TF_MASTER)\r\n\t\ttf.contrib.distribute.initialize_tpu_system(resolver)\r\n\t\tstrategy = tf.contrib.distribute.TPUStrategy(resolver)\r\n\r\n\r\n\twith strategy.scope():\r\n\r\n\t  effnet = efn.EfficientNetB5(weights='imagenet', include_top=False)\r\n\r\n\t  # Replace all Batch Normalization layers by Group Normalization layers\r\n\t  for i, layer in enumerate(effnet.layers):\r\n\t      if \"batch_normalization\" in layer.name:\r\n\t          effnet.layers[i] = GroupNormalization(groups=32, axis=-1, epsilon=0.00001)\r\n\r\n\r\n\t  model = Sequential()\r\n\t  model.add(effnet)\r\n\t  model.add(GlobalAveragePooling2D())\r\n\t  model.add(Dropout(0.5))\r\n\t  model.add(Dense(8, activation=elu))\r\n\t  model.compile(loss='mse', optimizer=RAdam(lr=0.00005), metrics=['mse', 'acc'])\r\n\t  print(model.summary())        \r\n\r\n\t  model_json = model.to_json()\r\n\t  with open(\"model_ef7_fn.json\", \"w\") as json_file:\r\n\t      json_file.write(model_json)\r\n\r\n\r\nThe error i get is \r\n\r\n\r\n\tInvalidArgumentError: Cannot assign a device for operation stem_conv_1/kernel/IsInitialized/VarIsInitializedOp: node stem_conv_1/kernel/IsInitialized/VarIsInitializedOp (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422) was explicitly assigned to /job:worker/replica:0/task:0/device:TPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t\t [[stem_conv_1/kernel/IsInitialized/VarIsInitializedOp]]\t\r\n\r\n\r\nAny idea how to fix this?", "@DecentMakeover I suspect this line is the culprit:\r\n```python\r\neffnet = efn.EfficientNetB5(weights='imagenet', include_top=False)\r\n```\r\nYou probably have to specify `weights = None` and after you run `model.compile(...)` load them yourself with `model.load_weights(...)`.", "God! I don\u2019t have my laptop near me , I\u2019ll check this soon as I\u2019m back and let you know , ", "@dimitry-ishenko \r\n\r\nwhen i run this\r\n\r\n    def get_model():\r\n     model = Sequential()\r\n     model.add(effnet)\r\n     model.add(GlobalAveragePooling2D())\r\n      model.add(Dropout(0.5))\r\n      model.add(Dense(8, activation=elu))\r\n  print(model.summary())\r\n      return model \r\n\r\nwhere the effecinetNet model weights are set to None.\r\n\r\n\r\n\t/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/values.py in validate_colocate_tpu_variable(v, extended)\r\n\t    727     raise ValueError(\r\n\t    728         \"`colocate_vars_with` must only be passed a variable created in this \"\r\n\t--> 729         \"tf.distribute.Strategy.scope(), not: %r\" % (v,))\r\n\t    730   _validate_colocate_extended(v, extended)\r\n\t    731 \r\n\r\n\tValueError: `colocate_vars_with` must only be passed a variable created in this tf.distribute.Strategy.scope(), not: <tf.Variable 'stem_bn/moving_mean:0' shape=(48,) dtype=float32_ref>\r\n\r\nAny ideas?", "@DecentMakeover you gotta post a bit more code... Where are you compiling the model and where are you setting the weights? Maybe look at my example above, where I (1) create the model, (2) compile the model and (3) set the weights. All from within strategy.scope().", "@dimitry-ishenko \r\n\r\nGetting a different error now this is the code\r\n\r\n    def get_model():\r\n      effnet = efn.EfficientNetB5(weights=None, include_top=False)\r\n\r\n      # Replace all Batch Normalization layers by Group Normalization layers\r\n      for i, layer in enumerate(effnet.layers):\r\n          if \"batch_normalization\" in layer.name:\r\n              effnet.layers[i] = GroupNormalization(groups=32, axis=-1, epsilon=0.00001)\r\n\r\n\r\n      model = Sequential()\r\n      model.add(effnet)\r\n      model.add(GlobalAveragePooling2D())\r\n      model.add(Dropout(0.5))\r\n      model.add(Dense(8, activation=elu))\r\n      print(model.summary())  \r\n      return model\r\n\r\n   resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TF_MASTER)\r\n    tf.contrib.distribute.initialize_tpu_system(resolver)\r\n    strategy = tf.contrib.distribute.TPUStrategy(resolver)\r\n\r\n    with strategy.scope():\r\n      model = get_model()\r\n      model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1), \r\n                   loss='sparse_categorical_crossentropy',\r\n                   metrics=['sparse_categorical_crossentropy'])\r\n\r\n    model.summary()\r\n\r\nAnd this is the error\r\n\r\n\r\n    InvalidArgumentError: Cannot assign a device for operation stem_conv/kernel/IsInitialized/VarIsInitializedOp: {{node stem_conv/kernel/IsInitialized/VarIsInitializedOp}}was explicitly assigned to /job:worker/replica:0/task:0/device:TPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.\r\n     [[stem_conv/kernel/IsInitialized/VarIsInitializedOp]]\r\n\r\n    During handling of the above exception, another exception occurred:\r\n\r\n  InvalidArgumentError                      Traceback (most recent call last)\r\n  /usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n     1368           pass\r\n     1369       message = error_interpolation.interpolate(message, self._graph)\r\n  -> 1370       raise type(e)(node_def, op, message)\r\n     1371 \r\n     1372   def _extend_graph(self):\r\n\r\n       InvalidArgumentError: Cannot assign a device for operation stem_conv/kernel/IsInitialized/VarIsInitializedOp: node stem_conv/kernel/IsInitialized/VarIsInitializedOp (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422) was explicitly assigned to /job:worker/replica:0/task:0/device:TPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.\r\n     [[stem_conv/kernel/IsInitialized/VarIsInitializedOp]]", "@DecentMakeover you are using tf 1.14, right? Cuz tf 2.0 doesn't work with TPU in Colab", "yes using 1.4, i dont think version is the problem here, also to get more i have posted a query in SO\r\n\r\nhttps://stackoverflow.com/questions/57758831/on-how-to-run-custom-keras-code-on-tpu-cloud", "@DecentMakeover \r\nMy earlier post:\r\n> Maybe look at my example above, where I (1) create the model, **(2) compile the model** and **(3) set the weights**.\r\n\r\nYour SO post:\r\n```python\r\nmodel.load_weights('saved_models/wieghts_ef5.h5',by_name = True)\r\nmodel.compile(loss='mse', optimizer=RAdam(lr=0.00005), metrics=['mse', 'acc'])\r\n```\r\nSo, one more time: (2) COMPILE the model, (3) SET THE WEIGHTS.", "Tried that already , the second error I posted is the error I get when set weights after compile ,but I\u2019ll take a stab at it again\r\n\r\n\r\nOn 04-Sep-2019, at 9:40 PM, Super-intelligent Shade of the Color Blue <notifications@github.com<mailto:notifications@github.com>> wrote:\r\n\r\n\r\n@DecentMakeover<https://github.com/DecentMakeover>\r\nMy earlier post:\r\n\r\nMaybe look at my example above, where I (1) create the model, (2) compile the model and (3) set the weights.\r\nYour SO post:\r\n\r\nmodel.load_weights('saved_models/wieghts_ef5.h5',by_name = True)\r\nmodel.compile(loss='mse', optimizer=RAdam(lr=0.00005), metrics=['mse', 'acc'])\r\n\r\nSo, one more time: (2) COMPILE the model, (3) SET THE WEIGHTS.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/29896?email_source=notifications&email_token=AGD5QFZNP7XHBXJVBHKEO7LQH7MVPA5CNFSM4HY2TVH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD54CS6Y#issuecomment-527968635>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AGD5QF5M237DVGCV55V2IL3QH7MVPANCNFSM4HY2TVHQ>.\r\n", "@DecentMakeover Sorry, I get it -- didn't realize your SO post was from 2 days ago.\r\n\r\nAnyway, the important thing is if you try to load any weights before compiling the model, you will get an error message. This is what this issue is about.", "@llandryll,\r\n \r\nSorry for the delayed response. I could execute the [official notebook](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb), you mentioned successfully even after `recompiling` and `fitting for the second time`. \r\n\r\nPlease find [the Gist](https://colab.research.google.com/gist/rmothukuru/5d7d011b9775c168aa82464bdf885fba/keras-fashion-mnist.ipynb#scrollTo=MICrRv8rmXVq) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29896\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29896\">No</a>\n"]}, {"number": 29895, "title": "bug in tf.histogram_fixed_width_bins", "body": "This PR is to correct a bug that is present in tf.histogram_fixed_width_bins. The issue was described in https://github.com/tensorflow/tensorflow/issues/29661.\r\n\r\nWhen the elements of `value_range` are same and matches with one or more of the elements in input array `values`, then the index of the element will be outside `nbins`.", "comments": ["Can one of the admins verify this patch?", "> Can one of the admins verify this patch?\r\n\r\nPlease give me couple of days. I will update the code as suggested by @alextp . Thanks!", "Any updates here, @jvishnuvardhan ?", "@mihaimaruseac Will complete it by tomorrow. Thanks!", "> Almost ready! Now all we need is a test to prevent this fix from being scaled back.\r\n\r\nThanks @alextp. Added a test case. Please let me know if that is sufficient? Thanks!", "@jvishnuvardhan Can you please check build failures? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 29894, "title": "Update version numbers for TensorFlow 1.14.0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 1 -> 1\nMinor: 14 -> 14\nPatch: 0 -> 0\n\nNo lingering old version strings \"1.14.0-rc1\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"1.14.0rc1\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 29893, "title": "lite: change usage of 'fileno' to allow for msvc implementation", "body": "Very straightforward change, just guarding the usage of `fileno` to sub it with `_fileno` where needed. \r\n\r\nIn this case there shouldn't be an instance where the differences in the MSVC definition which fall outside of the POSIX version will matter. See https://docs.microsoft.com/en-us/cpp/c-runtime-library/reference/fileno?view=vs-2019 for the specific differences, but it is quite common for libraries to utilize this workaround (see: gtest).\r\n\r\nAlso yes, you really can build TFLite on Windows if you're brave enough. ", "comments": []}, {"number": 29892, "title": "Update version numbers for TensorFlow 1.14.0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 1 -> 1\nMinor: 14 -> 14\nPatch: 0 -> 0\n\nNo lingering old version strings \"1.14.0-rc1\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"1.14.0rc1\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 29891, "title": "compiling a file for the coral tpu", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI am using custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nos: Windows\r\nos kernel version: 10.0.17763\r\nos release version: 10\r\nos platform: Windows-10-10.0.17763-SP0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nthe coral tpu kinda\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nb'v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version:\r\npython version: 3.7.3\r\npython branch: v3.7.3\r\npython build version: ('v3.7.3:ef4ec6ed12', 'Mar 25 2019 22:22:05')\r\npython compiler version: MSC v.1916 64 bit (AMD64)\r\npython implementation: CPython\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:  release 10.0, V10.0.130\r\n- GPU model and memory: NVIDIA TITAN RTX\r\n\r\n**Describe the current behavior**\r\nI trained a model with quantization aware training, froze the model and converted the file to a tflite file and then when trying to compile the file one the coral tpu website it fails to compile.\r\n\r\nWhen I print out the tensors of the tfilte file I get:\r\n```\r\n[{'name': 'output', 'index': 3, 'shape': array([2, 1]), 'dtype': <class 'numpy.int64'>, 'quantization': (0.0, 0)}]\r\n[{'name': 'A', 'index': 0, 'shape': array([2, 1]), 'dtype': <class 'numpy.uint8'>, 'quantization': (1.0, 0)}, {'name': 'activation', 'index': 1, 'shape': array([2, 2]), 'dtype': <class 'numpy.ui\r\nnt8'>, 'quantization': (0.0235294122248888, 0)}, {'name': 'mult_bias', 'index': 2, 'shape': array([2]), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0470588244497776, 0)}, {'name': 'output\r\n', 'index': 3, 'shape': array([2, 1]), 'dtype': <class 'numpy.int64'>, 'quantization': (0.0, 0)}, {'name': 'output/dimension', 'index': 4, 'shape': array([], dtype=int32), 'dtype': <class 'numpy\r\n.int32'>, 'quantization': (0.0, 0)}, {'name': 'weights_quant/FakeQuantWithMinMaxVars/transpose', 'index': 5, 'shape': array([2, 1]), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0470588244\r\n497776, 128)}]\r\n{'name': 'A', 'index': 0, 'shape': array([2, 1]), 'dtype': <class 'numpy.uint8'>, 'quantization': (1.0, 0)}\r\n{'name': 'activation', 'index': 1, 'shape': array([2, 2]), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0235294122248888, 0)}\r\n{'name': 'mult_bias', 'index': 2, 'shape': array([2]), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0470588244497776, 0)}\r\n{'name': 'output', 'index': 3, 'shape': array([2, 1]), 'dtype': <class 'numpy.int64'>, 'quantization': (0.0, 0)}\r\n{'name': 'output/dimension', 'index': 4, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0)}\r\n{'name': 'weights_quant/FakeQuantWithMinMaxVars/transpose', 'index': 5, 'shape': array([2, 1]), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0470588244497776, 128)}\r\n```\r\n\r\nSome of the tensors aren't being converted into uint8 and I'm not exactly sure how that is supposed to happen (I assume that is the reason the file won't compile.)\r\n**Describe the expected behavior**\r\nI am expecting that I can get a custom model to work on the coral tpu. (I'm assuming I'm doing something a bit wrong but I am having so much trouble figuring out what I'm doing wrong)\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nshape_a = (2, 1)\r\nshape_b = (1, 2)\r\n\r\na = tf.placeholder(dtype=tf.float32, shape=shape_a, name=\"A\")\r\ny = tf.placeholder(tf.int64, shape=(None), name=\"y\")\r\n\r\nb = tf.Variable(tf.truncated_normal((1, 2), name=\"weight\"))\r\n\r\nc = tf.matmul(a, b, name=\"mult\")\r\n\r\nact = tf.nn.relu(c, name=\"activation\")\r\n\r\nxentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=act)\r\nloss = tf.reduce_mean(xentropy, name=\"loss\")\r\n\r\noutput = tf.argmax(act, axis=1, name=\"output\")\r\n\r\n# Call the eval rewrite which rewrites the graph in-place with\r\n# FakeQuantization nodes and fold batchnorm for eval.\r\ng = tf.get_default_graph()\r\ntf.contrib.quantize.create_eval_graph(input_graph=g)\r\n\r\n#quant aware testing\r\ng = tf.get_default_graph()\r\ntf.contrib.quantize.create_training_graph(input_graph=g)\r\n\r\nlearning_rate = 0.01\r\n\r\nwith tf.name_scope(\"train\"):\r\n    optimizer = tf.train.AdamOptimizer(learning_rate)\r\n    training_op = optimizer.minimize(loss, var_list=[b])\r\n\r\n\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\n#handles different tensorboard runs\r\nnow = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\r\nroot_logdir = \"tf_logs\"\r\nlogdir = \"mat_mul/{}/run-{}\".format(root_logdir, now)\r\nlogdir_later = \"mat_mul_later/{}/run-{}\".format(root_logdir, now)\r\ncheckpoint_dir = \"./mat_mul/saveCKPT\"\r\n\r\nwith tf.Session() as sess:\r\n    init.run()\r\n    print(sess.run(training_op, feed_dict={a: [[1.0], [2.5]], y: [1, 0]}))\r\n    print(sess.run(act, feed_dict={a: [[1.0], [2.5]]}))\r\n    print(sess.run(output, feed_dict={a: [[1.0], [2.5]]}))\r\n    file_writer = tf.summary.FileWriter(logdir)\r\n    file_writer.add_graph(tf.get_default_graph())\r\n    file_writer.flush()\r\n\r\n    saver = tf.train.Saver()\r\n    saver.save(sess, checkpoint_dir)\r\n\r\n\r\n    frozen_graph = tf.graph_util.convert_variables_to_constants(sess, tf.graph_util.remove_training_nodes(tf.get_default_graph().as_graph_def()), [\"output\"])\r\n\r\n    output_graph = \"mat_mul/frozen_graph\"\r\n    with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n        f.write(frozen_graph.SerializeToString())\r\n\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(output_graph, [\"A\"], [\"output\"])\r\n    converter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\n    input_arrays = converter.get_input_arrays()\r\n    converter.quantized_input_stats = {input_arrays[0]: (0., 1.)}  # mean, std_dev\r\n\r\n    tflite_model = converter.convert()\r\n    open(\"simple_mat_mult.tflite\", \"wb\").write(tflite_model)\r\n\r\n    file_writer = tf.summary.FileWriter(logdir_later)\r\n    file_writer.add_graph(tf.graph_util.remove_training_nodes(tf.get_default_graph().as_graph_def()))\r\n    file_writer.flush()\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I was able to compile with the offline compiler so I think this isn't an issue."]}, {"number": 29890, "title": "[TF 2.0 API Docs] tf.keras.backend.count_params", "body": "## System Information\r\nTensorflow version 2.9\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/backend/count_params\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\nThe description is not clear, no details on how to use this symbol\r\n\r\n### Raises listed and defined\r\nNo errors have been defined \r\n\r\n### Request visuals, if applicable\r\nNo visuals? an example using arrays could be represented in visual form for clarification\r\n\r\n### Submit a pull request?\r\nNo\r\n", "comments": ["@lkmandy,\r\n[Example code](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/backend/count_params#example) is available, which demonstrates how to use this functionality. Can you please confirm if this is what you are looking for? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29889, "title": "Fix build break because of mv command", "body": "Commit 93ebb3fb4abefdfbcd39d07f27660d52c418e8a9 broke the build\r\nbecause the mv command only contained a single argument. Both the\r\nsource and destination are in one pair of quotes. Need to quote each\r\nthe source and destination.", "comments": ["Current build failure from https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/5351/console\r\n\r\n`mv: missing destination file operand after '/tmp/tmp.NPwF4KWIP0/tensorflow_core/__init__.out /tmp/tmp.NPwF4KWIP0/tensorflow_core/__init__.py'`\r\n\r\n\r\nWith this fix cherry-picked, the builds works:\r\nhttps://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/5357/ \r\n\r\nping @mihaimaruseac ", "Sorry for this, my mistake"]}, {"number": 29888, "title": " Push forward compat date of gradients-of-log/exp/sqrt change", "body": "Along with the horizon change in https://github.com/tensorflow/tensorflow/pull/29812 , this will disable these for 1.14", "comments": []}, {"number": 29887, "title": "TensorFlow Lite Op Request", "body": "**System information**\r\n- OS Platform and Distribution: (macOS Mojave 10.14.4)\r\n- TensorFlow installed from: (pip3)\r\n- TensorFlow version (or github SHA if from source): '1.13.1'\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, EXP, EXPAND_DIMS, FILL, GATHER, GREATER, LOGISTIC, MAX_POOL_2D, MUL, PACK, PAD, RELU, RESHAPE, RESIZE_BILINEAR, SHAPE, STRIDED_SLICE, SUB, TILE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: GatherNd, NonMaxSuppressionV3, ScatterNd, Size, Where.\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, EXP, EXPAND_DIMS, FILL, GATHER, GREATER, LOGISTIC, MAX_POOL_2D, MUL, PACK, PAD, RELU, RESHAPE, RESIZE_BILINEAR, SHAPE, STRIDED_SLICE, SUB, TILE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: GatherNd, NonMaxSuppressionV3, ScatterNd, Size, Where.\r\n\r\n\r\n\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please take a look at https://www.tensorflow.org/lite/guide/faq#models_operations to know more.", "Could you provide more information about the model?\r\nI saw `NonMaxSuppressionV3`, so it's likely a SSD model?\r\nIf that's the case, @achowdhery can comment more. Thanks", "Yes, it is a Retinanet model for object detection. It is performed on all the detections to remove the repetitious detections. Do you need any other information for the model? Thanks!", "@xuxiyang1993 We see you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions.refer **[link](https://www.tensorflow.org/lite/guide/ops_select)**.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29886, "title": "XLA builds with NCCL even when it is disabled", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.14/master\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): Linaro 7.4.0\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Jetson Xavier\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuild fails when NCCL is disabled. XLA's build file builds NCCL with the `if_cuda` conditional (https://github.com/tensorflow/tensorflow/blob/v1.14.0-rc1/tensorflow/compiler/xla/service/gpu/BUILD#L359), but should be using `if_nccl` instead (https://github.com/tensorflow/tensorflow/blob/v1.14.0-rc1/tensorflow/core/kernels/BUILD#L210).\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nTrying to build TensorFlow with CUDA support but with NCCL disabled (--config=cuda and --config=nonccl bazel options) causes this issue.\r\n", "comments": ["@jvishnuvardhan hey, I'm leaving the TensorFlow/XLA teams.  I've updated go/tf-who-do-i-notify to point to a different point of contact.  Ideally we'd have some sort of XLA group you could use instead, I don't know if that's possible to set up?", "Hi Matt,\r\n\r\nThanks for the report.  Are you willing to make a PR to fix this?  Your suggestion to use `if_nccl` sounds quite reasonable, although you'll also have to update `NcclAllReduceThunk::NcclIsEnabled` to do the right thing.", "Sure, I'll link the PR here as soon as I submit it.\r\n\r\nThanks,\r\nMatt Conley", "Submitted PR #30507 to address this issue", "Closing as @chsigg fixed in commit [9b9bea6](https://github.com/tensorflow/tensorflow/commit/9b9bea65159eea4c8452d7f8b702b32dcb3cea8e)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29886\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29886\">No</a>\n"]}, {"number": 29885, "title": "https://www.tensorflow.org/guide/using_tpu 2.0b", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/using_tpu\r\n\r\n## Description of issue (what needs changing):\r\n\r\nNeeds to be completely redone\r\n\r\n### Clear description\r\n\r\nEverything is wrong and examples do not work\r\n\r\n", "comments": ["@CrackerHax Thanks for the noticing issues in that page. Could you be little more clear in describing what do you mean by `completely redone` and could you point which examples donot work? Are you interested in raising a PR to update the page? Thanks!", "[Here](https://github.com/tensorflow/docs/blob/master/site/en/guide/using_tpu.md) is the page if you are interested to update and raise a PR. Thanks!", "Try the example... literally nothing works. Error for everything. If I knew how to do this I wouldn't mind fixing it but the whole reason I am reading the documentation in the first place is that I don't know how to do this.... Somebody who knows how this works since the code has been butchered needs to write the documentation.", "@CrackerHax Thanks! Yesterday I made some corrections and the PR https://github.com/tensorflow/docs/pull/710 raised was merged already. I agree this is one of the old content which need to be updated. More changes are in our pipeline. Please stay tuned. Thanks!\r\n", "I am closing this for now as we have listed this in our pipeline and will be updating in the near future. But, if you or any one want to contribute then please raise PR's to correct. Thanks!"]}, {"number": 29884, "title": "get_next() doesn't block mapped function for datasets made from a generator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux AMI\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0-beta0\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): 7.2.1\r\n- CUDA/cuDNN version: V9.0.176\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI am generating a dataset using `from_dataset` that involves copying and decrypting data at read time, which takes some time to process. Once the files are copied the filenames are yielded by the generator function so they can be read in a reading function that I am trying to map. However, the reading code doesn't get blocked by the copy operation, and tries to operate on garbage data containing `args_0:0` before the generator function has actually run and yielded a filename.\r\n\r\nAdditionally, when I add additional non-tensorflow code inside of my mapped function, that code only gets executed the first time it is called. This might be me not understanding the intended behavior of the `map` function, but making it so that file reads can happen correctly in a map function would be useful in cases like this.\r\n\r\n**Describe the expected behavior**\r\nThe code in the function passed as an argument to .map() should not be run until the generator has yielded a value for it to operate on, or an input argument for map allowing this behavior to be specified should be added. If this is actually the intended behavior, that should be made more clear in the documentation for .map()\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n\r\ndef process(x):\r\n    print(x)\r\n    print(\"test\")\r\n    return x+1\r\n\r\ndef get_next_batch_sleep():\r\n    while True:\r\n        time.sleep(10)\r\n        yield(1)\r\n\r\nds = tf.data.Dataset.from_generator(get_next_batch_sleep,output_types=(tf.int64))\r\nds = ds.map(process)\r\nit = ds.make_one_shot_iterator()\r\nfor i in range(5):\r\n    print(it.get_next())\r\n```\r\n\r\nThis will immediately output\r\n```\r\nTensor(\"args_0:0\", dtype=int64)\r\ntest\r\n```\r\nAnd then output every 10 seconds\r\n`tf.Tensor(2, shape=(), dtype=int64)`\r\n", "comments": ["I have found the information about `tf.py_function()` so this is not a bug.\r\n\r\nIf anyone in the future has this problem, you need to wrap all non-tensorflow functions in your map() call using [py_function](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/py_function). Also, there's a `tf.print()` function that puts the print in the compute graph.\r\n\r\nSo, the correct way to write `process()` is either\r\n```\r\ndef process(x):\r\n    tf.py_function(print,[x],())\r\n    tf.py_function(print,[\"test\"],())\r\n    return x+1\r\n```\r\nor\r\n```\r\ndef process(x):\r\n    tf.print(x)\r\n    tf.print(\"test\")\r\n    return x+1\r\n```\r\nIt might be nice to add this information to the documentation for `map` or to add a conversion feature in its implementation if possible."]}, {"number": 29883, "title": "always_record_summaries in tf 2.0", "body": "I can't find `tf.summary.always_record_summaries` or `do_always_record_summaries` in tensorflow 2.0. Where can I find it?", "comments": ["According to https://github.com/tensorflow/tensorflow/issues/26405, now I don't need `always_record_summaries` anymore, but I want to get the exact answer :)", "Hi! \r\nIt seems that if you want to set summary recording on or off, you can use the function  [`tf.summary.record_it()`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary/record_if), and defined in [`python/ops/summary_ops_v2.py`](https://github.com/tensorflow/tensorflow/blob/8e423e3d56390671f0d954c90f4fd163ab02a9c1/tensorflow/python/ops/summary_ops_v2.py#L102).\r\nAs it was said in #26405, the recording is on by default, so if you want to turn it off, I think you should use the method above.\r\nThe `always_record_summaries` is still defined in [`python/ops/summary_ops_v2.py`](https://github.com/tensorflow/tensorflow/blob/8e423e3d56390671f0d954c90f4fd163ab02a9c1/tensorflow/python/ops/summary_ops_v2.py#L136) and simply return `record_if(True)`, but I was not able to use it.\r\n", "@Oktai15  : Have a look on @brunapinos's suggestion and let us know if that helps to resolve the issue. Thanks!", "@achandraa yes, thank you"]}, {"number": 29882, "title": "Update core.py", "body": "fix typo in deprecation warning #29677", "comments": ["Thank you\r\n"]}, {"number": 29881, "title": "The call method of DenseFeatures and SequenceFeatures use deprecated attribute _num_buckets", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): from pip install\r\n- TensorFlow version (use command below): v1.12.1-3259-gf59745a381 2.0.0-beta0\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\n\r\nBy simply calling the `call` method of `DenseFeatures` and `SequenceFeatures` defined with `categorical_column_with_identity` and `sequence_categorical_column_with_identity` feature columns (along with `embedding_column`), we get warnings that the deprecated attributes `_num_buckets` are used (instead of the non-deprecated `num_buckets` I guess).\r\n\r\nAlso note, on the code example below, that a third warning about a deprecated method, `add_dispatch_support.<locals>.wrapper` arises. I do not understand it but there are instructions for updating given in the warning, see the code below.\r\n\r\n**Describe the expected behavior**\r\n\r\nI think that we should not get warnings about deprecated objects when we are not calling any deprecated method, attribute, etc. I think that somewhere in the code of the `call` method of `DenseFeatures` and `SequenceFeatures` there is a use of `_num_buckets` that should be replaced by `num_buckets`. Following the updating instructions about the third warning may be enough to get rid of it.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.feature_column import categorical_column_with_identity, embedding_column, \\\r\n                                            sequence_categorical_column_with_identity\r\nfrom tensorflow.keras.layers import DenseFeatures\r\nfrom tensorflow.keras.experimental import SequenceFeatures\r\n\r\n#print(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\nnb_features = 10\r\nemb_dim = 3\r\n\r\nfc = categorical_column_with_identity('feature1', nb_features)\r\nemb_fc = embedding_column(fc, emb_dim)\r\nlayer = DenseFeatures(emb_fc)\r\n\r\nseq_fc = sequence_categorical_column_with_identity('feature2', nb_features)\r\nemb_seq_fc = embedding_column(seq_fc, emb_dim)\r\nseq_layer = SequenceFeatures(emb_seq_fc)\r\n\r\ndata1 = np.array(range(nb_features))\r\nbatch_size, sequence_length = 2, 5\r\nraw_data2 = np.array(range(nb_features))\r\ndata2 = np.reshape(raw_data2, (batch_size, sequence_length))\r\n\r\ndict_data = {'feature1': data1, 'feature2': data2}\r\nprint(layer(dict_data))\r\nprint(seq_layer(dict_data))\r\n```\r\nproduces the following three warnings:\r\n\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0617 19:19:42.823884 140735678825344 deprecation.py:323] From /Users/myusername/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3040: IdentityCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nW0617 19:19:42.827694 140735678825344 deprecation.py:323] From /Users/myusername/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2655: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW0617 19:19:42.833320 140735678825344 deprecation.py:323] From /Users/myusername/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3040: SequenceCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\n```", "comments": ["I could see the warning message on colab with Tf 2.0.0.beta0. Thanks!", "Similar problem here from beta1:\r\n> \r\n> tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use tf.where in 2.0, which has the same broadcast rule as np.where", "I've got exactly the same issue ^^ I hope someone fixes it ", "Same here", "+1 - most annoying comment ever <3", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29881\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29881\">No</a>\n", "The warnings are still here in tf2.0.0-rc0.", "Actually there is even one new warning when using tf2.0.0-rc0, I didn't see it the first time I tested. For the same code as above, now the output is:\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0925 14:12:28.993772 140735485318016 deprecation.py:323] From /path/to/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3089: IdentityCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nW0925 14:12:28.994673 140735485318016 deprecation.py:323] From /path/to/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:353: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.add_weight` method instead.\r\nW0925 14:12:29.005573 140735485318016 deprecation.py:323] From /path/to/python3.6/site-packages/tensorflow_core/python/ops/embedding_ops.py:802: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW0925 14:12:29.010264 140735485318016 deprecation.py:323] From /path/to/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3089: SequenceCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\n```\r\n\r\nSee the new warning \r\n```\r\nLayer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.add_weight` method instead.\r\n```\r\n\r\nIs it possible to re-open this issue? Or should I open a new one?\r\n\r\nEdit: it says I unassigned @bananabowl but I don't know how to do that, I just posted this new comment and then edited it.\r\n\r\nEdit 2: mentioning @martinwicke as he is the author of the commit which caused this issue to be closed.", "All warnings have disappeared in tf2.0.0. This one can stay closed for good."]}, {"number": 29880, "title": "Reuse DeviceNameUtils::LocalName", "body": "for this\r\n```\r\n// TODO(zhifengc): We need to consolidate (full/partial) device name\r\n// parsing into one place.\r\n//\r\n// Parses and returns the local device part (e.g., cpu:0, gpu:4).\r\nstring GetLocalDeviceName(StringPiece fullname) {\r\n  auto pos = fullname.rfind('/');\r\n  CHECK_NE(pos, StringPiece::npos);\r\n  fullname.remove_prefix(pos + 1);\r\n  return string(fullname);\r\n}\r\n```", "comments": ["@rthadur Would you mind adding another reviewer?", "@rthadur It's been 27 days. @saeta may be busy and have no time to deal with it.", "@jiakai0419 thank you for your patience @guptapriya can you review this ?", "I think @jaingaurav is a better person to review runtime related changes.", "@jiakai0419: Thank you for the PR. Unfortunately, it is not building internally could you please add `//tensorflow/core:framework` as dep for the `remote_device` target here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/BUILD#L288", "> @jiakai0419: Thank you for the PR. Unfortunately, it is not building internally could you please add `//tensorflow/core:framework` as dep for the `remote_device` target here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/BUILD#L288\r\n\r\n@jaingaurav  done"]}, {"number": 29879, "title": "SequenceFeatures layer requires a SparseTensor... only to convert it to a regular Tensor", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): from pip install\r\n- TensorFlow version (use command below): v1.12.1-3259-gf59745a381 2.0.0-beta0\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I call a `SequenceFeatures` layer on a dense tensor, like a tensor produced with numpy, a `TypeError` is raised because `SequenceFeatures` `call` method expects a `SparseTensor` as input. When looking at the log it appears that the function producing the `TypeError` is `sparse_tensor_to_dense`. So, `SequenceFeatures` does expect a `SparseTensor`, only to convert it to a dense `Tensor`, and fails if given a `Tensor` that is already dense.\r\n\r\n**Describe the expected behavior**\r\n\r\nI think `SequenceFeatures` `call` should accepts dense `Tensor` objects and just don't try to convert them from sparse to dense.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.feature_column import sequence_numeric_column\r\nfrom tensorflow.keras.experimental import SequenceFeatures\r\n\r\n#print(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\nseq_fc = sequence_numeric_column('feature1')\r\nseq_layer = SequenceFeatures(seq_fc)\r\n\r\nbatch_size, sequence_length = 3, 5\r\nraw_data = np.array(range(batch_size * sequence_length), dtype=np.float32)\r\ndata = np.reshape(raw_data, (batch_size, sequence_length))\r\n\r\ndict_data = {'feature1': data}\r\nseq_layer(dict_data)\r\n```\r\nThis produces the following error log:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-5-aa46f578fa6e> in <module>\r\n      1 dict_data = {'feature1': data}\r\n----> 2 seq_layer(dict_data)\r\n\r\n~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    710           with base_layer_utils.autocast_context_manager(\r\n    711               input_list, self._mixed_precision_policy.should_cast_variables):\r\n--> 712             outputs = self.call(inputs, *args, **kwargs)\r\n    713           self._handle_activity_regularization(inputs, outputs)\r\n    714           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/feature_column/sequence_feature_column.py in call(self, features)\r\n    138       with ops.name_scope(column.name):\r\n    139         dense_tensor, sequence_length = column.get_sequence_dense_tensor(\r\n--> 140             transformation_cache, self._state_manager)\r\n    141         # Flattens the final dimension to produce a 3D Tensor.\r\n    142         output_tensors.append(self._process_dense_tensor(column, dense_tensor))\r\n\r\n~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/feature_column/sequence_feature_column.py in get_sequence_dense_tensor(self, transformation_cache, state_manager)\r\n    553     sp_tensor = transformation_cache.get(self, state_manager)\r\n    554     dense_tensor = sparse_ops.sparse_tensor_to_dense(\r\n--> 555         sp_tensor, default_value=self.default_value)\r\n    556     # Reshape into [batch_size, T, variable_shape].\r\n    557     dense_shape = array_ops.concat(\r\n\r\n~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py in sparse_tensor_to_dense(sp_input, default_value, validate_indices, name)\r\n   1447     TypeError: If `sp_input` is not a `SparseTensor`.\r\n   1448   \"\"\"\r\n-> 1449   sp_input = _convert_to_sparse_tensor(sp_input)\r\n   1450 \r\n   1451   return gen_sparse_ops.sparse_to_dense(\r\n\r\n~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py in _convert_to_sparse_tensor(sp_input)\r\n     66     return sparse_tensor.SparseTensor.from_value(sp_input)\r\n     67   if not isinstance(sp_input, sparse_tensor.SparseTensor):\r\n---> 68     raise TypeError(\"Input must be a SparseTensor.\")\r\n     69   return sp_input\r\n     70 \r\n\r\nTypeError: Input must be a SparseTensor.\r\n```\r\n\r\n**Workaround**\r\n\r\nConverting the input to sparse before calling `SequenceFeatures` does make the code work:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.feature_column import sequence_numeric_column\r\nfrom tensorflow.keras.experimental import SequenceFeatures\r\n\r\n#print(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\nseq_fc = sequence_numeric_column('feature1')\r\nseq_layer = SequenceFeatures(seq_fc)\r\n\r\nbatch_size, sequence_length = 3, 5\r\nraw_data = np.array(range(batch_size * sequence_length), dtype=np.float32)\r\ndata = np.reshape(raw_data, (batch_size, sequence_length))\r\n\r\n#####\r\n# convert the input tensor in a sparse tensor\r\nzero = tf.constant(0, dtype=tf.float32)\r\nwhere = tf.not_equal(data, zero)\r\nindices = tf.where(where)\r\nvalues = tf.gather_nd(data, indices)\r\nsparse = tf.SparseTensor(indices, values, data.shape)\r\ndict_data_but_sparse = {'feature1': sparse}\r\n#####\r\n\r\nseq_layer(dict_data_but_sparse)\r\n```\r\n\r\ncorrectly outputs the following:\r\n\r\n```\r\n(<tf.Tensor: id=367, shape=(3, 5, 1), dtype=float32, numpy=\r\n array([[[ 0.],\r\n         [ 1.],\r\n         [ 2.],\r\n         [ 3.],\r\n         [ 4.]],\r\n \r\n        [[ 5.],\r\n         [ 6.],\r\n         [ 7.],\r\n         [ 8.],\r\n         [ 9.]],\r\n \r\n        [[10.],\r\n         [11.],\r\n         [12.],\r\n         [13.],\r\n         [14.]]], dtype=float32)>,\r\n <tf.Tensor: id=355, shape=(3,), dtype=int64, numpy=array([5, 5, 5])>)\r\n```", "comments": ["I have tried  on colab with TF version 2.0 beta and was able to reproduce the issue.", "I looked at this a little but and this particular conversion is indeed simply to eliminate. I am not at all sure we're not requiring sparsity elsewhere in the sequence feature columns though. @rohan100jain would know better.", "The issue is still here in tf2.0.0 and it keeps needing to add unnecessary operations in my models. I use the new `tf.sparse_to_dense` as a workaround now but in the end it is almost the same as before.", "@rohan100jain Any update on this?  The issue persists in 2.2.0-dev20200306.  Many thanks.", "This could probably be fixed by not converting a dense tensor, and also needs to update how to get sequence_length from a dense tensor (tf.shape(input_tensor)[1])", "Not a fix but to avoid some code, you may with to use the `dense_to_sparse` from `tensorflow_probability` math library, which ultimately achieves the same thing as the work around suggested by @durandg12. \r\n\r\nContinuing from the Workaround example above:\r\n```\r\nimport tensorflow_probability as tfp\r\nprint(tfp.math.dense_to_sparse(data))\r\n```\r\n\r\nor easier still, from tensorflow itself:\r\n```\r\nprint(tf.sparse.from_dense(data))\r\n```", "I am able to replicate the issue on tf-nightly(2.4.0-dev20200809), please find the [gist here](https://colab.research.google.com/gist/Saduf2019/57ce89aa46458594e15178ab4c10349f/untitled345.ipynb).", "We'd encourage you to try [Keras Preprocessing Layers](https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md#workflow-1----official-guide-on-how-to-replace-feature-columns-with-kpl). Feel free to re-open this issue if this doesn't work.\r\nWe will try to deprecated SequenceFeatures (given it's experimental and it literally doesn't work well with Keras)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29879\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29879\">No</a>\n"]}, {"number": 29878, "title": "How can I activate Tensorflow's XLA for the C API?", "body": "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: - \r\n\r\nI have built Tensorflow from source and I am using it's C API. So far everything works good, I am also using AVX / AVX2. My Tensorflow build from source was also built with XLA support. I now would like to also activate XLA (accelerated linear algebra) as I hope that it will once again increase the performance / speed during inference.\r\n\r\nIf I start my run right now I get this message:\r\n\r\n```\r\n2019-06-17 16:09:06.753737: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1541] \r\n(One-time warning): Not using XLA:CPU for cluster because envvar \r\nTF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that\r\nenvvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass\r\n--vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or \r\nset the envvar XLA_FLAGS=--xla_hlo_profile.\r\n```\r\n\r\nOn the official XLA homepage (https://www.tensorflow.org/xla/jit) I found this information about how to turn on jit on a session level:\r\n\r\n```\r\n# Config to turn on JIT compilation\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\nsess = tf.Session(config=config)\r\n```\r\n\r\nHere (https://github.com/tensorflow/tensorflow/issues/13853) it was explained how to set the TF_SetConfig in the C API. I was able to limit to one core before using the output of this Python code:\r\n\r\n```\r\nconfig1 = tf.ConfigProto(device_count={'CPU':1})\r\nserialized1 = config1.SerializeToString()\r\nprint(list(map(hex, serialized1)))\r\n```\r\n\r\nI implemented it as follows:\r\n\r\n```\r\nuint8_t intra_op_parallelism_threads = maxCores; // for operations that can be parallelized internally, such as matrix multiplication \r\nuint8_t inter_op_parallelism_threads = maxCores; // for operations that are independent in your TensorFlow graph because there is no directed path between them in the dataflow graph\r\nuint8_t config[]={0x10,intra_op_parallelism_threads,0x28,inter_op_parallelism_threads};\r\nTF_SetConfig(sess_opts,config,sizeof(config),status);\r\n```\r\n\r\nTherefore I thought this would help out for the XLA activation:\r\n```\r\nconfig= tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\noutput = config.SerializeToString()\r\nprint(list(map(hex, output)))\r\n```\r\n\r\nImplementation this time:\r\n\r\n```\r\nuint8_t config[]={0x52,0x4,0x1a,0x2,0x28,0x1};\r\nTF_SetConfig(sess_opts,config,sizeof(config),status);\r\n```\r\n\r\nHowever XLA still seems to be deactivated. Can somebody help me out with this issue? Or, if you once again have a loot at the warning:\r\n\r\n```\r\n2019-06-17 16:09:06.753737: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1541] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n```\r\n\r\nDoes that mean I have to set XLA_FLAGS during the build?\r\n\r\nI build Tensorflow using these configurations:\r\n\r\n```\r\n    Please specify the location of python. [Default is /usr/bin/python]: default\r\n    -Please input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages]: default\r\n    Do you wish to build TensorFlow with XLA JIT support? [Y/n]: Y\r\n    Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\n    Do you wish to build TensorFlow with ROCm support? [y/N]: N\r\n    Do you wish to build TensorFlow with CUDA support? [y/N]: N\r\n    Do you wish to download a fresh release of clang? (Experimental) [y/N]: N\r\n    Do you wish to build TensorFlow with MPI support? [y/N]: y\r\n    Please specify the MPI toolkit folder. [Default is /usr]: default\r\n    Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: default\r\n    Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\n```\r\n\r\nThanks in advance!\r\n\r\nEdit: moreover after some reading I am still not sure about when I should use XLA JIT and when I should use XLA AOT. It would be great if somebody could give me a recommendation about that!\r\n\r\nEdit 2: I just read here (https://www.tensorflow.org/xla/jit):\r\n```\r\n# Config to turn on JIT compilation\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\nsess = tf.Session(config=config)\r\n______________________________________\r\nNote: Turning on JIT at the session level will not result in operations being compiled for the CPU. JIT compilation for CPU operations must be done via the manual method documented below.\r\n```\r\n\r\nSo - no surprise my command did not do anything. Instead it is suggested to use the manual method:\r\n```\r\n    jit_scope = tf.contrib.compiler.jit.experimental_jit_scope\r\n\r\n    x = tf.placeholder(np.float32)\r\n    with jit_scope():\r\n      y = tf.add(x, x)  # The \"add\" will be compiled with XLA.\r\n```\r\nHowever I do not know how to do this using the C API, as I do not have single operations like add, instead I just have the TF_SessionRun(...)-command.", "comments": ["@jlebar @sanjoy Can you please take a look? Thanks!", "I would be especially interested on how to use AOT. For JIT, it seems like i can just include the experimental c api header and then use:\r\n\r\n```\r\nTF_EnableXLACompilation(sess_opts,true);\r\n```", "Hey there.  When filing bugs, it is helpful to include full steps to reproduce your issue.  \"I'm trying to do X because Y.  (xyproblem.info, which is not a problem we're running into here.)  I run this exact command, and here is the full output.  I believe this is not enabling XLA because I did xyz and I observed foo.\"\r\n\r\nThis bug is hard for me to help with because it does not have a listing of the complete code you're using, doesn't have the exact commands thave you have run, and doesn't have the full output of running your program.\r\n\r\nBecause I always have more things to help with than I have time for, if it's hard for me to help with this one.  That means that I'm less likely to spend time on it, just out of my desire to help as many people in a day as possible.\r\n\r\nThat said, I will try to help a little.  I think these warnings are trying to tell you something:\r\n\r\n>> 2019-06-17 16:09:06.753737: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1541] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n>\r\n> Does that mean I have to set XLA_FLAGS during the build?\r\n\r\nThis warning is not about the build.  It is about setting an environment variable when you run your program.\r\n\r\n```$ TF_XLA_FLAGS=--tf_xla_cpu_global_jit path/to/your/program```\r\n\r\nI would start with that.\r\n\r\nNote that support requests (e.g. \"how do I use feature X?\") are not bugs.  @ymodak in the future I think we should be strict about moving these to the correct forums.  That's important in part because whatever resolution we come to here will be useful for other people who have the same question, and they aren't necessarily going to look in the bug tracker.", "@jlebar Thanks for the inputs. Will do now onward.", "Closing this issue since the jlebar's comment addresses the issue. Feel free to post it on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) if further have questions related to this issue. Thanks!", "@el1995  I have tried what you said as the followings:\r\n`\r\n#include \"c_api_experimental.h\"   \r\n  \r\nTF_SessionOptions* options = TF_NewSessionOptions();   \r\n   \r\nTF_EnableXLACompilation(options,true);\r\n`\r\nbut the output is **collect2: error: ld returned 1 exit status** when compiling.", "Have you followed all the instructions in https://www.tensorflow.org/install/lang_c (in particular, were you able to get https://www.tensorflow.org/install/lang_c#compile to run)?\r\n\r\nIf not, can you please post the entire linker error you get?", "Absolutely,i am not only able to get [https://www.tensorflow.org/install/lang_c#compile](url) to run but also [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/multibox_detector/main.cc](url) to run successfully .However, i will get the warning **(One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile** each time even i get the correct prediction result. I have tried some solutions like setting environment variable of **TF_XLA_FLAGS=--tf_xla_cpu_global_jit** or **TF_EnableXLACompilation(options,true);** but neither works.", "There are so many discussions about how to active XLA in python but very few in C++ API . I have set environment variable like this **export TF_XLA_FLAGS=--tf_xla_cpu_global_jit=/mytensorflowpath/tensorflow/compiler/xla:$TF_XLA_FLAGS=--tf_xla_cpu_global_jit** and the new warning **2019-07-23 10:17:57.259354: E tensorflow/core/util/command_line_flags.cc:106] Couldn't interpret value =/mytensorflowpath/tensorflow/compiler/xla:=--tf_xla_cpu_global_jit for fl_jlag tf_xla_cpu_globait.** occurs  just now. I think this new warning means setting environment failed, is it right?", "Can you please post the entire linker error you get?\r\n\r\nNote: you can ignore the \"(One-time warning): Not using XLA:CPU for cluster because envvar ...\" warning.  We had a (now fixed) bug where we'd print that warning even when it wasn't necessary.", "> Can you please post the entire linker error you get?\r\n> \r\n> Note: you can ignore the \"(One-time warning): Not using XLA:CPU for cluster because envvar ...\" warning. We had a (now fixed) bug where we'd print that warning even when it wasn't necessary.\r\n\r\nThis is my whole warnings here (not error because i can test many examples successfully which means get correct prediction result.)\r\n`    \r\n2019-07-16 10:33:52.057179: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n`\r\n`\r\n2019-07-16 10:33:52.082548: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3407965000 Hz\r\n`\r\n`\r\n2019-07-16 10:33:52.082883: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44d56d0 executing computations on platform Host. Devices:\r\n`\r\n`\r\n2019-07-16 10:33:52.082903: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n`\r\n`\r\n2019-07-16 10:33:52.557067: I tensorflow/core/common_runtime/optimization_registry.cc:35] Running all optimization passes in grouping 0. If you see this a lot, you might be extending the graph too many times (which means you modify the graph many times before execution). Try reducing graph modifications or using SavedModel to avoid any graph modification\r\n`\r\n`\r\n2019-07-16 10:33:53.228415: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1337] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n`\r\nlink [https://stackoverflow.com/questions/57049454/tensorflows-warningextending-the-graph-too-many-times-which-means-you-modify](url)", "Hi, I am also once again confused by XLA. I am using a CPU-only system, and although I do not get the errors mentioned about, I see the follwing:\r\n\r\n```\r\n I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44d56d0 executing computations on platform Host. Devices:\r\n I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): <undefined>, <undefined>\r\n```\r\nMoreover, the inference performance gets significantly worse every time I activate XLA. As described above, I used this command to activate XLA:\r\n```\r\nTF_EnableXLACompilation(sess_opts,true);\r\n```\r\n\r\nHowever, now i found out in the experimental header of the C API:\r\n```\r\n// When `enable` is true, set\r\n// tensorflow.ConfigProto.OptimizerOptions.global_jit_level to ON_1, and also\r\n// set XLA flag values to prepare for XLA compilation. Otherwise set\r\n// global_jit_level to OFF.\r\n```\r\n\r\nAnd here (https://www.tensorflow.org/xla/jit) I see for the command mentioned (```config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1```)\r\n\"Note: Turning on JIT at the session level will not result in operations being compiled for the CPU. JIT compilation for CPU operations must be done via the manual method documented below.\"\r\n\r\nDo I use XLA the way I do it now? I mean, obviously something changes, however with XLA the performance gets significantly lower. I know that this can happen with XLA and that it has an experimental status, but the note on the XLA JIT page confuses me, I am not sure any more whether I actually use XLA now or not...?", "> Hi, I am also once again confused by XLA. I am using a CPU-only system, and although I do not get the errors mentioned about, I see the follwing:\r\n> \r\n> ```\r\n>  I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44d56d0 executing computations on platform Host. Devices:\r\n>  I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): <undefined>, <undefined>\r\n> ```\r\n\r\nWe also create a StreamExecutor for the host, that's what that line is saying.  I'm working on a CL to make that log line clearer.\r\n\r\n> Moreover, the inference performance gets significantly worse every time I activate XLA. As described above, I used this command to activate XLA:\r\n> \r\n> ```\r\n> TF_EnableXLACompilation(sess_opts,true);\r\n> ```\r\n> \r\n> However, now i found out in the experimental header of the C API:\r\n> \r\n> ```\r\n> // When `enable` is true, set\r\n> // tensorflow.ConfigProto.OptimizerOptions.global_jit_level to ON_1, and also\r\n> // set XLA flag values to prepare for XLA compilation. Otherwise set\r\n> // global_jit_level to OFF.\r\n> ```\r\n> \r\n> And here (https://www.tensorflow.org/xla/jit) I see for the command mentioned (`config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1`)\r\n> \"Note: Turning on JIT at the session level will not result in operations being compiled for the CPU. JIT compilation for CPU operations must be done via the manual method documented below.\"\r\n> \r\n> Do I use XLA the way I do it now?\r\n\r\nAre you also setting TF_XLA_FLAGS=--tf_xla_cpu_global_jit?  You're right that without this XLA will not kick in on CPU.\r\n\r\nTo debug this further, you can enable `--v=1` and check if you see this log line from `xla_compilation_cache.cc`: https://github.com/tensorflow/tensorflow/blob/926adde0445face2aae3d6e5822b83fa9259b988/tensorflow/compiler/jit/xla_compilation_cache.cc#L354\r\n\r\nIf that line is logging then XLA is definitely enabled and working.\r\n\r\n> I mean, obviously something changes, however with XLA the performance gets significantly lower. I know that this can happen with XLA and that it has an experimental status, but the note on the XLA JIT page confuses me, I am not sure any more whether I actually use XLA now or not...?", "> > Can you please post the entire linker error you get?\r\n> > Note: you can ignore the \"(One-time warning): Not using XLA:CPU for cluster because envvar ...\" warning. We had a (now fixed) bug where we'd print that warning even when it wasn't necessary.\r\n> \r\n> This is my whole warnings here (not error because i can test many examples successfully which means get correct prediction result.)\r\n> ` 2019-07-16 10:33:52.057179: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA`\r\n> `2019-07-16 10:33:52.082548: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3407965000 Hz`\r\n> `2019-07-16 10:33:52.082883: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44d56d0 executing computations on platform Host. Devices:`\r\n> `2019-07-16 10:33:52.082903: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): <undefined>, <undefined>`\r\n> `2019-07-16 10:33:52.557067: I tensorflow/core/common_runtime/optimization_registry.cc:35] Running all optimization passes in grouping 0. If you see this a lot, you might be extending the graph too many times (which means you modify the graph many times before execution). Try reducing graph modifications or using SavedModel to avoid any graph modification`\r\n> `2019-07-16 10:33:53.228415: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1337] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set. If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU. To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.`\r\n> link [https://stackoverflow.com/questions/57049454/tensorflows-warningextending-the-graph-too-many-times-which-means-you-modify](url)\r\n\r\nHas already been solved several days !", "Hi @ all,  \r\nI now also set the environment variables and XLA should definitely be activated now. However, this leads to the termination of my simulations due to floating point exceptions, but only if I activate XLA. Is there a plausible reason for this?\r\n\r\nI am using the pre-built C API of TF 1.14 right now, so there is surely no building mistake. I activate XLA JIT by setting a variable ```xlaJitSupport``` to 1. Then this code does the activation:\r\n\r\n```\r\nif(xlaJitSupport!=0)\r\n{\r\n    TF_EnableXLACompilation(sess_opts,true);\r\n    std::string setFlags = \"export TF_XLA_FLAGS=--tf_xla_cpu_global_jit\";\r\n    const char *setXLAFlags = setFlags.c_str();\r\n    system(setXLAFlags);\r\n}\r\n```\r\n\r\nThe error I get during execution is a floating point exception after some inferences. If I set ```xlaJitSupport``` to 0, the error disappears, but also the XLA support.", "That sounds like an XLA bug.  Can you (yet again!) share the full log of the failure?\r\n\r\nI'm also working on some public docs on how to file good actionable bugs against XLA, hopefully I'll have something to show next week.", "After all the \"stardard\" XLA messages in the beginning, I get:\r\n\r\n```\r\n[3] #0  Foam::error::printStack(Foam::Ostream&) at ??:?\r\n[3] #1  Foam::sigFpe::sigHandler(int) at ??:?\r\n[3] #2  ? in \"/lib64/libc.so.6\"\r\n[3] #3  Foam::divide(Foam::Field<double>&, double const&, Foam::UList<double> const&) at ??:?\r\n[3] #4  Foam::tmp<Foam::GeometricField<double, Foam::fvPatchField, Foam::volMesh> > Foam::operator/<Foam::fvPatchField, Foam::volMesh>(Foam::dimensioned<double> const&, Foam::tmp<Foam::GeometricField<dou$\r\n[3] #5  ? at tabulatedCombustionFoam.C:?\r\n[3] #6  ? at ??:?\r\n[3] #7  __libc_start_main in \"/lib64/libc.so.6\"\r\n[3] #8  ? at ??:?\r\n[node127:14007:0:14007] Caught signal 8 (Floating point exception: tkill(2) or tgkill(2))\r\n==== backtrace ====\r\n 0 0x0000000000036280 killpg()  ???:0\r\n 1 0x0000000000036207 __GI_raise()  :0\r\n 2 0x0000000000036280 killpg()  ???:0\r\n 3 0x00000000005b824c Foam::divide()  ???:0\r\n 4 0x00000000004759ff Foam::operator/<Foam::fvPatchField, Foam::volMesh>()  ???:0\r\n 5 0x0000000000431d9e Foam::operator/<Foam::fvPatchField, Foam::volMesh>()  tabulatedCombustionFoam.C:0\r\n 6 0x00000000004276cd main()  ???:0\r\n 7 0x00000000000223d5 __libc_start_main()  ???:0\r\n 8 0x000000000042b7d3 _start()  ???:0\r\n===================\r\n--------------------------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code. Per user-direction, the job has been aborted.\r\n--------------------------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nmpiexec noticed that process rank 3 with PID 0 on node node127 exited on signal 8 (Floating point exception).\r\n--------------------------------------------------------------------------\r\n```\r\n\r\nPlease note that I linked TensorFlow to my simulation software OpenFOAM, that's why the errors do not look like TensorFlow errors. However, as mentioned before, the exactly identical simulation continues to run if I set my ```xlaJitSupport``` variable to zero. Another thing I want to point out is that only with ```TF_EnableXLACompilation(sess_opts,true);``` my simulation continues to run. If I additionally add the part with the ```TF_XLA_FLAGS```, I get the error.\r\n\r\nEdit: now I get also the message if I only use  ```TF_EnableXLACompilation(sess_opts,true);``` without setting the flags. I do not know why, but also there is no warning that XLA is deactivated because of missing flag setting. The error message is identical. Is it definitely required to additionally set the flags when already using ```TF_EnableXLACompilation(sess_opts,true);```?", "> Is it definitely required to additionally set the flags when already using TF_EnableXLACompilation(sess_opts,true);?\r\n\r\nYou can double check whether XLA is kicking in (or not) this way: https://github.com/tensorflow/tensorflow/issues/31199#issuecomment-517746363", "It work for me\r\nOn linux run in terminal: export TF_XLA_FLAGS=--tf_xla_cpu_global_jit\r\nOn Windows add on top of code: os.environ[\"TF_XLA_FLAGS\"] = \"tf_xla_cpu_global_jit\"", "Does any one know how to write in c not in python using tensorflow c api \r\nI need the installation steps on ubuntu LTS 18.04 i cloned the TF official repository already but i stuck in this case : some  tensorflow c .h files using # include from other .h files which not in the repo \r\nhow can i get this files so i can start use all functionalities of tensorflow on C or c++ ?\r\n "]}, {"number": 29876, "title": "CUDA_ERROR_NOT_INITIALIZED error when using Multiple GPUs with multiprocessing", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1,13\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10, cuDNN 7.7\r\n- GPU model and memory: 2x NVIDIA Tesla T4, 16GB GDDR5\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nHello, I am trying to parallelize a machine learning algorithm by using multiprocessing for my TensorFlow session. This way, I can delegate one GPU for each process (and it doesn't allocate all of the memory and combine the two GPUs into one at the beginning.) However, my methods are currently yielding a CUDA_ERROR_NOT_INITIALIZED. \r\n\r\n**Describe the expected behavior** Each multiprocessing Process should be only able to see one GPU and be forced to use that: however, neither are being detected.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport multiprocessing\r\n\r\nclass AlignProcess (multiprocessing.Process):\r\n    def __init__(self, gpu ):\r\n        multiprocessing.Process.__init__(self)\r\n        self.gpu = gpu\r\n      \r\ndef run(self):\r\n    if self.gpu is not None:\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(self.gpu - 1)\r\n\r\n        import tensorflow as tf \r\n        session = tf.Session()\r\n        session.run(tf.global_variables_initializer())\r\n        session.close()\r\n\r\n\r\n# Later on in code\r\n\r\nAlignProcess(1).start()\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\nUsing cuDNN version 7600 on context None\r\nMapped name None to device cuda: Tesla T4 (0000:00:04.0)\r\nINFO:root:Working on recording /home/user/program/media/recordings/6.wav\r\nRunning source separation for recording: /home/user/program/media/recordings/6.wav\r\nTesting...\r\nWARNING:tensorflow:From /home/user/program/src/align/wavenet/Models/UnetAudioSeparator.py:99: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.conv1d instead.\r\nWARNING:tensorflow:From /home/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-06-17 14:05:59.513204: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n2019-06-17 14:05:59.518744: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000170000 Hz\r\n2019-06-17 14:05:59.519588: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55ebf146ee30 executing computations on platform Host. Devices:\r\n2019-06-17 14:05:59.519622: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-17 14:05:59.623164: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2019-06-17 14:05:59.623244: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:161] retrieving CUDA diagnostic information for host: wavenet-base-2gpu-t4-4\r\n2019-06-17 14:05:59.623251: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:168] hostname: wavenet-base-2gpu-t4-4\r\n2019-06-17 14:05:59.623412: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:192] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\n2019-06-17 14:05:59.623465: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:196] kernel reported version is: 418.67.0\r\nNum of variables64\r\nWARNING:tensorflow:From /home/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nINFO:tensorflow:Restoring parameters from /home/user/program/src/align/wavenet/../../models_english/wavenet_full_44KHz/full_44KHz-236118\r\nUsing cuDNN version 7600 on context None\r\nPre-trained model restored for song prediction\r\nMapped name None to device cuda: Tesla T4 (0000:00:04.0)\r\nINFO:root:Working on recording /home/user/program/media/recordings/7.wav\r\nRunning source separation for recording: /home/user/program/media/recordings/7.wav\r\nTesting...\r\nWARNING:tensorflow:From /home/user/program/src/align/wavenet/Models/UnetAudioSeparator.py:99: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.conv1d instead.\r\nWARNING:tensorflow:From /home/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-06-17 14:06:04.528660: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n2019-06-17 14:06:04.551547: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000170000 Hz\r\n2019-06-17 14:06:04.552053: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55ebf3709e10 executing computations on platform Host. Devices:\r\n2019-06-17 14:06:04.552076: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-17 14:06:04.670544: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2019-06-17 14:06:04.670736: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:161] retrieving CUDA diagnostic information for host: wavenet-base-2gpu-t4-4\r\n2019-06-17 14:06:04.670788: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:168] hostname: wavenet-base-2gpu-t4-4\r\n2019-06-17 14:06:04.670981: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:192] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\n2019-06-17 14:06:04.671078: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:196] kernel reported version is: 418.67.0\r\nNum of variables64\r\nWARNING:tensorflow:From /home/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nINFO:tensorflow:Restoring parameters from /home/user/program/src/align/wavenet/../../models_english/wavenet_full_44KHz/full_44KHz-236118\r\nPre-trained model restored for song prediction\r\n\r\n```\r\n", "comments": ["Hello,\r\n\r\nI figured it out! This error was happening because I was importing TensorFlow, and thusly, cuDNN, from earlier on in my program.", "@romanscott How did you fixwd it?\r\n", "> Hello,\r\n> \r\n> I figured it out! This error was happening because I was importing TensorFlow, and thusly, cuDNN, from earlier on in my program.\r\n\r\nhello, How do you slove this problem?", "I've got this warning too (along with the frozen training process) when I tried to train maskRcnn on coco dataset.\r\nIt's strange but I removed `import tensorflow as tf` from the begining of my code and after getting a lot of this error again, finally they stopped and the model continued to training!\r\nOf course it's not a good solution because I need to import tf anyway, but maybe that helps somebody."]}]