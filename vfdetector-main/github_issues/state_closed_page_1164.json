[{"number": 18282, "title": "Fix control edge issues and handle unsupported ops gracefully in int8 mode during TensorRT conversion ", "body": "This PR fixes issues with incorrect handling of control edges in some rare cases during the conversion and improves the handling of unsupported ops in creation of INT8 calibration graphs.\r\n", "comments": ["Tagging @zheng-xq."]}, {"number": 18281, "title": "Docs: Fix formatting in programmers_guide/debugger", "body": "Fixes #17946 ", "comments": ["Staged: go/tfo-stage/programmers_guide/debugger"]}, {"number": 18279, "title": "h5py Dockerfile", "body": "h5py is a quite hard dependency but it is not in all the flavors of TF Dockerfile.\r\nI.e. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/applications/vgg16.py#L50-L51\r\n\r\nSee https://github.com/keras-team/keras/pull/9830\r\n/cc @fchollet ", "comments": ["/cc @lenlen", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@angersson, do you know who is working on docker right now?", "@av8ramit?", "See also https://github.com/tensorflow/tensorflow/pull/11360", "@caisq Do you know why it was not introduced in all the Dockerfile on #11360 review process.", "@bhack @av8ramit I'm fine with adding h5py to the devel Docker images if users require it.", "I'll put together a change to add the dependencies to the dockerfiles. \r\n(edit: never mind, @av8ramit beat me to it. Thanks!)", "https://github.com/tensorflow/tensorflow/pull/18868", "Resolved by #18868. Thanks Amit!"]}, {"number": 18278, "title": "Branch 191797853", "body": "", "comments": []}, {"number": 18277, "title": "Add win_def_file attribute for tensorflow/python:pywrap_tensorflow_internal", "body": "This attribute is somehow missing when pushing for internal to github.\r\n\r\nThis should fix the TensorFlow Bazel postsubmit for github.\r\n\r\n@yifeif ", "comments": ["It's added by copybara, because internally we don't support this attribute. It works in the internal postsubmit, so this seems to be a merge error during pushing to github.", "Looks like this change is good to be merged! :)"]}, {"number": 18276, "title": "tf.train.latest_checkpoint always return the same checkpoint for GCS paths (pywrap_tensorflow cache bug?)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.3 (confirmed on Ubuntu 16.04.4 LTS as well)\r\n- **TensorFlow installed from (source or binary)**: binary (confirmed from 1.6 source as well)\r\n- **TensorFlow version (use command below)**: ('v1.6.0-0-gd2e24b6039', '1.6.0')\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A (confirmed with CUDA 9.1 cuDNN 7.1)\r\n- **GPU model and memory**: N/A (confirmed with P100 16GB)\r\n- **Exact command to reproduce**:\r\n\r\nOpen 2 terminals, 1 and 2\r\n\r\n**From terminal 1**\r\n```\r\ncat <<EOT > checkpoint\r\nmodel_checkpoint_path: \"model.ckpt-4\"\r\nall_model_checkpoint_paths: \"model.ckpt-0\"\r\nall_model_checkpoint_paths: \"model.ckpt-1\"\r\nall_model_checkpoint_paths: \"model.ckpt-2\"\r\nall_model_checkpoint_paths: \"model.ckpt-3\"\r\nall_model_checkpoint_paths: \"model.ckpt-4\"\r\nEOT\r\ngsutil cp checkpoint gs://path_to_checkpoint_dir/\r\n```\r\n\r\n**From terminal 2**\r\n```\r\npython\r\nimport tensorflow as tf\r\nprint tf.train.get_checkpoint_state('gs://path_to_checkpoint_dir')\r\n```\r\n\r\n**From terminal 1**\r\n```\r\ncat <<EOT > checkpoint\r\nmodel_checkpoint_path: \"model.ckpt-7\"\r\nall_model_checkpoint_paths: \"model.ckpt-3\"\r\nall_model_checkpoint_paths: \"model.ckpt-4\"\r\nall_model_checkpoint_paths: \"model.ckpt-5\"\r\nall_model_checkpoint_paths: \"model.ckpt-6\"\r\nall_model_checkpoint_paths: \"model.ckpt-7\"\r\nEOT\r\ngsutil cp checkpoint gs://path_to_checkpoint_dir/\r\n```\r\n\r\n**From terminal 2** (python shell still opened, **do not reimport tensorflow**)\r\n```\r\nprint tf.train.get_checkpoint_state('gs://path_to_checkpoint_dir')\r\n```\r\n\r\ntensorflow always returns the same cached copy of the checkpoint file.\r\n\r\n\r\n### Describe the problem\r\ntf.train.latest_checkpoint and tf.train.get_checkpoint_state always returns a cached version of the latest checkpoint file on GCS, ignoring new checkpoints written to the checkpoint file. This is extremely problematic when training a model.\r\n\r\nI traced the problem to pywrap_tensorflow so my guess is it's coming from the underlying native code. I would be happy with a way to reset the cache or an input bool to just not use it at all.\r\n\r\n### Source code / logs\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> print tf.train.get_checkpoint_state('gs://path_to_checkpoint_dir')\r\nmodel_checkpoint_path: \"gs://path_to_checkpoint_dir/model.ckpt-4\"\r\nall_model_checkpoint_paths: \"gs://path_to_checkpoint_dir/model.ckpt-0\"\r\nall_model_checkpoint_paths: \"gs://path_to_checkpoint_dir/model.ckpt-1\"\r\nall_model_checkpoint_paths: \"gs://path_to_checkpoint_dir/model.ckpt-2\"\r\nall_model_checkpoint_paths: \"gs://path_to_checkpoint_dir/model.ckpt-3\"\r\nall_model_checkpoint_paths: \"gs://path_to_checkpoint_dir/model.ckpt-4\"\r\n\r\n>>> print tf.train.get_checkpoint_state('gs://path_to_checkpoint_dir')\r\nmodel_checkpoint_path: \"gs://path_to_checkpoint_dir/model.ckpt-4\"\r\nall_model_checkpoint_paths: \"gs://path_to_checkpoint_dir/model.ckpt-0\"\r\nall_model_checkpoint_paths: \"gs://path_to_checkpoint_dir/model.ckpt-1\"\r\nall_model_checkpoint_paths: \"gs://path_to_checkpoint_dir/model.ckpt-2\"\r\nall_model_checkpoint_paths: \"gs://path_to_checkpoint_dir/model.ckpt-3\"\r\nall_model_checkpoint_paths: \"gs://path_to_checkpoint_dir/model.ckpt-4\"\r\n```\r\n", "comments": ["Looks like tensorflow 1.7 doesn't have this behavior, is 1.6 still maintained?", "Nagging Assignee @shivaniag: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 50 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 65 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 80 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 95 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 110 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 124 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18275, "title": "SIGILL, Illegal instruction, when importing tensorflow", "body": "I'm getting a `terminated by signal SIGILL (Illegal instruction)` when importing tensorflow.\r\n\r\n`less /proc/cpuinfo` yields\r\n\r\n```\r\nprocessor       : 0\r\nvendor_id       : GenuineIntel\r\ncpu family      : 6\r\nmodel           : 26\r\nmodel name      : Intel(R) Core(TM) i7 CPU         920  @ 2.67GHz\r\nstepping        : 5\r\nmicrocode       : 0x11\r\ncpu MHz         : 1598.913\r\ncache size      : 8192 KB\r\nphysical id     : 0\r\nsiblings        : 8\r\ncore id         : 0\r\ncpu cores       : 4\r\napicid          : 0\r\ninitial apicid  : 0\r\nfpu             : yes\r\nfpu_exception   : yes\r\ncpuid level     : 11\r\nwp              : yes\r\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti retpoline tpr_shadow vnmi flexpriority ept vpid dtherm ida\r\nbugs            : cpu_meltdown spectre_v1 spectre_v2\r\nbogomips        : 5331.70\r\nclflush size    : 64\r\ncache_alignment : 64\r\naddress sizes   : 36 bits physical, 48 bits virtual\r\npower management:\r\n```\r\n\r\nIs my CPU too old to use tensorflow?", "comments": ["I don't see \"avx\" under \"flags\", so yes, that's probably that's the reason. You need tensorflow 1.5 binary, or build one yourself", "@yaroslavvb Thank you for your response. If I understand correctly, the version of tensorflow that is now shipped with `pip` only supports CPUs that support AVX?", "That's not good. IMO it is a bug / faulty build and should be changed urgently in a next patch. CPU extensions that are not functionally necessary shouldn't be required for minimal installation. Could be frustrating for beginners following the simple installation instructions.", "As @yaroslavvb mentioned, from TF 1.6 onwards the pip install requires CPU's with AVX capability (https://www.tensorflow.org/install/install_sources). You can always do\r\n\r\npip install tensorflow==1.5\r\nor \r\npip install tensorflow-gpu==1.5", "@rohan100jain Why does tensorflow require AVX capability from version 1.6? Why drop the support of CPUs that don't have AVX capabilities?", "I don't get it either...\r\nSo users with older CPU are forced to use an outdated tensorflow API, probably leading to incompatible code in the future, just because AVX is required - unnecessarily! (Because building it without AVX shows that it is not really needed.) I understand that it improves performance. But in my opinion the most basic pip-installed build should target as many systems as possible. Performance aware users should use an advanced/custom build anyway. So what's the point of requiring AVX for everyone?", "@saluto  @tgy TensorFlow 1.6 supports CPUs without AVX capabilities. TensorFlow 1.6 did not drop support of older CPUs. The __prebuilt official binary__ of TensorFlow 1.6 does not support older CPUs. You can always build from source on older CPUs or look for non-official binary others have built.\r\n\r\nThere are lots of options building a binary and there could be only one binary called \"tensorflow==1.6.0\" on pypi. So a choice has to be made. And I think requiring AVX is just one choice to balance speed and availability.", "@ppwwyyxx \"You can always build from source\" - You should say this to people wanting a more not less advanced build. Especially because Tensorflow is also widely used among beginners.\r\nI stay with it: For the most basic pip-build it's the wrong choice. Maybe a better compromise would be to activate AVX only for the prebuilt **GPU**-version, since this already is a performance optimized one. I think that would be a better balance of speed and availability. (I really understand and appreciate your effort for performance.)", "The real issue is that TensorFlow binary is pre-baked with a specific architecture instruction set. This makes sense for Google internal deployments -- there's a small number of supported configurations, so it makes sense to maintain a different binary for each configuration. But it makes less sense for python ecosystem where there's a large number of hardware configurations, and pypi doesn't even support architecture specific binaries.\r\n\r\nOther libraries like MKL and PyTorch provide a multi-instruction binary, which use dynamic dispatch to select the most efficient instructions for the architecture. cc @tatianashp ", "I just built Tensorflow 1.8 from source on OS X 10.11.6, and it's core dumping.  \r\nThe build parameter to 'bazel' was: -march=native.\r\nDoesn't that select options my CPU supports?  What build option(s) should I specify for an older CPU?\r\n```\r\n$  sysctl -a | grep machdep.cpu\r\nmachdep.cpu.max_basic: 13\r\nmachdep.cpu.max_ext: 2147483656\r\nmachdep.cpu.vendor: GenuineIntel\r\nmachdep.cpu.brand_string: Intel(R) Core(TM)2 Duo CPU     T9900  @ 3.06GHz\r\nmachdep.cpu.family: 6\r\nmachdep.cpu.model: 23\r\nmachdep.cpu.extmodel: 1\r\nmachdep.cpu.extfamily: 0\r\nmachdep.cpu.stepping: 10\r\nmachdep.cpu.feature_bits: 290732854951541759\r\nmachdep.cpu.extfeature_bits: 4832888832\r\nmachdep.cpu.signature: 67194\r\nmachdep.cpu.brand: 0\r\nmachdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 DTES64 MON DSCPL VMX SMX EST TM2 SSSE3 CX16 TPR PDCM SSE4.1 XSAVE\r\nmachdep.cpu.extfeatures: SYSCALL XD EM64T LAHF\r\n```\r\n..."]}, {"number": 18274, "title": "Fix math equation rendering format in api definitions", "body": "This PR is to fix mess-up math equation format in api definition related pbtxts according to the [Math in markdown guideline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md#math-in-markdown).\r\n\r\nTake [tf.gather_nd](https://www.tensorflow.org/api_docs/python/tf/gather_nd) as an example below:\r\nBefore:\r\n![image](https://user-images.githubusercontent.com/1680977/38387596-e9a3c364-394a-11e8-8594-116c1cb9fb7b.png)\r\nAfter:\r\n![image](https://user-images.githubusercontent.com/1680977/38387667-25d5b52c-394b-11e8-8da0-e06873811fd3.png)\r\n\r\n\r\n\r\n", "comments": ["Adding Martin because if we decide to change the formatting here, it might be better to change everything at once.", "@MarkDaoust will this render correctly? Some of these are MathJax inside code blocks. I'm happy with this change if it works.", "Nagging Assignee @andrewharp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", ">It looks like a lot of the spots where you reduced the escaping one level on the \\\\( are already rendering correctly. So I think the reduced escaping will break them. you need to use \\\\\\\\( in the .pbtxt files.\r\n\r\nNope. I was wrong about this.\r\n\r\nIt's the summary line that looses one level of escaping. The summary lines (like the change to Exp) need the double escaping `\\\\\\\\( \\\\\\\\)`. The description fields need the single escaping (that's why [polygamma](https://www.tensorflow.org/api_docs/python/tf/polygamma) already works). "]}, {"number": 18273, "title": "Change --output_png to --output_image", "body": "The argument is incorrect.\r\n\r\nWhen running the given command, we get\r\n```\r\nE tensorflow/examples/wav_to_spectrogram/main.cc:54] Unknown argument\r\n--output_png=/tmp/spectrogram.png\r\n```\r\n\r\nTESTED:Rerun the updated command and verify that the flag is correct.\r\n```\r\nbazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram --\r\n--input_wav=/tmp/speech_dataset/happy/ab00c4b2_nohash_0.wav\r\n--output_image=/tmp/spectrogram.png\r\n```", "comments": []}, {"number": 18272, "title": "hi, oops", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["oh hi mark", "Sorry. That was an accident.\nMark\n\nOn Apr 5, 2018 3:45 PM, \"Krish Ravindranath\" <notifications@github.com>\nwrote:\n\n> oh hi mark\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/18272#issuecomment-379053655>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ASozfIIHzeM4RxJsraY1cCiQ2xEFNBTsks5tlnQmgaJpZM4TJAeO>\n> .\n>\n", "that's ok! I was just making [a silly joke](https://www.youtube.com/watch?v=C-IvV8thrO4). Consider closing this issue!", "Ok, I will.\nM\n\nOn Thu, Apr 5, 2018 at 3:56 PM, Krish Ravindranath <notifications@github.com\n> wrote:\n\n> that's ok! I was just making a silly joke\n> <https://www.youtube.com/watch?v=C-IvV8thrO4>. Consider closing this\n> issue!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/18272#issuecomment-379056743>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ASozfPGerhqyYYXEtL9VVFZbqdlJ-pLeks5tlnbSgaJpZM4TJAeO>\n> .\n>\n\n\n\n-- \n-------------------------------------------------------\nMark Orr\nResearch Associate Professor\nNetwork Dynamics and Simulation Science Laboratory\nBiocomplexity Institute of Virginia Tech\n900 North Glebe Rd.\nArlington, VA 22203\np: 571-858-3116\nf: 571-858-3015\nmorr9@vt.edu <morr9@vbi.vt.edu>\n"]}, {"number": 18271, "title": "[Feature Request] Python - like behavior of tf.string_split. Consider whole multi-byte delimiter.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS, Ubuntu\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.7\r\n- **Python version**: 3.6,3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:No\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\n`tf.string_split` is documented as `If delimiter contains multiple bytes, it is treated as a set of delimiters with each considered a potential split point.` Hence, each character in a multi-byte string delimiter will become a separate delimiter, which is very non-intuitive and difficult to implement.\r\n\r\nso for example, I want to split by to the token `<eos>`\r\n```\r\ntf.string_split([\"hello world <eos> tensorflow\"], delimiter=\"<eos>\").values\r\n# equal:     ['h', 'll', ' w', 'rld ', ' t', 'n', 'rfl', 'w']  \r\n# should be ['hello world ', ' tensorflow]  \r\n```\r\n\r\nCan tf.string_split be reimplemented so that it consider the whole delimiter string, whether single or multiple character, to be its true delimiter instead making every single character in it delimiter?\r\n\r\nThank you\r\n\r\n", "comments": ["Also it would be very nice to be able to split utf-8 encoded string into multibyte characters instead of separate raw bytes to be able to implement character-level NLP completely with TF's OPs. Current solution is dirty hack: py_func that decodes string to utf-8, insert some separator (for example: \"|\") after each character and then this string tensor is separated by tf.string_split(input_tensor, delimiter=\"|\")\r\nIt could be better)", "@martinwicke I think this is a good feature suggestion. What do you think?", "I agree `string_split` should work exactly like Python's [`str.split`](https://docs.python.org/2/library/stdtypes.html#str.split). This would be a fantastic feature to contribute, but we'll have to talk details about backwards compatibility, and we need to make sure this works in an unsurprisingly way across Python 2 and 3 and different string encodings.", "I am not sure if we actually need to compute backward pass with strings. Usually people deal with strings at preprocess steps, which there is no need to backpropagate. \r\n\r\nYet, is possible to backpropagate with strings any way, even with the current tf.string_split implementation ?", "I'm not talking about gradients, I'm talking about backwards compatibility guarantees for the API. We cannot change the current behavior (the delimiter string will be unpacked into a set of single characters). We have to find a backwards compatible version for the signature of string_split does doesn't change the current behavior.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Added a PR #19650 to expose in `tf.strings.split` which matches the behaviors of python's `str.split`."]}, {"number": 18270, "title": "why the kfac in tensorflow is not working suddenly?", "body": "Hi,\r\n\r\nI wrote a simple code using tf.contrib.kfac for regression. It worked fine in the last two weeks. However, when I tried to run the code again the day before yesterday, there was an error: \r\n\r\n**AttributeError: module 'tensorflow.contrib' has no attribute 'kfac'**\r\n\r\n![image](https://user-images.githubusercontent.com/38111150/38382416-c53529aa-38be-11e8-9220-d6f9102f61e8.png)\r\n\r\nI just don't know why suddenly this goes wrong. Can anyone help me?\r\n\r\nThanks so much.\r\n", "comments": ["cc @duckworthd ", "Thank you, @yaroslavvb!\r\n\r\nHi @Luanming. I had some difficulty getting `kfac` knit into `tf.contrib` near the beginning, though hopefully all is well in the latest version of TensorFlow. Could you print out which version of TensorFlow you're using, so I could inspect the source code?\r\n\r\n```shell\r\n$ python -c \"import tensorflow; print tensorflow.__version__\"\r\n```", "Oh, that's where things went wrong. My version was 1.2.1. After updating, it is working now.\r\n@duckworthd  Thank you  so much for your answer!\r\n@yaroslavvb Thank you, too", "That was easy :)\r\n\r\n![image](https://user-images.githubusercontent.com/626236/38401640-c84cff9e-390b-11e8-9ee3-527463062598.png)\r\n"]}, {"number": 18269, "title": "[TFTS] Switch to use tf.data for NumpyReader.read_full()", "body": "Looked into the `input_pipeline.py` a bit. This one is a simple change but the other ones involve some refactoring and API changes in input pipeline so I think it's better to do it internally. ", "comments": ["Unfortunately this is equivalent to converting all of the Numpy arrays with tf.constant and embedding them in the graph (which wouldn't require the use of a Datasets at all and would be more memory efficient).\r\n\r\nSo we may end up with tf.constants here in order to get rid of `numpy_input_fn`, but I don't think this change makes sense as is. If you want to get rid of `numpy_input_fn` completely, we can use `tf.constant` without a `Dataset` in `read_full`, then use `from_tensor_slices` in `read()`.", "Oh that makes sense. Thanks for the review. I am closing this now as I was not thinking through this at all. "]}, {"number": 18268, "title": "Distributed Tensorflow :: Worker is getting terminated after running few training steps", "body": "Hi,\r\n\r\nI am having 2 nodes cluster and running parameter server and first worker on one machine, and second worker on another machine.\r\nEnvironment and version details are below,\r\nHave I written custom code - Yes\r\nOS Platform and Distribution - CentOS 7.2.1511\r\nTensorFlow installed from - Binary\r\nTensorFlow version - 1.3.0\r\nBazel version - N/A\r\nCUDA/cuDNN version - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce - N/A\r\n\r\nParameter server and first worker that is also the chief(master) worker, starts correctly.\r\nWhen I start send worker that also starts and training runs for few steps and then the worker is terminated with below exception.\r\nPlz noote all are CPU machines (no GPUs)\r\n\r\nINFO:tensorflow:global step 482: loss: 0.8154 (15.70 sec/step)\r\nINFO:tensorflow:global step 483: loss: 1.1079 (15.20 sec/step)\r\nINFO:tensorflow:global step 485: loss: 0.8833 (15.21 sec/step)\r\nINFO:tensorflow:global step 487: loss: 1.0541 (15.78 sec/step)\r\nINFO:tensorflow:global step 489: loss: 0.8346 (14.84 sec/step)\r\nNow run summeries\r\nINFO:tensorflow:global step 491: loss: 1.0193 (14.91 sec/step)\r\nloss is done\r\n**INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framewo                                                                                           rk.errors_impl.CancelledError'>, Step was cancelled by an explicit call to `Session::Close()`.**\r\nTraceback (most recent call last):\r\n  File \"./DTF_train_image_classifier.py\", line 464, in <module>\r\n    tf.app.run()\r\n\t\r\nOn parameter server, I see below error logs,\r\n\r\n**2018-04-05 14:47:12.860894: W tensorflow/core/kernels/queue_base.cc:295] _0_parallel_read/filenames: Skipping cancelled enqueue attempt with queue not closed\r\n2018-04-05 14:47:12.861414: W tensorflow/core/kernels/queue_base.cc:295] _2_parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2018-04-05 14:47:12.861552: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2018-04-05 14:47:12.864402: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2018-04-05 14:47:12.864472: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2018-04-05 14:47:12.864504: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed**\r\n\r\nI am attaching below my code snippet ,\r\n\r\nserver = tf.train.Server(\r\n        cluster,\r\n        job_name=FLAGS.job_name,\r\n        task_index=FLAGS.task_index)\r\n   \r\n    if FLAGS.pipeline_id is None:\r\n        raise ValueError('pipeline_id [%s] was not recognized', FLAGS.pipeline_id)\r\n\r\n    #print('job name '+FLAGS.job_name)\r\n    #print('task index name ' + FLAGS.task_index)\r\n    if FLAGS.job_name == \"ps\":\r\n        server.join()\r\n    elif FLAGS.job_name == \"worker\":\r\n\tdataset = get_split(FLAGS.dataset_split_name, FLAGS.tf_records_dir, 'flowers', FLAGS.num_classes, labels_file)\r\n    print 'num of classes ------------------->', dataset.num_classes\r\n    images, _, labels = load_batch(FLAGS, dataset, batch_size=FLAGS.training_batch_size, height=FLAGS.image_resize,\r\n                                       width=FLAGS.image_resize, is_training=True)\r\n\r\n    # Get number of steps to decay\r\n    num_batches_per_epoch = int(dataset.num_samples / FLAGS.training_batch_size)\r\n    num_steps_per_epoch = num_batches_per_epoch  # Because one step is one batch processed\r\n    decay_steps = int(FLAGS.num_epochs_before_decay * num_steps_per_epoch)\r\n\r\n\twith tf.device(tf.train.replica_device_setter(\r\n\t\t\tworker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n\t\t\tcluster=cluster)):\r\n\t\t\t\t\t\twith slim.arg_scope(inception_v3_arg_scope()):\r\n\t\t\tlogits, end_points = inception_v3(images, num_classes = dataset.num_classes, is_training = True)\r\n\r\n\r\n\r\n\t\texclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits']\r\n\t\t#exclude = get_variables_to_exclude()\r\n\t\tfor i in exclude:\r\n\t\t   print \"var to exclude -> \",i\r\n\t\tvariables_to_restore = slim.get_variables_to_restore(exclude = exclude)\r\n\r\n\t\t#Perform one-hot-encoding of the labels\r\n\t\tone_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\r\n\r\n\t\t#Calculate loss\r\n\t\tloss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\r\n\t\ttotal_loss = tf.losses.get_total_loss()\r\n\r\n\t\t#Create the global step\r\n\t\tglobal_step = get_or_create_global_step()\r\n\r\n\r\n\t\t#Define your exponentially decaying learning rate\r\n\t\tlr = tf.train.exponential_decay(\r\n\t\t\tlearning_rate = FLAGS.initial_learning_rate,\r\n\t\t\tglobal_step = global_step,\r\n\t\t\tdecay_steps = decay_steps,\r\n\t\t\tdecay_rate = FLAGS.learning_rate_decay_factor,\r\n\t\t\tstaircase = True)\r\n\r\n\t\t#Get optimizer as configured by user\r\n\t\toptimizer = tf.train.AdamOptimizer(learning_rate = lr)\r\n\t\t#optimizer = getOptimizer(learning_rate = lr)\r\n\r\n\t\t#Create the train_op.\r\n\t\tvariables_to_train = get_variables_to_train()\r\n\t\t#for j in variables_to_train:\r\n\t\t  #print \"var to train \",j\r\n\t\t#vn = tf.trainable_variables()\r\n\t\ttrain_op = slim.learning.create_train_op(total_loss, optimizer,variables_to_train=variables_to_train)\r\n\r\n\r\n\t\tpredictions = tf.argmax(end_points['Predictions'], 1)\r\n\t\tprobabilities = end_points['Predictions']\r\n\t\taccuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\r\n\t\tmetrics_op = tf.group(accuracy_update, probabilities)\r\n\r\n\r\n\t\t#create all the summaries you need to monitor\r\n\t\ttf.summary.scalar('losses/Total_Loss', total_loss)\r\n\t\ttf.summary.scalar('accuracy', accuracy)\r\n\t\ttf.summary.scalar('learning_rate', lr)\r\n\t\tmy_summary_op = tf.summary.merge_all()\r\n\r\n\t\t#Define train step to run training operation\r\n\t\tdef train_step(sess, train_op, global_step):\r\n\r\n\t\t\tstart_time = time.time()\r\n\t\t\ttotal_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\r\n\t\t\ttime_elapsed = time.time() - start_time\r\n\r\n\r\n\t\t\tlogging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\r\n\r\n\t\t\treturn total_loss, global_step_count\r\n\t\t\t\r\n\tsaver = tf.train.Saver(variables_to_restore)\r\n        def restore_fn(sess):\r\n            return saver.restore(sess, FLAGS.checkpoint_path)\r\n\r\n\t#Create supervisor\r\n\tsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path, summary_op = None, init_fn = restore_fn)\r\n\r\n\r\n\t#Run the managed session\r\n\twith sv.prepare_or_wait_for_session(server.target) as sess:\r\n\t\tfor step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):\r\n\t\t\t#Log info at each epoch:\r\n\t\t\tif step % num_batches_per_epoch == 0:\r\n\t\t\t\tlogging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)\r\n\t\t\t\tlearning_rate_value, accuracy_value = sess.run([lr, accuracy])\r\n\t\t\t\tlogging.info('Current Learning Rate: %s', learning_rate_value)\r\n\t\t\t\tlogging.info('Current Streaming Training Accuracy: %s', accuracy_value)\r\n\r\n\r\n\t\t\t\tlogits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\r\n\t\t\t\tprint 'logits: \\n', logits_value\r\n\t\t\t\tprint 'Probabilities: \\n', probabilities_value\r\n\t\t\t\tprint 'predictions: \\n', predictions_value\r\n\t\t\t\tprint 'Labels:\\n:', labels_value\r\n\r\n\t\t\t#Log the summaries every 10 step.\r\n\t\t\tif step % FLAGS.steps_update_frequency == 0 and step != 0:\r\n\t\t\t\tloss, gs = train_step(sess, train_op, sv.global_step)\r\n\t\t\t\tsummaries = sess.run(my_summary_op)\r\n\t\t\t\tsv.summary_computed(sess, summaries)\r\n\t\t\t\tprint \"**** SAVE THE MODEL ****\"\r\n\t\t\t\tsv.saver.save(sess,sv.save_path,global_step=sv.global_step)\r\n\t\t\t\t##\r\n\r\n\t\t\t\tcheckpoint_path = tf.train.latest_checkpoint(train_logs_path)\r\n\t\t\t\ttraining_json = '{\"model_path\":\"'+ checkpoint_path +'\",\"no_of_steps\":' +str(gs)+',\"loss\":'+str(loss)+'}'\r\n\t\t\t\tcurrent_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\r\n\t\t\t\t#dbClient.store_metrices([(FLAGS.pipeline_id,FLAGS.operation_type,training_json,current_time)])\r\n\r\n\t\t\t\t\r\n\r\n\r\n\t\t\telse:\r\n\t\t\t\tloss, _ = train_step(sess, train_op, sv.global_step)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "When filing issues, please do fill out the [New Issue Template](https://github.com/tensorflow/tensorflow/issues/new) - in particular the details about the version number used, instructions to reproduce etc are helpful when diagnosing a problem.\r\n\r\nThat said, with the snippet provided, nothing obvious sticks out. Though, I notice that you're using `tf.train.Supervisor`, which was something that predates TensorFlow 1.0. Since TensorFlow 1.0 we've been recommending the use of `MonitoredTrainingSession`. Is it possible for you to use that instead? See the guide on distributed TensorFlow at: https://www.tensorflow.org/deploy/distributed", "I have the same kind of issue though used MonitoredTrainingSession. \r\nIn other similar issues, someone suggested to use the default value of num_tokens, code show as below.\r\n```\r\nsync_opt = tf.train.SyncReplicasOptimizer(\r\n            optimizer,\r\n            replicas_to_aggregate=num_workers,\r\n            total_num_replicas=num_workers)\r\n\r\nsync_replicas_hook = sync_opt.make_session_run_hook(is_chief, num_tokens=0)\r\n```\r\nIf `num_tokens=0`, the program will raise **CancelledError** Exception. \r\nIf code changed to like this, the program will be terminated normally. but distributed model couldn't sync.\r\n```\r\nsync_replicas_hook = sync_opt.make_session_run_hook(is_chief)\r\n```\r\nI want to know the reason and how to avoid the exception.\r\n\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "@deepak-2017  @fengrussell  I am facing exactly the same problem. Have you figured out the cause? Thanks.", "We got the similar error of `CancelledError: Step was cancelled by an explicit call to Session::Close()` in `TensorFlow 1.8.0`.", "Similar error in Tensorflow 1.12.1.", "Similar issue in Tensorflow 1.14.0.\r\n"]}, {"number": 18267, "title": "Validate the shape of TakeDataset and SkipDataset", "body": "The `count` inputs of the TakeDataset and SkipDataset require scalar. That was not validated before though.\r\n\r\nThis fix validates the shape of count for TakeDataset and SkipDataset.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18266, "title": "Cache lockfile already exists", "body": "Using the train_and_evaluate method from estimator API and cache policy on filesystem, I get an error because the evaluation starts before that all cache is written on the filesystem. So when the train runs the second time, after the evaluation, I get the error because it finds the lock file. \r\nIf the cache is written before the first evaluation I don't get the error.\r\n\r\nHere a snippet runnable on colab \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow import keras as ks\r\nimport itertools\r\n\r\n\r\ndef batch_reshape(a, b):\r\n    a.set_shape([None, None, 3])\r\n    b.set_shape([None, None, 1])\r\n    return (a, b)\r\n\r\n\r\ndef gen(n):\r\n    a = np.array(np.random.rand(5, 5, 3).astype(np.float32))\r\n    b = np.array(np.random.rand(5, 5, 1).astype(np.float32))\r\n    return (a, b)\r\n\r\n\r\ndef my_input_fn_train():\r\n    ds = tf.data.Dataset.range(100000)\r\n    ds = ds.map(\r\n        lambda x: tf.py_func(func=gen, inp=[x], Tout=[tf.float32, tf.float32]))\r\n    ds = ds.map(batch_reshape)\r\n    ds = ds.cache(\"/tmp/mycache_train\")\r\n\r\n    ds = ds.repeat(3)\r\n\r\n    value = ds.make_one_shot_iterator().get_next()\r\n\r\n    return {\"input_rgb\": value[0]}, {\"softmax\": value[1]}\r\n\r\n\r\ndef my_input_fn_eval():\r\n    ds = tf.data.Dataset.range(100000)\r\n    ds = ds.map(\r\n        lambda x: tf.py_func(func=gen, inp=[x], Tout=[tf.float32, tf.float32]))\r\n    ds = ds.map(batch_reshape)\r\n    ds = ds.cache(\"/tmp/mycache_eval\")\r\n\r\n    ds = ds.repeat(3)\r\n\r\n    value = ds.make_one_shot_iterator().get_next()\r\n\r\n    return {\"input_rgb\": value[0]}, {\"softmax\": value[1]}\r\n\r\n\r\ndef main():\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    input_rgb = ks.layers.Input(shape=(1, 5, 5, 3), name=\"input_rgb\")\r\n    x = ks.layers.Dense(1, activation='relu', name=\"Dense_1\")(input_rgb)\r\n    x = ks.layers.Dense(1, activation='softmax', name=\"softmax\")(x)\r\n    model = ks.models.Model(inputs=[input_rgb], outputs=[x])\r\n    model.compile(\r\n        loss={'softmax': 'binary_crossentropy'},\r\n        optimizer=tf.keras.optimizers.Adam())\r\n\r\n    estimator = ks.estimator.model_to_estimator(keras_model=model)\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn=my_input_fn_train)\r\n\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn=my_input_fn_eval, steps=5, throttle_secs=2)\r\n\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\nand here the output\r\n\r\n```\r\nINFO:tensorflow:Using the Keras model from memory.\r\nINFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpd1r865ry\r\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpd1r865ry', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb48f9ecf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 2 secs (eval_spec.throttle_secs) or training is finished.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/keras_model.ckpt\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpd1r865ry/model.ckpt.\r\nINFO:tensorflow:loss = 9.25762, step = 1\r\nINFO:tensorflow:global_step/sec: 612.893\r\nINFO:tensorflow:loss = 9.348354, step = 101 (0.166 sec)\r\nINFO:tensorflow:global_step/sec: 952.69\r\nINFO:tensorflow:loss = 8.319871, step = 201 (0.108 sec)\r\nINFO:tensorflow:global_step/sec: 838.883\r\nINFO:tensorflow:loss = 7.7207212, step = 301 (0.119 sec)\r\nINFO:tensorflow:global_step/sec: 917.55\r\nINFO:tensorflow:loss = 8.657611, step = 401 (0.110 sec)\r\nINFO:tensorflow:global_step/sec: 866.062\r\nINFO:tensorflow:loss = 8.567427, step = 501 (0.118 sec)\r\nINFO:tensorflow:global_step/sec: 862.789\r\nINFO:tensorflow:loss = 7.4227247, step = 601 (0.110 sec)\r\nINFO:tensorflow:global_step/sec: 847.577\r\nINFO:tensorflow:loss = 6.676866, step = 701 (0.118 sec)\r\nINFO:tensorflow:global_step/sec: 911.827\r\nINFO:tensorflow:loss = 7.4915752, step = 801 (0.112 sec)\r\nINFO:tensorflow:global_step/sec: 837.15\r\nINFO:tensorflow:loss = 7.7332606, step = 901 (0.120 sec)\r\nINFO:tensorflow:global_step/sec: 857.915\r\nINFO:tensorflow:loss = 7.283838, step = 1001 (0.116 sec)\r\nINFO:tensorflow:global_step/sec: 786.144\r\nINFO:tensorflow:loss = 8.61713, step = 1101 (0.127 sec)\r\nINFO:tensorflow:Saving checkpoints for 1150 into /tmp/tmpd1r865ry/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 8.417924.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2018-04-05-15:36:08\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/model.ckpt-1150\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Evaluation [1/5]\r\nINFO:tensorflow:Evaluation [2/5]\r\nINFO:tensorflow:Evaluation [3/5]\r\nINFO:tensorflow:Evaluation [4/5]\r\nINFO:tensorflow:Evaluation [5/5]\r\nINFO:tensorflow:Finished evaluation at 2018-04-05-15:36:08\r\n\r\nINFO:tensorflow:Saving dict for global step 1150: global_step = 1150, loss = 8.261229\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/model.ckpt-1150\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\n\r\n---------------------------------------------------------------------------\r\nAlreadyExistsError                        Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1360     try:\r\n-> 1361       return fn(*args)\r\n   1362     except errors.OpError as e:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1339           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\r\n-> 1340                                    target_list, status, run_metadata)\r\n   1341 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    515             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 516             c_api.TF_GetCode(self.status.status))\r\n    517     # Delete the underlying status object from memory otherwise it stays alive\r\n\r\nAlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAlreadyExistsError                        Traceback (most recent call last)\r\n<ipython-input-5-9d8332f50ddd> in <module>()\r\n     66 \r\n     67 if __name__ == \"__main__\":\r\n---> 68     main()\r\n\r\n<ipython-input-5-9d8332f50ddd> in main()\r\n     62         input_fn=my_input_fn_eval, steps=5, throttle_secs=2)\r\n     63 \r\n---> 64     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n     65 \r\n     66 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    419         '(with task id 0).  Given task id {}'.format(config.task_id))\r\n    420 \r\n--> 421   executor.run()\r\n    422 \r\n    423 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in run(self)\r\n    492         config.task_type != run_config_lib.TaskType.EVALUATOR):\r\n    493       logging.info('Running training and evaluation locally (non-distributed).')\r\n--> 494       self.run_local()\r\n    495       return\r\n    496 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in run_local(self)\r\n    624           input_fn=self._train_spec.input_fn,\r\n    625           max_steps=self._train_spec.max_steps,\r\n--> 626           hooks=train_hooks)\r\n    627 \r\n    628       # Final export signal: For any eval result with global_step >= train\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    350 \r\n    351     saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 352     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    353     logging.info('Loss for final step: %s.', loss)\r\n    354     return self\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n    889         loss = None\r\n    890         while not mon_sess.should_stop():\r\n--> 891           _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n    892       return loss\r\n    893 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    544                           feed_dict=feed_dict,\r\n    545                           options=options,\r\n--> 546                           run_metadata=run_metadata)\r\n    547 \r\n    548   def run_step_fn(self, step_fn):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n   1020                               feed_dict=feed_dict,\r\n   1021                               options=options,\r\n-> 1022                               run_metadata=run_metadata)\r\n   1023       except _PREEMPTION_ERRORS as e:\r\n   1024         logging.info('An error was raised. This may be due to a preemption in '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n   1111         raise six.reraise(*original_exc_info)\r\n   1112       else:\r\n-> 1113         raise six.reraise(*original_exc_info)\r\n   1114 \r\n   1115 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n   1096   def run(self, *args, **kwargs):\r\n   1097     try:\r\n-> 1098       return self._sess.run(*args, **kwargs)\r\n   1099     except _PREEMPTION_ERRORS:\r\n   1100       raise\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n   1168                                   feed_dict=feed_dict,\r\n   1169                                   options=options,\r\n-> 1170                                   run_metadata=run_metadata)\r\n   1171 \r\n   1172     for hook in self._hooks:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n    948 \r\n    949   def run(self, *args, **kwargs):\r\n--> 950     return self._sess.run(*args, **kwargs)\r\n    951 \r\n    952   def run_step_fn(self, step_fn, raw_session, run_with_hooks):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    903     try:\r\n    904       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 905                          run_metadata_ptr)\r\n    906       if run_metadata:\r\n    907         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1135     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1136       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1137                              feed_dict_tensor, options, run_metadata)\r\n   1138     else:\r\n   1139       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1353     if handle is None:\r\n   1354       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1355                            options, run_metadata)\r\n   1356     else:\r\n   1357       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1372         except KeyError:\r\n   1373           pass\r\n-> 1374       raise type(e)(node_def, op, message)\r\n   1375 \r\n   1376   def _extend_graph(self):\r\n\r\nAlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\r\nCaused by op 'IteratorGetNext', defined at:\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-5-9d8332f50ddd>\", line 68, in <module>\r\n    main()\r\n  File \"<ipython-input-5-9d8332f50ddd>\", line 64, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\", line 421, in train_and_evaluate\r\n    executor.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\", line 494, in run\r\n    self.run_local()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\", line 626, in run_local\r\n    hooks=train_hooks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 352, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 809, in _train_model\r\n    input_fn, model_fn_lib.ModeKeys.TRAIN))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 668, in _get_features_and_labels_from_input_fn\r\n    result = self._call_input_fn(input_fn, mode)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 760, in _call_input_fn\r\n    return input_fn(**kwargs)\r\n  File \"<ipython-input-5-9d8332f50ddd>\", line 28, in my_input_fn_train\r\n    value = ds.make_one_shot_iterator().get_next()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 330, in get_next\r\n    name=name)), self._output_types,\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 866, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nAlreadyExistsError (see above for traceback): There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\r\n```\r\n", "comments": ["@saeta Could this be an instance of the same bug as the `ds.cache().take(N).repeat()` one that you're looking at?", "@mrry I think that there is a conceptual problem with `train_and_evaluate`. \r\nMainly you control train/eval phases with `EvalSpec` (with a default throttle_secs=600) and/or check point in `RunConfig`.\r\nIf the dataset is quite big you cannot evaluate until the cache is done cause a new TrainSpec call will rebuild the input/dataset pipeline and you will find a lock on the cache dir.\r\nThere could be a not so trivial workaround with `RunConfig` `save_checkpoints_steps` to block throttle until you will not have covered the epoch  but in the distributed setup I suppose that the dataset cache is working locally per node so is it still plausible?\r\n\r\nDo you see any other solution (that I suppose still require documentation)?\r\n  ", "/cc @isaprykin for suggestions about how to use `train_and_evaluate()`.", "If I understand the problem correctly, user defines the dataset with cache(filename). For this input_fn, as the input_fn is invoked multiple times, train_and_evalaute will error out. Is this the issue? \r\n\r\nUnfortunately, train_and_evaluate does not specify the contract how many times train input_fn and eval input_fn will be invoked today. It will limit the implementation space we have. \r\n\r\n@mrry can user change the code into following way? \r\n\r\n    ds = tf.data.Dataset.range(100000)\r\n    ds = ds.map(\r\n        lambda x: tf.py_func(func=gen, inp=[x], Tout=[tf.float32, tf.float32]))\r\n    ds = ds.map(batch_reshape)\r\n    ds = ds.cache(\"/tmp/mycache_train\")\r\n    ds = ds.repeat(3)\r\n\r\n    def my_input_fn_train():\r\n        value = ds.make_one_shot_iterator().get_next()\r\n       return {\"input_rgb\": value[0]}, {\"softmax\": value[1]}", "@xiejw Unfortunately that won't work because `ds` will be bound to a graph that is different from the estimator's graph. \r\n\r\nI'm not sure why `train_and_evaluate()` is calling the input function multiple times\u2014it would be more efficient to build a single pipeline and split the elements across the device(s) in the process\u2014but the right solution would seem to be allowing concurrent iterators to build a cache with the same eventual filename.", "[slightly offtopic] too many issues with the high level APIs. I really hope that there is a  concrete plan with https://github.com/tensorflow/tensorflow/issues/16182#issuecomment-372508885", "What do you think about @ispirmustafa [comment](https://github.com/tensorflow/tensorflow/issues/18016#issuecomment-380603186)?", "current local run implementation is something like:\r\nfor ...:\r\n  estimator.train(...)\r\n  estimator.evaluate(...)\r\n\r\nbut making train_and_evaluate local mode to create this graphs only once is a good feature request. \r\n\r\nfor now as a workaround you can do following:\r\n```\r\ncnt = 0\r\ndef input_fn():\r\n  cnt += 1\r\n  cache_file_name = ... + cnt\r\n```", "@ispirmustafa The workaround It will be useless cause will skip the lock but you will never match the cache. Also generally what about cache matching if the pipeline is rebuild and shuffled at every Train phase?", "@bhack could you please explain more? The error message is about using same file as cache. this workaround resolves that error. what do you mean by \"skip the lock and never match\"?", "@ispirmustafa  I.e. You have a large dataset over a network file system that needs to be cached over multiple train/evaluate rounds. With this you will workaround the lock but you will never match the cache after the first evaluation.", "Also any temporary workaround is quite invalidated by limits/bugs in https://github.com/tensorflow/tensorflow/issues/17650", "Some news at https://github.com/tensorflow/tensorflow/issues/19062#issuecomment-400129963. But I think it will not solve this.", "Do you need to fix  https://github.com/tensorflow/tensorflow/issues/19062#issuecomment-401736720?", "tensorflow 2.0.0-alpha0\r\ncache lockfile already exists.  When going for the 2nd epoch, of evaluation, the previous lock file isn't removed i can see it on the file system.  Doesnt happen with training data hence it fires this error as it cant get hold of ds.cache(file) it seems:\r\nCode:\r\n\r\n\r\n    def get_ds_tfdata_cache(self, X_train, y_train, batch_size, load_preprocess_image, filename):\r\n        import tensorflow as tf\r\n        from Source.tfdata.PathConstants import Constants\r\n        cachedir = Constants().get_tfcache_dir_path()\r\n        ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\r\n        ds = ds.map(load_preprocess_image)\r\n        ds = ds.cache(filename=cachedir + \"/\" + filename)\r\n        ds = ds.apply(\r\n            tf.data.experimental.shuffle_and_repeat(buffer_size=batch_size))\r\n        ds = ds.batch(batch_size=batch_size)\r\n        ds = ds.prefetch(buffer_size=batch_size)\r\n        return ds\r\n\r\nimage_ds_train = data.get_ds_tfdata_cache(X_train=X_train, y_train=y_train, batch_size=params_list.get_batch_size(),\r\n                                          load_preprocess_image=helpermethods.load_and_preprocess_image,\r\n                                          filename=params_list.get_tf_cache_train_filename())\r\n\r\nimage_ds_validate = data.get_ds_tfdata_cache(X_train=X_train, y_train=y_train,\r\n                                             batch_size=params_list.get_batch_size(),\r\n                                             load_preprocess_image=helpermethods.load_and_preprocess_image,\r\n                                             filename=params_list.get_tf_cache_test_filename())\r\n\r\nearly_stop_patience = 5\r\ntraining_epochs = 2\r\nhistory = model.fit(x=image_ds_train, validation_data=image_ds_validate, epochs=training_epochs,\r\n                    steps_per_epoch=steps_per_epoch,\r\n                    validation_steps=validation_steps, callbacks=[early_stop, tensorboard])\r\n\r\n\r\nError:\r\n2019-05-09 11:12:53.961797: W tensorflow/core/framework/op_kernel.cc:1431] OP_REQUIRES failed at iterator_ops.cc:988 : Already exists: There appears to be a concurrent caching iterator running - cache lockfile already exists ('./tfdata_dir/tfcache_dir/cache_validate.tf-data_0.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1557364206\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 53, in <module>\r\n  File \"C:\\ML_VirtualEnv\\tf2_alpha\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 791, in fit\r\n    initial_epoch=initial_epoch)\r\n  File \"C:\\ML_VirtualEnv\\tf2_alpha\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1515, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"C:\\ML_VirtualEnv\\tf2_alpha\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 315, in model_iteration\r\n    steps_name='validation_steps')\r\n  File \"C:\\ML_VirtualEnv\\tf2_alpha\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 213, in model_iteration\r\n    batch_data = _get_next_batch(generator, mode)\r\n  File \"C:\\ML_VirtualEnv\\tf2_alpha\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 355, in _get_next_batch\r\n    generator_output = next(generator)\r\n  File \"C:\\ML_VirtualEnv\\tf2_alpha\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 556, in __next__\r\n    return self.next()\r\n  File \"C:\\ML_VirtualEnv\\tf2_alpha\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 585, in next\r\n    return self._next_internal()\r\n  File \"C:\\ML_VirtualEnv\\tf2_alpha\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 577, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"C:\\ML_VirtualEnv\\tf2_alpha\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 1984, in iterator_get_next_sync\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('./tfdata_dir/tfcache_dir/cache_validate.tf-data_0.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1557364206 [Op:IteratorGetNextSync]", "+1 this is a bug in the design.  cache files should support concurrent usage since one can easily get there via `tf.data.experimental.parallel_interleave()`.  ", "I wrote a little utility function to get rid of directories containing lock files\r\n```\r\nimport os\r\nimport shutil\r\nfrom pathlib import Path\r\n\r\ndef delete_trailing_lock_files(cache_root):\r\n  lock_file_dirs = {}\r\n  for filename in Path(cache_root).glob('**/*.lock*'):\r\n    lock_file_dirs[os.path.split(filename)[0] + '/'] = ''\r\n  for dir in lock_file_dirs.keys():\r\n    try:\r\n      shutil.rmtree(dir)\r\n    except:\r\n      print('failed removing dir', filename)\r\n```\r\n\r\nNote that what you want to do is to create a single dataset iterator and keep using it:\r\n```\r\nval_dataset = tf.data.Dataset.range(10)\r\nval_dataset = val_dataset.cache('/tmp/cacheset/')\r\nval_dataset = val_dataset.repeat()\r\n\r\nval_iterator = iterator(val_dataset) #Never do this more than once!\r\n```", "I also find the `.cache` method of `tf.data.Dataset` a bit odd, with the docs stating:\r\n \r\n> filename: A tf.string scalar tf.Tensor, representing the name of a directory on the filesystem to use for caching tensors in this Dataset. If a filename is not provided, the dataset will be cached in memory.\r\n\r\ne.g. asking for `filename` when `filename` actually represents the name of a `directory`. \r\n\r\nPart of the `cache` method problem is that the cached file is not always auto-cleaned up.\r\n\r\nI think adding a positional argument `overwrite` solves this issue by allowing users to `cache` to a known location like:\r\n\r\n```python\r\nds_train = ds_train.cache(filename=os.path.join(cache_dir, 'train'), overwrite=True)\r\nds_valid = ds_valid.cache(filename=os.path.join(cache_dir, 'valid'), overwrite=True)\r\n```\r\n", "@lenlen, did you try the @SumNeuron's workaround. Please let us know if that solves your problem. Thanks!", "@gadagashwini the `overwrite` kwarg does not exist, it's just a proposal: https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/data/ops/dataset_ops.py#L1738-L1740 \r\n\r\n`overwrite` would be helpful but wouldn't solve the concurrency problem.  The op needs a do-over.", "This is fixed with TF version 1.15.0 Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/18266\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/18266\">No</a>\n"]}, {"number": 18265, "title": "tf.Print does not print anything in Spyder IPython console", "body": "### System information\r\n- Both Windows and Debian\r\n- Anaconda3\r\n- Python3.6\r\n\r\n### Describe the problem\r\nI tried this simple code, but nothing was printed! I use Anaconda3, python 3.6. Anyone can give a hint? Thanks.\r\n\r\n### Source code / logs\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\na = tf.constant([1.0, 3.0])\r\na = tf.Print(a, [a], message=\"This is a: \")\r\nb = tf.add(a, a).eval()\r\n\r\n###I posted the problem here on stackoverflow:\r\nhttps://stackoverflow.com/questions/49676232/tf-print-does-not-print-anything-in-spyder-ipython-console", "comments": ["Nevermind, it's not Tensorflow problem, it's Spyder problem. I tried with PyCharm, and it works like a charm :)"]}, {"number": 18264, "title": "tensorflow new feature request: tf.regex_match", "body": "The new tf.regex_replace feature in tensorflow 1.7.0 works fine for modifying data and returning tf.string. However to filter a dataset, a separate method is needed in order to return tf.bool for use in tf.dataset.filter\r\n\r\nI would like to be able to filter blank lines, or lines with only whitespace from a dataset, like:\r\ndataset = dataset.filter(lambda line: tf.regex_match(line, \"^\\s*$\"))\r\n\r\nCurrently, I can do this using tf.py_func, see details: https://stackoverflow.com/questions/49654187/tensorflow-removing-blank-lines-in-dataset\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I'm open to doing this but it might entail making re2 more portable, or whipping something up in Ragel. I'm not sure if we have time for that now. A workaround exists. It's also possible to build custom ops with re2 which is already in the Bazel codebase. I'm leaning towards closing and possibly revisiting later. Pinging @vrv for a second glance.", "Have I written custom code N/A\r\nOS Platform and Distribution N/A\r\nTensorFlow installed from N/A\r\nTensorFlow version N/A\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A\r\n", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We probably don't have time for this right now. I'm not certain if this could be implemented in portable way. POSIX.2 libc regex probably isn't available on Windows. @vrv has mentioned in the past we've encountered portability issues with RE2. If I see more community support for this, I'll reopen.", "This is probably a 'contributions welcome'; given that regex_replace exists, regex_match could probably be added using a similar pattern.  https://www.tensorflow.org/extend/adding_an_op has the information one needs to add such an op."]}, {"number": 18263, "title": "Branch 191729654", "body": "", "comments": []}, {"number": 18262, "title": "image Segmentation:why is the tensor value of the dimension resolved when reading tfrecord files is twice as much as is required to rescontruct the dimension\uff1f", "body": "ERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 26136 values, but the requested shape has 13068\r\n\t [[Node: Reshape = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](DecodeRaw, Reshape/shape)]]\r\nERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](DecodeRaw_1, Reshape_1/shape)]]\r\nERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](DecodeRaw_1, Reshape_1/shape)]]\r\nERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](DecodeRaw_1, Reshape_1/shape)]]\r\nERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](DecodeRaw_1, Reshape_1/shape)]]\r\nERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](DecodeRaw_1, Reshape_1/shape)]]\r\nERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18261, "title": "How to set the truncation time series length parameter for RNN training in this example: tensorflow/tensorflow/contrib/timeseries/examples/lstm.py", "body": "In the function of tf.contrib.timeseries.ARRegressor, we can set the input_window_size and output_window_size for building a time series forecasting model.\r\nBut in this example, we can only set the parameter of num_units of cell and num_features of input data for _LSTMModel\uff0cbut we can not set the parameter of the truncation time series length in this model of TFTS.\r\n", "comments": ["Please ask questions on StackOverflow instead of opening bugs. For timeseries stuff feel free to send the SO link to allenl@google.com.\r\n\r\nBut the LSTM example model does not operate on a fixed window; the same model can take any window size, like `dynamic_rnn`. You can set the truncated backprop length by setting a different window size in the input function. For `ARRegressor` the model and input function window sizes have to match for training."]}, {"number": 18260, "title": "[BUG] [1.8] prefetch_to_device() doesn't work with eager Iterators", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES, Minimal (non-) working example can be seen below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pip install tf-nightly-gpu (05.04.2018)\r\n- **TensorFlow version (use command below)**: 1.8.0.dev20180329\r\n- **Python version**: Python 3.6.3\r\n- **Bazel version (if compiling from source)**: --\r\n- **GCC/Compiler version (if compiling from source)**: --\r\n- **CUDA/cuDNN version**: CUDA 9, CuDNN 7.1\r\n- **GPU model and memory**: 2x GTX 1080 8GB\r\n- **Exact command to reproduce**: \r\n```python\r\nimport tensorflow as tf\r\ncontrib_data=tf.contrib.data\r\ndata=tf.data\r\nimport tensorflow.contrib.eager as tfe\r\ntf.enable_eager_execution()\r\nds=data.Dataset.range(50)\r\nds=ds.apply(contrib_data.prefetch_to_device(\"/gpu:1\"))\r\nit=tfe.Iterator(ds)\r\n```\r\n### Describe the problem\r\nEven when applying prefetch_to_device as the last operation, it throws the following error:\r\n\r\n### logs\r\n```\r\nWARNING:tensorflow:From C:\\Users\\PHANTOM\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\n2018-04-05 14:40:24.616506: I C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-04-05 14:40:24.831563: I C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1355] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.835\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.52GiB\r\n2018-04-05 14:40:24.883091: I C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1355] Found device 1 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.835\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.52GiB\r\n2018-04-05 14:40:24.883562: I C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1434] Adding visible gpu devices: 0, 1\r\n2018-04-05 14:40:25.804203: I C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:922] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-05 14:40:25.804487: I C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:928]      0 1 \r\n2018-04-05 14:40:25.804689: I C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:941] 0:   N N \r\n2018-04-05 14:40:25.804889: I C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:941] 1:   N N \r\n2018-04-05 14:40:25.805180: I C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1052] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6303 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-04-05 14:40:26.469466: I C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1052] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 6303 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"MWE.py\", line 17, in <module>\r\n    it=tfe.Iterator(ds)\r\n  File \"C:\\Users\\PHANTOM\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\eager\\python\\datasets.py\", line 76, in __init__\r\n    super(Iterator, self).__init__(dataset)\r\n  File \"C:\\Users\\PHANTOM\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 463, in __init__\r\n    ds_variant = dataset._as_variant_tensor()  # pylint: disable=protected-access\r\n  File \"C:\\Users\\PHANTOM\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\prefetching_ops.py\", line 154, in _as_variant_tensor\r\n    raise NotImplementedError(\"`prefetch_to_device()` must be the last \"\r\nNotImplementedError: `prefetch_to_device()` must be the last transformation in a dataset pipeline.\r\n```", "comments": ["Is this related to the migration of eager mode switch in 1.7? I believe they are available outside contrib now. ", "@byronyi which specific migration do you mean?\r\ntfe.Iterator?\r\n\r\nBecause I used the migrated switch...\r\n\r\ni also tried:\r\n```python\r\nimport tensorflow as tf\r\ncontrib_data=tf.contrib.data\r\n\r\ntf.enable_eager_execution()\r\n\r\nds=tf.data.Dataset.range(50)\r\nds=ds.apply(contrib_data.prefetch_to_device(\"/gpu:1\"))\r\n\r\nfor c in ds:\r\n    print(c)\r\n```\r\n\r\nsame error.", "This is currently broken and I have a fix in review, but here's a temporary workaround that should work:\r\n\r\n```python\r\nimport tensorflow as tf\r\ncontrib_data=tf.contrib.data\r\ndata=tf.data\r\nimport tensorflow.contrib.eager as tfe\r\ntf.enable_eager_execution()\r\n\r\nds=data.Dataset.range(50)\r\nwith tf.device(\"/gpu:1\"):\r\n  it=tfe.Iterator(ds)\r\n```", "yeah, that works. Doesn't have the performance benefits of `prefetch_to_device()` though...", "It is supposed to have the same benefits... the `tfe.Iterator` has an on-device buffer internally when it\u2019s placed on a non-CPU device. Are you observing different performance?", "In my current Implementation I see no difference between putting the Iterator on GPU or not.  \r\nIn both cases my GPU spends a decent amount of time copying data, though that might be for different batches, GPU Compute load stays the same.\r\n\r\nDoesn't mean there is no difference though. Might just be bound somewhere else.  \r\nI'll have to try disabling eager mode, using prefetch_to_device and see if it's faster then...", "Update: I checked without my model code (just putting some dummy GPU op in place) and that is significantly faster with the Iterator on GPU.", "Thanks for confirming!", "Here's how to check if anyone's interested:\r\n<details><p>\r\n  <summary>Click to expand</summary>\r\n\r\n```python\r\nimport tensorflow as tf\r\ncontrib_data=tf.contrib.data\r\nimport tensorflow.contrib.eager as tfe\r\ncp=tf.ConfigProto()\r\ncp.allow_soft_placement=True\r\ncp.gpu_options.allow_growth=True\r\ncp.log_device_placement=False\r\ntf.enable_eager_execution(cp)\r\nimport time\r\n\r\n##GPU Version\r\n\r\n#generate some large dummy values\r\nds=tf.data.Dataset.range(100000*64*20)\r\nds=ds.batch(100000)\r\nds=ds.map(lambda x: tf.reshape(x,(100000,)))\r\nds=ds.batch(64)\r\n#cache them to not be input-bound\r\nds=ds.cache('')\r\nds=ds.repeat(1000)\r\n#prefetch on CPU\r\nds=ds.prefetch(5)\r\n\r\n#create Iterator on GPU\r\nwith tf.device(\"/gpu:1\"):\r\n    it=tfe.Iterator(ds)\r\n\r\n#make sure all batches are cached \r\n#so it's not bound anywhere else    \r\nfor i in range(20):\r\n    next(it)\r\n\r\nmt=0\r\nnt=0\r\nt=time.time()\r\nwith tf.device(\"/gpu:1\"):\r\n    for c in it:\r\n        o=c*c\r\n        nt+=1\r\nmt=time.time()-t\r\nprint(mt/nt)#>>>0.01107s/batch\r\n\r\n##CPU Version\r\n\r\n#generate some large dummy values\r\nds=tf.data.Dataset.range(100000*64*20)\r\nds=ds.batch(100000)\r\nds=ds.map(lambda x: tf.reshape(x,(100000,)))\r\nds=ds.batch(64)\r\n#cache them to not be input-bound\r\nds=ds.cache('')\r\nds=ds.repeat(1000)\r\n#prefetch on CPU\r\nds=ds.prefetch(5)\r\n\r\n#create Iterator on CPU\r\nwith tf.device(\"/cpu:0\"):\r\n    it=tfe.Iterator(ds)\r\n\r\n#make sure all batches are cached \r\n#so it's not bound anywhere else    \r\nfor i in range(20):\r\n    next(it)\r\n\r\nmt=0\r\nnt=0\r\nt=time.time()\r\nwith tf.device(\"/gpu:1\"):\r\n    for c in it:\r\n        o=c*c\r\n        nt+=1\r\nmt=time.time()-t\r\nprint(mt/nt)#>>0.02176s/batch\r\n```\r\n</p>\r\n</details>", "Hi guys, I facing that error.How can I do that?\r\n\r\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\n"]}, {"number": 18259, "title": "give some writing updates to tensorflow/contrib/slim/README.md", "body": "", "comments": ["@jpienaar Please take a look if the updates are necessary. Thanks. "]}, {"number": 18258, "title": "Feature Request: Porting tensorflow to onion omega 2+", "body": "I have been working with onion omega 2+ an iot computer with following specifications :\r\nOperating system-OpenWrt\r\nSystem-on-chip used MediaTek MT7688\r\nCPU-580 MHz 32-bit computing Single-core MIPS 24KEc\r\nMemory-128 MB for Omega2 Plus\r\nStorage-32 MB and a MicroSDHC slot\r\nGraphics-No graphic\r\nPower-0.6 Watts\r\n\r\nI would like to run inception V3 model but I am unable to install tensorflow on omega 2+. Is it possible to run tensorflow or tensorflow lite on omega 2+????", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Update 1\r\nNo I have not written any custom code \r\nI am now using OnionOS on onion omega 2+\r\nTensorflow installed : Unable to install tensorflow\r\nTensorflow version : N/A\r\nBazel version : N/A\r\nCUDA/cuDNN version : N/A\r\nGPU model and memory : N/A\r\nExact command to reproduce : N/A", "Since you haven't given any indictation of what problem you have and what you have tried. I am closing this issue for now. This is certainly  currently an unsupported platform, but we are willing to help you if you provide enough information to provide suggestions. Thanks!"]}, {"number": 18257, "title": "tf.while example is not working in eager mode", "body": "\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.7, release\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: 9.0 / 7.0\r\n- **GPU model and memory**:  GTX TITAN, 6GB\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\n`tf.while` example is not working in eager mode\r\n\r\n### Source code / logs\r\n\r\nHere is the code \r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution() # the only line added to the example\r\n\r\ni = tf.constant(0)\r\nc = lambda i: tf.less(i, 10)\r\nb = lambda i: tf.add(i, 1)\r\nr = tf.while_loop(c, b, [i])\r\n```\r\n\r\nresult:\r\nTypeError: Cannot iterate over a scalar tensor.\r\n\r\n### Reason\r\n\r\n```python\r\nb = lambda i: tf.add(i, 1)       # original\r\nb = lambda i: (tf.add(i, 1), )   # fixed, working \r\n```\r\n\r\n- Body code (`b`) returns a single scalar, which code later tries to unpack.\r\n- Eager code doesn't support auto-wrapping single item to tuple\r\n\r\nPossible solutions:\r\n- require body function to always return tuple\r\n- always check if body output is a single variable", "comments": ["Thanks for the report, @akshaym is looking into fixing that.", "This is fixed on TF 1.15 but broken on TF 2.0.", "I can reproduce this on 2.0. @saxenasaurabh can you please help take a look?", "On Colab, TF 2, api: https://www.tensorflow.org/api_docs/python/tf/while_loop\r\n\r\nExample in api doc:\r\n```\r\ni = tf.constant(0)\r\nc = lambda i: tf.less(i, 10)\r\nb = lambda i: tf.add(i, 1)\r\nr = tf.while_loop(c, b, [i])\r\n```\r\n\r\nTF 2 on Eager mode by default:<br/>\r\n`TypeError: Cannot iterate over a scalar tensor.`\r\n\r\nMy work-around (wrap in autograph `@tf.function`):\r\n```\r\n@tf.function\r\ndef f():\r\n  i = tf.constant(0)\r\n  c = lambda i: tf.less(i, 10)\r\n  b = lambda i: tf.add(i, 1)\r\n  r = tf.while_loop(c, b, [i])\r\n  return r\r\n```", "Has this bug been solved? I run tf2.1 and the problem is still there. ", "I was able to replicate and resolve this issue in [2.7 ](https://colab.sandbox.google.com/gist/mohantym/38a9d55e009edf9022393bf073f37dab/github_18257.ipynb)", "It turns out this has been causing a headache for me through several versions of TF 2.x. I anxiously await a resolution.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 18256, "title": "use lib static tensorflow-core.a error when build", "body": "undefined reference to 'google::protobuf::internal::fixed_address_empty_string' ....\r\ni think my program can't find lib protobuf to used. Even I have already lib protbuf.a and protobuf.so. Before i use lib tensorflow-core.a I have used lib dynamic: tensorflow_cc.so and lib tensorflow_framework.so i build with bazel, and lib protbuf.so include. I don't know ho to fix this error. Can anyone help me?\r\n#issue using static lib tensorflow", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "the  Issue about TensorFlow installed from source lib static for my program have problem.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18255, "title": "InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'x_1' with dtype float and shape [?,300,300,3] \t [[Node: x_1 = Placeholder[dtype=DT_FLOAT, shape=[?,300,300,3], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize, but I'm having trouble understanding the problem here. Closing this out. If there is a bug or feature request, please feel free to file a new issue but do fill in all the details asked for in the [New Issue Template](https://github.com/tensorflow/tensorflow/issues/new). It seems all of the information asked for in there was left blank.\r\n\r\nIn particular, more information about the problem (what you're trying to do, what errors are you seeing etc.), ideally, detailed instructions to reproduce the problem will be required to provide any assistance. Thanks!"]}, {"number": 18253, "title": "undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringB5cxx11E error for building r1.7 from source Ubuntu 16.10", "body": "bazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc: symbol lookup error: bazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringB5cxx11E\r\n\r\n\r\nmgyong@mgyong-Precision-5510:/tmp/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n..........\r\nWARNING: /home/mgyong/.cache/bazel/_bazel_mgyong/e5cce820cc082410b4fcc604db349066/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/mgyong/.cache/bazel/_bazel_mgyong/e5cce820cc082410b4fcc604db349066/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nWARNING: /home/mgyong/.cache/bazel/_bazel_mgyong/e5cce820cc082410b4fcc604db349066/external/grpc/WORKSPACE:1: Workspace name in /home/mgyong/.cache/bazel/_bazel_mgyong/e5cce820cc082410b4fcc604db349066/external/grpc/WORKSPACE (@com_github_grpc_grpc) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions\r\nWARNING: /tmp/tensorflow/tensorflow/core/BUILD:1955:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /tmp/tensorflow/tensorflow/tensorflow.bzl:1179:30\r\nWARNING: /tmp/tensorflow/tensorflow/core/BUILD:1955:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /tmp/tensorflow/tensorflow/tensorflow.bzl:1179:30\r\nWARNING: /tmp/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /tmp/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (260 packages loaded).\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy-c.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nexternal/snappy/snappy.cc: In member function 'void snappy::SnappySinkAllocator::Flush(size_t)':\r\nexternal/snappy/snappy.cc:1403:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < blocks_.size(); ++i) {\r\n                     ~~^~~~~~~~~~~~~~~~\r\nIn file included from external/snappy/snappy-internal.h:34:0,\r\n                 from external/snappy/snappy.cc:30:\r\nexternal/snappy/snappy.cc: In instantiation of 'bool snappy::SnappyScatteredWriter<Allocator>::AppendFromSelf(size_t, size_t) [with Allocator = snappy::SnappySinkAllocator; size_t = long unsigned int]':\r\nexternal/snappy/snappy.cc:715:13:   required from 'void snappy::SnappyDecompressor::DecompressAllTags(Writer*) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>]'\r\nexternal/snappy/snappy.cc:799:3:   required from 'bool snappy::InternalUncompressAllTags(snappy::SnappyDecompressor*, Writer*, snappy::uint32) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>; snappy::uint32 = unsigned int]'\r\nexternal/snappy/snappy.cc:1460:78:   required from here\r\nexternal/snappy/snappy.cc:1316:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (PREDICT_TRUE(offset - 1u < op_ptr_ - op_base_ && op_end <= op_limit_)) {\r\n                      ~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/snappy/snappy-stubs-internal.h:80:25: note: in definition of macro 'PREDICT_TRUE'\r\n #define PREDICT_TRUE(x) x\r\n                         ^\r\nINFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/debug/debugger_event_metadata.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/python/framework/cpp_shape_inference.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/worker.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/master.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/contrib/lite/toco/types.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/contrib/lite/toco/toco_flags.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/contrib/lite/toco/model_flags.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/profiler/profile.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/contrib/cloud/kernels/bigquery_table_partition.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/debug/debug_service.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From Compiling tensorflow/core/platform/stacktrace_handler.cc [for host]:\r\ntensorflow/core/platform/stacktrace_handler.cc: In function 'void tensorflow::testing::InstallStacktraceHandler()':\r\ntensorflow/core/platform/stacktrace_handler.cc:118:51: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]\r\n       (void)write(STDERR_FILENO, buf, strlen(buf));\r\n                                                   ^\r\ntensorflow/core/platform/stacktrace_handler.cc:125:51: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]\r\n       (void)write(STDERR_FILENO, buf, strlen(buf));\r\n                                                   ^\r\ntensorflow/core/platform/stacktrace_handler.cc: In function 'void tensorflow::testing::StacktraceHandler(int, siginfo_t*, void*)':\r\ntensorflow/core/platform/stacktrace_handler.cc:80:47: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]\r\n   (void)write(STDERR_FILENO, buf, strlen(buf));\r\n                                               ^\r\ntensorflow/core/platform/stacktrace_handler.cc:90:70: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]\r\n   (void)write(STDERR_FILENO, stacktrace.c_str(), stacktrace.length());\r\n                                                                      ^\r\ntensorflow/core/platform/stacktrace_handler.cc: In function 'void tensorflow::testing::SafePrintStackTrace()':\r\ntensorflow/core/platform/stacktrace_handler.cc:47:59: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]\r\n   (void)write(STDERR_FILENO, begin_msg, strlen(begin_msg));\r\n                                                           ^\r\ntensorflow/core/platform/stacktrace_handler.cc:58:55: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]\r\n   (void)write(STDERR_FILENO, end_msg, strlen(end_msg));\r\n                                                       ^\r\nINFO: From Compiling tensorflow/stream_executor/stream_executor_pimpl.cc [for host]:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/stream_executor/lib/status.h:21,\r\n                 from ./tensorflow/stream_executor/stream_executor_pimpl.h:25,\r\n                 from tensorflow/stream_executor/stream_executor_pimpl.cc:20:\r\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = int; T2 = long long unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':\r\ntensorflow/stream_executor/stream_executor_pimpl.cc:634:3:   required from here\r\n./tensorflow/core/platform/default/logging.h:230:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n                         ==)  // Compilation error with CHECK_EQ(NULL, x)?\r\n                          \r\n./tensorflow/core/platform/macros.h:79:29: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (x)\r\n                             ^\r\n./tensorflow/core/platform/default/logging.h:229:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\r\n ^~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/kernels/initializable_lookup_table.cc [for host]:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/core/lib/core/errors.h:19,\r\n                 from ./tensorflow/core/framework/tensor_shape.h:23,\r\n                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:20,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/framework/lookup_interface.h:19,\r\n                 from ./tensorflow/core/kernels/initializable_lookup_table.h:19,\r\n                 from tensorflow/core/kernels/initializable_lookup_table.cc:16:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:372:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:340:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n                              \r\n./tensorflow/core/platform/macros.h:78:30: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (x)\r\n                              ^\r\n./tensorflow/core/util/tensor_format.h:340:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:381:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:355:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n                              \r\n./tensorflow/core/platform/macros.h:78:30: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (x)\r\n                              ^\r\n./tensorflow/core/util/tensor_format.h:355:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nINFO: From Compiling tensorflow/core/kernels/lookup_util.cc [for host]:\r\ntensorflow/core/kernels/lookup_util.cc: In member function 'virtual void tensorflow::lookup::{anonymous}::TextFileLineIterator::Next()':\r\ntensorflow/core/kernels/lookup_util.cc:134:46: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (std::max(key_index_, value_index_) >= tokens.size()) {\r\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/core/lib/core/errors.h:19,\r\n                 from ./tensorflow/core/framework/tensor_shape.h:23,\r\n                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:20,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/framework/lookup_interface.h:19,\r\n                 from ./tensorflow/core/kernels/lookup_util.h:19,\r\n                 from tensorflow/core/kernels/lookup_util.cc:16:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:372:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:340:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n                              \r\n./tensorflow/core/platform/macros.h:78:30: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (x)\r\n                              ^\r\n./tensorflow/core/util/tensor_format.h:340:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:381:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:355:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n                              \r\n./tensorflow/core/platform/macros.h:78:30: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (x)\r\n                              ^\r\n./tensorflow/core/util/tensor_format.h:355:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nINFO: From Compiling tensorflow/core/grappler/utils.cc [for host]:\r\ntensorflow/core/grappler/utils.cc: In function 'void tensorflow::grappler::PermuteNodesInPlace(tensorflow::GraphDef*, std::vector<int>*, bool)':\r\ntensorflow/core/grappler/utils.cc:319:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     while (n != (*permutation)[n]) {\r\n               \r\ntensorflow/core/grappler/utils.cc: In member function 'std::__cxx11::string tensorflow::grappler::SimpleGraphView::PrintToString() const':\r\ntensorflow/core/grappler/utils.cc:422:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int j = 0; j < outputs(i).size(); ++j) {\r\n                     ~~^~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/stream_executor/stream.cc [for host]:\r\ntensorflow/stream_executor/stream.cc: In function 'std::__cxx11::string perftools::gputools::{anonymous}::ToVlogString(perftools::gputools::dnn::DataType)':\r\ntensorflow/stream_executor/stream.cc:184:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\nINFO: From Compiling tensorflow/core/protobuf/config.pb_text.cc [for host]:\r\nbazel-out/host/genfiles/tensorflow/core/protobuf/config.pb_text.cc: In function 'bool tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::ConfigProto*)':\r\nbazel-out/host/genfiles/tensorflow/core/protobuf/config.pb_text.cc:826:23: warning: 'map_value' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n       (*map)[map_key] = map_value;\r\n                        \r\nbazel-out/host/genfiles/tensorflow/core/protobuf/config.pb_text.cc:817:9: note: 'map_value' was declared here\r\n   int32 map_value;\r\n         ^~~~~~~~~\r\nINFO: From Compiling tensorflow/core/common_runtime/gpu/process_state.cc [for host]:\r\ntensorflow/core/common_runtime/gpu/process_state.cc:62:6: warning: 'bool tensorflow::{anonymous}::useCudaMemoryGuardAllocator()' defined but not used [-Wunused-function]\r\n bool useCudaMemoryGuardAllocator() {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/process_state.cc:56:6: warning: 'bool tensorflow::{anonymous}::useCudaMallocAllocator()' defined but not used [-Wunused-function]\r\n bool useCudaMallocAllocator() {\r\n      ^~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From ProtoCompile tensorflow/core/example/example.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From Compiling tensorflow/python/eager/python_eager_op_gen.cc [for host]:\r\ntensorflow/python/eager/python_eager_op_gen.cc: In function 'std::__cxx11::string tensorflow::{anonymous}::VectorToTuple(const std::vector<std::__cxx11::basic_string<char> >&)':\r\ntensorflow/python/eager/python_eager_op_gen.cc:65:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < l.size(); ++i) {\r\n                   ~~^~~~~~~~~~\r\ntensorflow/python/eager/python_eager_op_gen.cc: In function 'void tensorflow::{anonymous}::Unflatten(const string&, const std::vector<std::__cxx11::basic_string<char> >&, const string&, std::__cxx11::string*)':\r\ntensorflow/python/eager/python_eager_op_gen.cc:77:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < output_sizes.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/eager/python_eager_op_gen.cc:81:17: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (i + 1 < output_sizes.size()) {\r\n           ~~~~~~^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/eager/python_eager_op_gen.cc: In member function 'virtual std::__cxx11::string tensorflow::{anonymous}::GenEagerPythonOp::Code()':\r\ntensorflow/python/eager/python_eager_op_gen.cc:302:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = op_def_.input_arg_size(); i < params_no_default_.size(); ++i) {\r\n                                          ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/eager/python_eager_op_gen.cc:330:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < attrs_.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~\r\ntensorflow/python/eager/python_eager_op_gen.cc: In member function 'bool tensorflow::{anonymous}::GenEagerPythonOp::GetEagerFunctionSetup(const string&, std::__cxx11::string*)':\r\ntensorflow/python/eager/python_eager_op_gen.cc:480:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < attrs_.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/python/framework/python_op_gen.cc [for host]:\r\ntensorflow/python/framework/python_op_gen.cc: In member function 'virtual std::__cxx11::string tensorflow::python_op_gen_internal::GenPythonOp::Code()':\r\ntensorflow/python/framework/python_op_gen.cc:548:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = op_def_.input_arg_size(); i < params_no_default.size(); ++i) {\r\n                                          ~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/framework/python_op_gen.cc:551:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < params_with_default.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nERROR: /tmp/tensorflow/tensorflow/cc/BUILD:510:1: Executing genrule //tensorflow/cc:remote_fused_graph_ops_genrule failed (Exit 127)\r\nbazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc: symbol lookup error: bazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringB5cxx11E\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 391.219s, Critical Path: 57.13s\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I meet the same error. Have you solved this error?"]}, {"number": 18252, "title": "Empty tensorflow r1.7 api after virtualenv install of tensorflow r1.7 ", "body": "Ubuntu 16.10 using below commands for tensorflow r1.7\r\n########\r\nvirtualenv --system-site-packages -p python3 tensorflow\r\npip3 install tensorflow\r\n########\r\npip3 install --upgrade tensorflow\r\nCollecting tensorflow\r\n  Using cached tensorflow-1.7.0-cp36-cp36m-manylinux1_x86_64.whl\r\nCollecting tensorboard<1.8.0,>=1.7.0 (from tensorflow)\r\n  Using cached tensorboard-1.7.0-py3-none-any.whl\r\nRequirement not upgraded as not directly required: six>=1.10.0 in /usr/local/lib/python3.6/site-packages (from tensorflow) (1.11.0)\r\nRequirement not upgraded as not directly required: wheel>=0.26 in ./tensorflow/lib/python3.6/site-packages (from tensorflow) (0.31.0)\r\nCollecting gast>=0.2.0 (from tensorflow)\r\nCollecting grpcio>=1.8.6 (from tensorflow)\r\n  Using cached grpcio-1.10.0-cp36-cp36m-manylinux1_x86_64.whl\r\nCollecting astor>=0.6.0 (from tensorflow)\r\n  Using cached astor-0.6.2-py2.py3-none-any.whl\r\nRequirement not upgraded as not directly required: absl-py>=0.1.6 in /usr/local/lib/python3.6/site-packages (from tensorflow) (0.1.10)\r\nRequirement not upgraded as not directly required: numpy>=1.13.3 in /usr/local/lib/python3.6/site-packages (from tensorflow) (1.14.0)\r\nRequirement not upgraded as not directly required: protobuf>=3.4.0 in /usr/local/lib/python3.6/site-packages (from tensorflow) (3.5.1)\r\nCollecting termcolor>=1.1.0 (from tensorflow)\r\nRequirement not upgraded as not directly required: html5lib==0.9999999 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow) (0.9999999)\r\nRequirement not upgraded as not directly required: werkzeug>=0.11.10 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow) (0.14.1)\r\nRequirement not upgraded as not directly required: bleach==1.5.0 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow) (1.5.0)\r\nRequirement not upgraded as not directly required: markdown>=2.6.8 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow) (2.6.11)\r\nRequirement not upgraded as not directly required: setuptools in ./tensorflow/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow) (39.0.1)\r\ntensorboard 1.7.0 requires bleach==1.5.0, which is not installed.\r\ntensorboard 1.7.0 requires html5lib==0.9999999, which is not installed.\r\ntensorboard 1.7.0 requires markdown>=2.6.8, which is not installed.\r\ntensorboard 1.7.0 requires numpy>=1.12.0, which is not installed.\r\ntensorboard 1.7.0 requires protobuf>=3.4.0, which is not installed.\r\ntensorboard 1.7.0 requires six>=1.10.0, which is not installed.\r\ntensorboard 1.7.0 requires werkzeug>=0.11.10, which is not installed.\r\ngrpcio 1.10.0 requires protobuf>=3.5.0.post1, which is not installed.\r\ngrpcio 1.10.0 requires six>=1.5.2, which is not installed.\r\ntensorflow 1.7.0 requires absl-py>=0.1.6, which is not installed.\r\ntensorflow 1.7.0 requires numpy>=1.13.3, which is not installed.\r\ntensorflow 1.7.0 requires protobuf>=3.4.0, which is not installed.\r\ntensorflow 1.7.0 requires six>=1.10.0, which is not installed.\r\nInstalling collected packages: tensorboard, gast, grpcio, astor, termcolor, tensorflow\r\nSuccessfully installed astor-0.6.2 gast-0.2.0 grpcio-1.10.0 tensorboard-1.7.0 tensorflow-1.7.0 termcolor-1.1.0\r\n\r\n##########\r\nWhen i tried to verify tensorflow r1.7 installed properly, i got an empty api\r\nPython 3.6.4 (default, Jan 28 2018, 16:44:18) \r\n[GCC 6.2.0 20161005] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'constant'\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I encounter the same question\r\n```\r\ntensorflow 1.7.0 requires six>=1.10.0, which is not installed.\r\ntensorboard 1.7.0 requires six>=1.10.0, which is not installed.\r\nprotobuf 3.5.2.post1 requires six>=1.9, which is not installed.\r\nhtml5lib 0.9999999 requires six, which is not installed.\r\ngrpcio 1.10.1 requires six>=1.5.2, which is not installed.\r\nbleach 1.5.0 requires six, which is not installed.\r\nabsl-py 0.1.13 requires six, which is not installed.\r\nkeras 2.1.5 requires pyyaml, which is not installed.\r\nkeras 2.1.5 requires six>=1.9.0, which is not installed.\r\n```", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18251, "title": "Check input dimension for contrib.layers.conv2d/conv3d", "body": "This fix tries to fix the issue raised in #14583 where the input dimension was not checked for contrib.layers.conv2d/conv3d and contrib.slim.conv2d/conv3d.\r\n\r\nThe issue was that conv2d/conv3d were just aliases of convolution. This fix wrap the conv2d/conv3d with the input dimension check so that incorrect usage will return ValueError.\r\n\r\nThis fix fixes #14583.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@sguada Thanks for the review. The PR has been updated. Please take a look."]}]