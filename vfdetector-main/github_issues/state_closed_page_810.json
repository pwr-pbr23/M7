[{"number": 29237, "title": "[TF 2.0 API Docs] tf.complex", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/dtypes/complex\r\n\r\n## Description of issue (what needs changing):\r\n### Raises listed and defined\r\nNo\r\n\r\n### Submit a pull request?\r\n#29237\r\nI will", "comments": ["Closing this issue since the associated PR has been merged. Feel free to reopen if the problem still persists. Thanks!"]}, {"number": 29236, "title": "Node.js (JavaScript) TensorFlow Lite Wrapper API", "body": "Is there any Node.js (JavaScript) API wrapper for TF Lite, for instance, if we can install:\r\nnpm install @tensorflow/tfjs-lite\r\n", "comments": ["Good Idea!", "Just curious why you would want this over the Node.js bindings to the TensorFlow C library or to TensorFlow.js backends?", "I am working on an embedded  aarch64_armv8 and I have compiled Tensorflow Lite by using this  [link](https://www.tensorflow.org/lite/guide/build_arm64), at the end, I have two these files:\r\nbenchmark-lib.a  libtensorflow-lite.a\r\nNow, I want to bind these output files to node.js and run the following code for an instance:\r\n\r\n\r\n```\r\n// posenet\r\nconst tf = require('@tensorflow/tfjs-lite');\r\nconst posenet = require('@tensorflow-models/posenet');\r\nconst {\r\n    createCanvas, Image\r\n} = require('canvas')\r\nconst imageScaleFactor = 0.5;\r\nconst outputStride = 16;\r\nconst flipHorizontal = false;\r\nconst videoWidth =426; \r\nconst videoHeight =240; \r\nconst img = new Image();\r\nconst canvas = createCanvas(videoWidth, videoHeight);\r\nconst ctx = canvas.getContext('2d');\r\n\r\nconst tryModel = async(imgsrc) => {\r\n    const net = await posenet.load(0.75);\r\n    img.src = imgsrc;\r\n    ctx.drawImage(img, 0, 0);\r\n    const input = tf.browser.fromPixels(canvas);\r\n    const pose = await net.estimateSinglePose(input, imageScaleFactor, flipHorizontal, outputStride);\r\n    return JSON.stringify(pose);\r\n}\r\nlet tmodel = tryModel('./01_standing.png');\r\ntmodel.then(function(result) {\r\n   console.log('result:' + result);\r\n});\r\n\r\n```\r\n", "@hossein-b I am in your same situation right now. Did you figure out what to do next ? ", "@hossein-b Could you please let us know if you still need help on this ? if it is resolved then please feel free to move this issue to close status ? Thanks!", "> @hossein-b Could you please let us know if you still need help on this ? if it is resolved then please feel free to move this issue to close status ? Thanks!\r\n\r\nThanks for your comment. I changed my mind to use Python Tensorflow, and it solved my problem. ", "@hossein-b Could you please close this ticket if it is resolved for you ?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29236\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29236\">No</a>\n"]}, {"number": 29235, "title": "Issue with flags in TF2.0", "body": "I am using flags in my code in tensorflow and I have not been getting errors not until I upgraded from TF 1X to TF2.0 and these are the errors I am getting. I need an assistance on resolving this issue.\r\ntf.compat.v1.flags.FLAGS.delattr()\r\ndef del_all_flags(FLAGS):\r\nflags_dict = FLAGS._flags()\r\nkeys_list = [keys for keys in flags_dict]\r\nfor keys in keys_list:\r\nFLAGS.delattr(keys)\r\n\r\ndel_all_flags(tf.compat.v1.flags.Flag)\r\n\r\nflags = tf.app.flags\r\nFLAGS = tf.app.flags.FLAGS\r\nflags.DEFINE_float(\"learning_rate\", default = 0.0001, help = \"Initial learning rate.\")\r\nflags.DEFINE_integer(\"epochs\", default = 700, help = \"Number of epochs to train for\")\r\nflags.DEFINE_integer(\"batch_size\", default =128, help = \"Batch size.\")\r\nflags.DEFINE_integer(\"eval_freq\", default = 400, help =\" Frequency at which to validate the model.\")\r\nflags.DEFINE_float(\"kernel_posterior_scale_mean\", default = -0.9, help = \"Initial kernel posterior mean of the scale (log var) for q(w)\")\r\nflags.DEFINE_float(\"kernel_posterior_scale_constraint\", default = 0.2, help = \"Posterior kernel constraint for the scale (log var) for q(w)\")\r\nflags.DEFINE_float(\"kl_annealing\", default = 50, help = \"Epochs to anneal the KL term (anneals from 0 to 1)\")\r\nflags.DEFINE_integer(\"num_hidden_layers\", default = 4, help = \"Number of hidden layers\")\r\nflags.DEFINE_integer(\"num_monte_carlo\",\r\n\r\n                 default=50,\r\n\r\n                 help=\"Network draws to compute predictive probabilities.\")\r\ntf.compat.v1.app.flags.DEFINE_string('f', '', 'kernel')\r\n\r\nI end up getting these error while doing some manipulation: DuplicateFlagError: The flag 'batch_size' is defined twice. First from D:/Python/workspace/FCN_dataset/FCN.tensorflow-master/FCN.py, Second from D:/Python/workspace/FCN_dataset/FCN.tensorflow-master/FCN.py. Description from first occurrence: batch size for training and\r\nTypeError: delattr() missing 1 required positional argument: 'flag_name'\r\n_\r\nand DuplicateFlagError: The flag 'master' is defined twice. First from object_detection/train.py, Second from object_detection/train.py.  Description from first occurrence: Name of the TensorFlow master to use.", "comments": ["I need help on this. It has been stagnating my work "]}, {"number": 29234, "title": " Fix links in README of TF-TRT", "body": "", "comments": []}, {"number": 29233, "title": "Fix ImportError: No module named builtins", "body": "", "comments": ["@bananabowl "]}, {"number": 29232, "title": "Fix tensorflow_estimator version in pip_package/setup.py", "body": "", "comments": []}, {"number": 29231, "title": "1.14-rc1 cherry-pick request: NCCL broadcast bug", "body": "Without this cherry-pick, NCCL broadcast does not work.\r\n\r\nRequested by NVIDIA.  FYI @nluehr ", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29231) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29231) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 29230, "title": "Wrong color documented in explanation TensorFlow Lite Android", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@darrencapner Please provide the correct link where the problem is persist.Thanks \r\n\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29229, "title": "ExecutorState crash on specific iOS devices", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iPhone XR, iPhone XS using iOS 12.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: See above. \r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.22.0 \r\n- GCC/Compiler version (if compiling from source): GCC\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nLast year we built an LSTM RNN using Tensorflow 1.12 and Python 3.6.8. After struggling for a couple of weeks we managed to get the tensorflow mobile static libraries compiled and running on iPhone and were able to run our model (a real-time Human Activity Recognition application, FWIW). \r\n\r\nOn Apple's latest hardware (iPhone XR and XS) we are seeing seemingly random crashes after running inference for 20+ seconds. We receive a EXC_BAD_ACCESS crash as follows:\r\n\r\ntensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)\r\n\r\nThis custom model was built with BasicLSTMCell which makes it, from what we can discern, unsuitable for TFLite at this point (since LSTM Ops are not yet supported outside of the experimental branch). For reference (python code for the graph): \r\n\r\n        lstm_cells = [tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)\r\n                      for layer in range(n_layers)]\r\n        multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\r\n        outputs, states = tf.nn.dynamic_rnn(multi_cell, x, dtype=tf.float32)\r\n        top_layer_h_state = states[-1][1]\r\n  \r\n**Other info / logs**\r\n\r\n0 | PlayFitt | tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 56780\r\n-- | -- | --\r\n1 | PlayFitt | tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 58744\r\n2 | PlayFitt | std::__1::__function::__func<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&>, std::__1::allocator<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&> >, void ()>::operator()() + 89040\r\n3 | PlayFitt | Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 276848\r\n4 | PlayFitt | std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 275152\r\n5 | PlayFitt | void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 56\r\n6 | libsystem_pthread.dylib | _pthread_body + 132\r\n8 | (Missing) | (Missing)\r\n", "comments": ["Hi Ben,\r\n\r\nFirstly, I would recommend you to try out tflite lstm (yes, it's under experimental, but I think switching to tflite can give you benefits on performance & binary size)\r\n\r\nBack to the question, please note tf.nn.dynamic_rnn returns final_state, see [here](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn#returns)\r\n\r\nso you should do\r\n\r\n```\r\noutputs, final_state = tf.nn.dynamic_rnn(multi_cell, x, dtype=tf.float32)\r\ntop_layer_h_state = final_state[1]\r\n```\r\n\r\ncheers,\r\n"]}, {"number": 29228, "title": "[ROCm] Add ROCm support for most cwise ops", "body": "This minor mod adds ROCm support for most of the cwise ops.\r\n\r\nBackground info\r\nThese ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF.\r\n\r\nWe have published docker images at: https://hub.docker.com/r/rocm/tensorflow/tags\r\nAnd also PyPI packages: https://pypi.org/project/tensorflow-rocm/\r\n\r\nFor a sample ROCm test run you can refer to:\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\n```\r\n//tensorflow/python/kernel_tests:cwise_ops_binary_test                   PASSED in 270.4s\r\n//tensorflow/python/kernel_tests:cwise_ops_binary_test_gpu               PASSED in 274.3s\r\n//tensorflow/python/kernel_tests:cwise_ops_test                          PASSED in 24.2s\r\n//tensorflow/python/kernel_tests:cwise_ops_test_gpu                      PASSED in 23.7s\r\n//tensorflow/python/kernel_tests:cwise_ops_unary_test                    PASSED in 12.0s\r\n//tensorflow/python/kernel_tests:cwise_ops_unary_test_gpu                PASSED in 12.4s\r\n````", "comments": []}, {"number": 29227, "title": "Refactor {ParallelInterleave, ParallelMap, Repeat} DatasetOps", "body": "This PR refactors `ParallelInterleaveDatasetOp`, `ParallelMapDatasetOp`, and `RepeatDatasetOp`.\r\n\r\ncc: @jsimsa", "comments": ["@jsimsa Thanks for your review! The logic in `parallel_map_iterator.h` is merged into `parallel_map_dataset_op.h`. Please have a look at this commit (https://github.com/tensorflow/tensorflow/pull/29227/commits/c58dc03bc40aa3eaae45b19ba019b03b4a2095e6).", "@jsimsa This PR is rebased to resolve the conflicts in `name_utils.cc`. Could you please have a look at the [changes](https://github.com/tensorflow/tensorflow/pull/29227/commits/e344de3f973ebf04cbd06379c5b60787ae650420#diff-c8617724c8e312e213eaad9f4c2aae58R40)?", "@jsimsa Thanks for your review! The comments are addressed in this commit (https://github.com/tensorflow/tensorflow/pull/29227/commits/64b88732f819927eb1519ee68c60905579036ae4). Could you please have another look?", "@jsimsa The internal checks failed. It seems to be caused by the redundant import [here](https://github.com/tensorflow/tensorflow/pull/29227/commits/9073d159e7f7efc920d5b94e128e447a4bc596d8#diff-ba5deacaba3737b4b1cd20f5fcb9ae92L15). I amend the last commit by removing the redundant import, but it makes the link of the internal check failures disappear. Could you help re-trigger the tests?", "All internal checks pass but the CL. I will help to get this submitted.", "Thanks, @jsimsa !", "@jsimsa The code conflicts with this merged PR(https://github.com/tensorflow/tensorflow/pull/29669) in name_utils.cc are resolved, and make `name_utils::OpName()` handle a special case (e.g. `OpName(ParallelInterleaveDatasetOp::kDatasetType)` -> `ParallelInterleaveDatasetV2` (instead of `ParallelInterleaveV2Dataset`)). Could you please have a look at the change (https://github.com/tensorflow/tensorflow/pull/29227/commits/1d939596c92a55b8db995052ba4f600373e3a31f)?", "@jsimsa Yeah, thanks for the suggestion! I rewrite this commit (https://github.com/tensorflow/tensorflow/pull/29227/commits/170a416ed01f5f595de77c70abb05a0d8bdf388d) using the regular expression. Please have another look.", "Can you please resolve merge conflicts? Thanks.", "@jsimsa The conflicts are resolved. Could you please have a look at the merged code [here](https://github.com/tensorflow/tensorflow/pull/29227/files#diff-181c07ecd651ad4a9d09beebf0504eb3R930) and [here](https://github.com/tensorflow/tensorflow/pull/29227/files#diff-df634c8243713c0afd2e05c1689412e2R195)?", "@jsimsa The internal checks failed. Could you help paste the logs here?", "it seems that the internal copy of the PR has not picked up your changes yet ... @rthadur could you please make sure this PR is synced with the internal copy and tests are executed? thanks", "@jsimsa The internal checks failed again. Could you please paste the logs here? Thanks!", "I will take over the internal CL and get it merged."]}, {"number": 29226, "title": "Refactor {ParallelInterleave, ParallelMap, Repeat} DatasetOps", "body": "This PR refactors `ParallelInterleaveDatasetOp`, `ParallelMapDatasetOp`, and `RepeatDatasetOp`.\r\n\r\ncc: @jsimsa ", "comments": ["I will close this PR due to the conflicts with this commit (https://github.com/tensorflow/tensorflow/commit/ffdec8e49f14541afaf90bb4a40b89eafd6602ec) submitted a few minutes ago. Will resubmit it after resolving the conflicts."]}, {"number": 29225, "title": "[ROCm] Adding ROCm support for partitioned_function_ops", "body": "This PR adds ROCm support for partitioned_function_ops\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n-----\r\nTest performed:\r\n//tensorflow/core/kernels:partitioned_function_ops compiles\r\n\r\n@tatianashp @whchung", "comments": ["FYI Linux GPU is failing on //tensorflow/compiler/tests:binary_ops_test_gpu. The failure message is \r\n\r\n> 2019-05-31 18:44:31.322831: E tensorflow/stream_executor/cuda/cuda_driver.cc:625] failed to load PTX text as a module: CUDA_ERROR_INVALID_PTX: a PTX JIT compilation failed\r\n2019-05-31 18:44:31.322862: E tensorflow/stream_executor/cuda/cuda_driver.cc:630] error log buffer (142 bytes): ptxas application ptx input, line 12; fatal   : Parsing error near '.visible': syntax error\r\nptxas fatal   : Ptx assembly aborted due to error\r\n2019-05-31 18:44:31.322951: F tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:889] Check failed: module != nullptr\r\n\r\nIt is infrastructure related and not specifically related to the code change in this PR."]}, {"number": 29224, "title": "Fast 3DY reduction when the middle dim Y is large but X and Z are small", "body": "The existing TF uses a simple CUDA kernel to solve the 3DY reduction. It performs bad when the middle dim Y is large but X and Z are small, because the total thread number is determined by X * Z.\r\n\r\nThis PR solves this problem by using a new 3DY reduction kernel of two stages: (1) inter-block reduction: starting many blocks to cooperatively reduce the whole dataset by half. This iterates until one block can cover a single or multiple planes; (2) intra-block reduction: each block works independently to reduce the assigned plane(s). For the (2), the shared memory are used for better performance. (Note, for the large X and Z, we will fall back to the original kernel.)\r\n\r\nFor our user case of N = 16 HW = 1024*1024 C =4 and reduction on HW: before 42.67ms, after 0.97ms, speedup: 44x.\r\n\r\nFYI. @nluehr \r\n", "comments": ["@chsigg could you help to take a look at this? Thanks.", "Solved a conflict issue.", "Lambda to find the original code owner.", "@chsigg Thanks for the comments. And I made a set of changes accordingly. Please check. Thx.", "@chsigg thanks for reviewing!"]}, {"number": 29223, "title": "Fix code of conduct link in covenant badge", "body": "cc @wicke @bhack ", "comments": []}, {"number": 29222, "title": "Allow the config APIs dealing with memory growth to work with", "body": "enable_v2_behavior\r\n\r\nThis entails allowing the Context() object to be created early (which already\r\nseemed like the intention given the physical device APIs).\r\nIf enable_v2_behavior is called after, then we update the thread local data to\r\nsay we are in eager mode.\r\n\r\nPiperOrigin-RevId: 250727360", "comments": []}, {"number": 29221, "title": "[ROCm] Adding ROCm support to diag_op", "body": "This PR adds ROCm support for diag_op\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n-----\r\nTest performed: \r\n//tensorflow/python/kernel_tests:diag_op_test\r\n//tensorflow/core/kernels:diag_op_test\r\n\r\n@tatianashp @whchung", "comments": []}, {"number": 29220, "title": "Build failure with upcoming Python toolchain change in Bazel 0.27", "body": "Bazel 0.27 will flip [`--incompatible_use_python_toolchains`](https://github.com/bazelbuild/bazel/issues/7899). A [downstream presubmit](https://buildkite.com/bazel/bazel-at-head-plus-downstream/builds/1008#0016c105-4c66-4506-83fd-72ee26749782) shows Tensorflow failing for this change.\r\n\r\nOne of the side-effects of the flag is that it causes Bazel to actually run Python targets with the interpreter selected by Bazel at analysis time, rather than whatever the system `python` command happens to be. It's likely you have targets that should be Python 2, and were previously running as Python 2, that are now running as Python 3 due to this bug fix. (Note that as of Bazel 0.25, `python_version` defaults to PY3 if not given.)\r\n\r\nFor each Python 2 `py_binary` / `py_test` target (or macros creating such targets), add the attribute `python_version = \"PY2\"`. If you have any PY2 targets built in the host configuration, you must add `--host_force_python=PY2` to your bazelrc.\r\n\r\nLooking at the failures, I see for instance [`tf_python_api_gen_v1`](https://github.com/tensorflow/tensorflow/blob/50883f7ca9b98e30d303dad5d20b76e05a947f36/tensorflow/BUILD#L774), generated by `gen_api_init_files`, which [creates](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/api/generator/api_gen.bzl#L66-L76) a `py_binary` with no `python_version` attribute.\r\n\r\nSee the flag's tracking issue (above) for more details.", "comments": ["Bazel at head now emits [helpful warning messages](https://buildkite.com/bazel/bazel-at-head-plus-downstream/builds/1017#8f969893-683c-4c1a-b71e-fddc82257ed8/265-628) when a host-configured tool may be failing due to having the wrong Python version. But all such host target failures can be fixed at once by setting `--host_force_python=PY2`.", "> Looking at the failures, I see for instance [`tf_python_api_gen_v1`](https://github.com/tensorflow/tensorflow/blob/50883f7ca9b98e30d303dad5d20b76e05a947f36/tensorflow/BUILD#L774), generated by `gen_api_init_files`, which [creates](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/api/generator/api_gen.bzl#L66-L76) a `py_binary` with no `python_version` attribute.\r\n\r\nFTR, while `python_version` ought to be set for clarity's sake, it won't help since this is a host-configured tool. Only `--host_force_python` can affect its version. You should still set the attribute for documentation purposes and because eventually it'll have an effect once the host config is eliminated from Bazel.", "It's a bit painful to reproduce the build locally. At the same time, TF is disabled in CI right now, waiting on protocolbuffers/protobuf#6207. So I don't think I'm in a position to confirm that setting this flag fixes the Python issue right now.", "Ping @gunan?", "Ah, I am sorry that I have missed this issue. will try looking into this today, or tomorrow.\r\nAlso pinging @annarev as she is knowledgeable about python_api_gen_* targets", "I tested `--host_force_python=PY2`, it does fix the failure. But I wonder what if we don't have python2 installed on the system. Will it cause a failure? @brandjon ", "Maybe a better way to fix this is to make all targets compatible with Python3?", "> But I wonder what if we don't have python2 installed on the system. Will it cause a failure?\r\n\r\nIn principle, if a target is analyzed as PY2 or PY3, it ought to be run under Python 2 or Python 3 respectively at execution time. If the target platform doesn't have that version of Python installed, that should be a build failure (if the absence of the interpreter is known by the toolchain) or execution failure (if the toolchain does autodetection at execution time).\r\n\r\nThe tricky use case is if you want to define a target that works under either version of Python. We don't directly support this but it should be possible with a macro and a dispatching target that chooses the right dependency based on a select() on platform constraints."]}, {"number": 29219, "title": "[ROCm] Adding macro GPU_DYNAMIC_SHARED_MEM_DECL", "body": "Add the macro so that gpu kernels can use dynamic shared memory. Dependent kernels includes:\r\n\r\n- depthwise_conv_op\r\n- bias_op\r\n- concat_lib\r\n- split_lib\r\n- bucketsize_op\r\n\r\n-----\r\n@tatianashp @whchung @chsigg", "comments": ["@jerryyin  this seems to be a duplicate of PR #29095 .  please close this one out", "> @jerryyin this seems to be a duplicate of PR #29095 . please close this one out\r\n\r\nHa, thanks for point out. I thought there should be one, but didn't try hard enough."]}, {"number": 29218, "title": "Added tf.math.normalize, tf.linalg.normalize and tf.nn.normalize", "body": "As discussed in #28741.\r\n\r\nNote: Now calling `tf.linalg.l2_normalize(x)` is identical to `tf.linalg.normalize(x, axis=None)`.\r\nI did not peform any changes to `l2_normalize` to not break anything.\r\n\r\nThe only thing that would break here is the name scoping as that would result in `l2_normalize>normalize>norm` instead of just `l2_normalize`.\r\n\r\nRegarding the test. This test is based on the the test provided for `tf.norm`. I can change `nn_test.py` to add individual tests for each case if required.", "comments": ["Hmm, test ran through on my side. Will check again tomorrow.", "Ran the test. Everything should work now.", "Sigh... i will refactor it to multiple tests. This will prevent a single test to take too long even though it runs through after a while.", "Ok, didn't know that.\r\nCan you provide me a quick hint as to how I do that?\r\nCurrently I would add the following to `tensorflow\\python\\kernel_tests\\BUILD`\r\n\r\n```\r\ncuda_py_test(\r\n    name = \"normalize_op_test\",\r\n    size = \"medium\",\r\n    srcs = [\"normalize_op_test.py\"],\r\n    additional_deps = [\r\n        \"//third_party/py/numpy\",\r\n        \"//tensorflow/python:client_testlib\",\r\n        \"//tensorflow/python:framework_for_generated_wrappers\",\r\n        \"//tensorflow/python/ops:nn_impl\",\r\n    ],\r\n    shard_count = 20,\r\n    xla_enable_strict_auto_jit = True,\r\n)\r\n```\r\n\r\nI am not 100% sure I get the syntax and pathing right.", "That looks good to me, give it a try.\n\nOn Mon, Jun 10, 2019 at 11:08 AM Julian Niedermeier <\nnotifications@github.com> wrote:\n\n> Ok, didn't know that.\n> Can you provide me a quick hint as to how I do that?\n> Currently I would add the following to\n> tensorflow\\python\\kernel_tests\\BUILD\n>\n> `cuda_py_test(\n>     name = \"normalize_op_test\",\n>     size = \"medium\",\n>     srcs = [\"normalize_op_test.py\"],\n>     additional_deps = [\n>         \"//third_party/py/numpy\",\n>         \"//tensorflow/python:client_testlib\",\n>         \"//tensorflow/python:framework_for_generated_wrappers\",\n>         \"//tensorflow/python/ops:nn_impl\",\n>     ],\n>     shard_count = 20,\n>     xla_enable_strict_auto_jit = True,\n> )\n>\n> I am not 100% sure I get the syntax and pathing right.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/29218?email_source=notifications&email_token=AAABHRNKU3HEVYZX7RJTR2TPZ2KCLA5CNFSM4HR3PL4KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXKUWDY#issuecomment-500517647>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLIOTGZB6ZFVIW5UQDPZ2KCLANCNFSM4HR3PL4A>\n> .\n>\n\n\n-- \n - Alex\n", "For the Linux GPU test I do not understand what `Broken` refers to :(\r\n\r\nEdit:\r\nno such package 'tensorflow/python/ops': BUILD file not found on package path and referenced by '//tensorflow/python/kernel_tests:normalize_op_test_gpu'\r\n\r\nI must admit I am a little overwhelmed by all the output.\r\nThe test definitely runs through, it is just a matter of the right plumbing with the TF ecosystem.\r\nI could use a little help in that regard.", "@alextp Now it is throwing missing ops:nn ... :(\r\n\r\n`Broken by missing target //tensorflow/python/ops:nn `", "Oh sorry it's not ops:nn, it's //tensorflow/python:nn\n\nOn Tue, Jun 11, 2019 at 1:05 AM Julian Niedermeier <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> Now it is throwing missing ops:nn ...\n> :(\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/29218?email_source=notifications&email_token=AAABHRM4VJSIMKW3VNFXLWDPZ5MGDA5CNFSM4HR3PL4KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXMIZFY#issuecomment-500731031>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRKMC3HDV6WN6NYCW3LPZ5MGDANCNFSM4HR3PL4A>\n> .\n>\n\n\n-- \n - Alex\n", "Next try :)\r\n\r\nCan you elaborate why it has to be `//tensorflow/python:nn`?", "Because there's no BUILD file in tensorflow/python/ops, and there's a BUILD\nfile in tensorflow/python with a rule named \"nn\" which includes \"nn_impl.py\"\n\nOn Tue, Jun 11, 2019 at 2:15 PM Julian Niedermeier <notifications@github.com>\nwrote:\n\n> Next try :)\n>\n> Can you elaborate why it has to be //tensorflow/python:nn?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/29218?email_source=notifications&email_token=AAABHRPN5T23CRWN5IK3JJLP2AIWFA5CNFSM4HR3PL4KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXOQHUQ#issuecomment-501023698>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHROZSMLIIYA4IYXNZYDP2AIWFANCNFSM4HR3PL4A>\n> .\n>\n\n\n-- \n - Alex\n", "Now the API tests are failing, so follow the instructions in the log (copy\npasted here)\n\n```\n   $ bazel build tensorflow/tools/api/tests:api_compatibility_test\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\n          --update_goldens True\n```\n\nOn Tue, Jun 11, 2019 at 2:16 PM Alexandre Passos <apassos@google.com> wrote:\n\n> Because there's no BUILD file in tensorflow/python/ops, and there's a\n> BUILD file in tensorflow/python with a rule named \"nn\" which includes\n> \"nn_impl.py\"\n>\n> On Tue, Jun 11, 2019 at 2:15 PM Julian Niedermeier <\n> notifications@github.com> wrote:\n>\n>> Next try :)\n>>\n>> Can you elaborate why it has to be //tensorflow/python:nn?\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/29218?email_source=notifications&email_token=AAABHRPN5T23CRWN5IK3JJLP2AIWFA5CNFSM4HR3PL4KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXOQHUQ#issuecomment-501023698>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AAABHROZSMLIIYA4IYXNZYDP2AIWFANCNFSM4HR3PL4A>\n>> .\n>>\n>\n>\n> --\n>  - Alex\n>\n\n\n-- \n - Alex\n", "Thank you for the explanation. Next time I know where to look. Will do it \"tomorrow\".", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29218) for more info**.\n\n<!-- need_author_consent -->", "Ups... That was me making a big mistake while trying to get bazel running ...", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29218) for more info**.\n\n<!-- ok -->", "@alextp I will close this messed up pull-request an open a new one..."]}, {"number": 29216, "title": "CUDA Compability 3.0 not working", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.5.2\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 5.5.0\r\n- CUDA/cuDNN version: 10.0/7\r\n- GPU model and memory: GTX770 4GB\r\n\r\n**Describe the problem**\r\nI compiled tensorflow from sources with compute compability 3.0 but after installation it still looks for 3.5\r\n\r\n> 2019-05-31 15:06:10.209035: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299735000 Hz\r\n2019-05-31 15:06:10.209418: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4042a70 executing computations on platform Host. Devices:\r\n2019-05-31 15:06:10.209526: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-05-31 15:06:10.252169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-05-31 15:06:10.252663: I tensorflow/compiler/xla/service/platform_util.cc:194] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0\r\n2019-05-31 15:06:10.252779: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\n\r\n**Any other info / logs**\r\nI followed official guides\r\nhttps://www.tensorflow.org/install/gpu\r\nhttps://www.tensorflow.org/install/source\r\nfor installation of NVIDIA packages and build process.\r\n\r\nIn configure step I chose CUDA only, set the paths and select version 3.0 (btw it was default). Everything else was default.\r\nI built tensorflow from v1.13.1 tag.\r\n", "comments": ["You need to build with XLA off, if you want to use CUDA 3.0. This message was added in 1.14, but it not backported into 1.13.1\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.14.0-rc0/configure.py#L1021-L1023\r\n```\r\n        if ver < 3.5:\r\n          print('WARNING: XLA does not support CUDA compute capabilities '\r\n          'lower than 3.5. Disable XLA when running on older GPUs.')\r\n```", "That was it. Thanks."]}, {"number": 29215, "title": "[TF 2.0] Inconvenient graph visualization", "body": "**System information**\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6.5\r\n\r\nWhen visualizing the graph with tensorboard, there are many nodes, which maybe generated by auotgraph, to make it quite inconvenient to debug with the graph.\r\n\r\nTo reproduce:\r\n<pre>\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(100)\r\n\r\n    def call(self, inputs):\r\n        outputs = self.dense1(inputs)\r\n        return outputs\r\n\r\n\r\nmodel = Model()\r\noptimizer = tf.keras.optimizers.Adam()\r\nwriter = tf.summary.create_file_writer(\"./logdir\")\r\n\r\n\r\n@tf.function\r\ndef train(data):\r\n    with tf.name_scope(\"xxx\"):\r\n        with tf.GradientTape() as tape:\r\n            y = model(data)\r\n            loss = tf.reduce_mean(tf.square(y))\r\n        grads = tape.gradient(loss, model.trainable_weights)\r\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n\r\n\r\nx = np.random.rand(10, 100).astype(np.float32)\r\n# y = model(x)\r\n# train.python_function(x)\r\ntf.summary.trace_on()\r\ntrain(x)\r\nwith writer.as_default():\r\n    tf.summary.trace_export(\"graph\", step=0)\r\n    tf.summary.trace_off()\r\n    writer.flush()\r\n</pre>\r\n\r\nThe graph visualization contains many `ReadVariableOp` and `AssignVariableOp` outside, as follows:\r\n![png](https://user-images.githubusercontent.com/22030149/59081332-196c5680-8920-11e9-9b76-bfa482ccf2b2.png)\r\n\r\nIf we unocmment `y = model(x)`, the number of outside ops decrease, as follows:\r\n![png (1)](https://user-images.githubusercontent.com/22030149/59081425-9bf51600-8920-11e9-8dfb-4c3b813f9f5c.png)\r\n\r\nIf we uncomment `train.python_function(x)`, all `ReadVariableOp` and `AssignVariableOp` decrease:\r\n![png (2)](https://user-images.githubusercontent.com/22030149/59081460-c6df6a00-8920-11e9-8448-6e368ef5d573.png)\r\n\r\nI think this is quite related to #29442 . The keras layers add variables during the first call. If the first call is inside `tf.function`, these variables are created in a graph and some unexpected behaviors happen as reported in #29442 .\r\n\r\nUncommenting `y = model(x)` makes sure that the model variables are created in eager mode, and uncommenting `train.python_function(x)` makes sure that optimizer-related variables are also created in eager mode.", "comments": ["@llan-ml Code snippet looks incomplete could you provide the complete code to reproduce the issue. Thanks!", "@gadagashwini I updated the code snippet above.", "@llan-ml Autograph doesn't affect name scopes in the default configuration, so it should not affect the graph display. Can you re-run the snippet setting `@tf.function(autograph=False)` to confirm?\r\n\r\n@fchollet Any thoughts about which ops are created outside the `name_scope` inside `train`?", "@mdanatg Aftering setting `autograph=False`, there is no difference.\r\n\r\nAs shown in the above picture, the `ReadVariableOp` and `AssignVariableOp` are outside.", "@mdanatg I updated the issue and it shows more findings now.", "Thanks for double-checking. I'm not sure what creates the ops outside the name scope - I suspect it's either the optimizer or the model tracer.", "how do you add a keras model to tensorflow graph when you're using GradientTape? No reference available on this ... docs only describe the callbacks with model.fit, seems impossible to add a keras model to tensorboard if using custom training loop", "@bionicles The decorator `tf.function` will automatically transform the code in a function into the graph-style code, and `tf.summary.trace_on` will trace graph execution.", "@llan-ml thank you", "It's not clear to me that this is a Keras issue as opposed to a more general issue with how Tensorboard visualizes variable assignment operators when variables are captured from tf.functions vs. created in a run of the tf.function.\r\n\r\nThat said, we've made a lot of changes to Keras's execution path recently, so if you try the new nightly you *might* get a less noisy visualization. Meanwhile, re-assigning the more-general question of how to visualize assignment operators to the tensorboard team for triaging.", "Reassigning to davidsoergel to prioritize for the Graph plugin.", "I ran @llan-ml code in recently released `TF2.0` and see simpler graph than the one with `TF2.0.alpha0`. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/129a7c6950a0c001da57319aa0adaad1/tf_29215.ipynb) is the gist with `TF2.0`. Thanks!", "@jvishnuvardhan Thanks for your effort!\r\n\r\nFor now, there are still some extra internal ops existing like\r\n![image](https://user-images.githubusercontent.com/22030149/66018682-9d230200-e512-11e9-9c3e-25f2415bde6f.png)\r\n\r\nIt would be better if either we can batch these extra ops (except for `data`) within a predefined or customized scope or they are removed from the main graph by default.\r\n", "> @bionicles The decorator `tf.function` will automatically transform the code in a function into the graph-style code, and `tf.summary.trace_on` will trace graph execution.\r\n\r\nWhen I do this and open tensorboard using tensorboard --logdir logdirlocation only a scalars tab opens up. Do you happen to know why the graphs tab does not appear? I have tf.summary.trace_on defined after I instantiated my models.", "I have the same problem with graph visualization.\r\nI used to rely on TB graph visualization in TF 1.X for sanity checks and it was extremely useful.\r\nNow, it's frustrating to normally keep track of the I/O flow of each block.\r\nThere are plenty of nodes with this naming pattern: **_XXXX_cast_readvariableop_resource_**\r\nPutting a custom model in Keras also doesn't work for visualizing a conceptual graph.", "For those who still suffer from this issue, I found that we can first convert the custom model into a functional model (i.e., initialize the model [in a graph manner](https://github.com/tensorflow/tensorflow/blob/25fba035f3e453d94490932096282c7b0624bbb3/tensorflow/python/keras/engine/network.py#L173)) and then visualize the graph with `tf.keras.callbacks.Tensorboard`, which usually results in a much better graph visualization than that generated as in this [link](https://www.tensorflow.org/tensorboard/graphs#graphs_of_tffunctions).", "I believe that this issue has been resolved with the new public summary operation [`tf.summary.graph`](https://www.tensorflow.org/api_docs/python/tf/summary/graph).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29215\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29215\">No</a>\n"]}, {"number": 29214, "title": "[TF 2.0 API Docs] Edited Documentation to tf.dtypes.cast", "body": "Fix #29210 by changing example to tf.dtypes.cast.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29214) for more info**.\n\n<!-- need_sender_cla -->", "@kooock can you please sign CLA.", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29214) for more info**.\n\n<!-- ok -->"]}, {"number": 29213, "title": "GPU OOM error when using keras and distributions strategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly-gpu==1.14.1.dev20190524\r\n- Python version: Python 3.7.3\r\n- CUDA/cuDNN version: CUDA 10, CUDNN 7.4.2.24\r\n- GPU model and memory: 4 x NVIDIA V100\r\n\r\n**Describe the current behavior**\r\nUsing `tf.distribute.MirroredStrategy()` together with Keras to train models as described in https://www.tensorflow.org/alpha/tutorials/distribute/keras results in GPU out of memory errors appearing after several epochs of training. We excluded our custom written code as the source of the memory leaks and made sure that the model actually fits into memory with enough headroom. It seams that either `tf.data` or `tf.keras.metrics` have a memory leak that starts showing up after several epochs of training and evaluation.\r\n\r\n**Describe the expected behavior**\r\nTensorflow doesn't throw OOM errors.\r\n\r\n**Code to reproduce the issue**\r\nUnfortunately I cannot give a concrete code example to reproduce this issue since memory leaks appear anytime in between 10min to 12h of training. Though I am happy to provide more information and would be eager to get suggestions on how to properly debug this problem.\r\n\r\n**Other info / logs**\r\n```python traceback\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\", line 644, in fit\r\nuse_multiprocessing=use_multiprocessing)\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 899, in fit\r\nvalidation_freq=validation_freq)\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 149, in fit_distributed\r\nsteps_name='steps_per_epoch')\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 409, in model_iteration\r\nsteps_name='validation_steps')\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 274, in model_iteration\r\nbatch_outs = f(actual_inputs)\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\", line 3351, in __call__\r\nrun_metadata=self.run_metadata)\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1458, in __call__\r\nrun_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\r\n(0) Resource exhausted: Failed to allocate memory for the batch of component 0\r\n\r\n[[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n[[RemoteCall]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n[[IteratorGetNext_7]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n(1) Resource exhausted: Failed to allocate memory for the batch of component 0\r\n[[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n[[RemoteCall]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n[[IteratorGetNext_7]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n[[metrics_4/categorical_accuracy/Identity_2/_3447]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n0 successful operations.\r\n3 derived errors ignored.\r\n```\r\n", "comments": ["Do you have anything in your code such as a callback that keep adding new ops to the graph?", "> Do you have anything in your code such as a callback that keep adding new ops to the graph?\r\n\r\nAs far as we can tell, no. We are using the TensorBoard and ModelCheckpoint callbacks to monitor training process and don't have any other callbacks.\r\n@yuefengz Is there a way to check if TensorFlow or our code adds new ops to the graph when training the keras model?\r\n\r\nWe are now able to also reproduce this error on a single GPU without distributions strategy. One thing to note is that we are using `tf.data.Dataset::prefetch(tf.data.experimental.AUTOTUNE)` as the last operation in the `tf.data` pipeline, though if I recall correctly `tf.data.Dataset::prefetch` shouldn't prefetch onto the GPU memory, correct?\r\n\r\nOnce we are able to reliably reproduce this issue I am happy to share a repo/gist with the code. Any help or pointer for debugging would be greatly appreciated in the meantime.", "To check whether there is any new op added you graph after training starts, you can add loggings to the create_op method: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L3221\r\n\r\n@jsimsa for dataset questions.", "@lgeiger tf.data without distribution strategy will not prefetch any data to GPU (unless you are using `prefetch_to_device` which has been deprecated in lieu of multi-device iterator used by the distribution strategy under the hoods)", "> To check whether there is any new op added you graph after training starts, you can add loggings to the create_op method\r\n\r\nThanks. I double checked, that we don't add any operations to the graph after training starts.", "Same problem. I also double checked my code that no new op after training starts. Suspect that there's some memory leak somewhere in tf.", "We where able to resolve this issue by not using `.prefetch(tf.data.experimental.AUTOTUNE)` as the last dataset transformation, which caused problems with TensorFlow running inside a Kubernetes cluster without the correct resource requests and limits and data streaming from GCS.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29213\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29213\">No</a>\n", "I encountered this issue today. For me it turned out that I accidentally batched my dataset multiple times, instead of once.", "Initially, I did `train_ds = train_ds.cache().prefetch(buffer_size=2)`, then i removed .prefetch, still errors, then i removed .cache(), and it worked.\r\n\r\nNote: cache.prefetch works fine on 1/5 the number of images. I had this error while scaling up the data.\r\n\r\nAlso there are other symptoms it will fail. I see my RAM go from 5gb to 15gb (using up all of it) as the training steps begin and always exactly at step 308/330 i get the error `Resource Exhausted: Failed to allocate memory for the batch of component 0 [[node IteratorGetNext`. \r\nAfter removing cache and prefetch, my RAM goes up to 9gb and maintains. \r\nAnyone has an explanation for how cache and prefetch affects RAM?\r\nAlso, how can we be sure AUTOTUNE will not prefetch too many batches and cause errors in the middle of training? \r\n\r\n\r\n@Trezorro Could you share how you batch your dataset multiple times accidentally and why would that cause this ResourceExhaustedError? \r\n\r\nIs it something like in a single method chain of data.cache().batch(32).prefetch(1).batch(32)? \r\nOr 4 assignments separated some distance apart in code? eg. data = data.cache(), data=data.batch(32), data = data.prefetch(1), data=data.batch(32)   \r\n", "I used assigments, so that I could put some steps within a conditional\nstatement. I used prefetch only at the very end however.\n\nWhile fiddling with my pipeline, I accidentally had two lines with:\n` examples = examples.batch(batchsize)`\n\nThis of course got me in some memory issues, even when every single example\nis just a 224x224 image.\n\nOp ma 29 jun. 2020 om 10:53 schreef gitgithan <notifications@github.com>\n\n> @Trezorro <https://github.com/Trezorro> Could you share how you batch\n> your dataset multiple times accidentally and why would that cause this\n> ResourceExhaustedError?\n>\n> Is it something like in a single method chain of\n> data.cache().batch(32).prefetch(1).batch(32)?\n> Or 4 assignments separated some distance apart in code? eg. data =\n> data.cache(), data=data.batch(32), data = data.prefetch(1),\n> data=data.batch(32)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/29213#issuecomment-651026270>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AESTRG2VUV2PNURLJ5OGD5DRZBI7DANCNFSM4HR2RBWA>\n> .\n>\n-- \nMet vriendelijke groet,\nMilan Tresoor\n"]}, {"number": 29212, "title": "[TF 2.0 API Docs] tf.queue.FIFOQueue", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/queue/FIFOQueue\r\n\r\n## Description of issue (what needs changing):\r\n\r\nSome methods not provides raising error lists\r\n\r\n### Raises listed and defined\r\n\r\n* dequeue\r\n* dequeue_many\r\n* dequeue_up_to\r\n* enqueue\r\n* enqueue_many\r\n\r\n### Submit a pull request?\r\nI will\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@HardPlant This is a stale issue. Is there any actionable item like raising PR? \r\n\r\nIf you plan to raise a PR, please feel free to use updated version https://www.tensorflow.org/api_docs/python/tf/queue/FIFOQueue\r\n\r\nThanks for your contribution. ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29211, "title": "Add default value in complex function ", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29211) for more info**.\n\n<!-- need_sender_cla -->", "@thisisiron thanks for your contribution , can you please sign CLA.", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29211) for more info**.\n\n<!-- ok -->", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 29210, "title": "[TF 2.0 API Docs] tf.dtypes.cast", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/dtypes/cast\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\nthe example: \r\n```python\r\nx = tf.constant([1.8, 2.2], dtype=tf.float32)\r\ntf.cast(x, tf.int32)  # [1, 2], dtype=tf.int32\r\n```\r\nis not correct.  \r\nIt need to change ```tf.cast``` to ```tf.dtypes.cast```.\r\n\r\nthis is correct example\r\n```python\r\nx = tf.constant([1.8, 2.2], dtype=tf.float32)\r\ntf.dtypes.cast(x, tf.int32)  # [1, 2], dtype=tf.int32\r\n```\r\n\r\n### Submit a pull request?\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/29214\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": []}, {"number": 29209, "title": "[TF 2.0 API Docs] tf.keras.layers.Conv2D", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D\r\n\r\n## Description of issue (what needs changing):\r\n\r\n\r\n### Correct links\r\n\r\nhttps://www.tensorflow.org/alpha/tutorials/distribute/multi_worker is incorrect. \r\nIt has 404 error.\r\n\r\n### Parameters defined\r\n\r\n**kwargs is not defined.\r\n\r\n### Raises listed and defined\r\n\r\nErrors are not defined.\r\n\r\n### Usage example\r\n\r\nNo usage example is provided.\r\n\r\n### Request visuals, if applicable\r\n\r\nNo visuals are included.\r\n\r\n", "comments": ["The incorrect link has been fixed. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29209\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29209\">No</a>\n"]}, {"number": 29208, "title": "build micro_speech error", "body": "Deep Learning VM  images/tf-2-0-cpu-experimental-20190502\r\n```bash\r\ngoogcheng@tensorflow-4-vm:~$ git clone https://github.com/tensorflow/tensorflow.git\r\nCloning into 'tensorflow'...\r\nremote: Enumerating objects: 2167, done.\r\nremote: Counting objects: 100% (2167/2167), done.\r\nremote: Compressing objects: 100% (845/845), done.\r\nremote: Total 598119 (delta 1392), reused 1643 (delta 1320), pack-reused 595952\r\nReceiving objects: 100% (598119/598119), 343.85 MiB | 47.04 MiB/s, done.\r\nResolving deltas: 100% (483846/483846), done.\r\ngoogcheng@tensorflow-4-vm:~$ ls\r\ntensorflow\r\ngoogcheng@tensorflow-4-vm:~$ cd tensorflow/\r\ngoogcheng@tensorflow-4-vm:~/tensorflow$ ls\r\nACKNOWLEDGMENTS     AUTHORS             CODEOWNERS    CONTRIBUTING.md    LICENSE       RELEASE.md   third_party\r\nADOPTERS.md         BUILD               configure     ISSUES.md          models.BUILD  SECURITY.md  tools\r\narm_compiler.BUILD  CODE_OF_CONDUCT.md  configure.py  ISSUE_TEMPLATE.md  README.md     tensorflow   WORKSPACE\r\ngoogcheng@tensorflow-4-vm:~/tensorflow$ bazel run -c opt --copt=-mavx2 --copt=-mfma \\\r\n> tensorflow/examples/speech_commands:train -- \\\r\n> --model_architecture=tiny_conv --window_stride=20 --preprocess=average \\\r\n> --wanted_words=\"yes,no\" --silence_percentage=25 --unknown_percentage=25 --quantize=1\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=112\r\nINFO: Reading rc options for 'run' from /home/googcheng/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nERROR: /home/googcheng/.cache/bazel/_bazel_googcheng/c7d71bbdf8501e4441f5b58bb2db4ae1/external/io_bazel_rules_closure/closure/protobuf/closure_proto_library.bzl:66:21: name 'ProtoInfo' is not defined (did you mean 'protos'?)\r\nERROR: error loading package '': Extension 'closure/protobuf/closure_proto_library.bzl' has errors\r\nERROR: error loading package '': Extension 'closure/protobuf/closure_proto_library.bzl' has errors\r\nINFO: Elapsed time: 5.973s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\ngoogcheng@tensorflow-4-vm:~/tensorflow$ git log\r\ncommit 94feeb1c0ade3c673a758c0794e19c82b6e868b6\r\nAuthor: Tiezhen WANG <wangtz@google.com>\r\nDate:   Fri May 31 03:18:30 2019 -0700\r\n\r\n    nit remove unnecessary indirection in all_ops_resolver.\r\n    \r\n    PiperOrigin-RevId: 250856298\r\n\r\ncommit b1a535b3abc11de5e00f5b40b2a7f795b7160376\r\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\r\nDate:   Fri May 31 02:49:19 2019 -0700\r\n\r\n    Test that tf.saved_model.load() does not add to the TRAINABLE_VARIABLES\r\n    collection and add a comment why. There is no change in behavior.\r\n```", "comments": ["Please help us to know whether you were able to import TensorFlow. Also let us know what platform (operating system etc) you are using. Thanks!", "```bash\r\nWelcome to the Google Deep Learning VM\r\n======================================\r\nVersion: tf-cpu.1-13.m26\r\nBased on: Debian GNU/Linux 9.9 (stretch) (GNU/Linux 4.9.0-9-amd64 x86_64\\n)\r\nResources:\r\n * Google Deep Learning Platform StackOverflow: https://stackoverflow.com/questi\r\nons/tagged/google-dl-platform\r\n * Google Cloud Documentation: https://cloud.google.com/deep-learning-vm\r\n * Google Group: https://groups.google.com/forum/#!forum/google-dl-platform\r\n```", "@achandraa  the train needs existed tensorflow install ?", "@goog : Looks like this is a duplicate of issue #29206 . Can we track resolution to that issue and close on this. This would help us to track better. Thanks!"]}, {"number": 29207, "title": "TFL GPU gpu::gl::reshape Is attr redundant?", "body": "I'm trying to write a few test cases for `gl::reshape` and noticed that `attr` appears to be of not much use. The only use I can find is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/gl/kernels/reshape.cc#L49-L52):\r\n\r\n```\r\n    if (attr.new_shape != output->tensor.shape) {\r\n      return InvalidArgumentError(\r\n          \"Dimensions for output does not match new_shape attribute\");\r\n    }\r\n```\r\n\r\nBut `attr.new_shape` always take the tensor shape of the output ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/common/model_builder.cc#L811)). Under what circumstances, if any, would the matching `attr.new_shape` against `output->tensor.shape` fail?", "comments": ["@mun3 This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}]