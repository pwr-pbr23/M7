[{"number": 6814, "title": "mnist preload data reader example fails ", "body": "I am using tensorflow version 12, following the documentation at \r\n\r\nhttps://www.tensorflow.org/how_tos/reading_data/\r\n\r\nwhich points to the example:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_preloaded_var.py\r\n\r\nhowever, when I run that example, I get errors about un-initialized variables. Here is the output:\r\n\r\n```\r\npython tf_mnist_preload.py \r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\r\nExtracting /tmp/data/train-images-idx3-ubyte.gz\r\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\r\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\r\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\r\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\r\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\r\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:87:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:87:00.0)\r\nW tensorflow/core/framework/op_kernel.cc:975] Failed precondition: Attempting to use uninitialized value input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\r\n\t [[Node: input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\"], limit=2, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs)]]\r\nW tensorflow/core/framework/op_kernel.cc:975] Out of range: FIFOQueue '_1_input/batch/fifo_queue' is closed and has insufficient elements (requested 100, current size 0)\r\n\t [[Node: input/batch = QueueDequeueMany[_class=[\"loc:@input/batch/fifo_queue\"], component_types=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input/batch/fifo_queue, input/batch/n)]]\r\nSaving\r\nDone training for 2 epochs, 0 steps.\r\nTraceback (most recent call last):\r\n  File \"tf_mnist_preload.py\", line 195, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"tf_mnist_preload.py\", line 147, in main\r\n    run_training()\r\n  File \"tf_mnist_preload.py\", line 142, in run_training\r\n    coord.join(threads)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n    sess.run(enqueue_op)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\r\n\t [[Node: input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\"], limit=2, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs)]]\r\n\t [[Node: input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo/_12 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_13_input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op u'input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo', defined at:\r\n  File \"tf_mnist_preload.py\", line 195, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"tf_mnist_preload.py\", line 147, in main\r\n    run_training()\r\n  File \"tf_mnist_preload.py\", line 63, in run_training\r\n    [input_images, input_labels], num_epochs=FLAGS.num_epochs)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 305, in slice_input_producer\r\n    shared_name=shared_name)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 262, in range_input_producer\r\n    shared_name, name, \"fraction_of_%d_full\" % capacity)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 156, in input_producer\r\n    input_tensor = limit_epochs(input_tensor, num_epochs)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 96, in limit_epochs\r\n    counter = epochs.count_up_to(num_epochs)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 652, in count_up_to\r\n    return state_ops.count_up_to(self._variable, limit=limit)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 126, in count_up_to\r\n    result = _op_def_lib.apply_op(\"CountUpTo\", ref=ref, limit=limit, name=name)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/davidsch/miniconda2/envs/mlearn/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\r\n\t [[Node: input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs\"], limit=2, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs)]]\r\n\t [[Node: input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo/_12 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_13_input/input_producer/input_producer/fraction_of_32_full/limit_epochs/CountUpTo\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\n```\r\n\r\n\r\n", "comments": ["@mrry I'm noticing two recent issues about \"attempting to use uninitialized value\" in the past couple days. Mind taking a look?\r\n\r\nSee also: https://github.com/tensorflow/tensorflow/issues/6785", "I suspect this has been broken since 4c85a95925a68fec324f70cd0d7f3d4548f97a38, which made the internal variable in a `tf.train.slice_input_producer()` a local variable, but didn't add any code to initialize the local variables in this example.\r\n\r\nAssigning to @ilblackdragon, who made that change. ", "This is working in both r1.2 and master."]}, {"number": 6813, "title": "Fix wrong order for op_name/summary_name in range_input_producer", "body": "Fixes #6808 ", "comments": ["Can one of the admins verify this patch?", "@dandelionmane how does that look?", "Jenkins, test this please.", "I saw a PR pass by, I think, the failure is unrelated.\r\n\r\nJenkins, test this please."]}, {"number": 6812, "title": "python documentation for tensorflow.abs out of sync", "body": "I checked out the master branch tonight:\r\nd4b5c606fc9fbd1a20b5b113b4bc831f31d889a3\r\n\r\nI installed the code with pip into a conda environment\r\n\r\nIt seems the tensorflow.complex_abs() function has been merged into the overloaded tensorflow.abs() function.\r\n\r\nThe docstring, along with the rest of the python documentation don't seem to reflect this change:\r\n\r\n> In [53]: tf.abs?\r\n> Signature: tf.abs(x, name=None)\r\n> Docstring:\r\n> Computes the absolute value of a tensor.\r\n> \r\n> Given a tensor of real numbers `x`, this operation returns a tensor\r\n> containing the absolute value of each element in `x`. For example, if x is\r\n> an input element and y is an output element, this operation computes\r\n> \\\\(y = |x|\\\\).\r\n> \r\n> Args:\r\n>   x: A `Tensor` or `SparseTensor` of type `float32`, `float64`, `int32`, or\r\n>     `int64`.\r\n>   name: A name for the operation (optional).\r\n> \r\n> Returns:\r\n>   A `Tensor` or `SparseTensor` the same size and type as `x` with absolute\r\n>     values.\r\n> File:      ~/src/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\r\n> Type:      function\r\n> ", "comments": ["This change is being documented as part of the 1.0 release process. Please see https://github.com/tensorflow/tensorflow/releases/tag/v1.0.0-alpha\r\n\r\nAre you suggesting that we change the docstring above to say something like \"Given a tensor of real or complex numbers x, this operation returns a tensor [...]\"? If so, maybe this would be an easy opportunity for you to make a contribution to the codebase. We'd be grateful for a pull request. Otherwise maybe @xmbrst could take a look.", "Closing due to lack of recent activity. if you provide additional information, we can reopen this. Thanks!"]}, {"number": 6811, "title": "tf.unstack:AttributeError: 'module' object has no attribute 'unstack'", "body": "I am trying to use tf.unstack but is getting the error:\r\n\r\nAttributeError: 'module' object has no attribute 'unstack'\r\n\r\nI am trying the alternative code version of the two lines in ptb_word_lm.py", "comments": ["Could you fill out the issue template? (version info, reproducible example)", "version: 0.11.0\r\nreproducible example:\r\n\r\ninputs = tf.unstack(inputs, num=num_steps, axis=1)\r\n\r\nI am trying out ptb_word_lm.py provided with tensorflow itself. In the middle of the code a commented out portion provides a suggested improvement to the code. I am trying that suggestion. The line of code above is already provided by the original version of the ptb_word_lm.py but commented.My entire code is:\r\n[http://pastebin.com/U9guTEbv](http://pastebin.com/U9guTEbv)\r\n", "tf.unstack wasn't added until [v1.0.0-alpha](https://github.com/tensorflow/tensorflow/releases/tag/v1.0.0-alpha). Try upgrading.", "jart can you pls tell me which of the .py files have the unstack function? Actually upgrading is a problem in my machine.", "@sharod Here's the commit that added unstack: https://github.com/tensorflow/tensorflow/commit/0ee63d0cdb63df0f34dada5176d6a5ad92fbccab. It appears to be a renaming of tf.unpack. So you should actually be able to just use that function in its place."]}, {"number": 6810, "title": "Update LSTMBlockCell to use LSTMStateTuple as state", "body": "Fixes #6582", "comments": ["Can one of the admins verify this patch?", "@ebrevdo any interest?", "Reading my codes written at midnight again, I found LSTMBlockFusedCell should not be fixed! I will revert it back.", "@jihunchoi just to make sure, this PR is blocked on #7387 ?", "No, I mentioned this PR wrongly in #7387 but the message did not disappear after I deleted the mention... :(", "It's okay, just wanted to understand the status -- so this is ready to review again?", "Yes, the mention can be safely ignored.", "Can you resolve merge conflicts?", "Was there a conflict? It was able to be automatically merged with tensorflow/master branch.\r\nAnyway, I updated it to follow the latest master branch and it seems there exists no conflict.", "thanks!  please ensure all tests pass.", "@tensorflow-jenkins test this please"]}, {"number": 6809, "title": "TensorFlow 1.0.0 alpha binaries require SSE4.1", "body": "The TensorFlow 1.0.0 alpha binaries require SSE4.1, which was not necessary to run the binaries of earlier versions (e.g. 0.12.1). SSE4.1 is only supported by a few AMD architectures. In particular, it is not supported by the  \"Barcelona\" and the \"Bobcat\" architecture. They support SSE4a, which is not supported by Intel processors.\r\n\r\nTherefore, running the 1.0.0 binaries on our workstations with \"AMD Opteron(tm) Processor 6174\" fails with the following error:\r\n`The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.`\r\n\r\nIs it possible to make the official TensorFlow 1.0.0 binaries not dependent on vendor specific SSE4.1 instructions? I am aware that these AMD processors are not the newest ones and that it's possible to compile TensorFlow without the SSE4.1 requirement (at least, I assume so), but I wanted to raise this issue to see if you think it might be possible to make the official 1.0.0 binaries work without SSE4.1. Alternatively, would you consider offering alternative binaries without SSE4.1 similar to how you offer different binaries for different python versions?", "comments": ["I'm also having trouble finding the correct compiler flags to enable SSE4.1, SSE4.2 and AVX instructions. If anyone has insights, I'd really appreciate it.\r\n\r\nBuilding on a Retina MBP with 2.6 GHz Intel Core i7 on Mac OS X El Capitan 10.11.6 \r\nfrom commit https://github.com/tensorflow/tensorflow/commit/d4b5c606fc9fbd1a20b5b113b4bc831f31d889a3 with this custom build rule:\r\n```\r\ncc_binary(\r\n    name = \"libtensorflow_all.so\",\r\n    linkshared = 1,\r\n    deps = [\r\n    \t\"//tensorflow/c:c_api\",\r\n        \"//tensorflow/cc:cc_ops\",\r\n        \"//tensorflow/core:framework_internal\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n```\r\n\r\nBased on [this StackOverflow thread](http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions), I tried:\r\n\r\n```\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 tensorflow:libtensorflow_all.so\r\n```\r\n\r\nwhich threw the compiler error:\r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_pjh/932673a2a7490816ef213d3cdb401ef6/external/gif_archive/BUILD:8:1: C++ compilation of rule '@gif_archive//:gif' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 -DNDEBUG ... (remaining 33 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nerror: unknown FP unit 'both'\r\n```\r\n\r\nI was able to compile with this modification:\r\n```\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 tensorflow:libtensorflow_all.so\r\n```\r\n\r\nbut the resulting .so immediately crashed at runtime.\r\n\r\nI recompiled with the basic:\r\n```\r\nbazel build tensorflow:libtensorflow_all.so\r\n```\r\n\r\nbut at runtime, I get the following warnings:\r\n```\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n```\r\n\r\nThanks!", "Having the TensorBoard binary require SSE4.1 sounds problematic. Could you take a look @zffchen78?", "Is this issue receiving any love? I'd like to run tensorflow on a machine that doesn't have that instruction set.\r\n\r\nAt the very least, could someone help me find the most recent commit I could use that _doesn't_ have this issue?", "Thank you for your patience @abejfehr. I've sent a friendly ping to a couple Googlers via email. Expect an update very soon.", "@gunan, did you already fix the build scripts/Jenkins setup to explicitly set the optimization flags? We should remove 4.1.", "Well, with our latest change, (--config=opt) the binaries we build for distribution should now be without sse 4.1.\r\n\r\nCould you try downloading and installing nightly binaries?", "OK, with 1.0rc0, now the binaries should not need sse 4.1\r\nPlease let me know if you run into any issues.\r\n\r\nI will close this issue now as our latest binaries are more compatible.", "@gunan \r\nI have similar issue issues issue. My cpu does not support avx2 and it will cause core dumped each time when import tensorflow. Is there any option to compile without this optimization?", "@Noahsark yes, 1.0rc0 should not have SSE 4.1", "@Noahsark It also does not have avx2.\r\nIt is back to the original compilation options.", "Thanks a lot!", "@gunan @yaroslavvb \r\nI do switch to 1.0rc0 with the default compile option, but the problem still exists. \r\n\r\nHere is the output when I try to import it.\r\n\r\n>>> import tensorflow\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally\r\nF tensorflow/core/platform/cpu_feature_guard.cc:35] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine.\r\nAborted (core dumped)\r\n\r\n \r\nAny ideas ?\r\nThank you so much.", "@Noahsark can you give a link to the binary that you've used?\r\n(unless your built it yourself, which would be a different issue)", "@yaroslavvb \r\nYes, I build it from source. Any way to get avoid this?", "This is surprising -- you are compiling (by default) with\n-march=native. \u200bThis means that unless you compile on a computer other than\nthe one you run on, it should not use optimizations your machine doesn't\nunderstand. Did you set the optimization flags? Which hash are you building\nfrom?\n", "Bad news, 1.0.0rc1 and the nightlies again require SSE4.1. @gunan Can you please reopen the issue. (Looks like the 1.0.0rc0 build also requires SSE4.1, although I thought I checked that before and it worked?! Maybe I checked it on a more modern CPU accidentally\u2026)", "I just verified that somehow we have some sse 4.1 instructions in the disassembled code.\r\nBut we are not sure how it crept back in.\r\nWill do some debugging in our build scripts.", "Found the problem. It was bad, because when you are building from sources on any machine it was enforcing `-msse4.1`. I am hoping to get fixed binaries everywhere by tomorrow.", "Tested it with 1.0.0rc2. Looks good :-) Thanks a lot!", "Glad to hear this is resolved.\r\nThank you for your patience."]}, {"number": 6808, "title": "Wrong argument order for input_producer", "body": "The signature of `input_producer` is:\r\n```python\r\ndef input_producer(input_tensor,\r\n                   element_shape=None,\r\n                   num_epochs=None,\r\n                   shuffle=True,\r\n                   seed=None,\r\n                   capacity=32,\r\n                   shared_name=None,\r\n                   summary_name=None,\r\n                   name=None,\r\n                   cancel_op=None):\r\n\"\"\"\r\n    summary_name: (Optional.) If set, a scalar summary for the current queue\r\n      size will be generated, using this name as part of the tag.\r\n\"\"\"\r\n```\r\nand this function adds scalar summary of the fraction of the queue that is full (not actually queue size), with name `summary_name`.\r\n\r\nBut `range_input_producer` is calling `input_producer` like this:\r\n```python\r\ndef range_input_producer(limit, num_epochs=None, shuffle=True, seed=None,\r\n                         capacity=32, shared_name=None, name=None):\r\n  with ops.name_scope(name, \"input_producer\", [limit]) as name:\r\n    range_tensor = math_ops.range(limit)\r\n    return input_producer(\r\n        range_tensor, [], num_epochs, shuffle, seed, capacity,\r\n        shared_name, name, \"fraction_of_%d_full\" % capacity)\r\n```\r\nIt looks like a bug to me: name and summary_name might need to swap.", "comments": ["Nice catch. BTW, easy fixes like this may be easier to deal with as Pull Requests. I was able to make this PR in about 60 seconds using github's \"Edit Feature\"\r\n\r\n```\r\ngit remote add y https://github.com/yaroslavvb/tensorflow\r\ngit remote add tfmain https://github.com/tensorflow/tensorflow.git\r\n\r\ngit fetch tfmain\r\ngit checkout tfmain/master -b bugfix\r\ngit push --set_upstream y\r\n```\r\n\r\nThen in `bugfix` branch on yaroslavvb/tensorflow, find file, click \"Edit\" button, edit file, then click commit, then click \"Pull Requests\" and select \"tensorflow/master\" on left and \"yaroslavvb/tensorflow:bugfix\" on right"]}, {"number": 6807, "title": "[TensorBoard] To small dropdown menu", "body": "This is not a big issue, but it is a tad annoying.\r\n\r\nWhen using TensorBoard to visualize the graph the 'run' drop-down menu is very small.\r\n\r\n![tb_menu](https://cloud.githubusercontent.com/assets/8115763/21893472/783d3b5e-d8db-11e6-81b0-08bc6fadf982.png)\r\n\r\nI am using Chrome `Version 55.0.2883.87 m`", "comments": ["Thoughts on this one @dandelionmane?", "Yeah, we should fix this, and it should be trivial. I'll file a bug internally and send it to @chihuahua", "Hi @Faur\u2014thanks for reporting. I've migrated this to our new repository at https://github.com/tensorflow/tensorboard/issues/47.\r\n\r\nSorry for the long delay\u2014we're going to try to be more responsive to issues and PRs moving forward. :-)"]}, {"number": 6806, "title": "convert_graphdef_memmapped_format produces corrupt graphs", "body": "I tried memory mapping an exported/frozen graph. Let's say the graph is named `graph.pb`. After converting the graph like this\r\n\r\n    ./path/to/convert_graphdef_memmapped_format --in_graph=graph.pb --out_graph=graph_mmap.pb\r\n\r\nI get\r\n\r\n- `Converted 16 nodes` for an unquantized graph\r\n- `Converted 0 nodes` for the 8bit quantized version of the graph\r\n\r\nIf I try to load any of these output files using the C++ API `ReadBinaryProto` and then `Run` the graph the following errors occur\r\n\r\n- Unquantized: `Session was not created with a graph before Run()`\r\n- Quantized: `Out of range: Read less bytes than requested`\r\n\r\nso I guess the output file is a corrupt protobuf file (or a different version).\r\n", "comments": ["Could you please provide the full stack traces?", "I'm not sure what stack trace and how to provide it since it doesn't crash. `ReadBinaryProto` just returns a status with `code` `OUT_OF_RANGE` and `msg` `Read less bytes than requested` for e.g. the 8bit quantized graph.", "We try to keep this issue tracker limited to bugs and feature requests. We recommend posting on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support."]}, {"number": 6805, "title": "Fix a bug in windows_file_system.cc for reading file", "body": "When read_result is TRUE, result should be set to bytes_read.\r\n\r\nhttps://msdn.microsoft.com/en-us/library/windows/desktop/ms686358(v=vs.85).aspx\r\n\"When a function is called to perform an overlapped operation, the operation might be completed before the function returns. When this happens, the results are handled as if the operation had been performed synchronously. If the operation was not completed, however, the function's return value is FALSE, and the GetLastError function returns ERROR_IO_PENDING.\"\r\n", "comments": ["I found this bug when I was setting a TF job for windows on Bazel ci where `python_op_gen_main` failed to read `hidden_ops.txt`.", "Is there another function that won't have a race condition?", "I am not sure. @mrry Do we use `::ReadFile` for a reason? The problem is the code didn't deal with the situation when the function is performed as if it's synchronously. Is there a better solution?", "@meteorcloudy I believe @vit-stepanovs contributed that particular piece of code, and I assume he used overlapped I/O here because it's the easiest way to read from a particular offset **without** a race condition if multiple threads are accessing the file. (@drpngx: I'm not sure what race condition you mean here. The return value is non-deterministic, sure, but as long as we handle it as Yun does here, I think it's threadsafe.)\r\n\r\nThe fix looks correct to me... I've had to fix similar issues with Windows API calls that way in the past, and I can't think of a more elegant solution.", "I was thinking of GetLastError. Not a huge problem, but if other code thrashes errno then it could become a problem.", "Ah, I see what you mean. That does sound ominous, but according to the docs, `GetLastError()` returns errno for the calling thread, so we should be alright:\r\n\r\n> Retrieves the calling thread's last-error code value. The last-error code is maintained on a per-thread basis. Multiple threads do not overwrite each other's last-error code.\r\n\r\n(How nice of the doc writer to state the property in triplicate!)\r\n\r\nhttps://msdn.microsoft.com/en-us/library/windows/desktop/ms679360(v=vs.85).aspx", "Oh nice, good to know. Thanks!"]}, {"number": 6804, "title": "Defun broken: errors_impl.NotFoundError: Op type not registered", "body": "This code fails:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import function\r\n\r\n@function.Defun(tf.float32)\r\ndef custom_op(x):\r\n    return x\r\n\r\nx = tf.Variable(1, dtype=tf.float32)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run(custom_op(x)))\r\n\r\nsess.close()\r\n```\r\n\r\nException:\r\n\r\n    tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'custom_op_da39a3ee'\r\n", "comments": ["The example is partly taken from #3710 although the bug reported there is probably a different one and was already fixed.", "Version info?", "Version 0.12.0.\r\n", "Somehow running the init op breaks things. The code below works, but fails in the same way if you uncomment the `sess.run(init_op)` line\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import function\r\n\r\ntf.reset_default_graph()\r\n\r\n@function.Defun(tf.float32)\r\ndef custom_op(x):\r\n    return tf.identity(x)\r\n\r\nx = tf.placeholder(dtype=tf.float32)\r\nsess = tf.Session()\r\ninit_op = tf.global_variables_initializer()\r\n#sess.run(init_op)\r\nx2 = custom_op(x)\r\nprint(sess.run(x2, feed_dict={x: 1.0}))\r\n\r\n```", "@zffchen78 must all the custom ops be created before the first `session.run` call? I didn't see this req in the docs", "@albertz a work-around is to create your `custom_op` and save node before first run call", "How do I save it before the first run call?", "Ie in my example, you can change order of first  session run and creating init_op and it'll work", "I see. So the workaround for my example:\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import function\r\n\r\n@function.Defun(tf.float32)\r\ndef custom_op(x):\r\n    return x\r\n\r\nx = tf.Variable(1, dtype=tf.float32)\r\ny = custom_op(x)\r\ninit_op = tf.global_variables_initializer()\r\n\r\nsess = tf.Session()\r\nsess.run(init_op)\r\nprint(sess.run(y))\r\n\r\nsess.close()\r\n```\r\n\r\nBut I think this is a bug then, right? Because I would suppose that this should work. Or is this hard to fix?\r\n", "Yes, I think it was unexpected, I'll leave to @zffchen78 to comment if it's intended behavior or not", "@albertz the behavior you see is explainable.\r\n\r\nthe line \r\n  sess = tf.Session() \r\neffectively snapshots the default graph which includes the functions instantiated so far at that point. \r\nIf the function is instantiated (the line custom_op(x)) after that point, so far, our implementation does not re-snapshot the graph. That might be feasible but may be a bit complex, too."]}, {"number": 6803, "title": "dynamic seq2seq add example of using context_state", "body": "tf v1.0, in contrib/seq2seq/seq2seq_test.py , the code do not show how to use context_state, so we can get best path as output instead of rnn outputs.\r\nCan we add sample code?", "comments": ["More samples and documentation is always a good thing. I'm going to mark this as contributions welcome. Although I would like to bring this request to the attention of @xiejw who seems to be a big seq2seq contributor.", "Well, I have find out how to do this. like below in decoder_fn \r\n    \r\n    if cell_output is None:\r\n      context_state = tensor_array_ops.TensorArray(\r\n        dtype=dtype, tensor_array_name=\"best_path\", size=0, dynamic_size=True, infer_shape=False)\r\n    else:\r\n      context_state = context_state.write(time - 1, next_input_id)\r\n\r\nand use it :\r\n\r\n    tf.transpose(decoder_context_state_inference.stack(), [1, 0])", "In seq2seq_test.py 108-119, might be a minor bug, here should    \r\n\r\n    maximum_length=decoder_sequence_length\r\n\r\n    # currently Inference decoder\r\n        decoder_fn_inference = Seq2SeqTest._decoder_fn_with_context_state(\r\n            decoder_fn_lib.simple_decoder_fn_inference(\r\n                output_fn=output_fn,\r\n                encoder_state=encoder_state,\r\n                embeddings=decoder_embeddings,\r\n                start_of_sequence_id=start_of_sequence_id,\r\n                end_of_sequence_id=end_of_sequence_id,\r\n                #TODO: find out why it goes to +1\r\n                maximum_length=decoder_sequence_length - 1,\r\n                num_decoder_symbols=num_decoder_symbols,\r\n                dtype=dtypes.int32))\r\n\r\nwhile in decoder_fun.py , should be\r\n    done = control_flow_ops.cond(math_ops.equal(time, maximum_length)\r\n    \r\n    # currently decoder_fun.py line 244\r\n    done = control_flow_ops.cond(math_ops.greater(time, maximum_length),\r\n            lambda: array_ops.ones([batch_size,], dtype=dtypes.bool),\r\n            lambda: done)\r\n\r\nAfter the change the test can pass, with decoder_sequence_length 7 equal decoder_context_state_train_res 7,  meaning last time is 7,(time is from 0 to 7) \r\nand output shape is (7,2,20)\r\n\r\n    self.assertEqual(decoder_sequence_length,\r\n                             decoder_context_state_train_res)", "Since I haven't found any example codes or explanations for `contrib.legacy_seq2seq` and `contrib.seq2seq` API after 1.1 release, I made my own sample codes.\r\nhttps://github.com/j-min/tf_tutorial_plus/tree/master/RNN_seq2seq\r\nIs there a way that I can contribute by refactoring my code as part of official tutorials seq2seq APIs?", "Feel free to send a PR ([this](https://help.github.com/articles/merging-a-pull-request/) might help if you're not sure how to contribute!!) otherwise, I am closing this issue now"]}, {"number": 6802, "title": "build with CPU optimization and GPU support are conflict?", "body": "```\r\nbazel build --copt=-march=native -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nCan I use both --copt=-march=native and --config=cuda , for now it gives out an error\r\n```\r\nERROR: /home/wenjian/tensorflow-1.0.0-alpha/tensorflow/core/BUILD:1200:1: C++ compilation of rule '//tensorflow/core:framework_internal' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 123 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from ./tensorflow/core/framework/numeric_types.h:25:0,\r\n                 from ./tensorflow/core/framework/allocator.h:23,\r\n                 from ./tensorflow/core/framework/tensor.h:21,\r\n                 from ./tensorflow/core/util/sparse/group_iterator.h:21,\r\n                 from tensorflow/core/util/sparse/group_iterator.cc:16:\r\n./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:52: fatal error: src/Tensor/TensorContractionThreadPool.h: No such file or directory\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 78.339s, Critical Path: 66.15s\r\n\r\n```\r\n", "comments": ["Having the same issue.", "Similar thing was fixed 2 days ago in https://github.com/tensorflow/tensorflow/pull/6759, can you check with master?", "@yaroslavvb master branch seems solved the problem"]}, {"number": 6801, "title": "1.0alpha configure problem", "body": "```\r\nERROR: /home/wenjian/pkgs/tensorflow-1.0.0-alpha/tensorflow/workspace.bzl:345:3: no such package '@junit_jar//jar': Error downloading [https://github.com/junit-team/junit4/releases/download/r4.12/junit-4.12.jar] to /home/wenjian/.cache/bazel/_bazel_wenjian/72810017c7bd644d4bee7673555b820d/external/junit_jar/junit-4.12.jar: Tried to reconnect at offset 121,286 but server didn't support it and referenced by '//external:junit'.\r\nERROR: /home/wenjian/pkgs/tensorflow-1.0.0-alpha/tensorflow/workspace.bzl:345:3: no such package '@junit_jar//jar': Error downloading [https://github.com/junit-team/junit4/releases/download/r4.12/junit-4.12.jar] to /home/wenjian/.cache/bazel/_bazel_wenjian/72810017c7bd644d4bee7673555b820d/external/junit_jar/junit-4.12.jar: Tried to reconnect at offset 121,286 but server didn't support it and referenced by '//external:junit'.\r\n```", "comments": ["shoud be the network problem"]}, {"number": 6800, "title": "Getting AbortionError when running modified tensorflow serving client", "body": "(I initially posted this on stackoverflow but have gotten no response.) I modified the mnist_export.py and mnist_client.py to run an LSTM on some excel data. No issue with the training and exporting, but I run into this error below when running the client code. \r\n\r\n```\r\ngrpc.framework.interfaces.face.face.AbortionError: \r\n    AbortionError(code=StatusCode.INTERNAL, details=\"Output 0 of type \r\n    double does not match declared output type float for node _recv_x_0 = \r\n    _Recv[client_terminated=true, \r\n    recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", \r\n    send_device=\"/job:localhost/replica:0/task:0/cpu:0\", \r\n    send_device_incarnation=-9032417372349471954, tensor_name=\"x:0\", \r\n    tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"()\")\r\n```\r\nMy input data is in the shape of [None, 1, 20] where 1 is the time_step and 20 is the features. \r\n\r\nBelow are the relevant parts of my training and export code: \r\n\r\n   ```\r\n def RNN(x, weights, biases):\r\n        # Prepare data shape to match `rnn` function requirements\r\n        # Current data input shape: (batch_size, n_steps, n_input)\r\n        # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\r\n        # Permuting batch_size and n_steps\r\n        x = tf.transpose(x, [1, 0, 2])\r\n        # Reshaping to (n_steps*batch_size, n_input)\r\n        x = tf.reshape(x, [-1, n_input])\r\n        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\r\n        x = tf.split(0, n_steps, x)\r\n        # Define a lstm cell with tensorflow\r\n        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\r\n        # Add dropout\r\n        #lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, input_keep_prob=keep_prob)\r\n        #lstm_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * n_layers, state_is_tuple=True)\r\n        # Get lstm cell output\r\n        outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32)\r\n        # Linear activation, using rnn inner loop last output\r\n        return tf.matmul(outputs[-1], weights['out']) + biases['out']\r\n    \r\n    # get data\r\n    train_data_set, test_data_set = read_data_sets('AUDJPY Data.csv')\r\n    \r\n    # tf Graph input\r\n    #x = tf.placeholder(\"float\", [None, n_steps, n_input])\r\n    y_ = tf.placeholder(\"float\", [None, n_classes])\r\n    keep_prob = tf.placeholder(tf.float32)\r\n    \r\n    # Exporter signatures\r\n    serialized_tf_example = tf.placeholder(tf.string, name='tf_example')\r\n    feature_configs = {\r\n        'x': tf.FixedLenFeature(shape=[n_steps,n_input], dtype=tf.float32),\r\n    }\r\n    tf_example = tf.parse_example(serialized_tf_example, feature_configs)\r\n    x = tf.identity(tf_example['x'], name='x')  # use tf.identity() to assign name\r\n    \r\n    # Define weights\r\n    weights = {\r\n        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\r\n    }\r\n    biases = {\r\n        'out': tf.Variable(tf.random_normal([n_classes]))\r\n    }\r\n    \r\n    pred = RNN(x, weights, biases)\r\n    \r\n    # Define loss and optimizer\r\n    y = tf.nn.softmax(pred, name='y')\r\n    values, indices = tf.nn.top_k(y, k=4)\r\n    classes = tf.contrib.lookup.index_to_string(tf.to_int64(indices), mapping=tf.constant([str(i) for i in range(n_classes)]))\r\n    cost = -tf.reduce_sum(y_ * tf.log(y))\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\n    \r\n    # Evaluate model\r\n    correct_pred = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n    \r\n    # Initializing the variables\r\n    init = tf.global_variables_initializer()\r\n    \r\n    # Launch the graph\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        step = 1\r\n        # Keep training until reach max iterations\r\n        while step * batch_size < training_iters:\r\n            batch_x, batch_y = train_data_set.next_batch(batch_size)\r\n            # Reshape data to get 28 seq of 28 elements\r\n            batch_x = batch_x.reshape((batch_size, n_steps, n_input))\r\n            # Run optimization op (backprop)\r\n            sess.run(optimizer, feed_dict={x: batch_x, y_: batch_y})\r\n            if step % display_step == 0:\r\n                # Calculate batch accuracy\r\n                acc = sess.run(accuracy, feed_dict={x: batch_x, y_: batch_y})\r\n                # Calculate batch loss\r\n                loss = sess.run(cost, feed_dict={x: batch_x, y_: batch_y})\r\n                print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\r\n                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\r\n                      \"{:.5f}\".format(acc))\r\n            step += 1\r\n        print(\"Optimization Finished!\")\r\n        print(\"Testing Accuracy:\", \\\r\n                sess.run(accuracy, feed_dict={x: test_data_set.features, y_: test_data_set.labels}))\r\n        # Export inference model.\r\n        export_path = '/tmp/'\r\n        print('Exporting trained model to %s' % export_path)\r\n        init_op = tf.group(tf.initialize_all_tables(), name='init_op')\r\n        saver = tf.train.Saver(sharded=True)\r\n        classification_signature = exporter.classification_signature(\r\n            input_tensor=serialized_tf_example,\r\n            classes_tensor=classes,\r\n            scores_tensor=values)\r\n        named_graph_signature = {\r\n            'inputs': exporter.generic_signature({'images': x}),\r\n            'outputs': exporter.generic_signature({'scores': y})}\r\n        model_exporter = exporter.Exporter(saver)\r\n        model_exporter.init(\r\n            init_op=init_op,\r\n            default_graph_signature=classification_signature,\r\n            named_graph_signatures=named_graph_signature)\r\n        model_exporter.export(export_path, tf.constant(FLAGS.export_version), sess)\r\n        print('Done exporting!')\r\n```\r\n\r\nAnd below are the relevant part of the client code:\r\n\r\n```\r\ndef do_inference(hostport, work_dir, concurrency, num_tests):\r\n      \"\"\"Tests PredictionService with concurrent requests.\r\n      Args:\r\n        hostport: Host:port address of the PredictionService.\r\n        work_dir: The full path of working directory for test data set.\r\n        concurrency: Maximum number of concurrent requests.\r\n        num_tests: Number of test images to use.\r\n      Returns:\r\n        The classification error rate.\r\n      Raises:\r\n        IOError: An error occurred processing test data set.\r\n      \"\"\"\r\n      train_data_set, test_data_set = read_data_sets(work_dir)\r\n      print('read test data')\r\n      host, port = hostport.split(':')\r\n      channel = implementations.insecure_channel(host, int(port))\r\n      stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n      request = predict_pb2.PredictRequest()\r\n      request.model_spec.name = 'mnist'\r\n      image, label = test_data_set.next_batch(1)\r\n      print(image.shape)\r\n      request.inputs['images'].CopyFrom(\r\n          tf.contrib.util.make_tensor_proto(image[0], shape=[1,1,20]))\r\n      result = stub.Predict(request, 10.0)\r\n```\r\n\r\nAlso below is the traceback if that helps:\r\n\r\n```\r\nTraceback (most recent call last):\r\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py\", line 225, in <module>\r\n        tf.app.run()\r\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 44, in run\r\n        _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py\", line 220, in main\r\n        FLAGS.concurrency, FLAGS.num_tests)\r\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py\", line 208, in do_inference\r\n        result = stub.Predict(request, 10.0)\r\n      File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 305, in __call__\r\n        self._request_serializer, self._response_deserializer)\r\n      File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 203, in _blocking_unary_unary\r\n        raise _abortion_error(rpc_error_call)\r\n    grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INTERNAL, details=\"Output 0 of type double does not match declared output type float for node _recv_x_0 = _Recv[client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=-9032417372349471954, tensor_name=\"x:0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()\")\r\n```\r\n\r\nAppreciate any help on how to fix this!\r\n\r\n", "comments": ["After some trial and error, adding a dtype=tf.float32 seems to have fixed it\r\n\r\n```\r\nrequest.inputs['images'].CopyFrom(\r\n          tf.contrib.util.make_tensor_proto(image[0], shape=[1,1,20], dtype=tf.float32))\r\n```\r\n\r\n", "I'm glad to hear you solved the issue. In the future, please be mindful of the fact that this issue tracker is for bugs and feature requests. We understand that not everything on Stack Overflow gets a response. But there are other venues you can potentially try, such as Reddit maybe, when that happens."]}, {"number": 6799, "title": "Go: Unable to create two ops of the same type using generated op wrapper functions", "body": "As reported in https://github.com/tensorflow/tensorflow/issues/10#issuecomment-272045853 , the following test (at head: d4b5c606fc9fbd1a20b5b113b4bc831f31d889a3):\r\n\r\n```go\r\npackage bug\r\n\r\nimport (\r\n        \"testing\"\r\n\r\n        tf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n        \"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n)\r\n\r\nfunc TestBug(t *testing.T) {\r\n        scope := op.NewScope()\r\n        op.Placeholder(scope.Subscope(\"x\"), tf.Float)\r\n        op.Placeholder(scope.Subscope(\"y\"), tf.Float)\r\n        if _, err := scope.Finalize(); err != nil {\r\n                t.Fatal(err)\r\n        }\r\n}\r\n```\r\n\r\nfails with:\r\n\r\n```sh\r\n--- FAIL: TestBug (0.01s)\r\n\tbug_test.go:15: failed to add operation \"Placeholder\": Duplicate node name in graph: 'Placeholder' (Stacktrace: goroutine 19 [running]:\r\n\t\truntime/debug.Stack(0x0, 0x0, 0x0)\r\n\t\t\t/usr/local/go/src/runtime/debug/stack.go:24 +0x79\r\n\t\tgithub.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).UpdateErr(0xc420072420, 0x411b8ac, 0xb, 0x4196640, 0xc42007c030)\r\n\t\t\t/home/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:106 +0x72\r\n\t\tgithub.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).AddOperation(0xc420072420, 0x411b8ac, 0xb, 0x0, 0x0, 0x0, 0x0, 0x0, 0xc420072450, 0x4108b00)\r\n\t\t\t/home/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:70 +0xbf\r\n\t\tgithub.com/tensorflow/tensorflow/tensorflow/go/op.Placeholder(0xc420072420, 0x1, 0x0, 0x0, 0x0, 0xc42006e4f0, 0x0)\r\n\t\t\t/home/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/wrappers.go:4734 +0x1c7\r\n\t\tgithub.com/tensorflow/tensorflow/tensorflow/go/b10.TestBug(0xc420098180)\r\n\t\t\t/home/go/src/github.com/tensorflow/tensorflow/tensorflow/go/b10/bug_test.go:13 +0xd2\r\n\t\ttesting.tRunner(0xc420098180, 0x4129d88)\r\n\t\t\t/usr/local/go/src/testing/testing.go:610 +0x81\r\n\t\tcreated by testing.(*T).Run\r\n\t\t\t/usr/local/go/src/testing/testing.go:646 +0x2ec\r\n\t\t)\r\n```\r\n\r\n(Thanks for pointing this out @sdeoras).", "comments": ["Thanks folks for quick resolution. It is working great now!", "Woohoo!"]}, {"number": 6798, "title": "Poisson Number Generator", "body": "Hi,\r\n\r\nI was wondering if it is possible to implement the Poisson Random Number generator in the random_ops in the future? \r\n\r\nThanks.", "comments": ["Does [tf.contrib.distributions.Poisson](https://www.tensorflow.org/api_docs/python/contrib.distributions/univariate__scalar__distributions#Poisson) serve your needs?", "Thanks a lot for the reply.\r\n\r\n`tf.contrib.distributions.Poisson` does not implement the \"sample_n\" method, even though in the API documentation it says so.", "This is being worked on internally.", "@yyaodong The requested feature will be pushed to GitHub probably sometime next week. When that happens, this issue will be automatically closed.", "@jart Is there any progress on the Poisson sampler? I'd be very interested in using this feature. Thanks!", "@shoyer is the better person to ask.", "Looks like it's still being finished up."]}, {"number": 6797, "title": "eiddccfiutbfedncengrfggkburbbbfvkufejukucetr", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": []}, {"number": 6796, "title": "Update DetectorActivity.java", "body": "Set default detector in DetectorActivity.java back to MultiBox", "comments": []}, {"number": 6795, "title": "Unable to compile HEAD on Windows with Bazel", "body": "I'm unable to successfully build the current HEAD (or versions 1.0 or 0.12) with Windows and Bazel.\r\n\r\n## Environment info\r\n\r\n* Operating System: Windows 10\r\n* Installed version of CUDA and cuDNN: None\r\n* Bazel version: 0.4.3 (Build time: Thu Dec 22 12:31:31 2016 (1482409891))\r\nI have done the following\r\n\r\n* install prerequisites for Bazel on Windows (https://bazel.build/versions/master/docs/windows.html)\r\n* checked out TF head (I also tried this with release 1.0, and release 0.12)\r\n* run exe C:\\tools\\msys64\\msys2.exe\r\n* set these env variables\r\n** export JAVA_HOME=\"$(ls -d C:/Program\\ Files/Java/jdk* | sort | tail -n 1)\"\r\n** export TMPDIR=\"C:/tmp\"\r\n** export BAZEL_SH=c:/tools/msys64/usr/bin/bash.exe\r\n** export BAZEL_VS=\"C:/Program Files (x86)/Microsoft Visual Studio 14.0\"\r\n** export BAZEL_PYTHON=\"C:/Users/.../Anaconda3/python.exe\"\r\n** export PYTHON_BIN_PATH=\"C:/Users/.../Anaconda3/python.exe\"\r\n** export PATH=/c/Users/.../Anaconda3/:$PATH\r\n** export PATH=/c/tools/bazel/:$PATH\r\n** export PATH=/c/Program\\ Files/CMake/bin/:$PATH\r\n* ./configure\r\n** no GPU, etc, just default options and path to Anaconda\r\n* bazel build -c opt --cpu=x64_windows_msvc //tensorflow/tools/pip_package:build_pip_package\r\n\r\nHowever I get errors after a few minutes of building.\r\nThis is reproducible and happens every time. I am following all the default options for the basic build. I would be interested to know if anyone is currently managing to compile on Windows?\r\nI can find a few tutorials using Docker or Cmake, however I'm trying to use the newest method described on TF's website.\r\n\r\n```\r\nINFO: From Linking external/protobuf/pyext/_message.so:\r\n   Creating library bazel-out/vc_14_0_x64-py3-opt/bin/external/protobuf/pyext/_m  essage.lib and object bazel-out/vc_14_0_x64-py3-opt/bin/external/protobuf/pyext/  _message.exp\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:1013:  1: null failed: protoc.exe failed: error executing command bazel-out/host/bin/ex  ternal/protobuf/protoc.exe --cpp_out=bazel-out/vc_14_0_x64-py3-opt/genfiles/ -I.   -I. -Iexternal/protobuf/src -Ibazel-out/vc_14_0_x64-py3-opt/genfiles/external/p  rotobuf/src ... (remaining 3 argument(s) skipped): com.google.devtools.build.lib  .shell.BadExitStatusException: Process exited with status -1073741515.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/python/BUILD:208  8:1: output 'tensorflow/python/framework/cpp_shape_inference_pb2.py' was not cre  ated.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/example/example.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/example/feature.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/allocation_description.pb.cc' was not create  d.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/attr_value.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/cost_graph.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/device_attributes.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/function.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/graph.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/kernel_def.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/log_memory.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/node_def.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/op_def.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/resource_handle.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/step_stats.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/summary.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor_description.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor_shape.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor_slice.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/types.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/versions.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/lib/core/error_codes.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/config.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/debug.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/tensor_bundle.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/saver.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/memmapped_file_system.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/saved_tensor_slice.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/example/example_parser_configuration.pb.cc' was not cr  eated.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/variable.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/control_flow.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/meta_graph.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/named_tensor.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/queue_runner.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/saved_model.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/tensorflow_server.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/event.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/test_log.pb.cc' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/example/example.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/example/feature.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/allocation_description.pb.h' was not created  .\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/attr_value.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/cost_graph.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/device_attributes.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/function.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/graph.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/kernel_def.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/log_memory.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/node_def.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/op_def.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/resource_handle.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/step_stats.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/summary.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor_description.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor_shape.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/tensor_slice.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/types.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/versions.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/lib/core/error_codes.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/config.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/debug.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/tensor_bundle.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/saver.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/memmapped_file_system.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/saved_tensor_slice.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/example/example_parser_configuration.pb.h' was not cre  ated.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/framework/variable.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/control_flow.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/meta_graph.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/named_tensor.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/queue_runner.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/saved_model.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/protobuf/tensorflow_server.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/event.pb.h' was not created.\r\nERROR: C:/Users/user/Clones/tensorflow/tensorflow/core/BUILD:163:1  : output 'tensorflow/core/util/test_log.pb.h' was not created.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 187.905s, Critical Path: 171.65s\r\n```", "comments": ["From a look at the error message, it seems to be failing when compiling a part of the protobuf library (viz. the Python extension that uses the optimized C++ implementation, which is the target `\"//:python/google/protobuf/pyext/_message.so\"` in [protobuf's `BUILD` file](https://github.com/google/protobuf/blob/228d242c583fb4e0dde17f0a52899f995d85c200/BUILD#L612)). The CMake build doesn't use this extension.\r\n\r\nIt looks like this feature is enabled by [this line in `tensorflow/tools/bazel.rc.template`](https://github.com/tensorflow/tensorflow/blob/b9d8d70f951d39e6c13bce704f95954c06211f93/tools/bazel.rc.template#L14).\r\n\r\nCan you try adding `--define=use_fast_cpp_protos=false` to your `bazel build` command, and see if that fixes things?", "@gunan Do we do anything special with the `bazel.rc` file on the Windows CI build to avoid hitting this? I couldn't see anything in the scripts, so presume it could be environmental, or controlled by some configuration file that I'm unaware of?", "'--cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=/w --verbose_failures --experimental_ui'\r\n\r\nThe above are all the options we use, and the build at master seems to be healthy for the last week(failures are infra issues):\r\nhttp://ci.tensorflow.org/job/tf-master-win-bzl/\r\n\r\n@meteorcloudy in case he knows something\r\n\r\n\r\nPS: the return value, `-1073741515` is too random, it looks fishy. Almost like a flaky integer overflow issue about it. Did you retry running?\r\n", "Just checking, by any chance did you accidentally install 32 bit version of anything?", "OK thank you everybody.\r\n\r\nthis argument fixed it:\r\n```\r\n--host_cpu=x64_windows_msvc \r\n```", "Please can I request that this page https://www.tensorflow.org/get_started/os_setup#installing_from_sources is updated, under section \"Create the pip package and install\":\r\n\r\n```\r\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n# To build with GPU support:\r\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n\r\n# To build on Windows (CPU mode only):\r\n$ bazel build -c opt --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc  //tensorflow/tools/pip_package:build_pip_package\r\n\r\n# The name of the .whl file will depend on your platform.\r\n$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.12.1-py2-none-any.whl\r\n```\r\n\r\nWith this update, the instructions on this page will now be fully comprehensive for Mac, Windows and Linux (unless I am missing something else?)", "@woodthom2 That seems like a reasonable request. We're currently revising some of the installation instructions, but it would be good to include these. Would also be worth getting @meteorcloudy to confirm that these are the best options to use on Windows, since the Bazel support is the result of his hard work.", "@woodthom2 @mrry It would be great to update the document for Windows.\r\nAnd I can confirm that `-c opt --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc` are the correct and necessary options to use on Windows. Maybe we can also mention [this script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/bazel/common_env.sh) as an example of how to set up environment variables correctly on Windows, since it's a little bit more complicated than Linux.\r\n\r\n", "@mrry @meteorcloudy Also those are only the CPU compiler options on Windows. Could someone confirm the Windows *GPU* compiler options, since the Linux CPU and GPU commands are both provided?", "@woodthom2 Yes, the GPU build command should be \r\n`bazel build -c opt --config=win-cuda --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc tensorflow/tools/pip_package:build_pip_package` after correctly running the `./configure` script for GPU build, an example for this is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/bazel/bazel_test_lib.sh#L172)."]}, {"number": 6794, "title": "Branch 144215355", "body": "Merge conflict in contrib/learn/python/learn/ops/seq2seq_ops.py:\r\nstack/pack (google side) and array_ops_ (github side)\r\nMerged to use array_ops.stack.\r\n", "comments": ["Jenkins, test this please.", "Ignoring timeouts."]}, {"number": 6793, "title": "One hot encoding of words?", "body": "How can I create one hot encoding of words with  each word represented by a sparse vector of vocab size and the index of that particular word equated to 1 ?\r\nsomething like `oneHotEncoding(words = ['a','b','c','d']) ->  [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]`", "comments": ["Thanks for reaching out. We try to keep this issue tracker limited to bugs and feature requests. We recommend posting on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support."]}, {"number": 6792, "title": "fix documentation of center argument in BN layers", "body": "fixes #6736", "comments": ["Can one of the admins verify this patch?"]}, {"number": 6791, "title": "Contrib support on Windows", "body": "# step to reproduce:\r\nattached file (https://www.dropbox.com/s/7yglwzjx4tyoguz/save_restore_model.zip?dl=0)\r\n is my source code\r\nI am testing saver.save and saver.restore function under windows.\r\nworking envirment : Windows R2 Server 2012 x64, \r\npython 3.5 x64, TF 1.0 and master daily build, working with virtualenv\r\nI also tested TF 0.12\r\n\r\n1. download source code save into C:\\work\\tensorflow001 , \r\n2. create folder mnist\r\n3. download files\r\n(t10k-images-idx3-ubyte.gz, \r\nt10k-labels-idx1-ubyte.gz, \r\ntrain-images-idx3-ubyte.gz, \r\ntrain-labels-idx1-ubyte.gz) from http://yann.lecun.com/exdb/mnist/\r\nand save into mist folder(why not using input_data from examples? because that is another bug...I think.)\r\n4. create folder \"model\" for saving values\r\n6. python save_restore_model.py\r\n7. output as fallow\r\n```\r\n(venv643) C:\\work\\tensorflow001>python save_restore_model.py\r\nExtracting mnist\\train-images-idx3-ubyte.gz\r\nExtracting mnist\\train-labels-idx1-ubyte.gz\r\nExtracting mnist\\t10k-images-idx3-ubyte.gz\r\nExtracting mnist\\t10k-labels-idx1-ubyte.gz\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtrem\r\nelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-\r\n\r\n\r\nwin\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\n\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretS\r\ntringToFloat\r\nWARNING:tensorflow:From save_restore_model.py:66: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\nStarting 1st session...\r\nEpoch: 0001 cost= 171.464307973\r\nEpoch: 0002 cost= 43.585798355\r\nEpoch: 0003 cost= 27.564399517\r\nFirst Optimization Finished!\r\nAccuracy: 0.9124\r\nModel saved in file: C:\\work\\tensorflow001\\model\\result\r\nStarting 2nd session...\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\n\r\n\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:993] Out of range: Read fewer bytes than requested\r\nTraceback (most recent call last):  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1\r\n022, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1\r\n004, in _run_fn\r\n    status, run_metadata)\r\n  File \"c:\\python35\\Lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\",\r\n line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested\r\n         [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/t\r\nask:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"save_restore_model.py\", line 113, in <module>\r\n    saver.restore(sess, model_path)\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1\r\n439, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 7\r\n67, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 9\r\n65, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1\r\n015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1\r\n035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested\r\n         [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/t\r\nask:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]\r\n]\r\n\r\nCaused by op 'save/RestoreV2_3', defined at:\r\n  File \"save_restore_model.py\", line 69, in <module>\r\n    saver = tf.train.Saver()\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1\r\n051, in __init__\r\n    self.build()\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1\r\n081, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 6\r\n75, in build\r\n    restore_sequentially, reshape)\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 4\r\n02, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 2\r\n42, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 6\r\n68, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.p\r\ny\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 23\r\n92, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\work\\tensorflow001\\venv643\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 12\r\n64, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nOutOfRangeError (see above for traceback): Read fewer bytes than requested\r\n         [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/t\r\nask:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]\r\n\r\n```\r\n\r\n# expect result\r\n## why fallow message shows up ?\r\n`E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: `\r\n\r\n## most import is when I using saver.restore, it  always show fallowing error message\r\n```\r\nOutOfRangeError (see above for traceback): Read fewer bytes than requested\r\n         [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/t\r\nask:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]\r\n\r\n```\r\n## why I cannot download example automatic by using \r\n```\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"mnist\", one_hot=True)\r\n```\r\n\r\n", "comments": ["Thanks for reaching out and we're sorry to hear about the issue you're facing. Could you please provide a minimal inline code example demonstrating the bug and showing how to reproduce? Downloading a zip file and troubleshooting another app is not a level of support we're able to offer.", "Thanks your response,\r\nfallow is my code:\r\nplease download checkpoint that created by TensorFlow saver\r\nhttps://www.dropbox.com/s/v5xl1dzssi58opf/model.zip?dl=0\r\ncopy it to \"C:\\work\\tensorflow001\\model\\\"\r\nin model will have 5 files, checkpoint, result, result.index.....\r\nwhen TensorFlow restore checkpoint will crash.\r\n```\r\nfrom __future__ import print_function\r\n\r\n# Import MNIST data\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"mnist\", one_hot=True)\r\n\r\nimport tensorflow as tf\r\n\r\n# Parameters\r\nlearning_rate = 0.001\r\nbatch_size = 100\r\ndisplay_step = 1\r\nmodel_path = u\"C:\\\\work\\\\tensorflow001\\\\model\\\\result\"\r\n\r\n# Initializing the variables\r\ninit = tf.initialize_all_variables()\r\n\r\n# 'Saver' op to save and restore all the variables\r\nsaver = tf.train.Saver()\r\n\r\n# Running a new session\r\nprint(\"Starting 2nd session...\")\r\n\r\nwith tf.Session() as sess:\r\n    # Initialize variables\r\n    sess.run(init)\r\n\r\n    # Restore model weights from previously saved model\r\n    saver.restore(sess, model_path)\r\n\r\n```\r\n", "Hmm if you insert a line into your script to trigger the lazy loading of the contrib library, e.g. `_ = tf.contrib.tensor_forest` then does it work?", "thanks your response, \r\nI am insert that line of code, please check it out, \r\nif that insert to right place, the result still crash.\r\n\r\n```\r\nfrom __future__ import print_function\r\n\r\n# Import MNIST data\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"mnist\", one_hot=True)\r\n\r\nimport tensorflow as tf\r\n\r\n# Parameters\r\nlearning_rate = 0.001\r\nbatch_size = 100\r\ndisplay_step = 1\r\nmodel_path = u\"C:\\\\work\\\\tensorflow01\\\\model\\\\result\"\r\n\r\n# Initializing the variables\r\ninit = tf.initialize_all_variables()\r\n\r\n# 'Saver' op to save and restore all the variables\r\nsaver = tf.train.Saver()\r\n\r\n# Running a new session\r\nprint(\"Starting 2nd session...\")\r\n_ = tf.contrib.tensor_forest\r\nwith tf.Session() as sess:\r\n    # Initialize variables\r\n    sess.run(init)\r\n    \r\n    # Restore model weights from previously saved model\r\n    saver.restore(sess, model_path)\r\n```", "Can you put that line right after the `import tensorflow as tf` line? I ask because your stack trace indicates the `saver = ...` line is what crashes, and the code above inserted it after that.\r\n\r\nI'm pretty confident that my suggestion will fix your problem. This is the result of a known issue we're planning to fix eventually, where the build graph and op loading needs to be smarter about loading libraries. It's a hard problem to solve.\r\n\r\nIf your issue is not solved by my suggestion, let me know and I'll re-open.", "Thanks your answer, but it not work, still crash\r\nthe fallow is my code:\r\ncan you help me to verify, may I miss understand your meaning, insert the code to wrong place.\r\n```\r\nfrom __future__ import print_function\r\n\r\n# Import MNIST data\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"mnist\", one_hot=True)\r\n\r\nimport tensorflow as tf\r\n\r\n# Parameters\r\nlearning_rate = 0.001\r\nbatch_size = 100\r\ndisplay_step = 1\r\nmodel_path = u\"C:\\\\work\\\\tensorflow01\\\\model\\\\result\"\r\n\r\n# Initializing the variables\r\ninit = tf.initialize_all_variables()\r\n\r\n# 'Saver' op to save and restore all the variables\r\nsaver = tf.train.Saver()\r\n\r\n# Running a new session\r\nprint(\"Starting 2nd session...\")\r\n_ = tf.contrib.tensor_forest  #here is that code\r\nwith tf.Session() as sess:\r\n    # Initialize variables\r\n    sess.run(init)\r\n    \r\n    # Restore model weights from previously saved model\r\n    saver.restore(sess, model_path)\r\n```", "Can you try this?\r\n\r\n```\r\nfrom __future__ import print_function\r\n\r\n# Import MNIST data\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"mnist\", one_hot=True)\r\n\r\nimport tensorflow as tf\r\n_ = tf.contrib.tensor_forest  #here is that code\r\n\r\n# Parameters\r\nlearning_rate = 0.001\r\nbatch_size = 100\r\ndisplay_step = 1\r\nmodel_path = u\"C:\\\\work\\\\tensorflow01\\\\model\\\\result\"\r\n\r\n# Initializing the variables\r\ninit = tf.initialize_all_variables()\r\n\r\n# 'Saver' op to save and restore all the variables\r\nsaver = tf.train.Saver()\r\n\r\n# Running a new session\r\nprint(\"Starting 2nd session...\")\r\nwith tf.Session() as sess:\r\n    # Initialize variables\r\n    sess.run(init)\r\n    \r\n    # Restore model weights from previously saved model\r\n    saver.restore(sess, model_path)\r\n```", "It still crash\r\ndo you have any idea ?\r\n\r\n```\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel\r\n.cc:975] Out of range: Read fewer bytes than requested\r\nTraceback (most recent call last):\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 102\r\n1, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 100\r\n3, in _run_fn\r\n    status, run_metadata)\r\n  File \"c:\\program files\\anaconda3\\Lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", l\r\nine 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/tas\r\nk:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"save_restore_model.py\", line 114, in <module>\r\n    saver.restore(sess, model_path)\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 138\r\n8, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 766\r\n, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 964\r\n, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 101\r\n4, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 103\r\n4, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/tas\r\nk:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nCaused by op 'save/RestoreV2', defined at:\r\n  File \"save_restore_model.py\", line 69, in <module>\r\n    saver = tf.train.Saver()\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 100\r\n0, in __init__\r\n    self.build()\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 103\r\n0, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 624\r\n, in build\r\n    restore_sequentially, reshape)\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 361\r\n, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 200\r\n, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 441\r\n, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\"\r\n, line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2240\r\n, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\work\\tensorflow01\\venv64\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1128\r\n, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nOutOfRangeError (see above for traceback): Read fewer bytes than requested\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/tas\r\nk:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n```", "Oh now the problem seems obvious to me. Windows support is still brand new. We haven't had a chance yet to make contrib Windows compatible. It's only possible to use the core TensorFlow codebase right now. See: https://github.com/tensorflow/tensorflow/blob/d5062e8/tensorflow/python/BUILD#L81 This means it's not possible at the moment to load a model on Windows that uses ops from contrib.\r\n\r\nSince I don't see an issue tracking this, I'm going to re-open and rename this one.", "@jart Can you explain what this issue has to do with `tf.contrib`? I don't see any reference to `tf.contrib` in the original code, and&mdash;while you're right that the contrib libraries aren't fully supported on Windows&mdash;I can't see how that's related to @alarmz's original problem.", "In the original post, the following error message is cited:\r\n\r\n> **why fallow message shows up ?**\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op:\r\n\r\nThat op is defined in tensorflow/contrib/tensor_forest/core/ops/best_splits_op.cc.", "Ah I see. We should definitely fix this so that TensorForest works on Windows, but I think this log message (which should really be [a warning or a fatal error](https://github.com/tensorflow/tensorflow/blob/99fe61a8a8f3dd41b4e1e4dedfc53b45f67e88a7/tensorflow/core/framework/op_kernel.cc#L943)) is a red herring. It looks like the real problem @alarmz is facing is in `tf.train.Saver` code and/or its related ops. Perhaps @concretevitamin has seen this before?", "(PR #6908 should fix the error message and make much of `tf.contrib.tensor_forest` work on Windows.)", "@alarmz can you confirm that `tf.contrib.tensor_forest` works after that PR?", "Closing this issue, since `tf.contrib.tensor_forest` has (as far as I know) been working with tests passing on Windows for some time now. Feel free to open a new issue if there are other parts of `tf.contrib` for which you'd like to see us prioritize support!"]}, {"number": 6790, "title": "Only Master worker do predict when using estimator.Estimator.predict", "body": "I run distributed training via a estimator.Estimator.The model is save at hdfs with multiple files.\r\nWhen I use this model to predict, only the master worker do prediction while other workers do nothing,\r\njust stop at the beginning of the for loop.\r\n\r\nclassifier = tf.contrib.learn.estimator.Estimator(\r\n        model_fn=l2m_model_fn,\r\n        params=model_params,\r\n        config=run_config,\r\n        model_dir=FLAGS.ckp_dir\r\n    )\r\n\r\npredictions = classifier.predict(\r\n        input_fn=lambda: l2m_input_fn2(args),\r\n        as_iterable=True\r\n)\r\n\r\nfor i, p in enumerate(predictions):\r\n        #do sth with p", "comments": ["Thanks for reaching out. I'm responsible for triaging bugs today. I'm not deeply familiar with the TensorFlow distributed runtime. Could you please help me understand why this is a bug and not a configuration issue?", "@ispirmustafa -- do Estimators distribute their work over multiple workers?", "I don't see a reason of estimator.predict hanging. Is your input_fn same for all workers?", "Could you please try your code with the head?", "Closing due to lack of activity.  Please reopen if necessary."]}, {"number": 6789, "title": "TensorBoard: Improve handling of tall images", "body": "**[Example of how it looks today](https://cloud.githubusercontent.com/assets/1595907/21849403/e4a3a9e4-d805-11e6-9210-705cff983b4a.png), and [an example of how it would look without stretching](https://cloud.githubusercontent.com/assets/1595907/21849399/da03e30a-d805-11e6-80f4-02f2a44417b9.png).**\r\n\r\nTensorBoard's CSS for image summaries stretches the image to 100% which makes it hard to interpret images, especially with pooling operations like in [VGG-like neural networks](https://www.cs.toronto.edu/~frossard/post/vgg16/).\r\n\r\nAlso, considering datasets as [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html) have images of 32x32, the large amount of stretching leads to very blurry images. Could we either have control of the number of columns in a pane, or disable image stretching altogether?", "comments": ["What browser are you using? What happens if you tune the interpolation algorithm in the CSS? Those screenshots look like nearest neighbor, and it does kind of feel like it's losing something when resized.\r\n\r\nDisabling image stretching is probably not something we would consider. Perhaps instead we should provide a choice of image interpolation algorithms.\r\n\r\ncc: @dandelionmane ", "The latest version of TensorBoard has a button attached to every image that lets you view it as true pixel size. Will that solve the need for you?\r\n\r\nAlso, +1 for Justine's question about which browser you're using. On chrome by default it doesn't interpolate, just increases size, so it will become blocky rather than blurry. Maybe we don't have the right CSS setting for your browser, though.", "Yes, screenshots are from Chrome.\r\n\r\nThe button to show the actual size is nice, but IMO it would be better if the stretching didn't occur, or at least was controlled with `max-height: 50vh` or something because `width: 100%` really goes bonkers with `height: auto`. For example, [this slice of a spectrogram](https://cloud.githubusercontent.com/assets/1595907/21936675/62e87a70-d9b2-11e6-8040-93af926bb6bd.png) stretches from 62x672 to 340x3685 pixels on my 1080p display in Chrome.\r\n\r\nCould we at least control the number of columns or something with a slider?", "I retract my previous comment about stretching algorithm. I failed to notice the two screenshots were at different steps, which explains why they looked so different.\r\n\r\nIt's been really difficult for us to find a way to make the images dashboard fit all use cases. For example, the mnist tutorial has really tiny images, so we'd naturally want to stretch those.\r\n\r\nYour suggestion of vh is actually a really interesting idea that we hadn't considered. Especially if we can make the number of card columns increase when `max-height: 100vh` kicks in. However this could potentially make the card title less useful in your case, since it's overflow hidden.", "3f6404f2069a1baf795cc618e700dd43dcbeeb15 limits the default max height of images to 800px. One can always view true pixel size (the full image) if they desire, so I am closing this issue."]}, {"number": 6788, "title": "Tf-slim: Unable to read dataset using slim.data_set_provider.DataSetProvider when images are not in JPEG.", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nCouldn't find relevant threads as there aren't many tf-slim questions.\r\n\r\n### Environment info\r\nOperating System:\r\n**Ubuntu 16.04**\r\n\r\nInstalled version of CUDA and cuDNN: \r\n**8.00**\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n**0.11**\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nBasically I have directory of subdirectories, with each subdirectory containing png images of a certain class. Editing the tf-slim download_and_convert_flowers.py to suit my images, I created a set of tfrecord files (train and validation both included) and stored it in a directory.\r\n\r\nFollowing which, I used the 'get_split' function from dataset_utils in https://github.com/tensorflow/models/blob/master/slim/datasets/dataset_utils.py\r\nto create a DataSet class from reading the tfrecord files of a certain type (either train or validation. I indicated 'train'). So now I have a DataSet class to read.\r\n\r\nThe problem comes when I try to use a batch loading function to actually start reading the DataSet object for extracting images and creating a batch:\r\n\r\n```\r\ndef load_batch(dataset, batch_size=32, height=299, width=299, is_training=False):\r\n    \"\"\"Loads a single batch of data.\r\n    \r\n    Args:\r\n      dataset: The dataset to load.\r\n      batch_size: The number of images in the batch.\r\n      height: The size of each image after preprocessing.\r\n      width: The size of each image after preprocessing.\r\n      is_training: Whether or not we're currently training or evaluating.\r\n    \r\n    Returns:\r\n      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\r\n      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\r\n      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\r\n    \"\"\"\r\n    data_provider = slim.dataset_data_provider.DatasetDataProvider(\r\n        dataset)\r\n        # common_queue_capacity=32)\r\n        # common_queue_min=8)\r\n    image_raw, label = data_provider.get(['image', 'label'])\r\n    \r\n    # Preprocess image for usage by Inception.\r\n    image = inception_preprocessing.preprocess_image(image_raw, height, width, is_training=is_training)\r\n    \r\n    # Preprocess the image for display purposes.\r\n    image_raw = tf.expand_dims(image_raw, 0)\r\n    image_raw = tf.image.resize_images(image_raw, [height, width])\r\n    image_raw = tf.squeeze(image_raw)\r\n\r\n    # Batch it up.\r\n    images, images_raw, labels = tf.train.batch(\r\n          [image, image_raw, label],\r\n          batch_size=batch_size,\r\n          num_threads=1,\r\n          capacity=2 * batch_size)\r\n    \r\n    return images, images_raw, labels\r\n```\r\nThe problem comes from `slim.dataset_data_provider.DatasetDataProvider` as I couldn't read the dataset at all.\r\n\r\nHere is my error traceback:\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \r\nname: GeForce GTX 860M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.0195\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.60GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 1898 get requests, put_count=1100 evicted_count=1000 eviction_rate=0.909091 and unsatisfied allocation rate=1\r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110\r\nINFO:tensorflow:Starting Session.\r\nINFO:tensorflow:Starting Queues.\r\nINFO:tensorflow:global_step/sec: 0\r\nNot a JPEG file: starts with 0x89 0x50\r\nNot a JPEG file: starts with 0x89 0x50\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors.InvalidArgumentError'>, Invalid JPEG data, size 26498\r\n\t [[Node: case/If_0/DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](case/If_0/DecodeJpeg/Switch:1, ^case/Assert/AssertGuard/Merge/_7825)]]\r\n\r\nCaused by op u'case/If_0/DecodeJpeg', defined at:\r\n  File \"code.py\", line 148, in <module>\r\n    images, images_raw, labels = load_batch(dataset, height = image_size, batch_size = batch_size, width = image_size, is_training = True)\r\n  File \"code.py\", line 27, in load_batch\r\n    dataset)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py\", line 78, in __init__\r\n    tensors = dataset.decoder.decode(data, items)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\", line 398, in decode\r\n    outputs.append(handler.tensors_to_item(keys_to_tensors))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\", line 297, in tensors_to_item\r\n    image = self._decode(image_buffer, image_format)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\", line 324, in _decode\r\n    }, default=decode_jpg, exclusive=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2886, in case\r\n    case_seq = _build_case()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2868, in _build_case\r\n    name=\"If_%d\" % i)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1710, in cond\r\n    orig_res, res_t = context_t.BuildCondBranch(fn1)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1613, in BuildCondBranch\r\n    r = fn()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\", line 317, in decode_jpg\r\n    return image_ops.decode_jpeg(image_buffer, self._channels)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_image_ops.py\", line 283, in decode_jpeg\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Invalid JPEG data, size 26498\r\n\t [[Node: case/If_0/DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](case/If_0/DecodeJpeg/Switch:1, ^case/Assert/AssertGuard/Merge/_7825)]]\r\n\r\nW tensorflow/core/framework/op_kernel.cc:968] Out of range: FIFOQueue '_0_batch/fifo_queue' is closed and has insufficient elements (requested 3, current size 0)\r\n\t [[Node: batch = QueueDequeueMany[_class=[\"loc:@batch/fifo_queue\"], component_types=[DT_FLOAT, DT_FLOAT, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]\r\nW tensorflow/core/framework/op_kernel.cc:968] Out of range: FIFOQueue '_0_batch/fifo_queue' is closed and has insufficient elements (requested 3, current size 0)\r\n\t [[Node: batch = QueueDequeueMany[_class=[\"loc:@batch/fifo_queue\"], component_types=[DT_FLOAT, DT_FLOAT, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]\r\nTraceback (most recent call last):\r\n  File \"code.py\", line 171, in <module>\r\n    number_of_steps = 10)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 780, in train\r\n    raise\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 35, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 969, in managed_session\r\n    self.stop(close_summary_writer=close_summary_writer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 797, in stop\r\n    stop_grace_period_secs=self._stop_grace_secs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner.py\", line 225, in _run\r\n    sess.run(enqueue_op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 915, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 985, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors.InvalidArgumentError: Invalid JPEG data, size 26498\r\n\t [[Node: case/If_0/DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](case/If_0/DecodeJpeg/Switch:1, ^case/Assert/AssertGuard/Merge/_7825)]]\r\n\r\nCaused by op u'case/If_0/DecodeJpeg', defined at:\r\n  File \"code.py\", line 148, in <module>\r\n    images, images_raw, labels = load_batch(dataset, height = image_size, batch_size = batch_size, width = image_size, is_training = True)\r\n  File \"code.py\", line 27, in load_batch\r\n    dataset)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py\", line 78, in __init__\r\n    tensors = dataset.decoder.decode(data, items)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\", line 398, in decode\r\n    outputs.append(handler.tensors_to_item(keys_to_tensors))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\", line 297, in tensors_to_item\r\n    image = self._decode(image_buffer, image_format)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\", line 324, in _decode\r\n    }, default=decode_jpg, exclusive=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2886, in case\r\n    case_seq = _build_case()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2868, in _build_case\r\n    name=\"If_%d\" % i)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1710, in cond\r\n    orig_res, res_t = context_t.BuildCondBranch(fn1)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1613, in BuildCondBranch\r\n    r = fn()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\", line 317, in decode_jpg\r\n    return image_ops.decode_jpeg(image_buffer, self._channels)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_image_ops.py\", line 283, in decode_jpeg\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Invalid JPEG data, size 26498\r\n\t [[Node: case/If_0/DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](case/If_0/DecodeJpeg/Switch:1, ^case/Assert/AssertGuard/Merge/_7825)]]\r\n```\r\nUpon inspection, I have checked that a likely error comes from the line ```  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\", line 297, in tensors_to_item\r\n    image = self._decode(image_buffer, image_format)```\r\n\r\nin tfexample_decoder.py, the image_format, if not indicated, will be a JPEG image by default unless there are 4 channels (RGBA) in the images. But since my images are grayscale, a JPEG decoder is used by default. Yet, I have no way to specify the image format as 'png' specifically unless the source code is changed. How should I go about this, or am I mistaken about the error I have arrived at?\r\n\r\nI have previously successfully run the code (many of which are referenced from the tf-slim walkthrough ipynb file), but the images I used were in jpeg. \r\n\r\nAny help is very much appreciated. Thank you for your time.", "comments": ["@kwotsin would you be able to do what you want to do just by using the core (i.e. non-contrib) APIs?\r\n\r\n@sguada This issue makes a compelling case that there's an API design issue in contrib/slim that prevents the user from loading grayscale PNG files. It's a niche use case, but looks like something that ought to be fixed, if you have time to help our friend.", "tf-slim.data can decode jpeg, png or raw see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py#L269)\r\n\r\nThe only caveat is that it needs to know that the encoded image has png format when creating the [tf-example](https://github.com/tensorflow/models/blob/master/slim/datasets/dataset_utils.py#L56). \r\nWhen you created the dataset did you specify the image was a png?  So for instance the flowers dataset assumes that the images are [jpg](https://github.com/tensorflow/models/blob/master/slim/datasets/download_and_convert_flowers.py#L145)", "@sguada Yes I have changed the code to read the images in png format before I read them using the slim API, however I can't seem to find a way to get tf-slim to know the files are encoded in png.\r\n\r\n@jart I am figuring out how to do this in pure TF, but it seems I might have to rewrite a custom function for reading the dataset or simply read from disk the images (might be much slower). But for the time being I have bypassed the problem by trying out jpeg files first. ", "Just change 'jpg' to 'png' in this line when the file is 'png' \r\nhttps://github.com/tensorflow/models/blob/master/slim/datasets/download_and_convert_flowers.py#L146 \r\nThat way it would store the image format and it would be able to decode it.", "@kwotsin , based on @sguada 's reply above, it seems that tf-slim does support png as you want and the problem was with the sample `download_and_convert_flowers.py` script.\r\n\r\nClosing this out, though feel free to reopen if I misunderstood.\r\n"]}, {"number": 6787, "title": "could not find cuDevicePrimaryCtxSetFlags in libcuda DSO; dlerror: /usr/lib/libcuda.so.1: undefined symbol: cuDevicePrimaryCtxSetFlags", "body": "Hi, when trying to get tensorflow on my Nvidia GPU, the output started with the following:\r\n\r\n    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\n    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\n    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\n    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\n    I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n    F tensorflow/stream_executor/cuda/cuda_driver.cc:94] Check failed: s.ok() could not find cuDevicePrimaryCtxSetFlags in libcuda DSO; dlerror: /usr/lib/libcuda.so.1: undefined symbol: cuDevicePrimaryCtxSetFlags\r\nAfter which, it exits.\r\n\r\nI'm running with the following:\r\n\r\n`uname -r`:\r\n    \r\n    4.8.13-1-ARCH\r\n\r\n\r\n`lspci`:\r\n    \r\n    VGA compatible controller: NVIDIA Corporation GM107 [GeForce GTX 750] (rev a2)\r\n    Subsystem: Gigabyte Technology Co., Ltd Device 362e\r\n    Kernel driver in use: nvidia\r\n\r\n`cat /proc/driver/nvidia/version`:\r\n\r\n    NVRM version: NVIDIA UNIX x86_64 Kernel Module  340.101  Thu Dec  1 15:52:31 PST 2016\r\n    GCC version:  gcc version 6.2.1 20160830 (GCC)\r\n\r\n`ls -l /usr/lib/libcuda*`:\r\n\r\n    lrwxrwxrwx 1 root root       12 Dec 16 01:18 /usr/lib/libcuda.so -> libcuda.so.1*\r\n    lrwxrwxrwx 1 root root       18 Dec 16 01:18 /usr/lib/libcuda.so.1 -> libcuda.so.340.101*\r\n    -rwxr-xr-x 1 root root 14011752 Dec 16 01:18 /usr/lib/libcuda.so.340.101*\r\n\r\nTensorflow version:\r\n    \r\n    0.12.1\r\n\r\n**Note:** I _am_ running tensorflow on Python 3.6 having changed 'cp35' to 'cp36' in the file name of the binary, but I presume that this does not matter considering that this is a cuda issue, and that the CPU version worked fine.\r\n\r\nThe line that the error references is this one:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_driver.cc#L94\r\n\r\nWhats going on here, and what can I do to fix this?", "comments": ["I installed the `nvidia`,`nvidia-libgl` and `nvidia-utils` packages instead of their `nvidia-340xx` equivalents, which did the trick.", "Hi,\r\n\r\nI am getting this error when i run TF1.0.1 on Windows with GPU support.\r\nI installed CUDA version 8.0 and CuDNN version 5.1\r\n\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_driver.cc:94] Check failed: s.ok() could not find cuDevice\r\nPrimaryCtxSetFlags in libcuda DSO; dlerror: cuDevicePrimaryCtxSetFlags not found\r\n\r\nWhat should i do to fix this?\r\n", "Same here on a K520 (AWS g2 instance) with Ubuntu 16.04, TensorFlow 1.0.1, CUDA 8.0, cuDNN 5.1.\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nF tensorflow/stream_executor/cuda/cuda_driver.cc:94] Check failed: s.ok() could not find cuDevicePrimaryCtxSetFlags in libcuda DSO; dlerror: /usr/lib/x86_64-linux-gnu/libcuda.so.1: undefined symbol: cuDevicePrimaryCtxSetFlags\r\nAborted (core dumped)\r\n```", "Exchanging the NVIDIA Display Driver from Ubuntu's repo (340.102) to the latest one from nvidia.com (367.57) solved it.\r\n\r\n**Tech support 101:**\r\n1. Reboot.\r\n2. Update drivers.\r\n3. Does it work now?\r\n:panda_face: "]}, {"number": 6786, "title": "Variable \"matching_filenames\" @  tf.train.match_filenames_once() should be local?", "body": "This var holds the matched file names for the input stream pipeline.\r\nSince it is added to the graph, when using the contractor of tf.train.Saver() it is saved\r\nas part of the checkpoint state.\r\nHence loading saved models from checkpoints for inference, the training file names are loaded instead of the val / inference files.\r\n\r\nShouldn't this Var become a local variable? will it not be saved by default treated as local?\r\n", "comments": ["Would it be possible for you to use `tf.matching_files` instead?", "@jart , I will give it a try, if this is the recommended method, the documentation at\r\nhttps://www.tensorflow.org/how_tos/reading_data/ should be changed.\r\n", "If I understand correctly, the goal of match_filenames_once is that it delegates to matching_files and then saves the result to checkpoints, so the operation only happens once. The documentation doesn't appear to be making any recommendations, so I'm not sure what is actionable here for  TensorFlow. If you believe there is a bug in the software or documentation, let me know and I'll re-open this issue.", "There is not a bug, but it may cause undesired and unnoticed behavior in the following scenario:\r\n1. Use \"match_filenames_once\" to feed data in the training phase.\r\n2. Save model checkpoints with default 'tf.train.Saver()'\r\n\r\n3. Use again 'match_filenames_once' with a different pattern to load the inference / test files.\r\n4. Load previously saved model checkpoint for inference / test.\r\n\r\nIn this scenario, at step #4  the training files and not the test files will be loaded, since the old file list was loaded from the checkpoint.\r\n\r\nMaybe a comment clarifying the issue should be added to the documentation.\r\n\r\n ", "@dr4b Do you feel we should add further clarity to our documentation on loading models as @keotic is suggesting?", "Will this change in the API any time soon?  And if not, sure, go ahead and add a comment?", "That seems like a footgun, a typical user might not expect `match_filenames_once` to be affected by checkpoint. A similar issue existed checkpoint affecting state of input pipelines that were affected by `epochs` variables, this was solved by making them local, as here\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/4c85a95925a68fec324f70cd0d7f3d4548f97a38", "@keotic Is using your following steps right now? Will the correct queue be used at test time? \r\n\r\n> \r\n> Use \"match_filenames_once\" to feed data in the training phase.\r\n> Save model checkpoints with default 'tf.train.Saver()'\r\n> Use again 'match_filenames_once' with a different pattern to load the inference / test files.\r\n> Load previously saved model checkpoint for inference / test.\r\n\r\n", "@jiqiujia Should be OK if you are using latest versions of TF"]}, {"number": 6785, "title": "Bug with TensorFlow Serving in TensorFlow 0.12", "body": "I have found the following code works fine when using TensorFlow 0.11, installed using:\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl\r\n\r\nHowever when using TensorFlow 0.12 installed using:\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl\r\n\r\nI get the following issue when loading the model:\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 960M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.176\r\npciBusID 0000:02:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.92GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:02:00.0)\r\nTraceback (most recent call last):\r\n  File \"./demo2.py\", line 101, in <module>\r\n    outp = sess.run(out, feed_dict={inp: [v]})[0]\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value convolution2d_1_W_1\r\n\t [[Node: convolution2d_1_W_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@convolution2d_1_W_1\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](convolution2d_1_W_1)]]\r\n\t [[Node: Reshape_9/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2_Reshape_9\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'convolution2d_1_W_1/read', defined at:\r\n  File \"./demo2.py\", line 32, in <module>\r\n    sess,inp,out,classes = load_graph(\"/tmp/cnn/00000001/\")\r\n  File \"./demo2.py\", line 18, in load_graph\r\n    sess, meta_graph_def = session_bundle.load_session_bundle_from_path(output_graph_path)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/session_bundle/session_bundle.py\", line 95, in load_session_bundle_from_path\r\n    saver = tf.train.import_meta_graph(meta_graph_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1526, in import_meta_graph\r\n    **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 502, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 285, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value convolution2d_1_W_1\r\n\t [[Node: convolution2d_1_W_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@convolution2d_1_W_1\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](convolution2d_1_W_1)]]\r\n\t [[Node: Reshape_9/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2_Reshape_9\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\nCreate model:\r\n\r\n```\r\n#!/usr/bin/env python2\r\n\r\nfrom __future__ import division, print_function, absolute_import\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport keras\r\nfrom tensorflow.contrib.session_bundle import exporter\r\n\r\nfrom keras.optimizers  import Adam\r\nfrom keras.constraints import MaxNorm\r\n\r\nimport keras.models as models\r\nfrom keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten\r\nfrom keras.layers.noise import GaussianNoise\r\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\r\nfrom keras.regularizers import *\r\nfrom keras.optimizers import adam\r\n\r\n\r\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape, Merge\r\nfrom keras import backend as K\r\nfrom keras.callbacks import TensorBoard\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.regularizers import l2\r\n\r\n\r\nmods = [\"psk\",\"fm\"]\r\n\r\nsess = tf.Session()\r\n\r\ninit_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables())\r\n\r\nK.set_session(sess)\r\nK.set_learning_phase(1)\r\n\r\nclasses = mods\r\n\r\nX_train = np.array([[[  1.23725709e-04,   3.21811251e-03,   8.11603665e-03,  -3.23975401e-04,\r\n    1.07787224e-02,   2.94970046e-03,   4.02887911e-03,  -1.88605103e-03,\r\n    1.07172830e-03,  -4.49557183e-03,   4.48013563e-03,  -4.36049374e-03,\r\n    2.03782506e-03,   3.55514325e-03,  -6.66785333e-03,   1.42298348e-03,\r\n   -1.50406240e-02,  -2.57269153e-03,   5.56691317e-03,   1.13693373e-02,\r\n    2.71817949e-03,   5.25468131e-05,  -3.36736324e-03,  -9.75306786e-04,\r\n    3.72400926e-03,   1.18668824e-02,  -1.42240804e-03,   1.44388294e-02,\r\n    8.27464042e-04,   6.72260066e-03,  -5.76557242e-04,   1.67079712e-03,\r\n   -1.02783879e-02,  -6.51435228e-04,  -1.59728453e-02,  -2.92223319e-03,\r\n   -9.65267327e-03,   8.49308504e-04,  -4.86527057e-03,  -2.24598357e-03,\r\n   -6.85441250e-04,  -7.86077511e-03,   8.25393759e-03,  -8.98091588e-03,\r\n   -4.13634349e-03,   5.30316820e-03,  -2.96568638e-03,   7.47767324e-03,\r\n    7.87805114e-03,   3.33608547e-03,   6.81637728e-04,  -3.26122390e-03,\r\n   -5.83499251e-03,   6.05301978e-03,   3.19009693e-03,  -4.02274629e-04,\r\n   -1.09375454e-02,   1.74824963e-03,   1.75286271e-03,  -1.26824277e-02,\r\n    7.39318645e-03,   3.90595663e-03,   1.98931666e-03,   6.12980360e-03,\r\n   -1.79607305e-03,   1.40197761e-02,  -9.81968828e-03,   5.02704596e-03,\r\n    3.01699433e-03,  -6.52436400e-03,  -1.62929588e-03,   7.39145232e-03,\r\n   -9.01302416e-03,   3.07239546e-03,  -1.82828668e-03,  -5.16610499e-03,\r\n    2.61074631e-03,  -1.33442272e-05,   5.14040841e-03,  -6.64286781e-03,\r\n    1.71982939e-03,  -4.85043926e-03,  -3.97882238e-03,  -1.37700920e-03,\r\n   -1.10943802e-02,   2.20915396e-03,  -3.39583290e-04,   4.58237901e-03,\r\n    4.64649638e-03,  -2.94209481e-03,  -1.55386878e-02,   5.68915205e-03,\r\n   -5.80187945e-04,  -5.83021576e-03,  -3.40874423e-04,  -1.83014176e-03,\r\n    3.61575768e-03,  -8.90286360e-03,  -4.51745838e-03,   1.93125161e-03,\r\n   -8.45910795e-03,  -5.77241089e-03,   8.37801304e-03,  -1.38715087e-02,\r\n   -3.31607228e-03,   5.12827048e-03,   9.08580422e-03,   1.84341776e-03,\r\n   -8.04373343e-03,  -9.29000136e-03,  -9.47526656e-04,   4.20172885e-03,\r\n   -5.26063796e-03,  -6.72675669e-03,  -3.59727233e-03,   4.08909051e-03,\r\n    7.33140949e-03,  -7.76879140e-04,  -2.54971418e-03,  -8.70507117e-03,\r\n    7.90149346e-03,   7.96920154e-03,   2.96101277e-03,  -5.38653834e-03,\r\n   -4.88629332e-04,   1.00093251e-02,  -4.25783452e-03,  -5.71854087e-03],\r\n [ -6.87929394e-04,   4.60408907e-03,   7.26573868e-04,   4.26992076e-03,\r\n   -7.02272868e-03,   2.22673942e-03,   1.22035667e-02,  -8.09120014e-03,\r\n    1.39266049e-04,  -1.15088280e-02,   5.04087773e-04,  -2.15286622e-03,\r\n   -7.33058015e-03,  -9.15534515e-03,  -5.08288946e-03,  -1.30671002e-02,\r\n    1.47830602e-03,   1.16440572e-03,   4.02440550e-03,   8.58596340e-03,\r\n    3.03325080e-03,  -2.05237372e-03,   1.05325170e-02,  -1.80078077e-03,\r\n    4.63060196e-03,   1.23807620e-02,  -6.47541787e-03,  -4.13759379e-03,\r\n   -2.83148466e-03,   7.43190618e-03,  -1.15842454e-03,   6.59148069e-03,\r\n    5.59045048e-03,   3.75851267e-03,  -3.95106524e-03,  -2.56526005e-03,\r\n    6.27654884e-03,  -1.24440319e-03,   3.46388144e-04,  -1.55166397e-03,\r\n    1.04056811e-02,   1.30844014e-02,  -6.36276463e-03,  -6.97820855e-04,\r\n   -3.15165240e-03,  -1.41060480e-03,   1.38492498e-03,   8.64384789e-03,\r\n   -7.11268140e-03,  -1.76842324e-03,  -1.25329485e-02,  -4.83873859e-03,\r\n   -5.18619781e-03,   1.30472714e-02,  -5.54988487e-03,   8.61867797e-03,\r\n   -3.99610912e-03,   6.70848880e-04,  -9.35312640e-03,   1.23843951e-02,\r\n   -3.27547453e-03,   4.86938097e-03,  -2.92926189e-03,   2.20531784e-03,\r\n    4.75586858e-03,   3.00767994e-03,   7.01231230e-03,   1.93257479e-03,\r\n    5.04882913e-03,  -1.04642799e-02,   7.11998856e-03,  -2.53466447e-03,\r\n    1.29708648e-03,   1.07713938e-02,  -2.99122441e-03,   5.51079051e-04,\r\n    5.26238093e-03,  -8.22351780e-04,   5.74991386e-03,   8.99204868e-04,\r\n    1.13037638e-02,  -1.46015978e-03,   6.79054251e-03,  -2.91314325e-03,\r\n    6.34925021e-03,   4.45276871e-03,   7.88977742e-03,  -5.24963858e-03,\r\n   -2.57161981e-03,   5.67252794e-03,  -2.77268351e-03,   2.27351789e-03,\r\n    3.13360780e-03,   9.63459164e-03,   3.79459164e-03,   2.40193959e-03,\r\n    3.09617817e-03,   5.46766398e-03,  -1.21412217e-03,  -7.90829584e-03,\r\n   -1.29530125e-03,  -7.43942289e-03,   3.87186417e-03,  -8.09667457e-04,\r\n    1.91524532e-03,  -3.64716118e-03,   8.53588711e-03,  -5.08366944e-03,\r\n    1.74835534e-03,   7.45685189e-04,  -4.69580526e-03,  -1.06729409e-02,\r\n   -4.90031298e-03,  -2.10527773e-03,  -1.65928528e-02,  -9.58569441e-03,\r\n    4.94536944e-03,   6.38392800e-03,  -1.75752665e-03,  -2.40226928e-03,\r\n    4.23104968e-03,   4.33860486e-03,   4.77843359e-03,   4.94898483e-03,\r\n    8.75659316e-05,   6.20994205e-03,   1.03254039e-02,   1.75160269e-04]]])\r\n\r\nY_train = np.array([[1.,0.]])\r\n#X_test = test_i\r\n#Y_test = test_o\r\nin_shp = list(X_train.shape[1:])\r\n\r\ndr = 0.5 # dropout rate (%)\r\nmodel = models.Sequential()\r\nmodel.add(Reshape([1]+in_shp, input_shape=in_shp))\r\nmodel.add(ZeroPadding2D((0, 2)))\r\nmodel.add(Convolution2D(256, 1, 3, border_mode='valid', activation=\"relu\", init='glorot_uniform'))\r\nmodel.add(Dropout(dr))\r\nmodel.add(ZeroPadding2D((0, 2)))\r\nmodel.add(Convolution2D(80, 2, 3, border_mode=\"valid\", activation=\"relu\",  init='glorot_uniform'))\r\nmodel.add(Dropout(dr))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(256, activation='relu', init='he_normal'))\r\nmodel.add(Dropout(dr))\r\nmodel.add(Dense( len(classes), init='he_normal' ))\r\nmodel.add(Activation('softmax',name=\"out\"))\r\nmodel.add(Reshape([len(classes)]))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\r\nmodel.summary()\r\n\r\nbatch_size=1\r\nnb_epoch = 1\r\n\r\nsess.run([init_op])\r\n\r\n\r\nhistory = model.fit(X_train,Y_train,batch_size=batch_size,nb_epoch=nb_epoch,show_accuracy=False,verbose=2,callbacks = [])\r\n\r\nK.set_learning_phase(0)\r\n\r\nconfig = model.get_config()\r\nweights = model.get_weights()\r\n\r\nnew_model = models.Sequential.from_config(config)\r\nnew_model.set_weights(weights)\r\n\r\nexport_path = \"/tmp/cnn\"\r\nexport_version = 1\r\n\r\nlabels_tensor = tf.constant(mods)\r\n\r\nsaver = tf.train.Saver(sharded=True)\r\nmodel_exporter = exporter.Exporter(saver)\r\nsignature = exporter.classification_signature(\r\ninput_tensor=new_model.input,classes_tensor=labels_tensor, scores_tensor=new_model.output)\r\nmodel_exporter.init(sess.graph.as_graph_def(),\r\n                    default_graph_signature=signature)\r\nmodel_exporter.export(export_path, tf.constant(export_version), sess)\r\n```\r\n\r\nLoad model \r\n```\r\n#!/usr/bin/python2\r\n\r\nfrom __future__ import division, print_function, absolute_import\r\nimport tensorflow as tf    \r\nfrom tensorflow.contrib.session_bundle import manifest_pb2\r\nfrom tensorflow.contrib.session_bundle import constants\r\nfrom tensorflow.contrib.session_bundle import session_bundle\r\n \r\nimport pmt\r\nimport numpy as np\r\nfrom gnuradio import gr\r\nimport tensorflow as tf\r\nfrom numpy import zeros, newaxis\r\nimport collections\r\n\r\ndef load_graph(output_graph_path):\r\n\r\n    sess, meta_graph_def = session_bundle.load_session_bundle_from_path(output_graph_path)\r\n    with sess.as_default():\r\n        collection_def = meta_graph_def.collection_def\r\n        signatures_any = collection_def[\r\n        constants.SIGNATURES_KEY].any_list.value\r\n        signatures = manifest_pb2.Signatures()\r\n        signatures_any[0].Unpack(signatures)\r\n        default_signature = signatures.default_signature\r\n        input_name = default_signature.classification_signature.input.tensor_name\r\n        output_name = default_signature.classification_signature.scores.tensor_name\r\n        classes = default_signature.classification_signature.classes.tensor_name\r\n        classes = sess.run(sess.graph.get_tensor_by_name(classes))\r\n        return (sess, input_name, output_name,classes)\r\n\r\nsess,inp,out,classes = load_graph(\"/tmp/cnn/00000001/\")\r\n\r\n\r\nv = np.array([[  1.23725709e-04,   3.21811251e-03,   8.11603665e-03,  -3.23975401e-04,\r\n    1.07787224e-02,   2.94970046e-03,   4.02887911e-03,  -1.88605103e-03,\r\n    1.07172830e-03,  -4.49557183e-03,   4.48013563e-03,  -4.36049374e-03,\r\n    2.03782506e-03,   3.55514325e-03,  -6.66785333e-03,   1.42298348e-03,\r\n   -1.50406240e-02,  -2.57269153e-03,   5.56691317e-03,   1.13693373e-02,\r\n    2.71817949e-03,   5.25468131e-05,  -3.36736324e-03,  -9.75306786e-04,\r\n    3.72400926e-03,   1.18668824e-02,  -1.42240804e-03,   1.44388294e-02,\r\n    8.27464042e-04,   6.72260066e-03,  -5.76557242e-04,   1.67079712e-03,\r\n   -1.02783879e-02,  -6.51435228e-04,  -1.59728453e-02,  -2.92223319e-03,\r\n   -9.65267327e-03,   8.49308504e-04,  -4.86527057e-03,  -2.24598357e-03,\r\n   -6.85441250e-04,  -7.86077511e-03,   8.25393759e-03,  -8.98091588e-03,\r\n   -4.13634349e-03,   5.30316820e-03,  -2.96568638e-03,   7.47767324e-03,\r\n    7.87805114e-03,   3.33608547e-03,   6.81637728e-04,  -3.26122390e-03,\r\n   -5.83499251e-03,   6.05301978e-03,   3.19009693e-03,  -4.02274629e-04,\r\n   -1.09375454e-02,   1.74824963e-03,   1.75286271e-03,  -1.26824277e-02,\r\n    7.39318645e-03,   3.90595663e-03,   1.98931666e-03,   6.12980360e-03,\r\n   -1.79607305e-03,   1.40197761e-02,  -9.81968828e-03,   5.02704596e-03,\r\n    3.01699433e-03,  -6.52436400e-03,  -1.62929588e-03,   7.39145232e-03,\r\n   -9.01302416e-03,   3.07239546e-03,  -1.82828668e-03,  -5.16610499e-03,\r\n    2.61074631e-03,  -1.33442272e-05,   5.14040841e-03,  -6.64286781e-03,\r\n    1.71982939e-03,  -4.85043926e-03,  -3.97882238e-03,  -1.37700920e-03,\r\n   -1.10943802e-02,   2.20915396e-03,  -3.39583290e-04,   4.58237901e-03,\r\n    4.64649638e-03,  -2.94209481e-03,  -1.55386878e-02,   5.68915205e-03,\r\n   -5.80187945e-04,  -5.83021576e-03,  -3.40874423e-04,  -1.83014176e-03,\r\n    3.61575768e-03,  -8.90286360e-03,  -4.51745838e-03,   1.93125161e-03,\r\n   -8.45910795e-03,  -5.77241089e-03,   8.37801304e-03,  -1.38715087e-02,\r\n   -3.31607228e-03,   5.12827048e-03,   9.08580422e-03,   1.84341776e-03,\r\n   -8.04373343e-03,  -9.29000136e-03,  -9.47526656e-04,   4.20172885e-03,\r\n   -5.26063796e-03,  -6.72675669e-03,  -3.59727233e-03,   4.08909051e-03,\r\n    7.33140949e-03,  -7.76879140e-04,  -2.54971418e-03,  -8.70507117e-03,\r\n    7.90149346e-03,   7.96920154e-03,   2.96101277e-03,  -5.38653834e-03,\r\n   -4.88629332e-04,   1.00093251e-02,  -4.25783452e-03,  -5.71854087e-03],\r\n [ -6.87929394e-04,   4.60408907e-03,   7.26573868e-04,   4.26992076e-03,\r\n   -7.02272868e-03,   2.22673942e-03,   1.22035667e-02,  -8.09120014e-03,\r\n    1.39266049e-04,  -1.15088280e-02,   5.04087773e-04,  -2.15286622e-03,\r\n   -7.33058015e-03,  -9.15534515e-03,  -5.08288946e-03,  -1.30671002e-02,\r\n    1.47830602e-03,   1.16440572e-03,   4.02440550e-03,   8.58596340e-03,\r\n    3.03325080e-03,  -2.05237372e-03,   1.05325170e-02,  -1.80078077e-03,\r\n    4.63060196e-03,   1.23807620e-02,  -6.47541787e-03,  -4.13759379e-03,\r\n   -2.83148466e-03,   7.43190618e-03,  -1.15842454e-03,   6.59148069e-03,\r\n    5.59045048e-03,   3.75851267e-03,  -3.95106524e-03,  -2.56526005e-03,\r\n    6.27654884e-03,  -1.24440319e-03,   3.46388144e-04,  -1.55166397e-03,\r\n    1.04056811e-02,   1.30844014e-02,  -6.36276463e-03,  -6.97820855e-04,\r\n   -3.15165240e-03,  -1.41060480e-03,   1.38492498e-03,   8.64384789e-03,\r\n   -7.11268140e-03,  -1.76842324e-03,  -1.25329485e-02,  -4.83873859e-03,\r\n   -5.18619781e-03,   1.30472714e-02,  -5.54988487e-03,   8.61867797e-03,\r\n   -3.99610912e-03,   6.70848880e-04,  -9.35312640e-03,   1.23843951e-02,\r\n   -3.27547453e-03,   4.86938097e-03,  -2.92926189e-03,   2.20531784e-03,\r\n    4.75586858e-03,   3.00767994e-03,   7.01231230e-03,   1.93257479e-03,\r\n    5.04882913e-03,  -1.04642799e-02,   7.11998856e-03,  -2.53466447e-03,\r\n    1.29708648e-03,   1.07713938e-02,  -2.99122441e-03,   5.51079051e-04,\r\n    5.26238093e-03,  -8.22351780e-04,   5.74991386e-03,   8.99204868e-04,\r\n    1.13037638e-02,  -1.46015978e-03,   6.79054251e-03,  -2.91314325e-03,\r\n    6.34925021e-03,   4.45276871e-03,   7.88977742e-03,  -5.24963858e-03,\r\n   -2.57161981e-03,   5.67252794e-03,  -2.77268351e-03,   2.27351789e-03,\r\n    3.13360780e-03,   9.63459164e-03,   3.79459164e-03,   2.40193959e-03,\r\n    3.09617817e-03,   5.46766398e-03,  -1.21412217e-03,  -7.90829584e-03,\r\n   -1.29530125e-03,  -7.43942289e-03,   3.87186417e-03,  -8.09667457e-04,\r\n    1.91524532e-03,  -3.64716118e-03,   8.53588711e-03,  -5.08366944e-03,\r\n    1.74835534e-03,   7.45685189e-04,  -4.69580526e-03,  -1.06729409e-02,\r\n   -4.90031298e-03,  -2.10527773e-03,  -1.65928528e-02,  -9.58569441e-03,\r\n    4.94536944e-03,   6.38392800e-03,  -1.75752665e-03,  -2.40226928e-03,\r\n    4.23104968e-03,   4.33860486e-03,   4.77843359e-03,   4.94898483e-03,\r\n    8.75659316e-05,   6.20994205e-03,   1.03254039e-02,   1.75160269e-04]])\r\n\r\n\r\noutp = sess.run(out, feed_dict={inp: [v]})[0]\r\n\r\nprint(outp)\r\n```\r\n\r\nThanks in advance!\r\n", "comments": ["It's possible that this is related to https://github.com/tensorflow/tensorflow/issues/6814. It would help us if you could provide a minimal example of how to reproduce this issue.", "(Incoming from the mention in #6814...) This looks less straightforward than the problem in #6814. I'm not familiar with the session bundle code, but since the problem appears to be an uninitialized model parameter, I'm assuming that the example code is either not saving the model parameters as intended, or there's some missing call in the loading code.\r\n\r\n@chrisruk Can you inspect the checkpoint that's written as part of the session bundle using [`inspect_checkpoint.py`](https://github.com/tensorflow/tensorflow/blob/fe0c09ac1443dce3ea4d37d221c641b723159bcb/tensorflow/python/tools/inspect_checkpoint.py) to see if the expected model variables are there?\r\n\r\n@nfiedel, is there anything obvious missing from the session bundle-using code?", "I copied the inspect_checkpoint.py file from your link, but I seem to get an error when it attempts to load it:\r\n\r\n```\r\n ./inspect_ck.py --file_name=/tmp/cnn/00000001/checkpoint \r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nW tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /tmp/cnn/00000001/checkpoint: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\r\nUnable to open table file /tmp/cnn/00000001/checkpoint: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\r\n```", "A new checkpoint format was introduced in 0.12. `CheckpointReader` was updated to be backwards compatible. The way it appears to determine if it should use the new checkpoint reader is if there's a name.index file. Does `/tmp/cnn/00000001/checkpoint.index` exist on your hard drive? If so, how do you think it got there?", "Hmm, I only seem to have the following files:\r\n```\r\n/tmp/cnn/00000001 $  ls\r\ncheckpoint  export.data-00000-of-00001  export.index  export.meta\r\n```", "@mrry and @chrisruk, at least one possibility here. The first root error, \"Attempting to use uninitialized value convolution2d_1_W_1\", appears due to an uninitialized Tensor. \r\n\r\nWhile I do see the model code setting the init_op, I don't see it passed to the model_exporter. As-is, the exporter will not set an init_op on the model export, so at load time the init_op will not be called.\r\n\r\nTo try this fix (setting the init_op), please add the \"init_op=init_op,\" line as below:\r\n\r\nmodel_exporter.init(\r\n      sess.graph.as_graph_def(),\r\n      **init_op=init_op**,\r\n      ... / other params\r\n)", "@nfiedel I still get the same error message alas in 0.12.", "Another thing to look into is the init_op definition:\r\n`init_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables())`\r\n\r\nYou might want to check & make sure that the convolution2d_1_W_1 tensor is initialized...", "I'm a little confused how to check the tensors are initialised.\r\n\r\nI just did the following, after the .fit() \r\n\r\n```\r\nfor t in tf.get_default_graph().as_graph_def().node:\r\n    print(t.name)\r\n```\r\nAlong with changing one line to:\r\ninput_tensor=model.input,classes_tensor=labels_tensor, scores_tensor=model.output)\r\n\r\nWhich seems to show 'convolution2d_1_W' is part of the graph.\r\n\r\nWhen attempting to load I get:\r\n```\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value convolution2d_1_W\r\n```\r\n\r\nCheers\r\n", "perhaps this is the same issues as: https://github.com/tensorflow/tensorflow/issues/6336 ?", "@cancan101 Woo, that seems to have fixed it :) It loads the model correctly now.  Thanks a lot!\r\nI assume bundle_shim, will be part of TensorFlow 0.13?", "Will there be a 0.13? They indicate will be in next major release: https://github.com/tensorflow/tensorflow/issues/6695 which may be 1.0", "Ah I didn't realise that, cheers.", "Marking as closed by #6336. Feel free to re-open if that didn't work."]}]