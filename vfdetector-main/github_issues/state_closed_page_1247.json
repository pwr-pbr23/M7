[{"number": 15742, "title": "Use Eigen version of the `scalar_pow_op` for `pow` ops", "body": "This fix use `scalar_pow_op` in Eigen to replace customerized scalar_binary_pow_op_google, as `scalar_pow_op` seems to be in place in Eigen.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Jenkins, test this please.", "@yongtang Unfortunately we cannot accept this PR, since it would break TF inside Google. The Eigen version at Google is not compatible with this, as explained in the comment.", "@rmlarsen Thanks for letting me know. Appreciate the help."]}, {"number": 15741, "title": "Python tensorflow module cannot be reloaded (bug)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  confirmed on both Ubuntu 16.04 LTS in VirtualBox and OS X 10.12.6\r\n- **TensorFlow installed from (source or binary)**:  installed via pip\r\n- **TensorFlow version (use command below)**:  ('v1.4.0-19-ga52c8d9b01', '1.4.1')\r\n- **Python version**: Python 2.7.13\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n> import tensorflow as tf\r\n> reload(tf)\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nSimple bug:\r\n\r\nTrying to reload the module causes a failure.  Not a major problem, in general, but troublesome for the task, which is automated testing of the tensorflow Python API using TSTL (https://github.com/agroce/tstl).\r\n\r\nThe exact sequence is trivial:\r\n\r\n>>> import tensorflow as tf\r\n>>> reload(tf)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/__init__.py\", line 40, in <module>\r\n    del python\r\nNameError: name 'python' is not defined\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@gunan can you please take a look at this issue? Thanks", "Thanks for the detailed bug report, I can understand and reproduce the problem perfectly.\r\n@drpngx any ideas? This looks related to how we seal our python API", "Good news is that thus far tensorflow doesn't raise any unexpected exceptions/crash after 30 hours of fuzzing some core API stuff:\r\n\r\nhttps://github.com/agroce/tstl/blob/master/examples/tensorflow/tf.tstl", "Yes, this is probably an issue because we `del`. We could fix that, however, it won't work. This is not supportable in the current state of affairs. Between the library loading, registration of gradients, and other actual things being run during `import tensorflow`, reloading is not easy to support.", "Thanks.  For test reproducibility, how likely do you think it is that starting \"fresh\" tensorflow tests without a reload will cause nondeterminism (as in, old state not cleared by a reload is corrupt)?  I've been running without reload, just added a --noReload to TSTL compile, but if I find bugs, worry it'll make it harder to report good test cases.", "Tensorflow should behave deterministically. You should be able to tear down\nthe session, clear the default graph, and you're good to go.\n\nOn Sun, Dec 31, 2017, 8:00 PM Alex Groce <notifications@github.com> wrote:\n\n> Thanks. For test reproducibility, how likely do you think it is that\n> starting \"fresh\" tensorflow tests without a reload will cause\n> nondeterminism (as in, old state not cleared by a reload is corrupt)? I've\n> been running without reload, just added a --noReload to TSTL compile, but\n> if I find bugs, worry it'll make it harder to report good test cases.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15741#issuecomment-354635494>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbR2iCY89Cd_7YKMlxCjm0X--UBKpks5tGFhggaJpZM4RPt3F>\n> .\n>\n", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing since we don't have plans to support reload anytime soon.", "How can I refresh KMP settings dynamically since TF loads them on startup and we cannot reload the library? "]}, {"number": 15740, "title": "Simplify the dense_to_one_hot method", "body": "Simplified the `dense_to_one_hot` method.\r\nThe updated method behaves exactly like the old one, but is more concise.", "comments": ["I think it was probably done to avoid the quadratic memory allocation. I am not sure we want to do that.", "Can one of the admins verify this patch?", "Thanks @drpngx, you are right, it is quadratic in `num_classes`.\r\nI somehow expected numpy to be smarter with diagonal matrices, but it actually allocates the whole matrix on initialization.\r\nPR makes no sense then, closing it."]}, {"number": 15739, "title": "Fix the headers error due to recent CUDA9.1 change", "body": "Some headers in CUDA 9.1 has been move to cuda/include/crt directory. ", "comments": ["/CC @gunan FYI", "Can one of the admins verify this patch?", "Jenkins, test this please.", "I feel like I saw this somewhere else (in another PR or issue, too)\r\n@nluehr Is this expected?\r\n@zheng-xq @tfboyd FYI.", "Yes, I merged a PR earlier which added this in some other places.", "That one was by me too.\r\n But I accidently sent that PR to the 1.4 release branch so now it is closed and not being merged. \r\nSorry about that. \r\n", "  Linux CPU Tests failed. But I do not suppose to be so? This is a commit only related to GPU and CUDA. I wonder why it failed. And the ci.tensorflow.org website has a ssl error says that its security certificate expired 3 days ago  Would that be the reason? ", "Jenkins, test this please\n\n\nOn Sun, Dec 31, 2017, 2:01 AM Puyu Wang <notifications@github.com> wrote:\n\n> Linux CPU Tests failed. But I do not suppose to be so? This is a commit\n> only related to GPU and CUDA. I wonder why it failed. And the\n> ci.tensorflow.org website has a ssl error. Would that be the reason?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15739#issuecomment-354595232>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbeecVgGZWRgYAxomDEGcPtN5OYDnks5tF1tsgaJpZM4RPra3>\n> .\n>\n", "This PR exposes the internal implementations within the cuda/include/crt directory. These files are subject to frequent changes between CUDA versions, and are not intended for direct inclusion. A better solution would be to update the Eigen sources to hash 034b6c3 or newer, which substitutes the 'public' cuda_runtime.h header for math_functions.hpp.", "Given the information by @nluehr, I would like to revert this and update eigen dependency."]}, {"number": 15738, "title": "wrong output(extra symbol \"b\")", "body": "**System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows  Home\r\nTensorFlow installed from (source or binary): binary (pip)\r\nTensorFlow version (use command below): 1.4.0 (GPU)\r\nPython version: 3.6**\r\nGPU: GTX1050 Ti M (disabled Intel visualization)\r\n\r\nBasically any output I try to write in console is with extra symbol : \"b\"\r\nfor example:\r\nwith cmmand:\r\n```\r\nC: python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nb'unknown' 1.4.0 //OUTPUT\r\n```\r\nI downloaded GPU verison form guide from official webiste(with:Anaconda):\r\n[https://www.tensorflow.org/install/install_windows#installing_with_anaconda](url)\r\n\r\ni have tried the validation example and same problem (extra symbol : \"b\")\r\n", "comments": ["I just tried using custom code:\r\n```\r\n>>> node1 = tf.constant(3.0, dtype=tf.float32)\r\n>>> node2 = tf.constant(4.0, dtype=tf.float32)\r\n>>> print(node1,node2)\r\nTensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\r\n```\r\nand the oputput looks good \r\nI am bit scared something is not right \r\nthx for any help", "Don't worry. The b prefix indicates a byte string for python, see https://www.python.org/dev/peps/pep-3112/", "Closing as this seems to not be an issue. Please let me know if I misunderstood"]}, {"number": 15737, "title": "AttentionWrapper zero_state(batch_size, tf.float32).clone(cell_state=encoder_state) fails when batch size is 1", "body": "Hello!\r\nI believe to have found a small bug when using the `zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)` command. When batch size is 1, the error `ValueError: The shape for decoder/while/Merge_5:0 is not an invariant for the loop. It enters the loop with shape (1, 512), but has shape (?, 512) after one iteration. Provide shape invariants using either the shape_invariants argument of tf.while_loop or set_shape() on the loop variables.` is thrown. This error does not occur when batch size is 2 or larger. The error also doesn't occur if I remove the .clone command. \r\nI tried investigating where the error is, but couldn't find the cause. I'm using this in context of trying to build a neural transducer, but also get the same error for basic seq2seq:\r\n(Based on the NMT tutorial)\r\n\r\n```python\r\n# .... Encoder, constants etc...\r\n# Decoder\r\nhelper = tf.contrib.seq2seq.TrainingHelper(\r\n    decoder_inputs_embedded, decoder_full_length, time_major=True) \r\n\r\nattention_states = tf.transpose(encoder_outputs, [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n    encoder_hidden_units, attention_states,\r\n    memory_sequence_length=encoder_inputs_length)\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    tf.contrib.rnn.LSTMCell(decoder_hidden_units),\r\n    attention_mechanism,\r\n    attention_layer_size=decoder_hidden_units)\r\n\r\nprojection_layer = layers_core.Dense(\r\n    vocab_size, use_bias=False)\r\n\r\n\r\ndecoder = tf.contrib.seq2seq.BasicDecoder(\r\n    decoder_cell, helper, decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state),\r\n    output_layer=projection_layer)\r\n\r\n# ---- Training ----\r\noutputs, last_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True)\r\nlogits = outputs.rnn_output\r\ndecoder_prediction = outputs.sample_id\r\n```\r\n\r\n\r\nTF Version: ('v1.4.0-19-ga52c8d9', '1.4.1')\r\nSystem details:\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"17.10 (Artful Aardvark)\"\r\nVERSION_ID=\"17.10\"\r\nVERSION_CODENAME=artful\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.2.0-8ubuntu3) 7.2.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.5.1)\r\ntensorflow (1.4.1)\r\ntensorflow-tensorboard (0.4.0rc3)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.4.1\r\ntf.GIT_VERSION = v1.4.0-19-ga52c8d9\r\ntf.COMPILER_VERSION = v1.4.0-19-ga52c8d9\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```\r\nI believe this is an important issue, as often times when experimenting with new seq2seq models I start off with trying to get it to work for a batch size of 1.\r\n\r\nThanks!\r\nNikita", "comments": ["@ebrevdo can you please take a look. Thanks.\r\n\r\n\r\n", "This looks like an important bug to fix.  Can you provide a pre-amble creating properly sized input tensors, like `decoder_inputs_embedded`, `encoder_outputs`, etc, using `tf.ones` or `tf.random_...`, so that I can directly replicate the issue?", "CC += @oahziur ", "@ebrevdo \r\nI've made two very hacky standalone code samples and I found out that this error only occurs when using placeholders:\r\n\r\nThis example fails only when the clone method is called:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.layers import core as layers_core\r\n\r\n# -- Constants ---\r\nvocab_size = 10\r\ninput_dimensions = 20\r\ninput_embedding_size = 50\r\nencoder_hidden_units = 512\r\ndecoder_hidden_units = encoder_hidden_units\r\nbatch_size = 1\r\ntime_to_sim_input = 5\r\n\r\n# ---- Build model ----\r\nencoder_inputs = tf.placeholder(shape=(None, None, input_dimensions), dtype=tf.float32, name='encoder_inputs')\r\nencoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\r\ndecoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\r\ndecoder_target_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='decoder_target_length')\r\ndecoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')\r\ndecoder_full_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='decoder_full_length')\r\n\r\n\r\nembeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\r\nencoder_inputs_embedded = encoder_inputs\r\ndecoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)\r\ndecoder_inputs_embedded = tf.transpose(decoder_inputs_embedded, perm=[1, 0, 2])  # Make it time major again\r\n\r\n\r\n# ---- Encoder ------\r\nencoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\r\n\r\n# Run Dynamic RNN\r\n#   encoder_outputs: [max_time, batch_size, num_units]\r\n#   encoder_state: [batch_size, num_units]\r\nencoder_outputs, encoder_state = tf.nn.dynamic_rnn(\r\n    encoder_cell, encoder_inputs_embedded,\r\n    sequence_length=encoder_inputs_length, time_major=True,\r\n    dtype=tf.float32)\r\n\r\n\r\n# ---- Decoder -----\r\nhelper = tf.contrib.seq2seq.TrainingHelper(\r\n    decoder_inputs_embedded, decoder_full_length, time_major=True)\r\n\r\nattention_states = tf.transpose(encoder_outputs, [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n    encoder_hidden_units, attention_states,\r\n    memory_sequence_length=encoder_inputs_length)\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    tf.contrib.rnn.LSTMCell(decoder_hidden_units),\r\n    attention_mechanism,\r\n    attention_layer_size=decoder_hidden_units)\r\n\r\nprojection_layer = layers_core.Dense(\r\n    vocab_size, use_bias=False)\r\n\r\n\r\n# ------------------------- CLONE --------------------------\r\ndecoder = tf.contrib.seq2seq.BasicDecoder(\r\n    decoder_cell, helper, decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state),\r\n    output_layer=projection_layer)\r\n\r\n# ---- Training ----\r\noutputs, last_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True)\r\nlogits = outputs.rnn_output\r\ndecoder_prediction = outputs.sample_id\r\n\r\n\r\ntargets_one_hot = tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32)\r\nstepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=targets_one_hot, logits=logits)\r\nloss = tf.reduce_mean(stepwise_cross_entropy)\r\ntrain_op = tf.train.AdamOptimizer().minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\n\r\n# ---- Loader -----\r\n\r\n\r\ndef next_batch(amount=batch_size):\r\n\r\n    e_in = np.random.uniform(-1.0, 1.0, size=(batch_size, time_to_sim_input, input_dimensions))\r\n    e_in_length = [time_to_sim_input]\r\n    d_targets = np.asarray([[1, 2, 3, 4, 5]])\r\n    d_targets_length = [len(np.asarray([1, 2, 3, 4, 5]))]\r\n    offset_din = np.asarray([[9, 1, 2, 3, 4, 5]])\r\n\r\n    return {\r\n        encoder_inputs: np.transpose(e_in, axes=[1, 0, 2]),\r\n        encoder_inputs_length: e_in_length,\r\n        decoder_targets: np.transpose(d_targets),\r\n        decoder_target_length: d_targets_length,\r\n        decoder_inputs: offset_din,\r\n        decoder_full_length: np.asarray([len(np.asarray([1, 2, 3, 4, 5]))] * amount),\r\n    }\r\n\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    losses = []\r\n\r\n    feed = next_batch()\r\n    _, l, predict = sess.run([train_op, loss, decoder_prediction], feed)\r\n    losses.append(l)\r\n\r\n    print 'Finished'\r\n\r\n```\r\n\r\nAnd this example is fine, but doesn't use placeholders:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers import core as layers_core\r\n\r\n# -- Constants ---\r\nvocab_size = 10  # PAD = 8, EOS = 9\r\ninput_dimensions = 20\r\ninput_embedding_size = 50\r\nencoder_hidden_units = 512\r\ndecoder_hidden_units = encoder_hidden_units\r\nbatch_size = 1\r\ntime_to_sim_input = 8\r\n\r\n# ---- Build model ----\r\n\r\nencoder_inputs = tf.random_uniform(shape=(time_to_sim_input, batch_size, input_dimensions))\r\nencoder_inputs_length = tf.constant(time_to_sim_input, shape=(batch_size,))\r\ndecoder_targets = tf.constant([1, 2, 3, 4, 5], shape=(5, batch_size))\r\ndecoder_targets_length = tf.constant(4, shape=(batch_size,))\r\ndecoder_inputs = tf.constant([9, 1, 2, 3, 4], shape=(batch_size, 5))\r\ndecoder_full_length = tf.constant([5] * batch_size, shape=(batch_size,))\r\n\r\nembeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\r\nencoder_inputs_embedded = encoder_inputs\r\ndecoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)\r\ndecoder_inputs_embedded = tf.transpose(decoder_inputs_embedded, perm=[1, 0, 2])  # Make it time major again\r\n\r\n\r\n# ---- Encoder ------\r\nencoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\r\n\r\n# Run Dynamic RNN\r\n#   encoder_outputs: [max_time, batch_size, num_units]\r\n#   encoder_state: [batch_size, num_units]\r\nencoder_outputs, encoder_state = tf.nn.dynamic_rnn(\r\n    encoder_cell, encoder_inputs_embedded,\r\n    sequence_length=encoder_inputs_length, time_major=True,\r\n    dtype=tf.float32)\r\n\r\n# ---- Decoder -----\r\nhelper = tf.contrib.seq2seq.TrainingHelper(\r\n    decoder_inputs_embedded, decoder_full_length, time_major=True)\r\n\r\nattention_states = tf.transpose(encoder_outputs, [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n    encoder_hidden_units, attention_states,\r\n    memory_sequence_length=encoder_inputs_length)\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    tf.contrib.rnn.LSTMCell(decoder_hidden_units),\r\n    attention_mechanism,\r\n    attention_layer_size=decoder_hidden_units)\r\n\r\nprojection_layer = layers_core.Dense(\r\n    vocab_size, use_bias=False)\r\n\r\n# ------------------------- CLONE --------------------------\r\ndecoder = tf.contrib.seq2seq.BasicDecoder(\r\n    decoder_cell, helper, decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state),\r\n    output_layer=projection_layer)\r\n\r\n# ---- Training ----\r\noutputs, last_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True)\r\nlogits = outputs.rnn_output\r\ndecoder_prediction = outputs.sample_id\r\n\r\n\r\ntargets_one_hot = tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32)\r\nstepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=targets_one_hot, logits=logits)\r\nloss = tf.reduce_mean(stepwise_cross_entropy)\r\ntrain_op = tf.train.AdamOptimizer().minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\n\r\n# ---- Loader -----\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    losses = []\r\n\r\n    _, l, predict = sess.run([train_op, loss, decoder_prediction])\r\n    losses.append(l)\r\n\r\n    print 'Finished'\r\n\r\n```", "@nikita68 \r\n\r\nI think you need to make sure the the encoder_state has a static batch_size which means you need to set your placeholder's batch dimension as `batch_size` instead of `None`.\r\n\r\n or \r\n\r\nyou can just use a tensor as the zero_states' batch size instead of a constant.\r\n\r\nFor example:\r\n\r\n```\r\ndecoder_init_state = decoder_cell.zero_state(tf.size(decoder_full_length), tf.float32).clone(cell_state=encoder_state)\r\n```", "Do we need to, in general, have a static batch size for the encoder?  We\nshouldn't require that.\n\nOn Tue, Jan 2, 2018 at 2:22 PM, Rui Zhao <notifications@github.com> wrote:\n\n> @nikita68 <https://github.com/nikita68>\n>\n> I think you need to make sure the the encoder_state has a static\n> batch_size which means you need to set your placeholder's batch dimension\n> as batch_size instead of None.\n>\n> or\n>\n> you can just use a tensor as the zero_states' batch size instead of a\n> constant.\n>\n> For example:\n>\n> decoder_init_state = decoder_cell.zero_state(tf.size(decoder_full_length), tf.float32).clone(cell_state=encoder_state)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15737#issuecomment-354890871>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3AgdSckSYq6WP3jROttKBbKWOsLks5tGqwQgaJpZM4RPqFa>\n> .\n>\n", "@ebrevdo The encoder doesn't need to have static batch size, as long as we don't give a static value to the zero_states in this case.\r\n\r\nI think the problem here is clone() copies an encoder state without static batch size to an attention wrapper state that was initialized with static batch size.", "Ah!  I see; that's why it lost that size information.  I wonder if we can\nbring that static shape info back in, inside the decode body.\n\nOn Tue, Jan 2, 2018 at 3:35 PM, Rui Zhao <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> The encoder doesn't need to have\n> static batch size, as long as we don't give a static vale to the\n> zero_states in this case.\n>\n> I think the problem here is clone() copies an encoder state without static\n> batch size to an attention wrapper state that was initialized with static\n> batch size.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15737#issuecomment-354904052>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6yTs3NfmlGXllm2udMKYgVvl5-yks5tGr1QgaJpZM4RPqFa>\n> .\n>\n", "@oahziur Thanks, it fixed the problem.\r\n@ebrevdo I've also been thinking the same thing.\r\nIs this issue considered closed?", "@nikita68 probably not; @oahziur is looking at possible solutions.  we had one that made `clone()` perform some additional static shape inference, but that's a bit fraught.  probably we can add some `set_shape` calls inside either `AttentionWrapper`.  @oahziur wdyt?", "@ebrevdo \r\n\r\n`clone()` maybe the best place to set the new Tensor's batch_size info since that is the only place users may update tensors in the `AttentionWrapperState`.\r\n\r\nInstead of doing `set_shape` to copy the static shape information, I think we can use the attention_mechanism.batch_size for the initialization in `AttentionWrapper.zero_state()` to fix issues like this, so the batch_size info is always consistent with `memory` and `encoder_state`.", "Was this resolved? I'm still having this issue in 1.4"]}, {"number": 15736, "title": "Importing submodules from tensorflow.keras fails with No module named 'tensorflow.keras'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (pip)\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: 3.6\r\n- **Exact command to reproduce**:\r\nfrom tensorflow.keras import datasets # fails but should work\r\nimport tensorflow as tf #succeeds\r\ntf.keras.datasets #succeeds\r\nfrom tensorflow.python.keras import datasetst # succeeds\r\n\r\n### Describe the problem\r\nImporting submodules from tensorflow.keras fails with error: `ModuleNotFoundError: No module named 'tensorflow.keras'`. but `import tensorflow as tf` and then doing `tf.keras.datasets` works. This is a big inconsistency, also it means that every time an element from within the `tensforlow.keras` module you need to write the complete path (which is very annoying) this removes the simplicity and readability of the `keras` API. A work around is to import submodules from `tensorflow.python.keras`, which again is inconsistent. \r\n\r\nIn my opinion, since the documentation states that `keras` is availabe at `tf.keras` that should be the access path to the submodules and not `tensorflow.python.keras`. I'll try to  make a pull request for this.\r\n\r\n### Source code / logs\r\n\r\n```python\r\n# fails but should work\r\nfrom tensorflow.keras import datasets\r\n```\r\n```python\r\n# succeeds\r\nimport tensorflow as tf\r\ntf.keras.datasets\r\n```\r\n```python\r\n# succeeds\r\nfrom tensorflow.python.keras import datasetst\r\n```", "comments": ["I can confirm this problem exists with my system settings that are similar.\r\n\r\n@rragundez Thanks for reposting this issue here and taking care of it!", "Pull request submitted https://github.com/tensorflow/tensorflow/pull/15757\r\nThis only solves the import problem with `tensorflow.python.keras`, can be easily modified to do the same for any submodule inside the `tensorflow.python` module.\r\n", "Sorry, we only support `tf.keras.datasets`, and not individual imports.", "@rragundez and @drpngx thanks for the quick fix and response, especially on New Years!\r\n\r\n@drpngx Please explain the philosophy. Why have you made that decision and what do you propose is a good way of importing Keras from TensorFlow?\r\n\r\nPerhaps @fchollet has a comment on the best way of importing Keras from TensorFlow?\r\n\r\nCurrently I am writing:\r\n\r\n    from tensorflow.python.keras.models import Sequential\r\n    from tensorflow.python.keras.layers import Dense\r\n    model = Sequential()\r\n    model.add(Dense(...))\r\n\r\nBut this is a bit clumsy compared to just writing the following, which unfortunately does not work:\r\n\r\n    from tensorflow.keras.models import Sequential\r\n    from tensorflow.keras.layers import Dense\r\n    model = Sequential()\r\n    model.add(Dense(...))\r\n\r\nIt would be even more clumsy to write e.g.:\r\n\r\n    import tensorflow as tf\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Dense(...))\r\n    model.add(tf.keras.layers.Dense(...))\r\n    model.add(tf.keras.layers.Dense(...))\r\n\r\nOne workaround would seem to be:\r\n\r\n    import tensorflow as tf\r\n    Sequential = tf.keras.models.Sequential\r\n    Dense = tf.keras.layers.Dense\r\n    model = Sequential()\r\n    model.add(Dense(...))\r\n    model.add(Dense(...))\r\n    model.add(Dense(...))\r\n\r\nBut I am not really a Python expert so I don't know if this is considered bad style for some reason?\r\n\r\nHappy New Years! :-)", "I see. We might decide to do something else for keras, but the last two are the ones supported by tensorflow.\r\n\r\nThe rationale behind this is that tensorflow is more like numpy. It's one big monolithic package that you import. We don't support importing piecemeal, because our code doesn't internally distinguish between private and exportable symbols. You might see `def foo` which really means `def _foo`, or `import sys` which really means `import sys as _sys`. So, if you import random modules in tensorflow, you will have access to private symbols. We don't want to support that, as it would be a large maintenance burden.", "> So, if you import random modules in tensorflow, you will have access to private symbols. \r\n\r\nJust a note to say that this isn't the case for imports in `tf.keras`, because the underlying files only contain public symbols (they're separate API-only files).", "Happy to have direct imports for keras, if you and @martinwicke agrees.", "@Hvass-Labs @drpngx @fchollet Adding to: if it is a bad style in Python? it is, the bad style comes from the fact that you can access submodules from `tf` (`tf.keras.layers.Dense` for example) but you cannot import `Dense`  as `from tensorflow.keras.layers import Dense`. When deep functionality wants to be exposed in a higher module is a direct exposure of objects (functions or classes) then you never have the problem at hand. In this case the functionality being exposed to the user is a module itself which causes this confusion. \r\n\r\nYou can easily look at `pyspark` or `sklearn` for example, where the functionality taken to a higher level in the package are objects (a type of estimator for example) but not modules themselves (as they will create the problem at hand).\r\n\r\nSo indeed the current approach is not pythonic and specially I would argue that adds complexity to the simplistic Keras API. ", "I imagine most users of TensorFlow need to use a builder API. It has been a never-ending source of frustration for myself and the community that you have had numerous different and unfinished builder API's, instead of consolidating their best features into a single default API. You never even finished trivial features of the Layers API before you abandoned it.\r\n\r\nTo my knowledge Keras is the best builder API for TensorFlow. I would therefore encourage you to consider Keras to be the default builder API that many people will be using with TensorFlow. I think it would be wise to give it special treatment in the code-base if that is necessary.\r\n\r\nSimilarly goes for Keras' author @fchollet whose philosophy on API design is wonderfully user-centric so I think his opinion should weigh heavily in your discussions on the rest of TensorFlow's API as well.\r\n", "The standard python methods for importing modules and symbols from modules do not appear to work for parts of tensorflow that are described as modules.  That includes tensorflow.keras, tensorflow.keras.layers, and tensorflow.keras.models.  The result is that tensorflow comes across as a second class python package written by programmers who do not know what they are doing.  It also results in ugly code when the full path has to be written for every symbol in a tensorflow script that uses the keras object model.  If Google wants to project tensorflow as high quality software, this second class implementation needs to be fixed.", "@dnjake Hehehe! I'm glad to hear I'm not the only one who was frustrated by the TensorFlow API :-)", "I only have a modest experience with tensorflow.  So my opinion might change.\nBut my past does include two decades of experience developing computer system\nsoftware.  My view is that the big problem with tensorflow is mainly that it is\nintrinsically hard.  No group is able to architect a complex UI in advance of\nany experience with developing and using it.  On a realistic scale, I would give\ntensorflow an high grade.  But, it is certainly a work in progress that is hard to\nunderstand by those developing it, hard to understand how to use it, and even harder to use\nproductively after it is understood.  I think the base design did make a big\nmistake with a configuration architecture that left major objects like the\nsession context and the default graph as implicit global variables.  I also am skeptical of\nthe value of eager mode.  Had the base design been better, perhaps only\none architecture would have emerged for models, graphs, and layers.  But, I\ndoubt that navigating the variations in the different object models that have\nbeen built on top of the tensorflow python base will prove to be the limiting\nfactor for most people in working with tensorflow\n________________________________\nFrom: Hvass-Labs <notifications@github.com>\nSent: Saturday, July 7, 2018 9:56 AM\nTo: tensorflow/tensorflow\nCc: dnjake; Mention\nSubject: Re: [tensorflow/tensorflow] Importing submodules from tensorflow.keras fails with No module named 'tensorflow.keras' (#15736)\n\n\n@dnjake<https://github.com/dnjake> Hehehe! I'm glad to hear I'm not the only one who was frustrated by the TensorFlow API :-)\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/15736#issuecomment-403217300>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AVvuh14LQQo_RibDj7e1iFuL2_pzJV9Sks5uEL4dgaJpZM4RPo9S>.\n", "@dnjake \r\n\r\n> The standard python methods for importing modules and symbols from modules do not appear to work for parts of tensorflow that are described as modules. That includes tensorflow.keras, tensorflow.keras.layers, and tensorflow.keras.models. The result is that tensorflow comes across as a second class python package written by programmers who do not know what they are doing. It also results in ugly code when the full path has to be written for every symbol in a tensorflow script that uses the keras object model. If Google wants to project tensorflow as high quality software, this second class implementation needs to be fixed.\r\n\r\n\"second class python package written by programmers who do not know what they are doing\" seems to be a bold statement. Google open-sourced tensorflow and Facebook open-sourced pytorch. tensorflow and pytorch are not perfect, but they are definitely valuable. Let us thank Google and Facebook for open-sourcing these valuable software. (I am a novice for tensorflow and pytorch.)", "from tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.optimizers import Adam\r\nimport tensorflow.keras.backend as K\r\n\r\nWorks for me. Why?", "Can someone clearly explain this:\r\n\r\n\"Works\"\r\nimport tensorflow \r\nfrom tensorflow.keras import _______\r\n\r\n\r\nDoesnot work:\r\nimport tensorflow as tf\r\nfrom tf.keras import ________", "Is there a solution to this?\r\n\r\nFor :\r\n from tensorflow.keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten\r\n\r\nI get:\r\nModuleNotFoundError: No module named 'tensorflow.keras.layers.core'\r\n", "I have not double checked this answer.  But I believe the reason for the problem is the way Tensorflow uses package __init__.py modules to restrict user access to the fixed Tensorflow API.  The way those modules are coded prevents the standard python techniques of mapping symbols to arbitrary module content from working.\n", "I can call keras.something when I installed keras on NVIDIA Jetson TX2 with `sudo apt-get install python3-keras`", "I tried \r\n`sudo pip3 install keras`\r\nsomething compiles loong cython, etc. on a single core then fails for a missing library or sth.\r\nso @ajeyaajeya's solution **above** works best.\r\n\r\n", "> `from tensorflow.keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten`\r\n\r\n`from tensorflow.python.keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten`\r\n\r\nworks for me", "Had the same problem. None of the solutions above worked for me. Could not import `tensorflow.keras` or any sub/super variation of it, such as `tensorflow.python.keras`  Frustrating.  Tested with Conda environment and python 3.7+ virtual env. Decided to `pip install keras` and import direct from that while tensorflow was `conda install`..ed. I am assuming that the conda install yields c based binaries for all languages while the pip is bringing back something python can understand. Anyway, not on to trying to get gpu working :-) ", "Are you using tensorflow 2?  I have not tried it.  But, the talks I have heard over the web indicated that changes to tensorflow 2 make it possible to reference all of the module variables and functions.  In tensorflow 1, the __init__.py files for packages have been extensively manipulated to limit references to those specified in the api.  It is possible to go into the source code and modify the __init__.py files.  But, otherwise access to module internals is limited by the api.\n\n________________________________\nFrom: Frank Coliviras <notifications@github.com>\nSent: Thursday, February 27, 2020 2:20 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: dnjake <dnjake@live.com>; Mention <mention@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Importing submodules from tensorflow.keras fails with No module named 'tensorflow.keras' (#15736)\n\n\nHad the same problem. None of the solutions above worked for me. Could not import tensorflow.keras.. or any variation of it, such as tensorflow.python.keras. Frustrating. Tested with Conda environment and python 3.7+ virtual env. Decided to pip install keras and import direct from that while tensorflow was conda install..ed. I am assuming that the conda install yields c based binaries for all languages while the pip is bringing back something python can understand. Anyway, not on to trying to get gpu working :-)\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/15736?email_source=notifications&email_token=AFN65BY4DL6FQFKTNDVMPZLRFAHAVA5CNFSM4EJ6R5JKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOENFTPPA#issuecomment-592132028>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AFN65B3QMDUB23QQK7POZ5DRFAHAVANCNFSM4EJ6R5JA>.\n", "> Can someone clearly explain this:\r\n> \r\n> \"Works\"\r\n> import tensorflow\r\n> from tensorflow.keras import _______\r\n> \r\n> Doesnot work:\r\n> import tensorflow as tf\r\n> from tf.keras import ________\r\n\r\nwhen you import tensorflow as tf, you can use it to directly call functions into variable but not import sub-modles.\r\n\r\n####Example####\r\nimport tensorflow as tf\r\nmodel=tf.keras.models.sequential()", "If you're getting this error in tensorflow==2.0.0, then do the following steps -\r\n\r\nStep 1: Uninstall the keras by using '!python -m pip uninstall keras --yes' \r\nStep 2: Reinstall keras by using 'pip install keras'\r\nStep 3: Now, install tensorflow 2.0.0 by using 'pip install tensorflow==2.0.0'\r\n\r\nThis should work, without downgrading the tensorflow version. Thank You!"]}, {"number": 15735, "title": "Segmentation fault when using cuDNN LSTMs + orthogonal initializer", "body": "On TF 1.4.0 with CUDA 8.0 and cuDNN 7, I get a segmentation fault if I use an orthogonal initializer for the `kernel_initializer` argument of `cudnn_rnn.CudnnLSTM` (the layers version not the op). I'm aware of #14306 but since I'm running into this problem with the cuDNN LSTMs and not the native TF ones I suspect the underlying cause is different.", "comments": ["@ebrevdo can you please take a look?", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Same as the other issue, could you try and build with ASAN and give us more information about where it crahes?\r\n\r\nCC @protoget Note the related ticket. It could be something to do with the orthogonal initializer.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 15734, "title": "fix typo", "body": "fix typo", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 15733, "title": "\"Failed to load the native TensorFlow runtime.\" error on Raspbian stretch", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Raspbian Strecth\r\n- **TensorFlow installed from (source or binary)**: Source (Compiled from git on raspberry pi 3)\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc version 4.8.5 (Raspbian 4.8.5-4) \r\n- **CUDA/cuDNN version**: No (nonconfigured)\r\n- **GPU model and memory**:No (nonconfigured)\r\n- **Exact command to reproduce**: python3 \">>> import tensorflow\"\r\nHi,\r\nI have compiled tensorflow's 1.4.1 source code from git source. There was no error while compiling from source but after installition by pip3, I can't import tensorflow library in python3\r\nWhen I gave \"import tensorflow\" command in the python 3 shell it gives this error:\r\n\r\n \">>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws11Environment6GetEnvEPKc\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws11Environment6GetEnvEPKc\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\"\r\n\r\n", "comments": ["Thank you for your feedback but tensorflow currently doesn't support raspberry pi. \r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). There is also a larger community that reads questions there. \r\n\r\nOr you might visit \r\nhttps://petewarden.com/2017/08/20/cross-compiling-tensorflow-for-the-raspberry-pi/.\r\nfor more information.\r\nThanks!", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@bignamehyp thank you, you solved my issue", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "I encountered this building TensorFlow 1.13.1 with Bazel 0.21.0 for Python 3.7.3, so in case it helps anyone:\r\n\r\nThe problem is that `third_party/aws/BUILD.bazel` does not have a case for whatever it thinks your architecture is -- it does have a case for the rule, `raspberry_pi_armeabi`, but that case is *not* being selected automatically when you actually build on an RPI (because the corresponding rule definition in `tensorflow/BUILD` matches settings for cross compilation only).  An easy local approach is to fix the catchall (patch below).\r\n\r\nTo the maintainers: It seems like `third_party/aws/BUILD.bazel` should be made to cause an error as soon as the select doesn't match (or, better, just use 'linux-shared' per the patch below), since the default (which is `[]` -- no platform implementation) is clearly ***not*** the right thing.\r\n\r\n```\r\ndiff --git a/third_party/aws/BUILD.bazel b/third_party/aws/BUILD.bazel\r\nindex 5426f79e46..b8eb50a27f 100644\r\n--- a/third_party/aws/BUILD.bazel\r\n+++ b/third_party/aws/BUILD.bazel\r\n@@ -24,7 +24,9 @@ cc_library(\r\n         \"@org_tensorflow//tensorflow:raspberry_pi_armeabi\": glob([\r\n             \"aws-cpp-sdk-core/source/platform/linux-shared/*.cpp\",\r\n         ]),\r\n-        \"//conditions:default\": [],\r\n+        \"//conditions:default\": glob([\r\n+            \"aws-cpp-sdk-core/source/platform/linux-shared/*.cpp\",\r\n+        ]),\r\n     }) + glob([\r\n         \"aws-cpp-sdk-core/include/**/*.h\",\r\n         \"aws-cpp-sdk-core/source/*.cpp\",\r\n```\r\n"]}, {"number": 15732, "title": "Missing OpKernel when using selective registration header", "body": "I use the `bazel-bin/tensorflow/python/tools/print_selective_registration_header` to find out all necessary ops and kernels for my `test.pb`, and then compile them into a android executable\r\n\r\n`bazel build test/test:test_run --copt=-DSELECTIVE_REGISTRATION  --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a`\r\n\r\nThe `ops_to_register.h` is as follows\r\n\r\n```\r\n// This file was autogenerated by print_selective_registration_header.py\r\n#ifndef OPS_TO_REGISTER\r\n#define OPS_TO_REGISTER\r\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\r\n  return false\r\n     || (strcmp(op, \"Add\") == 0)\r\n     || (strcmp(op, \"AddN\") == 0)\r\n     || (strcmp(op, \"ApplyGradientDescent\") == 0)\r\n     || (strcmp(op, \"Assign\") == 0)\r\n     || (strcmp(op, \"AssignAdd\") == 0)\r\n     || (strcmp(op, \"BiasAdd\") == 0)\r\n     || (strcmp(op, \"BiasAddGrad\") == 0)\r\n     || (strcmp(op, \"BroadcastGradientArgs\") == 0)\r\n     || (strcmp(op, \"ConcatOffset\") == 0)\r\n     || (strcmp(op, \"ConcatV2\") == 0)\r\n     || (strcmp(op, \"Const\") == 0)\r\n     || (strcmp(op, \"ExpandDims\") == 0)\r\n     || (strcmp(op, \"Fill\") == 0)\r\n     || (strcmp(op, \"Floor\") == 0)\r\n     || (strcmp(op, \"FloorMod\") == 0)\r\n     || (strcmp(op, \"Gather\") == 0)\r\n     || (strcmp(op, \"Identity\") == 0)\r\n     || (strcmp(op, \"L2Loss\") == 0)\r\n     || (strcmp(op, \"MatMul\") == 0)\r\n     || (strcmp(op, \"Minimum\") == 0)\r\n     || (strcmp(op, \"Mul\") == 0)\r\n     || (strcmp(op, \"Neg\") == 0)\r\n     || (strcmp(op, \"NoOp\") == 0)\r\n     || (strcmp(op, \"Pack\") == 0)\r\n     || (strcmp(op, \"Placeholder\") == 0)\r\n     || (strcmp(op, \"PlaceholderWithDefault\") == 0)\r\n     || (strcmp(op, \"PreventGradient\") == 0)\r\n     || (strcmp(op, \"RandomUniform\") == 0)\r\n     || (strcmp(op, \"RealDiv\") == 0)\r\n     || (strcmp(op, \"Reshape\") == 0)\r\n     || (strcmp(op, \"ScatterSub\") == 0)\r\n     || (strcmp(op, \"Shape\") == 0)\r\n     || (strcmp(op, \"Sigmoid\") == 0)\r\n     || (strcmp(op, \"SigmoidGrad\") == 0)\r\n     || (strcmp(op, \"Size\") == 0)\r\n     || (strcmp(op, \"Slice\") == 0)\r\n     || (strcmp(op, \"Softmax\") == 0)\r\n     || (strcmp(op, \"SparseSoftmaxCrossEntropyWithLogits\") == 0)\r\n     || (strcmp(op, \"Split\") == 0)\r\n     || (strcmp(op, \"SplitV\") == 0)\r\n     || (strcmp(op, \"Sqrt\") == 0)\r\n     || (strcmp(op, \"Squeeze\") == 0)\r\n     || (strcmp(op, \"StridedSlice\") == 0)\r\n     || (strcmp(op, \"Sub\") == 0)\r\n     || (strcmp(op, \"Sum\") == 0)\r\n     || (strcmp(op, \"Tanh\") == 0)\r\n     || (strcmp(op, \"TanhGrad\") == 0)\r\n     || (strcmp(op, \"Tile\") == 0)\r\n     || (strcmp(op, \"TopKV2\") == 0)\r\n     || (strcmp(op, \"Unpack\") == 0)\r\n     || (strcmp(op, \"VariableV2\") == 0)\r\n     || (strcmp(op, \"ZerosLike\") == 0)\r\n     || (strcmp(op, \"_Recv\") == 0)\r\n     || (strcmp(op, \"_Send\") == 0)\r\n  ;\r\n}\r\n#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)\r\n\r\n\r\n    namespace {\r\n      constexpr const char* skip(const char* x) {\r\n        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;\r\n      }\r\n\r\n      constexpr bool isequal(const char* x, const char* y) {\r\n        return (*skip(x) && *skip(y))\r\n                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))\r\n                   : (!*skip(x) && !*skip(y));\r\n      }\r\n\r\n      template<int N>\r\n      struct find_in {\r\n        static constexpr bool f(const char* x, const char* const y[N]) {\r\n          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);\r\n        }\r\n      };\r\n\r\n      template<>\r\n      struct find_in<0> {\r\n        static constexpr bool f(const char* x, const char* const y[]) {\r\n          return false;\r\n        }\r\n      };\r\n    }  // end namespace\r\n    constexpr const char* kNecessaryOpKernelClasses[] = {\r\n\"BinaryOp< CPUDevice, functor::add<float>>\",\r\n\"AddNOp< CPUDevice, float>\",\r\n\"ApplyGradientDescentOp<CPUDevice, float>\",\r\n\"AssignOpT<CPUDevice, ::tensorflow::int64>\",\r\n\"AssignOpT<CPUDevice, float>\",\r\n\"DenseUpdateOp<CPUDevice, ::tensorflow::int64, DenseUpdateType::ADD>\",\r\n\"BiasOp<CPUDevice, float>\",\r\n\"BiasGradOp<CPUDevice, float>\",\r\n\"BCastGradArgsOp\",\r\n\"ConcatOffsetOp\",\r\n\"ConcatV2Op<CPUDevice, ::tensorflow::int32>\",\r\n\"ConcatV2Op<CPUDevice, float>\",\r\n\"ConstantOp\",\r\n\"ExpandDimsOp\",\r\n\"FillOp<CPUDevice, float>\",\r\n\"UnaryOp< CPUDevice, functor::floor<float>>\",\r\n\"BinaryOp< CPUDevice, functor::safe_floor_mod<int32>>\",\r\n\"GatherOp<CPUDevice, float, int32>\",\r\n\"IdentityOp\",\r\n\"L2LossOp<CPUDevice, float>\",\r\n\"MatMulOp<CPUDevice, float, false >\",\r\n\"BinaryOp< CPUDevice, functor::minimum<float>>\",\r\n\"BinaryOp< CPUDevice, functor::mul<float>>\",\r\n\"UnaryOp< CPUDevice, functor::neg<float>>\",\r\n\"NoOp\",\r\n\"PackOp<CPUDevice, ::tensorflow::int32>\",\r\n\"PackOp<CPUDevice, float>\",\r\n\"PlaceholderOp\",\r\n\"IdentityOp\",\r\n\"IdentityOp\",\r\n\"PhiloxRandomOp<CPUDevice, random::UniformDistribution< random::PhiloxRandom, float> >\",\r\n\"BinaryOp< CPUDevice, functor::div<float>>\",\r\n\"ReshapeOp\",\r\n\"ScatterUpdateOp< CPUDevice, float, int32, scatter_op::UpdateOp::SUB>\",\r\n\"ShapeOp<int32>\",\r\n\"UnaryOp< CPUDevice, functor::sigmoid<float>>\",\r\n\"SimpleBinaryOp< CPUDevice, functor::sigmoid_grad<float>>\",\r\n\"SizeOp<int32>\",\r\n\"SliceOp<CPUDevice, ::tensorflow::int32>\",\r\n\"SliceOp<CPUDevice, float>\",\r\n\"SoftmaxOp<CPUDevice, float>\",\r\n\"SparseSoftmaxXentWithLogitsOp<CPUDevice, float, int32>\",\r\n\"SplitOpCPU<float>\",\r\n\"SplitVOpCPU<float, int32>\",\r\n\"UnaryOp< CPUDevice, functor::sqrt<float>>\",\r\n\"SqueezeOp\",\r\n\"StridedSliceOp<CPUDevice, ::tensorflow::int32>\",\r\n\"StridedSliceOp<CPUDevice, float>\",\r\n\"BinaryOp< CPUDevice, functor::sub<float>>\",\r\n\"ReductionOp<CPUDevice, float, Eigen::internal::SumReducer<float>>\",\r\n\"UnaryOp< CPUDevice, functor::tanh<float>>\",\r\n\"SimpleBinaryOp< CPUDevice, functor::tanh_grad<float>>\",\r\n\"TileOp<CPUDevice>\",\r\n\"TopK<float>\",\r\n\"UnpackOp<CPUDevice, float>\",\r\n\"VariableOp\",\r\n\"ZerosLikeOp< CPUDevice, float>\",\r\n\"RecvOp\",\r\n\"SendOp\",\r\n};\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\r\n\r\n#define SHOULD_REGISTER_OP_GRADIENT false\r\n#endif\r\n```\r\n\r\nHowever, when I try to run the executable on my Android device, it says some kernels are missed.\r\n\r\n```\r\nError creating graph: Invalid argument: No OpKernel was registered to support Op 'Placeholder' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: OnlineTraining/Model/Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[2000,400]]()]]\r\n```\r\n\r\nHow can I generate a complete file for this? If necessary I can also upload the `test.pb` file for testing.", "comments": ["@cwhipkey sorry to disturb you. Hopefully I can hear suggestions from you as I have seen you give answers to similar posts.", "I don't really see anything wrong with the header file that you are using.\n\nDoes test/test:test_run depend on the required kernel libaries?  One way to\nverify this is to change tensorflow/core/kernels/constant_op.cc to add a\ncompilation error around the REGISTER_KERNEL_BUILDER calls for Placeholder,\nthen build to see whether it gets rebuilt in that case.\n\n\nOn Mon, Jan 1, 2018 at 6:01 PM, Mengwei Xu <notifications@github.com> wrote:\n\n> @cwhipkey <https://github.com/cwhipkey> sorry to disturb you. Hopefully I\n> can hear suggestions from you as I have seen you give answers to similar\n> posts.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15732#issuecomment-354692892>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQw4wWmFCN8CU_2lPelWhX8ysigqECDpks5tGY4IgaJpZM4RPmUf>\n> .\n>\n", "Thanks @cwhipkey I think that I didn't include any kernel into the compilation. The current BUILD file is as follows\r\n\r\n```\r\ncc_binary(\r\n    name = \"librnn_dict_test\",\r\n    srcs = glob([\"utils/*.h\"])\r\n        + [\r\n            \"test_rnn.cc\",\r\n        ]\r\n        + if_android([\r\n            \"abort_wrapper.cc\",\r\n            \"rnn_dict_wrap.cxx\",\r\n            \"//tensorflow/core:android_op_registrations_and_gradients\",\r\n          ]),\r\n    copts = tf_copts() + [\r\n            \"-fexceptions\",\r\n            \"-DSUPPORT_SELECTIVE_REGISTRATION\",\r\n          ],\r\n    linkopts = [\"-lm\"] + if_android([\r\n        \"-pie\",\r\n        \"-landroid\",\r\n        \"-llog\",\r\n        \"-lz\",\r\n        \"-z defs\",\r\n        \"-Wl,--version-script\", LINKER_SCRIPT,\r\n        \"-Wl,--wrap=abort\",\r\n    ]),\r\n    deps = [\r\n            \"//xinmei/quantized_graph_loader:quantized_graph_proto_cc\",\r\n        ] + if_android([\r\n            \"@gemmlowp//:gemmlowp\",\r\n            \"//tensorflow/core:android_tensorflow_lib_selective_registration\",\r\n            LINKER_SCRIPT]),\r\n)\r\n```\r\n\r\nWhat I need to add to make it work?", "I think tensorflow/core/kernels:android_tensorflow_kernels might be enough.\n\nOn Tue, Jan 2, 2018 at 6:33 PM, Mengwei Xu <notifications@github.com> wrote:\n\n> Thanks @cwhipkey <https://github.com/cwhipkey> I think that I didn't\n> include any kernel into the compilation. The current BUILD file is as\n> follows\n>\n> cc_binary(\n>     name = \"librnn_dict_test\",\n>     srcs = glob([\"utils/*.h\"])\n>         + [\n>             \"test_rnn.cc\",\n>         ]\n>         + if_android([\n>             \"abort_wrapper.cc\",\n>             \"rnn_dict_wrap.cxx\",\n>             \"//tensorflow/core:android_op_registrations_and_gradients\",\n>           ]),\n>     copts = tf_copts() + [\n>             \"-fexceptions\",\n>             \"-DSUPPORT_SELECTIVE_REGISTRATION\",\n>           ],\n>     linkopts = [\"-lm\"] + if_android([\n>         \"-pie\",\n>         \"-landroid\",\n>         \"-llog\",\n>         \"-lz\",\n>         \"-z defs\",\n>         \"-Wl,--version-script\", LINKER_SCRIPT,\n>         \"-Wl,--wrap=abort\",\n>     ]),\n>     deps = [\n>             \"//xinmei/quantized_graph_loader:quantized_graph_proto_cc\",\n>         ] + if_android([\n>             \"@gemmlowp//:gemmlowp\",\n>             \"//tensorflow/core:android_tensorflow_lib_selective_registration\",\n>             LINKER_SCRIPT]),\n> )\n>\n> What I need to add to make it work?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15732#issuecomment-354926061>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQw4wbtsPb01Fxp13UNvNBk9GHvgJhTLks5tGucIgaJpZM4RPmUf>\n> .\n>\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "@xumengwei did that work for you?", "@drpngx @cwhipkey Sorry for the late response. Unfortunately adding tensorflow/core/kernels:android_tensorflow_kernels doesn't solve my problem. I work around this missing kernel issue by manually adding all needed //tensorflow/core/kernels/*.cc *.h files to srcs. Maybe I was wrong about some details, but I will update if I solve it more elegantly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I have the same issue...", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This is probably still an issue, but I figured out how to put this in the build scripts. It would be VERY nice thought if the selective thing would work.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 15731, "title": "Error while building from source on Ubuntu ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**:  3.6.3\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9.1\r\n- **GPU model and memory**: GTX 950 , 2GB\r\n- **Exact command to reproduce**:  `bazel build --config=opt --config=cuda --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n### Describe the problem\r\nThe following error occurring while building tensorflow from source. I'm not able to understand what the error is.\r\n\r\n### Source code / logs\r\n```\r\nERROR: /home/rakshith/Downloads/Tensorflow/tensorflow/tensorflow/contrib/boosted_trees/BUILD:419:1: Linking of rule '//tensorflow/contrib/boosted_trees:gen_gen_training_ops_py_py_wrappers_cc' failed (Exit 1)\r\n/usr/bin/ld: warning: libcublas.so.9.1, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Utraining_Uops_Upy_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)\r\n```\r\n", "comments": []}, {"number": 15730, "title": "Tensorflow Python3", "body": "i recently installed tensorflow in my linux machine with pip.\r\n\r\nwhen i try to import tensorflow it show me this \r\n```\r\nPython 3.6.4 (default, Dec 19 2017, 14:09:48) \r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n```\r\n\r\ni installed tensorflow cpu version with this command `pip install tensorflow`\r\n\r\nthis warning don't affect the script but its annoying\r\n\r\ncan anyone explain to me what is this\r\n**OS: Linux ubuntu**\r\n**Python Version: 3.6.4**", "comments": ["It's been discussed in #14182. You can find the solution [on StackOverflow](https://stackoverflow.com/q/47225210/712995).", "If you are trying to install tensorflow-gpu on python 3.6 and Ubuntu version 16.04, you can follow the step-by-step process in this blog. It worked for me and doesn't give any warnings whatsoever.\r\n\r\nhttp://www.python36.com/install-gpu-version-of-tensorflow/", "The blog post recommends installing tensorflow 1.0.0", "For tensorflow 1.4.1, check this out: http://www.python36.com/install-tensorflow141-gpu/"]}, {"number": 15729, "title": "Feature Request: 'msg' parameter for test cases.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Not relevant\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: Master rev. 3629fc4e98254c37e614ac3f77fa250b75c70f8d\r\n- **Python version**: 2/3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  Not relevant\r\n- **GPU model and memory**: Not relevant\r\n- **Exact command to reproduce**: Not relevant\r\n\r\n### Describe the problem\r\nPython's unittest module as well as numpy's testing tools allow to optionally pass a message to various assertion functions. I'd love to have this for all functions in tf.TestCase as well (quite a few already have this paramter). It allows for more descriptive error messages where many permutations of ops/dtype/cpu/gpu configurations are tested (e.g. [here](https://github.com/tensorflow/tensorflow/blob/3629fc4e98254c37e614ac3f77fa250b75c70f8d/tensorflow/python/kernel_tests/segment_reduction_ops_test.py#L109))\r\n\r\nAs many of the underlying testing functions already have a msg parameter this could easily be implemented, e.g.\r\n```\r\n  def assertAllClose(self, a, b, rtol=1e-6, atol=1e-6, msg=None):\r\n    ...\r\n      self.assertItemsEqual(\r\n          a.keys(), b.keys(),\r\n          msg=\"mismatched keys, expected %s, got %s\\n%s\" % (a.keys(), b.keys(), msg if msg else \"\"))\r\n      for k in a:\r\n        self._assertArrayLikeAllClose(\r\n            a[k], b[k], rtol=rtol, atol=atol,\r\n            msg=\"%s: expected %s, got %s.\\n%s\" % (k, a, b, msg if msg else \"\"))\r\n    else:\r\n      self._assertArrayLikeAllClose(a, b, rtol=rtol, atol=atol, msg=msg)\r\n```\r\n\r\nRelevant functions:\r\n- `assertAllClose`\r\n- `assertAllCloseAccordingToType`\r\n- `assertAllEqual`\r\n- `assertAlmostEqual`\r\n- `assertAlmostEquals`\r\n- `assertArrayNear`\r\n- `assertDeviceEqual`\r\n- `assertNDArrayNear`\r\n- `assertProtoEquals`\r\n- `assertProtoEqualsVersion`\r\n- (`assertRaises...`) adding a msg parameter to these test functions would probably break lot's of test cases, so I'd omit it\r\n- `assertShapeEqual`\r\n- `checkedThread`\r\n\r\n\r\nIf you agree, I'll submit a quick pull request.\r\n", "comments": ["@yifeif what do you think?\r\n", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "I think the tensorflowbutler was a bit overly motivated here ;)", "@martinwicke does this count as API change?", "Yes. But as long as the msg arg comes in the end it sounds reasonable. Adding API review for confirmation. ", "Yeah, the API addition seems reasonable. Contributions welcome.", "I submitted #16667 to address this ;)"]}, {"number": 15728, "title": "Clarify the description of batch_norm in order to highlight the dimen\u2026", "body": "\u2026sion selection for normalization.\r\n\r\nThis should work now.", "comments": ["Can one of the admins verify this patch?", "Thank you very much! I did not take the `data_format` into account. Now the description is more complete."]}, {"number": 15727, "title": "using tf.layers.batch_normalization() gives erratic validation loss though implementation seems correct.", "body": "I am trying to use Batch Normalization using [tf.layers.batch_normalization()][1] and I have followed the documentation closely. My code looks like this:\r\n\r\n<!-- language: python -->\r\n\r\n    def create_conv_exp_model(fingerprint_input, model_settings, is_training):\r\n      \r\n\r\n      # Dropout placeholder\r\n      if is_training:\r\n        dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\r\n\r\n      # Mode placeholder\r\n      mode_placeholder = tf.placeholder(tf.bool, name=\"mode_placeholder\")\r\n\r\n      he_init = tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\")\r\n\r\n      # Input Layer\r\n      input_frequency_size = model_settings['bins']\r\n      input_time_size = model_settings['spectrogram_length']\r\n      net = tf.reshape(fingerprint_input,\r\n                       [-1, input_time_size, input_frequency_size, 1],\r\n                       name=\"reshape\")\r\n      net = tf.layers.batch_normalization(net, \r\n                                          training=mode_placeholder,\r\n                                          name='bn_0')\r\n\r\n      for i in range(1, 6):\r\n        net = tf.layers.conv2d(inputs=net,\r\n                               filters=8*(2**i),\r\n                               kernel_size=[5, 5],\r\n                               padding='same',\r\n                               kernel_initializer=he_init,\r\n                               name=\"conv_%d\"%i)\r\n        net = tf.layers.batch_normalization(net,\r\n                                            training=mode_placeholder,\r\n                                            name='bn_%d'%i)\r\n        with tf.name_scope(\"relu_%d\"%i):\r\n          net = tf.nn.relu(net)\r\n        net = tf.layers.max_pooling2d(net, [2, 2], [2, 2], 'SAME', \r\n                                      name=\"maxpool_%d\"%i)\r\n\r\n      net_shape = net.get_shape().as_list()\r\n      net_height = net_shape[1]\r\n      net_width = net_shape[2]\r\n      net = tf.layers.conv2d( inputs=net,\r\n                              filters=1024,\r\n                              kernel_size=[net_height, net_width],\r\n                              strides=(net_height, net_width),\r\n                              padding='same',\r\n                              kernel_initializer=he_init,\r\n                              name=\"conv_f\")\r\n      net = tf.layers.batch_normalization( net, \r\n                                            training=mode_placeholder,\r\n                                            name='bn_f')\r\n      with tf.name_scope(\"relu_f\"):\r\n        net = tf.nn.relu(net)\r\n\r\n      net = tf.layers.conv2d( inputs=net,\r\n                              filters=model_settings['label_count'],\r\n                              kernel_size=[1, 1],\r\n                              padding='same',\r\n                              kernel_initializer=he_init,\r\n                              name=\"conv_l\")\r\n\r\n      ### Squeeze\r\n      squeezed = tf.squeeze(net, axis=[1, 2], name=\"squeezed\")\r\n\r\n      if is_training:\r\n        return squeezed, dropout_prob, mode_placeholder\r\n      else:\r\n        return squeezed, mode_placeholder\r\n\r\nAnd my train step looks like this:\r\n\r\n<!-- language: python -->\r\n\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    with tf.control_dependencies(update_ops):\r\n      optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_input)\r\n      gvs = optimizer.compute_gradients(cross_entropy_mean)\r\n      capped_gvs = [(tf.clip_by_value(grad, -2., 2.), var) for grad, var in gvs]\r\n      train_step = optimizer.apply_gradients(gvs))\r\n\r\nDuring training, I am feeding the graph with:\r\n\r\n<!-- language: python -->\r\n\r\n    train_summary, train_accuracy, cross_entropy_value, _, _ = sess.run(\r\n        [\r\n            merged_summaries, evaluation_step, cross_entropy_mean, train_step,\r\n            increment_global_step\r\n        ],\r\n        feed_dict={\r\n            fingerprint_input: train_fingerprints,\r\n            ground_truth_input: train_ground_truth,\r\n            learning_rate_input: learning_rate_value,\r\n            dropout_prob: 0.5,\r\n            mode_placeholder: True\r\n        })\r\n\r\nDuring validation, \r\n\r\n<!-- language: python -->\r\n\r\n    validation_summary, validation_accuracy, conf_matrix = sess.run(\r\n                    [merged_summaries, evaluation_step, confusion_matrix],\r\n                    feed_dict={\r\n                        fingerprint_input: validation_fingerprints,\r\n                        ground_truth_input: validation_ground_truth,\r\n                        dropout_prob: 1.0,\r\n                        mode_placeholder: False\r\n                    })\r\n\r\nMy loss and accuracy curves (orange is training, blue is validation):\r\n[Plot of loss vs number of iterations][2],\r\n[Plot of accuracy vs number of iterations][3]\r\n\r\nThe validation loss (and accuracy) seem very erratic. Is my implementation of Batch Normalization wrong? Or is this normal with Batch Normalization and I should wait for more iterations? Or maybe, moving statistics are not being saved and hence poor performance. I tried StackOverflow and found many people have the same problem and there is no definitive guide on how to resolve this.\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\r\n  [2]: https://i.stack.imgur.com/ZAqDw.png\r\n  [3]: https://i.stack.imgur.com/CYKJX.png", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15726, "title": "Batch support for deterministic image ops", "body": "Based on #14854 and working on #8926\r\n\r\nPreviously I had implemented batch support for a number of image ops. However performance concerns were raised and the changes were reverted.\r\n\r\nI have re-implemented the changes for:\r\n - `flip_left_right`\r\n- `flip_up_down`\r\n- `transpose_image`\r\n- `rot90`\r\n\r\nI ran performance tests from https://github.com/tensorflow/tensorflow/pull/15348 with:\r\n\r\n`bazel run -c opt //tensorflow/python:image_ops_test -- --benchmarks=FlipImageBenchmark`\r\n\r\n| Operation | Before (\u03bcs) | After (\u03bcs) |\r\n| --- | --- | --- |\r\n| benchmarkFlipLeftRight_299_299_3_/cpu:0_1 | 274.49  | 264.26 |\r\n| benchmarkFlipLeftRight_299_299_3_/cpu:0__all | 292.76 |  266.10 |\r\n| benchmarkFlipLeftRight_299_299_3___all | 273.80 |  265.71 |\r\n| *benchmarkRandomFlipLeftRight_299_299_3_/cpu:0_1 | 242.58  | 241.89 |\r\n| *benchmarkRandomFlipLeftRight_299_299_3_/cpu:0__all | 245.27 | 239.88 |\r\n| *benchmarkRandomFlipLeftRight_299_299_3___all | 252.71 | 241.20 |\r\n\r\n*\\* There were no changes made to `RandomFlipLeftRight in this PR*\r\n\r\nLet me know if you would like me to add more performance tests for the other methods. I don't think there should be any performance impact, but I'm happy to add more if you'd like.", "comments": ["/cc @martinwicke @jhseu \r\n\r\nI would still like to include support for the random flip ops as well, but I'll do that in another PR if this looks acceptable.", "@martinwicke could you take a look, please?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm sorry this took me so long. Could you fix the conflicts? I think it's just that proper name_scopes were added to all functions in this module, which is nice and should be easy to merge.", "@martinwicke I've merged the commits and I think I've done so properly. All the tests pass and there are still no performance regressions so it's probably ready for a code review.\r\n\r\nI also introduced `_AssertAtLeast3DImage()` in the style of `_Assert3DImage()` which had been added since I originally submitted this PR.", "Sanity complains about indentation: \r\n```\r\nFAIL: Found 5 non-whitelited pylint errors:\r\ntensorflow/python/ops/image_ops_impl.py:422: [C0330(bad-continuation), ] Wrong continued indentation (add 2 spaces).\r\n\r\ntensorflow/python/ops/image_ops_impl.py:428: [C0330(bad-continuation), ] Wrong continued indentation (add 3 spaces).\r\n\r\ntensorflow/python/ops/image_ops_impl.py:437: [C0301(line-too-long), ] Line too long (82/80)\r\n\r\ntensorflow/python/ops/image_ops_impl.py:458: [C0330(bad-continuation), ] Wrong continued indentation (add 3 spaces).\r\n\r\ntensorflow/python/ops/image_ops_test.py:1125: [C0330(bad-continuation), ] Wrong continued indentation (add 22 spaces).\r\n```", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Sorry for the delay fixing indentation. I forgot to run the sanity checks locally. :(\r\n\r\nThanks for taking care of it!"]}, {"number": 15725, "title": "Source Built r1.4 on GPUs with CPU optimized is always slower than 'no cpu optimization'", "body": "### System information\r\n- **Have I written custom code**: Yes, the model named PSIque [arXiv:1711.10644](https://arxiv.org/abs/1711.10644)\r\n- **OS Platform and Distribution**: CentOS 7.1/CentOS 7.3\r\n- **TensorFlow installed from**: source build w/ Bazel\r\n- **TensorFlow version**: 1.4\r\n- **Python version**: Anaconda 3.6.2\r\n- **Bazel version**: 0.8.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc version 4.8.3 20140911 (Red Hat4.8.3-9) (GCC)\r\n- **CUDA/cuDNN version**: CUDA 8.0/r375.26/cuDNN 6.0.0 & CUDA 9.0/r384.81/cuDNN 7.0.5 \r\n- **GPU model and memory**: E5-2660v3*2 Socket, K40m 12GB, P100-PCIE-16GB\r\n- **Exact command to reproduce**: python model.py\r\n\r\n### Describe the problem\r\n* I built tensorflow from source for boosting operation performance.\r\n\r\n* 6 different distributions were built;\r\n  * CPU only & No CPU optimization (NO EXTRA flags)\r\n  * CPU only & CPU optimization (--config=opt)\r\n  * GPU support & CUDA 8/9 & No CPU optimization (--config=cuda)\r\n  * GPU support & CUDA 8/9 & CPU optimization (--config=opt --config=cuda)\r\n\r\n* Experiments were basically done by 5 phases; experiments on CPU only are still going on,\r\nso please focus on GPU version results.\r\n\r\n* Results are quite frustrating me, because 'most of CPU optimized versions' gave me slow results.\r\n![results](https://user-images.githubusercontent.com/20734988/34451067-1d8c1cf0-ed5e-11e7-9a4f-3ff21a5c9aea.png)\r\n\r\n* Test were made on multiple machine with random order.\r\n  * P100: 2 nodes\r\n  * K40m: 7 nodes\r\n  * CPU only: 8 nodes\r\n\r\n* I am curious why CPU optimized version is slow\r\n  * on every experiment combinations\r\n  * even different GPU environments\r\n  * even Dual CPU socket (E5-2660v3)\r\n\r\n* (Extra) I believe my current model does not require high throughputs\r\n\r\n### tf_env_collect.sh\r\n== cat /etc/issue ===============================================\r\nLinux <hostname> 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.3 20140911 (Red Hat 4.8.3-9)\r\nCopyright (C) 2013 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux vis5 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.4.0)\r\ntensorflow (1.4.0)\r\ntensorflow-tensorboard (0.4.0rc3)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.4.0\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nSat Dec 30 12:39:08 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\r\n| N/A   28C    P0    35W / 250W |      0MiB / 16276MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla P100-PCIE...  On   | 00000000:82:00.0 Off |                    0 |\r\n| N/A   35C    P0    38W / 250W |  15661MiB / 16276MiB |     19%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    1     68169      C   python                                     15643MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/lib64/libcudart_static.a\r\n", "comments": ["Can you please reproduce this result with some of our tutorial/performance code? For example,\r\nhttps://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks", "Great! I'll have same 5-phase test and share the results. Thx for sharing a benchmark tool.", "Here's the result!\r\n![tf_cnn_benchmark](https://user-images.githubusercontent.com/20734988/34561299-b0adc1e6-f18c-11e7-956f-dfafc345f287.png)\r\n\r\nSince the 'tf-cnn-benchmark' requires nightly version and built version does not compatible with our system, I started from source build (and I believe this version might be dev. version of r1.5).\r\nTensorFlow branch: [6b6d843](https://github.com/tensorflow/tensorflow/tree/6b6d843ccab78f9f91c3b98a43ca09ffecad4747)\r\nBenchmark branch: [ecf4e0b](https://github.com/tensorflow/benchmarks/tree/ecf4e0b474f6f889c6d97b924b0c469decd4f4f9)\r\n\r\n- Model: ResNet-50\r\n- NumGpus: 1\r\n- Batch Size: 128\r\n- Batches: 100\r\n\r\nAs far as I know, it's hard to find difference between cpu w/ opt and w/o optimization.\r\nSo CPU optimization is mandatory or not?\r\nI hope this results are helpful to you @bignamehyp !", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "So it seems you could not reproduce the problem on `tf_cnn_benchmarks`, but you could PSIque. I'm having trouble reading the table: which columns correspond to Cuda 8 and Cuda 9 and CPU only?\r\n\r\n@tfboyd any ideas what the problem could be, or who else could have any ideas?", "My testing with AVX vs SSE3 was on K80 and found the following:\r\n\r\n- For synthetic data tests that is almost no difference between SSE3 and AVX2 until 8 GPUs and usually more noticeable with real data. \r\n- For single GPU, for the models tested.  VGG16, ResNet50, AlexNet, and InceptionV3 the difference was less than 2% until 8 GPUs.\r\n- The biggest difference was AlexNet with SSE3 being 15-18% slower on 8xK80 on a p2.8xlarge AWS instance.  \r\n\r\nThe results may be different today with improvements to the data pipeline.  When doing GPU training the CPU is mostly only handling the input pipeline.  The input pipeline can benefit from having code optimized for the CPU but it may not be visible.  In the testing above my hypothesis was that because AlexNet pushes the input pipeline the difference becomes visible.  That theory may not be perfect.  TensorFlow will go AVX by default in 1.6 and likely in nightlies in a day or so.  I will try to remember to report back if I see a bump for SSE3 to AVX for the some nightly tests. "]}, {"number": 15724, "title": "How to register all kernels to Android lib", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.0.1\r\n- **Python version**: 3.4\r\n- **Bazel version (if compiling from source)**: 0.4\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI am trying to compile a Android lib which can load a MetaGraph into a session as [this link](https://stackoverflow.com/questions/35508866/tensorflow-different-ways-to-export-and-run-graph-in-c/43639305#43639305) specifies. I first extracts a GraphDef from this MetaGraph and uses this GraphDef to generate `ops_to_register.h`. Then I compile the lib as \r\n\r\n`bazel build my_model/test:test_lib --copt=-DSELECTIVE_REGISTRATION  --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a`\r\n\r\nHowever when I trying to run the test_lib, it complains some kernels are not registered. Then I brutally adds all kernels to be registered as\r\n\r\n`#define SHOULD_REGISTER_OP_KERNEL(clz) true`\r\n\r\nHowever, it still complains\r\n\r\n```\r\nError creating graph: Invalid argument: No OpKernel was registered to support Op 'Const' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: save/RestoreV2_8/shape_and_slices = Const[_output_shapes=[[1]], dtype=DT_STRING, value=Tensor<type: string shape: [1] values: >]()]]\r\n```\r\n\r\nHow can I solve this by adding this Const kernel or simply registering all kernels?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15723, "title": "Load region from `~/.aws/config` if possible in S3", "body": "This fix tries to address the issue raised in https://github.com/tensorflow/tensorflow/issues/15662#issuecomment-354272697 where TensorFlow does not load region from `~/.aws/config` if exists. The reason was that AWS C++ SDK does not use the config file by default.\r\n\r\nThis fix adds the loading of config file (`~/.aws/config`) explicitly, if either AWS_REGION or S3_REGION is not available. In case none of the `AWS_REGION`, `S3_REGION`, `~/.aws/config` is available, then the default `use-east-1` is used (by AWS C++ SDK).\r\n\r\nThis fix is related to #15562.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Not sure about this. Generally it's not a great idea to silently read configs outside of the regular environment. It makes things hard to debug.", "I agree with @drpngx , especially if the location of the config file isn't something that is baked into the AWS SDK to begin with.\r\n\r\nBut then again, I know very little about the AWS SDK and the idioms there. Perhaps @jhseu would have a more informed opinion.", "From Official SDK Docs https://sdk.amazonaws.com/cpp/api/LATEST/index.html\r\n> Default Credential Provider Chain\r\n> The default credential provider chain does the following:\r\n> Checks your environment variables for AWS Credentials\r\n> Checks your $HOME/.aws/credentials file for a profile and credentials\r\n>  Contacts the ECS TaskRoleCredentialsProvider service to request credentials if Environment variable AWS_CONTAINER_CREDENTIALS_RELATIVE_URI has been set. Otherwise contacts the EC2MetadataInstanceProfileCredentialsProvider service to request credentials\r\n> **The simplest way to communicate with AWS is to ensure we can find your credentials in one of these locations.**\r\n\r\nSo probably we could use `Aws::Auth::AWSCredentialsProviderChain`\r\n", "@bhack The credential information for AWS is stored in `~/.aws/credentials` and is picked up by AWS C++ SDK by default (`DefaultAWSCredentialsProviderChain`). So the credentials will not be an issue.\r\n\r\nThe region information for AWS is optionally stored in `~/.aws/config` and the `region` information is not part of the `credentials`.\r\n\r\nFor AWS C++ SDK, the region information will not be picked up, unless explicitly loaded through `AWSConfigFileProfileConfigLoader` (could load either `~/.aws/config` or `~/.aws/credentials`).", "So with the `else` you are trying to mimic the Chain for the region field.", "I'd prefer not to add this file read, particularly since it would have to be installed on every machine in a distributed setting.", "It looks like we're already opening files for credentials for GCS, so it probably wouldn't be too bad to also add it here. Looking at some documentation, it looks like it checks for an environment variable first before loading the config. Could we do the same here?\r\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-region.html#setting-region-config-file", "@jhseu Thanks for the suggestion. The PR has been updated. Now config file for region (`~/.aws/config`) is only used  if `AWS_SDK_LOAD_CONFIG` is set, and the location of the config file could also be overwritten by `AWS_CONFIG_FILE`. Please take a look."]}, {"number": 15721, "title": "Is it possible to have labels for outputs( I found it on the documentation)", "body": "For example, currently if I want to run a graph with multiple output:\r\n```\r\nsession.run([output1, output2], feed_dict = feed_dict)\r\n```\r\nwhich return a list of results. I would like to have a label for each result, so the code will look like:\r\n```\r\nsession.run({label1: output1, label2: output2}, feed_dict = feed_dict)\r\n```\r\nwhich return a dictionary maps labels to results, so I can get specific result based on the output label.", "comments": []}, {"number": 15720, "title": "How to remove unwanted warning while using GPU with tensorflow", "body": "### System information\r\n- I am using tensorflow version :\r\n- OS Platform and Distribution: Linux64-:\r\n- TensorFlow installed from pip version 1.4.1\r\n- Python 3.6.3 :: Anaconda custom (64-bit) \r\n- Cuda-8.0\r\n- GPU model and memory: nVIDIA K20 (Kepler)\r\n\r\n### Describe the problem\r\nI am using Adam Optimizer with a normal sequential network created via keras using tensorflow as backend. \r\n\r\n\r\nI get the following logs repeatedly for fitting, and creating the network. I also applied batch-normalization for the Dense Layer\r\n\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K20Xm, pci bus id: 0000:84:00.0, compute capability: 3.5\r\nAdam_2/decay: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\r\nIsVariableInitialized_14: (IsVariableInitialized): /job:localhost/replica:0/task:0/device:GPU:0\r\nAdam_2/decay/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\r\nAdam_2/decay/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\r\nAdam_2/beta_2: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\r\nIsVariableInitialized_13: (IsVariableInitialized): /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\ntraining/Adam/gradients/batch_normalization_3_1/batchnorm/mul_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\ntraining/Adam/gradients/batch_normalization_3_1/batchnorm/mul_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\ntraining/Adam/gradients/batch_normalization_3_1/moments/Squeeze_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\ntraining/Adam/gradients/zeros_87/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\nHow to turn off these unwanted logs? \r\nI have already applied the following solution for switching off the warning logs from the tensorflow.\r\nhttps://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information\r\n\r\n### Source code \r\nfrom keras.layers.normalization import BatchNormalization\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(Dense(64, input_dim=14, init='relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\n\r\nmodel.add(Dense(64, init='uniform'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\n\r\nmodel.add(Dense(1, init='uniform'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('softmax'))\r\n\r\nmodel.compile(loss='binary_crossentropy', optimizer=Adam())\r\n\r\nmodel.fit(X_train, y_train)", "comments": ["@vrv can you take a look or redirect to someone who can? Thanks.", "Added XQ who can redirect to someone who knows how the GPU info logging should work.", "Closing due to staleness. Please check with the latest version of TensorFlow. Feel free to reopen if the issue still persists. Thanks!"]}, {"number": 15719, "title": "Fixed a typo for CreateBody()", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 15718, "title": "MKL: update mkldnn to the latest release", "body": "This commit will pull the latest changes from the mkl-dnn tree.\r\n\r\nthe mirror.bazel.build URL doesn't exist: \"https://mirror.bazel.build/github.com/01org/mkl-dnn/archive/e0bfcaa7fcb2b1e1558f5f0676933c1db807a729.tar.gz\" can you create it?", "comments": ["@gunan  @drpngx have closed the pr#15707 and open this one", "Jenkins, test this please.", "Can one of the admins verify this patch?"]}, {"number": 15717, "title": "Performance issues when multiplying constant matrices", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, the code sample is provided below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Installed from official wheel\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**:  Python 3.6.3 | packaged by conda-forge | (default, Dec  9 2017, 16:18:26) \r\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**:  8.0/6.0.21\r\n- **GPU model and memory**: GTX 1070, 8GB\r\n- **Exact command to reproduce**: run the provided code sample\r\n\r\n### Describe the problem\r\n_I think this is a bug or an unclear performance issue. I also posted on StackOverflow to check if it was a known issue before posting, but not getting replies and I don't think it's a support problem._\r\n\r\nI'm using Tensorflow for some non-DL computation, and I'm running into a behaviour I don't understand. I am testing the multiplication of a square matrix by itself: tf.matmul(a, a):\r\n\r\n1. when the matrix is created with tf.constant\r\n2. when the matrix is randomly initialized at each run\r\n\r\nMy expectation is that the first case should have some overhead for transferring the initial data, 100 MB (5000x5000 matrix using float32) but then the execution of the second case should be slightly slower due to the random initialization at each run.\r\n\r\nHowever, what I see is that the multiplication of the constant is much slower even on successive runs in the same session.\r\n\r\nBelow I include logs generated on different GPUs: it seems that on lower-level GPUs (K1100M, GTX 940MX) constant multiplication is faster or the same, while on newer GPUs (GTX 1070, Tesla P100) it's slower. Details included in the logs.\r\n\r\n### Source code \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom timeit import timeit\r\nimport os\r\n\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"2\"\r\nSIZE = 5000\r\nNUM_RUNS = 10\r\n\r\na = np.random.random((SIZE, SIZE))\r\n_const_a = tf.constant(a, dtype=tf.float32, name=\"Const_A\")\r\n_mul_const_a = tf.matmul(_const_a, _const_a, name=\"Mul_Const\")\r\n\r\n_random_a = tf.random_uniform((SIZE, SIZE), dtype=tf.float32, name=\"Random_A\")\r\n_mul_random_a = tf.matmul(_random_a, _random_a, name=\"Mul_Random\")\r\n\r\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as s:\r\n    # Run once to make sure everything is initialised\r\n    s.run((_const_a, _mul_const_a, _random_a, _mul_random_a))\r\n\r\n    # timeit\r\n    print(\"TF with const\\t\", timeit(lambda: s.run((_mul_const_a.op)), number=NUM_RUNS))\r\n    print(\"TF with random\\t\", timeit(lambda: s.run((_mul_random_a.op)), number=NUM_RUNS))\r\n\r\n```\r\n### Logs: I have accurate environment details only for the GTX 1070 and the P100, as reported above. \r\n\r\n#### GTX 1070 X (multiplying constants is much slower)\r\n```\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\r\nRandom_A/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\nMul_Random: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nMul_Const: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nConst_A: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nTF with const    2.9953213009994215\r\nTF with random   0.513827863998813\r\n```\r\n\r\n#### Tesla P100 (multiplying constants is much slower)\r\n```\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\r\n/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:05.0, compute capability: 6.0\r\nRandom_A/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\nMul_Random: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nMul_Const: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nConst_A: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nTF with const     1.5770663949660957\r\nTF with random     0.32687677699141204\r\n\r\n```\r\n#### K1100M (multiplying constants is much faster. But I am not sure which version of TF this was run with)\r\n```\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Quadro K1100M, pci bus id: 0000:01:00.0, compute capability: 3.0\r\nRandom_A/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\nMul_Random: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nMul_Const: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nConst_A: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nTF with const    4.3167382130868175\r\nTF with random   9.889055849542306\r\n```\r\n\r\n#### GTX 940 MX (multiplying constants is slightly slower)\r\n```\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0\r\nRandom_A/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nRandom_A/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/gpu:0\r\nRandom_A/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nRandom_A: (Add): /job:localhost/replica:0/task:0/gpu:0\r\nMul_Random: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nMul_Const: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nRandom_A/max: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nRandom_A/min: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nRandom_A/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nConst_A: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nTF with const    3.5542741210010718\r\nTF with random   3.519956939999247\r\n\r\n\r\n```", "comments": ["@yzhwang ", "I did a bit more research on this.\r\n\r\nIf you substitute\r\n\r\n`s.run((_const_a, _mul_const_a, _random_a, _mul_random_a))`\r\n\r\nwith \r\n\r\n```\r\n    s.run((_mul_const_a.op))\r\n    s.run((_mul_random_a.op))\r\n```\r\n\r\nThe overhead disappears and constant multiplication is much faster. However, if you run:\r\n\r\n`s.run((_mul_const_a.op, _mul_random_a.op))`\r\n\r\nthen again constant multiplication exhibits the overhead. \r\n\r\nIt seems that, while the two parts of the graph are disjoint, retrieving nodes from different portions of the graph causes them to be re-initialized from the source, i.e. transferring again from the initialization data from the CPU to the GPU. \r\n\r\nHow can that be explained? Doesn't seem like a desireable feature.\r\n  ", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Hi @acbellini , I modified your code a bit to get more info. Changed the NUM_RUNS to 100, added four tests: one session run for both const op and random op, two session runs for const op and random op, also session run for each op separately:\r\n\r\n    def combined_run():\r\n        s.run((_mul_const_a.op, _mul_random_a.op))\r\n    def separate_run():\r\n        s.run((_mul_random_a.op))\r\n        s.run((_mul_const_a.op))\r\n    def const_run():\r\n        s.run((_mul_const_a.op))\r\n    def random_run():\r\n        s.run((_mul_random_a.op))\r\n    # timeit\r\n    print(\"run with separate const and random\\t\", timeit(lambda: separate_run(), number=NUM_RUNS))\r\n    print(\"run with combined const and random\\t\", timeit(lambda: combined_run(), number=NUM_RUNS))\r\n\r\n    print(\"run with const only\\t\", timeit(lambda: const_run(), number=NUM_RUNS))\r\n    print(\"run with random only\\t\", timeit(lambda: random_run(), number=NUM_RUNS))\r\n\r\nI'm using TensorFlow 1.4.1 with CUDA 8. I haven't been able to reproduce your results on both GTX 1080 and P100. Here are my results:\r\n\r\nGTX 1080:\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080, pci bus id: 0000:84:00.0, compute capability: 6.1\r\nRandom_A/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\nMul_Random: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nMul_Const: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nConst_A: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nrun with separate const and random\t 5.317255422996823\r\nrun with combined const and random\t 5.299742974006222\r\nrun with const only\t 0.02027931998600252\r\nrun with random only\t 3.677839469004539\r\n\r\nP100:\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0\r\nRandom_A/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\nMul_Random: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nMul_Const: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nRandom_A/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nConst_A: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n('run with separate const and random\\t', 4.535215854644775)\r\n('run with combined const and random\\t', 4.544884204864502)\r\n('run with const only\\t', 0.008514881134033203)\r\n('run with random only\\t', 2.8619189262390137)\r\n\r\nFor me run with const op only shows much better performance, which meets the expectation. Could there be any other factor that affect the tests by your side?", "Hi @yzhwang. \r\n\r\nIf you change the code it's no longer the same test code, right? So it can be expected that you don't get the same results. What results do you get running the same code that I used? \r\n\r\nIf you check my comment from Jan 8, I had already nailed down the problem to a duplicate initialization of the constant if the graph query is different. ", "Hi @acbellini ,\r\nI tried with your original reproducer, and yes, with NUM_RUNS set to 10, I get the same results:\r\nTF with const\t 1.5651498469960643\r\nTF with random\t 0.3725004669977352\r\n\r\nI then changed to NUM_RUNS to 1, and 100 respectively, running time of Mul_Const is constant, while running time of Mul_Random increases linearly as NUM_RUNS increases:\r\nNUM_RUNS=1:\r\nTF with const\t 1.6104715690016747\r\nTF with random\t 0.04290775100525934\r\nNUM_RUNS=100:\r\nTF with const\t 1.625495141997817\r\nTF with random\t 3.7049192739941645\r\nNUM_RUNS=1000:\r\nTF with const\t 1.6929292179993354\r\nTF with random\t 37.42843811100465\r\n\r\nI think this shows that Mul_Const performance is invariant to number of session runs.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 108 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 123 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Whenever you run a session with a new combination of fetches for the first time, there is a lot of extra work in initializing data structures and optimizing the graph. Also, IIRC, you are correct in that the constant is retransferred from the CPU to the GPU for every new combination of fetches (although that may have been fixed by now).\r\n\r\nSo, when timing a session.run call, make sure you previously ran with the exact same fetches. That is why your problem went away when running `s.run((_mul_const_a.op))` and `s.run((_mul_random_a.op))` as two separate session.run() calls, but you still had the problem when running those two ops as part of a single call."]}, {"number": 15716, "title": "ImportError: No module named contracts", "body": "", "comments": []}, {"number": 15715, "title": "ValueError: Batch length of predictions should be same", "body": "Hi,\r\n\r\nI'm trying to visualize the output of a convolutional autoencoder using TensorFlow Estimator API.\r\nI use 64*64 images stored in a Numpy array as input, and use `tf.estimator.inputs.numpy_input_fn` to feed this input to my estimator.\r\nEverything works perfectly fine during training, but as soon as I try to do predictions, it seems that I have an issue with my input_fn.\r\nPlease forgive me in advance, I am not 100% sure this is a bug, but I think I have tried everything I had in mind.\r\n\r\n```\r\ninput_fn = tf.estimator.inputs.numpy_input_fn(\r\n    x={\"x\": train_data},\r\n    y=None,\r\n    batch_size=8,\r\n    num_epochs=None,\r\n    shuffle=True)\r\n\r\nautoencoder = tf.estimator.Estimator(model_fn=autoencoder_model_fn, model_dir=model_dir)\r\ntensors_to_log = {\"loss\": \"loss\"}\r\nlogging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=1000)\r\n\r\nautoencoder.train(\r\n    input_fn=input_fn,\r\n    steps=50000,\r\n    hooks=[logging_hook])\r\n\r\ninput_fn_predict = tf.estimator.inputs.numpy_input_fn(\r\n    x={\"x\": train_data},\r\n    y=None,\r\n    batch_size=1,\r\n    num_epochs=None,\r\n    shuffle=False)\r\npredictions = autoencoder.predict(input_fn=input_fn_predict)\r\npredictions = [p['decoded_image'] for p in predictions]\r\nprint predictions[0].shape\r\n```\r\nI tried among other things to set `batch_size` to the same value for both input_fn, but it doesn't work and I get the following error : \r\n```bash \r\nTraceback (most recent call last):\r\n  File \"AutoEncoder.py\", line 164, in <module>\r\n    main()\r\n  File \"AutoEncoder.py\", line 157, in main\r\n    predictions = [p['decoded_image'] for p in predictions]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 425, in predict\r\n    for i in range(self._extract_batch_length(preds_evaluated)):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 592, in _extract_batch_length\r\n    'different batch length then others.' % key)\r\nValueError: Batch length of predictions should be same. features has different batch length then others.\r\n```\r\n\r\nCan anybody see what's wrong?\r\nThanks in advance :) ", "comments": ["Same here. Wonder if there is any solution towords it. Tks", "The batch_size of function input_fn and input_fn_predict should be the same.\r\nThis works for me."]}, {"number": 15714, "title": "Optimizing code and adding from http to https", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks! Changes are reverted as per your suggestion. Can we go ahead :)", "Jenkins, test this please."]}, {"number": 15713, "title": "getting latest pull", "body": "", "comments": []}, {"number": 15712, "title": "Removing extra space, preventing double declared \"if\" statement and from http to https", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}]