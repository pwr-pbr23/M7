[{"number": 33317, "title": "no attribute 'ConfigProto' & no attribute '_MTCNN__session'", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pycharm addons\r\n- TensorFlow version: 2.0.0\r\n- Python version:3.7.2\r\n- Installed using virtualenv? pip? conda?:pip\r\n\r\n\r\nRookie from China, not a native speaker. Working on tutorials at 'https://machinelearningmastery.com/how-to-perform-face-recognition-with-vggface2-convolutional-neural-network-in-keras/'. \r\nProblem is that I couldn't install the previous tensorflow version in >Project Interpreter>package of Pycharm no matter how I tried to install the other packages related with tensorflow. So that I installed version 2.0.0 which I've known that version 2.0.0 will not work as expected as is mentioned in that tutorial. \r\nKeyed those first part '**How to Detect Faces for Face Recognition**' codes. Here are the Erorrs:\r\nAttributeError: module 'tensorflow' has no attribute 'ConfigProto'\r\nException ignored in: <function MTCNN.__del__ at 0x00000199DEC60B70>\r\nTraceback (most recent call last):\r\n  File \"C:\\PycharmProjects\\VGG\\venv\\lib\\site-packages\\mtcnn\\mtcnn.py\", line 616, in __del__\r\n    self.__session.close()\r\nAttributeError: 'MTCNN' object has no attribute '_MTCNN__session'\r\n\r\nThen I found ways to solve this problem but couldn't quite understand which push me to post this issue of course the first time posting things.\r\n\r\nAny suggestions and help are warmly welcomed. Thanks a lot.\r\n\r\nKym\r\n", "comments": ["@kyimin11 \r\n\r\nAs per my understanding you have installed TF 2.0.0. But you want to install previous TF versions.Please, let us know where exactly you are facing the issue with tensorflow. It will be great if you share  simple standalone code, then it is easy for localizing the issue faster. Thanks again.!", "I could successfully run mtcnn (https://github.com/ipazc/mtcnn) with TF 2.0.0. Please check the version of mtcnn. The latest version of mtcnn is 0.0.9 that can work with TF 2.0.0."]}, {"number": 33316, "title": "With lazy_import tensorflow cannot be imported", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 (and gLinux)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version: 1.15.0.{rc1,rc2,rc3}\r\n- Python version: python 3.6.8\r\n- Installed using virtualenv? pip? conda?: anaconda\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n**Describe the problem**\r\n\r\nThe following works correctly:\r\n\r\n```bash \r\n$ python -c 'import tensorflow as tf; print(tf.__version__)'\r\n1.15.0-rc3\r\n```\r\n\r\nIn an interactive shell:\r\n\r\n```python\r\nimport tensorflow as tf\r\n```\r\n\r\nI get the following error: `ValueError: operator __getitem__ cannot be overwritten again on class <class 'tensorflow.python.framework.ops.Tensor'>`.\r\n\r\n**This happens when [`lazy_import`](https://github.com/mnmelo/lazy_import) is installed.** I think tensorflow's internal lazy_import has a conflict with some external library. TF 1.14 has no problem.\r\n\r\nThough lazy_import is a third-party library, I think what it is doing is pretty non-hacky. Since this bug has appeared since 1.15.dev, we will need to have this fixed in the release version (given that 1.15 will be the last 1.x release).\r\n\r\nTraceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow/__init__.py\", line 99, in <module>\r\n    from tensorflow_core import *\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 34, in <module>\r\n    from tensorflow._api.v1 import autograph\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/_api/v1/autograph/__init__.py\", line 22, in <module>\r\n    from tensorflow._api.v1.autograph import experimental\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/_api/v1/autograph/experimental/__init__.py\", line 10, in <module>\r\n    from tensorflow.python.autograph.core.converter import Feature\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/autograph/__init__.py\", line 35, in <module>\r\n    from tensorflow.python.autograph import operators\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.autograph.operators.control_flow import for_stmt\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py\", line 65, in <module>\r\n    from tensorflow.python.autograph.operators import py_builtins\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/py_builtins.py\", line 28, in <module>\r\n    from tensorflow.python.autograph.utils import py_func\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/autograph/utils/__init__.py\", line 21, in <module>\r\n    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/autograph/utils/context_managers.py\", line 24, in <module>\r\n    from tensorflow.python.ops import tensor_array_ops\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 35, in <module>\r\n    from tensorflow.python.ops import array_ops\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\", line 1045, in <module>\r\n    ops.Tensor._override_operator(\"__getitem__\", _slice_helper)\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 745, in _override_operator\r\n    _override_helper(Tensor, operator, func)\r\n  File \"/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 177, in _override_helper\r\n    (operator, clazz_object))\r\nValueError: operator __getitem__ cannot be overwritten again on class <class 'tensorflow.python.framework.ops.Tensor'>.\r\n```", "comments": ["I cannot reproduce:\r\n\r\n```\r\n(gh_lazy_import) mihaimaruseac@ankh:/tmp/gh_lazy_import$ pip install lazy_import tensorflow==1.15.0rc3\r\n...\r\nSuccessfully installed absl-py-0.8.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.24.1 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 lazy-import-0.2.2 markdown-3.1.1 numpy-1.17.2 opt-einsum-3.1.0 protobuf-3.10.0 six-1.12.0 tensorboard-1.15.0 tensorflow-1.15.0rc3 tensorflow-estimator-1.15.1 termcolor-1.1.0 werkzeug-0.16.0 wrapt-1.11.2\r\n(gh_lazy_import) mihaimaruseac@ankh:/tmp/gh_lazy_import$ python\r\nPython 3.6.8 (default, Jan  3 2019, 03:42:36) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> \r\n```", "Oh, that's interesting. Let me try to figure out how to reproduce it consistently.", "@mihaimaruseac Did you by any chance try importing lazy_import?\r\n\r\n```\r\nimport lazy_import\r\ntf = lazy_import.lazy_module(\"tensorflow\")\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n```", "Tried, in 3 different ways\r\n\r\n```\r\n(gh_lazy_import) mihaimaruseac@ankh:/tmp/gh_lazy_import$ python\r\nPython 3.6.8 (default, Jan  3 2019, 03:42:36) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import lazy_import\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.15.0-rc3'\r\n>>> \r\n(gh_lazy_import) mihaimaruseac@ankh:/tmp/gh_lazy_import$ python\r\nPython 3.6.8 (default, Jan  3 2019, 03:42:36) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import lazy_import\r\n>>> tf1 = lazy_import.lazy_module(\"tensorflow\")\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.15.0-rc3'\r\n>>> tf1.__version__\r\n'1.15.0-rc3'\r\n>>> \r\n(gh_lazy_import) mihaimaruseac@ankh:/tmp/gh_lazy_import$ python\r\nPython 3.6.8 (default, Jan  3 2019, 03:42:36) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import lazy_import\r\n>>> tf1 = lazy_import.lazy_module(\"tensorflow\")\r\n>>> import tensorflow as tf\r\n>>> tf1.__version__\r\n'1.15.0-rc3'\r\n>>> tf.__version__\r\n'1.15.0-rc3'\r\n>>> \r\n```", "I tired all of the code above and they are indeed working as expected (printing version without the problem). Sorry about confusing. I found that it was actually from another lazy_import of **numpy**, specifically, when numpy is yet to be fully loaded:\r\n\r\n```python\r\nimport lazy_import\r\nnp = lazy_import.lazy_module(\"numpy\")\r\nimport tensorflow as tf\r\n# gives ValueError: operator __getitem__ cannot be overwritten again on class <class 'tensorflow.python.framework.ops.Tensor'>.\r\n```\r\n\r\nI can reproduce this on 3 different environments including glinux and macOS, both anaconda and brew python.\r\n\r\nNot sure tensorflow is to blame or lazy_import is, but let me now look into why it happens.", "Confirmed that it reproduces.\r\n\r\nFollowing https://github.com/tensorflow/tensorflow/issues/32159#issuecomment-542452574 I might get rid of `__getitem__` in one lazy import path. Let's see if that also solves this.", "Can you try again please? There has been some code changes on that code path.", "@mihaimaruseac : It's still a problem. I'm using tensorflow 2.2.0.\r\nSomehow you cannot lazy import numpy when tensorflow is used:\r\n\r\n```python\r\nimport lazy_import\r\nnp = lazy_import.lazy_module(\"numpy\")\r\nimport tensorflow as tf\r\n```\r\n\r\nthrows\r\n```python\r\nTraceback (most recent call last):\r\n  File \"test2.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/koepke/.local/share/virtualenvs/tensorflow-0PogEeTj/lib/python3.7/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/home/koepke/.local/share/virtualenvs/tensorflow-0PogEeTj/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 64, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"/home/koepke/.local/share/virtualenvs/tensorflow-0PogEeTj/lib/python3.7/site-packages/tensorflow/python/framework/framework_lib.py\", line 52, in <module>\r\n    from tensorflow.python.framework.importer import import_graph_def\r\n  File \"/home/koepke/.local/share/virtualenvs/tensorflow-0PogEeTj/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 28, in <module>\r\n    from tensorflow.python.framework import function\r\n  File \"/home/koepke/.local/share/virtualenvs/tensorflow-0PogEeTj/lib/python3.7/site-packages/tensorflow/python/framework/function.py\", line 36, in <module>\r\n    from tensorflow.python.ops import array_ops\r\n  File \"/home/koepke/.local/share/virtualenvs/tensorflow-0PogEeTj/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1428, in <module>\r\n    _NON_AUTOPACKABLE_TYPES = set(np.core.numerictypes.ScalarType)\r\nAttributeError: module 'numpy.core' has no attribute 'numerictypes'\r\n```\r\n\r\nDo note that\r\n```python\r\nimport lazy_import\r\nnp = lazy_import.lazy_module(\"numpy\")\r\nprint(np.core.numerictypes)\r\n```\r\nand\r\n```python\r\nimport lazy_import\r\nnp = lazy_import.lazy_module(\"numpy\")\r\nprint(np.core.numerictypes.ScalarType)\r\n```\r\nboth work as expected.", "I am unsure if this is a TF issue. The codepaths that caused me to initially think this has been an issue are gone in TF 2.2\r\n\r\nNow that we dropped support for Python <3.5, we can try converting our code to use Python's lazy loaders instead of a custom one, maybe that would solve the issue.\r\n\r\nThough I don't think I'll be able to work on this until at least mid of August", "We now dropped support for Python3.5 too, so we should be able to convert to Python's lazy loaders instead of using custom ones. PRs welcome", "Still the same\r\n\r\n```python3\r\nPython 3.8.7 (default, Jan 20 2021, 00:00:00) \r\n[GCC 10.2.1 20201125 (Red Hat 10.2.1-9)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import lazy_import\r\n>>> np = lazy_import.lazy_module(\"numpy\")\r\n>>> import tensorflow as tf\r\n2021-02-01 22:07:07.217289: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-02-01 22:07:07.217314: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/koepke/.local/share/virtualenvs/test-RD8o3laV/lib64/python3.8/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/home/koepke/.local/share/virtualenvs/test-RD8o3laV/lib64/python3.8/site-packages/tensorflow/python/__init__.py\", line 46, in <module>\r\n    from tensorflow.python import data\r\n  File \"/home/koepke/.local/share/virtualenvs/test-RD8o3laV/lib64/python3.8/site-packages/tensorflow/python/data/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.data import experimental\r\n  File \"/home/koepke/.local/share/virtualenvs/test-RD8o3laV/lib64/python3.8/site-packages/tensorflow/python/data/experimental/__init__.py\", line 96, in <module>\r\n    from tensorflow.python.data.experimental import service\r\n  File \"/home/koepke/.local/share/virtualenvs/test-RD8o3laV/lib64/python3.8/site-packages/tensorflow/python/data/experimental/service/__init__.py\", line 140, in <module>\r\n    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\r\n  File \"/home/koepke/.local/share/virtualenvs/test-RD8o3laV/lib64/python3.8/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 25, in <module>\r\n    from tensorflow.python.data.experimental.ops import compression_ops\r\n  File \"/home/koepke/.local/share/virtualenvs/test-RD8o3laV/lib64/python3.8/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py\", line 20, in <module>\r\n    from tensorflow.python.data.util import structure\r\n  File \"/home/koepke/.local/share/virtualenvs/test-RD8o3laV/lib64/python3.8/site-packages/tensorflow/python/data/util/structure.py\", line 33, in <module>\r\n    from tensorflow.python.ops import tensor_array_ops\r\n  File \"/home/koepke/.local/share/virtualenvs/test-RD8o3laV/lib64/python3.8/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 38, in <module>\r\n    from tensorflow.python.ops import array_ops\r\n  File \"/home/koepke/.local/share/virtualenvs/test-RD8o3laV/lib64/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 1498, in <module>\r\n    _NON_AUTOPACKABLE_TYPES = set(np.core.numerictypes.ScalarType)\r\nAttributeError: module 'numpy.core' has no attribute 'numerictypes'\r\n```\r\n\r\nwith latest tensorflow 2.4.1.\r\n\r\nIf you give me some hints on how to fix it, I would do it. I just need a point to start at.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33316\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33316\">No</a>\n", "@wookayin,\r\n\r\nIt is working fine in recent stable version of TF i.e `2.6.0` and I am able to import tensorflow with lazy_import. Please take a look at the [gist here](https://colab.research.google.com/gist/sanatmpa1/d122e3dcef78ea794a6c36cd99e3c5bb/33316.ipynb). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33316\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33316\">No</a>\n"]}, {"number": 33315, "title": "Training slows down with repeated calls to Model.fit()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Version 1903\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)]\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nCalls to `tf.keras.Model.fit()` run slower and slower the more there have been. The sample code below reports about 200 us/sample at the start but after running for 20 minutes report about 400 us per sample. This continues to increase with no apparent upper limit. It remains slow even when training subsequent models but resets to fast after restarting the program.\r\n\r\n**Describe the expected behavior**\r\nNo long term upward trend in training time.\r\n\r\n**Code to reproduce the issue**\r\n\r\n    import numpy\r\n    import tensorflow as tf\r\n    input = tf.keras.Input(shape=(1600,))\r\n    z = tf.keras.layers.Dense(units=200, activation='relu')(input)\r\n    z = tf.keras.layers.Dense(units=1)(z)\r\n    model = tf.keras.Model(inputs=input, outputs=z)\r\n    model.compile(loss='mse')\r\n    x = numpy.full((100,1600), fill_value=2.34, dtype=numpy.float32)\r\n    y = numpy.full((100,1), fill_value=1.23, dtype=numpy.float32)\r\n    while True:\r\n      tf.keras.backend.clear_session()\r\n      model.fit(x=x, y=y)\r\n\r\n\r\n**Other info / logs**\r\n", "comments": ["did you solved it?i met the same problem,it really troubles me", "> did you solved it?i met the same problem,it really troubles me\r\n\r\nNo. I avoided it by changing from CPU to GPU which was OK.", "> > did you solved it?i met the same problem,it really troubles me\r\n> \r\n> No. I avoided it by changing from CPU to GPU which was OK.\r\n\r\nsorry,i don't get it. Do you mean that when you train the model using cpu, the training time keep increasing. But,when you use gpu to train the model , it will be ok?", "Yes. I only found it slowing down on CPU and see constant speed on GPU. Slowdown can also be caused by overheating, so make sure the CPU clock frequency or GPU load isn't dropping.", "> Yes. I only found it slowing down on CPU and see constant speed on GPU. Slowdown can also be caused by overheating, so make sure the CPU clock frequency or GPU load isn't dropping.\r\n\r\nthx, i solved it", "@dlop Please feel free to close the issue if it was resolved already. Thanks!", "@dlop  Any updates regarding this issue? Thanks!", "> @dlop Any updates regarding this issue? Thanks!\r\n\r\nI'll try again with TF 2.1.", "I'm still experiencing this problem with TF 2.0 that I compiled for GPU but set to CPU-only and run on a different problem from the code in this issue. I've confirmed that it's _not_ due to CPU raising temperature or reducing clock speed since those are steady or improving overall. I haven't tried TF 2.1 yet.\r\n\r\nAttached file shows the speed halves after about 1 hour of training.\r\n\r\n![slowdown](https://user-images.githubusercontent.com/10569080/80658028-29ad9700-8ad9-11ea-8d5f-9845c8632f84.png)\r\n", "Strange thing. After making my last post with the graph, the slowdown immediately stopped and it's kept running at a steady 7ms/sample (140 samples/s) for a further 2 hours. Maybe it is just some odd computer issue.", "@dlop  is this still an issue? Please feel free to close the issue if resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33315\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33315\">No</a>\n"]}, {"number": 33314, "title": "cleared numpy depreciation warnings", "body": "fixed numpy compatibility issues with tensorflow, no longer displays depreciation messages when importing tensorflow on systems with np version > 1.14.5\r\n\r\nTested with numpy versions 1.14.5, 1.16.5, 1.17.0", "comments": ["@LordGhostX: Could you provide a bit more context on why this change is needed? What was the deprecation message you were seeing?", "Afaik, these warnings have been fixed from #30559. I don't think it's wise to use magic numpy constants instead of the actual `np.*` dtypes."]}, {"number": 33313, "title": "Typo in documentation for the Attention class", "body": "The value embeddings should be the token embedding of the value input, and not of the query input.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33313) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33313) for more info**.\n\n<!-- ok -->", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac ", "Let's take this one as the example after this fix is correct, whereas before it was not so.", "@mihaimaruseac sure, i can't reopen it , can you please reopen it.", "The original fork has been deleted sadly :("]}, {"number": 33311, "title": "tf.print usage throws SyntaxError: invalid syntax", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): n/a\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 2.7 (Ubuntu 18.04 system default + latest pip from get-pip.py)\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWile playing with `tf.print` as described in docs, the following error happens:\r\n```python\r\n$ python\r\nPython 2.7.15+ (default, Oct  7 2019, 17:39:04) \r\n[GCC 7.4.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> import sys\r\n>>> tensor = tf.range(10) \r\n>>> tf.print(tensor, output_stream=sys.stderr)\r\n  File \"<stdin>\", line 1\r\n    tf.print(tensor, output_stream=sys.stderr)\r\n           ^\r\nSyntaxError: invalid syntax\r\n>>>\r\n```\r\n**Describe the expected behavior**\r\n\r\n`tf.print` should print out values as was specified in https://www.tensorflow.org/api_docs/python/tf/print\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport sys\r\ntensor = tf.range(10) \r\ntf.print(tensor, output_stream=sys.stderr)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@yongtang \r\nIn python 2.7, make sure to import the following: `from __future__ import print_function`.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/113721be2d017fecb3d6fd80c83b74b4/untitled260.ipynb)Thanks!\r\n\r\n", "@ravikyram Yes I missed `from __future__ import print_function` line. \ud83d\udc4d Thanks for the help!"]}, {"number": 33310, "title": "Add filter support for autograph with dataset", "body": "`filter` is one of the builtin functions in python, though it is not supported with dataset in autograph yet (as opposed to `enumerate/map`).\r\n\r\ntf.data.Dataset already have filter support so adding it makes sense I think.\r\n\r\nThis PR adds the filter support for autograph with dataset.\r\n\r\nThis PR is related to #30802 (which adds `enumerate` support)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 33309, "title": "tf_upgrade_v2 is not reporting issues related to invalid imports in TensorFlow 2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina\r\n- TensorFlow installed from (source or binary): binary (pip install tensorflow)\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: Intel Iris Pro 1536 MB\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to use the script `tf_upgrade_v2` as follows \r\n\r\n```\r\ntf_upgrade_v2 --infile bayesian_nn.py --outfile bayesian_nn2.py\r\n```\r\nThe new file `bayesian_nn2.py` is created and the `report.txt` file is also generated, but `report.txt` does not report any issues, while in `bayesian_nn2.py` there is still code that cannot be run in TF2. For example, `report.txt` does not report that I have the import `from tensorflow.contrib.learn.python.learn.datasets import mnist`, which is no longer valid in TF2.\r\n\r\nMore specifically, this is my `report.txt`\r\n\r\n```\r\nTensorFlow 2.0 Upgrade Script\r\n-----------------------------\r\nConverted 1 files\r\nDetected 0 issues that require attention\r\n--------------------------------------------------------------------------------\r\n================================================================================\r\nDetailed log follows:\r\n\r\n================================================================================\r\n--------------------------------------------------------------------------------\r\nProcessing file 'bayesian_nn.py'\r\n outputting to 'bayesian_nn2.py'\r\n--------------------------------------------------------------------------------\r\n```\r\n\r\nI am trying to port the following module https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/bayesian_neural_network.py to TF2.\r\n\r\nAccording to the documentation, the upgrade script should report these issues: https://www.tensorflow.org/guide/upgrade, but it does not.", "comments": ["@nbro ,\r\nThank you for reporting the issue, can you please provide bayesian_nn.py file  or the code the used in the file ?", "@oanush Locally, the file is called `bayesian_nn.py`, but the code is the same as in https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/bayesian_neural_network.py.", "I was able to replicate the issue in local. Thanks!", "@tomerk -- I think to some extent this is inevitable (we are only considering \"tf.\" when warning/modifying), but we did discuss whether it might be feasible to handle some common patterns.", "@nbro,\r\n\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to `2.6` which is latest stable version of TF and let us know if the issue still persists in newer versions.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33309\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33309\">No</a>\n"]}, {"number": 33308, "title": "iterating over 'tf.Tensor' is not allowed AutoGraph did not convert this function. ", "body": "Hi, I am trying to convert below function to tensorflow graph using tf.function decorator.\r\n\r\nHowever, I got an error message. \r\n\r\nMy question is which part should I change the code below to use tf.function?\r\n\r\nthanks. \r\n\r\n```python\r\n@tf.function \r\ndef band2bin(bands):\r\n        tensor_list = [tf.tile(tf.expand_dims(bands[:, b, :], axis=1), [1, hp.band[b, 1]-hp.band[b, 0]+1, 1]) for b in tf.range(bands.shape[1])]\r\n        bins = tf.concat(tensor_list, axis=1)\r\n    return bins\r\n```\r\nerror message: \r\nOperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n\r\n\r\n", "comments": ["As the error message says, I think maybe you can try to avoid using \"for\" loop in the function. And maybe you can choose tf.numpy_function as an alternertive.\r\n\r\nI have the same problem when I use code like this:\r\n\r\noutputs, _ = tf.keras.layers.Bidirectional(LSTM(256, return_sequences=True), merge_mode='concat')(inputs)\r\n\r\nBecause the Bidirectional output is a tensor and cannot outpus as outputs and _.", "@suwoncjh, Can you try @Yablon's workaround and let us know if issue still persists. Thanks!", "Thanks @Yablon \r\n\r\nI solved it by changing tf.range to range and it works. ", "@suwoncjh, Glad that it resolved. \r\nClosing the issue now. Feel free to reopen if issue still persists. Thanks!", "I faced the same error, luckily I solved it.\r\nMy code\r\n```python\r\nrnn = rnn_cell(units)\r\n_, state = rnn(inputs, hidden_state) # Outputs only the state (the mistake I made)\r\n```\r\nAnd for some reason you can't split a tensor with shape (10,100) in two, which makes perfectly sense, but I think the error message is really misleading, so it would be helpful to either to make the error message more leading or give me the file where error messages is displayed, checked or something and then I'll create a pull request.", "I was not able to solve a similar problem. \r\n\r\nI have understood what the problem is, but I am not sure how to get around it using in order to decorate my function. \r\n\r\nIn a part of my code, I have to loop over different parameter values to solve a nonlinear equation. Because of this loop, however, my defined function cannot be decorated with `@tf.function`\r\n\r\nHere is a MWE to replicate the error: \r\n\r\n```\r\nimport numpy  as np \r\nimport tensorflow as tf\r\nimport scipy.optimize \r\n\r\n# fixed parameters\r\nkon = 0.01\r\nmu  = 1.5\r\nfi  = 0.5 \r\nkappa = 22\r\nw = 0.63\r\n\r\n# varying parameters \r\nn =100\r\nxs = tf.random.normal(shape=(n,), stddev=0.2)\r\neps = tf.random.normal(shape=(n,), stddev=0.17)\r\nz = tf.sigmoid(tf.random.normal(shape=(n,), stddev=0.22))\r\n```\r\n\r\nThis is the function that I am trying to decorate: \r\n```\r\n\r\ndef get_leisure(z, eps, x0):\r\n    hvec = np.empty((0,))\r\n    # leisure today\r\n    for ze,ei,xs in zip(z, eps, x0):\r\n        ei=np.exp(ei)\r\n        xs=np.exp(xs)\r\n        # define the function for leisure \r\n        def leisure_function(hi):\r\n            return (mu/fi)*np.log(hi) -(1-mu)*kappa*(hi)**(1+(1/fi))- mu*(np.log(w*ei*xs)-np.log(kon))-np.log(ze)\r\n\r\n        htemp = scipy.optimize.newton_krylov(leisure_function, 0.5)\r\n        hvec = np.append(hvec, htemp)\r\n    return hvec\r\n```\r\n\r\nWithout decoration, it works fine. But when I try to decorate it, gives the same error message as in this issue. \r\n\r\n"]}, {"number": 33307, "title": "No OpKernel was registered to support Op 'TPUReplicateMetadata' used by node TPUReplicateMetadata", "body": "When I run the following .ipynb:\r\n\r\nhttps://colab.research.google.com/drive/1DpUCBm58fruGNRtQL_DiSVbT90spdZgm\r\n\r\nI got:\r\nNo OpKernel was registered to support Op 'TPUReplicateMetadata' used by node TPUReplicateMetadata (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) with these attrs: [num_cores_per_replica=1, use_tpu=true, num_replicas=8, computation_shape=[], host_compute_core=[], device_assignment=[], _tpu_replicate=\"cluster\", padding_map=[], topology=\"\", step_marker_location=\"STEP_MARK_AT_ENTRY\", allow_soft_placement=false]\r\nRegistered devices: [CPU, XLA_CPU]", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?.\r\nI tried with TF 1.14 ,1.15.0-rc3, 1.13.1 and i am able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/f66344f948481ed6af0d25cbfcb08b06/untitled261.ipynb).But in the colab link you have shared i am not seeing any error.Can you please confirm is this expected behavior?.Thanks!\r\n\r\n", "@ravikyram Thanks for your replies.\r\nI use your gist and find new error:\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in configure_callbacks(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)\r\n    118   callback_list.model.stop_training = False\r\n    119   # pylint: disable=protected-access\r\n--> 120   if callback_list.model._ckpt_saved_epoch is not None:\r\n    121     # The attribute `_ckpt_saved_epoch` is supposed to be None at the start of\r\n    122     # training (it should be made None at the end of successful multi-worker\r\n\r\nAttributeError: 'KerasTPUModel' object has no attribute '_ckpt_saved_epoch'", "> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?.\r\n> I tried with TF 1.14 ,1.15.0-rc3, 1.13.1 and i am able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/f66344f948481ed6af0d25cbfcb08b06/untitled261.ipynb).But in the colab link you have shared i am not seeing any error.Can you please confirm is this expected behavior?.Thanks!\r\n\r\n\r\nI just use colab,all edition of (operating system,architecture) are same as yours.\r\nI just use the default tensorflow edition in colab.\r\nThe colab link is:\r\nhttps://colab.research.google.com\r\n Could you help me? thanks\r\n", "what's your meaning?\r\n\r\n\"I tried with TF 1.14 ,1.15.0-rc3, 1.13.1 and i am able to reproduce the issue.\r\nPlease, find the gist here.\r\nBut in the colab link you have shared i am not seeing any error.\"\r\n\r\nyou are         able to reproduce the issue\r\nyou are not able to reproduce the issue in my link?\r\n\r\n", "@appleyuchi \r\n\r\nNow,I am able to reproduce the issue in the link provided by you .Thanks!\r\n", "> @appleyuchi\r\n> \r\n> Now,I am able to reproduce the issue in the link provided by you .Thanks!\r\n\r\nwaiting for your help.\r\nI seems hard to find answer from Google.\r\nMuch Thanks", "@appleyuchi I looked at your code. There are multiple updates you need to do before it runs. For instance, `tpu_model.fit_generator` is not supported with TPU distribution strategy and you also need to define, compile and fit the `model` under `with strategy.scope():`. Can you please follow the [TF Resource here](https://www.tensorflow.org/guide/distributed_training) and update your code. \r\n\r\nYou could also find some TPU codes from github users [like this  one](https://github.com/tensorflow/tensorflow/issues/30162). Thanks!", "@jvishnuvardhan \r\nThis code is from some technical news by a small company.\r\nit did not tell any version requirements of tensorflow.\r\nSo I met such a problem.\r\nCould you help me fix it?Thanks\r\nAnd it seems that it is really an internal bug of Tensorflow\r\n", "@appleyuchi You could follow the links I provided and update your codes. Thanks!", "@jvishnuvardhan they seems are not the same issue.", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "> This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n\r\n\r\nBut only the developer know the meaning of each word of this error.\r\nIt's impossible for users to understand the meaning without the help of developer.\r\n\r\nThanks.\r\n", "OK,I'll try to ask this question on stackoveflow today.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33307\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33307\">No</a>\n"]}, {"number": 33306, "title": "when run example: speech_commands and try to run freeze.py, Tensor name \"final_fc_bias\" not found in checkpoint files /tmp/speech_commands_train/conv.ckpt-2000.index", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nno\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOS Sierra version 10.12.6\r\n\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n\r\n- TensorFlow version (use command below):\r\n2.0 beta\r\n\r\n- Python version:\r\n2.7\r\n\r\n- Run tf_env_collect.sh\r\n== check python ===================================================\r\npython version: 2.7.10\r\npython branch: \r\npython build version: ('default', 'Feb  7 2017 00:08:15')\r\npython compiler version: GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Darwin\r\nos kernel version: Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64\r\nos release version: 16.7.0\r\nos platform: Darwin-16.7.0-x86_64-i386-64bit\r\nlinux distribution: ('', '', '')\r\nlinux os distribution: ('', '', '')\r\nmac version: ('10.12.6', ('', '', ''), 'x86_64')\r\nuname: ('Darwin', 's-macbook-air', '16.7.0', 'Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64', 'x86_64', 'i386')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.0.0 (clang-900.0.38)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== check pips ===================================================\r\nnumpy                                  1.8.0rc1\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 147: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(2, 7, 10, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\nBuild label: 0.29.1\r\nBuild time: Tue Sep 10 13:47:42 2019 (1568123262)\r\nBuild timestamp: 1568123262\r\nBuild timestamp as int: 1568123262\r\n\r\n== check python ===================================================\r\npython version: 2.7.10\r\npython branch: \r\npython build version: ('default', 'Feb  7 2017 00:08:15')\r\npython compiler version: GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Darwin\r\nos kernel version: Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64\r\nos release version: 16.7.0\r\nos platform: Darwin-16.7.0-x86_64-i386-64bit\r\nlinux distribution: ('', '', '')\r\nlinux os distribution: ('', '', '')\r\nmac version: ('10.12.6', ('', '', ''), 'x86_64')\r\nuname: ('Darwin', 's-macbook-air', '16.7.0', 'Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64', 'x86_64', 'i386')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.0.0 (clang-900.0.38)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== check pips ===================================================\r\nnumpy                                  1.16.5              \r\nprotobuf                               3.9.1               \r\ntensorflow                             2.0.0b0             \r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.0.0-beta0\r\ntf.version.GIT_VERSION = v1.12.1-3259-gf59745a381\r\ntf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 147: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.0.0b0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /Users/liupai/tensorflow/lib/python2.7/site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(2, 7, 10, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\nBuild label: 0.29.1\r\nBuild time: Tue Sep 10 13:47:42 2019 (1568123262)\r\nBuild timestamp: 1568123262\r\nBuild timestamp as int: 1568123262\r\n\r\n\r\n**Describe the current behavior**\r\nI am following the tutorials:\r\nexamples/speech_commands\r\n\r\npython tensorflow/examples/speech_commands/train.py \r\n--how_many_training_steps=1500,500  \r\n--data_dir=\"speech_commands_v0.02\" \r\n--data_url=\r\n\r\n\r\npython tensorflow/examples/speech_commands/freeze.py \r\n--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-2000.index \r\n--output_file=/tmp/my_frozen_graph.pb\r\n\r\nit failed with:\r\n(tensorflow) s-macbook-air:tensorflow liupai$ python tensorflow/examples/speech_commands/freeze.py --start_checkpoint=/tmp/speech_commands_train/conv.ckpt-200.index --output_file=/tmp/my_frozen_graph.pb\r\n2019-10-13 18:55:56.670314: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nWARNING:tensorflow:From /Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nW1013 18:55:56.782098 140736832123840 deprecation.py:323] From /Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nINFO:tensorflow:Restoring parameters from /tmp/speech_commands_train/conv.ckpt-200.index\r\nI1013 18:55:56.784265 140736832123840 saver.py:1280] Restoring parameters from /tmp/speech_commands_train/conv.ckpt-200.index\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/speech_commands/freeze.py\", line 250, in <module>\r\n    tf.compat.v1.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"tensorflow/examples/speech_commands/freeze.py\", line 176, in main\r\n    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)\r\n  File \"/Users/liupai/tensorflow/tensorflow/tensorflow/examples/speech_commands/models.py\", line 101, in load_variables_from_checkpoint\r\n    saver.restore(sess, start_checkpoint)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1302, in restore\r\n    err, \"a Variable name or other graph key that is missing\")\r\ntensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n\r\nTensor name \"final_fc_bias\" not found in checkpoint files /tmp/speech_commands_train/conv.ckpt-200.index\r\n\t [[node save/RestoreV2 (defined at /Users/liupai/tensorflow/tensorflow/tensorflow/examples/speech_commands/models.py:100) ]]\r\n\r\nOriginal stack trace for u'save/RestoreV2':\r\n  File \"tensorflow/examples/speech_commands/freeze.py\", line 250, in <module>\r\n    tf.compat.v1.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"tensorflow/examples/speech_commands/freeze.py\", line 176, in main\r\n    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)\r\n  File \"/Users/liupai/tensorflow/tensorflow/tensorflow/examples/speech_commands/models.py\", line 100, in load_variables_from_checkpoint\r\n    saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 825, in __init__\r\n    self.build()\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 837, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 875, in _build\r\n    build_restore=build_restore)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 508, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 328, in _AddRestoreOps\r\n    restore_sequentially)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 575, in bulk_restore\r\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1696, in restore_v2\r\n    name=name)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3296, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1692, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n**Describe the expected behavior**\r\nwhen run freeze.py, it should success according to the tutorials.\r\n\r\n", "comments": ["@FridaCai \r\nCan you please let us know which tutorial you are referring to? Thanks!.", "tutorial: https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md", "another issue when following the tutorial:\r\n\r\nbazel run tensorflow/examples/speech_commands:generate_streaming_test_wav\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'run' from /Users/liupai/tensorflow/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_liupai/75ba6c3dceb6d95ea0c24bbee6657cc0/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\nINFO: Call stack for the definition of repository 'local_config_git' which is a git_configure (rule definition at /Users/liupai/tensorflow/tensorflow/third_party/git/git_configure.bzl:63:17):\r\n - /Users/liupai/tensorflow/tensorflow/tensorflow/workspace.bzl:74:5\r\n - /Users/liupai/tensorflow/tensorflow/WORKSPACE:19:1\r\nERROR: An error occurred during the fetch of repository 'local_config_git':\r\n   Traceback (most recent call last):\r\n\tFile \"/Users/liupai/tensorflow/tensorflow/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/Users/liupai/tensorflow/tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_liupai/75ba6c3dceb6d95ea0c24bbee6657cc0/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nINFO: Call stack for the definition of repository 'grpc' which is a tf_http_archive (rule definition at /Users/liupai/tensorflow/tensorflow/third_party/repo.bzl:121:19):\r\n - /Users/liupai/tensorflow/tensorflow/tensorflow/workspace.bzl:513:5\r\n - /Users/liupai/tensorflow/tensorflow/WORKSPACE:19:1\r\nINFO: Call stack for the definition of repository 'llvm' which is a tf_http_archive (rule definition at /Users/liupai/tensorflow/tensorflow/third_party/repo.bzl:121:19):\r\n - /Users/liupai/tensorflow/tensorflow/tensorflow/workspace.bzl:548:5\r\n - /Users/liupai/tensorflow/tensorflow/WORKSPACE:19:1\r\nERROR: /Users/liupai/tensorflow/tensorflow/tensorflow/core/BUILD:2751:1: //tensorflow/core:version_info_gen depends on @local_config_git//:gen/spec.json in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/Users/liupai/tensorflow/tensorflow/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/Users/liupai/tensorflow/tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_liupai/75ba6c3dceb6d95ea0c24bbee6657cc0/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nERROR: /Users/liupai/tensorflow/tensorflow/tensorflow/core/BUILD:2751:1: //tensorflow/core:version_info_gen depends on @local_config_git//:gen/head in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/Users/liupai/tensorflow/tensorflow/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/Users/liupai/tensorflow/tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_liupai/75ba6c3dceb6d95ea0c24bbee6657cc0/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nERROR: /Users/liupai/tensorflow/tensorflow/tensorflow/core/BUILD:2751:1: //tensorflow/core:version_info_gen depends on @local_config_git//:gen/branch_ref in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/Users/liupai/tensorflow/tensorflow/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/Users/liupai/tensorflow/tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_liupai/75ba6c3dceb6d95ea0c24bbee6657cc0/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nERROR: Analysis of target '//tensorflow/examples/speech_commands:generate_streaming_test_wav' failed; build aborted: no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/Users/liupai/tensorflow/tensorflow/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/Users/liupai/tensorflow/tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_liupai/75ba6c3dceb6d95ea0c24bbee6657cc0/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nINFO: Elapsed time: 18.452s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (157 packages loaded, 4227 targets\\\r\nFAILED: Build did NOT complete successfully (157 packages loaded, 4227 targets\\\r\n configured)\r\n", "@FridaCai Is this still an issue. Can you please try with recent `tf-nightly` and let us know whether the issue persists with latest TF version. Thanks!\r\n\r\nPlease close the issue if this was already resolved with the recent TF version. Thanks!", "@FridaCai Since the stacktrace says `Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint.`\r\nYou may want to try complete training for 18000 steps as mentioned in the tutorial before freezing.\r\nSee https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md", "I think it was resolved. I am closing the issue. Please open a new ticket if you see similar issue again. Thanks!"]}, {"number": 33305, "title": "layer reuse", "body": "```\r\n\r\ndef head(self, input, num_anchors, name, flatten=False):\r\n\r\n    out_channels = (self.num_classes + 4) * num_anchors\r\n    conv = layers.Conv2D(256, 3, 1, 'same', activation='relu', name=name+'_conv1')(input)\r\n    conv = layers.Conv2D(256, 3, 1, 'same', activation='relu', name=name+'_conv2')(conv)\r\n    conv = layers.Conv2D(256, 3, 1, 'same', activation='relu', name=name+'_conv3')(conv)\r\n    out = layers.Conv2D(out_channels, 3, 1, 'same', name=name+'output')(conv)\r\n    return out\r\n\r\n```\r\n\r\nI want to know how to reuse these layers as tf.variable_scope(scope resue=tf.AUTO_REUSE) in tensorflow1\r\n\r\nIn tensorflow1\r\n\r\n```\r\nwith tf.variable_scope('a', resue=tf.AUTO_REUSE) as scope:\r\n\r\n         all layers here could be auto reuse\r\n\r\n```\r\n", "comments": ["@Stick-To ,\r\nCan you share complete code to reproduce the issue reported here? Also provide the TF version being used.", "TF version is TF2.0\r\n\r\n\r\n```\r\ndef head(self, input, num_anchors, name, flatten=False):\r\n\r\n    out_channels = (self.num_classes + 4) * num_anchors\r\n    conv = layers.Conv2D(256, 3, 1, 'same', activation='relu', name=name+'_conv1')(input)\r\n    conv = layers.Conv2D(256, 3, 1, 'same', activation='relu', name=name+'_conv2')(conv)\r\n    conv = layers.Conv2D(256, 3, 1, 'same', activation='relu', name=name+'_conv3')(conv)\r\n    out = layers.Conv2D(out_channels, 3, 1, 'same', name=name+'output')(conv)\r\n    return out\r\n```\r\n```\r\nout1 = self.head(fpn_feat[0], len(anchors[0]), 'head', flatten=True)\r\nout2 = self.head(fpn_feat[1], len(anchors[1]), 'head', flatten=True)\r\nout3 = self.head(fpn_feat[2], len(anchors[2]), 'head', flatten=True)\r\nout4 = self.head(fpn_feat[3], len(anchors[3]), 'head', flatten=True)\r\nout5 = self.head(fpn_feat[4], len(anchors[4]), 'head', flatten=True)\r\n```\r\n\r\n@oanush  I want to resue the layers in head()", "@Stick-To ,\r\nHi, I see that there are entities like `input, num_anchors, name` code for which are not given, can you provide the complete code for same ?", "Also, can you please use proper markdown format around the code so that it can be read?", ">  how to reuse these layers as tf.variable_scope(scope resue=tf.AUTO_REUSE) in tensorflow1\r\n\r\nHi, have you figured out how to reuse?"]}, {"number": 33304, "title": "tf.feature_column.indicator_column OOM", "body": "\r\n**Describe the current behavior**\r\ni'm building a CTR model\uff0c use tf.feature_column.indicator_column  and tf.feature_column.categorical_column_with_hash_bucket  to make my  categorical feature turn into a multi-hot feature  .  But OOM happens , the error show that API indicator_column generate many one-hot feature , this cause the OOM.   \r\nit seems produce many one-hot [batch_size ,categorical list len, bucket_size] then reduce_sum to multi-hot ,this will cost many memory ,how can i solve this problem?  \r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 33303, "title": "Convert to tflite will change output order for models with multiple outputs", "body": "Hello everyone, \r\n\r\nCurrently I have a model with one input and multiple outputs. I train my model with tensorflow eager execution mode and saved trained model into keras format. after that I converted model to tflite format to use in mobile devices. The issue is when I converted my model to tflite the output order will changed. I tested same image with keras model and tflite model and the output order is different.\r\n \r\n**System information**\r\n- OS Platform and Distribution (Mac OS Mojave)\r\n- TensorFlow version : 2.0.0\r\n- Number of outputs: 16\r\n- Base Model: MobilenetV1(0.25)\r\n\r\n\r\nHere is my outputs for both keras and tflite :\r\n\r\ntflite :6157037319861035\r\n\r\nkeras :6219861085157097\r\n\r\nand here is a picture of output layer of my model :\r\n\r\n\r\n![Screen Shot 2019-10-13 at 3 06 29 PM](https://user-images.githubusercontent.com/28183901/66715108-1c92ba00-edcc-11e9-8f26-eaa147e49e76.png)\r\n\r\n\r\n", "comments": ["@shayantabatabaee, Will it be possible to provide the keras model. And also, code snippet used to convert keras to tflite. Thanks!", "Yes , Here is my model : \r\n\r\n```\r\n base_model = MobileNet(input_shape=(64, 640, 3),\r\n                           alpha=0.5,\r\n                           depth_multiplier=1,\r\n                           include_top=False,\r\n                           weights='imagenet',\r\n                           input_tensor=None,\r\n                           pooling=None)\r\n\r\n    x = base_model.output\r\n    x = layers.GlobalAveragePooling2D()(x)\r\n    x = layers.Reshape((1, 1, 512), name='reshape_1')(x)\r\n\r\n    digits = []\r\n    for i in range(16):\r\n        temp = layers.Dropout(1e-1, name=str(i) + \"_dropout\")(x)\r\n        temp = layers.Conv2D(10, 1, 1, padding='same', name=str(i) + '_digits')(temp)\r\n        temp = layers.Reshape((10,), name=str(i) + '_reshape')(temp)\r\n        temp = layers.Activation('softmax', name=str(i) + '_softmax')(temp)\r\n        digits.append(temp)\r\n\r\n    model = Model(inputs=base_model.inputs, outputs=digits)\r\n\r\n    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n\r\n    model.summary()\r\n    return model\r\n\r\n```\r\n\r\nand I convert my model using this script:\r\n\r\n```\r\ndef convert_to_lite_model(save_path, model, tflite_name):\r\n    print(\"Start converting to lite model\")\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = representative_dataset_gen\r\n    tflite_model = converter.convert()\r\n    open(os.path.join(save_path, tflite_name), \"wb\").write(tflite_model)\r\n\r\n```", "I made some changes in my model, I stack the outputs layer into one layer using tf.stack : \r\n`outputs=tf.stack(digits, axis=1)` \r\nIt seems that now outputs order is same with my keras model but I have another issue.\r\nthis model will be used for OCR of credit card numbers, The accuracy of my keras model is about 98% and everything works fine , but although the outputs is constant now, my model finds the numbers but the orders is not correct. Let's see this example,\r\nI have a credit card with this number : 5022 2**9**1**0** 3**62**0 4580 \r\nmy model will find numbers  : 5022 2**2**1**6** 3**09**0 4580.\r\nIt seems that feature extraction in mobilenet will not have order.\r\nThe numbers is correct with high precision but the orders is not correct and it is happened only for middle numbers.\r\n\r\n", "@shayantabatabaee Can you provide `representative_dataset_gen`?. I tried making a [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/1501dc688045f353b3f52bce8e44b686/untitled640.ipynb) with your code so that we can reproduce the issue. However, `representative_dataset_gen` is missing. If you cannot provide that dataset, can you please create a simple standalone code to reproduce the issue. Thanks!", "@jvishnuvardhan actually you do not need dataset to reproduce bug, just comment representative_dataset_gen line and you will see the bug, I tested your gist and saw the bug. I use Netron application to see that the orders of outputs layers is not the same with the code. It seemes that second output of model is that 10th output of code, I attached the image. \r\n<img width=\"400\" alt=\"Screen Shot 2019-11-15 at 10 41 05 AM\" src=\"https://user-images.githubusercontent.com/28183901/68923965-e6fb3b00-0794-11ea-8d1a-b10d5cfdc5fa.png\">\r\nAnd for the second issue that I saw I just change the aspect ratio of input image , I doubled the height and change the image size to 100 x 500 so the second issue is gone. ", "I have noticed that the orders of both the inputs and outputs are mixed up after tflite conversion. \r\n\r\nAnother problem this creates is when you are feeding in the representative dataset you don't know what order the inputs are in. ", "This is also partially related to #32180", "providing the output_arrays (and input_arrays if needed) will solve the problem as you can guarantee the order by doing so\r\n[https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter#from_keras_model_file](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter#from_keras_model_file)", "I am also seeing this with `from_saved_model`, also seems that `output_arrays` solution is no longer supported in TF2", "i'm facing exactly the same issue.\r\n[lite_chaos_order.zip](https://github.com/tensorflow/tensorflow/files/6014492/lite_chaos_order.zip)\r\njust feed with same random to both h5 and lite model, you can regen the issue\uff08same output value collection but different order\uff09", "I can reproduce this issue in TF 2.4.1 . Inspecting the list of output tensors from:\r\n `interpreter = tf.lite.Interpreter(\r\n    model_path='/path/to/tflite/model'\r\ninterpreter.get_output_details()` \r\nYeilds a out-of-order list, compared to the `.predict` call in tf.keras, specifically, the list has been mirrored. For 4 output tensors: \r\ntf.keras.Model.predict `output[0] ` is now `output[3] ` in tf.lite\r\ntf.keras.Model.predict `output[1] ` is now `output[2] ` in tf.lite\r\ntf.keras.Model.predict `output[2] ` is now `output[1] ` in tf.lite\r\ntf.keras.Model.predict `output[3] ` is now `output[0] ` in tf.lite\r\nI have not attempted to fix the export to TFlite, since I can deal with this simply by parsing the interpreter output in reverse order. \r\n", "I'm seeing the same thing in TF 2.2.0: output 1, 2, 3 is returned as 1, 3, 2. ", "> I am also seeing this with `from_saved_model`, also seems that `output_arrays` solution is no longer supported in TF2\r\n\r\nThis is correct. \r\n\r\nI have been working for 2 days trying to find a solution to this issue, and have yet to find one. The issues for me is, if I push models to production, and each time the export has a different output order, it can't be used. I'd be keen to contribute to fixing this but will be a while before I can get onto that. Thanks all!\r\n", "Naming the outputs in keras such that their names' alphabetical order correspond to their desired order in tflite worked well for me. ", "> Naming the outputs in keras such that their names' alphabetical order correspond to their desired order in tflite worked well for me.\r\n\r\nThanks very much for the suggestion! Didn't manage to get this working because the outputs are still out-of-order. If i understand correctly you mean labeling the outputs as such:\r\n\r\n`model = tf.keras.Model(inputs=in_tensor, outputs={'1': x1,'2':x2,'3': x3,'4': x4}, name= 'my_model'`", "Yes, that's what I mean. Perhaps it is equivalent, but specifically, I did:\r\n\r\n```python\r\n# (...)\r\ny1 = Dense(1, name='A')(some_input1)\r\ny2 = Dense(1, name='B')(some_input2)\r\nreturn Model(x, [y1, y2])\r\n```", "Please fix this. I was hoping to use TFLite as a drop in replacement for the TF runtime. However, the unpredictable ordering for multi-output models is surprising behavior and took me a long time to figure out what was going on.", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/bde60d31524f87e0d1466a4a1d1228b4/untitled21.ipynb#scrollTo=jFMmYHFP_KbA),Thanks ! ", "I managed to get the order back by sorting the index of each output using tflite_interpreter.get_output_details()", "When I use `interpreter.get_output_details()`, I got a list like the following\r\n```\r\n  [\r\n    {'name': 'StatefulPartitionedCall:5', 'index': 714, ...}, \r\n    {'name': 'StatefulPartitionedCall:4', 'index': 716, ...}, \r\n    {'name': 'StatefulPartitionedCall:3', 'index': 717, ...}, \r\n    {'name': 'StatefulPartitionedCall:2', 'index': 734, ...}, \r\n    {'name': 'StatefulPartitionedCall:1', 'index': 736, ...}, \r\n    {'name': 'StatefulPartitionedCall:0', 'index': 737, ...}\r\n  ]\r\n```\r\n\r\nHowever, the order of my keras model outputs is in the reversed order, i.e. I need to use number `X` in the `StatefulPartitionedCall:X` to get the correct order. I don't know if this is the best and safest way to get the original ordering.", "For me that doesn\u2019t work. But the index order works in your case too. \n\n\n\n> On May 31, 2021, at 3:11 AM, Yih-Dar ***@***.***> wrote:\n> \n> \ufeff\n> When I use interpreter.get_output_details(), I got a list like the following\n> \n>   [\n>     {'name': 'StatefulPartitionedCall:5', 'index': 714, ...}, \n>     {'name': 'StatefulPartitionedCall:4', 'index': 716, ...}, \n>     {'name': 'StatefulPartitionedCall:3', 'index': 717, ...}, \n>     {'name': 'StatefulPartitionedCall:2', 'index': 734, ...}, \n>     {'name': 'StatefulPartitionedCall:1', 'index': 736, ...}, \n>     {'name': 'StatefulPartitionedCall:0', 'index': 737, ...}\n>   ]\n> However, the order of my keras model outputs is in the reversed order, i.e. I need to use number X in the StatefulPartitionedCall:X to get the correct order. I don't know if this is the best and safest way to get the original ordering.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "@charleswg ,  index order doesn't work in my case above, because the ascending order of index is the descending order of my keras model output.\r\n\r\nI hope TFLite team can let us specify explicitly the input/output orders, for the sake of the ML production stability.", "@shayantabatabaee \r\nCould you please try on latest stable version of tf and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> \r\n> \r\n> @shayantabatabaee\r\n> Could you please try on latest stable version of tf and let us know if this is still an issue.Thanks!\r\n\r\nWe've tried it and it looks like it's fixed in the newest version :) Thanks!", "Yes I tested it ,and it seems to be fixed\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33303\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33303\">No</a>\n", "Thank you for your update, glad the issue is resolved.", "@shayantabatabaee which tensorflow version did you use? (Issue fixed version)", "I would like to chime in. I am using TensorFlow 2.7.0 to export TFLite models, and the output tensor order still changes in the conversion.", "I struggled with this in flutter tflite for Android - after a lot of failed attempts to change the TFLite generation/rename outputs I gave up and just changed the order of my TFLite interpreter.\r\n\r\nhttps://github.com/shaqian/flutter_tflite/pull/235", "I use Tensorflow 2.8.0. I have the same problem.\r\nWith or Without assigning name of each inputs and outputs of my model, TFLite converter change orders\r\n", "I have the same problem with 2.7.0\r\nBaffled why such a simple thing could be so difficult..", "I downgraded to tf2.6.1. It works.  So it's a regression.\r\nThe bug is only fixed in 2.5 and 2.6.\r\n@zldrobit @swarmidentity @NAM-hj "]}, {"number": 33302, "title": "Tflite coverter error. tensorflow/lite/toco/tooling_util.cc:935", "body": "<em>I'm trying to convert a TensorFlow model to tflite model from a frozen graph.\r\nI' getting an error saying I should make a bug report to tflite team.\r\n\r\n**you can download the graph from [HERE.**](https://drive.google.com/file/d/1LqyZWJsZadOHCxY8kG8JAskMoRgOWzSH/view?usp=sharing) \r\n\r\nany help would be much appreciated. </em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\ntensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array \"sample_sequence/while/Exit_3\" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.\r\nAborted (core dumped)\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n**import tensorflow as tf\r\nimport sys\r\nfrom tensorflow.python.platform import gfile\r\n\r\nfrom tensorflow.core.protobuf import saved_model_pb2\r\nfrom tensorflow.python.util import compat\r\n\r\ngraph_def_file = \"frozen_355.pb\"\r\ninput_arrays = [\"sample_sequence/model/Shape\"]\r\noutput_arrays = [\"sample_sequence/while/Exit_3\"]\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model_0.tflite\", \"wb\").write(tflite_model)**\r\n\r\n\r\n**Other info / logs**\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-4-e27c2f171b9b> in <module>()\r\n      4 \r\n      5 converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\n----> 6 tflite_model = converter.convert()\r\n      7 open(\"content/drive/My Drive/converted_model_0.tflite\", \"wb\").write(tflite_model)\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198       stdout = _try_convert_to_unicode(stdout)\r\n    199       stderr = _try_convert_to_unicode(stderr)\r\n--> 200       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    201   finally:\r\n    202     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2019-10-13 11:18:54.608948: F tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array \"sample_sequence/while/Exit_3\" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.\r\nAborted (core dumped)\r\n\r\n", "comments": ["I was able to replicate the issue, please find the .ipynb file.\r\n[33302.ipynb.zip](https://github.com/tensorflow/tensorflow/files/3723813/33302.ipynb.zip)\r\nThanks!\r\n", "have you found any reason, why it's happening?\r\n\r\nlet me know .\r\n", "An `Exit` op means that this graph has control flow. TensorFlow Lite doesn't support control flow generated by TensorFlow 1.X so this functionality is not expected to work. We are working on supporting control flow 2.0 so please regenerate your graph in 2.0 if you want to run it using TenosrFlow Lite.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33302\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33302\">No</a>\n"]}, {"number": 33301, "title": "tf.io.gfile.GFile.read() dose not return but terminates my ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS: Windows 10 18362\r\n- TensorFlow installed from (source or binary): sourece\r\n- TensorFlow version (use command below):  v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: python 3.68\r\n\r\n**Describe the current behavior**\r\nThe func read() does not work (it return noting) and terminates my program. \r\n**Describe the expected behavior**\r\nIn the doc, read() shoud return the contents of the file as string\r\n**Code to reproduce the issue**\r\n`import tensorflow as tf `\r\n`image = tf.io.gfile.GFile(r\"..\\datasets\\icdar2015\\img_1.jpg\", 'r')`\r\n`image.read()`\r\n\r\nMy TF2.0 version below has the same problem with the beyond.\r\n`image_data = tf.io.gfile.GFile(os.path.abspath(path), 'rb').read()`\r\n\r\nMy TF1.x version below run well \r\n`image_data = tf.gfile.FastGFile(os.path.abspath(path), 'rb').read()`\r\n", "comments": ["@Jinof, I tried replicating the issue with Tf 2.0.0 but it worked as expected. `tf.io.gfile.GFile(path).read() `returns the content of the file as string. Please refer [this link](https://www.tensorflow.org/api_docs/python/tf/io/gfile/GFile#read). Thanks", "> @Jinof, I tried replicating the issue with Tf 2.0.0 but it worked as expected. `tf.io.gfile.GFile(path).read() `returns the content of the file as string. Please refer [this link](https://www.tensorflow.org/api_docs/python/tf/io/gfile/GFile#read). Thanks\r\n\r\nYeah i have made it.My program does have some problem.What puzzles me is my TF2.0 version quit without reporting any error but my TF1.X version report my errors correctly.\r\nIt interesting that arfter restarting my computer both of them work well.And the error info can be reported by TF2.0 correctly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33301\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33301\">No</a>\n"]}, {"number": 33300, "title": "Force synchronization for GPU ops accessing host memory (issue #33294)", "body": "Forcibly synchronize the stream when BaseGPUDevice::Compute() is called with a kernel with inputs in host memory, since the kernel may be executed on the host and therefore may not automatically wait for completion of other kernels in the stream (which may be generating its inputs).", "comments": ["Reassigning this GPU runtime-related PR to @iganichev.\r\n\r\nI haven't reviewed the code in detail, but I would be concerned that blocking the stream at the beginning of *every* kernel that takes a hostmem input might have a non-trivial performance impact. It would be helpful to include an example program that doesn't work correctly without this change.", "Please see https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/325. It affects all sorts of programs. I've seen it at least a dozen times with tensor2tensor. For some reason, it happens disproportionally more often with RoCm backend, but tracking it down leads me to believe that it's a generic problem (maybe ROCm just has slower device->host copies and that makes the error more frequent.)\r\n\r\nWhen this happens, training fails at a random moment (could be 20 min or 2 hours after start) with an error that looks something like this\r\n```\r\n\r\n> \r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n>   (0) Invalid argument: Input to reshape is a tensor with 3780 values, but the requested shape has 1116980138090984363\r\n>          [[node training/gradients/transformer/parallel_0_5/transformer/transformer/body/encoder/layer_1/self_attention/layer_prepostprocess/layer_norm/Mean_grad/Reshape (defined at usr/\r\n> local/lib/python2.7/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n>          [[training/control_dependency/_6393]]\r\n> \r\n\r\n```\r\n\r\nP.S. I've noticed that this imposes synchronization on quite a bit more kernels than I expected. Most  visible failures are caused by race conditions in Reshape, Pack, DynamicStitch and the like. There are numerous others like Mul https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_mul_1.cc#L38-L44 that could fail silently.\r\n\r\nBut I don't see an easy way around this. If the GPU kernel takes any inputs in host memory, it needs to wait for those inputs to be available. If the code could distinguish between an input that was scheduled for a device->host copy 500 microseconds ago and an input that was in host memory from the beginning, that could reduce the number of synchronizations. But I don't know how to do that.", "The underlying issue does look like a race condition, but the description in https://github.com/tensorflow/tensorflow/issues/33294 does not seem accurate.\r\n\r\nPackOp will run only after _HostRecv's callback is invoked. The callback passed to _HostRecv is this https://github.com/tensorflow/tensorflow/blob/aec60bd94b1ef6b0a65abf6a9699e0cfe149ae26/tensorflow/core/common_runtime/executor.cc#L1850. No consumer of its outputs can be invoked until the PropagateOutputs runs.\r\n\r\nNow, maybe when _HostRecv callback is invoked, the copy is not done yet and the destination buffer contains garbage. This should also not be the case, because the Recv logic ends up getting here: https://github.com/tensorflow/tensorflow/blob/e2c703019e78f1e34bf2e7e628f2ebcdc9e62445/tensorflow/core/common_runtime/gpu/gpu_util.cc#L294. This callback will run only after the memcpy, scheduled 10 lines above, actually finishes.\r\n\r\nIn light of the above and the fact that this change will likely cause large performance regressions, I will close this PR. It would be great if you can provide a simple repro of the race condition you are seeing.", "I stand corrected. Yes, it looks like it should wait for the transfer to finish before calling the next op. Something else must be going on - I'll need to investigate further.\r\n\r\nIf this race condition could be simply reproduced, issue 325 linked above probably wouldn't have been still open 8 months after the original report. It is exceedingly hard to debug something that occurs randomly and sometimes less than once per hour, and tends to stop happening altogether as soon as you enable tracing. I only got this far because I stumbled on a workload that tended to reproduce it most times in under 5-10 min (and, on that workload, the synchronization change took care of it.) But it would not be easy to share."]}, {"number": 33299, "title": "[tf2] isinstance(numeric_column, FeatureColumn)  == false", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tf2\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\ndiffrent call code,  make `transformation_cache.get(self, state_manager)`  use different FeatureColumn, result in exception.\r\n\r\n```python\r\ntransformation_cache.get(self, state_manager):\r\n    if not isinstance(key, FeatureColumn):\r\n      raise TypeError('\"key\" must be either a \"str\" or \"FeatureColumn\". '\r\n                      'Provided: {}'.format(key))\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column\r\nimport tensorflow.keras as keras\r\nfrom tensorflow.python.feature_column.feature_column import _LazyBuilder\r\nfrom tensorflow_core.python.feature_column.feature_column_v2 import _StateManagerImpl, FeatureTransformationCache\r\n\r\n\r\n\r\ndef tnumeric():\r\n    price = {'price': [[1.], [2.], [3.], [4.]]} \r\n    builder = _LazyBuilder(price)\r\n\r\n    def transform_fn(x):\r\n        return x + 2\r\n\r\n    price_column = feature_column.numeric_column('price', normalizer_fn=transform_fn)\r\n\r\n    from tensorflow.python.feature_column.feature_column_v2 import FeatureColumn\r\n    print(isinstance(price_column, FeatureColumn))  # true\r\n    from tensorflow_core.python.feature_column.feature_column_v2 import FeatureColumn\r\n    print(isinstance(price_column, FeatureColumn))   # false\r\n\r\n    feature_layer = keras.layers.DenseFeatures([price_column])\r\n    a = feature_layer(price)  # use tensorflow,  true\r\n\r\n    sm = _StateManagerImpl(feature_layer, feature_layer.trainable)\r\n    ff=  FeatureTransformationCache(price)\r\n    dt = price_column.get_dense_tensor(ff, sm) # use tensorflow_core, false\r\n\r\ntnumeric()\r\n```\r\n\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Applications/PyCharm 2019.3 EAP.app/Contents/helpers/pydev/pydevd.py\", line 1415, in _exec\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/Applications/PyCharm 2019.3 EAP.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/Users/lqk/project/PycharmProjects/TF2/numeric.py\", line 49, in <module>\r\n  File \"/Users/lqk/project/PycharmProjects/TF2/numeric.py\", line 35, in tnumeric\r\n    \r\n  File \"/Users/lqk/anaconda2/envs/p37t2/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py\", line 2845, in get_dense_tensor\r\n    return transformation_cache.get(self, state_manager)\r\n  File \"/Users/lqk/anaconda2/envs/p37t2/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py\", line 2604, in get\r\n    'Provided: {}'.format(key))\r\nTypeError: \"key\" must be either a \"str\" or \"FeatureColumn\". Provided: NumericColumn(key='price', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function tnumeric.<locals>.transform_fn at 0x11aa0f170>)\r\n\r\n```\r\n", "comments": ["from tensorflow_core.python.feature_column.feature_column_v2 import _StateManagerImpl, FeatureTransformationCache\r\n\r\nchange to \r\n\r\nfrom tensorflow.python.feature_column.feature_column_v2 import _StateManagerImpl, FeatureTransformationCache\r\n\r\nsuc.\r\nbut still don't  know why,  the code of FeatureColumn is same, no matter tensorflow or tensorflow_core.\r\n\r\n"]}, {"number": 33298, "title": "Not able to install tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Windows 7):\r\n\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:1.13.1\r\n- Python version:3.7.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen i install tensorflow and i import tensorflow ,i get the folowing error:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\ptetensor\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Tensorflow details installed:\r\nName: tensorflow\r\nVersion: 2.0.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: c:\\users\\admin\\anaconda3\\envs\\pte\\lib\\site-packages\r\nRequires: tensorflow-estimator, gast, astor, six, wrapt, termcolor, grpcio, wheel, protobuf, tensorboard, keras-applications, keras-preprocessing, absl-py, google-pasta, numpy, opt-einsum\r\nRequired-by:", "Does your CPU support AVX?", "@amritanaik ,\r\nAny update on the issue? thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33298\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33298\">No</a>\n"]}, {"number": 33297, "title": "Surprising random seed behavior when using @tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nRandom seeds work in surprising ways in TF 2.0 when using `@tf.function`. In particular, the value of the global random seed is only taken into account when a function is traced, not when it is called. This is surprising and different from TF 1.x behavior.\r\n\r\n**Describe the expected behavior**\r\nI expect the value of the global random seed to be taken into account every time a pseudo-random number is generated.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n@tf.function\r\ndef rnd():\r\n    return tf.random.uniform(shape=[])\r\n\r\ntf.random.set_seed(42)\r\nprint(rnd()) # The rnd() function's seed is generated randomly now, based on\r\nprint(rnd()) # the current random seed (which is 42).\r\nprint()\r\n\r\ntf.random.set_seed(43) # resets the random sequence but ignores this seed!\r\nprint(rnd())\r\nprint(rnd())\r\nprint()\r\n\r\ntf.random.set_seed(42) # resets the random sequence but ignores this seed!\r\nprint(rnd())\r\nprint(rnd())\r\nprint()\r\n```\r\n\r\nThe output value is:\r\n\r\n```\r\ntf.Tensor(0.63789964, shape=(), dtype=float32)\r\ntf.Tensor(0.8774011, shape=(), dtype=float32)\r\n\r\ntf.Tensor(0.63789964, shape=(), dtype=float32)\r\ntf.Tensor(0.8774011, shape=(), dtype=float32)\r\n\r\ntf.Tensor(0.63789964, shape=(), dtype=float32)\r\ntf.Tensor(0.8774011, shape=(), dtype=float32)\r\n```\r\n\r\nNotice that we get the same sequence of random numbers every time, ignoring the value of the global random seed. The only value that matters is the first one (when the function gets traced).\r\n\r\nMore code and examples of surprising behavior in [this colab](https://colab.research.google.com/drive/1C3LZkt5hfO6T2Uo2xaYVG8hiNQL8c3xu).\r\n\r\n**Other info / logs**\r\n\r\nSpecifically, I would expect the output to look the same as when the function is not decorated with `@tf.function`:\r\n\r\n```\r\ntf.Tensor(0.6645621, shape=(), dtype=float32)\r\ntf.Tensor(0.68789124, shape=(), dtype=float32)\r\n\r\ntf.Tensor(0.2733041, shape=(), dtype=float32)\r\ntf.Tensor(0.5168259, shape=(), dtype=float32)\r\n\r\ntf.Tensor(0.6645621, shape=(), dtype=float32)\r\ntf.Tensor(0.68789124, shape=(), dtype=float32)\r\n```\r\n\r\nNote that the second sequence is different, as expected (in fact, the pseudo-random numbers should be identical whether the function is decorated or not, but that's a nice-to-have).", "comments": ["@ageron,\r\nCan you please refer the issue #33034 and let us know if it helps. Thanks!", "Hi @rmothukuru ,\r\nI looked at issue #33034 before filing this issue, I think it's a different problem. I am not using per-operation seeds in this issue.", "Could reproduce the issue with TF Version 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/faae8f0424cd0bcfcee29854140cab8c/random-seeds-in-tf-2-0.ipynb).", "@alextp @wangpengmit FYI\r\n\r\nUnfortunately, this is a known issue of `tf.random.seed`. We'll fix the documentation to help avoid the issue, and I'll post an example for how the original example can be updated so that it consistently works in `tf.function` as well..\r\n\r\n", "Update: here is a method that currently works in TF2, and which will give you the expected results. Unfortunately, `tf.set_seed` and `tf.random.uniform` (and similar ops) are currently broken, although they will probably be either fixed or removed in the future:\r\n\r\n```\r\n@tf.function\r\ndef rnd(rng):\r\n  return rng.uniform(shape=[])\r\n\r\nrng = tf.random.experimental.Generator.from_seed(seed=42)\r\nprint(rnd(rng))\r\nprint(rnd(rng))\r\nprint()\r\n\r\nrng = tf.random.experimental.Generator.from_seed(seed=43)\r\nprint(rnd(rng))\r\nprint(rnd(rng))\r\nprint()\r\n\r\nrng = tf.random.experimental.Generator.from_seed(seed=42)\r\nprint(rnd(rng))\r\nprint(rnd(rng))\r\nprint()\r\n```\r\n\r\nNote: currently, calling the function with a new generator will causing it to be retraced (that is, will be slow the first time you call it); this will be fixed soon.\r\n\r\n@yashk2810 it would be nice to make sure this snippet or something similar is visible in one of our tutorials or perhaps the docs for tf.random?\r\n", "Its fixed now: \r\n* https://www.tensorflow.org/api_docs/python/tf/random/set_seed\r\n* https://www.tensorflow.org/api_docs/python/tf/random/uniform", "We also link off to the notebooks where the particular symbol is being used.\r\n\r\nSee this for example: https://www.tensorflow.org/api_docs/python/tf/random/uniform#used_in_the_guide", "Thanks @mdanatg and @yashk2810 , the documentation is clearer now, and it's nice to have a workaround. I really like the `tf.random.experimental.Generator` solution, and in fact I think I'll stop using `tf.random.set_seed()` altogether, as well as op-level seeds. Their behavior is way too complicated and error-prone, IMHO.\r\n\r\nIdeally, a function's behavior should not change when you decorate it with `@tf.function`, and things should be as simple and intuitive as possible. In NumPy, there is a single global random number generator (RNG), and all random ops use it by default. If users need other RNGs, they just create them. Works fine.\r\n\r\nHere's a solution that works fine and which I find very simple and intuitive:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.random.experimental.Generator.set_seed = tf.random.experimental.Generator.reset_from_seed\r\ntf.random_v2 = tf.random.experimental.Generator.from_non_deterministic_state()\r\n\r\n@tf.function\r\ndef rng():\r\n    return tf.random_v2.uniform(shape=[])\r\n\r\nprint(rng()) # prints ?1, different for every run of this program\r\nprint(rng()) # prints ?2\r\n\r\ntf.random_v2.set_seed(42)\r\nprint(rng()) # prints A1\r\nprint(rng()) # prints A2\r\n\r\ntf.random_v2.set_seed(43)\r\nprint(rng()) # prints B1\r\nprint(rng()) # prints B2\r\n\r\ntf.random_v2.set_seed(42)\r\nprint(rng()) # prints A1\r\nprint(rng()) # prints A2\r\n```\r\n\r\nEverything works exactly like you would expect, and it gives the exact same result whether the `rng()` function is decorated with `@tf.function` or not.\r\n\r\nThe only problem is that `reset_from_seed()` does not support `None`. That's really something that should be fixed IMHO, it should generate a random state like in `from_non_deterministic_state()`, using `non_deterministic_ints()`.\r\n\r\nNext, if I want to use a specific RNG, I can do so easily:\r\n\r\n```python\r\n@tf.function\r\ndef my_fn(rng):\r\n    return rng.uniform(shape=[])\r\n\r\nrng1 = tf.random.experimental.Generator.from_seed(42)\r\nrng2 = tf.random.experimental.Generator.from_seed(43)\r\n\r\nprint(my_fn(rng1)) # prints A1\r\nprint(my_fn(rng1)) # prints A2\r\n\r\nprint(my_fn(rng2)) # prints B1\r\nprint(my_fn(rng2)) # prints B2\r\n\r\nprint(my_fn(rng1)) # prints A3\r\nprint(my_fn(rng1)) # prints A4\r\n\r\nrng1.set_seed(42)\r\nprint(my_fn(rng1)) # prints A1\r\nprint(my_fn(rng1)) # prints A2\r\n``` \r\n\r\nAgain, this is simple and behaves exactly like you would expect, and just the same way with or without decorating the function with `@tf.function`.\r\n\r\nIt's a pity the `reset()` method explodes if I don't pass it a `state` argument. I would expect it to revert to its initial state instead. It should just save its initial state upon creation, and revert to that state when `reset()` is called without any argument.\r\n\r\nWdyt?", "Thank you for the thorough suggestion!\r\n\r\nYes, it's absolutely crucial that code works the same way with or without `tf.function` - anything that doesn't is a bug.\r\n\r\nI'll let Peng weigh in on a solution, he's the RNG expert. Personally I like your proposal because it's clean and minimizes the symbols we'd need to export: `Generator` is the sole RNG, the \"easy\" `tf.random.uniform` functions just offer access to the default generator and we can omit `tf.stateless.stateless_uniform`.", "One thing I'm not sure about is how to add this to TF 2.0 while keeping backward compatibility. I see a few options:\r\n\r\n1. Keep the existing `tf.random` and add `tf.random_v2` in parallel, as I did. But it feels wrong to add yet a *_v2 just a few weeks after 2.0 is released, which had finally removed all the *_v2s that had accumulated in TF1.\r\n2. Add `tf.random.enable_simple_behavior()` to make the new behavior opt-in. Not very elegant: shouldn't the simple behavior be the default? Perhaps make it the default behavior in TF 2.1?\r\n3. Or just update `tf.random` to use the new behavior and consider that we're just fixing a bug. Since the output differs when you decorate the function with `@tf.function`, it is a bug. Since TF 2 was just released, there's probably not much production code out there anyway, and the impact is limited: it will just change the random sequences. But in that case the recently added documentation should be updated quickly.\r\n\r\n", "Hi @ageron, your code is roughly what we have in mind for the future `tf.random.uniform` and alike, if we choose to keep them. `Generator.reset_from_non_deterministic_state` seems like a useful feature which I'll consider. I'm hesitant to make `Generator` save the initial state to support an argument-less `reset`, since for basic facilities such `Generator` we need to be careful about memory usage. After all, the user can retrieve and store the initial state herself and use it to reset the generator later.", "Hi @wangpengmit ,\r\n\r\nThanks for your feedback. Just to be clear, are you planning to go for option 3 in my previous comment? In other words, will the new (simpler) behavior replace the old one or will they live side by side (if so, how)?\r\n\r\nAlso, I'd like to suggest providing a `set_seed()` method in the `Generator` class that calls `reset_from_seed()` if a seed is provided or `reset_from_non_deterministic_state()` if not. IMHO, it would be more convenient, more intuitive and less verbose.\r\n\r\nRegarding the initial state, I see your point, it makes sense. Perhaps the initial state could be saved by default when creating the `Generator` (since most people won't create millions of `Generator`s, RAM usage should be fine) and have an option in the constructor such as `save_initial_state=True` so that people who need the extra RAM can set this to `False` if they want? Or perhaps set it to `False` by default and make the `reset()` method print our a clear error message if the user doesn't provide a state: \"You need to set save_initial_state=True when constructing the Generator if you want to be able to call reset without any argument\". Not sure about this.", "@ageron Your @ not me :)", "We are pretty sure the old behavior is wrong and will go away. We haven't decided whether to remove symbols such as `tf.random.uniform` and only provide `tf.random.Generator`, or to give `tf.random.uniform` the new behavior. Thank you for the other suggestions! We'll consider them.", "Thanks for your feedback. Please don't remove `tf.random.uniform()` and other random functions: there's really no need to break everyone's existing code, IMHO. Most people won't even notice the change. I think this modification is mostly an implementation detail and a bug fix, it doesn't justify changing the API much. Just adding a warning in the release notes explaining what's changed and the motivation would be sufficient.\r\n\r\nMoreover, I would recommend making the default `Generator` available directly through the API. Something like `tf.random.default_generator()` or `tf.random.get_default_generator()`, for example in case someone needs to pass a `Generator` to a function, and they want to pass the default one.\r\n\r\nThanks for your work, it's a really nice improvement!", "We already exported `tf.random.experimental.get_global_generator` (https://github.com/tensorflow/tensorflow/blob/ba96c40cb452c68ba08eb30f4e34795f4774cb92/tensorflow/python/ops/stateful_random_ops.py#L682). Is it the same as the `tf.random.get_default_generator` you have in mind?", "Ah yes, that's exactly what I had in mind, thanks @wangpengmit . \ud83d\udc4d ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33297\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33297\">No</a>\n", "Hey @wangpengmit ,\r\nWhen do you expect `tf.random.experimental.get|set_global_generator()` functions to be moved to `tf.random`, and when will `tf.random.uniform` (and other random functions) move to using the global generator by default?\r\n", "We don't have a timeline yet.", "I understand. In the meantime, don't you want to leave this issue open? The `Generator` class is a great workaround, but as long as the default random functions don't use it, they're still kind of broken, IMHO.", "Sure, I've reopened the issue.", "Hi @ageron, there is a discussion about RNG migration plan: https://groups.google.com/a/tensorflow.org/d/topic/developers/WLBbnQMB2Rw/discussion . We welcome your comments!", "Thanks @wangpengmit ,\r\nIt looks good to me. I added a note about making it possible to revert to the old behavior if needed, but I understand if that's hard to do.", "I'm closing this bug because after many discussions we've decided that we will leave the old RNGs unchanged, and discourage and eventually deprecate them in favor of the new RNGs and stateless RNGs. The main reasons include (1) we can't find a way to fully reproduce the old RNGs' behavior using the new mechanism and (2) we can't afford to change the old RNGs' behavior which will break a lot of users' code.\r\n\r\nBTW there is a new guide for the recommended ways to generate random numbers in TF2: https://www.tensorflow.org/guide/random_numbers", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33297\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33297\">No</a>\n"]}, {"number": 33296, "title": "Remove tf.enable_eager_execution() in README.md", "body": "As TensorFlow moves to 2.0, eager execution is enabled by default\r\nso there is no need to invoke `tf.enable_eager_execution()`\r\nexplicitly.\r\n\r\nThis PR removed tf.enable_eager_execution() from README.md\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 33295, "title": "Update CONTRIBUTING.md to use in repo pylint style", "body": "In CONTRIBUTING.md, pylint style definition file was downloaded from\r\nthe web before used for style checking. However, as the\r\nstyle definition file `tensorflow/tools/ci_build/pylintrc`\r\nis really part of the repo, the download could be avoided with just\r\nthe following:\r\n```\r\npylint --rcfile=tensorflow/tools/ci_build/pylintrc myfile.py\r\n```\r\n\r\nThis PR updates CONTRIBUTING.md\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 33294, "title": "PackOp race condition / heisenbug", "body": "tensorflow (1.x trunk) has a peculiar kernel: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/pack_op.cc#L168-L174\r\nit is registered for the op \"Pack\" as a GPU kernel, but it is implemented on the host (CPU), and it takes its inputs in host memory. (There may be others like it, this is merely the one I noticed.)\r\n\r\nConsider what happens when its input comes from the GPU memory. Graph builder will insert a HostRecv op, which copies the data from the GPU to the host. HostRecv is, by default, asynchronous.\r\n\r\nAfter HostRecv, tensorflow will call BaseGPUDevice::Compute() on the PackOp kernel. \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L479\r\nWhich will immediately proceed calling Compute() on the assumption that it is a GPU kernel that's going to be executed in the same stream (therefore synchronization is unnecessary).\r\nSince HostRecv is asynchronous, the data may or may not be there by the time Pack attempts to read it, resulting in a race condition.\r\n\r\n(There was some code in BaseGPUDevice::Compute() that did attempt waiting for inputs up until about 2 weeks ago, but it did not prevent the race condition either.)", "comments": ["May be related to https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/325."]}, {"number": 33293, "title": "Operators not supported by tensorflow lite.", "body": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, CAST, CONCATENATION, CONV_2D, DIV, EQUAL, EXP, EXPAND_DIMS, FULLY_CONNECTED, GATHER, GATHER_ND, GREATER, GREATER_EQUAL, LESS, LOG, LOGICAL_AND, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, PADV2, RANGE, REDUCE_MAX, RESHAPE, RESIZE_NEAREST_NEIGHBOR, ROUND, SHAPE, SOFTMAX, SPARSE_TO_DENSE, SPLIT, SQRT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TOPK_V2, TRANSPOSE_CONV, UNIQUE, WHERE. Here is a list of operators for which you will need custom implementations: CropAndResize, DenseToDenseSetOperation, Enter, Exit, LoopCond, Merge, NonMaxSuppressionV3, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 33292, "title": "Add TensorFlow MicroLite Support for the OpenMV Cam", "body": "Really simple.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33292) for more info**.\n\n<!-- need_sender_cla -->", "@kwagyeman thank you for your contribution, please sign CLA.", "I did. It's signed.", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33292) for more info**.\n\n<!-- ok -->", "@kwagyeman Can you please resolve conflicts? Thanks!", "Can this get merged anytime soon?", "@petewarden  Can you please take a look at this? Thanks!", "@kwagyeman Can you please resolve conflicts? Thanks!", "Fixed! Can you guys merge this please. When you sit on it the build just breaks randomly.", "Can you guys merge this? You've sat on it for so long.", "It's literally 1 file being added now. No changes..."]}, {"number": 33291, "title": "added sprite options to Keras Tensorboard Callback", "body": "I added `embeddings_image_path` and `embeddings_image_size` so that keras users can use the sprite feature with the embedding projector.\r\n\r\n```python\r\nclass Tensorboard:\r\n    def __init__(...\r\n        embeddings_freq=0,\r\n        embeddings_metadata=None,\r\n        # added these\r\n        embeddings_image_path=None,\r\n        embeddings_image_size=None,\r\n    )\r\n```\r\n\r\nFixes issue #31050", "comments": ["@omalleyt12 adding you as you may have some context about TensorBoard callback, thanks", ">Could you add a unit test\r\n\r\nSure thing. It won't be in the next couple days though. \r\n\r\nFirst though, I think there needs to be a bit of discussion about how the embedding projector is expected to be used with the Tensorboard callback.\r\n\r\nJudging by the changes to the TB callback in v2, it seems like it's only intended to work with word embeddings and support for image embeddings was pretty much dropped (image embeddings aren't generally used via `Embedding` layers, and `embedding_layer_names`/`embedding_data` were removed as parameters. Now it only gathers Embedding layers)\r\n\r\nHere's an issue someone opened about the topic which describes the problem: https://github.com/tensorflow/tensorflow/issues/33230\r\n\r\nI think the solution would be to bring back some of the v1 code and just clean it up so that it's not so janky (with manual batching and all that)", "> Sure thing. It won't be in the next couple days though.\r\n\r\nThat works, thanks! Even just a simple test to show that the code with the new options sets runs and that the file it writes to is not empty would be fine (especially if you could attach an image to this PR showing how the resulting embeddings look on TensorBoard for a sample model)\r\n\r\n> it seems like it's only intended to work with word embeddings and support for image embeddings was pretty much dropped\r\n\r\nThis wasn't intended, as long as the TensorBoard projector can support image embeddings we should support them in the TensorBoard callback. Our test coverage was likely too poor to catch this regression.\r\n\r\nDoes this PR make it possible to visualize image embeddings?", ">Does this PR make it possible to visualize image embeddings?\r\n\r\nNot entirely. It supports image embeddings that make use of the Embedding layer, (but I'm not sure if that's even possible to do.) Making these changes in `v1` would though, but I'm not sure if we're supposed to make changes there.\r\n\r\nOnce we add `embedding_layer_names` (and `embedding_data`) back in we'll be able to visualize the output of a dense layer which is usually where image/non-discrete embeddings come from.", "@beasteers Can you please check build failures? Thanks!", "@beasteers Could you please address Ubuntu Sanity errors? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 33290, "title": "Tensorboard Embedding Projector - non-Embedding Layers?", "body": "I noticed at some point, the Tensorboard Callback stopped taking `embedding_layer_names`. Now it simply uses `isinstance(layer, Embedding)`\r\n\r\nSo my question is, how are we supposed to visualize the embeddings for say, an autoencoder, which would be a `Dense` layer?\r\n\r\nThe easiest solution would be to create a subclass of `layers.Embedding` that operates something like `tf.identity`, but obviously, this is hacky and not an ideal solution.\r\n\r\nI think the ideal solution would be either to create a BaseEmbedding layer that doesn't modify anything, but allows users to insert into a model where they want embeddings visualized.\r\n\r\nOr, adding something like `tf.summary.embedding` which seems like it would be fitting and semantic.\r\n\r\nHere's the change in question:\r\nhttps://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/callbacks.py#L1474-L1477\r\n", "comments": ["This is addressed in https://github.com/tensorflow/tensorflow/issues/33230"]}, {"number": 33289, "title": "Cannot import tensorflow.train", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nImporting tensorflow.train raises a `ModuleNotFoundError: No module named 'tensorflow.train'`.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error.\r\nNote: this used to work fine in tf-nightly-2.0-preview.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow.train\r\n```\r\n\r\n**Other info / logs**\r\n\r\nTraceback:\r\n\r\n```\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-1-0b4f8a92bbad> in <module>\r\n----> 1 import tensorflow.train\r\n\r\nModuleNotFoundError: No module named 'tensorflow.train'\r\n```\r\n\r\n**Workaround**\r\n\r\nInstead of:\r\n\r\n```python\r\nfrom tensorflow.train import BytesList\r\n```\r\n\r\nuse:\r\n\r\n```python\r\nimport tensorflow as tf\r\nBytesList = tf.train.BytesList\r\n```\r\n", "comments": ["issue replicating for TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/f346c12da815f02b9c54ad983e73e9cf/33289.ipynb) of colab.Thanks!", "Same issue here.", "@ageron \r\n\r\nThis issue was resolved in TF version 2.2.rc3(`!pip install tensorflow==2.2-rc3 `).Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/db65d7af7fbb1175072cdfbc82e2320b/untitled817.ipynb)Please verify once and close the issue. Thanks!", "I am closing this thread as the issue got resolved in latest TF versions.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33289\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33289\">No</a>\n"]}, {"number": 33288, "title": "Multiple builds of zlib library", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution:Linux Ubuntu 18.04.1 LTS\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version:1.15\r\n- Python version:3.7\r\n- Installed using virtualenv? pip? conda?:build with bazel\r\n- Bazel version (if compiling from source):0.26.1\r\n- GCC/Compiler version (if compiling from source):7.4.0\r\n\r\n**Describe the problem**\r\nI'm trying to build tensorflow from source with bazel.\r\nBut I faced errors like follow.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nCommand : bazel build --define=grpc-no-ares=true --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nERROR: /home/butterfly/tf/tensorflow/tensorflow/BUILD:563:1: Linking of rule '//tensorflow:libtensorflow_framework.so.1.15.0' failed (Exit 1)\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/adler32.pic.o: multiple definition of 'adler32'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/adler32.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/adler32.pic.o: multiple definition of 'adler32_combine'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/adler32.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/adler32.pic.o: multiple definition of 'adler32_combine64'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/adler32.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/adler32.pic.o: multiple definition of 'adler32_z'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/adler32.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/compress.pic.o: multiple definition of 'compress'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/compress.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/compress.pic.o: multiple definition of 'compress2'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/compress.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/compress.pic.o: multiple definition of 'compressBound'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/compress.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/crc32.pic.o: multiple definition of 'crc32'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/crc32.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/crc32.pic.o: multiple definition of 'crc32_combine'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/crc32.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/crc32.pic.o: multiple definition of 'crc32_combine64'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/crc32.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/crc32.pic.o: multiple definition of 'crc32_z'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/crc32.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/crc32.pic.o: multiple definition of 'get_crc_table'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/crc32.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflate'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflateBound'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflateCopy'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflateEnd'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflateGetDictionary'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflateInit2_'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflateInit_'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflateParams'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflatePending'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflatePrime'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflateReset'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflateResetKeep'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflateSetDictionary'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflateSetHeader'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflateTune'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/deflate.pic.o: multiple definition of 'deflate_copyright'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/deflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzclose.pic.o: multiple definition of 'gzclose'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzclose.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gz_error'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gzbuffer'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gzclearerr'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gzdopen'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gzeof'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gzerror'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gzoffset'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gzoffset64'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gzopen'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gzopen64'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gzrewind'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gzseek'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gzseek64'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gztell'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzlib.pic.o: multiple definition of 'gztell64'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzlib.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzread.pic.o: multiple definition of 'gzclose_r'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzread.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzread.pic.o: multiple definition of 'gzdirect'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzread.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzread.pic.o: multiple definition of 'gzfread'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzread.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzread.pic.o: multiple definition of 'gzgetc'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzread.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzread.pic.o: multiple definition of 'gzgetc_'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzread.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzread.pic.o: multiple definition of 'gzgets'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzread.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzread.pic.o: multiple definition of 'gzread'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzread.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzread.pic.o: multiple definition of 'gzungetc'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzread.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzwrite.pic.o: multiple definition of 'gzclose_w'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzwrite.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzwrite.pic.o: multiple definition of 'gzflush'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzwrite.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzwrite.pic.o: multiple definition of 'gzfwrite'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzwrite.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzwrite.pic.o: multiple definition of 'gzprintf'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzwrite.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzwrite.pic.o: multiple definition of 'gzputc'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzwrite.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzwrite.pic.o: multiple definition of 'gzputs'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzwrite.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzwrite.pic.o: multiple definition of 'gzsetparams'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzwrite.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzwrite.pic.o: multiple definition of 'gzvprintf'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzwrite.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/gzwrite.pic.o: multiple definition of 'gzwrite'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/gzwrite.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/infback.pic.o: multiple definition of 'inflateBack'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/infback.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/infback.pic.o: multiple definition of 'inflateBackEnd'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/infback.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/infback.pic.o: multiple definition of 'inflateBackInit_'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/infback.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inffast.pic.o: multiple definition of 'inflate_fast'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inffast.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflate'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateCodesUsed'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateCopy'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateEnd'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateGetDictionary'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateGetHeader'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateInit2_'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateInit_'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateMark'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflatePrime'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateReset'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateReset2'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateResetKeep'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateSetDictionary'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateSync'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateSyncPoint'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateUndermine'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inflate.pic.o: multiple definition of 'inflateValidate'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inflate.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inftrees.pic.o: multiple definition of 'inflate_copyright'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inftrees.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/inftrees.pic.o: multiple definition of 'inflate_table'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/inftrees.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/trees.pic.o: multiple definition of '_dist_code'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/trees.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/trees.pic.o: multiple definition of '_length_code'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/trees.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/trees.pic.o: multiple definition of '_tr_align'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/trees.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/trees.pic.o: multiple definition of '_tr_flush_bits'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/trees.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/trees.pic.o: multiple definition of '_tr_flush_block'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/trees.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/trees.pic.o: multiple definition of '_tr_init'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/trees.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/trees.pic.o: multiple definition of '_tr_stored_block'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/trees.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/trees.pic.o: multiple definition of '_tr_tally'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/trees.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/uncompr.pic.o: multiple definition of 'uncompress'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/uncompr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/uncompr.pic.o: multiple definition of 'uncompress2'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/uncompr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/zutil.pic.o: multiple definition of 'zError'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/zutil.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/zutil.pic.o: multiple definition of 'z_errmsg'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/zutil.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/zutil.pic.o: multiple definition of 'zcalloc'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/zutil.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/zutil.pic.o: multiple definition of 'zcfree'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/zutil.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/zutil.pic.o: multiple definition of 'zlibCompileFlags'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/zutil.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/zlib_archive/_objs/zlib/zutil.pic.o: multiple definition of 'zlibVersion'\r\n/usr/bin/ld.gold: bazel-out/host/bin/external/zlib/_objs/zlib/zutil.pic.o: previous definition here\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 16.852s, Critical Path: 5.74s\r\nINFO: 14 processes: 14 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["Can you check if #31196 helps?", "@butterfly-coder, Did you get a chance to look at the @mihaimaruseac's comment. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33288\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33288\">No</a>\n"]}, {"number": 33287, "title": "image rotate", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):tensorflow2.0\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n\r\nrotate an image with a degree as \r\ntf.contrib.image.rotate  in tensorflow1", "comments": ["@Stick-To \r\n\r\nCan you please go through the below link and see if it helps you.Thanks!\r\nhttps://www.tensorflow.org/addons/api_docs/python/tfa/image/rotate\r\n", "@ravikyram  Yes, thank you very much"]}]