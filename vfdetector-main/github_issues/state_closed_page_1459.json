[{"number": 9183, "title": "The running speed of Windows is lower than that in Linux", "body": "Please go to Stack Overflow for help and support. http://stackoverflow.com/questions/tagged/tensorflow\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### System Information\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*:\r\n- *TensorFlow installed from (source or binary)?*:\r\n- *TensorFlow version* (use command below):\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\n\r\nYou can collect some of this information using our environment capture script https://github.com/tensorflow/tensorflow/blob/master/tools/\r\nYou can collect the TensorFlow version with\r\n```sh\r\npython -c \"import tensorflow as tf; print (tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n\r\n### Describe the problem clearly\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["I install Tensorflow in Windows, and run the same code which I have already run in Linux. But I find the running time is twice as much as Linux.", "We're sorry, but without a reproducible test case, it is impossible to help you with this problem. Please submit a new issue when you can submit the exact code. Also include the exact platform information (see template above) for both your Linux and Windows machine."]}, {"number": 9182, "title": "tf.layers.conv3d channels_first not supported", "body": "Hi , \r\n\r\nWhen I use tf.layers.conv3d function, I meet a problem, there is a data_format setting in this function, and I want to set this to 'channels_first' , however error occurred: Data format \"channels_first\" not supported for inputs with rank 5.\r\nThe tensorflow version I used is 1.0.0, I wonder in which version does 'channels_first' is supported in 3d convolution?\r\n\r\nThanks\r\n\r\nQiang", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  **Make sure you also include the exact command if possible to produce  the output included in your test case**.   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Thanks,\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  1.0.0\r\n- **Bazel version (if compiling from source)**:0.4.5\r\n- **CUDA/cuDNN version**: 8.0/5\r\n- **GPU model and memory**: K40\r\n- **Exact command to reproduce**: \r\n\r\n\r\n### Describe the problem\r\n\r\nI want to use tensorflow to implement a 3D neural network, so I test conv3d first. I use tf.layers.conv3d to test a 3D convolutional layer. You can refer the source code below, there is one parameter to set for tf.layers.conv3d, which is 'data_format', there are two options for that parameter, one is 'channels_first' , the other is 'channels_last', there is no problem with 'channels_last', however, there are problems when I set 'data_format' to 'channels_first'. The output is like following:\r\n\r\n$python  conv3d_test.py\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nTraceback (most recent call last):\r\n  File \"/home/deng/qianglan/Opensource/3DModelParallel/conv3d_test.py\", line 26, in <module>\r\n    tf.app.run()\r\n  File \"/home/deng/anaconda/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/deng/qianglan/Opensource/3DModelParallel/conv3d_test.py\", line 23, in main\r\n    run_benchmark()\r\n  File \"/home/deng/qianglan/Opensource/3DModelParallel/conv3d_test.py\", line 14, in run_benchmark\r\n    test_output = tf.layers.conv3d(test_input, filters, kernel_size=(3,3,3), strides=[1, 1,1], padding='SAME', data_format='channels_first', trainable=False)\r\n  File \"/home/deng/anaconda/lib/python2.7/site-packages/tensorflow/python/layers/convolutional.py\", line 682, in conv3d\r\n    return layer.apply(inputs)\r\n  File \"/home/deng/anaconda/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 303, in apply\r\n    return self.__call__(inputs, **kwargs)\r\n  File \"/home/deng/anaconda/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 273, in __call__\r\n    outputs = self.call(inputs, **kwargs)\r\n  File \"/home/deng/anaconda/lib/python2.7/site-packages/tensorflow/python/layers/convolutional.py\", line 156, in call\r\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\r\n  File \"/home/deng/anaconda/lib/python2.7/site-packages/tensorflow/python/layers/utils.py\", line 49, in convert_data_format\r\n    raise ValueError('Data format \"channels_first\" not supported for '\r\nValueError: Data format \"channels_first\" not supported for inputs with rank 5.\r\n\r\nProcess finished with exit code 1\r\n\r\n\r\n### Source code / logs\r\n\r\nimport tensorflow as tf\r\n\r\ndef run_benchmark():\r\n    with tf.Graph().as_default():\r\n        input_shape = [1,64,224,224,224]\r\n        test_input = tf.Variable(tf.random_normal(input_shape, dtype=tf.float32, stddev=1e-1))\r\n        filters = 64\r\n        with tf.variable_scope('test_conv_layer'):\r\n            test_output = tf.layers.conv3d(test_input, filters, kernel_size=(3,3,3), strides=[1, 1,1],           padding='SAME', data_format='channels_first', trainable=False)\r\n\r\n        init = tf.global_variables_initializer()\r\n        sess = tf.Session()\r\n        sess.run(init)\r\n        result = sess.run(test_output)\r\n        print result.shape\r\n\r\ndef main(_):\r\n    run_benchmark()\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run()\r\n", "The doc says you have to have rank 4 [`(batch, channels, depth, height, width)`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L656) not 5. You can also see the example in the [test](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional_test.py#L85)."]}, {"number": 9181, "title": "fix filter position for Im2Col", "body": "cac52fdebb292bc7706383d07c7987be66882085 fixed the filter position for\r\nthe reference class but the same problem exists in the Im2Col class.\r\n\r\nThis patch makes the calculation of the filter position consistent\r\nwith the reference quantized class and the gemm/fused conv_ops\r\nclasses.\r\n\r\n@petewarden you reviewed the reference class fix. Would you have the time to review this too?", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@petewarden Can you give an official approval or comments for this PR? Thanks."]}, {"number": 9179, "title": "Failed to add dependency with tf.identity", "body": "### System Information\r\nArchLinux, TensorFlow 1.0.1 binary for py3.6\r\n- *Exact command to reproduce*:\r\n```python\r\nimport tensorflow as tf\r\na = tf.get_variable('a', shape=[])\r\nupdate_op = a.assign_add(1.0)\r\n\r\ncost = tf.reduce_mean(tf.get_variable('x', shape=[10]))\r\nwith tf.control_dependencies([update_op]):\r\n    cost = tf.identity(cost)\r\n\r\ntrain = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nfor k in range(10):\r\n    sess.run([cost])\r\n    print(sess.run([a]))    # a increases\r\n\r\nfor k in range(10):\r\n    sess.run([train])\r\n    print(sess.run([a]))    # a doesn't increase\r\n```\r\nLooks like the `train` op doesn't depend on `update_op`.\r\nIn terms of computation, backprop doesn't depend on the value of cost.  Is this intended?\r\n\r\nUPDATE: Turns out that `tf.gradients` doesn't have a dependency over its first argument, although intuitively I thought the opposite. This is a feature. Closing..", "comments": []}, {"number": 9178, "title": "Unable to use ci_build to build tensorflow", "body": "### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: No\r\n- *TensorFlow installed from (source or binary)?*: Source\r\n- *TensorFlow version*: `master` and  `f8dce81aeaff40dc78d398741854ad8766806f91`\r\n- *Bazel version (if compiling from source)*: 0.4.5 \r\n- *CUDA/cuDNN version*: N/A\r\n- *GPU Model and Memory*: N/A\r\n- *Exact command to reproduce*:\r\n\r\n`tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/...`\r\n\r\nNote: make sure you run with a clean state machine\r\n\r\n### Describe the problem clearly\r\n\r\nI'm trying to use ci_build tool to build tensorflow from source, but I am always got the error at step `/install/install_buildifier.sh`\r\n\r\n```\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\n____Loading package: buildifier\r\n____Loading package: @bazel_tools//tools/cpp\r\n____Loading package: @bazel_tools//tools/jdk\r\n____Loading package: @local_config_xcode//\r\n____Loading package: @local_jdk//\r\n____Loading package: @local_config_cc//\r\n____Loading complete.  Analyzing...\r\n____Loading package:\r\n____Loading package: @bazel_tools//tools/genrule\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 4,184,939 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 8,690,539 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 14,736,235 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 19,537,920 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 24,485,888 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 29,597,696 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 34,791,424 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 39,903,232 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 45,064,192 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 50,339,840 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 55,648,256 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 61,497,344 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 67,166,208 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 73,048,064 bytes\r\n____Downloading https://storage.googleapis.com/golang/go1.7.4.linux-amd64.tar.gz: 78,733,312 bytes\r\n____Loading package: @io_bazel_rules_go_toolchain//\r\nERROR: /buildifier/build/BUILD.bazel:4:1: no such package '@org_golang_x_tools//cmd/goyacc': no such package '@io_bazel_rules_go_repository_tools//': Traceback (most recent call last):\r\n    File \"/root/.cache/bazel/_bazel_root/972a279007c925820266f3ac7b1d6afd/external/io_bazel_rules_go/go/private/go_repositories.bzl\", line 85\r\n        _fetch_repository_tools_deps(ctx, goroot, gopath)\r\n    File \"/root/.cache/bazel/_bazel_root/972a279007c925820266f3ac7b1d6afd/external/io_bazel_rules_go/go/private/go_repositories.bzl\", line 54, in _fetch_repository_tools_deps\r\n        ctx.download_and_extract('%s/archive/%s.zip' % (dep.repo,...), <4 more arguments>)\r\njava.io.IOException: Prefix buildifier-4190564903f61ddc94bcfda3dc2cdd32d4b330e5 was given, but not found in the zip and referenced by '//build:parse.y.go_yacc'.\r\nERROR: Analysis of target '//buildifier:buildifier' failed; build aborted.\r\n____Elapsed time: 10.455s\r\n```\r\n\r\nI did google around and may be this is root cause https://github.com/bazelbuild/rules_go/issues/361", "comments": ["To build TensorFlow from sources, please follow [the procedure outlined in the website](https://www.tensorflow.org/install/install_sources).\r\n\r\nThe scripts in the `ci_build` directory are specific to the machine setup on the continuous integration Jenkins worker machines (ci.tensorflow.org). \r\n\r\nSince using the ci_build scripts for purposes other than the continuous integration setup is not a supported use case, I'd like to close this issue out.", "`ci_build` script is very helpful to me, i think it is not fair if you are not supporting it ..\r\n\r\nBy the way, I found the root cause of the problem: buildifier is renamed to buildtools. So the ci will fail if you run with from the beginning. I upgrade to the right version and it works now.\r\n\r\n```\r\n--- a/tensorflow/tools/ci_build/install/install_buildifier.sh\r\n+++ b/tensorflow/tools/ci_build/install/install_buildifier.sh\r\n@@ -17,7 +17,7 @@\r\n set -e\r\n BUILDIFIER_DIR=\"buildifier\"\r\n mkdir ${BUILDIFIER_DIR}\r\n-curl -Ls https://github.com/bazelbuild/buildifier/archive/0.4.3.tar.gz | \\\r\n+curl -Ls https://github.com/bazelbuild/buildtools/archive/0.4.5.tar.gz | \\\r\n     tar -C \"${BUILDIFIER_DIR}\" --strip-components=1 -xz\r\n pushd ${BUILDIFIER_DIR}\r\n```\r\nCan i open a PR for this?\r\n"]}, {"number": 9177, "title": "How to add an external java library(.jar) into Bazel build path.", "body": "### Describe the problem clearly\r\nI follow the rule in [Bazel website](https://bazel.build/versions/master/docs/be/android.html#android_binary.shrink_resources) to add a JAR into Bazel build path, this is my script:\r\n\r\n...\r\nandroid_binary(\r\n    ...\r\n    deps = [\r\n        \":tensorflow_native_libs\",\r\n        \"//tensorflow/contrib/android:android_tensorflow_inference_java\",\r\n\t**\":libs/nd4j-api-0.8.0.jar\"**\r\n    ],\r\n)\r\n...\r\n\r\nbut the log says that my JAR is misplaced.\r\nHow should I do to add a JAR library into build path or do I miss out something ?\r\n\r\nThanks,\r\n\r\n### Source Code / Logs\r\nERROR: /home/bob/deep_learning/tensorflow/tensorflow/examples/android/BUILD:76:12: in deps attribute of android_binary rule //tensorflow/examples/android:tensorflow_demo: file '//tensorflow/examples/android:libs/nd4j-api-0.8.0.jar' is misplaced here (expected no files).\r\nERROR: Analysis of target '//tensorflow/examples/android:tensorflow_demo' failed; build aborted.\r\n\r\n", "comments": ["This question is more about use of bazel than TensorFlow, so I'm going to close it out and recommend that you look into bazel documentation/support.\r\n\r\nThat said, perhaps you're looking for the [`java_import`](https://bazel.build/versions/master/docs/be/java.html#java_import) rule", "That is a correct answer. Thanks a lot ~"]}, {"number": 9176, "title": "ImportError: No module named 'tensorflow.python.debug.lib'", "body": "C:\\Users\\varada.vamsi\\AppData\\Local\\Programs\\Python\\Python35\\python.exe C:/Users/varada.vamsi/PycharmProjects/untitled/test.py\r\nTraceback (most recent call last):\r\n  File \"C:/Users/varada.vamsi/PycharmProjects/untitled/test.py\", line 31, in <module>\r\n    from tensorflow.python import debug as tf_debug\r\n  File \"C:\\Users\\varada.vamsi\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\debug\\__init__.py\", line 37, in <module>\r\n    from tensorflow.python.debug.lib.debug_data import DebugDumpDir\r\nImportError: No module named 'tensorflow.python.debug.lib'\r\n\r\nProcess finished with exit code 1", "comments": ["can any one help me in fixing the above issue No module named 'tensorflow.python.debug.lib'", "@vamsivarada Thanks for reporting this issue. May I get some additional information from to help me understand the issue better.\r\n1) What is the version of tensorflow you are using?\r\n2) What command are you running when this problem happens? If it's a custom Python scrpit/program, can you post the relevant part to help me reproduce this issue?\r\n\r\n", "hello ciasq iam using 1.0.1 tensorflow  flow while running the below code iam getting error\r\n# Copyright 2015 Conchylicultor. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\r\n\"\"\"\r\nMain script. See README.md for more information\r\n\r\nUse python 3\r\n\"\"\"\r\n\r\nimport argparse  # Command line parsing\r\nimport configparser  # Saving the models parameters\r\nimport datetime  # Chronometer\r\nimport os  # Files management\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport math\r\n\r\nfrom tqdm import tqdm  # Progress bar\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\nfrom chatbot.textdata import TextData\r\nfrom chatbot.model import Model\r\n\r\n\r\nclass Chatbot:\r\n    \"\"\"\r\n    Main class which launch the training or testing mode\r\n    \"\"\"\r\n\r\n    class TestMode:\r\n        \"\"\" Simple structure representing the different testing modes\r\n        \"\"\"\r\n        ALL = 'all'\r\n        INTERACTIVE = 'interactive'  # The user can write his own questions\r\n        DAEMON = 'daemon'  # The chatbot runs on background and can regularly be called to predict something\r\n\r\n    def __init__(self):\r\n        \"\"\"\r\n        \"\"\"\r\n        # Model/dataset parameters\r\n        self.args = None\r\n\r\n        # Task specific object\r\n        self.textData = None  # Dataset\r\n        self.model = None  # Sequence to sequence model\r\n\r\n        # Tensorflow utilities for convenience saving/logging\r\n        self.writer = None\r\n        self.saver = None\r\n        self.modelDir = ''  # Where the model is saved\r\n        self.globStep = 0  # Represent the number of iteration for the current model\r\n\r\n        # TensorFlow main session (we keep track for the daemon)\r\n        self.sess = None\r\n\r\n        # Filename and directories constants\r\n        self.MODEL_DIR_BASE = 'save/model'\r\n        self.MODEL_NAME_BASE = 'model'\r\n        self.MODEL_EXT = '.ckpt'\r\n        self.CONFIG_FILENAME = 'params.ini'\r\n        self.CONFIG_VERSION = '0.5'\r\n        self.TEST_IN_NAME = 'data/test/samples.txt'\r\n        self.TEST_OUT_SUFFIX = '_predictions.txt'\r\n        self.SENTENCES_PREFIX = ['Q: ', 'A: ']\r\n\r\n    @staticmethod\r\n    def parseArgs(args):\r\n        \"\"\"\r\n        Parse the arguments from the given command line\r\n        Args:\r\n            args (list<str>): List of arguments to parse. If None, the default sys.argv will be parsed\r\n        \"\"\"\r\n\r\n        parser = argparse.ArgumentParser()\r\n\r\n        # Global options\r\n        globalArgs = parser.add_argument_group('Global options')\r\n        globalArgs.add_argument('--test',\r\n                                nargs='?',\r\n                                choices=[Chatbot.TestMode.ALL, Chatbot.TestMode.INTERACTIVE, Chatbot.TestMode.DAEMON],\r\n                                const=Chatbot.TestMode.ALL, default=None,\r\n                                help='if present, launch the program try to answer all sentences from data/test/ with'\r\n                                     ' the defined model(s), in interactive mode, the user can wrote his own sentences,'\r\n                                     ' use daemon mode to integrate the chatbot in another program')\r\n        globalArgs.add_argument('--createDataset', action='store_true', help='if present, the program will only generate the dataset from the corpus (no training/testing)')\r\n        globalArgs.add_argument('--playDataset', type=int, nargs='?', const=10, default=None,  help='if set, the program  will randomly play some samples(can be use conjointly with createDataset if this is the only action you want to perform)')\r\n        globalArgs.add_argument('--reset', action='store_true', help='use this if you want to ignore the previous model present on the model directory (Warning: the model will be destroyed with all the folder content)')\r\n        globalArgs.add_argument('--verbose', action='store_true', help='When testing, will plot the outputs at the same time they are computed')\r\n        globalArgs.add_argument('--debug', action='store_true', help='run DeepQA with Tensorflow debug mode. Read TF documentation for more details on this.')\r\n        globalArgs.add_argument('--keepAll', action='store_true', help='If this option is set, all saved model will be kept (Warning: make sure you have enough free disk space or increase saveEvery)')  # TODO: Add an option to delimit the max size\r\n        globalArgs.add_argument('--modelTag', type=str, default=None, help='tag to differentiate which model to store/load')\r\n        globalArgs.add_argument('--rootDir', type=str, default=None, help='folder where to look for the models and data')\r\n        globalArgs.add_argument('--watsonMode', action='store_true', help='Inverse the questions and answer when training (the network try to guess the question)')\r\n        globalArgs.add_argument('--autoEncode', action='store_true', help='Randomly pick the question or the answer and use it both as input and output')\r\n        globalArgs.add_argument('--device', type=str, default=None, help='\\'gpu\\' or \\'cpu\\' (Warning: make sure you have enough free RAM), allow to choose on which hardware run the model')\r\n        globalArgs.add_argument('--seed', type=int, default=None, help='random seed for replication')\r\n\r\n        # Dataset options\r\n        datasetArgs = parser.add_argument_group('Dataset options')\r\n        datasetArgs.add_argument('--corpus', choices=TextData.corpusChoices(), default=TextData.corpusChoices()[0], help='corpus on which extract the dataset.')\r\n        datasetArgs.add_argument('--datasetTag', type=str, default='', help='add a tag to the dataset (file where to load the vocabulary and the precomputed samples, not the original corpus). Useful to manage multiple versions. Also used to define the file used for the lightweight format.')  # The samples are computed from the corpus if it does not exist already. There are saved in \\'data/samples/\\'\r\n        datasetArgs.add_argument('--ratioDataset', type=float, default=1.0, help='ratio of dataset used to avoid using the whole dataset')  # Not implemented, useless ?\r\n        datasetArgs.add_argument('--maxLength', type=int, default=10, help='maximum length of the sentence (for input and output), define number of maximum step of the RNN')\r\n        datasetArgs.add_argument('--filterVocab', type=int, default=1, help='remove rarelly used words (by default words used only once). 0 to keep all words.')\r\n        datasetArgs.add_argument('--skipLines', action='store_true', help='Generate training samples by only using even conversation lines as questions (and odd lines as answer). Useful to train the network on a particular person.')\r\n        datasetArgs.add_argument('--vocabularySize', type=int, default=40000, help='Limit the number of words in the vocabulary (0 for unlimited)')\r\n\r\n        # Network options (Warning: if modifying something here, also make the change on save/loadParams() )\r\n        nnArgs = parser.add_argument_group('Network options', 'architecture related option')\r\n        nnArgs.add_argument('--hiddenSize', type=int, default=512, help='number of hidden units in each RNN cell')\r\n        nnArgs.add_argument('--numLayers', type=int, default=2, help='number of rnn layers')\r\n        nnArgs.add_argument('--softmaxSamples', type=int, default=0, help='Number of samples in the sampled softmax loss function. A value of 0 deactivates sampled softmax')\r\n        nnArgs.add_argument('--initEmbeddings', action='store_true', help='if present, the program will initialize the embeddings with pre-trained word2vec vectors')\r\n        nnArgs.add_argument('--embeddingSize', type=int, default=64, help='embedding size of the word representation')\r\n        nnArgs.add_argument('--embeddingSource', type=str, default=\"GoogleNews-vectors-negative300.bin\", help='embedding file to use for the word representation')\r\n\r\n        # Training options\r\n        trainingArgs = parser.add_argument_group('Training options')\r\n        trainingArgs.add_argument('--numEpochs', type=int, default=30, help='maximum number of epochs to run')\r\n        trainingArgs.add_argument('--saveEvery', type=int, default=2000, help='nb of mini-batch step before creating a model checkpoint')\r\n        trainingArgs.add_argument('--batchSize', type=int, default=256, help='mini-batch size')\r\n        trainingArgs.add_argument('--learningRate', type=float, default=0.002, help='Learning rate')\r\n        trainingArgs.add_argument('--dropout', type=float, default=0.9, help='Dropout rate (keep probabilities)')\r\n\r\n        return parser.parse_args(args)\r\n\r\n    def main(self, args=None):\r\n        \"\"\"\r\n        Launch the training and/or the interactive mode\r\n        \"\"\"\r\n        print('Welcome to DeepQA v0.1 !')\r\n        print()\r\n        print('TensorFlow detected: v{}'.format(tf.__version__))\r\n\r\n        # General initialisation\r\n\r\n        self.args = self.parseArgs(args)\r\n\r\n        if not self.args.rootDir:\r\n            self.args.rootDir = os.getcwd()  # Use the current working directory\r\n\r\n        #tf.logging.set_verbosity(tf.logging.INFO) # DEBUG, INFO, WARN (default), ERROR, or FATAL\r\n\r\n        self.loadModelParams()  # Update the self.modelDir and self.globStep, for now, not used when loading Model (but need to be called before _getSummaryName)\r\n\r\n        self.textData = TextData(self.args)\r\n        # TODO: Add a mode where we can force the input of the decoder // Try to visualize the predictions for\r\n        # each word of the vocabulary / decoder input\r\n        # TODO: For now, the model are trained for a specific dataset (because of the maxLength which define the\r\n        # vocabulary). Add a compatibility mode which allow to launch a model trained on a different vocabulary (\r\n        # remap the word2id/id2word variables).\r\n        if self.args.createDataset:\r\n            print('Dataset created! Thanks for using this program')\r\n            return  # No need to go further\r\n\r\n        # Prepare the model\r\n        with tf.device(self.getDevice()):\r\n            self.model = Model(self.args, self.textData)\r\n\r\n        # Saver/summaries\r\n        self.writer = tf.summary.FileWriter(self._getSummaryName())\r\n        self.saver = tf.train.Saver(max_to_keep=200)\r\n\r\n        # TODO: Fixed seed (WARNING: If dataset shuffling, make sure to do that after saving the\r\n        # dataset, otherwise, all which cames after the shuffling won't be replicable when\r\n        # reloading the dataset). How to restore the seed after loading ??\r\n        # Also fix seed for random.shuffle (does it works globally for all files ?)\r\n\r\n        # Running session\r\n        self.sess = tf.Session(config=tf.ConfigProto(\r\n            allow_soft_placement=True,  # Allows backup device for non GPU-available operations (when forcing GPU)\r\n            log_device_placement=False)  # Too verbose ?\r\n        )  # TODO: Replace all sess by self.sess (not necessary a good idea) ?\r\n\r\n        if self.args.debug:\r\n            self.sess = tf_debug.LocalCLIDebugWrapperSession(self.sess)\r\n            self.sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\r\n\r\n        print('Initialize variables...')\r\n        self.sess.run(tf.global_variables_initializer())\r\n\r\n        # Reload the model eventually (if it exist.), on testing mode, the models are not loaded here (but in predictTestset)\r\n        if self.args.test != Chatbot.TestMode.ALL:\r\n            self.managePreviousModel(self.sess)\r\n\r\n        # Initialize embeddings with pre-trained word2vec vectors\r\n        if self.args.initEmbeddings:\r\n            self.loadEmbedding(self.sess)\r\n\r\n        if self.args.test:\r\n            if self.args.test == Chatbot.TestMode.INTERACTIVE:\r\n                self.mainTestInteractive(self.sess)\r\n            elif self.args.test == Chatbot.TestMode.ALL:\r\n                print('Start predicting...')\r\n                self.predictTestset(self.sess)\r\n                print('All predictions done')\r\n            elif self.args.test == Chatbot.TestMode.DAEMON:\r\n                print('Daemon mode, running in background...')\r\n            else:\r\n                raise RuntimeError('Unknown test mode: {}'.format(self.args.test))  # Should never happen\r\n        else:\r\n            self.mainTrain(self.sess)\r\n\r\n        if self.args.test != Chatbot.TestMode.DAEMON:\r\n            self.sess.close()\r\n            print(\"The End! Thanks for using this program\")\r\n\r\n    def mainTrain(self, sess):\r\n        \"\"\" Training loop\r\n        Args:\r\n            sess: The current running session\r\n        \"\"\"\r\n\r\n        # Specific training dependent loading\r\n\r\n        self.textData.makeLighter(self.args.ratioDataset)  # Limit the number of training samples\r\n\r\n        mergedSummaries = tf.summary.merge_all()  # Define the summary operator (Warning: Won't appear on the tensorboard graph)\r\n        if self.globStep == 0:  # Not restoring from previous run\r\n            self.writer.add_graph(sess.graph)  # First time only\r\n\r\n        # If restoring a model, restore the progression bar ? and current batch ?\r\n\r\n        print('Start training (press Ctrl+C to save and exit)...')\r\n\r\n        try:  # If the user exit while training, we still try to save the model\r\n            for e in range(self.args.numEpochs):\r\n\r\n                print()\r\n                print(\"----- Epoch {}/{} ; (lr={}) -----\".format(e+1, self.args.numEpochs, self.args.learningRate))\r\n\r\n                batches = self.textData.getBatches()\r\n\r\n                # TODO: Also update learning parameters eventually\r\n\r\n                tic = datetime.datetime.now()\r\n                for nextBatch in tqdm(batches, desc=\"Training\"):\r\n                    # Training pass\r\n                    ops, feedDict = self.model.step(nextBatch)\r\n                    assert len(ops) == 2  # training, loss\r\n                    _, loss, summary = sess.run(ops + (mergedSummaries,), feedDict)\r\n                    self.writer.add_summary(summary, self.globStep)\r\n                    self.globStep += 1\r\n\r\n                    # Output training status\r\n                    if self.globStep % 100 == 0:\r\n                        perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\r\n                        tqdm.write(\"----- Step %d -- Loss %.2f -- Perplexity %.2f\" % (self.globStep, loss, perplexity))\r\n\r\n                    # Checkpoint\r\n                    if self.globStep % self.args.saveEvery == 0:\r\n                        self._saveSession(sess)\r\n\r\n                toc = datetime.datetime.now()\r\n\r\n                print(\"Epoch finished in {}\".format(toc-tic))  # Warning: Will overflow if an epoch takes more than 24 hours, and the output isn't really nicer\r\n        except (KeyboardInterrupt, SystemExit):  # If the user press Ctrl+C while testing progress\r\n            print('Interruption detected, exiting the program...')\r\n\r\n        self._saveSession(sess)  # Ultimate saving before complete exit\r\n\r\n    def predictTestset(self, sess):\r\n        \"\"\" Try predicting the sentences from the samples.txt file.\r\n        The sentences are saved on the modelDir under the same name\r\n        Args:\r\n            sess: The current running session\r\n        \"\"\"\r\n\r\n        # Loading the file to predict\r\n        with open(os.path.join(self.args.rootDir, self.TEST_IN_NAME), 'r') as f:\r\n            lines = f.readlines()\r\n\r\n        modelList = self._getModelList()\r\n        if not modelList:\r\n            print('Warning: No model found in \\'{}\\'. Please train a model before trying to predict'.format(self.modelDir))\r\n            return\r\n\r\n        # Predicting for each model present in modelDir\r\n        for modelName in sorted(modelList):  # TODO: Natural sorting\r\n            print('Restoring previous model from {}'.format(modelName))\r\n            self.saver.restore(sess, modelName)\r\n            print('Testing...')\r\n\r\n            saveName = modelName[:-len(self.MODEL_EXT)] + self.TEST_OUT_SUFFIX  # We remove the model extension and add the prediction suffix\r\n            with open(saveName, 'w') as f:\r\n                nbIgnored = 0\r\n                for line in tqdm(lines, desc='Sentences'):\r\n                    question = line[:-1]  # Remove the endl character\r\n\r\n                    answer = self.singlePredict(question)\r\n                    if not answer:\r\n                        nbIgnored += 1\r\n                        continue  # Back to the beginning, try again\r\n\r\n                    predString = '{x[0]}{0}\\n{x[1]}{1}\\n\\n'.format(question, self.textData.sequence2str(answer, clean=True), x=self.SENTENCES_PREFIX)\r\n                    if self.args.verbose:\r\n                        tqdm.write(predString)\r\n                    f.write(predString)\r\n                print('Prediction finished, {}/{} sentences ignored (too long)'.format(nbIgnored, len(lines)))\r\n\r\n    def mainTestInteractive(self, sess):\r\n        \"\"\" Try predicting the sentences that the user will enter in the console\r\n        Args:\r\n            sess: The current running session\r\n        \"\"\"\r\n        # TODO: If verbose mode, also show similar sentences from the training set with the same words (include in mainTest also)\r\n        # TODO: Also show the top 10 most likely predictions for each predicted output (when verbose mode)\r\n        # TODO: Log the questions asked for latter re-use (merge with test/samples.txt)\r\n\r\n        print('Testing: Launch interactive mode:')\r\n        print('')\r\n        print('Welcome to the interactive mode, here you can ask to Deep Q&A the sentence you want. Don\\'t have high '\r\n              'expectation. Type \\'exit\\' or just press ENTER to quit the program. Have fun.')\r\n\r\n        while True:\r\n            question = input(self.SENTENCES_PREFIX[0])\r\n            if question == '' or question == 'exit':\r\n                break\r\n\r\n            questionSeq = []  # Will be contain the question as seen by the encoder\r\n            answer = self.singlePredict(question, questionSeq)\r\n            if not answer:\r\n                print('Warning: sentence too long, sorry. Maybe try a simpler sentence.')\r\n                continue  # Back to the beginning, try again\r\n\r\n            print('{}{}'.format(self.SENTENCES_PREFIX[1], self.textData.sequence2str(answer, clean=True)))\r\n\r\n            if self.args.verbose:\r\n                print(self.textData.batchSeq2str(questionSeq, clean=True, reverse=True))\r\n                print(self.textData.sequence2str(answer))\r\n\r\n            print()\r\n\r\n    def singlePredict(self, question, questionSeq=None):\r\n        \"\"\" Predict the sentence\r\n        Args:\r\n            question (str): the raw input sentence\r\n            questionSeq (List<int>): output argument. If given will contain the input batch sequence\r\n        Return:\r\n            list <int>: the word ids corresponding to the answer\r\n        \"\"\"\r\n        # Create the input batch\r\n        batch = self.textData.sentence2enco(question)\r\n        if not batch:\r\n            return None\r\n        if questionSeq is not None:  # If the caller want to have the real input\r\n            questionSeq.extend(batch.encoderSeqs)\r\n\r\n        # Run the model\r\n        ops, feedDict = self.model.step(batch)\r\n        output = self.sess.run(ops[0], feedDict)  # TODO: Summarize the output too (histogram, ...)\r\n        answer = self.textData.deco2sentence(output)\r\n\r\n        return answer\r\n\r\n    def daemonPredict(self, sentence):\r\n        \"\"\" Return the answer to a given sentence (same as singlePredict() but with additional cleaning)\r\n        Args:\r\n            sentence (str): the raw input sentence\r\n        Return:\r\n            str: the human readable sentence\r\n        \"\"\"\r\n        return self.textData.sequence2str(\r\n            self.singlePredict(sentence),\r\n            clean=True\r\n        )\r\n\r\n    def daemonClose(self):\r\n        \"\"\" A utility function to close the daemon when finish\r\n        \"\"\"\r\n        print('Exiting the daemon mode...')\r\n        self.sess.close()\r\n        print('Daemon closed.')\r\n\r\n    def loadEmbedding(self, sess):\r\n        \"\"\" Initialize embeddings with pre-trained word2vec vectors\r\n        Will modify the embedding weights of the current loaded model\r\n        Uses the GoogleNews pre-trained values (path hardcoded)\r\n        \"\"\"\r\n\r\n        # Fetch embedding variables from model\r\n        with tf.variable_scope(\"embedding_rnn_seq2seq/rnn/embedding_wrapper\", reuse=True):\r\n            em_in = tf.get_variable(\"embedding\")\r\n        with tf.variable_scope(\"embedding_rnn_seq2seq/embedding_rnn_decoder\", reuse=True):\r\n            em_out = tf.get_variable(\"embedding\")\r\n\r\n        # Disable training for embeddings\r\n        variables = tf.get_collection_ref(tf.GraphKeys.TRAINABLE_VARIABLES)\r\n        variables.remove(em_in)\r\n        variables.remove(em_out)\r\n\r\n        # If restoring a model, we can leave here\r\n        if self.globStep != 0:\r\n            return\r\n\r\n        # New model, we load the pre-trained word2vec data and initialize embeddings\r\n        embeddings_path = os.path.join(self.args.rootDir, 'data', 'embeddings', self.args.embeddingSource)\r\n        embeddings_format = os.path.splitext(embeddings_path)[1][1:]\r\n        print(\"Loading pre-trained word embeddings from %s \" % embeddings_path)\r\n        with open(embeddings_path, \"rb\") as f:\r\n            header = f.readline()\r\n            vocab_size, vector_size = map(int, header.split())\r\n            binary_len = np.dtype('float32').itemsize * vector_size\r\n            initW = np.random.uniform(-0.25,0.25,(len(self.textData.word2id), vector_size))\r\n            for line in tqdm(range(vocab_size)):\r\n                word = []\r\n                while True:\r\n                    ch = f.read(1)\r\n                    if ch == b' ':\r\n                        word = b''.join(word).decode('utf-8')\r\n                        break\r\n                    if ch != b'\\n':\r\n                        word.append(ch)\r\n                if word in self.textData.word2id:\r\n                    if embeddings_format == 'bin':\r\n                        vector = np.fromstring(f.read(binary_len), dtype='float32')\r\n                    elif embeddings_format == 'vec':\r\n                        vector = np.fromstring(f.readline(), sep=' ', dtype='float32')\r\n                    else:\r\n                        raise Exception(\"Unkown format for embeddings: %s \" % embeddings_format)\r\n                    initW[self.textData.word2id[word]] = vector\r\n                else:\r\n                    if embeddings_format == 'bin':\r\n                        f.read(binary_len)\r\n                    elif embeddings_format == 'vec':\r\n                        f.readline()\r\n                    else:\r\n                        raise Exception(\"Unkown format for embeddings: %s \" % embeddings_format)\r\n\r\n        # PCA Decomposition to reduce word2vec dimensionality\r\n        if self.args.embeddingSize < vector_size:\r\n            U, s, Vt = np.linalg.svd(initW, full_matrices=False)\r\n            S = np.zeros((vector_size, vector_size), dtype=complex)\r\n            S[:vector_size, :vector_size] = np.diag(s)\r\n            initW = np.dot(U[:, :self.args.embeddingSize], S[:self.args.embeddingSize, :self.args.embeddingSize])\r\n\r\n        # Initialize input and output embeddings\r\n        sess.run(em_in.assign(initW))\r\n        sess.run(em_out.assign(initW))\r\n\r\n\r\n    def managePreviousModel(self, sess):\r\n        \"\"\" Restore or reset the model, depending of the parameters\r\n        If the destination directory already contains some file, it will handle the conflict as following:\r\n         * If --reset is set, all present files will be removed (warning: no confirmation is asked) and the training\r\n         restart from scratch (globStep & cie reinitialized)\r\n         * Otherwise, it will depend of the directory content. If the directory contains:\r\n           * No model files (only summary logs): works as a reset (restart from scratch)\r\n           * Other model files, but modelName not found (surely keepAll option changed): raise error, the user should\r\n           decide by himself what to do\r\n           * The right model file (eventually some other): no problem, simply resume the training\r\n        In any case, the directory will exist as it has been created by the summary writer\r\n        Args:\r\n            sess: The current running session\r\n        \"\"\"\r\n\r\n        print('WARNING: ', end='')\r\n\r\n        modelName = self._getModelName()\r\n\r\n        if os.listdir(self.modelDir):\r\n            if self.args.reset:\r\n                print('Reset: Destroying previous model at {}'.format(self.modelDir))\r\n            # Analysing directory content\r\n            elif os.path.exists(modelName):  # Restore the model\r\n                print('Restoring previous model from {}'.format(modelName))\r\n                self.saver.restore(sess, modelName)  # Will crash when --reset is not activated and the model has not been saved yet\r\n            elif self._getModelList():\r\n                print('Conflict with previous models.')\r\n                raise RuntimeError('Some models are already present in \\'{}\\'. You should check them first (or re-try with the keepAll flag)'.format(self.modelDir))\r\n            else:  # No other model to conflict with (probably summary files)\r\n                print('No previous model found, but some files found at {}. Cleaning...'.format(self.modelDir))  # Warning: No confirmation asked\r\n                self.args.reset = True\r\n\r\n            if self.args.reset:\r\n                fileList = [os.path.join(self.modelDir, f) for f in os.listdir(self.modelDir)]\r\n                for f in fileList:\r\n                    print('Removing {}'.format(f))\r\n                    os.remove(f)\r\n\r\n        else:\r\n            print('No previous model found, starting from clean directory: {}'.format(self.modelDir))\r\n\r\n    def _saveSession(self, sess):\r\n        \"\"\" Save the model parameters and the variables\r\n        Args:\r\n            sess: the current session\r\n        \"\"\"\r\n        tqdm.write('Checkpoint reached: saving model (don\\'t stop the run)...')\r\n        self.saveModelParams()\r\n        model_name = self._getModelName()\r\n        with open(model_name, 'w') as f:  # HACK: Simulate the old model existance to avoid rewriting the file parser\r\n            f.write('This file is used internally by DeepQA to check the model existance. Please do not remove.\\n')\r\n        self.saver.save(sess, model_name)  # TODO: Put a limit size (ex: 3GB for the modelDir)\r\n        tqdm.write('Model saved.')\r\n\r\n    def _getModelList(self):\r\n        \"\"\" Return the list of the model files inside the model directory\r\n        \"\"\"\r\n        return [os.path.join(self.modelDir, f) for f in os.listdir(self.modelDir) if f.endswith(self.MODEL_EXT)]\r\n\r\n    def loadModelParams(self):\r\n        \"\"\" Load the some values associated with the current model, like the current globStep value\r\n        For now, this function does not need to be called before loading the model (no parameters restored). However,\r\n        the modelDir name will be initialized here so it is required to call this function before managePreviousModel(),\r\n        _getModelName() or _getSummaryName()\r\n        Warning: if you modify this function, make sure the changes mirror saveModelParams, also check if the parameters\r\n        should be reset in managePreviousModel\r\n        \"\"\"\r\n        # Compute the current model path\r\n        self.modelDir = os.path.join(self.args.rootDir, self.MODEL_DIR_BASE)\r\n        if self.args.modelTag:\r\n            self.modelDir += '-' + self.args.modelTag\r\n\r\n        # If there is a previous model, restore some parameters\r\n        configName = os.path.join(self.modelDir, self.CONFIG_FILENAME)\r\n        if not self.args.reset and not self.args.createDataset and os.path.exists(configName):\r\n            # Loading\r\n            config = configparser.ConfigParser()\r\n            config.read(configName)\r\n\r\n            # Check the version\r\n            currentVersion = config['General'].get('version')\r\n            if currentVersion != self.CONFIG_VERSION:\r\n                raise UserWarning('Present configuration version {0} does not match {1}. You can try manual changes on \\'{2}\\''.format(currentVersion, self.CONFIG_VERSION, configName))\r\n\r\n            # Restoring the the parameters\r\n            self.globStep = config['General'].getint('globStep')\r\n            self.args.watsonMode = config['General'].getboolean('watsonMode')\r\n            self.args.autoEncode = config['General'].getboolean('autoEncode')\r\n            self.args.corpus = config['General'].get('corpus')\r\n\r\n            self.args.datasetTag = config['Dataset'].get('datasetTag')\r\n            self.args.maxLength = config['Dataset'].getint('maxLength')  # We need to restore the model length because of the textData associated and the vocabulary size (TODO: Compatibility mode between different maxLength)\r\n            self.args.filterVocab = config['Dataset'].getint('filterVocab')\r\n            self.args.skipLines = config['Dataset'].getboolean('skipLines')\r\n            self.args.vocabularySize = config['Dataset'].getint('vocabularySize')\r\n\r\n            self.args.hiddenSize = config['Network'].getint('hiddenSize')\r\n            self.args.numLayers = config['Network'].getint('numLayers')\r\n            self.args.softmaxSamples = config['Network'].getint('softmaxSamples')\r\n            self.args.initEmbeddings = config['Network'].getboolean('initEmbeddings')\r\n            self.args.embeddingSize = config['Network'].getint('embeddingSize')\r\n            self.args.embeddingSource = config['Network'].get('embeddingSource')\r\n\r\n            # No restoring for training params, batch size or other non model dependent parameters\r\n\r\n            # Show the restored params\r\n            print()\r\n            print('Warning: Restoring parameters:')\r\n            print('globStep: {}'.format(self.globStep))\r\n            print('watsonMode: {}'.format(self.args.watsonMode))\r\n            print('autoEncode: {}'.format(self.args.autoEncode))\r\n            print('corpus: {}'.format(self.args.corpus))\r\n            print('datasetTag: {}'.format(self.args.datasetTag))\r\n            print('maxLength: {}'.format(self.args.maxLength))\r\n            print('filterVocab: {}'.format(self.args.filterVocab))\r\n            print('skipLines: {}'.format(self.args.skipLines))\r\n            print('vocabularySize: {}'.format(self.args.vocabularySize))\r\n            print('hiddenSize: {}'.format(self.args.hiddenSize))\r\n            print('numLayers: {}'.format(self.args.numLayers))\r\n            print('softmaxSamples: {}'.format(self.args.softmaxSamples))\r\n            print('initEmbeddings: {}'.format(self.args.initEmbeddings))\r\n            print('embeddingSize: {}'.format(self.args.embeddingSize))\r\n            print('embeddingSource: {}'.format(self.args.embeddingSource))\r\n            print()\r\n\r\n        # For now, not arbitrary  independent maxLength between encoder and decoder\r\n        self.args.maxLengthEnco = self.args.maxLength\r\n        self.args.maxLengthDeco = self.args.maxLength + 2\r\n\r\n        if self.args.watsonMode:\r\n            self.SENTENCES_PREFIX.reverse()\r\n\r\n\r\n    def saveModelParams(self):\r\n        \"\"\" Save the params of the model, like the current globStep value\r\n        Warning: if you modify this function, make sure the changes mirror loadModelParams\r\n        \"\"\"\r\n        config = configparser.ConfigParser()\r\n        config['General'] = {}\r\n        config['General']['version']  = self.CONFIG_VERSION\r\n        config['General']['globStep']  = str(self.globStep)\r\n        config['General']['watsonMode'] = str(self.args.watsonMode)\r\n        config['General']['autoEncode'] = str(self.args.autoEncode)\r\n        config['General']['corpus'] = str(self.args.corpus)\r\n\r\n        config['Dataset'] = {}\r\n        config['Dataset']['datasetTag'] = str(self.args.datasetTag)\r\n        config['Dataset']['maxLength'] = str(self.args.maxLength)\r\n        config['Dataset']['filterVocab'] = str(self.args.filterVocab)\r\n        config['Dataset']['skipLines'] = str(self.args.skipLines)\r\n        config['Dataset']['vocabularySize'] = str(self.args.vocabularySize)\r\n\r\n        config['Network'] = {}\r\n        config['Network']['hiddenSize'] = str(self.args.hiddenSize)\r\n        config['Network']['numLayers'] = str(self.args.numLayers)\r\n        config['Network']['softmaxSamples'] = str(self.args.softmaxSamples)\r\n        config['Network']['initEmbeddings'] = str(self.args.initEmbeddings)\r\n        config['Network']['embeddingSize'] = str(self.args.embeddingSize)\r\n        config['Network']['embeddingSource'] = str(self.args.embeddingSource)\r\n\r\n        # Keep track of the learning params (but without restoring them)\r\n        config['Training (won\\'t be restored)'] = {}\r\n        config['Training (won\\'t be restored)']['learningRate'] = str(self.args.learningRate)\r\n        config['Training (won\\'t be restored)']['batchSize'] = str(self.args.batchSize)\r\n        config['Training (won\\'t be restored)']['dropout'] = str(self.args.dropout)\r\n\r\n        with open(os.path.join(self.modelDir, self.CONFIG_FILENAME), 'w') as configFile:\r\n            config.write(configFile)\r\n\r\n    def _getSummaryName(self):\r\n        \"\"\" Parse the argument to decide were to save the summary, at the same place that the model\r\n        The folder could already contain logs if we restore the training, those will be merged\r\n        Return:\r\n            str: The path and name of the summary\r\n        \"\"\"\r\n        return self.modelDir\r\n\r\n    def _getModelName(self):\r\n        \"\"\" Parse the argument to decide were to save/load the model\r\n        This function is called at each checkpoint and the first time the model is load. If keepAll option is set, the\r\n        globStep value will be included in the name.\r\n        Return:\r\n            str: The path and name were the model need to be saved\r\n        \"\"\"\r\n        modelName = os.path.join(self.modelDir, self.MODEL_NAME_BASE)\r\n        if self.args.keepAll:  # We do not erase the previously saved model by including the current step on the name\r\n            modelName += '-' + str(self.globStep)\r\n        return modelName + self.MODEL_EXT\r\n\r\n    def getDevice(self):\r\n        \"\"\" Parse the argument to decide on which device run the model\r\n        Return:\r\n            str: The name of the device on which run the program\r\n        \"\"\"\r\n        if self.args.device == 'cpu':\r\n            return '/cpu:0'\r\n        elif self.args.device == 'gpu':\r\n            return '/gpu:0'\r\n        elif self.args.device is None:  # No specified device (default)\r\n            return None\r\n        else:\r\n            print('Warning: Error in the device name: {}, use the default device'.format(self.args.device))\r\n            return None\r\nif __name__ == '__main__':\r\n    chat = Chatbot()\r\n    chat.main()\r\n\r\n", "@vamsivarada Thanks for the detailed info. However, the license text you copy-pasted doesn't have information about the version of TensorFlow you are using. \r\n\r\nCan you do the following in Python to get the version:\r\nimport tensorflow as tf\r\nprint(tf.__version__)", "![capture](https://cloud.githubusercontent.com/assets/11680996/25030811/63b7b0e0-20e5-11e7-8cb7-703d0430aa78.JPG)\r\nThanks for your quick turn around time ciasq the version which i am using for tensor flow is 1.0.1", "@vamsivarada TensorFlow 1.0.x is known to have bugs on Windows. Can you try downloading and installing the latest version (1.1.0rc1)?\r\nhttps://pypi.python.org/pypi/tensorflow", "I have tried using two ways for installing the tensorflow the version you have specified not able to install it please find the below screens\r\n![image](https://cloud.githubusercontent.com/assets/11680996/25031039/6f16bdda-20e7-11e7-9362-9ff267eeed9f.png)\r\n![image](https://cloud.githubusercontent.com/assets/11680996/25031041/7a2a4ca0-20e7-11e7-8a1d-7eb7fecc126f.png)\r\n", "@vamsivarada did you try following the instructions at\r\nhttps://www.tensorflow.org/install/install_windows ?", "Thanks for your detailed help caisq , I have solved my above issue. It would be great if you help me in getting sample folder structure or example for chat-bot(which automatically responds to the questions asked by user)."]}, {"number": 9175, "title": "Nadam optimizer", "body": "It has the implementation of Nadam optimizer by using a use_nesterov flag in Adam optimizer (as done in Momentum class)\r\n\r\nIssue link: https://github.com/tensorflow/tensorflow/issues/7715", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Could you:\r\n1. Sign the CLA\r\n2. git pull rebase and push again\r\n\r\nTake a look at the recent merge:\r\nhttps://github.com/tensorflow/tensorflow/pull/8405\r\n\r\n@lukaszkaiser it looks like this implementation might be more compact and easier to maintain? Rather than splitting the class into two, it modifies the current class.", "I fully agree. But the other CL also had RAdam. I'm not sure how to do it best to assure these are easy to maintain. This one should also have the advantage of working with LazyAdam.", "@lukaszkaiser @drpngx I agree @jitindua's code looks more compact and has testing and maintenance advantages.\r\n\r\nOne of my calculations may be more efficient in the CPU version of Nadam because of how I'm using Eigen. Perhaps @jitindua could try to adapt this efficiency if you're gonna go with his PR? See code near https://github.com/tensorflow/tensorflow/pull/8405/files#diff-fee33779233fb6ff133bbb93c7830880R301\r\n\r\nOne of the subtle / critical issues I was fixing with my Nadam interface (which this can't fix) was setting saner default decay factors.\r\n\r\nKingma & Ba's paper on Adam only sets beta1=0.9 and beta2=0.999 to promote their pet initialization scheme (that no one uses). And if you don't use it, those factors are absurdly large + bad. I know in theory that developers could set good decay factors on their own. But in practice, 100 out of 100 developers incorrectly compensate for Adam's poorly set default decay factors by setting depressingly low learning rates to prevent it from diverging instantly.\r\n\r\nIn some sense, Radam itself is a research \"discovery\" about how wrong the most widely-used learning rate / decay factors in Adam are. Its gamma factor is just a more convenient knob to help change beta1 and beta2 in the correct proportions so you can properly decrease them in tandem.\r\n\r\nHow about you pull this PR, and I'll submit two thin, python-only, init-only front-ends into core (like my current Radam one) so that there's an explicit Radam call and a Nadam optimizer that doesn't have unconscionably bad default decay factors? ", "@louiehelm Just to give you some context: I was fighting for different defaults in TF Adam since before the first release of TF. Unluckily, people protest when things become incomparable to other frameworks and papers, so here we are with bad defaults (and we probably have to stick to them in the core).\r\n\r\nHow about this: we merge @jitindua's code first (please, take a good look at it too and comment and help to make it efficient) -- and this goes into core (it's just a slight change of core API, I hope Martin will be ok with that). Then, using his changes, we merge your NAdam and RAdam into contrib/opt, also ensuring they handle sparse updates fast, same as LazyAdam which is also in contrib/opt (as you saw, Martin is not ok to have them in core API yet). Later, when everyone starts using them instead of the core Adam, we'll migrate them into core. Does that sound ok?", "@lukaszkaiser sorry to hijack the conversation a bit, but where can I find out about better defaults?", "I guess @louiehelm can tell you more. I was using large epsilon (1e-4) to stabilize for a while, then I lowered beta1 to 0.85 and beta2 to 0.997 and brought epsilon back down to 1e-7. But I doubt this is the best. @louiehelm what do you use with pure Adam?", "Cool thanks, also what NADAM parameters do you recommend? Perhaps that could be included in the docstring if they won't be defaults?\r\n\r\nI know performance will vary by application but maybe there is a good place to provide reference values to reduce the searching (via hyperopt or google) required.", "I've had good performance with beta1 = 0.825, beta2 = 0.99685, lr = double.\r\n\r\nIf you have a well-tuned model using Adam's stock decay params, this beta2 lets you simply double the learning rate, while still landing back in the same favorable gradient landscape you were navigating into before (but with more stability).\r\n\r\nI didn't do the explicit math, so there might be a more precise set of beta1 / beta2 that give this benefit. Given how many people are getting comfortable with well-tuned models that hum along with Adam, maybe this is a desirable target for future default decay factors, so that we can easily migrate researchers / developers to a new standard. At very least, figuring out decay factors that do this and then mentioning them in Adam's documentation like @ahundt suggests seems wise.\r\n\r\nI share @lukaszkaiser's intuition that epsilon should be higher than 1e-8 but never benefited from raising it myself in practice. However, that's a parameter you wouldn't see harming your results until it bites you... so I maybe just got lucky. I haven't done the proper data interrogation to see the warning signs if 1e-8 is concerningly low or not.", "@jitindua how much bandwidth do you have for this? Could you sign the CLA?", "I only have problems with epsilon when training on tasks where the accuracy gets very high, and gradients very low, sometimes close to 0. Some tasks are like this, you want to get to 99% accuracy. But then with low epsilon Adam makes this crazy large step somewhere and gets bad. I think that a good way would be instead of having `(sqrt(v_t) + epsilon)` to have something smarter, like `max(sqrt(v_t), epsilon)` but maybe a little smoothed. So one could set epsilon higher without effecting all the normal gradients. Not sure if it's possible though, but I'd love not to have to care about that...", "@lukaszkaiser @jitindua What's the verdict here?", "Again, here is my suggestion:\r\n\r\nHow about this: we merge @jitindua's code first (please, take a good look at it too and comment and help to make it efficient) -- and this goes into core (it's just a slight change of core API, I hope Martin will be ok with that). Then, using his changes, we merge your NAdam and RAdam into contrib/opt, also ensuring they handle sparse updates fast, same as LazyAdam which is also in contrib/opt (as you saw, Martin is not ok to have them in core API yet). Later, when everyone starts using them instead of the core Adam, we'll migrate them into core.\r\n\r\nDo we all agree on that general plan?\r\n\r\n", "@jitindua 's code is cool and all. But let's not forget that my PR still has a signed CLA, is properly based, linted, and ready to merge.\r\n\r\nRemember those 12 hours or so it was live? Good times!\r\n\r\n@martinwicke -- why don't we just add Nadam and Radam to golden and go back to those good dope days?\r\n\r\nI was all set to merge down the repeated C++ for another PR when my Nadam code got unpulled  \ud83d\udc94\r\nhttps://github.com/tensorflow/tensorflow/pull/8405#issuecomment-293402921\r\n\r\nStill game to clean up TF's C++ optimizers or even migrate some of their interfaces towards Python eventually. Can I have Nadam and Radam in core in exchange for properly refactoring all the Optimizer interfaces?", "Sorry @jitindua but if you don't sign the CLA there's nothing we can do. Also, if you don't have cycles to respond then I'm not sure we can really help.\r\n\r\n@louiehelm we have made the decision that the code should live in contrib first, as we usually do. Contrib is our canary. Code that lives there has time mature with a lot of freedom.\r\nAs long as the API doesn't change, we can refactor code and make it cleaner.\r\nAs I understand, this PR has the advantage that there is less replicated code. If you think you can resubmit your PR, in contrib, with more shared code, then I think that might be a good avenue and we can drop this PR.", "It looks like @jitindua has not responded since the original PR, and it's been 3 weeks.  So I'm going to close this PR and hope that it gets opened by someone (anyone!) who can sign the CLA, preferably under 'contrib', and with the suggestions / improvements posted here!\r\n"]}, {"number": 9174, "title": "Update word2vec_basic.py", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Looks like the diff become null somehow. I'm closing this PR for now. Feel free to reopen it if you'd like to work on it further.", "I signed it!", "I signed it!"]}, {"number": 9173, "title": "Branch 152974181", "body": "", "comments": []}, {"number": 9172, "title": "possible doc inconsistency `tf.contrib.framework.load_variable`", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *TensorFlow installed from (source or binary)?*:\r\n- *TensorFlow version*:\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\nIn my experience, this function `tf.contrib.framework.load_variable` returns a `numpy.ndarray` instead of a `Tensor` as the [doc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/framework/checkpoint_utils.py#L74) suggests. Furthermore according to the [unit test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/framework/checkpoint_utils_test.py#L96), it is expected to return the value of a tensor instead of a `Tensor` object itself. I think the behavior of returning tensor values is desired (to load select variables from arbitrary checkpoints without a session), so this should be a minor doc issue.\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["Right. We sometimes refer to Tensors as numpy arrays, rather than the symbolic operations (in python). @jhseu might have some opinion here.", "If you find a good wording, place submit a PR.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 9171, "title": "tf.set_random_seed does not reset random op state", "body": "TF Version: 1.1.0rc1 (installed from nightly: `Apr 10, 2017 1:03 AM`)\r\n(run on CPU, Python 2)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsess = tf.InteractiveSession()\r\nsess.run(tf.global_variables_initializer())\r\n\r\ntf.set_random_seed(1)\r\na = tf.truncated_normal_initializer(seed=None)([1])\r\nprint(a.eval())\r\n\r\ntf.set_random_seed(1)\r\nb = tf.truncated_normal_initializer(seed=None)([1])\r\nprint(b.eval())\r\n```\r\n\r\nOutput:\r\n```\r\n[ 1.05293429]\r\n[-0.4487586]\r\n```\r\n\r\nExpected:\r\nThe same value, since...\r\n> If the graph-level seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the graph-level seed so that it gets a unique random sequence.\r\n\r\nThe values are identical in repeated runs of the whole script, but not after resetting the graph-level seed (as in the example above).\r\n\r\n(possibly related to https://github.com/tensorflow/tensorflow/issues/9003)", "comments": ["The [release notes](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md) say determinism is only promised for `tf.set_random_seed(0)`. Does using `0` fix things for you? If not, then please tell me more and I'll re-open.", "Thanks, unfortunately it doesn't seem to work with seed=0 either, tested on the latest nightly from `Build #457 (Apr 14, 2017 1:03:00 AM)` (nightly whl binary downloaded from the repository, for Mac, CPU-only version, Python 2.)\r\n\r\nI get the following output for the above code when using seed 0 in both cases:\r\n```\r\n[ 1.53692079]\r\n[ 1.01875198]\r\n```\r\n\r\nIs there a reason why it should only be deterministic for that specific seed?\r\nEdit: It appears commit e9786df5e89f0345b2eb32d688c7be31c5259ba0 only fixed a specific bug for `tf.set_random_seed(0)` so it seems unrelated since I've tried various seeds and always get different results for a and b.", "@jart I found the relevant part of the code and I'm suspecting this to be intended behavior. I'll open a new ticket concerning the random seed logic.", "Or we can reopen this one, as promised. Let me find someone to rope into this conversation while you do some extra investigation.", "@girving Is the code example in this bug relating to `tf.set_random_seed(0)` nondeterminism WAI? See: https://github.com/tensorflow/tensorflow/commit/e9786df5e89f0345b2eb32d688c7be31c5259ba0", "Alright!\r\n\r\nSimpler, more low-level code example:\r\n```\r\nfor i in range(2):\r\n    tf.set_random_seed(1)\r\n    print(tf.random_uniform([1], seed=None).eval())\r\n```\r\nOutput:\r\n```\r\n[ 0.77878559]\r\n[ 0.54316521]\r\n```\r\nJudging from the current implementation, this seems to be intended behavior. In `random_seed.py`, `op_seed` is set to `ops.get_default_graph()._last_id` if the passed parameter for `op_seed` is `None`. This value depends on the execution and can change (as it does in the example above). `graph_seed` is constant as long as it is not set again via `set_graph_seed`.\r\n\r\nThe wording in the documentation could be understood either way:\r\n\r\n>   3. If the graph-level seed is not set, but the operation seed is set:\r\n>     A default graph-level seed and the specified operation seed are used to\r\n>     determine the random sequence.\r\n\r\nI think the following behavior would be more natural:\r\n- `graph_seed` is changed randomly upon each creation of a random variable (current implementation: `graph_seed` is fixed!)\r\n- if `op_seed` is `None`, use only `graph_seed` for the creation of the random values\r\n- if `op_seed` is given, use only `op_seed` (and ignore `graph_seed`)\r\n\r\nMotivation:\r\nThere doesn't seem to be an elegant way to have, say, 2 neural network models\r\ninitialized to the same random values in the same execution graph. Current\r\nworkarounds include the usage of `update_ops` to copy values from one network to\r\nthe other, or to set the `op_seeds` for all operations which leads to\r\nhard-to-maintain code (because the `op_seeds` would need to be deterministic but\r\ndifferent for each layer to ensure that we don't get the same values for all\r\nvariables).\r\n", "Correct, a seed of `None` without a graph level seed is intentional nondeterministic behavior.  Have you considered setting the seed to a value other than `None`?  If you set two random ops to the same seed, they will produce the same stream of random numbers.", "@girving \r\n> Correct, a seed of None without a graph level seed is intentional nondeterministic behavior.\r\n\r\nBut the graph level seed is set in the example. Fixing the op seeds is possible, but then the user needs to produce pseudo-random op_seeds for all parts of the code to avoid identical random values in the same section (see also the motivation part of my reply above).\r\n\r\nIn other words, the graph seed is currently only useful for identical results over repetitions of the whole code, while the proposed solution would be more flexible and additionally make the creation of identical values much easier. ", "Ah, your issue is poorly named; fixed.  The thing you want is still impossible, since the state you're trying to set is not part of the graph: it is part of the session.  Resetting the state on the session would be a nice feature (@langmore wants it too).\r\n\r\nThe other option is to use random ops with custom seed control, which I am about to add as `tf.contrib.stateless.stateless_random_uniform`, etc.", "To set the state on the session and reuse the existing random ops, we would need some plumbing to query the OpKernels for their internal state.", "Oh, I see: yes, the thing you want to do is possible after all; we'd just have to make the default per-op seeds count from the last time `tf.set_random_seed` is called.", "That would be great!", "Well, I didn't volunteer to do it. :)\r\n\r\n@michaelisard, @aselle: Do you have thoughts about this?  Modifying `tf.set_random_seed` to reset the per-op seed counter is trivial to do, but it would probably be quite surprising to users when it didn't work to reset states after graph construction is complete.", "What about a dedicated function `reset_op_seeds` which can only be called during graph construction? Or is it possible to reset those seeds already (manually)?", "It appears @girving's stateless random ops are available in TF 1.2rc0:\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/stateless\r\n\r\nThis is a useful addition, however it does not simplify the use case of initializing 2 neural network models with identical weights since the weights are generally built via tf.layers/tf.contrib modules which call stateful random-ops. I might be missing an obvious easy solution for this, but I guess stackoverflow is the place for those questions.\r\n\r\nAny updates @michaelisard @aselle?", "@asimshankar assigning to you for triage. It seems as if the current way we set random seeds has missing use cases: do you want to look at future redesigns?", "I think adding a `reset_op_seed` parameter to `tf.set_random_seed` could be acceptable. Something like: `tf.set_random_seed(seed, reset_op_seed=False)` (I'll ask around if anyone has objections for it defaulting to `True`).\r\n\r\n@ekelsen has been looking at determinism in general and might have other ideas.\r\n\r\n@girving : RE [your comment](https://github.com/tensorflow/tensorflow/issues/9171#issuecomment-294188068) - I don't think it would be any more or less surprising for the op-seed reset to not take effect after graph construction is complete than it is for `tf.set_random_seed` to have no effect after graph construction is complete today. Or am I missing something?\r\n", "any progress on this? looking for a simple approach to implement dropout as a bayesian representation, https://arxiv.org/pdf/1506.02142.pdf\r\n", "I ran into the same issue. However, for my use case I was able to use the stateless randomness:\r\n\r\n```python\r\nfor i in range(2):\r\n    seed=(1,1)\r\n    print(tf.contrib.stateless.stateless_random_uniform([1], seed).eval())\r\n```", "@frthjf These new additions are useful, but as mentioned above, they are not being used by the common layer building blocks (yet?), so one would need to pass these manually to all layers (including special initialization schemes).", "@seaotterman have you found a way to initialize the same weights for Neural Network multiple times over different session?", "@TYS11 I haven't had the time to look into this any further, unfortunately. I agree with @asimshankar that a reset_op_seed parameter could be a decent solution that doesn't break backwards compatibility. Until then, there's likely no better way than to write a custom function that creates 2 neural networks and then copies over the initialization weights from one to the other.", "We are addressing this problem by a comprehensive revamp in RFC tensorflow/community#38. Please comment on that.", "> We are addressing this problem by a comprehensive revamp in RFC [tensorflow/community#38](https://github.com/tensorflow/community/pull/38). Please comment on that.\r\n\r\nJust to be clear, does this mean nothing will happen on this issue until v2.0 is released? It really hinders debugging.", "> > We are addressing this problem by a comprehensive revamp in RFC [tensorflow/community#38](https://github.com/tensorflow/community/pull/38). Please comment on that.\r\n> \r\n> Just to be clear, does this mean nothing will happen on this issue until v2.0 is released? It really hinders debugging.\r\n\r\nI'm afraid so.", "What are my options prior to 2.0? Do I need to create a new graph every time I need to reproduce a result with some fixed seed?", "> What are my options prior to 2.0? Do I need to create a new graph every time I need to reproduce a result with some fixed seed?\r\n\r\nAlways providing the 'seed' argument to ops that accept it, along with manually setting the global seed via `tf.set_random_seed`, can mitigate the problem to some degree.", "Is there a way to reset all the internal state in the ops to its original value without recreating graph?", "> Is there a way to reset all the internal state in the ops to its original value without recreating graph?\r\n\r\nNo, the op's state is stored in a C++ member variable with no access from outside. Exposing it is among the main motivations of the RFC.", "I'm closing this since the new RNGs (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/stateful_random_ops.py) are ready.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=9171\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=9171\">No</a>\n", "try to use `tf.reset_default_graph()`\r\n\r\n```python\r\nimport tensorflow as tf  \r\nfor i in range(2):  \r\n    with tf.Session() as sess:  \r\n        tf.set_random_seed(2)  \r\n        var = tf.Variable(tf.random_normal([1, 1], 0.0, 0.01))  \r\n        init = tf.global_variables_initializer()  \r\n        sess.run(init)  \r\n        print(var.eval())  \r\n    tf.reset_default_graph() \r\n```\r\n\r\nOutput:\r\n```\r\n[[-0.0142362]]\r\n[[-0.0142362]]\r\n```"]}, {"number": 9170, "title": "[Windows] `import tensorflow` error messages are uninformative.", "body": "When a user runs `import tensorflow`, we attempt to dynamically load the DLL `_pywrap_tensorflow.pyd`. If this fails, the user gets an uninformative (perhaps downright misleading) message:\r\n\r\n```\r\nTraceback (most recent call last): \r\nFile \"C:\\...\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(file)])\r\nFile \"C:\\...\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n```\r\n\r\nThis error can happen for several reasons:\r\n\r\n* (Rare) The package has not been installed correctly, and the file `_pywrap_tensorflow.pyd` is not present in the expected location.\r\n* The package has been installed for an incompatible version of Python (e.g. installed on Python 3.6, but built for 3.5, as in #9167). I suspect this is because it fails to load `python35.dll`.\r\n* The library has been installed correctly, but one or more of its native dependencies is missing. Common examples include:\r\n  * `MSVCP140.dll` (the Microsoft Visual C++ redistributable library). The compiled C++ code depends on this library being present, but it is not installed as standard, unless you use Anaconda. (See [this Stack Overflow question](http://stackoverflow.com/a/42011114/3574081), for example.)\r\n  * CUDA libraries. The user's `%PATH%` may not include the directory that contains the CUDA DLLs.\r\n  * `cudnn64_5.dll`. This is typically installed in a different directory from the CUDA libraries, and must be added to the user's `%PATH%` explicitly.\r\n\r\nIt would be helpful if we could provide more information about the cause of an `ImportError`, and in particular we would like to show the name of the missing DLL to aid the user in diagnosing the problem. It's less clear to me how we achieve this, since the error is reported by `LoadLibrary()` of our compiled code before we have the chance to execute anything.\r\n\r\nA couple of thoughts spring to mind:\r\n\r\n1. Can we use [`/DELAYLOAD`](https://msdn.microsoft.com/en-us/library/151kt790.aspx) during the build process so that (at least) we have the chance to probe the CUDA-related libraries before using them? Are there any performance consequences to doing this? \r\n2. Alternatively, can we implement a sanity check in Python before we import `_pywrap_tensorflow.pyd` to ensure that the relevant files will be found? \r\n\r\n/cc @guschmue @vit-stepanovs in case they have any smart ideas!", "comments": ["we could add a couple of checks that run during pip install and check cuda dlls, python dll ... that would let us print a good error message for the most common problems.", "(As a temporary workaround...) I've created a script for diagnosing common TensorFlow on Windows installation issues. Please download the script from https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c and run `python tensorflow_self_check.py`. It will print suggestions for how to fix your installation.", "hi @mrry,\r\nhow to solve it, if the error is \r\n\r\n> The package has not been installed correctly, and the file _pywrap_tensorflow.pyd is not present in the expected location."]}, {"number": 9169, "title": "Loading LSTM \"SavedModel\" from golang results in crash", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: Yes\r\n- *TensorFlow installed from (source or binary)?*: binary\r\n- *TensorFlow version*: 1.0.1\r\n- *Bazel version (if compiling from source)*: -\r\n- *CUDA/cuDNN version*: - \r\n- *GPU Model and Memory*: -\r\n- *Exact command to reproduce*: -\r\n\r\n### Describe the problem clearly\r\n\r\nI have a 'saved model' of a NN that look like this (keras - model.summary()):\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 15, 300)           0         \r\n_________________________________________________________________\r\nmasking_1 (Masking)          (None, 15, 300)           0         \r\n_________________________________________________________________\r\nlstm_1 (LSTM)                (None, 300)               721200    \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 300)               0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 145)               43645     \r\n_________________________________________________________________\r\nactivation_1 (Activation)    (None, 145)               0         \r\n=================================================================\r\nTotal params: 764,845.0\r\nTrainable params: 764,845.0\r\nNon-trainable params: 0.0\r\n_________________________________________________________________\r\n```\r\nWhen I try to load it using the golang bindings, it results in a crash:\r\n```\r\n$ go run example.go \r\nI tensorflow/cc/saved_model/loader.cc:194] Loading SavedModel from: ./load\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/cc/saved_model/loader.cc:114] Restoring SavedModel bundle.\r\nI tensorflow/cc/saved_model/loader.cc:148] Running LegacyInitOp on SavedModel bundle.\r\nI tensorflow/cc/saved_model/loader.cc:238] Loading SavedModel: success. Took 150044 microseconds.\r\nfatal error: unexpected signal during runtime execution\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x8c1a6cc]\r\n\r\nruntime stack:\r\nruntime.throw(0x40cadf9, 0x2a)\r\n\t/usr/local/go/src/runtime/panic.go:596 +0x95\r\nruntime.sigpanic()\r\n\t/usr/local/go/src/runtime/signal_unix.go:274 +0x2db\r\n\r\ngoroutine 1 [syscall, locked to thread]:\r\nruntime.cgocall(0x4094920, 0xc420053dd0, 0x40cabb6)\r\n\t/usr/local/go/src/runtime/cgocall.go:131 +0xe2 fp=0xc420053d90 sp=0xc420053d50\r\ngithub.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_LoadSessionFromSavedModel(0x4516690, 0x0, 0x45167c0, 0xc42000e030, 0x1, 0x4515e50, 0x0, 0x45135f0, 0x0)\r\n\tgithub.com/tensorflow/tensorflow/tensorflow/go/_obj/_cgo_gotypes.go:438 +0x51 fp=0xc420053dd0 sp=0xc420053d90\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.LoadSavedModel.func1(0x4516690, 0x0, 0x45167c0, 0xc42000e030, 0x1, 0x4515e50, 0x0, 0x45135f0, 0x414d0f0)\r\n\t/Users/berset/git/gopath/src/github.com/tensorflow/tensorflow/tensorflow/go/saved_model.go:56 +0x156 fp=0xc420053e28 sp=0xc420053dd0\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.LoadSavedModel(0x40c5552, 0x6, 0xc420053f68, 0x1, 0x1, 0x0, 0x0, 0x4124480, 0x40a49e0)\r\n\t/Users/berset/git/gopath/src/github.com/tensorflow/tensorflow/tensorflow/go/saved_model.go:56 +0x1b4 fp=0xc420053ed8 sp=0xc420053e28\r\nmain.main()\r\n\t/Users/berset/git/gopath/src/github.com/campanja/ssinet/example.go:9 +0x8e fp=0xc420053f88 sp=0xc420053ed8\r\nruntime.main()\r\n\t/usr/local/go/src/runtime/proc.go:185 +0x20a fp=0xc420053fe0 sp=0xc420053f88\r\nruntime.goexit()\r\n\t/usr/local/go/src/runtime/asm_amd64.s:2197 +0x1 fp=0xc420053fe8 sp=0xc420053fe0\r\n\r\ngoroutine 17 [syscall, locked to thread]:\r\nruntime.goexit()\r\n\t/usr/local/go/src/runtime/asm_amd64.s:2197 +0x1\r\nexit status 2\r\n$ \r\n```\r\n\r\n\r\n### Source Code / Logs\r\n\r\nThe go code looks like this:\r\n\r\n```\r\npackage main\r\n\r\nimport (\r\n    \"fmt\"\r\n    tf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n)\r\n\r\nfunc main() {\r\n    m, err := tf.LoadSavedModel(\"./load\", []string{\"serve\"}, nil)\r\n    if err != nil {\r\n        fmt.Println(err)\r\n    }\r\n    fmt.Println(m)\r\n    fmt.Println(\"load successful!\")\r\n}\r\n```\r\n\r\nI'm attaching a sample saved model and the code to generate the crash.\r\n\r\n[sample.zip](https://github.com/tensorflow/tensorflow/files/917065/sample.zip)\r\n\r\nI can use the same code/bundling process to save / load NNs with only Dense layers for example.\r\nI also did try to printf debug this and I think the problem arises around the \"TensorArrayV3\" or \"TensorArrayReadV3\" or some such operation.", "comments": ["Thanks for the very detailed report and the instructions to reproduce, this helps a lot.\r\n\r\nI believe https://github.com/tensorflow/tensorflow/commit/5fb1aeaa6df73fe3ed6e66c034cf4202e3aa90ed fixes the issue and was able to verify that your sample succeeds when using 1.1.0-rc1 instead of 1.0.1 (available at: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.1.0-rc1.tar.gz and similar links for gpu/OS X).\r\n\r\nClosing this out since it is fixed in the latest release candidate.\r\n\r\nLet us know if that is not the case or if you have other concerns.", "Yes, using that version does indeed fix the problem.\r\n\r\nThanks!"]}, {"number": 9168, "title": "When I use the Bazel to compile the image_retraining project when the error occurred, ", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *TensorFlow installed from (source or binary)?*:\r\n- *TensorFlow version*:\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\nWhen I use the Bazel to compile the image_retraining project when the error occurred, how to solve, spec.json, head, branch_ref these documents are not in that directory, did you made some change that now cannot compile directory:\r\nwei@wei-TA960:~/myprogramfile/TF-0.12/tensorflow-r0.12-1$ bazel build tensorflow/examples/image_retraining:retrain\r\nERROR: /home/wei/myprogramfile/TF-0.12/tensorflow-r0.12-1/tensorflow/core/BUILD:1117:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /home/wei/myprogramfile/TF-0.12/tensorflow-r0.12-1/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /home/wei/myprogramfile/TF-0.12/tensorflow-r0.12-1/tensorflow/core/BUILD:1117:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /home/wei/myprogramfile/TF-0.12/tensorflow-r0.12-1/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /home/wei/myprogramfile/TF-0.12/tensorflow-r0.12-1/tensorflow/core/BUILD:1117:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /home/wei/myprogramfile/TF-0.12/tensorflow-r0.12-1/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["Please provide all information asked for in the new issue template. Without that information, help is not easy to provide.\r\n\r\nThat said, in your case it seems that you need to run `./configure` as outlined in the guide to [Installing TensorFlow from Sources](https://www.tensorflow.org/install/install_sources).\r\n"]}, {"number": 9167, "title": "Windows 7: No module named '_pywrap_tensorflow'", "body": "My problem is with import TF module. Here is my configuration: Python 3.6.1, Windows 7 64-bit. I've MSVCP140.dll library in 'C:\\Windows\\System32\\' and 'C:\\Windows\\SysWOW64\\' folders. I also have instaled Update 3 for VS2015 C++.\r\n\r\nIn Windows PATH varitable I have such value related to python:\r\nC:\\Users\\Jacek\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\\r\nC:\\Users\\Jacek\\AppData\\Local\\Programs\\Python\\Python36\\\r\n\r\nSteps which I made:\r\n\r\nI've installed TensorFlow by command (in power shell). It works.\r\n\r\n`python -m pip install --upgrade tensorflow`\r\nBut when I run python environment and try import Tensor Flow\r\n\r\n`import tensorflow as tf`\r\nI get errors, this error raise another errors related with it, but at the beginning I want to resolve this first\r\n\r\n> Traceback (most recent call last): File \"C:\\Users\\Jacek\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(file)]) File \"C:\\Users\\Jacek\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 296, in find_module raise ImportError(_ERR_MSG.format(name), name=name) ImportError: No module named '_pywrap_tensorflow'", "comments": ["Python 3.6 is not supported on Windows, see #6999 \r\nOnly building from source you would be able to use it currently.\r\n", "Thanks @Carmezim for being helpful as always! I'm closing this as a duplicate of #6999... feel free to chime in to the discussion there. ", "@mrry Always glad to help :)", "You are great. Now it working.\r\n\r\n@Carmezim @mrry : I wonder if in such case there should should be message \"Your Python version is not supported, please downgrade for X.X.X\". Then you could avoid questions like mine.\r\n\r\nThank you,", "@Jacorr Glad it is working now :) \r\nThis situation of the vague error messages is being addressed in #9170 and hopefully soon as you suggested more descriptive logs will be in place to provide more clarity and avoid confusion.\r\nThank you for opening the issue and feel free to ask further questions.", "For users who find this issue when searching for the error message, I've created a script for diagnosing common TensorFlow on Windows installation issues. Please download the script from https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c and run `python tensorflow_self_check.py`. It will print suggestions for how to fix your installation. Let me know if you are still facing problems after running the script."]}, {"number": 9166, "title": "Example Cluster Spec for Distributed YoutTube-8m challenge training", "body": "Hi,\r\n\r\nCan you please post a  ClusterSpec for distributed training of the models defined in  the [YouTube-8m Challenge code](https://github.com/google/youtube-8m)? \r\nThe [code](https://github.com/google/youtube-8m/blob/master/train.py#L628) tries to load a cluster spec from TF_CONFIG environment variable. However, I'm not sure what the value for TF_CONFIG should be. I have access to 2 GPUs on one machine and just want to run the model with data-level parallelism.\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9165, "title": "fix windows gpu build - add cusolver to cmake", "body": "required by:\r\nhttps://github.com/tensorflow/tensorflow/commit/72c023d3967a3218cd3d830ce6e57f7c4d87a18c\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 9164, "title": "tf.slim should not use relu as a default activation function", "body": "It's hard to understand the reason why relu is chosen as a default activation function. It looks like the choice of default activation function is against common sense. If someone never read the actual code or forget about it, it's so easy to make a mistake.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1387\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L822", "comments": ["Some things of note:\r\n\r\n- [Documentation has been updated in the next release](https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/layers/fully_connected) so this choice will be clearer and one doesn't have to read through the code\r\n- This is in `contrib`, as layers make their way to the fully supported public API ([`tf.layers`](https://www.tensorflow.org/api_docs/python/tf/layers)), many of these choices are being revisited and documentation updated.\r\n\r\nFYI @sguada @nathansilberman @martinwicke in case they have anything to add.\r\nThanks!"]}, {"number": 9163, "title": "fix missing Gather.png in tf.gather document", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Looks like there is some IPv6 issue on the libpng archive. Trying again.\r\n\r\nJenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.\r\n\r\nIt has issues fetching the core repo. I'm assuming this is transient."]}, {"number": 9162, "title": "Stop gradient for some entry of a tensor", "body": "As far as I know, the tf.stop_gradient function can only stop the gradient of a whole tensor. I recently wanted to implement a model that requires the stop of gradient for some entry of a tensor (not the whole tensor). and I have come up with an workaround,\r\n```\r\ndef entry_stop_gradients(target, mask):\r\n    mask_h = tf.logical_not(mask)\r\n    \r\n    mask = tf.cast(mask, dtype=target.dtype)\r\n    mask_h = tf.cast(mask_h, dtype=target.dtype)\r\n    \r\n    return tf.stop_gradient(mask_h * target) + mask * target\r\n```\r\n \r\nhope somebody could implement a low level version of this feature.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "It is in fact a feature request. Tf already have a stop_gradient function that stop the gradient for a tensor as a whole, why not implement a generalized version?", "I'm on board with this. It would be nice if there were an overloaded version of `tf.stop_grads` like the example above.", "Is this going to remain closed? It's definitely a feature request, not something that could be aided by StackOverflow.", "I wonder why this remains closed. I think this is a necessary feature to be added.", "Does [`tf.custom_gradient`](https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/python/ops/custom_gradient.py#L33) (in 1.7, `tf.contrib.eager.custom_gradient` in 1.6) help?\r\n\r\nYou can use that to define the custom gradient that appropriately masks out portions that are of interest.\r\n\r\nIf not, the workaround suggested in the original posting seems reasonable. \r\nI'm perhaps misunderstanding what you'd want as a feature. ", "Hi Jerrik,  if the mask is sparse(# of 0s is much greater than # of nonzeros), will your approach accelerate the backprop speed, because lots of gradients are no calculate?", "@7GrandPa \r\nNo, it won't. since it actually introduces another gradient masked identity function. more computation is required actually.", "I think asimshankar's solution: custom_gradient works", "@asimshankar Can you give an example of how `tf.custom_gradient` would achieve it? As far as I know, you can't use the autodiff to differentiate wrt to a `tf.Variable` slice. So in the backwards pass, all you can do is to register the custom gradient and multiply the incoming gradients by the mask? Is that what you are suggesting?", "For tensorflow 2.X, wouln't just computing the gradient then multiplying it by the mask before applying it be enough? I don't know if this would have much overhead, but e.g.:\r\n```python\r\nloss = lambda: (z_obj - model(X))**2  # squared error, I am not interested in the \"mean\" for my use case.\r\nwith tf.GradientTape() as tape:\r\n        z = loss()\r\ngrads = tape.gradient(z, X)\r\noptim.apply_gradients(zip([grads*mask], [X]))\r\n```\r\nI checked it on a small optimization problem and works as intended (optimizes in the available directions, leaving the rest unchanged)."]}, {"number": 9161, "title": "Hi, I am unable to access the documentation", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *TensorFlow installed from (source or binary)?*:\r\n- *TensorFlow version*:\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["https://www.tensorflow.org/get_started/get_started", "Thanks! Sorry, I mean the documentation of TensorFlow Debugger (tfdbg) https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/debug/examples", "I can not see too, is there any document of tensorflow which is whriten by java or scala?", "@ayat-khairy : Thanks for pointing out that broken link. In the mean time you can find documentation for `tfdbg` in https://www.tensorflow.org/programmers_guide/debugger and https://www.tensorflow.org/programmers_guide/tfdbg-tflearn \r\n\r\n@caisq : Do you want to update the link in the README?\r\n\r\n@geektcp : I'm not sure I understand your question. If you have a separate bug report about documentation, please file an issue with more details. Or if this comment answers your question, great.\r\n\r\nThanks!", "A fix has been submitted internally and will be pushed to GtiHub soon."]}, {"number": 9160, "title": "bazel build macOS failing \"ld: unknown option: -zmuldefs\"", "body": "- *TensorFlow installed from (source or binary)?*: source\r\n- *TensorFlow version*: latest one checked out from git\r\n- *Bazel version (if compiling from source)*:  0.4.5-homebrew\r\n- *CUDA/cuDNN version*: 8.0 / 5.1 (6.0 fails earlier in the build)\r\n- *GPU Model and Memory*: GeForce 750M 2GB\r\n- *Exact command to reproduce*:bazel build --config=cuda --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem clearly\r\nSo I followed this general guide https://gist.github.com/ageitgey/819a51afa4613649bd18, after which I solved some compilation issues with http://stackoverflow.com/questions/39865212/dyld-library-not-loaded-rpath-libcudart-8-0-dylib-while-building-tensorflow?rq=1.\r\n\r\nDoing this allowed me to compile more, but then the following errors occur.\r\n### Source Code / Logs\r\n```\r\nERROR: /Users/chris/git/tensorflow/tensorflow/python/BUILD:2533:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o ... (remaining 472 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nclang: warning: argument unused during compilation: '-pthread'\r\nld: unknown option: -zmuldefs\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1592.901s, Critical Path: 1309.60s\r\n```\r\n", "comments": ["Some searching has found this page http://stackoverflow.com/questions/17443976/how-to-allow-z-multidefs-with-g47, but i'm unsure how to proceed to make this build for me. currently grepping for zmuldefs to hopefully replace it.\r\n\r\nDeleting the option entirely leads to this error further down about not finding omp `d: library not found for -lgomp`\r\n\r\nAfter some playing around I replaced the only instance of \"-lgomp\" with \"-L/some/path/libiomp5.dylib\", turns out I had the library from matlab. Not sure how advised that is, but going to try carry on with the build as this eliminated the -lgomp error.\r\n\r\nwell it seemed to build the wheel and install that through pip fine, and run some training on gpu.", "The guide your are following is not maintained by the TensorFlow project. To build TensorFlow from source, please refer to [Installing TensorFlow from Sources](https://www.tensorflow.org/install/install_sources) on the TensorFlow website.\r\n\r\nSince this appears to be an issue with an external guide and not TensorFlow documentation, I'm going to close this issue. If you find bugs in the official guide, please feel free to reopen a new issue with all the details. Thanks!", "@ckalas You are using clang which does not support -zmuldefs. (clang is symlinked as gcc on MacOS, and bazel is not smart enough to try and identify the compiler)\r\nYou have two options: \r\nInstall gcc and rerun ./configure, selecting the correct compiler\r\nor\r\nrerun ./configure and select clang as the compiler. \r\n", "@ckalas I meet the same problems like you. Maybe there exist some bugs for macOS in the latest version checked out from git. It works when I use the branch 'remotes/origin/r1.1'.\r\n\r\n**Source Code / Logs**\r\n\r\n```\r\nINFO: From Compiling tensorflow/tools/graph_transforms/quantize_nodes.cc:\r\ntensorflow/tools/graph_transforms/quantize_nodes.cc:172:6: warning: unused function 'AreAttrsEqual' [-Wunused-function]\r\nbool AreAttrsEqual(const NodeDef* current_node, const NodeDef* other_node) {\r\n     ^\r\n1 warning generated.\r\nINFO: From Linking tensorflow/python/_pywrap_tensorflow_internal.so:\r\nclang: warning: argument unused during compilation: '-pthread'\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 1847.249s, Critical Path: 1683.09s\r\n\u279c  tensorflow git:(4874d43) \u2717 bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nFri Apr 14 22:29:14 CST 2017 : === Using tmpdir: /var/folders/lc/8t7355q97sq34vlvzp9vbby00000gn/T/tmp.XXXXXXXXXX.vgCZk00t\r\n~/Projects/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/Projects/tensorflow\r\n~/Projects/tensorflow\r\n/var/folders/lc/8t7355q97sq34vlvzp9vbby00000gn/T/tmp.XXXXXXXXXX.vgCZk00t ~/Projects/tensorflow\r\nFri Apr 14 22:29:19 CST 2017 : === Building wheel\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\n~/Projects/tensorflow\r\nFri Apr 14 22:29:35 CST 2017 : === Output wheel file is in: /tmp/tensorflow_pkg\r\n\u279c  tensorflow git:(4874d43) \u2717 sudo pip install /tmp/tensorflow_pkg/tensorflow-1.1.0rc2-cp27-none-macosx_10_11_x86_64.whl\r\n```\r\n\r\n", "see thread https://github.com/tensorflow/tensorflow/issues/9072 for mac bazel build walkthrough", "@ckalas You Mentioned that:\r\nDeleting the option entirely leads to this error further down about not finding omp d: library not found for -lgomp\r\n\r\nHow did u get out of this error??????/"]}, {"number": 9159, "title": "[C++ API] safe string to int64 conversion", "body": "In short:\r\n```c++ \r\nstrings::safe_strto64(\"+1\", &value)\r\n```\r\nThrows an error because of the `+`.\r\n\r\n\r\nI wrote an Op to read data in the LIBSVM format, which is the following:\r\n`<label> <index1>:<value1> <index2>:<value2> ....`\r\n\r\nThe sample data provided at the [LIBSVM site](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/) uses `+1` as label instead of only a `1`.\r\nThe safe conversion throws an error because of the `+`:\r\n```c++\r\nint64 value;\r\nstring label(\"+1\");\r\nOP_REQUIRES(ctx, strings::safe_strto64(label, &value),\r\n            errors::InvalidArgument(\"Label \", label, \r\n                                    \" is not a valid int64.\"));  \r\n```\r\nTraceback when calling the op in python:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib64/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Label +1 is not a valid int64.\r\n\t [[Node: DecodeLibsvm = DecodeLibsvm[OUT_TYPE=DT_INT64, num_features=123, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch)]]\r\n```\r\n\r\nIs this the wanted behaviour and should I handle this before calling the conversion?", "comments": ["Yes, you should handle the leading `+` before calling `strings::safe_strto64`."]}, {"number": 9158, "title": "[documentation] Updated comment for tf.multnomial", "body": "Self-explanatory.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Could you sign the CLA? We need this.", "Done.", "CLAs look good, thanks!\n\n<!-- ok -->", "I'm not sure this is an improvement.\r\n\r\n@aselle what say you?", "Up to you, though I think it's worth noting that \"unnormalized log probabilities\" makes it seem like the function accepts log probabilities (which it doesn't). It accepts log-odds!!!\r\n\r\nThe documentation of the underlying function that implements `tf.multinomial` is actually more precise; this is how I figured out that I needed to pass log-odds in the first place.", "Apologies, you are correct. Thanks!", "Jenkins, test this please."]}, {"number": 9157, "title": "missing dash in install from source at cxxopt added", "body": "option `--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"` needs two dashes.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "CLA signed.\n\nOn Wed, Apr 12, 2017 at 8:22 AM, googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address. Check your\n>    existing CLA data <https://cla.developers.google.com/clas> and verify\n>    that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If you signed the CLA as a corporation, please let us know the\n>    company's name.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9157#issuecomment-293484867>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFY1A3sfuneJ4Z2Mpcl0NSZl_XWAHBVrks5rvG2rgaJpZM4M6_57>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 9156, "title": "hi everyone! im having this issue when trying to import tensorflow, i've tried all the techniques i've found on stackoverflow and here at GH but they still don't work", "body": "Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AM\r\nD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", l\r\nine 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Le module sp\u00e9cifi\u00e9 est introuvable.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, i\r\nn <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", l\r\nine 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", l\r\nine 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <modu\r\nle>\r\n    from tensorflow.python import *\r\n  File \"C:\\python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, i\r\nn <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", l\r\nine 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Le module sp\u00e9cifi\u00e9 est introuvable.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, i\r\nn <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", l\r\nine 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", l\r\nine 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_st\r\narted/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>", "comments": ["This might be a better question for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow)."]}, {"number": 9155, "title": "[Android Example] Demo does not perform well in low light", "body": "Android [demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) does not perform well in low light. I think a button for flashlight is needed.\r\nFor example:-\r\n![demo](https://cloud.githubusercontent.com/assets/6515036/24878880/7b447f26-1e52-11e7-9b7f-df01f2e9f9d6.jpg)\r\n\r\n\r\n\r\n", "comments": ["We could add a flashlight button. But the user could always drag down the menu area and enable the flashlight without the help of the app. \r\n\r\nAs for the model itself, please consider that it's meant for demo purposes. If demos were perfect, they wouldn't be demos. Maybe that's a good thing. We want to give our users opportunities to explore TensorFlow and improve upon the examples we've provided."]}, {"number": 9154, "title": "consecutive calls of saver.restore(sess,path) slows down", "body": "### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: no\r\n- *TensorFlow installed from (source or binary)?*: binary\r\n- *TensorFlow version*: 0.12.1\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*: cpu\r\n\r\n### Describe the problem clearly\r\nHi I was building and evaluating ensemble models about 45 very simple neural networks.\r\n\r\nwhen evaluating, I noticed that consecutive calls to ```saver.restore(sess, path)``` slows down\r\n\r\nat first it spent about 0.08 seconds but after 45 calls to  ```saver.restore()```, time spent on restore increased to 0.5 seconds. It kept increasing to 1 second and beyond. \r\n\r\nIs anyone having the same problem? In the source code, I called ```test_model()``` consecutively. Other part didn't slow down but only the part with saver.restore() did\r\n\r\n### Source Code / Logs\r\n```python\r\ndef test_model(model, batch_gen, batch_num, batch_size, num_class, model_id):\r\n\r\n    saver = tf.train.Saver()\r\n    start_time = time.time()\r\n\r\n    print('## Testing model : {}'.format(model_id))\r\n    with tf.Session() as sess:\r\n        ot = time.time()\r\n        sess.run(tf.global_variables_initializer())\r\n        # Load latest model to evaluate\r\n        checkpoint_dir = CHECKPOINTS_DIR+str(model_id)+'/'\r\n\r\n        ckpt = tf.train.get_checkpoint_state(os.path.dirname(checkpoint_dir))\r\n\r\n        \r\n        if ckpt and ckpt.model_checkpoint_path:\r\n            saver.restore(sess, ckpt.model_checkpoint_path)\r\n        else:\r\n            print('# No trained weight found')\r\n            return\r\n        nt = time.time()\r\n        print('1 : {}'.format(nt-ot))\r\n\r\n        vote_list = []\r\n        for i in xrange(batch_num):\r\n            X_batch, Y_batch = batch_gen.next()\r\n            _, loss_batch, softmax_batch = sess.run([model.optimizer, model.loss, model.softmax], feed_dict={model.input: X_batch, model.output:Y_batch}) \r\n            \r\n            vote_batch = dense_to_one_hot(np.argmax(softmax_batch,1), num_class)\r\n            vote_list.append(vote_batch)\r\n\r\n        total_vote_list = np.concatenate(vote_list, 0)\r\n   \r\n        print('# time elapsed :{:.1f} seconds'.format(time.time() - start_time ))\r\n    return total_vote_list\r\n```\r\n\r\n------------------EDIT------------------\r\n\r\nProblem was not ```saver.restore()``` but consecutive calling of\r\n```python\r\nsess.run(tf.global_variables_initializer())\r\n````\r\nsession is suppose to free all the memories right?\r\nso I think there has to be no performance slow down but there is when making multiple sessions\r\n\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\n\r\na = tf.Variable([1,2,3,4,5])\r\n\r\ndef test():\r\n    for i in range(1000):\r\n        with tf.Session() as sess:\r\n            ot = time.time()\r\n            sess.run(tf.global_variables_initializer())\r\n            nt = time.time()\r\n            print('test : {:.3f}'.format(nt-ot))\r\n```\r\nrunning time of `tf.global_varaiables_initializer()` op slowly increases", "comments": ["Thanks for reducing the problem to a small reproducible code snippet in your edit, that helps a lot.\r\n\r\nThis is probably not documented as clearly as it should, but `tf.global_variables_initializer()` adds an operation to the graph (with control dependencies), so as time goes by you see the graph getting larger and larger. Instead, you should call out `tf.global_variables_initializer()` once, using something like this:\r\n\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\n\r\na = tf.Variable([1,2,3,4,5])\r\ninit_op = tf.global_variables_initializer()\r\n\r\ndef test():\r\n    for i in range(1000):\r\n        with tf.Session() as sess:\r\n            ot = time.time()\r\n            sess.run(init_op)\r\n            nt = time.time()\r\n            print('test : {:.3f}'.format(nt-ot))\r\n```\r\n\r\nHope that helps.\r\nClosing this out since it doesn't appear to be a bug. I'll look into improving the documentation."]}, {"number": 9153, "title": "Docker Build Error", "body": "\r\nI am building tf dockerimages for both cuda 8.0 and 7.5 from source. But both cames error at bazel build procedure with message:\r\n\r\n> The command '/bin/sh -c tensorflow/tools/ci_build/builds/configured GPU     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl &&     rm -rf /tmp/pip &&     rm -rf /root/.cache' returned a non-zero code: 2\r\n\r\n\r\nsome times there would be an additional message like:\r\n\r\n> ERROR: Evaluation of query \"deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))\" failed: errors were encountered while computing transitive closure\r\n\r\nI cloned r1.1 branch and mainly followed the scripts in original dockerfile(devel-gpu version), with only python and cuda env modified. Also I found that original dockerfile would also meet same issue. \r\nI guess it might be caused by some download error when getting dependencies from bazel-mirror on googleapis. \r\nIs that possible to manually download those packages or try some alternative way like disable some function? Any hint?", "comments": ["Was there more information that was logged?", "@jart I just searched building log for recent 3 times and find some timeout message when downloading and loading some packages for bazel. These packages are always from `webcomponents` and `polymerelements` on github. I guess that could be the reason if these packages are essential for building. ", "Could you copy and paste those errors into this issue?", "@jart \r\n\r\n> ...\r\n> ____Loading package: @jsoncpp_git//\r\n> ____Timeout connecting to https://github.com/polymerelements/iron-icon/archive/v1.0.11.tar.gz\r\n> ...\r\n\r\n> ...\r\n> ____Loading package: @numericjs_numeric_min_js//file\r\n> ____Timeout connecting to https://github.com/webcomponents/webcomponentsjs/archive/v0.7.22.tar.gz\r\n> ...\r\n\r\n> ...\r\n> ____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 4,707,593 bytes\r\n> ____Timeout connecting to https://github.com/polymerelements/iron-input/archive/1.0.10.tar.gz\r\n> ...\r\n\r\nBut I can download these packages via browser.\r\nAnd after all download procedure the builder fails again with same message: \r\n\r\n> The command '/bin/sh -c tensorflow/tools/ci_build/builds/configured GPU     bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl &&     rm -rf /tmp/pip &&     rm -rf /root/.cache' returned a non-zero code: 2\r\n\r\nIf that is the real cause, could I just manually add them?", "Would you mind attaching a file with the full output of the commands you run?", "Those downloader warnings are actually ephemeral progress messages. The downloader appears to have been failing over or retrying correctly. So I'm curious if anything was outputted before you got that non-zero error code message.", "@jart \r\nI print the full log. Seems more error massage appears this time.\r\n\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/915473/log.txt)\r\n\r\n", "I see the error:\r\n\r\nERROR: [...] java.io.IOException: Error downloading [...] Tried to reconnect at offset 1,808,859 but server didn't support it and referenced by '@io_bazel_rules_closure//java/io/bazel/rules/closure/webfiles/server:listing'.\r\n\r\nThat error should be ephemeral and was fixed in https://github.com/bazelbuild/bazel/commit/0e866a8b732e31df1a4ddfd13c4870d4721de70b. Try retrying `./configure`.\r\n\r\nFor further support, please try [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow)."]}]