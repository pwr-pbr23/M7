[{"number": 28352, "title": "Make the --copt=-mavx and --copt=-msse4.2 flags take effect for libtensorflow builds", "body": "PiperOrigin-RevId: 246392874", "comments": []}, {"number": 28351, "title": "Enable eager execution fails after testing if executing eagerly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): pip install -q \"tensorflow==1.13.1\"\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 2.7.15rc1\r\n\r\n**Describe the current behavior**\r\n\r\nCalling `tf.enable_eager_execution()` after `tf.executing_eagerly()` fails with:\r\n\r\n    ValueError: tf.enable_eager_execution must be called at program startup. \r\n\r\nNote that this happens *at* program startup (after runtime restart on Colab). Calling `tf.enable_eager_execution()` by itself does *not* result in the exception. Not to confuse with issue #21893\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should be possible to check whether tensorflow is executing eagerly before programs enable eager execution. This is relevant for forward compatibility with *tensorflow 2.0.0-alpha0* which does not implement enable_eager_execution() but instead reports:\r\n\r\n    AttributeError: 'module' object has no attribute 'enable_eager_execution'\r\n\r\n**Code to reproduce the issue**\r\n\r\n    import tensorflow as tf\r\n    if not tf.executing_eagerly():\r\n        tf.enable_eager_execution()\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nValueErrorTraceback (most recent call last)\r\n<ipython-input-1-a1e6f3444523> in <module>()\r\n      7 \r\n      8 if not tf.executing_eagerly():\r\n----> 9   tf.enable_eager_execution()\r\n\r\n1 frames\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in enable_eager_execution(config, device_policy, execution_mode)\r\n   5459         device_policy=device_policy,\r\n   5460         execution_mode=execution_mode,\r\n-> 5461         server_def=None)\r\n   5462 \r\n   5463 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in enable_eager_execution_internal(config, device_policy, execution_mode, server_def)\r\n   5538   else:\r\n   5539     raise ValueError(\r\n-> 5540         \"tf.enable_eager_execution must be called at program startup.\")\r\n   5541 \r\n   5542   # Monkey patch to get rid of an unnecessary conditional since the context is\r\n\r\nValueError: tf.enable_eager_execution must be called at program startup.", "comments": ["From what I tested, I believe this behavior has been fixed already in the latest tf-nightly and should be coming in 1.14. If not please let us know and re-open. ", "@TerjeNorderhaug: Though the exception has been fixed in the nightlies, were you looking for the `tf.executing_eagerly` symbol to be included in 2.0?", "The `tf.executing_eagerly` symbol is already [in the 2.0 api](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/executing_eagerly) and works as I expected with the _2.0.0-alpha0_ as well as the _tf-nightly-gpu-2.0-preview_."]}, {"number": 28350, "title": "Fix the return dtype of IsZero() function", "body": "`IsZero` function use `Equal` Op internally, so the return dtype should be `bool` rather than `T`.", "comments": []}, {"number": 28349, "title": "[TF-TRT] Adds support for INT64 constants (and bounds checks)", "body": "Modifies the Constant converter in TF-TRT so that it will cast int64 values down to int32 if they are in range. ", "comments": ["@pranavm-nvidia Could you please check reviewer comments and keep us posted. Thanks!", "Sorry for the delay - I was OOTO. I have addressed all the remaining comments now. "]}, {"number": 28348, "title": "Skip a SavedModel captured dataset loading test in v1 on gpu", "body": "It's broken not a very important use-case. Loading in v2 on GPU works.\r\n\r\nPiperOrigin-RevId: 245172655", "comments": []}, {"number": 28347, "title": "Tensorflow lite example with alternate image source", "body": "I would like an example similar to the TensorFlow lite Android example that doesn't use the camera but an alternate image source, like from network stream. Is there something available that has such an example?\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and provide support. Thanks!\r\n"]}, {"number": 28346, "title": "TrtGraphConverterV2 does not preserve output names in the signature_def", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):  master from April 22nd\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): 0.24\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version: 10.0 / 7.5.0\r\n- GPU model and memory: GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\nIf you use TrtGraphConverterV2 to convert a function in a saved_model to use TRT it does not preserve the output names in the signature_def of the saved model.\r\n\r\nIf the saved function (decorated with tf.function) returned a dict {'output_a': a, 'output_b': b} the names 'output_a' and 'output_b' are in the saved_model. After conversion with TrtGraphConverterV2 they are changed to the default names 'output_0' and 'output_1'.\r\n\r\n**Describe the expected behavior**\r\nThe names of the outputs should not change. This breaks all code that loads the model and relies on the correct names.\r\n\r\n**Code to reproduce the issue**\r\nTake any saved_model that contains a function returning a dict.\r\nThen run this:\r\n```\r\nconversion_params = trt_convert.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt_convert.TrtPrecisionMode.FP16, max_batch_size=1, max_workspace_size_bytes=8000000000)\r\n\r\ntrt_converter = trt_convert.TrtGraphConverterV2(input_saved_model_dir='your_saved_model', input_saved_model_signature_key='your_key', conversion_params=conversion_params)\r\ntrt_converter.convert()\r\ntrt_converter.save('your_saved_model')\r\n```\r\nUse saved_model_cli to inspect the saved_model.", "comments": ["@olesalscheider In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "You can use this code to reproduce the issue:\r\n\r\nhttps://gist.githubusercontent.com/olesalscheider/366f33115016ac9d5f2976ec17124496/raw/f5b68bf571f325742c1bc24658f0de04b3d3b33c/wrong_outputs.py\r\n\r\nThe output names should be the same before and after conversion but they are not.", "@olesalscheider Able to reproduce the issue.\r\n\r\nOur saved model has the following structured outputs:\r\n{'output_a': TensorSpec(shape=(), dtype=tf.float32, name='output_a'), 'output_b': TensorSpec(shape=(), dtype=tf.float32, name='output_b')}\r\nRunning TF-TRT conversion...\r\nOur converted model has the following structured outputs:\r\n{'output_0': TensorSpec(shape=(), dtype=tf.float32, name='output_0'), 'output_1': TensorSpec(shape=(), dtype=tf.float32, name='output_1')}", "Thanks for reporting this. I can reproduce the problem, will make a fix soon.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28346\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28346\">No</a>\n", "Is it actually fixed??????", "@bixia1 do you know if this is still a problem?"]}, {"number": 28345, "title": "Segmentation fault in  tflite::ops::builtin::BuiltinOpResolver when using libtensorflowlite.so via CMake", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04.6 LTS, Trusty Tahr\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.4.0\r\n- Python version: 3.4.3\r\n- Bazel version (if compiling from source): 0.23.0\r\n- GCC/Compiler version (if compiling from source):c++ (Ubuntu 4.8.5-4ubuntu8~14.04.2) 4.8.5\r\n\r\nMy goal is to build and run the examples in tensorflow/lite/ by using CMake on my Ubuntu 14.04 workstation\r\n\r\nI am able to build and run the stock examples without any issues using bazel like this : \r\nbazel build --config opt --cxxopt=-std=c++11 //tensorflow/lite/examples/minimal:minimal\r\n\r\nHowever, when I build **libtensorflowlite.so** like this : \r\nbazel build --config opt --cxxopt=-std=c++11 tensorflow/lite:libtensorflowlite.so\r\nand then link the shared object in my CMakeLists.txt to run the same program, I am seeing a segmentation fault when the program exits. I have narrowed the minimal example down to this : \r\n```\r\n#include <iostream>\r\n#include \"tensorflow/lite/kernels/register.h\"\r\n\r\n// This is an example that is minimal to read a model\r\n// from disk and perform inference. There is no data being loaded\r\n// that is up to you to add as a user.\r\n//\r\n// NOTE: Do not add any dependencies to this that cannot be built with\r\n// the minimal makefile. This example must remain trivial to build with\r\n// the minimal build tool.\r\n//\r\n// Usage: minimal <tflite model>\r\n\r\nusing namespace tflite;\r\n\r\nint main(int argc, char* argv[]) {\r\n\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  std::cout << \"DONE\" << std::endl;\r\n  return 0;\r\n}\r\n```\r\nGDB trace : \r\n```\r\nProgram received signal SIGSEGV, Segmentation fault.\r\nstd::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<std::pair<std::string, int> const, _TfLiteRegistration>, true> > >::_M_deallocate_nodes (this=0x7fffffffdb70, __n=0x5)\r\n    at /usr/include/c++/6/bits/hashtable_policy.h:1984\r\n1984\t\t  _M_deallocate_node(__tmp);\r\n(gdb) bt\r\n#0  std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<std::pair<std::string, int> const, _TfLiteRegistration>, true> > >::_M_deallocate_nodes (this=0x7fffffffdb70, __n=0x5)\r\n    at /usr/include/c++/6/bits/hashtable_policy.h:1984\r\n#1  std::_Hashtable<std::pair<std::string, int>, std::pair<std::pair<std::string, int> const, _TfLiteRegistration>, std::allocator<std::pair<std::pair<std::string, int> const, _TfLiteRegistration> >, std::__detail::_Select1st, std::equal_to<std::pair<std::string, int> >, tflite::op_resolver_hasher::OperatorKeyHasher<std::pair<std::string, int> >, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::clear (this=0x7fffffffdb70) at /usr/include/c++/6/bits/hashtable.h:1892\r\n#2  std::_Hashtable<std::pair<std::string, int>, std::pair<std::pair<std::string, int> const, _TfLiteRegistration>, std::allocator<std::pair<std::pair<std::string, int> const, _TfLiteRegistration> >, std::__detail::_Select1st, std::equal_to<std::pair<std::string, int> >, tflite::op_resolver_hasher::OperatorKeyHasher<std::pair<std::string, int> >, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::~_Hashtable (this=0x7fffffffdb70, __in_chrg=<optimized out>)\r\n    at /usr/include/c++/6/bits/hashtable.h:1218\r\n#3  0x00000000004012d0 in std::unordered_map<std::pair<std::string, int>, _TfLiteRegistration, tflite::op_resolver_hasher::OperatorKeyHasher<std::pair<std::string, int> >, std::equal_to<std::pair<std::string, int> >, std::allocator<std::pair<std::pair<std::string, int> const, _TfLiteRegistration> > >::~unordered_map (this=0x7fffffffdb70, \r\n    __in_chrg=<optimized out>) at /usr/include/c++/6/bits/unordered_map.h:98\r\n#4  tflite::MutableOpResolver::~MutableOpResolver (this=0x7fffffffdb30, __in_chrg=<optimized out>)\r\n    at ${PATH}/tensorflow/tensorflow/lite/mutable_op_resolver.h:55\r\n#5  tflite::ops::builtin::BuiltinOpResolver::~BuiltinOpResolver (this=0x7fffffffdb30, __in_chrg=<optimized out>)\r\n    at ${PATH}//tensorflow/tensorflow/lite/kernels/register.h:26\r\n#6  main (argc=<optimized out>, argv=<optimized out>)\r\n    at ${PATH}//applications/tflite/sample_minimal/impl/main_tflite_sample_minimal.cpp:134\r\n```\r\nThis line : tflite::ops::builtin::BuiltinOpResolver resolver; is the cause of the segmentation fault when the function exits. Any ideas on what might be causing the segmentation fault", "comments": ["@aphalak Unfortunately we don't provide support for the CMake build anymore. ", "I have indeed build **libtensorflowlite.so** using bazel as follows : \r\nbazel build --config opt --cxxopt=-std=c++11 tensorflow/lite:libtensorflowlite.so\r\n\r\nThe issue is when I am linking the shared object in my .cpp program", "@aphalak Thanks for the information. \r\n", "In general it is not really safe to take a bazel build made with a particular toolchain and compile with a different compiler in CMake. Make sure your cmake is using the same compiler and c++ standard library. You may consider using the makefile form of compiling tflite instead of extracting the binary from bazel.", "@gadagashwini Did you solve the issue? I got a similar issue. Thanks.", "@aphalak,\r\n\r\nCan you take a look at this above [comment](https://github.com/tensorflow/tensorflow/issues/28345#issuecomment-500028351) from @aselle and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28345\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28345\">No</a>\n"]}, {"number": 28344, "title": "[TF-TRT] Support constant params input for gather", "body": "This PR enable to support constant params input for gather conversion using TFTRT.\r\nThis change will be used by few networks (for example NCF), it will cause performance improvement.\r\nThis PR contains:\r\n-convert_node.py change (to update ConvertGather function)\r\n-convert_nodes_test.py change (to support constant params in our tests)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28344) for more info**.\n\n<!-- need_sender_cla -->", "Hey @aaroey, I believe I have addressed the remaining issues with this PR. As you discovered previously, the behavior of TRT's gather when both inputs are batched is different than what we expect in TF. TRT team has confirmed this as a bug. This weird gather behavior doesn't affect the cases when either input is not batched (i.e. it is a weight/constant).\r\n\r\nThe main goal of this PR is to support embedding lookups which are commonly of the form: `params = [vocab_size, embedding_size]`(constant), `indices = [N,]`(tensor), `axis=0`. Thus the output will be `[N, embedding_size]`.", "> Thanks @trevor-m. I still don't think this fixes the issue I mentioned. Let me add a commit and we can discuss from there.\r\n\r\nWhich issue are you referring to?", "@mdrozdowski1996 can you please sign CLA ", "The CLA error is coming from my commits, it says \"CLAs are signed, but unable to verify author consent\".", "I signed it!\r\n", "As chatted offline, let's make a new PR."]}, {"number": 28343, "title": "[ROCm] Adding Cuda Alias to Gpu functions", "body": "This PR is a follow-up to PR #24293. This PR depends on & includes code changes from PR #28116. Code updates for this PR are in the `tensorflow/core/util` path\r\n\r\nThrough utility functions defined in gpu_cuda_alias.h, this PR provides a Cuda* alias to existing Gpu* functions. The reason for this PR is to create backward compatibility to both the TensorFlow master branch as well as ROCm's develop-upstream branch. It allows TensorFlow master branch to slowly deprecate Cuda* aliased function, to the final goal of using GPU as the prefix to unify both Cuda and ROCm builds. All aliasing is done through perfect forwarding macro so that it is easy to maintain/take out.\r\n\r\nTests performed: Both Cuda/ROCm build successfully in both master and develop-upstream branch.\r\n\r\n@whchung @deven-amd ", "comments": ["I don't have the expertise to review this; removing myself.", "Will post back to address review feedback after PR[439](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/439) approved. Will also integrate macro only change from GPU_LAUNCH_KERNEL to GpuLaunchKernel", "@jerryyin multiple definition of `gpuGetErrorString(cudaError)'", "> @jerryyin multiple definition of `gpuGetErrorString(cudaError)'\r\n\r\nI was trying to forwarding the full set of `__host__` and `__device__` identifiers here according to Cuda's documentation [here](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html#group__CUDART__ERROR_1g4bc9e35a618dfd0877c29c8ee45148f1), but forgot that this is a header file with the function definition. When multiple compilation unit pulls it in, the compiler complains. I added `inline` identifier and local compilation works.", "The [on-going effort](https://github.com/tensorflow/tensorflow/commit/d8d67eff9d0cecc639ac926c68c5ff86b6535748) in commit d8d67eff9d0cecc639ac926c68c5ff86b6535748 to achieve the same goal with this PR seems to be duplicated, and is creating hard-to-resolve conflicts with this PR. \r\n\r\nCommit d8d67eff9d0cecc639ac926c68c5ff86b6535748 used a less desirable way to create aliases - manually copy and rename every function signature. Compared to the approach of creating dedicated macro to perfect forward function name, the commit looks hard to maintain and error prone.\r\n\r\nCould you help contact the internal contributor make sure our opinion on the same page with this PR? @chsigg @mrry @rthadur", "@jerryyin  for the time being, I recommend reverting d8d67ef in this PR, as perfect forwarding makes the code better maintainable.", "Hi Zhuoran. I started the rename from our side, sorry that I didn't look more carefully at your approach first. Generally, please do not mix several changes into the same PR. It makes it hard to understand and review.\r\nCould you please make a PR that introduces the macros only, and a follow-up to generate the various alias functions/macros/types, maybe even split into multiple PRs. At that point I can start preparing changes to use the new 'gpu' symbols.", "> Hi Zhuoran. I started the rename from our side, sorry that I didn't look more carefully at your approach first. Generally, please do not mix several changes into the same PR. It makes it hard to understand and review.\r\n> Could you please make a PR that introduces the macros only, and a follow-up to generate the various alias functions/macros/types, maybe even split into multiple PRs. At that point I can start preparing changes to use the new 'gpu' symbols.\r\n\r\n@chsigg No worries, good feedback on breaking the PR.\r\n\r\nPlease prioritize following reviews:\r\n- [x] One PR for create forwarding macros\r\n#28564 Creating Cuda forwarding alias macros\r\n\r\n- [x] One PR for create GpuLaunchKernel\r\n#28565 Creating GpuLaunchKernel \r\n\r\nThen the dependent ones:\r\n- [x] Corresponding changes in using the forwarding type marco\r\n#28567 Forward type names from gpu prefix to cuda prefix\r\n\r\n- [x] Corresponding changes in using the forwarding host function macro\r\n#28568 Forward host function names from gpu prefix to cuda prefix\r\n\r\n- [x] corresponding changes in using the forwarding device function macro\r\n#28571 Forward device function name from gpu prefix to cuda prefix \r\n\r\n- [x] One PR for Removing GPU_LAUNCH_KERNEL\r\n#28566 remove GPU_LAUNCH_KERNEL macro\r\n\r\nThe final goal of closing the large PR.", "Closing the review as all sub-PRs are merged"]}, {"number": 28342, "title": "Is it possible to add a layer to an Estimator after training it?", "body": "Hi all,\r\n\r\nAfter training an estimator, we'd like to modify the model and add a custom layer. \r\n\r\nIs there a way to modify the estimator model that way?\r\n\r\nThanks!", "comments": ["@jxieeducation It looks, This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there. Let us know. Thanks!\r\n"]}, {"number": 28341, "title": "Why `tf.reshape` function tries to evaluate the `shape` argument?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: 10\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\n### Describe the problem\r\nWhen feeding images to a deep network, normally I resize all images to a fixed size (512*512 for example). So\r\nthere might be a tensor `input:0` of shape `(32, 512, 512, 3)`. Afterwards, I resize the tensor to shape `(N*H,W,C)`, i.e., `(32*512, 512, 3)`. To do this, I should write something like\r\n```\r\nshape = tf.shape(inputs)\r\nreshaped = tf.reshape(inputs, (shape[0]*shape[1], shape[2], shape[3])) # Now the tensor has a known shape: (32*512, 512, 3)\r\n```\r\n\r\nNow, suppose the network is trained and I would like to make predictions. But note that during test time, the shape of data we want to feed into `input:0` might not be `(32, 512, 512, 3)`, since the batch size might vary(if we test images one by one, the batch size is 1) or the height/width might vary(if we do not want to resize images). So if I use `tf.import_graph_def()` to restore the graph and feed a image of a different size, the shape will be mismatched.\r\n\r\nThe main reason here might be invoking `tf.reshape` function, it will try to evaluate the `shape` argument, and since tensor `input:0` is of a fixed shape, the `shape` argument is easily evaluated, so the `reshaped` tensor is also of a fixed shape. If the function does not try to evaluate the `shape`, the target shape will be determined after the image is fed into the network, so everything will work fine.\r\n\r\nSo I would like to know why `tf.reshape` function tries to evaluate the `shape` argument. Is it a feature or not? If it is a feature, what should I do to fix the shape mismatch problem? Of course I can use the `input_map` argument in `tf.import_graph_def()` to change the shape of `input:0`, but it will not affect the shape of `reshape:0`.\r\n\r\n", "comments": ["Will it possible for you to give us some more info on the issue as for example a minimal reproducible code snippet that can depict the case. Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\n"]}, {"number": 28340, "title": "Initial implementation of general eigenvalue decomposition", "body": "", "comments": ["I'm going to punt this one to @rmlarsen, since linear algebra is more his domain!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "I've not got any response from you or anyone else, neither any review expect note about non-existing file, which was fixed. Are there any actions expected from me at this point?", "@rthadur Also I have no rights to reopen the PR ", "reopened , can you please resolve conflicts", "@rthadur done, rebased on master", "ping", "I\u2019m sorry you\u2019ve experienced a delay on this pull request. Unfortunately, this code is outside my domain of expertise. As I mentioned on May 2nd, @rmlarsen or someone else more familiar with Eigen would be a more appropriate reviewer.", "Can one of the admins verify this patch?", "ping @rmlarsen \r\nCan you please review or propose someone else who can review?", "This PR will be reviewed soon , thanks for your patience.", "@Randl my apologies for the long delay. I will review this.", "@Randl can you please check the build failures ?", "@rthadur fixed lint stuff, should work now", "Sorry, I messed up a bit with version control. The current version worked locally for Ubuntu 18.04.", "@rmlarsen can you rerun CI again please", "@rmlarsen Can you do it once again please? Sorry for bothering you, I'm new to the whole API thing. Hopefully, it should be the last one.", "@Randl no problem. It is rather tedious to add a new op, sorry.", "@rthadur @rmlarsen Is it ok now? Seems like windows fails are due to upstream. Is anything else required from my side?", "@Randl thank you. We'll take it from here."]}, {"number": 28339, "title": "Fix grammar in MirroredStrategy docstring", "body": "Fix grammar in MirroredStrategy docstring", "comments": []}, {"number": 28338, "title": "Improper metric computation in fit method (tf 2.0)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.13.6\r\n- TensorFlow installed from (source or binary): from pip I think, I don't remember and I don't know the difference\r\n- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0\r\n- Python version: 3.6.7\r\n\r\n**Describe the current behavior**\r\nWhen using the `fit` method on a custom model (the model in the guide [Writing layers and models with TensorFlow Keras](https://www.tensorflow.org/alpha/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example)), a simple `MeanSquaredError` metric seems to be improperly computed. When fitting, the metric is always greater than the loss, whereas the loss is computed as the mean squared error plus a custom error defined as a KL divergence in the model.\r\n\r\n**Describe the expected behavior**\r\nThe mean squared error metric should always be lesser than the loss, as it is when training the model with custom code instead of using the `fit` method.\r\n\r\n**Code to reproduce the issue**\r\n The following code is adapted from the guide [Writing layers and models with TensorFlow Keras](https://www.tensorflow.org/alpha/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example). I simply customized the formatting and added a metric to track the mean squared error in addition to the total loss.\r\n\r\nOn a side note, the code in [the guide](https://www.tensorflow.org/alpha/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example) seems to forget to use `reset_states` on the metric at the beginning of each epoch.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nfrom time import time\r\n\r\nclass Sampling(layers.Layer):\r\n    # Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\r\n\r\n    def call(self, inputs, training=None):\r\n        z_mean, z_log_var = inputs\r\n        batch = tf.shape(z_mean)[0]\r\n        dim = tf.shape(z_mean)[1]\r\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon # = z_mean + tf.sqrt(z_var) * epsilon\r\n\r\n\r\nclass Encoder(layers.Layer):\r\n    # Maps MNIST digits to a triplet (z_mean, z_log_var, z).\r\n\r\n    def __init__(self,\r\n                 latent_dim=32,\r\n                 intermediate_dim=64,\r\n                 name='encoder',\r\n                 **kwargs):\r\n        super(Encoder, self).__init__(name=name, **kwargs)\r\n        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\r\n        self.dense_mean = layers.Dense(latent_dim)\r\n        self.dense_log_var = layers.Dense(latent_dim)\r\n        self.sampling = Sampling()\r\n\r\n    def call(self, inputs, training=None):\r\n        x = self.dense_proj(inputs)\r\n        z_mean = self.dense_mean(x)\r\n        z_log_var = self.dense_log_var(x)\r\n        z = self.sampling((z_mean, z_log_var))\r\n        return z_mean, z_log_var, z\r\n\r\n\r\nclass Decoder(layers.Layer):\r\n    # Converts z, the encoded digit vector, back into a readable digit.\r\n\r\n    def __init__(self,\r\n                 original_dim,\r\n                 intermediate_dim=64,\r\n                 name='decoder',\r\n                 **kwargs):\r\n        super(Decoder, self).__init__(name=name, **kwargs)\r\n        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\r\n        self.dense_output = layers.Dense(original_dim, activation='sigmoid')\r\n\r\n    def call(self, inputs, training=None):\r\n        x = self.dense_proj(inputs)\r\n        return self.dense_output(x)\r\n\r\n\r\nclass VariationalAutoEncoder(tf.keras.Model):\r\n    # Combines the encoder and decoder into an end-to-end model for training.\r\n\r\n    def __init__(self,\r\n                 original_dim,\r\n                 intermediate_dim=64,\r\n                 latent_dim=32,\r\n                 name='autoencoder',\r\n                 **kwargs):\r\n        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\r\n        self.original_dim = original_dim\r\n        self.encoder = Encoder(latent_dim=latent_dim,\r\n                               intermediate_dim=intermediate_dim)\r\n        self.decoder = Decoder(original_dim=original_dim,\r\n                               intermediate_dim=intermediate_dim)\r\n\r\n    def call(self, inputs, training=None):\r\n        z_mean, z_log_var, z = self.encoder(inputs)\r\n        reconstructed = self.decoder(z)\r\n        # Add KL divergence regularization loss.\r\n        kl_loss = - 0.5 * tf.reduce_mean(\r\n            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\r\n        self.add_loss(kl_loss)\r\n        return reconstructed\r\n\r\n\r\n# Data preparation\r\noriginal_dim = 28 * 28\r\n(x_train, _), _ = tf.keras.datasets.mnist.load_data()\r\nx_train = x_train.reshape(60000, original_dim).astype('float32') / 255\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(x_train)\r\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\r\n\r\n# Model creation\r\nvae = VariationalAutoEncoder(original_dim, intermediate_dim=64, latent_dim=32)\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\nmse_loss_fn = tf.keras.losses.MeanSquaredError()\r\nloss_metric = tf.keras.metrics.Mean()\r\nmse_metric = tf.keras.metrics.Mean()\r\n\r\n@tf.function\r\ndef train_step(inputs):\r\n    with tf.GradientTape() as tape:\r\n          reconstructed = vae(inputs)\r\n          # Compute reconstruction loss\r\n          mse_loss = mse_loss_fn(inputs, reconstructed)\r\n          loss = mse_loss + tf.add_n(vae.losses)  # Add KLD regularization loss\r\n    \r\n    grads = tape.gradient(loss, vae.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, vae.trainable_variables))\r\n    # Update the metrics\r\n    loss_metric.update_state(loss)\r\n    mse_metric.update_state(mse_loss)\r\n\r\ndef train_over_epochs(train_dataset, epochs):\r\n    # Iterate over epochs.\r\n    for epoch in range(epochs):\r\n        begin_time = time()\r\n        print('Start of epoch {:d}'.format(epoch + 1))\r\n        # Reset the metrics\r\n        loss_metric.reset_states()\r\n        mse_metric.reset_states()\r\n    \r\n        # Iterate over the batches of the dataset.\r\n        for step, x_batch_train in enumerate(train_dataset):\r\n            train_step(x_batch_train)\r\n        \r\n        print('    step {:3d}: mean total loss = {:f}'.format(step + 1, loss_metric.result().numpy()))\r\n        print('              mean mse loss = {:f}'.format(mse_metric.result().numpy()))\r\n        print('    time: {:.2f}'.format(time() - begin_time))\r\n        \r\ntrain_over_epochs(train_dataset, epochs=4)\r\n```\r\nThe output of this code is something like this:\r\n```\r\nStart of epoch 1\r\n    step 938: mean total loss = 0.074619\r\n              mean mse loss = 0.074219\r\n    time: 4.24\r\nStart of epoch 2\r\n    step 938: mean total loss = 0.067608\r\n              mean mse loss = 0.067607\r\n    time: 2.51\r\nStart of epoch 3\r\n    step 938: mean total loss = 0.067527\r\n              mean mse loss = 0.067526\r\n    time: 2.64\r\nStart of epoch 4\r\n    step 938: mean total loss = 0.067471\r\n              mean mse loss = 0.067470\r\n    time: 2.50\r\n\r\n```\r\nThe mse loss is always smaller than the total loss which is expected.\r\n\r\nNow if we run the following code:\r\n```\r\ntrain_for_fit_dataset=train_dataset.map(lambda x: (x,x)) #create labels equal to the data for fit method\r\nvae = VariationalAutoEncoder(original_dim, intermediate_dim=64, latent_dim=32)\r\nvae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), \r\n           loss=tf.keras.losses.MeanSquaredError(),\r\n           metrics=[tf.keras.metrics.MeanSquaredError()])\r\nvae.fit(train_for_fit_dataset, epochs=4)\r\n```\r\nThe output looks like this:\r\n```\r\nEpoch 1/4\r\n938/938 [==============================] - 5s 5ms/step - loss: 0.0744 - mean_squared_error: 0.0923\r\nEpoch 2/4\r\n938/938 [==============================] - 4s 4ms/step - loss: 0.0676 - mean_squared_error: 0.0678\r\nEpoch 3/4\r\n938/938 [==============================] - 4s 4ms/step - loss: 0.0675 - mean_squared_error: 0.0677\r\nEpoch 4/4\r\n938/938 [==============================] - 4s 5ms/step - loss: 0.0675 - mean_squared_error: 0.0676\r\n```\r\nNote that the loss seems be computed as in the previous code, but the mean squared error is definitely not.\r\n\r\nEither there is a subtlety in `fit` I am not aware of, either there is a bug.\r\n\r\n\r\n", "comments": ["I have updated the code I provided because in my custom training loop I was updating `mse_metric` after applying gradients whereas I was updating `loss_metric` with the loss value computed before applying them. Now I consistently update both at the same time.\r\n\r\nI wonder if such a timing consideration is likely to explain the results observed with the `fit` method. The larger gap at epoch 1 is still mysterious.\r\n\r\nEdit: and I upgraded my tensorflow version to v1.12.0-9492-g2c319fb415 2.0.0-alpha0", "Same behaviour with the new beta version v1.12.1-3259-gf59745a381 2.0.0-beta0\r\n", "The issue seems solved in tf2.0.0.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28338\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28338\">No</a>\n"]}, {"number": 28337, "title": "Distributed TensorFlow does not work when using more than 26 workers", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5\r\n- TensorFlow version (use command below): TF Alpha 2.0 and TF. 1.13 CPU only\r\n- Python version: 2.7\r\n- Intel Xeon Gold 6150 2.7GHz 18 cores (16 cores enabled) 24.75MB L3 Cache (Max Turbo Freq. 3.7GHz, Min 3.4GHz), 180GB RAM (Six Channel), 4.8TB of Disk Space\r\n\r\nI have a performance issue using distributed TF for CPU machine with MultiWorkerStrategy using 58 workers. However, it seems that it does not work as the following errors:\r\n\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0502 21:43:31.821069 47528963346944 cross_device_ops.py:1106] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0502 21:43:31.821974 47863205896704 cross_device_ops.py:1106] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nterminate called after throwing an instance of 'std::system_error'\r\n  what():  Resource temporarily unavailable\r\nterminate called after throwing an instance of 'std::system_error'\r\n  what():  Resource temporarily unavailable\r\nterminate called after throwing an instance of 'std::system_error'\r\n  what():  Resource temporarily unavailable\r\n\r\nTypically, how many workers can be used for this scheme?  and how to solve this problem? Is it because I do not have sufficient memory?\r\n\r\n", "comments": ["@ymsaputra In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "The aforementioned error happens before the training process begins. Is it because I have 16 cores only?\r\n\r\nHere is the snippet of the code:\r\n\r\n```\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \r\n\tos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\r\n\r\n\tcluster = {'chief': ['localhost:2000'],\r\n             #'ps': ['localhost:2222'],\r\n             'worker': ['localhost:2001', 'localhost:2002', 'localhost:2003', 'localhost:2004', 'localhost:2005', 'localhost:2006', 'localhost:2007', 'localhost:2008', 'localhost:2009', 'localhost:2010', 'localhost:2011', 'localhost:2012', 'localhost:2013', 'localhost:2014', 'localhost:2015', 'localhost:2016', 'localhost:2017', 'localhost:2018', 'localhost:2019', 'localhost:2020', 'localhost:2021', 'localhost:2022', 'localhost:2023', 'localhost:2024', 'localhost:2025', 'localhost:2026', 'localhost:2027', 'localhost:2028', 'localhost:2029', 'localhost:2030', 'localhost:2031', 'localhost:2032', 'localhost:2033', 'localhost:2034', 'localhost:2035', 'localhost:2036', 'localhost:2037', 'localhost:2038', 'localhost:2039', 'localhost:2040', 'localhost:2041', 'localhost:2042', 'localhost:2043', 'localhost:2044', 'localhost:2045', 'localhost:2046', 'localhost:2047', 'localhost:2048', 'localhost:2049', 'localhost:2050', 'localhost:2051', 'localhost:2052', 'localhost:2053', 'localhost:2054', 'localhost:2055', 'localhost:2056', 'localhost:2057']\r\n          }\r\n\t# Input Flags\r\n\r\n\tos.environ['TF_CONFIG'] = json.dumps(\r\n\t\t{'cluster': cluster,\r\n\t\t 'task': {'type': FLAGS.job_name, 'index': FLAGS.task_index}})\r\n\tmirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(tf.distribute.experimental.CollectiveCommunication.RING)\r\n\tconfig = tf.estimator.RunConfig(train_distribute=mirrored_strategy, eval_distribute=mirrored_strategy, save_checkpoints_steps=200)\r\n```\r\n\r\nThen I run multiple pythons as follows:\r\n\r\n```\r\nimport subprocess\r\n\r\nsubprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name \"chief\" --task_index 0', shell = True)\r\nsubprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name \"worker\" --task_index 0', shell = True)\r\nsubprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name \"worker\" --task_index 1', shell = True)\r\nsubprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name \"worker\" --task_index 2', shell = True)\r\nsubprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name \"worker\" --task_index 3', shell = True)\r\nsubprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name \"worker\" --task_index 4', shell = True)\r\n...\r\nsubprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name \"worker\" --task_index 56', shell = True)\r\n```\r\n", "if i understand correctly, you're creating all the 58 workers on a single machine? if yes, what is the purpose behind that? ", "@guptapriya Yes, I use a single computer with Intel Xeon Gold 6150 2.7GHz 18 cores (16 cores enabled) 24.75MB L3 Cache (Max Turbo Freq. 3.7GHz, Min 3.4GHz), 180GB RAM (Six Channel), 4.8TB of Disk Space. I use 58 workers to represents 58 base stations for simulations. Should I need to use 58 machines to implement this?", "Typically we just run 1 worker on one machine, unless running multiple workers helps in some way in resource utilization. running such a large number of workers on a single machine is definitely not something that has been tested much and I can imagine it running into running out of memory or other resources. Is it possible for you to use more machines instead? \r\n\r\n", "Does it mean that I need to use more PCs/laptops, e.g., 6 laptops, to run at least 9 workers on each machine (to run 58 workers in total)?", "If you want to run 58 workers, you'll need 58 machines. It is not efficient to run more than one worker on a single machine other than for testing purposes. Each worker would start a TF engine and each would try to use all of your resources.", "Closing this issue for now since it doesn't look like a bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28337\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28337\">No</a>\n", "In a machine with 48 cores, why cannot 48 threads of worker be run. If that is not possible, is there a way to distribute load onto the 48 cores within a single worker ?", "@TransientObject Please create a new issue with description, platform details, TF version and a simple standalone code to reproduce the issue. Thanks!", "> If you want to run 58 workers, you'll need 58 machines. It is not efficient to run more than one worker on a single machine other than for testing purposes. Each worker would start a TF engine and each would try to use all of your resources.\r\n\r\n@ymsaputra Efficiency apart, Ubuntu 18.04 seems to reach a limit around ~50 sessions. Raising **DefaultTasksMax=infinity** inside /etc/systemd/user.conf & system.conf worked for me. Note that I don't know what kind of side effect that could cause.\r\n\r\nAlso, there is those two variables that seems to reduce the number of threads by a lot:\r\n\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.intra_op_parallelism_threads = 1\r\nconfig.inter_op_parallelism_threads = 1\r\ntf.Session(config=config)\r\n```\r\n\r\nAgain, I don't know what kind of side effect that could cause.\r\n"]}, {"number": 28336, "title": "Update nativeinterpreterwrapper_jni.cc", "body": "fixing [UnsatisfiedLinkError: No implementation found for void org.tensorflow.lite.NativeInterpreterWrapper.allowBufferHandleOutput(long, boolean)](https://github.com/tensorflow/tensorflow/issues/28335).", "comments": ["I've got a separate fix that I'll be merging today, thanks for flagging the issue."]}, {"number": 28335, "title": "UnsatisfiedLinkError: No implementation found for void org.tensorflow.lite.NativeInterpreterWrapper.allowBufferHandleOutput(long, boolean)", "body": "**System information**\r\n- OS Platform and Distribution: **Linux 4.4.0-17134-Microsoft #706-Microsoft x86_64 GNU/Linux (Ubuntu 1804)**\r\n- Mobile device: **Android, arm64**\r\n- TensorFlow installed from (source or binary): **source *and* binary**\r\n- TensorFlow version: **1.13.1**\r\n- Bazel version: **0.24.1**\r\n- GCC/Compiler version: **NDK r18, clang**\r\n\r\n**Describe the problem**\r\nUsing the 'official' `implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'`, I tried\r\n\r\n    opt.setAllowBufferHandleOutput(true)\r\n\r\nI get \r\n\r\n> E/aoyi.run.tflit: No implementation found for void org.tensorflow.lite.NativeInterpreterWrapper.allowBufferHandleOutput(long, boolean) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_allowBufferHandleOutput and Java_org_tensorflow_lite_NativeInterpreterWrapper_allowBufferHandleOutput__JZ)\r\n\r\nSame error reproduced when I build the **libtensorflowlite_jni.so** locally with **bazel**. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nTo verify that this was a build issue, I run\r\n\r\n    > nm -D jniLibs/arm64-v8a/libtensorflowlite_jni.so | grep allow\r\n    0000000000010940 T Java_org_tensorflow_lite_NativeInterpreterWrapper_allowFp16PrecisionForFp32\r\n\r\n**Analysis**\r\n\r\nThe root cause is that the build relies on [nativeinterpreterwrapper_jni.h](https://github.com/tensorflow/tensorflow/blob//61c6c84964b4aec80aeace187aab8cb2c3e55a72/tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.h) to declare the JNI functions as `extern \"C\"`. [`Java_org_tensorflow_lite_NativeInterpreterWrapper_allowBufferHandleOutput`](https://github.com/tensorflow/tensorflow/blame/master/tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc#L252) was introduced in [18 Dec 2018 commit](https://github.com/tensorflow/tensorflow/commit/3856a33dc65e7aff1df7ee4e940479ef37e9934b) which did not update the `.h` file.\r\n\r\n[**Proposed fix**](https://github.com/tensorflow/tensorflow/pull/28336)\r\n\r\nThe easy fix (tested here) is to tag the exported JNI functions as `extern \"C\"` in the `.cc` file. Relying on the header file is not necessary and (as we witness) error-prone.", "comments": []}, {"number": 28334, "title": "Distributed Training TF2.0 Input Pipeline Bug", "body": "System information\r\n\r\nOS Platform and Distribution: Docker-nvidia(2.0.0a0-gpu-py3)\r\nTensorFlow version: '2.0.0-alpha0'\r\nPython version: 3.5.2\r\nCUDA/cuDNN version: 10.0\r\n\r\n\r\nHello I'm trying the [Code](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/distribute/keras.ipynb) here.\r\nAnt it come out **error info**.\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0502 07:45:49.047524 140435350275840 distributed_training_utils.py:182] Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.\r\nW0502 07:45:49.050312 140435350275840 distributed_training_utils.py:182] Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.\r\nW0502 07:45:49.052523 140435350275840 distributed_training_utils.py:182] Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.\r\nEpoch 1/10\r\n      1/Unknown - 5s 5s/step - loss: 2.2896 - accuracy: 0.1406\r\nW0502 07:45:57.873010 140435350275840 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.178479). Check your callbacks.\r\n    468/Unknown - 9s 19ms/step - loss: 0.2455 - accuracy: 0.9304\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-16-d96263311d23> in <module>\r\n----> 1 model.fit(train_dataset, epochs=10, callbacks=callbacks)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    744             steps_per_epoch=steps_per_epoch,\r\n    745             validation_steps=validation_steps,\r\n--> 746             validation_freq=validation_freq)\r\n    747 \r\n    748     batch_size = self._validate_or_infer_batch_size(\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_distributed.py in fit_distributed(model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\r\n    129         validation_steps=validation_steps,\r\n    130         validation_freq=validation_freq,\r\n--> 131         steps_name='steps_per_epoch')\r\n    132 \r\n    133 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    260         try:\r\n    261           # `ins` can be callable in DistributionStrategy + eager case.\r\n--> 262           actual_inputs = ins() if callable(ins) else ins\r\n    263           batch_outs = f(actual_inputs)\r\n    264         except errors.OutOfRangeError:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py in get_distributed_inputs()\r\n    470     def get_distributed_inputs():\r\n    471       return distributed_training_utils._prepare_feed_values(\r\n--> 472           model, inputs, targets, sample_weights, mode)\r\n    473 \r\n    474     # In the eager case, we want to call the input method per step, so return\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in _prepare_feed_values(model, inputs, targets, sample_weights, mode)\r\n    565   \"\"\"\r\n    566   strategy = model._distribution_strategy\r\n--> 567   inputs, targets, sample_weights = _get_input_from_iterator(inputs, model)\r\n    568   inputs = flatten_perdevice_values(strategy, inputs)\r\n    569   targets = flatten_perdevice_values(strategy, targets)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in _get_input_from_iterator(iterator, model)\r\n    547   # Validate that all the elements in x and y are of the same type and shape.\r\n    548   validate_distributed_dataset_inputs(\r\n--> 549       model._distribution_strategy, x, y, sample_weights)\r\n    550   return x, y, sample_weights\r\n    551 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in validate_distributed_dataset_inputs(distribution_strategy, x, y, sample_weights)\r\n    247   # If each element of x and y are not tensors, we cannot standardize and\r\n    248   # validate the input and targets.\r\n--> 249   x_values_list = validate_per_device_inputs(distribution_strategy, x)\r\n    250 \r\n    251   if y is not None:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in validate_per_device_inputs(distribution_strategy, x)\r\n    294 \r\n    295     # Validate that the shape and dtype of all the elements in x are the same.\r\n--> 296     validate_all_tensor_shapes(x, x_values)\r\n    297     validate_all_tensor_types(x, x_values)\r\n    298 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in validate_all_tensor_shapes(x, x_values)\r\n    315     if x_shape != x_values[i].get_shape().as_list():\r\n    316       raise ValueError('Input tensor shapes do not match for distributed tensor'\r\n--> 317                        ' inputs {}'.format(x))\r\n    318 \r\n    319 \r\n\r\nValueError: Input tensor shapes do not match for distributed tensor inputs PerReplica:{\r\n  0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor(\r\n[[[[0.]\r\n   [0.]\r\n   [0.]\r\n   ...\r\n   [0.]\r\n   [0.]\r\n   [0.]]\r\n   ...\r\n   [0.]\r\n   [0.]\r\n   [0.]]]], shape=(64, 28, 28, 1), dtype=float32),\r\n  1 /job:localhost/replica:0/task:0/device:GPU:1: tf.Tensor(\r\n[[[[0.]\r\n   [0.]\r\n   [0.]\r\n   ...\r\n   [0.]\r\n   [0.]\r\n   [0.]]\r\n   ...\r\n   [0.]\r\n   [0.]\r\n   [0.]]]], shape=(32, 28, 28, 1), dtype=float32)\r\n}\r\n```\r\n\r\nit because gpu0 and gpu1 can't get the same shape, \r\nso I modify Input pipeline, add `.repeat(2)` in the pipeline\r\n```\r\ntrain_dataset = mnist_train.repeat(2).map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n```\r\nand it work!\r\n\r\nbut I think the Input pipeline should repeat by itself.\r\n", "comments": ["@KUASWoodyLIN I ran the google colab link provided using gpu and it went through without any errors and no modifications are done to the code. can you please check and let us know.\r\n\r\nW0503 10:49:24.512632 139824653981568 distributed_training_utils.py:182] Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.\r\nEpoch 1/10\r\n      1/Unknown - 6s 6s/step - loss: 2.2981 - accuracy: 0.0781W0503 10:49:32.975298 139824653981568 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.298758). Check your callbacks.\r\n    938/Unknown - 21s 23ms/step - loss: 0.2007 - accuracy: 0.9425\r\nLearning rate for epoch 1 is 0.0010000000474974513\r\n938/938 [==============================] - 21s 23ms/step - loss: 0.2007 - accuracy: 0.9425\r\nEpoch 2/10\r\n930/938 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9806\r\nLearning rate for epoch 2 is 0.0010000000474974513\r\n938/938 [==============================] - 14s 15ms/step - loss: 0.0658 - accuracy: 0.9807\r\nEpoch 3/10\r\n929/938 [============================>.] - ETA: 0s - loss: 0.0452 - accuracy: 0.9862\r\nLearning rate for epoch 3 is 0.0010000000474974513\r\n938/938 [==============================] - 14s 15ms/step - loss: 0.0455 - accuracy: 0.9862\r\nEpoch 4/10\r\n928/938 [============================>.] - ETA: 0s - loss: 0.0240 - accuracy: 0.9934\r\nLearning rate for epoch 4 is 9.999999747378752e-05\r\n938/938 [==============================] - 14s 14ms/step - loss: 0.0240 - accuracy: 0.9934\r\nEpoch 5/10\r\n9............................", "@muddham \r\nI also have test on google colab, and the google colab is only run on single GPU.\r\nbut I think this problem is only come out when with multi-gpu.\r\n", "got the same problem just now.\r\n  468/Unknown - 5s 12ms/step - loss: 0.0570 - accuracy: 0.9830---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-36-e5f10b39d923> in <module>\r\n----> 1 model.fit(train_dataset, epochs=10000, callbacks=callbacks)\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    744             steps_per_epoch=steps_per_epoch,\r\n    745             validation_steps=validation_steps,\r\n--> 746             validation_freq=validation_freq)\r\n    747\r\n    748     batch_size = self._validate_or_infer_batch_size(\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_distributed.py in fit_distributed(model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\r\n    129         validation_steps=validation_steps,\r\n    130         validation_freq=validation_freq,\r\n--> 131         steps_name='steps_per_epoch')\r\n    132\r\n    133\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    260         try:\r\n    261           # `ins` can be callable in DistributionStrategy + eager case.\r\n--> 262           actual_inputs = ins() if callable(ins) else ins\r\n    263           batch_outs = f(actual_inputs)\r\n    264         except errors.OutOfRangeError:\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in get_distributed_inputs()\r\n    470     def get_distributed_inputs():\r\n    471       return distributed_training_utils._prepare_feed_values(\r\n--> 472           model, inputs, targets, sample_weights, mode)\r\n    473\r\n    474     # In the eager case, we want to call the input method per step, so return\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/distributed_training_utils.py in _prepare_feed_values(model, inputs, targets, sample_weights, mode)\r\n    565   \"\"\"\r\n    566   strategy = model._distribution_strategy\r\n--> 567   inputs, targets, sample_weights = _get_input_from_iterator(inputs, model)\r\n    568   inputs = flatten_perdevice_values(strategy, inputs)\r\n    569   targets = flatten_perdevice_values(strategy, targets)\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/distributed_training_utils.py in _get_input_from_iterator(iterator, model)\r\n    547   # Validate that all the elements in x and y are of the same type and shape.\r\n    548   validate_distributed_dataset_inputs(\r\n--> 549       model._distribution_strategy, x, y, sample_weights)\r\n    550   return x, y, sample_weights\r\n    551\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/distributed_training_utils.py in validate_distributed_dataset_inputs(distribution_strategy, x, y, sample_weights)\r\n    247   # If each element of x and y are not tensors, we cannot standardize and\r\n    248   # validate the input and targets.\r\n--> 249   x_values_list = validate_per_device_inputs(distribution_strategy, x)\r\n    250\r\n    251   if y is not None:\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/distributed_training_utils.py in validate_per_device_inputs(distribution_strategy, x)\r\n    294\r\n    295     # Validate that the shape and dtype of all the elements in x are the same.\r\n--> 296     validate_all_tensor_shapes(x, x_values)\r\n    297     validate_all_tensor_types(x, x_values)\r\n    298\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/distributed_training_utils.py in validate_all_tensor_shapes(x, x_values)\r\n    315     if x_shape != x_values[i].get_shape().as_list():\r\n    316       raise ValueError('Input tensor shapes do not match for distributed tensor'\r\n--> 317                        ' inputs {}'.format(x))\r\n    318\r\n    319\r\n\r\nValueError: Input tensor shapes do not match for distributed tensor inputs PerReplica:{\r\n  0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor(", "This is fixed by method of KUASWoodyLIN. Thanks.", "@KUASWoodyLIN Can you please let us know if you are happy to close if no issue persists", "@muddham  \r\n```train_dataset = mnist_train.repeat(2).map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)```\r\nAdd repeat(2) fixed the problem is because: \r\n120000(train_num * repeat) / 64(batch) = 1875 it can be divisible.\r\nif change batch size to `62`:\r\n120000(train_num * repeat) / 62(batch) = 1935.48, Even though you repeat it 2 time **error will still come out**.\r\n\r\nI think the best solution is fix `tf.data`, that it automatic repeat at mulit-gpu.", "@KUASWoodyLIN As this is resolved, this issue will be closed."]}, {"number": 28333, "title": "Distributed Training TF2.0 validation_data Bug", "body": "**System information**\r\n- OS Platform and Distribution: Docker-nvidia(2.0.0a0-gpu-py3)\r\n- TensorFlow version: '2.0.0-alpha0'\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 10.0\r\n\r\nHello I'm trying the [Code](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/distribute/keras.ipynb) here, \r\nand change the training code.\r\n\r\n```\r\nhistory = model_1.fit(train_data,\r\n                      epochs=200, \r\n                      validation_data=valid_data,\r\n                      callbacks=[model_cbk, model_esp])\r\n```\r\n\r\nAnd it come out the error, I think it should be validation_data bug:\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0502 06:49:43.398314 140660990936832 distributed_training_utils.py:182] Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.\r\nW0502 06:49:43.399307 140660990936832 distributed_training_utils.py:182] Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-16-74ec5f39e862> in <module>\r\n      2                       epochs=200,\r\n      3                       validation_data=valid_data,\r\n----> 4                       callbacks=[model_cbk, model_esp])\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    744             steps_per_epoch=steps_per_epoch,\r\n    745             validation_steps=validation_steps,\r\n--> 746             validation_freq=validation_freq)\r\n    747 \r\n    748     batch_size = self._validate_or_infer_batch_size(\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_distributed.py in fit_distributed(model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\r\n    129         validation_steps=validation_steps,\r\n    130         validation_freq=validation_freq,\r\n--> 131         steps_name='steps_per_epoch')\r\n    132 \r\n    133 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    139 \r\n    140   if mode == ModeKeys.TRAIN:\r\n--> 141     _print_train_info(inputs, val_inputs, steps_per_epoch, verbose)\r\n    142 \r\n    143   # Enter DistributionStrategy scope.\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py in _print_train_info(inputs, val_inputs, steps_per_epoch, verbose)\r\n    437 def _print_train_info(inputs, val_inputs, steps_per_epoch, verbose):\r\n    438   if (val_inputs and steps_per_epoch is None and verbose and inputs and\r\n--> 439       hasattr(inputs[0], 'shape') and hasattr(val_inputs[0], 'shape')):\r\n    440     print('Train on %d samples, validate on %d samples' %\r\n    441           (inputs[0].shape[0], val_inputs[0].shape[0]))\r\n\r\nTypeError: 'BatchDataset' object does not support indexing\r\n```", "comments": ["@KUASWoodyLIN In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "@gadagashwini Here is [Code ](https://colab.research.google.com/gist/KUASWoodyLIN/5567f55bc205f04998db8f9f7a425c34/-keras-ipynb.ipynb)on google colab that I have modified.", "The ```validation_data```  argument in ```model.fit``` in your case is a ```'BatchDataset' object``` which is not among the accepted data structures.\r\nYou may want to convert it in one of the accepted types; tuple (x_val, y_val) of Numpy arrays, dataset iterator etc\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28333\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28333\">No</a>\n"]}, {"number": 28332, "title": "Issue when running tensorflow model (ssd_inception_v2_coco.pd) in webcam....", "body": "UnknownError                              Traceback (most recent call last)\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1333     try:\r\n-> 1334       return fn(*args)\r\n   1335     except errors.OpError as e:\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1318       return self._call_tf_sessionrun(\r\n-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1320 \r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1406         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1407         run_metadata)\r\n   1408 \r\n\r\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d/depthwise, FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights)]]\r\n\t [[{{node num_detections/_195}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1975_num_detections\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-25-05cfc96e122f> in <module>\r\n     17       (boxes, scores, classes, num_detections) = sess.run(\r\n     18           [boxes, scores, classes, num_detections],\r\n---> 19           feed_dict={image_tensor: image_np_expanded})\r\n     20       # Visualization of the results of a detection.\r\n     21       vis_util.visualize_boxes_and_labels_on_image_array(\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    927     try:\r\n    928       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 929                          run_metadata_ptr)\r\n    930       if run_metadata:\r\n    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1151       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1152                              feed_dict_tensor, options, run_metadata)\r\n   1153     else:\r\n   1154       results = []\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1326     if handle is None:\r\n   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1328                            run_metadata)\r\n   1329     else:\r\n   1330       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1346           pass\r\n   1347       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1348       raise type(e)(node_def, op, message)\r\n   1349 \r\n   1350   def _extend_graph(self):\r\n\r\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at <ipython-input-22-0d8b8f2357e8>:7)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d/depthwise, FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights)]]\r\n\t [[{{node num_detections/_195}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1975_num_detections\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d', defined at:\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\r\n    self.io_loop.start()\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 427, in run_forever\r\n    self._run_once()\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1440, in _run_once\r\n    handle._run()\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\asyncio\\events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\r\n    ret = callback()\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tornado\\gen.py\", line 781, in inner\r\n    self.run()\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tornado\\gen.py\", line 742, in run\r\n    yielded = self.gen.send(value)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\r\n    return runner(coro)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3049, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3214, in run_ast_nodes\r\n    if (yield from self.run_code(code, result)):\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-22-0d8b8f2357e8>\", line 7, in <module>\r\n    tf.import_graph_def(od_graph_def, name='')\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 442, in import_graph_def\r\n    _ProcessNewOps(graph)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 234, in _ProcessNewOps\r\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3440, in _add_new_tf_operations\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3440, in <listcomp>\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3299, in _create_op_from_tf_operation\r\n    ret = Operation(c_op, self)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nUnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at <ipython-input-22-0d8b8f2357e8>:7)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d/depthwise, FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights)]]\r\n\t [[{{node num_detections/_195}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1975_num_detections\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\n\r\n\u200b\r\nthis is the issue I am getting when running in Webcam. How can I resolve it?\r\n\r\nconfiguration:\r\ntensorflow-gpu 1.12\r\ncuda 9.0\r\ncudnn 8.0\r\ngpu : nvidia geforce gtx 1050  8gb\r\n", "comments": ["Please try to have a look on this [issue](https://github.com/tensorflow/tensorflow/issues/24496) and let us know if that helps to solve the problem. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\n", "**I have the same issue **\r\nInternalError                             Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1333     try:\r\n-> 1334       return fn(*args)\r\n   1335     except errors.OpError as e:", "Restart your python interpreter.\r\nThis clears the open sessions and resolves this error", "This actually works for me."]}, {"number": 28331, "title": "Broken link in Doc [ api_docs/python/tf/test ]", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: TensorFlow Core 1.13\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/test\r\n\r\n\r\n**Describe the documentation issue**\r\nThe link pointing to the testing guide is broken. \r\nbroken link -  [https://tensorflow.org/api_guides/python/test](https://tensorflow.org/api_guides/python/test)\r\n\r\n[A snapshot of the broken link, working as of 22nd Feb 2019](https://web.archive.org/web/20190222060703/https://tensorflow.org/api_guides/python/test)\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@ramithuh Thanks for finding the link and also for providing screenshot of earlier page. Thanks!", "@ramithuh please confirm if you are still facing the issue", "Thanks, it looks like this is a duplicate of: #26645 \r\nI'll go fix that.", "Thanks!"]}, {"number": 28330, "title": "r1.14 with cuda 10.1 build failed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14.0rc0\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source):  7.4.0\r\n- CUDA/cuDNN version: 10.1/7.5.2\r\n- GPU model and memory: RTX2080Ti GDDR6 11GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nbuild failed\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel build\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nexternal/protobuf_archive/src/google/protobuf/map.h:490:29: error: cannot call member function \u2018bool google::protobuf::Map<Key, T>::InnerMap::TableEntryIsNonEmptyList(google::protobuf::Map<Key, T>::size_type) const [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::Map<Key, T>::size_type = long unsigned int]\u2019 without object\r\n         if (m_->TableEntryIsNonEmptyList(bucket_index_)) {\r\n         ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/src/google/protobuf/map.h:504:24: error: cannot call member function \u2018bool google::protobuf::Map<Key, T>::InnerMap::TableEntryIsList(google::protobuf::Map<Key, T>::size_type) const [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::Map<Key, T>::size_type = long unsigned int]\u2019 without object\r\n         return m_->TableEntryIsList(bucket_index_);\r\n         ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~\r\nERROR: /home/wproject/repo/tensorflow/tensorflow/core/kernels/BUILD:4124:1: output 'tensorflow/core/kernels/_objs/histogram_op_gpu/histogram_op_gpu.cu.pic.o' was not created\r\nERROR: /home/wproject/repo/tensorflow/tensorflow/core/kernels/BUILD:4124:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.", "comments": ["I am having the same issue.  ", "This is due to a bug in the nvcc compiler with CUDA 10.1. NVIDIA is working on a fix.\r\n\r\nSame issue has been reported in https://github.com/tensorflow/tensorflow/issues/26155\r\n\r\nYou can try gcc 7.2", "closing this issue since today's cuda update resolve errors"]}, {"number": 28329, "title": "The source code compiles tensorflow off tensorrt is invalid.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: master branch\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):0.24\r\n- CUDA/cuDNN version:10.0/7.5\r\n- GPU model and memory:12G\r\n\r\nWhen the source code compiles tensorflow, it closes TensorRT through the prompt of ./configure. But still reports: Could not find any NvInfer.h matching version '' in any subdirectory error.\r\n\r\nConfigure option\uff1a\r\n\r\n./configure \r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1 installed.\r\nPlease specify the location of python. [Default is /home/guangyuan/anaconda3/envs/tfsource/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /home/guangyuan/anaconda3/envs/tfsource/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/guangyuan/anaconda3/envs/tfsource/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\n**Do you wish to build TensorFlow with TensorRT support? [y/N]: n**\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nError:\r\n**Could not find any NvInfer.h matching version '' in any subdirectory:**\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        '/lib'\r\n        '/lib/x86_64-linux-gnu'\r\n        '/lib32'\r\n        '/libx32'\r\n        '/usr'\r\n        '/usr/lib'\r\n        '/usr/lib/x86_64-linux-gnu'\r\n        '/usr/lib/x86_64-linux-gnu/libfakeroot'\r\n        '/usr/lib32'\r\n        '/usr/libx32'\r\n        '/usr/local/cuda-10.0'\r\n        '/usr/local/zlib/lib'\r\nAsking for detailed CUDA configuration...", "comments": ["This should have been fixed by this commit: https://github.com/tensorflow/tensorflow/commit/28fc9cccd176a113b9f0401d0c569ca1b9d207c3\r\n\r\nPlease try pulling the latest code.", "Please let us know if you are still facing the problem. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\n"]}, {"number": 28328, "title": "DLL Error - cuDNN I Believe?", "body": "Issue occurs when running anything that uses TensorFlow, including a simple script that should simply spit out my TF Version and included sample files.\r\nOS - Win10 Home\r\nCuda Toolkit - 10.0.130_411.31_win10\r\ncuDNN - 10.0\r\nPython - 3.6.8\r\nTensorflow - Pip install of current \"tensorflow-GPU\" as of 5-1-2019\r\nGPU - EVGA RTX 2080 ti FTW3 Ultra Hybrid (11gb GDDR6 memory)\r\n\r\nWhen running any .py that uses TensorFlow since setting up this new environment, I get the following error report. I've followed all the instructions very closely as I did when I set up a working Linux environment, just no luck on Win10. \r\n\r\n>>> \r\n RESTART: E:\\AI Playground\\Samples\\samples\\cookbook\\regression\\automobile_data.py \r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:\\AI Playground\\Samples\\samples\\cookbook\\regression\\automobile_data.py\", line 25, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": []}, {"number": 28327, "title": "Max retries error with BigtableClient", "body": "For reference, pertains to [`tf.contrib.cloud.bigtable_api.py`](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/bigtable/python/ops/bigtable_api.py) and `BigtableClient`.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not really\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 'v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nDataset constructed by way of `tf.contrib.cloud.BigtableClient` raises error that is primarily the following:\r\n\r\n```bash\r\nInternalError (see above for traceback): Error reading from Cloud Bigtable: No more retries allowed as per policy.\r\n\t [[node IteratorGetNext (defined at <ipython-input-1-b90b20ec4e01>:10) ]]\r\n```\r\n\r\nThis occurs both in graph and eager mode; occurs on a machine where the conventional bigtable python client apis function properly (appropriately credentialed); persists after alternatively activating a service account with the appropriate credentials via `gcloud auth activate-service-account ...`.\r\n\r\n**Describe the expected behavior**\r\n\r\nIterator yields training examples from Dataset when iterated over without error.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nfrom pcml.operations.tfrecord2bigtable import BigTableSelection\r\nimport tensorflow as tf\r\n\r\ndata = BigTableSelection(\r\n    project=\"clarify\",\r\n    instance=\"clarify-cbt-instance\",\r\n    table=\"clarify-cbt-devtabl1\",\r\n    prefix=\"train\",\r\n    column_family=\"tfexample\",\r\n    column_qualifier=\"example\")\r\n\r\nclient = tf.contrib.cloud.BigtableClient(data.project, data.instance)\r\n\r\ntable = client.table(data.table)\r\n\r\nds = table.parallel_scan_prefix(\r\n    data.prefix, columns=[(data.column_family, data.column_qualifier)])\r\n\r\nds_data = ds.map(lambda index, data: data)\r\n\r\nds_data = ds_data.repeat()\r\n\r\niterator = ds_data.make_initializable_iterator()\r\n\r\nnext_element = iterator.get_next()\r\n\r\nsess = tf.Session()\r\n\r\nsess.run(iterator.initializer)\r\n\r\n_ = sess.run(next_element)\r\n\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```bash\r\n\u200b\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nINFO:tensorflow:Entry Point [tensor2tensor.envs.tic_tac_toe_env:TicTacToeEnv] registered with id [T2TEnv-TicTacToeEnv-v0]\r\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1333     try:\r\n-> 1334       return fn(*args)\r\n   1335     except errors.OpError as e:\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1318       return self._call_tf_sessionrun(\r\n-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1320 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1406         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1407         run_metadata)\r\n   1408 \r\n\r\nInternalError: Error reading from Cloud Bigtable: No more retries allowed as per policy.\r\n\t [[{{node IteratorGetNext}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-1-acb694909ee3> in <module>\r\n     29 \r\n     30 sess.run(iterator.initializer)\r\n---> 31 _ = sess.run(next_element)\r\n     32 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    927     try:\r\n    928       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 929                          run_metadata_ptr)\r\n    930       if run_metadata:\r\n    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1151       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1152                              feed_dict_tensor, options, run_metadata)\r\n   1153     else:\r\n   1154       results = []\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1326     if handle is None:\r\n   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1328                            run_metadata)\r\n   1329     else:\r\n   1330       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1346           pass\r\n   1347       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1348       raise type(e)(node_def, op, message)\r\n   1349 \r\n   1350   def _extend_graph(self):\r\n\r\nInternalError: Error reading from Cloud Bigtable: No more retries allowed as per policy.\r\n\t [[node IteratorGetNext (defined at <ipython-input-1-acb694909ee3>:26) ]]\r\n\r\nCaused by op 'IteratorGetNext', defined at:\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\r\n    self.io_loop.start()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 1431, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.6/asyncio/events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\r\n    handler_func(fileobj, events)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/jovyan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"/home/jovyan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\r\n    return runner(coro)\r\n  File \"/home/jovyan/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"/home/jovyan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/jovyan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3209, in run_ast_nodes\r\n    if (yield from self.run_code(code, result)):\r\n  File \"/home/jovyan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-acb694909ee3>\", line 26, in <module>\r\n    next_element = iterator.get_next()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 414, in get_next\r\n    output_shapes=self._structure._flat_shapes, name=name)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1685, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInternalError (see above for traceback): Error reading from Cloud Bigtable: No more retries allowed as per policy.\r\n\t [[node IteratorGetNext (defined at <ipython-input-1-acb694909ee3>:26) ]]\r\n```", "comments": ["Further suggesting this is a credential issue the following modification to use `keys_by_prefix_dataset` gives an explicit error about credentials.\r\n\r\n```python\r\n\r\nimport os\r\nimport tensorflow as tf\r\nfrom pcml.operations.tfrecord2bigtable import BigTableSelection\r\n\r\ndata = BigTableSelection(\r\n    project=\"clarify\",\r\n    instance=\"clarify-cbt-instance\",\r\n    table=\"clarify-cbt-devtable1\",\r\n    prefix=\"train\",\r\n    column_family=\"tfexample\",\r\n    column_qualifier=\"example\")\r\n\r\nclient = tf.contrib.cloud.BigtableClient(data.project, data.instance)\r\ntable = client.table(data.table)\r\nkey_dataset = table.keys_by_prefix_dataset(\"train\")\r\nds = key_dataset.apply(table.lookup_columns((\"cf1\", \"tfexample\")))\r\nds_data = ds.map(lambda index, data: data)\r\nds_data = ds_data.repeat()\r\n\r\niterator = ds_data.make_initializable_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nsess = tf.Session()\r\nsess.run(iterator.initializer)\r\n_ = sess.run(next_element)\r\n\r\n```\r\n\r\nwith error:\r\n\r\n```bash\r\nUnauthenticatedError (see above for traceback): Error reading from Cloud Bigtable: Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.\r\n\t [[node IteratorGetNext_2 (defined at <ipython-input-26-f0eff0cf13a5>:36) ]]\r\n```\r\n\r\nThis occurring with and without `GOOGLE_APPLICATION_CREDENTIALS` being set explicitly and verified application-default-credentials are indeed set (`gcloud auth application-default print-access-token` prints an access token).\r\n\r\nAs you can tell my intuition is that the issue has either to do with the compiled client op not properly discovering and using credentials or the client machine being mis-configured vis. credentials.", "So this appears to be resolved (at least on my end) by resolving issues with both parameterization and environment credentials. In the former case the retries error can be produced if e.g. a non-existent table name or the wrong column family is provided. In the latter case I'm seeing the error occur if I don't unset `GOOGLE_APPLICATION_CREDENTIALS` which in my case is pointing to a default GCP service account (which in my case does not have the necessary scopes) instead of the service account that is activated and being used otherwise.\r\n\r\n```python\r\n\r\nimport os\r\nimport tensorflow as tf\r\nfrom pcml.operations.tfrecord2bigtable import BigTableSelection\r\n\r\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\"\r\n\r\ndata = BigTableSelection(\r\n    project=\"clarify\",\r\n    instance=\"clarify-cbt-instance\",\r\n    table=\"clarify-cbt-devtable\",\r\n    prefix=\"train\",\r\n    column_family=\"tfexample\",\r\n    column_qualifier=\"example\")\r\n\r\nclient = tf.contrib.cloud.BigtableClient(data.project, data.instance)\r\ntable = client.table(data.table)\r\n\r\n# ===\r\n# Option 1\r\nkey_dataset = table.keys_by_prefix_dataset(data.prefix)\r\nds = key_dataset.apply(table.lookup_columns((data.column_family,\r\n                                             data.column_qualifier)))\r\n# ===\r\n# Option 2\r\nds = table.parallel_scan_prefix(\r\n    data.prefix, columns=[(data.column_family, data.column_qualifier)])\r\n# ===\r\n\r\nds_data = ds.map(lambda index, data: data)\r\nds_data = ds_data.repeat()\r\niterator = ds_data.make_initializable_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nsess = tf.Session()\r\nsess.run(iterator.initializer)\r\n_ = sess.run(next_element)\r\n\r\n```", "Closing this issue since its resolved. Feel free to reopen if still having problems. Thanks!"]}, {"number": 28326, "title": "UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TF 2.0 alpha0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): I don't know\r\n- GCC/Compiler version (if compiling from source): I don't know\r\n- CUDA/cuDNN version: cudnn 7.3.1, Cuda 10.0.1\r\n- GPU model and memory: Titan RTX 24GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI just built one conv2d layer and gave it a random input, that's all. Throws error above.\r\nI'm using Anaconda. I installed tf-gpu 2.0 alpha0 via command line, cause Anaconda doesn't offer it.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nlayer = tf.keras.layers.Conv2D(2, (3,3))\r\nlayer(np.random.rand(5,5,3,1))\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-4-7d2f5dcef289> in <module>\r\n----> 1 layer(np.random.rand(5,5,3,1))\r\n\r\n~\\.conda\\envs\\alphagpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    658           with base_layer_utils.autocast_context_manager(\r\n    659               input_list, self._mixed_precision_policy.should_cast_variables):\r\n--> 660             outputs = self.call(inputs, *args, **kwargs)\r\n    661           self._handle_activity_regularization(inputs, outputs)\r\n    662           self._set_mask_metadata(inputs, outputs, previous_mask)\r\n\r\n~\\.conda\\envs\\alphagpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py in call(self, inputs)\r\n    194 \r\n    195   def call(self, inputs):\r\n--> 196     outputs = self._convolution_op(inputs, self.kernel)\r\n    197 \r\n    198     if self.use_bias:\r\n\r\n~\\.conda\\envs\\alphagpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in __call__(self, inp, filter)\r\n   1076 \r\n   1077   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin\r\n-> 1078     return self.conv_op(inp, filter)\r\n   1079 \r\n   1080 \r\n\r\n~\\.conda\\envs\\alphagpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in __call__(self, inp, filter)\r\n    632 \r\n    633   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin\r\n--> 634     return self.call(inp, filter)\r\n    635 \r\n    636 \r\n\r\n~\\.conda\\envs\\alphagpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in __call__(self, inp, filter)\r\n    231         padding=self.padding,\r\n    232         data_format=self.data_format,\r\n--> 233         name=self.name)\r\n    234 \r\n    235 \r\n\r\n~\\.conda\\envs\\alphagpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\r\n   1949                            data_format=data_format,\r\n   1950                            dilations=dilations,\r\n-> 1951                            name=name)\r\n   1952 \r\n   1953 \r\n\r\n~\\.conda\\envs\\alphagpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\r\n   1119             input, filter, strides=strides, use_cudnn_on_gpu=use_cudnn_on_gpu,\r\n   1120             padding=padding, explicit_paddings=explicit_paddings,\r\n-> 1121             data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\r\n   1122       except _core._SymbolicException:\r\n   1123         pass  # Add nodes to the TensorFlow graph.\r\n\r\n~\\.conda\\envs\\alphagpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py in conv2d_eager_fallback(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\r\n   1218   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\r\n   1219   _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\r\n-> 1220                              ctx=_ctx, name=name)\r\n   1221   _execute.record_gradient(\r\n   1222       \"Conv2D\", _inputs_flat, _attrs, _result, name)\r\n\r\n~\\.conda\\envs\\alphagpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   except TypeError as e:\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n~\\.conda\\envs\\alphagpu\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]\r\n", "comments": ["Some people reported the same thing but with tf 1.13, and they solved it by changing cudnn version to 7.4. In my case, Anaconda only offers 7.3.1 max! so, I tried downgrading to solve it, but that will knock down CUDA from 10.0 to 9.0 which makes TF 2.0 dead. So, I'm stuck with this version of Cudnn.\r\nOther people recommended allowing memory growth and stuff like that. Did not work.", "This error is for sure due to version mismatch of Cuda/cudnn. When I got this error I followed the instructions on this link https://www.tensorflow.org/install/gpu and it solved the problem. Have you tried that?", "@kernelizd : TensorFlow supports CUDA 10.0 (TensorFlow >= 1.13.0). You can try @Jonathan-Livingston-Seagull's suggestion and let us know if that helps. Thanks! ", "In the webpage you referred me to tensorflow.org, it mentions this:\r\n**cuDNN SDK (>= 7.4.1)**\r\nas software requirement for TF 2.0\r\nmy cuDNN however is only 7.3 cause that is the latest Anaconda is offering.\r\nIt could very well be because of this\r\n ", "You could uninstall cudnn from the conda environment and install the required version of cudnn in your windows system natively and update the path variable accordingly. There is a separate section for windows set up in that link. Please follow those steps.", "Great idea. Thanks", "> In the webpage you referred me to tensorflow.org, it mentions this:\r\n> **cuDNN SDK (>= 7.4.1)**\r\n> as software requirement for TF 2.0\r\n> my cuDNN however is only 7.3 cause that is the latest Anaconda is offering.\r\n> It could very well be because of this\r\n\r\nhi ,why there is also  UnknownError: Failed to get convolution algorithm. when i use cuda 10.0 cudann 7.4.1", "I haven't actually tried it, I just blamed the my 7.3 cudnn. Based on your report that the same problem exists in 7.4, I reopened the issue.", "@kernelizd : Did you get chance to look on #24828 . Let us know if that helps. Thanks!", "I did. That issue is about TF 1.13. In my case it is TF 2.0-alpha\r\nAt any rate, the solutions offered for both cases are almost the same. I did try them but no luck.\r\nI downgraded my cuda to 10, updated my cudnn to 7.4  and now it is working", "I hava 2 GPUs: RTX 2070\r\nMy TensorFlow is v1.13.1, cuda is 10.0 and cudnn is 7.4.1 for cuda10.\r\nSystem version: CentOS 7.6\r\n\r\nWhen I run the test code with TensorFlow and cnn layer, it still has this exception.\r\nI can run the sample codes provided by cudnn but error when tensorflow.", "RTX2060 and Win10 + Anaconda + CUDA 10.0 + Cudnn 7.4.1 ---- it's OK!\r\n\r\nBecause anaconda doesn't have cudnn 7.4.1.\r\nI tried to install tf by pip instead of conda, it's work!", "I was facing the same issue with tensorflow-gpu version 2.0-beta, and figured this is definitely a tensorflow-cuda-cudnn version mismatch problem. The issue occuered because my default Python  came from Anaconda, whereas I was installing CUDA/CUDNN as instructed on  TF-gpu installation page. I installed the latest versions of both CUDA and CUDNN, but, every time I checked the versions shown were older ones, and not the ones I had installed. So I installed everything again using conda instead, and it seemed to work flawlessly.   \r\n\r\n**Tensorflow version -- 2.0.0-beta1\r\nCUDA version -- 10.1.168 \r\nCUDNN version -- 7.6.0**\r\n\r\nUse the following command to check CUDA installation by Conda:\r\n\r\n`conda list cudatoolkit`\r\n\r\nAnd the following command to check CUDNN version installed by conda:\r\n\r\n`conda list cudnn`\r\n\r\nIf you want to install/update CUDA and CUDNN through CONDA, please use the following commands:\r\n\r\n```\r\nconda install -c anaconda cudatoolkit\r\nconda install -c anaconda cudnn\r\n```\r\n\r\nAlternatively you can use following commands to check CUDA installation:\r\n\r\n`nvidia-smi`\r\nOR\r\n`nvcc --version`\r\n\r\nIf you are using tensorflow-gpu through Anaconda package (You can verify this by simply opening Python in console and check if the default python shows Anaconda, Inc. when it starts, or you can run which python and check the location), then manually installing CUDA and CUDNN will most probably not work. You will have to update through conda instead.\r\n\r\nIf you want to install CUDA, CUDNN, or tensorflow-gpu manually, you can check out the instructions here https://www.tensorflow.org/install/gpu", "I downgraded cuDnn from 7.6.2 to 7.4.1 and restarted. the problem solved.\r\nmy cuda is 10.0 always.\r\nbut the tensorflow page says \r\n[cuDNN SDK (>= 7.4.1)]\r\nSo I installed cuDNN 7.6.2 at first and worked fine before I started to use CNN.\r\n\r\nSo this should be \r\n[cuDNN SDK (= 7.4.1)]", "@sidd-hart Installing everything via conda totally worked for me with Tensorflow 2.0.0, Cuda 10.1, and Cudnn 7.6.2. Thank you so much!", "Actually, I've tried docker For GPU support, bug it failed. I pulled tensorflow/tensorflow:1.15.0-gpu-py3, and it still has this problem, I think it better to reopen this issue. @kernelizd \r\n\r\nstack trace as below:\r\n```bash\r\n>>> model.fit(x=input_data, y=output_data, epochs=10, batch_size=16)\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nTrain on 1000 samples\r\nEpoch 1/10\r\n2019-11-19 07:31:08.837737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-19 07:31:09.597102: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-19 07:31:11.658410: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-11-19 07:31:11.674693: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 727, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 675, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 394, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\", line 3476, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1472, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node replica_0/model/conv2d_2/Conv2D}}]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node replica_0/model/conv2d_2/Conv2D}}]]\r\n\t [[replica_0/model/dense/Sigmoid/_105]]\r\n0 successful operations.\r\n1 derived errors ignored.\r\n```", "I was facing the same problem with tf2.0\r\n\r\nSetting up cudnn [7.6.1] and cudatoolkit [10.1] in my conda env fixed this for me! Thanks @sidd-hart ", "I am facing the same error with an RTX 2080ti. Here is my output to nviidia-smi:\r\n\r\n![image](https://user-images.githubusercontent.com/12706804/69486426-a32dc300-0e71-11ea-8cc9-c02375a3fb5c.png)\r\nLooks like my GPU memory is full? Is is possible I can make other apps to stop using gpu memory permanently?", "I am still facing the same problem with tf2.0, cudnn [7.6.4] and cudatoolkit [10.1] in my conda env. @sidd-hart", "Got this error with tf2.0, cudnn 7, cuda 10.2 and 10.0.", "Hi all. I'm also having the same problem.\r\n\r\nI'm using Ubuntu 18.04, cudnn 7.6.4 and using anaconda as well. Anyone can help? \r\n\r\nUnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d_189/convolution}}]]\r\n\t [[metrics_4/acc/Mean_1/_14657]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d_189/convolution}}]]\r\n0 successful operations.\r\n0 derived errors ignored.", "@dayangkunurfaizah checkout colab, it works, but free GPU only can be used up to 12 hours. I still did not find any good solution for this.", "@dayangkunurfaizah please try with the following settings:\r\n ``` \r\nfrom tensorflow.compat.v1.keras.backend import set_session\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\nconfig.log_device_placement = True  # to log device placement (on which device the operation ran)\r\nsess = tf.compat.v1.Session(config=config)\r\nset_session(sess)\r\n```", "check you GPU resources usage by run command `nvidia-smi` . The issue  will occur when the Memory-Usage is overwhelming", "try:\r\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\r\n\r\nref\r\nhttps://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in\r\n", "> try:\r\n> os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\r\n> \r\n> ref\r\n> https://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in\r\n\r\nThank u very mutch", "Thank you @piaobuliao . You saved my day!", "\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n  except RuntimeError as e:\r\n    print(e)\r\n#Just paste these lines to solve the problem", "> from tensorflow.compat.v1.keras.backend import set_session\r\n> config = tf.compat.v1.ConfigProto()\r\n> config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\n> config.log_device_placement = True  # to log device placement (on which device the operation ran)\r\n> sess = tf.compat.v1.Session(config=config)\r\n> set_session(sess)\r\n\r\nRTX 2070 requires this everytime I use Tensorflow ", "> https://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in\r\n\r\nsave me time!!!", "Was facing the same issue, I think GPU is not able to load all the data at once.\r\nI resolved it by reducing the batch size.", "GUYS, I was facing the same issue and found the solution..\r\nyou all must have copy and pasted the address of your cudnn folders in the environment variable, now delete them and copy and paste all the files in C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\\"your version\" and then in respected folders like bin files in bin folder and lib file in lib folder and in end the include file in include folder and try again YOU WILL NOT GET ERROR !! "]}, {"number": 28325, "title": "Two TensorRT related cherrypicks", "body": "Related to the TensorRT fixes in: https://github.com/tensorflow/tensorflow/pull/28296", "comments": []}, {"number": 28324, "title": "Random Uniform Not Supported for TFlite Conversion", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): tf-nightly 1.14.0-rc0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, BATCH_TO_SPACE_ND, CAST, CONCATENATION, CONV_2D, FULLY_CONNECTED, GREATER_EQUAL, MUL, RESHAPE, RESIZE_BILINEAR, SPACE_TO_BATCH_ND. Here is a list of operators for which you will need custom implementations: RandomUniform.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nI followed [#26300](https://github.com/tensorflow/tensorflow/issues/26300) by using\r\n```\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                        tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\nBut I still can't create .tflite file correctly.", "comments": ["There's a warning from lite/toco/graph_transformations/resolve_constant_random_uniform.cc:\r\n\r\n`RandomUniform op outputting \"segmentation_dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this`\r\n\r\nDoes it mean that currently RandomUniform only support constant?", "@futuyao In order to expedite the trouble-shooting process,Please provide the detail error message. Thanks!", "Resolved by downgrading Tensorflow version to r1.13.", "I can now convert the model to tflite with select tensorflow operators. But when I try to load the model in XCode for iOS, it sill can't find Random Uniform operator. Any advice for how to make this work?\r\n\r\n`Didn't find custom op for name 'RandomUniform' with version 1`", "Sorry for the late reply!\r\n\r\nCould you provide with us the .tflite flabuffer or the original GraphDef you are usin(assuming it's not confidential), so that I can give it a try.", "> Sorry for the late reply!\r\n> \r\n> Could you provide with us the .tflite flabuffer or the original GraphDef you are usin(assuming it's not confidential), so that I can give it a try.\r\n\r\nI have my tflite file here: https://drive.google.com/file/d/1yIjABOGxUTP81O0gJeaRHFayoNLiDKIi/view?usp=sharing\r\n\r\nThanks!", "Sorry for the late.\r\n\r\nIt seems like the iOS app is not built with the select TF ops (RandomUniform), so it can't resolve the op at runtime.\r\n\r\nCould you follow instructions here and try again? Thanks.\r\nhttps://www.tensorflow.org/lite/guide/ops_select#ios", "Hi, @futuyao\r\n\r\nSorry for the belated. Is ios ops_select working for your model? ", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28324\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28324\">No</a>\n"]}, {"number": 28323, "title": "Input-shape validation error when using TimeDistributed wrapper", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Mojave 10.14.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: Python 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nI have built a model consisting of a CNN feeding an LSTM. I've wrapped the CNN in a TimeDistributed wrapper. I've configured my training set (x values) to have a 5D shape (batch_size, num_steps, image_height, image_width, channels). When trying to train the model, I get the following error: `ValueError: Error when checking input: expected time_distributed_input to have 5 dimensions, but got array with shape (4, 28, 28, 1)`.\r\n\r\nI've tried a number of different model configurations, and have debugged through the code. I can see where the ValueError is being thrown, but I don't see any way to keep this from happening. Code details follow.\r\n\r\nI'm not expecting a discrepancy between what the model thinks I'm providing and what I'm providing. I'm providing a 5D data set, and that's what it wants, but it claims that I'm only providing a 4D data set.\r\n\r\nHere is the code I'm using. For the input data, I'm using MNIST images in 28x28x1 format. For the `x` data, I'm creating 20 records, each consisting of a list of four 28x28x1 image vectors, hence an input shape of (20,4,28,28,1). The `y` data consists of a 1D list of labels. \r\n```\r\ndef build_cnn(input_shape=None):\r\n    print('Building the CNN')\r\n    model = tfkm.Sequential()\r\n    if input_shape:\r\n        model.add(tfkl.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))\r\n    else:\r\n        model.add(tfkl.Conv2D(64, (3, 3), activation='relu'))\r\n    model.add(tfkl.MaxPooling2D((2, 2), strides=(1, 1)))\r\n    model.add(tfkl.Conv2D(128, (4, 4), activation='relu'))\r\n    model.add(tfkl.MaxPooling2D((2, 2), strides=(2, 2)))\r\n    model.add(tfkl.Conv2D(256, (4, 4), activation='relu'))\r\n    model.add(tfkl.MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\n    # extract features and dropout\r\n    model.add(tfkl.Flatten())\r\n    return model\r\n\r\n\r\ndef build_lstm(units, return_sequences, dropout):\r\n    print('Building the LSTM model')\r\n    model = tfkm.Sequential()\r\n    model.add(tfkl.LSTM(units, return_sequences=return_sequences, dropout=dropout))\r\n    return model\r\n\r\n\r\ndef build_classification_layer(num_classes):\r\n    print('Building the classification-layer model')\r\n    model = tfkm.Sequential()\r\n    # classifier with sigmoid activation for multilabel\r\n    model.add(tfkl.Dense(num_classes, activation='sigmoid'))\r\n    return model\r\n\r\n\r\ncnn = build_cnn(input_shape=(28, 28, 1))\r\nlstm = build_lstm(256, False, 0.5)\r\nclassification_layer = build_classification_layer(2)\r\n\r\n\r\ncombined_model = tfkm.Sequential()\r\n#combined_model.add(tfkl.TimeDistributed(cnn, batch_input_shape=(20, 4, 28, 28, 1)))\r\n#combined_model.add(tfkl.TimeDistributed(cnn, input_shape=(4, 28, 28, 1), batch_size=20))\r\ncombined_model.add(tfkl.TimeDistributed(cnn, input_shape=(4, 28, 28, 1)))\r\ncombined_model.add(lstm)\r\ncombined_model.add(classification_layer)\r\ncombined_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n\r\n.\r\n.\r\n.\r\ndata-set building:  creating 20 individual records, each containing a sequence of 4 28x28x1 images and the associated labels list.\r\n.\r\n.\r\n.\r\nprint('train_input shape: {}  train_labels shape: {}'.format(np.array(train_input).shape, np.array(train_labels).shape))\r\n\r\nprint('Training the model')\r\ncombined_model.fit(train_input, train_labels, epochs=2)\r\n```\r\n\r\nOutput from training attempt:\r\n```\r\ntrain_input shape: (20, 4, 28, 28, 1)  train_labels shape: (20,)\r\nTraining the model\r\nTraceback (most recent call last):\r\n  File \"/Users/scott/sandbox/CNN-LSTM-sandbox/build_and_train.py\", line 184, in <module>\r\n    combined_model.fit(train_input, train_labels, epochs=2)\r\n  File \"/anaconda3/envs/TF2_py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 806, in fit\r\n    shuffle=shuffle)\r\n  File \"/anaconda3/envs/TF2_py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 2596, in _standardize_user_data\r\n    exception_prefix='input')\r\n  File \"/anaconda3/envs/TF2_py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 340, in standardize_input_data\r\n    'with shape ' + str(data_shape))\r\nValueError: Error when checking input: expected time_distributed_input to have 5 dimensions, but got array with shape (4, 28, 28, 1)\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nNote that I get this same result regardless of which of the three `combined_model.add(tfkl.TimeDistributed(cnn...` versions I have in my code.\r\n\r\nWhile debugging, I found the place where the ValueError is thrown. It is in `training_utils.py`, line 336 - `if len(data_shape) != len(shape):` (lines 303-350 shown for context):\r\n\r\nlines 303-350 of training_utils.py:\r\n```\r\n  if len(data) != len(names):\r\n    if data and hasattr(data[0], 'shape'):\r\n      raise ValueError('Error when checking model ' + exception_prefix +\r\n                       ': the list of Numpy arrays that you are passing to '\r\n                       'your model is not the size the model expected. '\r\n                       'Expected to see ' + str(len(names)) + ' array(s), '\r\n                       'but instead got the following list of ' +\r\n                       str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\r\n    elif len(names) > 1:\r\n      raise ValueError('Error when checking model ' + exception_prefix +\r\n                       ': you are passing a list as input to your model, '\r\n                       'but the model expects a list of ' + str(len(names)) +\r\n                       ' Numpy arrays instead. The list you passed was: ' +\r\n                       str(data)[:200])\r\n    elif len(data) == 1 and not hasattr(data[0], 'shape'):\r\n      raise TypeError('Error when checking model ' + exception_prefix +\r\n                      ': data should be a Numpy array, or list/dict of '\r\n                      'Numpy arrays. Found: ' + str(data)[:200] + '...')\r\n    elif len(names) == 1:\r\n      data = [np.asarray(data)]\r\n\r\n  # Check shapes compatibility.\r\n  if shapes:\r\n    for i in range(len(names)):\r\n      if shapes[i] is not None:\r\n        if tensor_util.is_tensor(data[i]):\r\n          tensorshape = data[i].get_shape()\r\n          if not tensorshape:\r\n            continue\r\n          data_shape = tuple(tensorshape.as_list())\r\n        else:\r\n          data_shape = data[i].shape\r\n        shape = shapes[i]\r\n        if len(data_shape) != len(shape):\r\n          raise ValueError('Error when checking ' + exception_prefix +\r\n                           ': expected ' + names[i] + ' to have ' +\r\n                           str(len(shape)) + ' dimensions, but got array '\r\n                           'with shape ' + str(data_shape))\r\n        if not check_batch_axis:\r\n          data_shape = data_shape[1:]\r\n          shape = shape[1:]\r\n        for dim, ref_dim in zip(data_shape, shape):\r\n          if ref_dim != dim and ref_dim is not None and dim is not None:\r\n            raise ValueError('Error when checking ' + exception_prefix +\r\n                             ': expected ' + names[i] + ' to have shape ' +\r\n                             str(shape) + ' but got array with shape ' +\r\n                             str(data_shape))\r\n  return data\r\n```\r\nthe state of things when the debugger hits line 336 is:\r\n```\r\nnp.array(data).shape = <class 'tuple'>: (1, 4, 28, 28, 1)\r\ndata_shape = <class 'tuple'>: (4, 28, 28, 1)\r\nname = <class 'list'>: ['time_distributed_input']\r\nshapes = <class 'list'>: [(None, 4, 28, 28, 1)]\r\n```\r\nFrom this, one can see that the data shape getting to this point is correct, but the code pulls out the the first record in the batch and compares its shape to the shape the TimeDistributed layer is expecting. The problem is that the TimeDistributed layer is expecting a batch representation in the shape (`None`, 4, 28, 28, 1) but it's not there b/c the code only pulled the input record at that batch location `data_shape = data[i].shape`, that is, the code is using the `(4,28,28,1)` record's shape to compare with the TimeDistributed input shape `(None,4,28,28,1)` which can never succeed\r\n![Screen Shot 2019-05-01 at 2 50 51 PM](https://user-images.githubusercontent.com/16819138/57046609-56ad3c80-6c2e-11e9-9073-51ed7631d526.png)\r\n![Screen Shot 2019-05-01 at 2 42 13 PM](https://user-images.githubusercontent.com/16819138/57046632-6dec2a00-6c2e-11e9-9c3b-eafe53f0b694.png)\r\n\r\n.\r\n\r\nThe way I see it, the batch dimension shouldn't be represented in the tuple that is used to establish expected shape of data to the InputLayer, the batch length comparison needs to ignore `None` in the shape tuples, or the comparison should be `if len(np.array(data).shape) != len(shape):`\r\n", "comments": ["made some progress on this. Added the designated line to the following block of code in `training_utils.py`, starting with line 296:\r\n```\r\n  if shapes is not None:\r\n    data = np.expand_dims(data, 0)  # added by SQ\r\n    data = [\r\n        standardize_single_array(x, shape) for (x, shape) in zip(data, shapes)\r\n    ]\r\n  else:\r\n    data = [standardize_single_array(x) for x in data]\r\n```\r\n\r\nAfter adding this line, I can train the model.... I haven't verified that it actually works yet, but I'm no longer getting shape errors.", "The real issue is in this block (lines 274-295 of `training_utils.py`):\r\n```  if isinstance(data, dict):\r\n    try:\r\n      data = [\r\n          data[x].values\r\n          if data[x].__class__.__name__ == 'DataFrame' else data[x]\r\n          for x in names\r\n      ]\r\n    except KeyError as e:\r\n      raise ValueError('No data provided for \"' + e.args[0] + '\". Need data '\r\n                       'for each key in: ' + str(names))\r\n  elif isinstance(data, (list, tuple)):\r\n    if isinstance(data[0], (list, tuple)):\r\n      data = [np.asarray(d) for d in data]\r\n    elif len(names) == 1 and isinstance(data[0], (float, int)):\r\n      data = [np.asarray(data)]\r\n    else:\r\n      data = [\r\n          x.values if x.__class__.__name__ == 'DataFrame' else x for x in data\r\n      ]\r\n  else:\r\n    data = data.values if data.__class__.__name__ == 'DataFrame' else data\r\n    data = [data]\r\n```\r\n\r\nspecifically in the \r\n```  elif isinstance(data, (list, tuple)):\r\n    if isinstance(data[0], (list, tuple)):\r\n      data = [np.asarray(d) for d in data]\r\n    elif len(names) == 1 and isinstance(data[0], (float, int)):\r\n      data = [np.asarray(data)]\r\n    else:\r\n      data = [\r\n          x.values if x.__class__.__name__ == 'DataFrame' else x for x in data\r\n      ]\r\n```\r\nsection. My data input was a list of lists. So - I got into this block and the code essentially reformatted my list of lists into a list of numpy arrays. BUT - I still had the same shape. The other conditions, e.g `  else:\r\n    data = data.values if data.__class__.__name__ == 'DataFrame' else data\r\n    data = [data]` (which happens if the inbound data is a numpy array) actually adds the dimension to data that is needed to get through the `.zip` invocation with the original input shape. \r\n\r\nSo the correct fix isn't the fix I cited in the comment above, but to add it in the block cited in this comment as shown:\r\n```  elif isinstance(data, (list, tuple)):\r\n    if isinstance(data[0], (list, tuple)):\r\n      data = [np.asarray(d) for d in data]\r\n      data = np.expand_dims(data, 0)  # <=================== add this line here SQ\r\n    elif len(names) == 1 and isinstance(data[0], (float, int)):\r\n      data = [np.asarray(data)]\r\n    else:\r\n      data = [\r\n          x.values if x.__class__.__name__ == 'DataFrame' else x for x in data\r\n      ]\r\n```", "@jvishnuvardhan what would I need to do to take on this issue and do the fix?", "@scooter4j If you have a solution that works well, then fork tensorflow repository on your GitHub, update the codes in your folder, test it. If it is working as expected, create a pull-Request (PR). All PR's are reviewed by TF Code-owners and if it passes all the tests then your code will be merged into TF repository.  Please let me know if anything is not clear to you. Thanks!", "@jvishnuvardhan ok. I'm pretty confident that the fix shown in my last comment with code is what's needed. I'll try to get to that this week. \r\n\r\nThe one question I have is: how do I run unit tests for GPU if I don't have a GPU available? Even the GPU-based Dockerfile wouldn't emulate a GPU on my normal CPU system, right?", "@scooter4j Good to know that you want to contribute. You could use GPU from the [Google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb). It is free for public. In the `Runtime` dropdown menu in Google colab, click `change runtime type` and select \"GPU\" for `Hardware accelerator`. Thanks.!", "I could use Colab for manual validation - not sure how to run your existing suite of automated tests against my change on GPU. That's ok - maybe it'll become clear to me when I get in there. Plus I suppose I'd see if I broke any tests when I create a PR, and could adjust.", "@scooter4j Have you created a PR? If yes, can share it here so that we can close when the PR merge. Thanks!", "Any update on this? I got exactly the same Issue. If @scooter4j got this error:\r\n```\r\nValueError: Error when checking input: expected time_distributed_input to have 5 dimensions, but got array with shape (4, 28, 28, 1)\r\n```\r\n\r\nI got this one:\r\n```\r\nValueError: Error when checking input: expected input_image to have 5 dimensions, but got array with shape (8, 416, 416, 3)\r\n```\r\nWhich is the same but with different image dimensions and batch size", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@scooter4j Is this resolved? Did you raise any PR? Please close if the issue was resolved already. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28323\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28323\">No</a>\n"]}]