[{"number": 33530, "title": "improve GPU device info output", "body": "`memoryClockRate(GHz)` is core clock rate actually.\r\n\r\nThis problem has existed for several years.", "comments": []}, {"number": 33529, "title": "Improve math_ops.py docs", "body": "Added usage examples to the tf.math.cumsum() docstring. What exists currently is syntax-like declarations and not  `self contained` code samples.", "comments": ["This is guide you can use https://www.tensorflow.org/community/contribute/docs_ref", "@yashk2810 changes implemented as requested\r\n", "@steph-en-m looks like the builds are failing. \r\n\r\nCan you please take a look and update the doctests accordingly?\r\n\r\nhttps://source.cloud.google.com/results/invocations/753eb8c2-b257-406e-9827-42d0f09d712c/targets/%2F%2Ftensorflow%2Ftools%2Fdocs:tf_doctest/tests", "@steph-en-m Can you please check build failures? Thanks!", "> @steph-en-m Can you please check build failures? Thanks!\r\n\r\nthe tests pass locally but I think i'll have to clear my commit logs probably. https://github.com/tensorflow/tensorflow/pull/34014"]}, {"number": 33528, "title": "Broken Link", "body": "Hi\r\n\r\nFollowing URL seems to be down:\r\n\r\nhttps://mirror.bazel.build/github.com/pybind/pybind11/archive/v2.3.0.tar.gz\r\n\r\nBazel uses that to download pybind11 and for some unknown reason fallback link (https://github.com/pybind/pybind11/archive/v2.3.0.tar.gz) is never being used as an alternative. (Is that a fallback btw?)\r\n\r\nBuild logs:\r\n\r\n```\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (192 packages loaded, 2655 targets configured)                              \r\nINFO: Call stack for the definition of repository 'pybind11' which is a tf_http_archive (rule definition at /root/tensorflow-1.15.0/third_party/repo.bzl:124:19):                                                                                                                             \r\n - /root/tensorflow-1.15.0/tensorflow/workspace.bzl:925:5                                                                                      \r\n - /root/tensorflow-1.15.0/WORKSPACE:19:1                                                                                                      \r\nERROR: An error occurred during the fetch of repository 'pybind11':                                                                            \r\n   java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/pybind/pybind11/archive/v2.3.0.tar.gz, https://github.com/pybind/pybind11/archive/v2.3.0.tar.gz] to /root/.cache/bazel/_bazel_root/a0cf5ef42c8f00571631f8815d38246b/external/pybind11/v2.3.0.tar.gz: All mirrors are down: [GET returned 404 Not Found, connect timed out]       \r\n```                                                                           \r\n\r\nAny workaround? \r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 19.10\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.7\r\n- Bazel version: 0.26.1", "comments": ["It's not only me:\r\nhttp://www.site24x7.com/public/t/results-1571487777422.html", "There is also this one:\r\n\r\n```\r\n$ curl https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-py/archive/pypi-v0.7.1.tar.gz\r\n\r\n<?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Details>No such object: mirror.tensorflow.org/github.com/abseil/abseil-py/archive/pypi-v0.7.1.tar.gz</Details></Error>% \r\n```\r\nIs there something fundamentally broken about mirrors?", "Both should be fixed now.", "Oh, use http for mirror, not https. There is an issue with the SSL certificate that I haven't yet managed to work out. But since we already store checksums in workspace and that comes from GitHub via ssh/https there is not a high priority in fixing SSL for the mirror.", "Nope. Still broken:\r\n\r\nhttp://storage.googleapis.com/mirror.tensorflow.org/github.com/keras-team/keras-applications/archive/1.0.8.tar.gz\r\n", "Did you mean: \r\n`sed -i 's/https:\\/\\/storage.googleapis.com\\/mirror.tensorflow.org/http:\\/\\/storage.googleapis.com\\/mirror.tensorflow.org/g' tensorflow/workspace.bzl`\r\n?\r\n\r\nDIdn't help either...", "I see both keras-applications 1.0.8 and abseil-py 0.7.1 at http://mirror.tensorflow.org/", "And clicking on the link in https://github.com/tensorflow/tensorflow/issues/33528#issuecomment-544151665 gives me the keras tarball to download", "Now all links seem to work. I am not sure what happened:\r\n```\r\n$ curl https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-py/archive/pypi-v0.7.1.tar.gz --output /tmp/pypi-v0.7.1.tar.gz > /dev/null 2>&1\r\n$ sha256sum /tmp/pypi-v0.7.1.tar.gz\r\n3d0f39e0920379ff1393de04b573bca3484d82a5f8b939e9e83b20b6106c9bbe  /tmp/pypi-v0.7.1.tar.gz\r\n```", "Maybe some cache on the way? Glad to know it's solved in any case. Should we close?", "It was a docker build, and no cache that I am aware of was involved... \r\nYes please close the issue. Thank you."]}, {"number": 33527, "title": "Custom tf.keras.Model memory footprint drastically affects performance", "body": "\r\n**System information**\r\n16.04.6 LTS\r\nPython 3.5.2\r\nGPU TITAN X (Pascal) 13GB\r\n\r\n**Describe the current behavior**\r\n\r\nThe memory footprint of a custom tf.keras.Model object affects training performance by almost two orders of magnitude.\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe following is a stripped-down implementation of an RNN for text data loosely resembling the one in the [Effective Tensorflow 2.0 Tutorial](https://www.tensorflow.org/guide/effective_tf2)\r\n\r\n```ruby\r\nclass DynamicRNN(tf.keras.Model): \r\n\r\n  def __init__(self, state_size, embedding_size, vocab_size, something):\r\n    super(DynamicRNN, self).__init__(self)\r\n\r\n    self.state_size = state_size\r\n    self.embedding_size = embedding_size\r\n    self.vocab_size = vocab_size\r\n    self.something = something\r\n\r\n    # The internal GRU\r\n    self.cell = tf.keras.layers.GRUCell(state_size)\r\n    \r\n    # Input embedding and softmax parameters\r\n    emb_initializer = tf.initializers.RandomUniform(-0.5, 0.5)\r\n    self.emb = tf.Variable(emb_initializer(shape=[self.vocab_size, self.embedding_size]), name=\"emb\")\r\n    softmax_w_initializer = tf.initializers.GlorotUniform()\r\n    self.softmax_w = tf.Variable(softmax_w_initializer(shape=[self.vocab_size, self.state_size]), name=\"softmax_w\")\r\n    self.softmax_b = tf.Variable(tf.zeros([self.vocab_size]), name=\"softmax_b\")\r\n\r\n\r\n  @tf.function\r\n  def _step(self, last_state, last_symbol):\r\n    # Embed last symbol\r\n    last_symbol_emb = tf.nn.embedding_lookup(self.emb, last_symbol)\r\n    \r\n    # Compute new state\r\n    _, state = self.cell(last_symbol_emb, [last_state], training=False)\r\n    state = state[0]\r\n \r\n    # Predict new symbol\r\n    logits = tf.matmul(state, self.softmax_w, transpose_b=True) + self.softmax_b\r\n\r\n    return state, logits\r\n  \r\n\r\n  @tf.function\r\n  def __call__(self, inputs):\r\n    batch_size, L = tf.shape(inputs)[0], tf.shape(inputs)[1]\r\n\r\n    last_state = self.cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\r\n    last_symbol = tf.fill([batch_size], 0)\r\n\r\n    # One loss term per time-step\r\n    total_loss = 0.0\r\n    for i in tf.range(L):\r\n      last_state, logits = self._step(last_state, last_symbol)\r\n      last_symbol = inputs[:, i]\r\n  \r\n      batch_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=inputs[:,i])\r\n      total_loss += tf.reduce_mean(batch_loss)\r\n      \r\n    total_loss /= tf.cast(L, tf.float32)\r\n\r\n    return total_loss\r\n```\r\n\r\nTo rule out one culprit, the `TensorArray`s have been removed. Clearly, an efficient model would not perform embedding and loss computation in every step but that's not relevant here.\r\n\r\nTo perform training I use a standard tensorflow 2.0 loop (with dummy input for simplicity)\r\n\r\n```python\r\n# Parameters\r\nbatch_size = 64\r\nvocab_size = 30000\r\nseq_length = 28\r\n\r\n# Model\r\nsomething = {str(i) : i for i in range(400000)}\r\nrnn = DynamicRNN(512, 100, vocab_size, something)\r\n# Optimizer\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipvalue=0.5)\r\n\r\n# Dummy input\r\nseq = tf.ones([batch_size, seq_length], dtype=tf.int32)\r\n\r\n# Training loop\r\nfor step in tqdm.tqdm(range(100000)):\r\n  with tf.GradientTape() as tape:\r\n    loss = rnn(seq)\r\n  grads = tape.gradient(loss, rnn.trainable_variables)\r\n  optimizer.apply_gradients(zip(grads, rnn.trainable_variables))\r\n```\r\n\r\nThe crucial part here is the `something` object which is passed to the model. Note that the object is never used inside the model, hence we can also pass `None` instead.\r\n\r\nMy point is that the training performance on GPU and CPU is significantly degraded when the relatively memory costly  `something` object is passed instead of `None`. Precisely, I get a slowdown from 0.13s/it to 7.7s/it on the GPU and 2s/it to 9s/it on the CPU. Utilization drops from ~65% to %5 on the GPU and from 800% to 300% (24 cores available) on the cpu.\r\n\r\nThe above makes me suspicious that the model is being copied in *every* training step. I found this behavior nowhere documented. Is it? Also note that such an `something` dictionary is entirely realistic if what you do is NLP and you need to carry your vocabulary around. If that object is being used internally, a more efficient alternative might be to pass it as a function argument, instead of storing it internally.", "comments": ["@schmiflo,\r\nTried reproducing your code, but it is resulting in the error, `NameError: name 'tqdm' is not defined`. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/e91992c2896e8b6338adb28409d0de8e/33527_cpu.ipynb#scrollTo=rxpoE5BiiWQn). \r\n\r\nCan you please help us reproduce the issue. Thanks!", "@rmothukuru \r\n\r\nSorry, I forgot to add imports. Either add\r\n```python \r\nimport tensorflow as tf\r\nimport tqdm\r\n```\r\nif you have tqdm installed (`pip3 install --user tqdm`) or turn `tqdm.tqdm(range(100000))` into `range(100000)` but then you have to measure time per iteration in some other way.", "Could reproduce the issue with TF Version 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/27101d39d90aa55bd05b33d140dcaac4/33527_cpu.ipynb). \r\n\r\nObserved that more time is taken for `iteration/second` when used `rnn = DynamicRNN(512, 100, vocab_size, something)`, rather than `rnn = DynamicRNN(512, 100, vocab_size, None)`.", "Thanks for reporting the issue and sorry for the very late reply. Let me explain in details.\r\n\r\n1. In tensorflow, there is a concept for trackable item, which we use to track dependency between python objects, and we use this dependency graph to determine which part of the execution graph we need to save when we do model checkpointing and saving.\r\n\r\n2. For keras subclass model, like the DynamicRNN class in your example, we by default track any fields that is created by user as the attribute, eg user can do:\r\n```\r\nself.variable1 = tf.Variable()\r\n```\r\nIf we don't track those information, we won't be able to save the model correctly.\r\n\r\n3. When tracking attributes, we also check the type of value, and see if it worth tracking. Eg, tensors, and variables are key concepts which we want to track. On the other hand, int, string and primitive types  that are not mutable is not in the scope. For mutable container like list, dict, since we don't know whether user will add anything to it during training, we by default track them. And this is causing the slowdown in your example.\r\n\r\n4. To test my theory, you can do\r\n``` \r\n\r\nfrom tensorflow.python.training.tracking import data_structures\r\n....\r\n    self.something = data_structures.NoDependency(something)\r\n```\r\n\r\nThis will force tensorflow to not track the object, and bring the performance back to normal. Also you can change to `something = tuple(str(i) for i in range(400000))` and see the performance (tuple is immutable and we don't track them as long as it doesn't contain tensor or variable).\r\n\r\nHope this resolve your confusion. I am closing this bug for now, feel free to reopen it if u feel there is anything else need to be addressed here."]}, {"number": 33526, "title": "Error while trying to use tf.broadcast_weights ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Google Colab)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Google Colab\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.x\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: - \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nUnable to import tf.broadcast_weights in TF 2.0. \r\n\r\n**Describe the expected behavior**\r\nShould be able to import tf.broadcast_weights in TF 2.0\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nMethod 1: Plain python + TF 2.0\r\n```\r\nimport tensorflow as tf     # version 2.0\r\ntf.broadcast_weights\r\n```\r\n_throws_ ** AttributeError: module 'tensorflow' has no attribute 'broadcast_weights'**\r\n\r\nMethod 2: Codelab\r\nI found this error in a recent TF 2.0 + Keras tutorial - https://colab.sandbox.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO\r\n\r\n- Search for \"broadcast_weights\" in this codelab. \r\n- Run all cells before this.\r\n- Modify code \"m.update_state([0, 1, 1, 1], [0, 1, 0, 0])\" to \"m.update_state([0, 1, 1, 1], [0, 1, 0, 0]), sample_weight=[0.1,0.2,0.3,0.4]\"\r\n- Run this cell\r\n- throws AttributeError: module 'tensorflow' has no attribute 'broadcast_weights'\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have tried on colab with TF version 2.0  and was able to reproduce the issue. Please, find the gist here.[Method1](https://colab.sandbox.google.com/gist/ravikyram/a2a526b2e1d95d7a70d6361efdc04da9/tensorflow-2-0-keras-crash-course.ipynb) and [Method2](https://colab.sandbox.google.com/gist/ravikyram/dc8967174fec9d63a6630c2bff684c10/untitled288.ipynb).Thanks!", "@MeghnaNatraj Thanks for finding the bug in the doc. Could you point us to the source of the notebook. Where did you find this notebook? Thanks!", "@jvishnuvardhan I found this issue while trying out a recent tutorial published by Fran\u00e7ois Chollet\r\nhttp://link.oreilly.com/P000Q3rFWM0iPY010sCSs30\r\n(Original Link: https://colab.sandbox.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO)\r\nSearch for \"tf.broadcast_weights\" in this tutorial.\r\n\r\nUsage 2:\r\nThis API is also used in the TF website here: https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric\r\n\r\nDefinition:\r\nhttps://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/python/ops/weights_broadcast_ops.py#L136\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@MeghnaNatraj Thanks for the links. First link is already updated by the author. We will work on updating link in the `Usage 2`. Thanks!", "@MarkDaoust I think we need to update [this page](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric). The issue is in this line \r\n\r\n`sample_weight = tf.broadcast_weights(sample_weight, values)`\r\n\r\nwhere `tf.broadcast_weights` throws `AttributeError: module 'tensorflow' has no attribute 'broadcast_weights'`. \r\n\r\nI don't think this tf.broadcast_weights is exposed to the public. Thank you!", "@MarkDaoust I confirmed all the codes of the tf.broadcast_weights in https://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/python/ops/weights_broadcast_ops.py#L136,\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/python/ops/metrics.py#L23,\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/python/ops/metrics_impl.py#L35\r\n\r\nAnd these all seem fine. But still, tf.broadcast_weights throughs Attribute error. ", "tf.broadcast_weights does not work and is still referenced here (https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric).", "The issue is that `tf.broadcast_weights` is not exposed through `tf_export`, wondering if it makes sense to expose this API given it is referenced in multiple places? \r\n\r\n/cc @tensorflow/api-owners ", "I think we need to remove the reference to tf.broadcast_weights from the documentation. ", "I take a look at the usage of `tf.broadcast_weights` in docstring. The related docstring is just an example,  and `broadcast_weights` by itself is just to make sure it follow a subset of broadcast rules than `tf.multiply`. It actually makes sense to remove this line as it does not add much to be an example in docs:\r\n    \r\n```diff\r\n         sample_weight = tf.cast(sample_weight, self.dtype)\r\n-        sample_weight = tf.broadcast_weights(sample_weight, values)\r\n         values = tf.multiply(values, sample_weight)\r\n```\r\n\r\nCreated a PR #39161 for the fix."]}, {"number": 33525, "title": "how can tf.image.crop_and_resize receive a tensor as crop box? ", "body": "### I have a proble with tf.image.crop_and_resize()\r\nI have two tensor now, one with a dimension of (4,4,1), the other with a dimension of (1732,1732,3). I tried to get the index of maximum of the first tensor. and use the index multiply 433 as an index to crop the other tensor. But when I use tf.where to get the index, it returns a tensor, the tf.image.crop_and_resize just receive float32 as the box, I use keras to build a model, how can I transform the tensor index into value without run a session. \r\n\r\n\r\n\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 33524, "title": "Suspected fix for progressive slowdown when training", "body": "When training on multiple GPUs, with a validation dataset, I have been seeing a significant reduction in training speed after each epoch. When validation epochs are sufficiently frequent, this is very detremental. This does not present when running on a single GPU with the validation set, or when running on multiple GPUs without the validation set.\r\n\r\nThe graph below shows a toy example to exascerbate the problem for illustration, but this occurs in a much larger network that was taking around a week to train on a single GPU. After 24 hours, with a high validation frequency, the system had effectively stopped training. I've spent several days logging times and plotting graphs on different hardware, operating systems, versions of tensorflow and with different networks and datasets and this is what I have narrowed it down to. It also appears to have an exponential nature, initially it can look like quite a steady increase in training time, but some of the tests that I ran showed, a rapid increase in time beyond a certain point.\r\n\r\n![image](https://user-images.githubusercontent.com/13148657/67134089-1a838d80-f208-11e9-9540-f47cd142a852.png)\r\n\r\nHere's another graph. This shows Ubuntu with 4xGPUs on Amazon, 1 GPU on a local Windows machine and 2 GPUs on a local WIndows machine. Running TF 1.14 installed from pip. The issue is present in TF 1.15 too.\r\n\r\n![image](https://user-images.githubusercontent.com/13148657/67134473-36882e80-f20a-11e9-9f73-e00c0962b1b0.png)\r\n \r\n\r\nIt appears to me, looking through the code that the condition around the iterator reset for the validation set should be the same as the one for the training data set further down. Though I am not 100% sure if this is this.\r\n\r\n\r\nScript for reproducing the issue:\r\n```\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense\r\nimport tensorflow.keras.losses as losses\r\nfrom tensorflow.keras.optimizers import Adam\r\nimport numpy as np\r\n\r\ndef CreateModel():\r\n  model = Sequential()\r\n  model.add(Conv2D(filters=32, kernel_size=3, name='images', activation='relu', kernel_initializer='glorot_normal', input_shape=(39, 39, 3)))\r\n  model.add(Flatten())\r\n  model.add(Dense(10, bias_initializer='zeros', kernel_initializer='glorot_normal'))\r\n\r\n  return model\r\n\r\ndef generatorFunction():\r\n  while 1:\r\n    s = (39, 39, 3)\r\n    z = np.zeros(s, dtype=np.float64)\r\n\r\n    zo = np.zeros(10, dtype=np.float64)\r\n    yield z, zo\r\n\r\ndef RunScript():\r\n  # Setup session options\r\n  config = tf.ConfigProto()\r\n\r\n  if tf.test.is_gpu_available():\r\n    config.gpu_options.allow_growth = True\r\n\r\n  session = tf.Session(config=config)\r\n  session.run(tf.global_variables_initializer())\r\n  K.set_session(session)\r\n\r\n  # Setup strategy\r\n  cross_device_ops = tf.distribute.ReductionToOneDevice(reduce_to_device='/device:CPU:0')\r\n  mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=cross_device_ops)\r\n\r\n  # Setup data\r\n  trainingDataPrepped = tf.data.Dataset.from_generator(\r\n    generatorFunction,\r\n    output_types=(tf.float32, tf.float32),\r\n    output_shapes=((39,39,3), (10))).batch(32)\r\n\r\n  vDataPrepped = tf.data.Dataset.from_generator(\r\n    generatorFunction,\r\n    output_types=(tf.float32, tf.float32),\r\n    output_shapes=((39,39,3), (10))).batch(32)\r\n\r\n  # Create and train model\r\n  with mirrored_strategy.scope():\r\n    model = CreateModel()\r\n    lossType = losses.mean_squared_error\r\n    model.compile(optimizer=Adam(), loss=lossType)\r\n\r\n\r\n    model.fit(trainingDataPrepped,\r\n              validation_data=vDataPrepped,\r\n              validation_steps=1,\r\n              epochs=3000,\r\n              shuffle=True,\r\n              steps_per_epoch=1)\r\n\r\nif __name__ == \"__main__\":\r\n  RunScript()\r\n\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33524) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33524) for more info**.\n\n<!-- ok -->", "Note: I don't think this fully explains the slowdown that I am seeing, but I still think that the code fix in this PR is correct.", "@Loca123 Could you please check failed build errors? Thanks!", "> @Loca123 Could you please check failed build errors? Thanks!\r\n\r\nI can see why a validation loss might change, might require an update to the test case. I'm not sure why the Android one failed though, it said it was a 404 error getting data. I just synced my branch to the master but it doesn't seem to be running the same tests that it was doing before.\r\n", "@omalleyt12  Can you please take a look on this PR? Thanks!", "Looking at this again, shouldn't the validation iterator be reset every epoch? The validation data should always be fully consumed (as opposed to the training data, which can be partially consumed each epoch)", "@Loca123 Could you please check reviewer comments and keep us posted. Thanks!", "@gbaned @omalleyt12 : You may very well be right regarding it needing to be reset. Why this leads to a progressive slowdown I do not know. \r\n\r\nHowever this ultimately did not resolve my original slowdown that is shown in the second graph above. I was only able to resolve that by moving to TF2, there was something in TF1.11 and greater that was causing a severe progressive slowdown, but I could not determine where it was coming from, beyond it being related to the checkpoint callback. Even a very simple script demonstrated it. I believe the above script with a checkpoint callback added would show it.\r\n\r\nAs for this PR, it might be as well to close it as overall I don't know if the proposed change is correct and has only a little effect on the original reported problem.\r\n\r\nRegards,\r\n\r\nCarl", "@omalleyt12 Can you please take a look on this PR? Thanks!", "@Loca123 Could you please check reviewer comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 33523, "title": "Update sqlite to 3.30.1", "body": "This PR updates sqlite library to 3.30.1 (from 3.28.0, which was more than 6 months old)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thank you", "@yongtang Could you please address Ubuntu Sanity errors? Thanks!", "Sanity log is internal error, will be fixed soon and I'll come back here to retrigger merge"]}, {"number": 33522, "title": "Refactor {Repeat, Shard, Skip, TextLine, TFRecord}DatasetOpTest", "body": "This PR refactors `RepeatDatasetOpTest`, `ShardDatasetOpTest`, `SkipDatasetOpTest`, `TextLineDatasetOpTest`, and `TFRecordDatasetOpTest`.", "comments": ["> The changes look good to me, thanks for the improvements!\r\n> \r\n> Please update the description since the PR is updating 5 dataset tests, not just Repeat and Shared\r\n\r\n@aaudiber Thanks for the review! The title and description are updated now."]}, {"number": 33521, "title": "Update CONTRIBUTING.md", "body": "", "comments": []}, {"number": 33520, "title": "Remove unnecessary `else` clauses", "body": "", "comments": ["We will not be encouraging one liner changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac @chanshah", "Ideally, you would be doing this for all occurrences of this pattern, at least in this file.\r\n\r\nGiven that we spend many CPU/GPU hours on every CL, doing so for just one minor fix at a time is too wasteful."]}, {"number": 33519, "title": "TensorFlow models example hangs after changing to \"bazel build --config=v2\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): 0.26.0\r\n- GCC/Compiler version (if compiling from source): g++ 5.4.0 20160609\r\n- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7.5.1.10-1+cuda10.0 (+ additional CUDA 10.1 runtime needed for NCCL libs)\r\n- GPU model and memory: NVIDIA Tesla K40m\r\n\r\n**Describe the current behavior**\r\nObserved within [Mellanox TensorFlow CI](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build#mellanox-tensorflow-ci) after https://github.com/tensorflow/tensorflow/commit/2a8e8706a423a141bbdb7a8f99276c8aff172b4f commit.\r\nWe run TensorFlow models example on a 2-node cluster (1 GPU per a host): https://github.com/tensorflow/models/blob/master/official/r1/resnet/imagenet_test.py\r\n```\r\npython imagenet_main.py \\\r\n    --num_gpus=1 \\\r\n    --use_synthetic_data \\\r\n    --train_epochs=1 \\\r\n    --max_train_steps=100 \\\r\n    --use_train_and_evaluate=True \\\r\n    --all_reduce_alg=nccl \\\r\n    --worker_hosts \"host1:12346,host2:12346\" \\\r\n    --task_index ${TASK_INDEX} \\\r\n    --distribution_strategy=multi_worker_mirrored \\\r\n    --clean\r\n```\r\n**NOTE**: Custom version of NCCL is used. If needed I can try to reproduce the issue with the public NCCL version.\r\nTensorFlow build scenario:\r\n```\r\n$ cat ./.tf_configure.bazelrc\r\nbuild --host_force_python=PY2\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_CUDA_VERSION=\"10.0\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"2.4.7\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"<localpath>/cuda10.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr\"\r\nbuild --action_env NCCL_INSTALL_PATH=\"<localpath>\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"3.5\"\r\nbuild --action_env LD_LIBRARY_PATH=\"<localpath>\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-no_gpu\r\ntest --build_tag_filters=-no_gpu\r\ntest --test_env=LD_LIBRARY_PATH\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n$ ./configure\r\n$ bazel build --config=opt --config=cuda --config=v2 //tensorflow/tools/pip_package:build_pip_package\r\n$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg_$$\r\n```\r\n**NOTE**: the scenario works fine with `bazel build ... --config=v1 ...`\r\n\r\n**Describe the expected behavior**\r\nMentioned test scenario should not hang.\r\n\r\n**Code to reproduce the issue**\r\nSee above.\r\n\r\n**Other info / logs**\r\nLog files from the hanging test run (`bazel build ... --config=v2 ...`):\r\n[run_tf_models_imagenet_host1_bad.log](https://github.com/tensorflow/tensorflow/files/3745290/run_tf_models_imagenet_host1_bad.log)\r\n[run_tf_models_imagenet_host2_bad.log](https://github.com/tensorflow/tensorflow/files/3745292/run_tf_models_imagenet_host2_bad.log)\r\nLog files from the working test run (`bazel build ... --config=v1 ...`):\r\n[run_tf_models_imagenet_host1_good.log](https://github.com/tensorflow/tensorflow/files/3745291/run_tf_models_imagenet_host1_good.log)\r\n[run_tf_models_imagenet_host2_good.log](https://github.com/tensorflow/tensorflow/files/3745293/run_tf_models_imagenet_host2_good.log)\r\n", "comments": ["The issue is not reproducible anymore (e.g. no hanging with one of recent TensorFlow commits 95051b912c0b00f9cde62aada17687151944eb0d).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33519\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33519\">No</a>\n"]}, {"number": 33518, "title": "Fix micro SVDF unit test for hybrid-quantized scratch tensor dims.", "body": "This patch fixes incorrect logic setup for hybrid-quant tests of SVDF\r\nfor TF Micro. The previous logic incorrectly set the hybrid-quant\r\nscratch input tensor to have the wrong dimensions. This patch fixes that\r\nproblem and cleans up variable names in the test.", "comments": []}, {"number": 33517, "title": "[TF 2.0] Using keras.metrics in TPU training results in error", "body": "I am trying to train a BERT model from https://github.com/tensorflow/models/tree/master/official/nlp on TPU in google colab. I changed the metrics list passed to model in compile method to:\r\n```\r\nbert_model.compile(optimizer=optimizer, loss=loss_fn, metrics=get_metrics())\r\n```\r\nwhere _get_metrics_ is a function which returns a list of metrics (\"accuracy\" and instance for Recall and Precision built in _tensorflow.keras.metrics_):\r\n\r\n```\r\nfrom tensorflow.keras.metrics import Recall, Precision\r\n\r\ndef get_metrics():\r\n    return [\"accuracy\",\r\n            Recall(),\r\n            Precision(), ]\r\n```\r\n\r\nTraining results in the following error (after one epoch ends, before validation statistics are displayed):\r\n\r\n```\r\nI1018 16:34:07.313311 140541208393600 remote.py:151] Entering into master device scope: /job:worker/replica:0/task:0\r\n2019-10-18 16:34:07.359467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-10-18 16:34:07.465723: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2019-10-18 16:34:07.465842: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (7b6f1b4d4089): /proc/driver/nvidia/version does not exist\r\n2019-10-18 16:34:07.466260: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-18 16:34:07.472748: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-10-18 16:34:07.473076: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3172f40 executing computations on platform Host. Devices:\r\n2019-10-18 16:34:07.473114: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-10-18 16:34:07.475920: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> 10.29.203.98:8470}\r\n2019-10-18 16:34:07.475955: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30501}\r\n2019-10-18 16:34:07.476742: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:30501\r\n2019-10-18 16:34:07.497844: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> 10.29.203.98:8470}\r\n2019-10-18 16:34:07.497905: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30501}\r\nINFO:tensorflow:Initializing the TPU system: 10.29.203.98:8470\r\nI1018 16:34:07.499603 140541208393600 tpu_strategy_util.py:70] Initializing the TPU system: 10.29.203.98:8470\r\nINFO:tensorflow:Clearing out eager caches\r\nI1018 16:34:15.119202 140541208393600 tpu_strategy_util.py:94] Clearing out eager caches\r\nINFO:tensorflow:Finished initializing TPU system.\r\nI1018 16:34:15.121769 140541208393600 tpu_strategy_util.py:114] Finished initializing TPU system.\r\nINFO:tensorflow:Found TPU system:\r\nI1018 16:34:15.128222 140541208393600 tpu_system_metadata.py:148] Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nI1018 16:34:15.128440 140541208393600 tpu_system_metadata.py:149] *** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nI1018 16:34:15.129121 140541208393600 tpu_system_metadata.py:150] *** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nI1018 16:34:15.129209 140541208393600 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nI1018 16:34:15.129295 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nI1018 16:34:15.129720 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nI1018 16:34:15.129811 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nI1018 16:34:15.129892 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nI1018 16:34:15.129969 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nI1018 16:34:15.130045 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nI1018 16:34:15.130121 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nI1018 16:34:15.130197 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nI1018 16:34:15.130281 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nI1018 16:34:15.130358 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nI1018 16:34:15.130436 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nI1018 16:34:15.130511 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nI1018 16:34:15.130593 140541208393600 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nI1018 16:34:15.248266 140541208393600 train.py:212] Training using TF 2.0 Keras compile/fit API with distrubuted strategy.\r\nWARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\r\nW1018 16:35:33.236943 140541208393600 training_utils.py:1547] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\r\nTrain on 129 steps, validate on 65 steps\r\nEpoch 1/5\r\n2019-10-18 16:38:03.018892: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\r\n2019-10-18 16:38:03.020371: E tensorflow/core/platform/default/device_tracer.cc:70] CUDA error: CUDA_ERROR_NO_DEVICE\r\n  1/129 [..............................] - ETA: 5:12:28 - loss: 1.0083 - accuracy: 0.2031 - recall: 0.1719 - precision: 0.2619WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.610206). Check your callbacks.\r\nW1018 16:38:06.456197 140541208393600 callbacks.py:244] Method (on_train_batch_end) is slow compared to the batch update (1.610206). Check your callbacks.\r\n128/129 [============================>.] - ETA: 1s - loss: 0.5022 - accuracy: 0.7563 - recall: 0.5862 - precision: 0.81392019-10-18 16:38:45.271991: E tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:50] Unable to destroy remote tensor handles: Unable to find the relevant tensor remote_handle: Op ID: 55877, Output num: 0\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571416725.271891392\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 55877, Output num: 0\",\"grpc_status\":3}\r\n2019-10-18 16:38:45.272429: E tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:50] Unable to destroy remote tensor handles: Unable to find the relevant tensor remote_handle: Op ID: 55877, Output num: 1\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571416725.272350919\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 55877, Output num: 1\",\"grpc_status\":3}\r\n2019-10-18 16:38:45.272841: E tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:50] Unable to destroy remote tensor handles: Unable to find the relevant tensor remote_handle: Op ID: 55877, Output num: 2\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571416725.272756237\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 55877, Output num: 2\",\"grpc_status\":3}\r\n2019-10-18 16:38:45.273165: E tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:50] Unable to destroy remote tensor handles: Unable to find the relevant tensor remote_handle: Op ID: 55877, Output num: 3\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571416725.273105048\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 55877, Output num: 3\",\"grpc_status\":3}\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/gdrive/My Drive/DeepLearningBERT/nn/train.py\", line 340, in <module>\r\n    app.run(main)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/gdrive/My Drive/DeepLearningBERT/nn/train.py\", line 332, in main\r\n    run_bert(strategy, input_meta_data)\r\n  File \"/gdrive/My Drive/DeepLearningBERT/nn/train.py\", line 287, in run_bert\r\n    use_keras_compile_fit=FLAGS.use_keras_compile_fit)\r\n  File \"/gdrive/My Drive/DeepLearningBERT/nn/train.py\", line 226, in run_bert_classifier\r\n    custom_callbacks=None)\r\n  File \"/gdrive/My Drive/DeepLearningBERT/nn/train.py\", line 143, in run_keras_compile_fit\r\n    callbacks=custom_callbacks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 685, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 439, in model_iteration\r\n    steps_name='validation_steps')\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 299, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\", line 878, in execution_function\r\n    return [out.numpy() for out in distributed_function(input_fn)]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 526, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnimplementedError:  Compilation failure: Asked to propagate a dynamic dimension from hlo %tuple.5198 = (pred[], f32[4,2]{1,0}) tuple(pred[] %convert.5196, f32[4,2]{1,0} %add.5004), metadata={op_type=\"If\" op_name=\"metrics/precision/assert_greater_equal/Assert/AssertGuard\"}@{1}@0 to hlo %conditional.5209 = (pred[]) conditional(pred[] %convert.5196, (pred[], f32[4,2]{1,0}) %tuple.5198, (pred[], f32[4,2]{1,0}) %tuple.5198), true_computation=%metrics_precision_assert_greater_equal_Assert_AssertGuard_true_127733_const_0__.5199, false_computation=%metrics_precision_assert_greater_equal_Assert_AssertGuard_false_127734_const_0__.5204, metadata={op_type=\"If\" op_name=\"metrics/precision/assert_greater_equal/Assert/AssertGuard\"}, which is not implemented.\r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_6193329545322784681/_7}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571416725.270929013\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\" Compilation failure: Asked to propagate a dynamic dimension from hlo %tuple.5198 = (pred[], f32[4,2]{1,0}) tuple(pred[] %convert.5196, f32[4,2]{1,0} %add.5004), metadata={op_type=\"If\" op_name=\"metrics/precision/assert_greater_equal/Assert/AssertGuard\"}@{1}@0 to hlo %conditional.5209 = (pred[]) conditional(pred[] %convert.5196, (pred[], f32[4,2]{1,0}) %tuple.5198, (pred[], f32[4,2]{1,0}) %tuple.5198), true_computation=%metrics_precision_assert_greater_equal_Assert_AssertGuard_true_127733_const_0__.5199, false_computation=%metrics_precision_assert_greater_equal_Assert_AssertGuard_false_127734_const_0__.5204, metadata={op_type=\"If\" op_name=\"metrics/precision/assert_greater_equal/Assert/AssertGuard\"}, which is not implemented.\\n\\tTPU compilation failed\\n\\t [[{{node tpu_compile_succeeded_assert/_6193329545322784681/_7}}]]\",\"grpc_status\":12} [Op:__inference_distributed_function_127913]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n\r\n2019-10-18 16:38:53.401848: E tensorflow/core/distributed_runtime/rpc/eager/grpc_eager_client.cc:72] Remote EagerContext with id 6450803200035565614 does not seem to exist.\r\n```\r\n\r\nWith only \"accuracy\" returned it works well finishing all epochs. With custom metrics like:\r\n```\r\ndef precision(y_true, y_pred):\r\n    y_pred = tf.math.rint(y_pred)\r\n    TP = tf.math.reduce_sum(y_pred * y_true)\r\n    FP = tf.math.reduce_sum(y_pred * (1 - y_true))\r\n\r\n    _precision = tf.math.divide(TP, (TP + FP + eps))\r\n    return _precision\r\n```\r\nit works as well, but the values returned are not correct. I suppose this is happening because on the TPU there are X steps per loop computed and somehow (I didn't dig too much into it) messes up the output metric. I tried with builtin functions to verify the behavior but it resulted in the error previously mentioned.\r\n\r\nSnippet of the training call (the function is called _run_keras_compile_fit_ in the github link I provided and it can be found in _bert/run_classifier.py_ with almost none custom code added):\r\n```\r\n    with strategy.scope():\r\n        training_dataset = train_input_fn()\r\n        evaluation_dataset = eval_input_fn()\r\n        bert_model, sub_model = model_fn()\r\n        optimizer = bert_model.optimizer\r\n\r\n        if init_checkpoint:\r\n            checkpoint = tf.train.Checkpoint(model=sub_model)\r\n            checkpoint.restore(init_checkpoint).assert_existing_objects_matched()\r\n\r\n        bert_model.compile(optimizer=optimizer, loss=loss_fn, metrics=get_metrics())\r\n\r\n        summary_dir = os.path.join(model_dir, 'summaries')\r\n        summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)\r\n        checkpoint_path = os.path.join(model_dir, 'checkpoint')\r\n        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n            checkpoint_path, save_weights_only=True, save_best_only=True, mode='min')\r\n\r\n        if custom_callbacks is not None:\r\n            custom_callbacks += [summary_callback, checkpoint_callback]\r\n        else:\r\n            custom_callbacks = [summary_callback, checkpoint_callback]\r\n\r\n        bert_model.fit(\r\n            x=training_dataset,\r\n            validation_data=evaluation_dataset,\r\n            steps_per_epoch=steps_per_epoch,\r\n            epochs=epochs,\r\n            validation_steps=eval_steps,\r\n            callbacks=custom_callbacks)\r\n\r\n        return bert_model\r\n```\r\nIn colab I installed the stable release of tensorflow 2.0 as the nightly version doesn't work well with colab's TPU's for now. The keras metrics are supposed to work with TPUs or this is not yet a feature?", "comments": ["@georgealexandruvlad, As this is more related to TF models, please post this in [TF model repo](https://github.com/tensorflow/models/issues). Thanks!", "@gadagashwini The thing is that the model works with accuracy and it doesn't with other tf.keras.metrics. So the problem shouldn't be at the model's implementation if the only change is to add an additional metric to be evaluated. I implemented a simple example that still results in the error mentioned https://colab.research.google.com/drive/18okZncYBJOrd9AZxy4tCrHQ3O3s_tNsl.\r\n\r\nThe question is whether this is a bug or I am doing something wrong. It's very little documentation on TPUs and for other distributed strategies this case seems to be documented as working in tensorflow guide pages.", "@ymodak Could I have some insights about what was causing the problem? Is it a bug or the keras.metrics are not yet supported on TPUs? And if they are not supported is there any alternative to this?", "@gadagashwini could you give me some information regarding the issue? I would like to know whether there is a problem (I didn't really get a response) that can't be resolved in the next couple of days/weeks or should I search for other solutions on the matter (like using tf1.x).", "@georgealexandruvlad \r\n\r\nHi, sorry about the breakage. The internal version of this issue got routed to me yesterday and we should have a fix very soon today (at least on our nightly release).\r\n\r\nThe root cause is our compiler had trouble handling conditionals with dynamic shapes, which is introduced by \"Assert\" operation in Metric. \r\n\r\n@rxsang also added an option to disable the dynamic shapes behavior, IIRC you can enable that by setting strategy.experimental_enable_dynamic_batch_size = False", "@yunxing Thanks for the update! Looking forward to the next nightly release then.", "Hi, I am having a similar dynamic dimension issue when using OneHot:\r\n```\r\ntensorflow.python.framework.errors_impl.UnimplementedError: From /job:worker/replica:0/task:0:\r\nERROR   2019-10-30 11:01:09 +0100       master-replica-0                Compilation failure: Asked to propagate a dynamic dimension from hlo %select.513 = f32[12,305,215,56]{3,2,1,0} select(pred[12,305,215,56]{3,2,1,0} %compare.510, f32[12,305,215,56]{3,2,1,0} %broadcast.511, f32[12,305,215,56]{3,2,1,0} %broadcast.512), metadata={op_type=\"OneHot\" op_name=\"chargrid_encoder_2/chargrid_encoder_a0/one_hot/one_hot\"}@{}@0 to hlo %concatenate.529 = f32[12,305,215,58]{3,2,1,0} concatenate(f32[12,305,215,56]{3,2,1,0} %select.513, f32[12,305,215,2]{3,2,1,0} %convert.528), dimensions={3}, metadata={op_type=\"ConcatV2\" op_name=\"chargrid_encoder_2/chargrid_encoder_a0/concat\"}, which is not implemented.\r\n```\r\n\r\nIs that related, or should I open a separate issue? \ud83d\ude04 ", "Hi ahmadsalim@, we should have already fixed this issue a while ago. Which tf version are you using? ", "@yunxing Thanks for the response. I am using 1.14, should I upgrade to 2.0 instead? \ud83d\ude04 ", "Missed the notification. This should be fixed in nightly releases, do you have access to those ? I remember we also have 1.x nightly release which should also include the fix. cc @rxsang who is more familiar with this than me. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33517\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33517\">No</a>\n", "tf.keras.metrics are not working with TPUs on colab version 2.3\r\n\r\nhttps://colab.research.google.com/drive/1C09OUXP-7Es4KIthVA6daRcGq_bJKGe8#scrollTo=hbXc0o4p2W0a\r\n\r\nit looks like `tf.metrics.AUC` is causing the error.\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: 9 root error(s) found.\r\n  (0) Not found: {{function_node __inference_train_step_4729}} No proto found for key <<NO PROGRAM AS COMPILATION FAILED>>\r\n\t [[{{node TPUVariableReshard/reshard/_14520013377784163202/_28}}]]\r\n\t [[while/body/_1/while/Const_7/_337]]\r\n  (1) Not found: {{function_node __inference_train_step_4729}} No proto found for key <<NO PROGRAM AS COMPILATION FAILED>>\r\n\t [[{{node TPUVariableReshard/reshard/_2758051469797844661/_31}}]]\r\n  (2) Not found: {{function_node __inference_train_step_4729}} No proto found for key <<NO PROGRAM AS COMPILATION FAILED>>\r\n\t [[{{node TPUVariableReshard/reshard/_3997717623177238997/_19}}]]\r\n  (3) Not found: {{function_node __inference_train_step_4729}} No proto found for key <<NO PROGRAM AS COMPILATION FAILED>>\r\n\t [[{{node TPUVariableReshard/reshard/_14520013377784163202/_28}}]]\r\n\t [[while/loop_body_control/_50/_148]]\r\n  (4) Not found: {{function_node __inference_train_step_4729}} No proto found for key <<NO PROGRAM AS COMPILATION FAILED>>\r\n\t [[{{node TPUVariableReshard/reshard/_12310723377697630185/_10}}]]\r\n  (5) Not found: {{function_node __inference_train_step_4729}} No proto found for key <<NO PROGRAM AS COMPILATION FAILED>>\r\n\t [[{{node TPUVariableReshard/reshard/_13160019155312455749/_25}}]]\r\n  (6) Invalid argument: {{function_node __inference_train_step_4729}} Compilation failure: Incompatible shapes: [200,128] vs. [200,16384]\r\n\t [[{{node while/body/_1/while/LogicalAnd}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_18307140185544563773/_5]]\r\n  (7) Not found: {{function_node __inference_train_step_4729}} No proto found for key <<NO PROGRAM AS COMPILATION FAILED>>\r\n\t [[{{node TPUVariableReshard/reshard/_14520013377784163202/_28}}]]\r\n\t [[while/body/_1/while/Const_3/_281]]\r\n  (8) Not found: {{function_node __inference_train_step_4729}} No proto found for key <<NO PROGRAM AS COMPILATION FAILED>>\r\n\t [[{{node TPUVariableReshard/reshard/_14520013377784163202/_28}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```", "TPUs are not workong on Colab either.\r\n\r\ntensorflow: 2.4\r\n\r\nhttps://colab.research.google.com/drive/1slQKTzSOnE9U70QCQCoGJrcyXCdqRJwz#scrollTo=3Qz6XSPEDsyZ", "Could you try nightly? The fix yunxing@ mentioned may not be in TF2.4 release yet, it would be in TF2.5 and nightly.\r\n\r\n> Missed the notification. This should be fixed in nightly releases, do you have access to those ? I remember we also have 1.x nightly release which should also include the fix. cc @rxsang who is more familiar with this than me.\r\n\r\n", "Tried nightly, still experiencing the same issue, see my notebook on colab https://colab.research.google.com/drive/1PVBomGMDz5zbbCgSUTCKm6gIMQ2eG6V-?usp=sharing", "Anyone able to resolve the error? Am facing the same."]}, {"number": 33516, "title": "Dataset.map() with tf.data.experimental.AUTOTUNE runs out of memory when using batch size=1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.04 Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tried on Mobile\r\n- TensorFlow installed from (source or binary): BINARY\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): NO\r\n- GCC/Compiler version (if compiling from source): NO\r\n- CUDA/cuDNN version: CUDA 10.0 cuDNN 7.6\r\n- GPU model and memory: RTX2070 8GB\r\n\r\n**Describe the current behavior**\r\nI use Dataset.map to normalize images. When using tf.data.experimental.AUTOTUNE and BATCH_SIZE of 1, memory consumption grows up till the program is killed. The most intriguing part is that when setting the BATCH_SIZE to greater than 1, the program works correctly\r\n\r\nThis issue happens both with tensorflow 2.0.0 and tensorflow-gpu 2.0.0\r\n\r\n**Describe the expected behavior**\r\nCode should work for batch size of 1\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Flatten, Dense, Reshape\r\nfrom tensorflow.keras.losses import MeanSquaredError\r\n\r\n\r\n(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\r\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\r\n\r\n# Setting this here to True will break the code\r\nTOGGLE_ERROR = True\r\nif TOGGLE_ERROR:\r\n    BATCH_SIZE = 1\r\nelse:\r\n    BATCH_SIZE = 3\r\n\r\n\r\ndef map_function(train_image):\r\n    return (train_image - 127.5) / 127.5\r\n\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images)\r\ntrain_dataset = train_dataset.repeat()\r\ntrain_dataset = train_dataset.map(map_function, tf.data.experimental.AUTOTUNE)\r\ntrain_dataset = train_dataset.batch(BATCH_SIZE)\r\ntrain_dataset = train_dataset.prefetch(64)\r\n\r\n\r\nclass AutoEncoder(tf.keras.Model):\r\n    def __init__(self):\r\n        super(AutoEncoder, self).__init__()\r\n        self.flatten = Flatten()\r\n        self.dense_1 = Dense(128, activation=\"relu\")\r\n        self.dense_2 = Dense(784, activation=\"relu\")\r\n        self.reshape = Reshape((28, 28, 1))\r\n\r\n    @tf.function\r\n    def call(self, inputs):\r\n        flatten = self.flatten(inputs)\r\n        encoded = self.dense_1(flatten)\r\n        decoded = self.dense_2(encoded)\r\n        return self.reshape(decoded)\r\n\r\n\r\nauto_encoder = AutoEncoder()\r\nmse = MeanSquaredError()\r\noptimizer = tf.keras.optimizers.Adam(1e-5)\r\n\r\n\r\n@tf.function\r\ndef train_step(batch):\r\n    with tf.GradientTape() as tape:\r\n        auto_encoded = auto_encoder(batch)\r\n        loss = mse(batch, auto_encoded)\r\n\r\n    grads = tape.gradient(loss, auto_encoder.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, auto_encoder.trainable_variables))\r\n    return loss\r\n\r\n\r\nfor step, image_batch in enumerate(train_dataset):\r\n    loss = train_step(image_batch)\r\n    if step % 1000 == 0:\r\n        print(loss)\r\n```\r\n**Other info / logs**\r\nMight be related to [Issue #32052](https://github.com/tensorflow/tensorflow/issues/32052)", "comments": ["#### System Information\r\n\r\n-  Linux Ubuntu 18.04\r\n- TensorFlow installed from: pip install tensorflow-gpu\r\n- TensorFlow version: 2.0.0-beta1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): NO\r\n- GCC/Compiler version (if compiling from source): NO\r\n- CUDA/cuDNN version: CUDA 10.0 cuDNN 7.6\r\n- GPU model and memory: GTX 1080 Ti\r\n\r\n#### Describe the current behavior\r\nI found the same problem as @EduardoGRocha that for batch size 1 and number of parallel calls set to AUTOTUNE for map the memory consumption of the program rises until the program is killed.\r\n\r\nThis problem seems to occur due to an infinite number of calls to the map function.\r\n\r\nBelow you can find a minimal example.\r\n\r\n#### Code\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\ndata = np.random.normal(size=(200,32,32))\r\ntensor_slice_dataset = tf.data.Dataset.from_tensor_slices(data)\r\n\r\n######################## WORKING ########################\r\n\r\n# batch size equal to 1 works with any number\r\n# greater or equal to 1 for number of parallel calls\r\nmap_dataset = tensor_slice_dataset.map(\r\n    lambda x: x*x, num_parallel_calls=1)\r\nbatch_dataset = map_dataset.batch(1)\r\n\r\nfor v in batch_dataset:\r\n    time.sleep(0.1)\r\n\r\n##################### ALSO WORKING #####################\r\n\r\n# batch size greater than 1 works with any number\r\n# greater or equal to 1 and also with tf.data.experimental.AUTOTUNE\r\nmap_dataset = tensor_slice_dataset.map(\r\n    lambda x: x*x, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\nbatch_dataset = map_dataset.batch(2)\r\n\r\nfor v in batch_dataset:\r\n    time.sleep(0.1)\r\n\r\n######################## BROKEN  ########################\r\n\r\n# batch size equal to 1 DOES NOT work with\r\n# tf.data.experimental.AUTOTUNE\r\nmap_dataset = tensor_slice_dataset.map(\r\n    lambda x: x*x, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\nbatch_dataset = map_dataset.batch(1)\r\n\r\nfor v in batch_dataset:\r\n    time.sleep(0.1)\r\n```", "Could reproduce this issue with TF Version 2.0 in Google Colab, with CPU and GPU as Runtime. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/f4c7bf2b0b0e9908a4343be6f0935843/33516_oom_error.ipynb).", "Found the same issue in my project\r\n\r\nUpdate:\r\nIn case wrapping tf.data.Dataset with tf.distribute.Strategy.experimental_distribute_dataset and `batch size = 1*num_replicas` (batch size = 1 for each replica), the problem still exists.\r\n\r\nSorry I am not allowed to share the code for reproducing this problem,\r\nmy work flow is standard like this:\r\n\r\n```\r\nbatch_size = 1 * num_gpus\r\n\r\nfiles = tf.data.Dataset.list_files(tfrecord_files_pattern)\r\nif training:\r\n    files.shuffle(buffer_size=1024)\r\ndataset = files.interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\ndataset = dataset.repeat(1)\r\nnum_parallel_calls = tf.data.experimental.AUTOTUNE if batch_size > 1 else 8\r\ndataset = dataset.map(parse_fn, num_parallel_calls=num_parallel_calls)\r\ndataset = dataset.batch(batch_size=batch_size)\r\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\n...\r\nstrategy = ... # tf.distribute.Strategy here\r\ndataset = strategy.experimental_distribute_dataset(dataset)\r\n```\r\n\r\nFor ppl come across the same issue, it's not recommended to use `tf.data.experimental.AUTOTUNE` at this time (tf 2.0), manage `buffer_size/num_parallel_calls` yourself.", "Hello, I faced the same issue. I can confirm that the problem occurs on:\r\n- Windows 10, 32 GB RAM, tf version: v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Windows 7, 8 GB RAM, tf version: v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0\r\n\r\nPython version is 3.7.3\r\nIt consumes all available RAM, causing out of memory error on both devices.", "Update again, this is really a serous bug! With the pipeline in my previous comment, and batch size = 1024, the RAM was eat slowly from ~20% at the start of training to `out of memory` after one night training (total memory is 48G). So,\r\n1. with batch size = 1 for each gpus, the bug is triggered and runs out the memory after several training step.\r\n2. with batch size > 1 for each gpus, the memory increases slowly.\r\n3. without any AUTOTUNE at any batch size: testing.", "@rachellim could you please take a look?", "Thanks for the repro. Looking into it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33516\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33516\">No</a>\n", "This was indeed intriguing. I've submitted a fix, which will be in TF 2.1 :) "]}, {"number": 33515, "title": "tf.lite operators missing for `tf.nn.ctc_loss`: `Empty`, `InplaceAdd` and `While`", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.0.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_o\r\nps, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FLOOR_DIV, FULLY_CONNECTED, GATHER,\r\n LESS, LOG, LOG_SOFTMAX, MAXIMUM, MAX_POOL_2D, MEAN, MUL, NEG, ONE_HOT, PACK, PAD, PADV2, RANGE, RESHAPE, SHAPE, SPLIT_V, SQRT, SQUARE, SQUEEZE, STRIDED_SLICE, SUB, SUM, TILE, TRANSPOSE. Here is a list of operators for which\r\n you will need custom implementations: Empty, InplaceAdd, While.\r\nTraceback (most recent call last):\r\n  File \"/Users/ben/miniconda3/envs/wakeword/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main                                                                                \r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run                                                                                               \r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute                                                                             \r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflo\r\nw/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_o\r\nps, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FLOOR_DIV, FULLY_CONNECTED, GATHER,\r\n LESS, LOG, LOG_SOFTMAX, MAXIMUM, MAX_POOL_2D, MEAN, MUL, NEG, ONE_HOT, PACK, PAD, PADV2, RANGE, RESHAPE, SHAPE, SPLIT_V, SQRT, SQUARE, SQUEEZE, STRIDED_SLICE, SUB, SUM, TILE, TRANSPOSE. Here is a list of operators for which\r\n you will need custom implementations: Empty, InplaceAdd, While.\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nI'm trying to convert a keras model using tf.lite. The last layer of the model uses the `tf.nn.ctc_loss` in its `call` method. When I leave this layer off the model, the conversion works just fine.\r\n\r\nApparantly, only three operators are missing from making this work: `Empty`, `InplaceAdd` and `While`.\r\n\r\nThe error message also says this:\r\n```\r\n2019-10-18 18:19:25.694843: W tensorflow/lite/toco/tflite/operator.cc:2692] Op Empty is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.                                                 \r\n2019-10-18 18:19:25.694848: W tensorflow/lite/toco/tflite/operator.cc:2692] Op InplaceAdd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.                                            \r\n2019-10-18 18:19:25.694852: W tensorflow/lite/toco/tflite/operator.cc:2692] Op While is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set. \r\n```\r\nDoes this mean that the only thing missing from making this work is to somehow add these operators to the whitelist? Is this something that users can do? How are the chances that these operators are going to be included in tf.lite by default in the near feature?\r\n\r\nThanks!", "comments": ["Hi Benjamin,\r\n\r\nSimply add those ops into the whitelist won't work, unfortunately. Since 'while' is a control flow op, the flex runtime currently can't run control flow ops.\r\n\r\nI'm wondering why do we need compute ctc loss in tflite? isn't it related with training only? if we don't care about computing the loss in inference, i think we can just use the logits and decode the result from it? actually tflite has supported CTC_BEAM_SEARCH_DECODER as a custom op.", "Hey @haozha111, thank you for your response.\r\n\r\nI understand where you're coming from. Models trained with CTC loss usually try to return the most likely label sequence for the given input sequence (which would be solved by CTC_BEAM_SEARCH_DECODER).\r\n\r\nHowever, I'm using my model to return the probability that a *given target label sequence* is contained in the input sequence (which is probortional to the CTC loss itself).\r\n\r\nIn particular, I have a model that predicts phonemes using audio data as input and I am using the CTC loss function to calculate whether a given sequence of phonemes was spoken in an audio file.\r\n\r\nA pseudo-code example:\r\n```\r\ntarget_sequence = [\"H\", \"EH\", \"L\", \"L\", \"OH\"]\r\nlogits = model.predict(audio)\r\nprobability_of_target_sequence = ctc_loss(logits, target_sequence)\r\n```\r\n\r\nUsing full beam search for this use-case is somewhat wasteful, as the space of relevant label-paths is already known (the valid paths for the given target sequence). Also, if the top-paths returned by the beam search decoder do not include the exact form of the target sequence, the probability of the target sequence can only be calculated indirectly by using the top paths with the smallest edit distance to the target sequence.\r\n\r\nDo you see any way to handle this using tf.lite? If this is currently impossible, how likely is it that the required control flow ops will become part of tf.lite in the near future?\r\n\r\nThanks!", "Hi,\r\n\r\nThanks for the explanation. I think I understand your use case better now.\r\n\r\nI took a stab into the tf.nn.ctc_loss and a similar keras API tf.keras.backend.ctc_batch_cost. Unfortunately tflite can't support either of them at the moment:\r\n\r\n1) As you said, the tf.nn.ctc_loss contains unsupported op like tf.Empty, tf.InplaceAdd and While.\r\n2) tf.keras.backend.ctc_batch_cost's implementation is a bit different, but it contains unsupported ops like CTCLoss and While.\r\n\r\nThat being said, none of those two ctc loss could be supported in TF Lite at the moment..Which is a bit disappointing.\r\n\r\nCurrently I could suggestion several walk-arounds:\r\n1) Use the CTC_BEAM_SEARCH_DECODER, but the computation cost could be higher.\r\n2) Put the ctc loss function outside of the TF Graph. Compute the loss in your application logic instead.\r\n\r\n\r\nThanks!\r\n\r\n(As we are working towards a completely new TF Lite converter, hopefully some of the issues here will be easier to tackle with.. please stay tuned).", "Thank you @haozha111 for this explanation. I anticipated this would be the case. I'll try to work-around this issue and I'm looking forward to the revamped tf.lite converter.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33515\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33515\">No</a>\n", "Hey @haozha111,\r\n\r\nIn https://github.com/tensorflow/tensorflow/issues/30754#issuecomment-549471852 you have described how to use the experimental new optimizer to save RNNs in tf.lite format. Will this optimizer work for the operators missing for the ctc loss?\r\n\r\nThanks!", "Unfortunately it won't. Since CTC loss requires more ops like 'Empty' and 'InplaceAdd'.", "@BeWe11 curious: have you found a workaround or a speedy implementation for your CTC calculation use case?", "@BeWe11  Is it still an issue for you? If not, please go ahead and close the issue.Thanks!", "Hey @saikumarchalla,\r\n\r\nFirst, thank you for not auto-closing this issue. Unfortunately, I'm not working on this project anymore and don't have the time right now to write a new test-case. Feel free to close this issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33515\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33515\">No</a>\n"]}, {"number": 33514, "title": "Add a network test application.", "body": "This is a tool that can be used for testing different networks in a simple way. The input data, network model and expected output can be swapped out by supplying different arguments when running make -f tensorflow/.../Makefile as described in the README.md.", "comments": []}, {"number": 33513, "title": "Cannot build libtensorflowlite_c.so -- problem with bazel build", "body": "**System information**\r\n- Windows 10 64-bit\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.0.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): v1.0.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\n\r\nI am trying to build libtensorflowlite_c.so following the instructions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/unity/TensorFlowLitePlugin/README.md but the build is failing the the following log.\r\n\r\n**Any other info / logs**\r\nFirst run log:\r\n```\r\nC:\\Users\\Lorenzo\\Documents\\tensorflow>bazel build -c opt --cxxopt=--std=c++11 \\\r\nINFO: Writing tracer profile to 'C:/users/lorenzo/_bazel_lorenzo/6xammqtc/command.profile.gz'\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=172\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/Lorenzo/AppData/Local/Programs/Python/Python37/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\lorenzo\\documents\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\lorenzo\\documents\\tensorflow\\.bazelrc: --define=tf_api_version=2\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/bazel-toolchains/archive/92dd8a7a518a2fb7ba992d47c8b38299fe0be825.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at C:/users/lorenzo/_bazel_lorenzo/6xammqtc/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - C:/users/lorenzo/_bazel_lorenzo/6xammqtc/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - C:/users/lorenzo/documents/tensorflow/WORKSPACE:37:1\r\nERROR: Skipping '\\': Bad target pattern '\\': package names may contain A-Z, a-z, 0-9, or any of ' !\"#$%&'()*+,-./;<=>?[]^_`{|}~' (most 7-bit ascii characters except 0-31, 127, ':', or '\\')\r\nWARNING: Target pattern parsing failed.\r\nERROR: Bad target pattern '\\': package names may contain A-Z, a-z, 0-9, or any of ' !\"#$%&'()*+,-./;<=>?[]^_`{|}~' (most 7-bit ascii characters except 0-31, 127, ':', or '\\')\r\nINFO: Elapsed time: 0.139s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\nSecond run log:\r\n```\r\nC:\\Users\\Lorenzo\\Documents\\tensorflow>bazel build -c opt --cxxopt=--std=c++11 //tensorflow/lite/experimental/c:libtensorflowlite_c.so\r\nINFO: Writing tracer profile to 'C:/users/lorenzo/_bazel_lorenzo/6xammqtc/command.profile.gz'\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=172\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/Lorenzo/AppData/Local/Programs/Python/Python37/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\lorenzo\\documents\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\lorenzo\\documents\\tensorflow\\.bazelrc: --define=tf_api_version=2\r\nINFO: Build options --cxxopt, --host_cxxopt, --host_java_toolchain, and 1 more have changed, discarding analysis cache.\r\nINFO: Call stack for the definition of repository 'com_google_protobuf' which is a tf_http_archive (rule definition at C:/users/lorenzo/documents/tensorflow/third_party/repo.bzl:124:19):\r\n - C:/users/lorenzo/documents/tensorflow/tensorflow/workspace.bzl:432:5\r\n - C:/users/lorenzo/documents/tensorflow/WORKSPACE:19:1\r\nINFO: Repository 'com_google_protobuf' used the following cache hits instead of downloading the corresponding file.\r\n * Hash 'b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\r\nIf the definition of 'com_google_protobuf' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'com_google_protobuf':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/lorenzo/documents/tensorflow/third_party/repo.bzl\", line 104\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/users/lorenzo/documents/tensorflow/third_party/repo.bzl\", line 70, in _apply_patch\r\n                _wrap_bash_cmd(ctx, patch_command)\r\n        File \"C:/users/lorenzo/documents/tensorflow/third_party/repo.bzl\", line 28, in _wrap_bash_cmd\r\n                fail(\"BAZEL_SH environment variable i...\")\r\nBAZEL_SH environment variable is not set\r\nERROR: Analysis of target '//tensorflow/lite/experimental/c:libtensorflowlite_c.so' failed; build aborted: no such package '@com_google_protobuf//': Traceback (most recent call last):\r\n        File \"C:/users/lorenzo/documents/tensorflow/third_party/repo.bzl\", line 104\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/users/lorenzo/documents/tensorflow/third_party/repo.bzl\", line 70, in _apply_patch\r\n                _wrap_bash_cmd(ctx, patch_command)\r\n        File \"C:/users/lorenzo/documents/tensorflow/third_party/repo.bzl\", line 28, in _wrap_bash_cmd\r\n                fail(\"BAZEL_SH environment variable i...\")\r\nBAZEL_SH environment variable is not set\r\nINFO: Elapsed time: 4.906s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded, 0 targets configured)\r\n    currently loading: tensorflow\r\n```\r\n", "comments": ["@ravikyram, this looks like a TF Lite build issue. Can you re-triage to one of their experts, please?\r\n\r\n@lordalcol This may not be the root of the problem, but I don't think that TF supports Bazel 1.0 yet. It may help to downgrade to the maximum supported version, which I think is currently 0.29.1.", "Also @lordalcol are you able to build any other targets in TensorFlow? The failure here looks more like a toolchain/dependency issue that is likely not specific to TensorFlow Lite.", "I have a similar issue. When I change the .so to a .dll and try to load it in unity after compiling with bazel I get an error\r\n\r\n`\r\nFailed to load 'Assets/TensorFlowLite/SDK/Plugins/libtensorflowlite_c.dll', expected x64 architecture, but was Unknown architecture. You must recompile your plugin for x64 architecture.`\r\n\r\nedit: similar system build", "@smaerdlatigid can you share the exact build command that you used? And what version of Visual Studio is being used for compilation?", "The build command I am using: `bazel build -c opt --cxxopt=--std=c++11 \\\r\n  //tensorflow/lite/experimental/c:libtensorflowlite_c.so`\r\n\r\nWith visual studio v14.0 \r\n\r\nI can compile the file into a `.so` file but I'm not sure how to compile it to a `.dll` ", "> @lordalcol This may not be the root of the problem, but I don't think that TF supports Bazel 1.0 yet. It may help to downgrade to the maximum supported version, which I think is currently 0.29.1.\r\n\r\n@lordalcol,\r\nAny updates regarding this? Please take at @angerson's comment and let us know if you are still facing the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33513\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33513\">No</a>\n", "Note, you should now be able to build using\r\n```\r\nbazel build -c opt //tensorflow/lite/c:tensorflowlite_c\r\n```\r\nand it should produce the shared library with a `.dll` suffix."]}, {"number": 33512, "title": "OutOfRangeError: End of sequence [Op:IteratorGetNextSync] with tf.data", "body": "TensorFlow version: 2.0.0\r\nPlatform: Jupyter Notebook (macOS)\r\n\r\n**Description**: I am trying to determine the time it takes to load the entire FashionMNIST dataset in a batch-wise manner with `td.data`. I am using the following utility script taken from [here](https://www.tensorflow.org/tutorials/load_data/images#performance):\r\n\r\n```python\r\nimport time\r\ndefault_timeit_steps = 1000\r\n\r\ndef timeit(ds, steps=default_timeit_steps):\r\n  start = time.time()\r\n  it = iter(ds)\r\n  for i in range(steps):\r\n    batch = next(it)\r\n    if i%10 == 0:\r\n      print('.',end='')\r\n  print()\r\n  end = time.time()\r\n\r\n  duration = end-start\r\n  print(\"{} batches: {} s\".format(steps, duration))\r\n  print(\"{:0.5f} Images/s\".format(BATCH_SIZE*steps/duration))\r\n```\r\n\r\nHere's the [Jupyter Notebook](https://colab.research.google.com/drive/1jX30MMQmEbiNfz15a-UXmGk8v5KPD76V) with which the error can be reproduced. Following is the error trace:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nOutOfRangeError                           Traceback (most recent call last)\r\n/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in next(self)\r\n    665     try:\r\n--> 666       return self._next_internal()\r\n    667     except errors.OutOfRangeError:\r\n\r\n/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    650             output_types=self._flat_output_types,\r\n--> 651             output_shapes=self._flat_output_shapes)\r\n    652 \r\n\r\n/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py in iterator_get_next_sync(iterator, output_types, output_shapes, name)\r\n   2672         message = e.message\r\n-> 2673       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n   2674   # Add nodes to the TensorFlow graph.\r\n\r\n/miniconda3/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nOutOfRangeError: End of sequence [Op:IteratorGetNextSync]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nStopIteration                             Traceback (most recent call last)\r\n<ipython-input-8-7ac545d60e99> in <module>\r\n      1 # `tf.data`\r\n----> 2 timeit(train_dataset)\r\n\r\n<ipython-input-6-386e91380544> in timeit(ds, steps)\r\n      6     it = iter(ds)\r\n      7     for i in range(steps):\r\n----> 8         batch = next(it)\r\n      9         if i%10 == 0:\r\n     10             print('.',end='')\r\n\r\n/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in __next__(self)\r\n    620 \r\n    621   def __next__(self):  # For Python 3 compatibility\r\n--> 622     return self.next()\r\n    623 \r\n    624   def _next_internal(self):\r\n\r\n/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in next(self)\r\n    666       return self._next_internal()\r\n    667     except errors.OutOfRangeError:\r\n--> 668       raise StopIteration\r\n    669 \r\n    670   @property\r\n\r\nStopIteration: \r\n```\r\n\r\n", "comments": []}, {"number": 33511, "title": "Using GPU for Tflite on Android", "body": "My first problem was it this post #33011  So for now I have code which works according to answers from the previousl post.  \r\n                float[] masked = coreResult.getMasked();\r\n                float[] inverted = coreResult.getInverted();\r\n\r\n                byteBufferMasked.asFloatBuffer().put(masked);\r\n                byteBufferInverted.asFloatBuffer().put(inverted);\r\n                Object[] inputs = new Object[]{byteBufferMasked, byteBufferInverted};\r\n                Map<Integer, Object> outputs = new HashMap();\r\n                outputs.put(0, byteBufferOutput);\r\n\r\n                interpreter.runForMultipleInputsOutputs(inputs, outputs);\r\n\r\n                byteBufferOutput.rewind();\r\n                byteBufferOutput.asFloatBuffer().get(output);\r\n                initializeSecond(coreRequest, output);\r\n                interpreter.close();\r\noutput is resulf float array and method initializeSecond() make the bitmap from this array.\r\nThis code takes about 16-20 sec.And almost all this time is working of the model in the following line:\r\n                interpreter.runForMultipleInputsOutputs(inputs, outputs);\r\nI have added some threads through: \r\n               new Interpreter.Options().setNumThreads(4)\r\nAnd now it works faster in 2 times(interesting thing - if I put for example 4 threads or 20 threads - result the same). And this results are only for strong phones, for old models - it is about 20 sec after adding threads.(without it - about 35 sec)\r\nSo it  I am trying to add GPU delegate supporting for my Interpreter. According to Tflite docs from here https://www.tensorflow.org/lite/performance/gpu_advanced I am trying the following\r\n                GpuDelegate delegateGPU = new GpuDelegate();\r\n                interpreter = new Interpreter(map,  new \r\n                Interpreter.Options().addDelegate(delegateGPU).setNumThreads(4));\r\nAnd result is the same.(But in the demo from docs it is enough for GPU supporting) And the one thing that I did not do - adding OpenGL Shader (SSBO). And the main my problem I can not completry understant how I can implement it for my case. First of all I have two inputs and one output.  The code from tflite GPU demo is about getting bitmap from camera, in my case I already have the two inputs as images. I have seens in the docs that we can do it for only inputs or for only outputs, right? \r\nIf yes I do not know maybe in my case it will be better for output considering that I have two inputs. Also if I run  runForMultipleInputsOutputs(inputs, null); I will have the exception about null parameter. \r\nAnd only now I understand that method interpreter.runInference(null, outputArray); from GPU docs is from Tflite demo. And I think it is not so good for understanding. Also if I should do it for output I can not understand what is means the method from GPU docs : renderOutputSsbo(outputSsboId); There is not this function in the demo.", "comments": ["Hello, I still can not solve my problem. Should I wait some news from here? Thank you", "Sorry for the late reply; I was out on a business trip.\r\n\r\nGPU does *not* support `setNumThreads` and having the same result is expected.  You can employ SSBO for both input and output.  If you're looking for a full example of how that can be achieved, I suggest looking into MediaPipe's `TfLiteInferenceCalculator`.\r\n", "Oh sorry, only now I've seen your message. Okay thanks I will have a look through this example and ask my questions here. So, does not matter I use GpuDelegate or not, if I use setNumThreads I have the result faster. I mean okay GpuDelegate does not support setNumThreads, but if I will use it with GpuDelegate, will it interfere?", "@ArtyomMashkin \r\n\r\nNo.  It won't.  It will be just ignored.", "Hello, I've back again to the issue. I've investigated the `TfLiteInferenceCalculator` from MediaPipe and some other projects in MediaPipe, but they are mostly using c++, but my project is on Java so I'm not sure that they can help me.\r\n\r\nHow I understand my case is usual, I have inputs and outputs, only I don't use camera as in usual examples, I have a proceeded ByteBuffer from a file. \r\n\r\nI use:\r\n`interpreter.runForMultipleInputsOutputs(inputs, outputs);`\r\n\r\nAm I right, that if I use SSBO in my `runForMultipleInputsOutputs` method, it will work faster and using camera is not necessary for using SSBO?", "FYI MediaPipe has Java APIs for the entire graph (not at calculator level).\r\n\r\nIf your input is not from the camera, I don't see a strong reason for you to copy things to SSBO by yourself; the GPU delegate will take care of copying it to the right SSBO.", "I just put two images as float arrays in interpreter and get the result image as float array. I have not actions with camera. And sorry, what do you mean? As I wrote above if I just put GPU delegate in Interpreter, I do not see the better results. And  it is why the my main question - can I do speed of Tflite processing fastly somehow using GPU considering that simple adding delegate does not help? \r\nIf so, how? Because I do not quite understand how the GPU works and I don\u2019t know what the correct implementation should be for this to be a good result. Thanks!", "Oh, did you forget to call `interpreter.modifyGraphWithDelegate`?", "Hmm, yes I do not call it, so in general now I have the following code:\r\n` GpuDelegate delegateGPU = new GpuDelegate();`\r\n  `Interpreter interpreter = new Interpreter(map, new Interpreter.Options().addDelegate(delegateGPU)\r\n                        .setNumThreads(NUM_THREADS));`\r\n                `byteBufferMasked.asFloatBuffer().put(masked);`\r\n                `byteBufferInverted.asFloatBuffer().put(inverted);`\r\n                `Object[] inputs = new Object[]{byteBufferMasked, byteBufferInverted};`\r\n               ` Map<Integer, Object> outputs = new HashMap();`\r\n                `outputs.put(0, byteBufferOutput);`\r\n\r\n                `interpreter.runForMultipleInputsOutputs(inputs, outputs);`\r\n\r\n                `byteBufferOutput.rewind();`\r\n               ` byteBufferOutput.asFloatBuffer().get(output);`\r\n\r\nAnd if I put the following    ` interpreter.modifyGraphWithDelegate(delegateGPU);`\r\nafter  `Interpreter interpreter = new Interpreter(map, new Interpreter.Options().addDelegate(delegateGPU)\r\n                        .setNumThreads(NUM_THREADS));`\r\nI get this error:\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\nCAST: Operation is not supported.\r\nGREATER: Operation is not supported.\r\nMEAN: Operation is not supported.\r\nRESIZE_NEAREST_NEIGHBOR: Operation is not supported.\r\nSPLIT: Operation is not supported.\r\nFirst 1 operations will run on the GPU, and the remaining 198 on the CPU.\r\nModifyGraphWithDelegate is disallowed when graph is immutable.\r\n\r\nIs it mean that with my model I can not use GPU? Or I can do something for make my code faster?", "First of all, CAST, GREATER, MEAN, RESIZE_NEAREST_NEIGHBOR, SPLIT are not supported on the TFLite GPU.  You have to either modify the graph to exclude the ops or implement those ops yourself (for now).\r\n\r\nBut the real reason why this is failing is because of:\r\n\r\n> ModifyGraphWithDelegate is disallowed when graph is immutable.\r\n\r\nand not because of the missing ops.  If there are missing ops, TFLite will fall back to CPU.\r\n\r\nUnfortunately, I'm not entirely sure when this error condition of immutable graph is triggered; I haven't experienced it myself.  You would have to read the code (grep for that error message) and trace back.", "Okay, three questions:\r\n1) If I will not change the graph for now, anyway I can make my code faster, if I will find and fix this error `ModifyGraphWithDelegate is disallowed when graph is immutable.` right?\r\n2) Is it only my responsebillity right? I mean for each case, this error can appear for own reasons and only I am able to fing the cause  \r\n3) In general, should I use only `interpreter.modifyGraphWithDelegate` for make my code faster? I do not need use SSBO and for my case modifyGraphWithDelegate is the best way, right?", "1.  Well depends.  TFLite GPU inspects the graph, and the moment we see an unsupported op, we'll say \"nope, can't do this anymore\", and will delegate the remaining parts to CPU.  If the first op is the unsupported op, then the entire network will just run on the CPU without any GPU acceleration.\r\n\r\n2. Yes.\r\n\r\n3. Yes, you don't need SSBO.  That's for the advanced use-case when you have camera input (GPU) and/or viewfinder output (GPU).", "Thanks! I will invastigate it", "@ArtyomMashkin have you removed custom op from your graph? I need to use mediapipe graph but it's showing custom op error. Let me know if you fixed this issue. Thank you", "@ArtyomMashkin,\r\n\r\nWe are checking to see if this is still an issue. Have you got a chance to investigate as you mentioned above ? Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33511\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33511\">No</a>\n"]}, {"number": 33510, "title": "Using GPU for Tflite on Android", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 33509, "title": "Not allowed to load local resource on chrome", "body": "I have downloaded the examples/Iris file from github, and follow REAMME.md.\r\nBut when I enter the link\r\n`yarn watch`\r\nThe server was built, and the chrome came automatically. \r\nBut the css file and js file wasn't called successfully.\r\n When I read the console, it was said that\r\n**\" Not allowed to load local resource file:///C:/Program%20Files/Git/tfjs-examples.44bb3400.css \"**\r\nI spent two days finding the answers but i failed. Hope someone could help me..", "comments": ["@kingsleyljc,\r\nIt looks like this issue is related to TF JS, not Tensorflow. If so, request you to raise an issue in [TF JS Github Repository](https://github.com/tensorflow/tfjs/issues/) because it will be looked in by the respective experts. Thanks! "]}, {"number": 33508, "title": "    add micro optional debug tools as tflite.", "body": "@wangtz \r\nprintf mico interpreter tensors and node info. as that:\r\nInterpreter has 16 tensors and 7 nodes\r\nInputs: 1\r\nOutputs: 0\r\n\r\nTensor   0 Identity             kTfLiteFloat32  kTfLiteArenaRw         16 bytes ( 0.0 MB)  1 4\r\nTensor   1 conv2d_10_input      kTfLiteFloat32  kTfLiteArenaRw       1536 bytes ( 0.0 MB)  1 128 3 1\r\nTensor   2 sequential_5/conv2d_10/Conv2D/ReadVariableOp kTfLiteFloat32   kTfLiteMmapRo        384 bytes ( 0.0 MB)  1 4 3 8\r\nTensor   3 sequential_5/conv2d_10/Conv2D_bias kTfLiteFloat32   kTfLiteMmapRo         32 bytes ( 0.0 MB)  8\r\nTensor   4 sequential_5/conv2d_10/Relu kTfLiteFloat32  kTfLiteArenaRw      12288 bytes ( 0.0 MB)  1 128 3 8\r\nTensor   5 sequential_5/conv2d_11/Conv2D/ReadVariableOp kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  16 4 1 8\r\nTensor   6 sequential_5/conv2d_11/Conv2D_bias kTfLiteFloat32   kTfLiteMmapRo         64 bytes ( 0.0 MB)  16\r\nTensor   7 sequential_5/conv2d_11/Relu kTfLiteFloat32  kTfLiteArenaRw       2688 bytes ( 0.0 MB)  1 42 1 16\r\nTensor   8 sequential_5/dense_10/MatMul/ReadVariableOp/transpose kTfLiteFloat32   kTfLiteMmapRo      14336 bytes ( 0.0 MB)  16 224\r\nTensor   9 sequential_5/dense_10/MatMul_bias kTfLiteFloat32   kTfLiteMmapRo         64 bytes ( 0.0 MB)  16\r\nTensor  10 sequential_5/dense_10/Relu kTfLiteFloat32  kTfLiteArenaRw         64 bytes ( 0.0 MB)  1 16\r\nTensor  11 sequential_5/dense_11/BiasAdd kTfLiteFloat32  kTfLiteArenaRw         16 bytes ( 0.0 MB)  1 4\r\nTensor  12 sequential_5/dense_11/MatMul/ReadVariableOp/transpose kTfLiteFloat32   kTfLiteMmapRo        256 bytes ( 0.0 MB)  4 16\r\nTensor  13 sequential_5/dense_11/MatMul_bias kTfLiteFloat32   kTfLiteMmapRo         16 bytes ( 0.0 MB)  4\r\nTensor  14 sequential_5/max_pooling2d_10/MaxPool kTfLiteFloat32  kTfLiteArenaRw       1344 bytes ( 0.0 MB)  1 42 1 8\r\nTensor  15 sequential_5/max_pooling2d_11/MaxPool kTfLiteFloat32  kTfLiteArenaRw        896 bytes ( 0.0 MB)  1 14 1 16\r\n\r\nNode   0 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 1 2 3\r\n  Outputs: 4\r\nNode   1 Operator Builtin Code  17 MAX_POOL_2D\r\n  Inputs: 4\r\n  Outputs: 14\r\nNode   2 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 14 5 6\r\n  Outputs: 7\r\nNode   3 Operator Builtin Code  17 MAX_POOL_2D\r\n  Inputs: 7\r\n  Outputs: 15\r\nNode   4 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 15 8 9\r\n  Outputs: 10\r\nNode   5 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 10 12 13\r\n  Outputs: 11\r\nNode   6 Operator Builtin Code  25 SOFTMAX\r\n  Inputs: 11\r\n  Outputs: 0\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33508) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot I signed it!\r\n\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33508) for more info**.\n\n<!-- ok -->", "OK.\r\n\r\nI will change the pair to a struct.\r\n\r\n\r\nthanks.\r\n\r\nBest regards\uff0c\r\n\u6e29\u5e05\r\n---------------------------------\r\n\u4eba\u5de5\u667a\u80fd\u90e8\r\n\u79fb\u52a8\u7535\u8bdd: 13301171201\r\n________________________________\r\n\u53d1\u4ef6\u4eba: wangtz <notifications@github.com>\r\n\u53d1\u9001\u65f6\u95f4: 2019\u5e7410\u670824\u65e5 2:47\r\n\u6536\u4ef6\u4eba: tensorflow/tensorflow\r\n\u6284\u9001: \u6e29\u5e05; Author\r\n\u4e3b\u9898: [External Mail][\u8425\u9500\u90ae\u4ef6] Re: [tensorflow/tensorflow] add micro optional debug tools as tflite. (#33508)\r\n\r\n\r\n@wangtz commented on this pull request.\r\n\r\nLooks good! Thanks for the contribution.\r\n\r\n________________________________\r\n\r\nIn tensorflow/lite/experimental/micro/micro_interpreter.cc<https://github.com/tensorflow/tensorflow/pull/33508#discussion_r338211978>:\r\n\r\n> @@ -299,4 +299,42 @@ TfLiteTensor* MicroInterpreter::tensor(size_t index) {\r\n   return &context_.tensors[index];\r\n }\r\n\r\n+std::pair<TfLiteNode, const TfLiteRegistration*> MicroInterpreter::node_and_registration(int node_index) {\r\n\r\n\r\nWondering if there is a way to avoid std::pair dependency in the interpreter code? Probably either define a struct for std::pair or subclass/friend class?\r\n\r\n\u2015\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/33508?email_source=notifications&email_token=ANQXMC6RM7I3EMMTORRH5BTQQCL33A5CNFSM4JCGC5MKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCI7MK5Q#pullrequestreview-306103670>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ANQXMC2SKGT2XBJH5GWZ5XTQQCL33ANCNFSM4JCGC5MA>.\r\n\r\n#/******\u672c\u90ae\u4ef6\u53ca\u5176\u9644\u4ef6\u542b\u6709\u5c0f\u7c73\u516c\u53f8\u7684\u4fdd\u5bc6\u4fe1\u606f\uff0c\u4ec5\u9650\u4e8e\u53d1\u9001\u7ed9\u4e0a\u9762\u5730\u5740\u4e2d\u5217\u51fa\u7684\u4e2a\u4eba\u6216\u7fa4\u7ec4\u3002\u7981\u6b62\u4efb\u4f55\u5176\u4ed6\u4eba\u4ee5\u4efb\u4f55\u5f62\u5f0f\u4f7f\u7528\uff08\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u5168\u90e8\u6216\u90e8\u5206\u5730\u6cc4\u9732\u3001\u590d\u5236\u3001\u6216\u6563\u53d1\uff09\u672c\u90ae\u4ef6\u4e2d\u7684\u4fe1\u606f\u3002\u5982\u679c\u60a8\u9519\u6536\u4e86\u672c\u90ae\u4ef6\uff0c\u8bf7\u60a8\u7acb\u5373\u7535\u8bdd\u6216\u90ae\u4ef6\u901a\u77e5\u53d1\u4ef6\u4eba\u5e76\u5220\u9664\u672c\u90ae\u4ef6\uff01 This e-mail and its attachments contain confidential information from XIAOMI, which is intended only for the person or entity whose address is listed above. Any use of the information contained herein in any way (including, but not limited to, total or partial disclosure, reproduction, or dissemination) by persons other than the intended recipient(s) is prohibited. If you receive this e-mail in error, please notify the sender by phone or email immediately and delete it!******/#\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@wenshuai-xiaomi  Any update on this PR, please. ", "@wenshuai-xiaomi  Can you please resolve conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 33507, "title": "enable in place debugging without packing and install", "body": "- Problem\r\n  * It is a common practice to iteratively change the code, build, packing, install, and debug to understand/fix/enable certain features. The packing and installation phases are annoying because they are not strictly necessary and slowing down the whole cycle.\r\n\r\n- Solution\r\n   * add a tool to create symbolic links from the source code directory to the binary directory to `merge' these two so that we can change the code in the source tree, build (not necessary for scripting languages such as Python), and debug immediately without packing and installation, enabling a more efficient dev cycle.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33507) for more info**.\n\n<!-- need_author_cla -->", "@imzhenyu-dev thank you for your contribution, please sign CLA.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33507) for more info**.\n\n<!-- ok -->", "Cant you already do your iterative development using bazel?\r\nThe recommended way to do iterative development is to simply write a unit test, and run \"bazel test <your/unit/test>\"\r\n\r\nI am worried about this, because it will not work with all the shared objects we have, and will confuse the users.\r\nTherefore, I am inclined to reject this change.", "@imzhenyu-dev Could you please check reviewer comments and keep us posted. Thanks!"]}, {"number": 33506, "title": "How do we need to slice tensor in forward pass for writing the custom loss function.", "body": "I have created on a network for which input will be \r\n\r\ninput shape is : (20, 16, 240, 320, 3)\r\noutput (logits is ) : Tensor(\"logits:0\", shape=(?, 1), dtype=float32)\r\ni,e  output in forward pass  for my network is \r\n`[array([[ -3.8974638 ],\r\n       [ -5.2204075 ],\r\n       [ -3.105205  ],\r\n       [ -3.4843988 ],\r\n       [ -4.0670795 ],\r\n       [ -2.097713  ],\r\n       [  2.6989975 ],\r\n       [  3.1498132 ],\r\n       [  2.3710475 ],\r\n       [  2.0358462 ],\r\n       [ -9.698921  ],\r\n       [ -3.48162   ],\r\n       [ -3.4569516 ],\r\n       [ -2.9997227 ],\r\n       [ -6.9699297 ],\r\n       [-10.641993  ],\r\n       [  2.6629193 ],\r\n       [ -0.03444159],\r\n       [ -1.6006284 ],\r\n       [ -4.1530366 ]], dtype=float32)]\r\n`\r\nNow I want write loss functions on this logits as below mentioned \r\n\r\nloss = max(0,1-max(logits[0:10])+max(logits[10:20]) so that I can proceed for back prop.\r\n\r\nHow can I do this in  : tensorflow==1.14.0\r\n\r\n", "comments": ["@tiru1930,\r\nCan you please let us know what exactly you want to achieve using Custom Loss so that we can help you out. Thanks!", "Eventually I am creating network to assign high score to anomaly videos and\nlow score to normal videos. So my model will take batch of anomaly and\nnormal videos at once and with loss function it assigned parameters\ndistinguish the anomaly and normal videos\n\nOn Mon, 21 Oct 2019 at 7:35 PM, rmothukuru <notifications@github.com> wrote:\n\n> @tiru1930 <https://github.com/tiru1930>,\n> Can you please let us know what exactly you want to achieve using Custom\n> Loss so that we can help you out. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33506?email_source=notifications&email_token=AC5FIV2OBWI64PHW75DIP3TQPWZLBA5CNFSM4JCE6PZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEB2NJDI#issuecomment-544527501>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC5FIV2HEJ3IJUXB4H6DIILQPWZLBANCNFSM4JCE6PZQ>\n> .\n>\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there."]}, {"number": 33505, "title": "loading break due to undefined symbol about constexpr const char*", "body": "- Problem\r\n  * when build with ```bazel build --config opt -c dbg //tensorflow/tools/pip_package:build_pip_package```, there is a symbol not found error around ```kLowerUsingSwitchMergeAttr ``` when a python program loads the output dynamic linked libraries. \r\n\r\n- Root Cause\r\n  * this is related to the new C++ standard about ```constexpr const char*``` as discussed [here](https://stackoverflow.com/questions/8016780/undefined-reference-to-static-constexpr-char), and my local C++ compiler does not support this yet\r\n\r\n- Solution\r\n  * add definitions in .cc file and the problem is gone\r\n\r\nI don't know why the standard build options (without ```-c dbg```) do not break though ...\r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33505) for more info**.\n\n<!-- need_author_cla -->", "@imzhenyu-dev thank you for your contribution, please sign CLA.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33505) for more info**.\n\n<!-- ok -->"]}, {"number": 33504, "title": "module 'tensorflow' has no attribute 'ConfigProto'", "body": "```python\r\nimport tensorflow as tf\r\nimport os\r\nconfig = tf.ConfigProto()\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'ConfigProto'\r\n```", "comments": ["I guess you're using tensorflow 2.x. In that case, use tf.compat.v1.ConfigProto() instead.", "```tf.ConfigProto``` is deprecated in TF 2.0\r\nConfiguring device placements, device policy, gpu memory growth etc can still be done in TF 2.0 using [```tf.config.experimental```](https://www.tensorflow.org/api_docs/python/tf/config/experimental)", "> `tf.ConfigProto` is deprecated in TF 2.0\r\n> Configuring device placements, device policy, gpu memory growth etc can still be done in TF 2.0 using [`tf.config.experimental`](https://www.tensorflow.org/api_docs/python/tf/config/experimental)\r\n\r\nthank you\uff01", "> I guess you're using tensorflow 2.x. In that case, use tf.compat.v1.ConfigProto() instead.\r\n\r\nthank you\uff01", "@Grape-A, Are you happy to close this if no issue persists. Thanks!", "AttributeError: module 'tensorflow.compat' has no attribute 'v1'"]}, {"number": 33503, "title": "Model.fit displays wrong information on progress bars for attrs classes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, a minimal self-contained python script demonstrating the bug\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nOSX\r\n\r\n- TensorFlow installed from (source or binary):\r\nInstalled from anaconda\r\n\r\n- TensorFlow version (use command below):\r\n1.14\r\n\r\n- Python version:\r\n3.6.9\r\n\r\n**Describe the current behavior**\r\nWrong progress bar information [1000/12] overly long progress bar\r\n\r\n**Describe the expected behavior**\r\nCorrect progress bar information [100/100] progress bar reasonably short\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport attr\r\nimport numpy as np\r\n\r\nfrom tensorflow.python.keras import Input, Model\r\nfrom tensorflow.python.keras.layers import Dense\r\nfrom tensorflow.python.keras.optimizers import SGD\r\nfrom tensorflow.python.keras.utils import Sequence\r\n\r\n\r\ndef make_model(D):\r\n    x = Input(shape=[D])\r\n    y = Dense(1, activation=None)(x)\r\n    model = Model(inputs=x, outputs=y)\r\n    model.compile(optimizer=SGD(), loss=\"mean_squared_error\")\r\n    return model\r\n\r\n\r\n@attr.s\r\nclass Feed(Sequence):\r\n    foo = attr.ib(default=np.arange(12))  # Remove this attribute to get the expected behaviour\r\n    batch_size = attr.ib(default=10)\r\n    feature_dimension = attr.ib(default=3)\r\n\r\n    def __getitem__(self, idx):\r\n        features = np.random.randn(self.batch_size, self.feature_dimension)\r\n        targets = np.sum(features, axis=1, keepdims=True)\r\n        return features, targets\r\n\r\n    def __len__(self):\r\n        return 100\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    feed = Feed()\r\n    model = make_model(feed.feature_dimension)\r\n    model.fit(x=feed, epochs=10)\r\n```\r\n\r\n**Other info / logs**\r\nThe bug is caused by \r\n```python\r\ndef _get_num_samples_or_steps(data, steps_per_epoch):\r\n  \"\"\"Returns number of samples or steps, and whether to use steps count mode.\"\"\"\r\n  flat_inputs = nest.flatten(data)\r\n  if hasattr(flat_inputs[0], 'shape'):\r\n    return int(flat_inputs[0].shape[0]), False\r\n  return steps_per_epoch, True\r\n```\r\nin module `tensorflow.python.keras.engine.training_generator`.\r\n\r\n`_get_num_samples_or_steps` should always return `steps_per_epoch, True` for a `Sequence` irrespective of the `attrs.Attributes` of the class implementing the `Sequence` interface. \r\n\r\nThe statement `if data_utils.is_generator_or_sequence(x):` in module `tensorflow.python.keras.engine.training` is already ensuring that the passed object is a `Sequence` and therefore the call to function `_get_num_samples_or_steps` seems superfluous.\r\n", "comments": ["Could replicate the error with `TF Version 1.14 and 1.15` and by `importing` from `tensorflow.python.keras` and  `tensorflow.keras` as well.\r\n\r\nHere is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/c2fde49c1f29ff4dff930baf03f9ad81/33503_progressbar.ipynb). Thanks!", "@trendelkampschroer Thanks for the issue!\r\n\r\nThis is fixed in the latest tf-nightly: `pip install tf-nightly`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33503\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33503\">No</a>\n"]}, {"number": 33502, "title": "please fix docs  converter = tf.lite.TFLiteConverter.from_keras_model(model) doesn't work", "body": "Please update docs this example doesn't work\r\n\r\nhttps://www.tensorflow.org/lite/convert/python_api#converting_a_keras_model_\r\n\r\nConverting a Keras model \r\nThe following example shows how to convert a tf.keras model into a TensorFlow Lite FlatBuffer.\r\n\r\nimport tensorflow as tf\r\n\r\n# Create a simple Keras model.\r\nx = [-1, 0, 1, 2, 3, 4]\r\ny = [-3, -1, 1, 3, 5, 7]\r\n\r\nmodel = tf.keras.models.Sequential(\r\n    [tf.keras.layers.Dense(units=1, input_shape=[1])])\r\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\r\nmodel.fit(x, y, epochs=50)\r\n\r\n# Convert the model.\r\n**converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()**\r\n\r\ntf.__version__\r\n'1.15.0-rc3'\r\n\r\noutput in colab \r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-31-68e491526aaa> in <module>()\r\n     11 \r\n     12 # Convert the model.\r\n---> 13 converter = tf.lite.TFLiteConverter.get_input_arrays(model)\r\n     14 tflite_model = converter.convert()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py in get_input_arrays(self)\r\n   1001       List of strings.\r\n   1002     \"\"\"\r\n-> 1003     if self._has_valid_tensors():\r\n   1004       return [_get_tensor_name(tensor) for tensor in self._input_tensors]\r\n   1005     else:\r\n\r\nAttributeError: 'Sequential' object has no attribute '_has_valid_tensors'\r\n", "comments": ["it was tf 2.0"]}, {"number": 33501, "title": "tf.keras.applications download path should be made configurable.", "body": "The download path for the models downloaded using tf.keras.applications seems to be hardcoded to `~/.keras/models`.  \r\nThis path should be made configurable. \r\nIt will benefit people who have less storage space in their home folder, which is a feature in many large-scale computational clusters.", "comments": ["@indranaut, Will it be possible to provide any use case or sample standalone code. Thanks!", "@gadagashwini  There is no need for any sample standalone code here.\r\n\r\nThe issue is simple. When models are downloaded, they are downloaded to `~/.keras/models`. \r\n\r\nBut what if the disk quota policy does not allow for a lot of space in the home folder ? Hence, a much better thing would be to make the download path configurable.", "The cached_sub_dir is configurable. Do you need anything else?", "The arguments `cached_sub_dir` and `cache_dir` are configurable only on directly using `tf.keras.utils.get_file`. They are not configurable when you are using something like `model = `tf.keras.applications.ResNet50(include_top=False, weights='imagenet')` .\r\n\r\nI did some digging and found out that in order to download the models to some other folder, one needs to go inside `keras_applications` folder and directly modify the file `resnet50.py`. The modification is to explicitly add an argument called `cache_dir`  and using it in `get_file` function which is called inside `resnet50.py`. \r\n\r\nI think that an ideal solution of this issue would be to cohesively merge `keras_applications` and `tensorflow` repositories along with changes to reflect the configuration of path. \r\n\r\n\r\n", "I think this can be achieved via:\r\n1. using tf.keras.utils.get_file, and configure where you wanna download your checkpoint to, say `file_path`.\r\n2. passing `weights=file_path` when you're creating the model.\r\n\r\nSo we don't need to export every possible flag. Let us know if this works for you.", "Closing this for now. Let us know if you have any other questions."]}]