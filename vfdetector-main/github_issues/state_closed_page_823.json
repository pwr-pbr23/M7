[{"number": 28842, "title": "remove eager line from example", "body": "easy change for tf2.0 doc sprint.  Remove eager execution from example on tf.data", "comments": ["Please reopen once you have a branch rebased on top of master, that doesn't include thousands of commits."]}, {"number": 28841, "title": " tf.Dataset.map() only processes one example in my dataset", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.2\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: V8.0.61\r\n- GPU model and memory: Tesla P100-PCIE, 12193MiB\r\n\r\n**Describe the current behavior**\r\nI have a dataset containing 592 examples, but `tf.Dataset.map` only processes one of those, as evidenced by a global counter, which I increment in the function given to `map()`. Why is it not processing all examples in the dataset? I run in eager execution, but code inside `.map()` is not executed eagerly.\r\n\r\n**Describe the expected behavior**\r\nAll 592 examples in my dataset should be processed (with the counter being 592 afterwards).\r\n\r\n**Code to reproduce the issue**\r\nMy dataset: https://we.tl/t-rZYNJ6I9oU\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\nimport utilities\r\n\r\ncounter = 0\r\n\r\ndef decode_png_mask(image_buffer):\r\n    \"\"\"\r\n    Takes string of bytes encoding a PNG and produces a tensor image\r\n    \"\"\"\r\n    image = tf.squeeze(tf.image.decode_image(image_buffer, channels=1), axis=2)\r\n    image.set_shape([None, None])\r\n    image = tf.greater(image, 0)\r\n    image = tf.cast(image, dtype=tf.uint8)\r\n    return image\r\n\r\ndef masks_to_onehots(tag_masks, tag_class_indices, num_classes):\r\n    def onehotify(pixel_tag_masks):\r\n        tag_mask_sizes_suppressed = tf.where(tf.not_equal(tag_mask_sizes, 0), tag_mask_sizes, tag_mask_sizes + 9999999)\r\n        smallest_mask_index = tf.argmin(tag_mask_sizes_suppressed)\r\n        onehot = tf.one_hot(smallest_mask_index\r\n                            , depth=num_classes, dtype=tf.uint8)\r\n        return onehot\r\n    tag_mask_sizes = tf.reduce_sum(tag_masks, axis=[1, 2])\r\n    image_masks = tf.transpose(tag_masks, perm=[1, 2, 0])\r\n    onehots = tf.map_fn(lambda x: tf.map_fn(onehotify, x), image_masks)\r\n    return onehots\r\n\r\ndef parse_example(example_proto, width, height, num_classes):\r\n    features = {\r\n        'image/encoded': tf.FixedLenFeature((), tf.string),\r\n        'image/height': tf.FixedLenFeature((), tf.int64),\r\n        'image/width': tf.FixedLenFeature((), tf.int64),\r\n        'image/filename': tf.FixedLenFeature((), tf.string),\r\n        'image/object/bbox/xmin': tf.VarLenFeature(tf.float32),\r\n        'image/object/bbox/xmax': tf.VarLenFeature(tf.float32),\r\n        'image/object/bbox/ymin': tf.VarLenFeature(tf.float32),\r\n        'image/object/bbox/ymax': tf.VarLenFeature(tf.float32),\r\n        'image/object/class/label': tf.VarLenFeature(tf.int64),\r\n        'image/object/class/text': tf.VarLenFeature(tf.string),\r\n        'image/object/mask': tf.VarLenFeature(tf.string),\r\n        'image/depth': tf.FixedLenFeature((), tf.string)\r\n    }\r\n\r\n    global counter\r\n    counter = counter + 1\r\n\r\n    parsed_example = tf.parse_single_example(example_proto, features)\r\n\r\n    # Decode image\r\n    image = tf.image.decode_jpeg(parsed_example['image/encoded'])\r\n    parsed_example['image/encoded'] = image\r\n\r\n    tag_masks = tf.sparse.to_dense(parsed_example['image/object/mask'], default_value=\"\")\r\n    tag_masks = tf.map_fn(decode_png_mask, tag_masks, dtype=tf.uint8)\r\n    tag_masks = tf.reshape(tag_masks, shape=tf.stack([-1, height, width]), name='tag_masks')\r\n\r\n    # All segmentation now have their mask in mask, their labelmap index in classes_indices and their tagname in classes_text\r\n    tag_class_indices = tf.sparse.to_dense(parsed_example['image/object/class/label'])\r\n    tag_class_names = tf.sparse.to_dense(parsed_example['image/object/class/text'], default_value=\"\")\r\n    onehots = masks_to_onehots(tag_masks, tag_class_indices, num_classes)\r\n    parsed_example['image/labels'] = onehots\r\n\r\n    return parsed_example\r\n\r\ntf.enable_eager_execution()\r\n\r\nNUM_CLASSES = 21\r\n\r\ntfrecord_train = \"/path/to/tf.record\"\r\ndataset_train = tf.data.TFRecordDataset(tfrecord_train)\r\n\r\n# Read image widht/height from the TFRecord file\r\niterator = dataset_train.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\nparsed_element = np.fromstring(next_element.numpy(), dtype=np.uint8)\r\nexample = tf.train.Example.FromString(parsed_element)\r\nheight = example.features.feature['image/height'].int64_list.value[0]\r\nwidth = example.features.feature['image/width'].int64_list.value[0]\r\n\r\ndataset_train = dataset_train.map(lambda x: parse_example(x, width, height, NUM_CLASSES))\r\nprint(counter)\r\n```\r\n", "comments": ["@EmielBoss Ran the code provided here, I got the output as 1", "@EmielBoss Could you share the data as the link expired. Also, try to check it with Latest TF1.14.0. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28841\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28841\">No</a>\n"]}, {"number": 28840, "title": "tf.make_ndarray() throws an AttributeError: 'Tensor' object has no attribute 'tensor_shape'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.2\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: V8.0.61\r\n- GPU model and memory: Tesla P100-PCIE, 12193MiB\r\n\r\n**Describe the current behavior**\r\nWhen calling `tf.make_ndarray()` on a tensor (`<class 'tensorflow.python.framework.ops.Tensor'>`, dtype `uint8`, shape `(?, 1000, 1500)`), I get the following error:\r\n\r\n`File \"/home/.../python3.5/site-packages/tensorflow/python/framework/tensor_util.py\", line 563, in MakeNdarray\r\n    shape = [d.size for d in tensor.tensor_shape.dim]\r\nAttributeError: 'Tensor' object has no attribute 'tensor_shape'`\r\n\r\nI call the function inside a function given to `tf.Dataset.map()`, which is not executing eagerly. However, the error also occurs when using eager execution (as tested by using a for-loop on the dataset instead of `.map()`).\r\n\r\n**Describe the expected behavior**\r\nI expect `tf.make_ndarray()` to return a NumPy array as advertised :)\r\n\r\n**Code to reproduce the issue**\r\nMy dataset: https://we.tl/t-rZYNJ6I9oU\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport sys\r\n\r\ndef decode_png_mask(image_buffer):\r\n\r\n    \"\"\"\r\n    Takes string of bytes encoding a PNG and produces a tensor image\r\n    \"\"\"\r\n    image = tf.squeeze(tf.image.decode_image(image_buffer, channels=1), axis=2)\r\n    image.set_shape([None, None])\r\n    image = tf.greater(image, 0)\r\n    image = tf.cast(image, dtype=tf.uint8)\r\n    return image\r\n\r\ndef masks_to_onehots(tag_masks, tag_class_indices, num_classes):\r\n\r\n    def onehotify(pixel_tag_masks):\r\n        nonzero_indices = np.flatnonzero(pixel_tag_masks)\r\n        tag_mask_sizes_suppressed = [tag_mask_sizes[i] if i in nonzero_indices else 9999999 for i in range(len(tag_mask_sizes))]\r\n        smallest_mask_index = np.argmin(tag_mask_sizes_suppressed)\r\n        class_index = tag_class_indices[smallest_mask_index]\r\n        onehot = eye[class_index]\r\n        return onehot\r\n\r\n    eye = np.eye(num_classes)\r\n    tag_masks = tf.make_ndarray(tag_masks) #tag_masks = tag_masks.numpy()\r\n    tag_class_indices = tf.make_ndarray(tag_class_indices) #tag_class_indices = tag_class_indices.numpy()\r\n    tag_mask_sizes = np.sum(tag_masks, axis=(1, 2))\r\n    image_masks = np.transpose(tag_masks, axes=[1, 2, 0])\r\n    onehots = np.apply_along_axis(onehotify, axis=2, arr=image_masks)\r\n    #onehots = np.array([[eye[tag_class_indices[np.argmin([tag_mask_sizes[i] if i in np.flatnonzero(pixel_tag_masks) else 9999999 for i in range(len(tag_mask_sizes))])]] for pixel_tag_masks in image_masks[height]] for height in range(image_masks.shape[0])])\r\n    return tf.convert_to_tensor(onehots)\r\n\r\ndef parse_example(example_proto, width, height, num_classes):\r\n    features = {\r\n        'image/encoded': tf.FixedLenFeature((), tf.string),\r\n        'image/height': tf.FixedLenFeature((), tf.int64),\r\n        'image/width': tf.FixedLenFeature((), tf.int64),\r\n        'image/filename': tf.FixedLenFeature((), tf.string),\r\n        'image/object/bbox/xmin': tf.VarLenFeature(tf.float32),\r\n        'image/object/bbox/xmax': tf.VarLenFeature(tf.float32),\r\n        'image/object/bbox/ymin': tf.VarLenFeature(tf.float32),\r\n        'image/object/bbox/ymax': tf.VarLenFeature(tf.float32),\r\n        'image/object/class/label': tf.VarLenFeature(tf.int64),\r\n        'image/object/class/text': tf.VarLenFeature(tf.string),\r\n        'image/object/mask': tf.VarLenFeature(tf.string),\r\n        'image/depth': tf.FixedLenFeature((), tf.string)\r\n    }\r\n\r\n    parsed_example = tf.parse_single_example(example_proto, features)\r\n\r\n    # Decode image\r\n    image = tf.image.decode_jpeg(parsed_example['image/encoded'])\r\n    parsed_example['image/encoded'] = image\r\n\r\n    tag_masks = tf.sparse.to_dense(parsed_example['image/object/mask'], default_value=\"\")\r\n    tag_masks = tf.map_fn(decode_png_mask, tag_masks, dtype=tf.uint8)\r\n    tag_masks = tf.reshape(tag_masks, shape=tf.stack([-1, height, width]), name='tag_masks')\r\n\r\n    # All segmentation now have their mask in mask, their labelmap index in classes_indices and their tagname in classes_text\r\n    tag_class_indices = tf.sparse.to_dense(parsed_example['image/object/class/label'])\r\n    tag_class_names = tf.sparse.to_dense(parsed_example['image/object/class/text'], default_value=\"\")\r\n    onehots = masks_to_onehots(tag_masks, tag_class_indices, num_classes)\r\n    parsed_example['image/labels'] = onehots\r\n\r\n    return parsed_example\r\n\r\ntf.enable_eager_execution()\r\n\r\nNUM_CLASSES = 21\r\n\r\ntfrecord_train = \"/path/to/tf.record\"\r\ndataset_train = tf.data.TFRecordDataset(tfrecord_train)\r\n\r\n# Read image widht/height from the TFRecord file\r\niterator = dataset_train.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\nparsed_element = np.fromstring(next_element.numpy(), dtype=np.uint8)\r\nexample = tf.train.Example.FromString(parsed_element)\r\nheight = example.features.feature['image/height'].int64_list.value[0]\r\nwidth = example.features.feature['image/width'].int64_list.value[0]\r\n\r\ndataset_train = dataset_train.map(lambda x: parse_example(x, width, height, NUM_CLASSES))\r\n```\r\n\r\n**Other info / logs**\r\nFull traceback:\r\n\r\n`Traceback (most recent call last):\r\n  File \"/home/local/CYCLOMEDIA001/ebos/Workspace/all_cf/src/experimental/cityfusion/tasks/material_segmentation/semantic_fpn.py\", line 114, in <module>\r\n    dataset_train = dataset_train.map(lambda x: parse_example(x, width, height, NUM_CLASSES))\r\n  File \"/home/.../python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1038, in map\r\n    return MapDataset(self, map_func)\r\n  File \"/home/.../python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2611, in __init__\r\n    map_func, \"Dataset.map()\", input_dataset)\r\n  File \"/home/.../python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1860, in __init__\r\n    self._function.add_to_graph(ops.get_default_graph())\r\n  File \"/home/.../python3.5/site-packages/tensorflow/python/framework/function.py\", line 479, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"/home/.../python3.5/site-packages/tensorflow/python/framework/function.py\", line 335, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"/home/.../python3.5/site-packages/tensorflow/python/framework/function.py\", line 344, in _create_definition_if_needed_impl\r\n    self._capture_by_value, self._caller_device)\r\n  File \"/home/.../python3.5/site-packages/tensorflow/python/framework/function.py\", line 864, in func_graph_from_py_func\r\n    outputs = func(*func_graph.inputs)\r\n  File \"/home/.../python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1794, in tf_data_structured_function_wrapper\r\n    ret = func(*nested_args)\r\n  File \"/path/to/script.py\", line 114, in <lambda>\r\n    dataset_train = dataset_train.map(lambda x: parse_example(x, width, height, NUM_CLASSES))\r\n  File \"path/to/script.py\", line 86, in parse_example\r\n    onehots = masks_to_onehots(tag_masks, tag_class_indices, num_classes)\r\n  File \"/path/to/script.py\", line 25, in masks_to_onehots\r\n    tag_masks = tf.make_ndarray(tag_masks) #tag_masks = tag_masks.numpy()\r\n  File \"/home/.../python3.5/site-packages/tensorflow/python/framework/tensor_util.py\", line 563, in MakeNdarray\r\n    shape = [d.size for d in tensor.tensor_shape.dim]\r\nAttributeError: 'Tensor' object has no attribute 'tensor_shape'`\r\n", "comments": ["Since TensorFlow 1.0, tf.Tensor now has a tf.Tensor.shape property, which returns the same value as tf.Tensor.get_shape().\r\nIn versions prior to TensorFlow 1.0 tf.Tensor doesn't have a .shape property. You should use the Tensor.get_shape() method instead:\r\nHope this helps.", "Thanks for your reply! I don't fully understand what you mean. I am not using `.shape` in my call to `make_ndarray()`. Where exactly should I replace `.shape` with `.get_shape()`?\r\n\r\nI tried changing the first line in `make_ndarray()` (where the `AttributeError` is thrown) from `shape = [d.size for d in tensor.tensor_shape.dim]` to `shape = [d.size for d in tensor.get_shape().dim]`, but that only throws `AttributeError: 'TensorShape' object has no attribute 'dim'`. Furthermore I don't suppose I should be messing with TensorFlow code.", "I am able to reproduce the issue reported here on my system with same configuration. Got the error AttributeError: 'Tensor' object has no attribute 'tensor_shape' ", "@emaballarin Does your code execute successfully if you use the commented code (NumPy compatibility) in your script instead of tf.make_ndarray()\r\n```python\r\n#tag_masks = tag_masks.numpy()\r\n#tag_class_indices = tag_class_indices.numpy()\r\n```\r\nAlso can you please print the type of ```tag_masks``` and ```tag_class_indices```", "@ymodak No, because `.numpy()` only works in eager execution, and code ran through `tf.Dataset.map()` isn't executed eagerly. This is the reason I try to rely on `.make_ndarray()`. Both `tag_masks` and `tag_class_indices` are `<class 'tensorflow.python.framework.ops.Tensor'>`.\r\n\r\nI already moved on though. I abandoned TFRecords because they are getting me absolutely nowhere. However, other people might run into this same problem.\r\n\r\n", "The error observed can be reproduced with a smaller code snippet perhaps. Closing this issue for now since you are working past it and will update when new information is available. Thanks!", "I am having a similar issue.  I can reproduce it on TF 2 Beta (GPU version - if that is relevant) using the code below.  This is so simple that I must be misunderstanding something:\r\n\r\n`import tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nwith tf.device('/gpu:0'):\r\n    start = tf.convert_to_tensor(np.ones([100,100]))\r\n\r\ntf.make_ndarray(start)`", "[`tf.make_ndarray`](https://www.tensorflow.org/api_docs/python/tf/make_ndarray) is used to convert [`TensorProto`](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/framework/tensor.proto) values into NumPy arrays, not [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects. These values are generally the constants used in a graph. For example, in graph mode, when you use [`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant), you create a `Const` operation with an attribute `value` holding the constant value that the operation will produce. That attribute is stored as a [`TensorProto`](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/framework/tensor.proto). Hence, you can \"extract\" the value of a `Const` operation as a NumPy array like this (TF 1.x):\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\nA = tf.constant(5)\r\nC = tf.make_ndarray(A.op.get_attr('value'))\r\nprint(C, type(C))\r\n# 5 <class 'numpy.ndarray'>\r\n```\r\n\r\nIn general, though, you cannot convert arbitrary tensors into NumPy arrays in graph mode, as their values will depend on the values of the variables and the fed inputs within a particular session.", "I am having the same issue, trying to use tf.make_ndarray in tf.Dataset.map call", "I am also stuck with the same bug. And yes it is a bug since this is 100% non-pythonic. Please fix it. Eager execution in Tensorflow2+ is a lie that made me waste more time than it should. It should be instead \"eager execution sometimes\"", "Why is this issue still closed?", "can this issue be re-opened? I am experiencing the same problem as described above.", "I just wanna record the model's output value as numpy ndarray in each train_step, when calling model.fit() or .evaluate(), not only calculate the metrics and update trainable variables.\r\nAre there any solutions?", "why closed?i have the same problem in tf2.5", "@jiangno111  Can you please post a new issue and provide a minimal code to reproduce the bug? Thanks!", "I am also having same issue.\r\n", "The handling of this issue speaks volumes of why the research community is moving from tensorflow to pytorch.", "thanks tensorflow what can i say", "Did anyone manage to solve this one?\r\nIt's unbelievable the amount of hassle you need to go through to perform such a simple operation. \r\nReally unbelievable.", "This issue keeps popping up in my inbox and I don't even know why as there really is no bug to solve here. I already explained this [above](https://github.com/tensorflow/tensorflow/issues/28840#issuecomment-521626336), but let's go through it again for TensorFlow 2.\r\n\r\n[`tf.make_ndarray`](https://www.tensorflow.org/api_docs/python/tf/make_ndarray) is NOT a general function to convert tensors to NumPy arrays (in spite to what the admittedly misleading documentation of the function might say). What it does is it converts a [`TensorProto`](https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/core/framework/tensor.proto), which is a low-level binary representation of a tensor, into a NumPy array. Most typical TensorFlow code will never use `TensorProto` objects (directly), so [`tf.make_ndarray`](https://www.tensorflow.org/api_docs/python/tf/make_ndarray) is just not a useful function for most people. Depending on the case, it may be possible to build a NumPy array from a tensor with it, but, again, its purpose is not to convert tensors to NumPy arrays in general, and it should rarely or never be used.\r\n\r\nIf you want to convert a TensorFlow tensor into a NumPy array, you need to be mindful that TensorFlow code may run in eager mode or graph mode. Graph mode is less \"convenient\" to use, but it is important for performance, optimisation, serialisation and other reasons. Although it is rare to explicitly enter into graph mode as a library user, it is very normal to write code that runs in graph mode, like the code in a Keras model, for example. It is not always straightforward to know what mode your code is running, especially since TensorFlow aims to hide this complexity from the library users, but the complexity is still there and it is important to understand it.\r\n\r\nIf you are in eager mode, you can just do `.numpy()` on your tensor. In graph mode, though, you CANNOT obtain the NumPy array corresponding to a tensor, because tensors do not hold any value in particular in graph mode, but they rather expresses a symbolic intermediate results (with some exceptions like [`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant), which is possible to convert into a NumPy array even in graph mode, because it always has the same value). If you want to do a NumPy operation with your tensor in graph mode (which would not be differentiable in TensorFlow btw), you need to temporarily switch to eager mode within graph mode, which you can do with [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function). There, your tensors will be eager tensors and you can call `.numpy()` on them.\r\n\r\nNB: I am not a TensorFlow developer nor have any affiliation of any kind with anyone involved with its development.", "My issue is that the code is running in eager mode (`tf.executing_eagerly()` returns True) but if I do `.numpy()` on my tensors I still get _AttributeError: 'Tensor' object has no attribute 'numpy'_ . So I cannot just do `.numpy()` on my EagerTensor, it always returns an error.\r\n\r\nI am using TF 2.8.0 and NumPy 1.22.3", "@EdoardoCarlesi That is an unusual error. In eager mode, all your tensors should have the type `tf.EagerTensor`, but your error suggests that your object is actually a `tf.Tensor`, not a `tf.EagerTensor` (see \"**'Tensor'** object has no attribute 'numpy'\"). It may be that the tensor object you are trying to evaluate comes from a previous graph context, even if you are currently in eager mode. I'd recommend you to post a small reproducible example in StackOverflow or a similar forum to find out what exactly is going on.", "I know this should not happen. Unfortunately the code is quite long and complex, my workaround was to pre-process the data with numpy before feeding it to tensorflow, so I couldn't really solve this issue.\r\n\r\nOn other forums I found solutions which involved changing NumPy version and/or TF version.\r\n\r\nAll in all it's very frustrating that such a simple operation required so much time and effort, on PyTorch this is something that never gave me headaches, not even close. "]}, {"number": 28839, "title": "error: default initialization of an object of const type 'const Subgraph::Identity' without a user-provided default constructor", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nOS X 10.11.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version:\r\n13.1\r\n- Python version:\r\n3.6.7\r\n- Installed using virtualenv? pip? conda?:\r\nConda\r\n- Bazel version (if compiling from source):\r\nbazel 18.1 & 0.21.1\r\n- GCC/Compiler version (if compiling from source):\r\n$ g++ --version\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 8.0.0 (clang-800.0.42.1)\r\nTarget: x86_64-apple-darwin15.6.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\nMacBook-Pro:tensorflow davidlaxer$ c++ --version\r\nApple LLVM version 8.0.0 (clang-800.0.42.1)\r\nTarget: x86_64-apple-darwin15.6.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\nMacBook-Pro:tensorflow davidlaxer$ clang --version\r\nclang version 5.0.0 (tags/RELEASE_500/final)\r\nTarget: x86_64-apple-darwin15.6.0\r\nThread model: posix\r\nInstalledDir: /Users/davidlaxer/anaconda/bin\r\nMacBook-Pro:tensorflow davidlaxer$ which clang\r\n/Users/davidlaxer/anaconda/bin/clang\r\nMacBook-Pro:tensorflow davidlaxer$ /Users/davidlaxer/anaconda/bin/clang --version\r\nclang version 5.0.0 (tags/RELEASE_500/final)\r\nTarget: x86_64-apple-darwin15.6.0\r\nThread model: posix\r\n\r\n- CUDA/cuDNN version:\r\nCPU Only\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nERROR: /Users/davidlaxer/tensorflow/tensorflow/core/grappler/graph_analyzer/BUILD:5:1: C++ compilation of rule '//tensorflow/core/grappler/graph_analyzer:graph_analyzer_lib' failed (Exit 1)\r\ntensorflow/core/grappler/graph_analyzer/graph_analyzer.cc:75:28: error: default initialization of an object of const type 'const Subgraph::Identity' without a user-provided default constructor\r\n  const Subgraph::Identity empty_parent;\r\n                           ^\r\n                                       {}\r\n1 error generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["TensorFlow requires [macOS 10.12.6 (Sierra) or later (64-bit)](https://www.tensorflow.org/install/pip#system-requirements)", "I was able to Tensorflow built 1.12  from source on 10.11.6.\nIs that the final version I can build on this OS?\n\n> On May 20, 2019, at 3:46 PM, ymodak <notifications@github.com> wrote:\n> \n> TensorFlow requires macOS 10.12.6 (Sierra) or later (64-bit)\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "In Tensorflow 13.1 , I adjusted the Subgraph::Identity initialization.  But ...\r\n\r\n```\r\nERROR: /Users/davidlaxer/tensorflow/tensorflow/contrib/ignite/BUILD:148:1: Executing genrule //tensorflow/contrib/ignite:gen_igfs_ops_pygenrule failed (Trace/breakpoint trap): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\ndyld: lazy symbol binding failed: Symbol not found: _clock_gettime\r\n  Referenced from: /private/var/tmp/_bazel_davidlaxer/f9fe21ec5c09226e5ca0dce9376abe82/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/ignite/../../../_solib_darwin_x86_64/_U_S_Stensorflow_Scontrib_Signite_Cgen_Ugen_Uigfs_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so (which was built for Mac OS X 10.12)\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n\r\ndyld: Symbol not found: _clock_gettime\r\n  Referenced from: /private/var/tmp/_bazel_davidlaxer/f9fe21ec5c09226e5ca0dce9376abe82/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/ignite/../../../_solib_darwin_x86_64/_U_S_Stensorflow_Scontrib_Signite_Cgen_Ugen_Uigfs_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so (which was built for Mac OS X 10.12)\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n\r\n/bin/bash: line 1: 53224 Trace/BPT trap: 5       bazel-out/host/bin/tensorflow/contrib/ignite/gen_gen_igfs_ops_py_wrappers_cc , '' 0 0 > bazel-out/darwin-opt/genfiles/tensorflow/contrib/ignite/python/ops/gen_igfs_ops.py\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3007.455s, Critical Path: 200.07s\r\nINFO: 2483 processes: 2483 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "I am not familiar with this code. Please reassign to grappler developers.", "I'm not familiar with grappler unfortunately, unassigning myself.", "Hi @dbl001 ! \r\nIt seems you are using older versions(1.x versions) of Tensorflow. We recommend that you upgrade  your code base to 2.x  versions as many features and bug fixes has been done in newer versions and let us know if the issue still persists in newer versions. \r\n\r\nTensorflow 2.6 is available on Macbook M1 ,Please follow this [reference ]( https://developer.apple.com/metal/tensorflow-plugin/)\r\nYou can also build from using this [link](https://www.tensorflow.org/install/source) using Bazel. Please create a new issue ,if you face any problem!\r\n\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28839\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28839\">No</a>\n"]}, {"number": 28838, "title": "TFLite GPU support for C++", "body": "**System information**\r\n- TensorFlow version (you are using): 0.0.1-gpu-experimental\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nAnyone know if Tensorflow Lite has GPU support for C++ (or Python)? I've seen guides for Android and iOS, but I haven't come across anything about C++ (or Python).\r\n\r\n**Who will benefit with this feature?**\r\n\r\nC++ and Python developers.\r\n", "comments": ["Check the [READE.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/README.md) of `tensorflow/lite/delegates/gpu/`. I added GPU delegate to label_image and sent PR https://github.com/tensorflow/tensorflow/pull/27464. TFLite [benchmark_model](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark) supports GPU delegate already. I don't think the GPU delegate is exported to Python.", "@freedomtan Thanks for the pointer. If my understanding of the code is correct, GPU delegates are only used on Android. Can delegates work on desktop? I believe `use_cpu` for benchmark_model only works on Android.", "Yes, as far as I know, current GL ES and Metal backends were for Android and iOS. But, it should be possible to make the GL ES compute shader backend work on non-Android platforms. I didn't try. With some hacks, I made the iOS Metal backend work on my iMac before.", "@hoonkai \r\n\r\nSome of our engineers were able to run the GPU delegate on desktop with Mesa, but it's not a use case we can support as ... I am one of the people who couldn't get it to work.  Also, the optimization parameters may not be applicable for desktop use.  You're welcome to try out, but I wouldn't be able to support any problems with it :("]}, {"number": 28837, "title": "Support Stateful LSTM on TPU", "body": "<em>Cannot use LSTM model with tf.keras if Stateful = True on TPU</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13\r\n\r\n**Describe the feature and the current behavior/state.**\r\nif I build tf.keras model and then try to convert to TPU model it gives: \r\n\r\n`WARNING:tensorflow:Model replication does not currently support stateful models.  Degrading to a single core.`\r\n\r\n**Will this change the current api? How?**\r\nunlikely\r\n\r\n**Who will benefit with this feature?**\r\nanyone to research time-series prediction\r\n\r\n**Any Other info.**\r\n", "comments": ["I think RNN model will be supported on TPU after 2.0, TPU team is working it. Adding @xiejw from TPU team for status update.", "Sorry for the confusion. \r\n\r\n@qlzh727  This PR is about stateful RNN not just RNN.\r\n\r\nI would like to provide some context here. \r\n\r\nThe warning is accurate. It is supported but the code is using single TPU core for that, instead of 8 TPU cores. The code is working but it will not fully utilize the TPU power. \r\n\r\nThe reason for that is: Keras uses TF variable to store the states for RNN.  However, the current TPU software stack does not have the infrastructure to have different variables for different TPU cores. This is mainly due to software limitation not hardware. The expected change to make that happen is quite large and we do not have near-term plan to work on that. \r\n\r\nWith that, I will close this. And in future, if we have the bandwidth to solve this, will reopen.\r\n", "Is it so difficult to hardcode 8 sets of those variables (sorry I don't know how your TPU software stack looks like)? If so - **can Keras team help** by making it work differently when on TPUs?\r\n\r\nAnd I am just curious - who is defining your priorities? Is it a popular vote or something? Like they voted for Trump and Brexit? You voted to cut TPUs out for anyone who is doing ML on sequences and time-series? Fine."]}, {"number": 28836, "title": "No gradients provided for any variable: ['cnn_model_14/conv2d_42/kernel:0', 'cnn_model_14/conv2d_42/bias:0', 'cnn_model_14/conv2d_43/kernel:0', 'cnn_model_14/conv2d_43/bias:0', 'cnn_model_14/dense_28/kernel:0', 'cnn_model_14/dense_28/bias:0', 'cnn_model_14/dense_29/kernel:0', 'cnn_model_14/dense_29/bias:0'].", "body": "> ---------------------------------------------------------------------------\r\n> ValueError                                Traceback (most recent call last)\r\n> <timed eval> in <module>()\r\n> \r\n> <ipython-input-92-4e8fa0eeafe4> in train(train_dataset, train_labels_dataset, epochs)\r\n>       8             labels = label_batch\r\n>       9             print('logits->',logits.shape , 'labels->',labels.shape)\r\n> ---> 10             train_step(logits,labels)\r\n>      11 \r\n>      12         if (epoch + 1) % 15 == 0:\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n>     424     # This is the first call of __call__, so we have to initialize.\r\n>     425     initializer_map = {}\r\n> --> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n>     427     if self._created_variables:\r\n>     428       try:\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n>     368     self._concrete_stateful_fn = (\r\n>     369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n> --> 370             *args, **kwds))\r\n>     371 \r\n>     372     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n>    1311     if self._input_signature:\r\n>    1312       args, kwargs = None, None\r\n> -> 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n>    1314     return graph_function\r\n>    1315 \r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n>    1578           or call_context_key not in self._function_cache.missed):\r\n>    1579         self._function_cache.missed.add(call_context_key)\r\n> -> 1580         graph_function = self._create_graph_function(args, kwargs)\r\n>    1581         self._function_cache.primary[cache_key] = graph_function\r\n>    1582         return graph_function, args, kwargs\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n>    1510             arg_names=arg_names,\r\n>    1511             override_flat_arg_shapes=override_flat_arg_shapes,\r\n> -> 1512             capture_by_value=self._capture_by_value),\r\n>    1513         self._function_attributes)\r\n>    1514 \r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n>     692                                           converted_func)\r\n>     693 \r\n> --> 694       func_outputs = python_func(*func_args, **func_kwargs)\r\n>     695 \r\n>     696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n>     315         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n>     316         # the function a weak reference to itself to avoid a reference cycle.\r\n> --> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n>     318     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n>     319 \r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n>     684                   optional_features=autograph_options,\r\n>     685                   force_conversion=True,\r\n> --> 686               ), args, kwargs)\r\n>     687 \r\n>     688         # Wrapping around a decorator allows checks like tf_inspect.getargspec\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\r\n>     390     return _call_unconverted(f, args, kwargs)\r\n>     391 \r\n> --> 392   result = converted_f(*effective_args, **kwargs)\r\n>     393 \r\n>     394   # The converted function's closure is simply inserted into the function's\r\n> \r\n> /tmp/tmplz2l0k7l.py in tf__train_step(logits, labels)\r\n>       5     loss = ag__.converted_call('mean_squared_error', tf.losses, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (labels, logits), {})\r\n>       6     gradient_of_cnn = ag__.converted_call('gradient', cnn_tape, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (loss, model.trainable_variables), {})\r\n> ----> 7     ag__.converted_call('apply_gradients', cnn_optimizer, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (zip(gradient_of_cnn, model.trainable_variables),), {})\r\n>       8 \r\n>       9 \r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\r\n>     265 \r\n>     266   if not options.force_conversion and conversion.is_whitelisted_for_graph(f):\r\n> --> 267     return _call_unconverted(f, args, kwargs)\r\n>     268 \r\n>     269   # internal_convert_user_code is for example turned off when issuing a dynamic\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs)\r\n>     186     return f.__self__.call(args, kwargs)\r\n>     187 \r\n> --> 188   return f(*args, **kwargs)\r\n>     189 \r\n>     190 \r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name)\r\n>     394       ValueError: If none of the variables have gradients.\r\n>     395     \"\"\"\r\n> --> 396     grads_and_vars = _filter_grads(grads_and_vars)\r\n>     397     var_list = [v for (_, v) in grads_and_vars]\r\n>     398 \r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)\r\n>     922   if not filtered:\r\n>     923     raise ValueError(\"No gradients provided for any variable: %s.\" %\r\n> --> 924                      ([v.name for _, v in grads_and_vars],))\r\n>     925   if vars_with_empty_grads:\r\n>     926     logging.warning(\r\n> \r\n> ValueError: No gradients provided for any variable: ['cnn_model_14/conv2d_42/kernel:0', 'cnn_model_14/conv2d_42/bias:0', 'cnn_model_14/conv2d_43/kernel:0', 'cnn_model_14/conv2d_43/bias:0', 'cnn_model_14/dense_28/kernel:0', 'cnn_model_14/dense_28/bias:0', 'cnn_model_14/dense_29/kernel:0', 'cnn_model_14/dense_29/bias:0'].\r\n> \r\n> \r\n\r\n\r\n\r\n====================================================================\r\n\r\n```\r\n\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n#!pip install tensorflow-gpu==2.0.0-alpha0\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import datasets, layers, models\r\nprint(tf.__version__)\r\nimport os\r\nimport time\r\nimport numpy as np\r\n\r\n\r\n(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\r\n\r\ntrain_images = train_images.reshape((60000, 28, 28, 1))\r\ntest_images = test_images.reshape((10000, 28, 28, 1))\r\n\r\n# Normalize pixel values to be between 0 and 1\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\n# Load training and eval data\r\n((train_data, train_labels),\r\n (eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\r\ntrain_data = train_data.reshape((60000,28,28,1))\r\neval_data = eval_data.reshape((10000,28,28,1))\r\n\r\ntrain_data = train_data/np.float32(255)\r\ntrain_labels = train_labels.astype(np.int32)  # not required\r\n\r\neval_data = eval_data/np.float32(255)\r\neval_labels = eval_labels.astype(np.int32)  # not required\r\n\r\nEPOCHS = 50\r\nnoise_dim = 100\r\nnum_examples_to_generate = 16\r\nBUFFER_SIZE = 60000\r\nBATCH_SIZE = 256\r\n\r\n# We will reuse this seed overtime (so it's easier)\r\n# to visualize progress in the animated GIF)\r\nseed = tf.random.normal([num_examples_to_generate, noise_dim])\r\n\r\n\r\n# Batch and shuffle the data\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\ntrain_labels_dataset =  tf.data.Dataset.from_tensor_slices(train_labels).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n\r\n\r\ninput_shape = (28,28.1)\r\nclass cnn_model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(cnn_model,self).__init__()\r\n        \r\n        #self.conv1 = layers.Conv2D(32,(3,3),activation='relu',input_shape= input_shape)\r\n        self.conv1 = layers.Conv2D(32, 3, 3, padding='same', activation='relu')\r\n        self.maxpool = layers.MaxPool2D((2,2))\r\n        self.conv2 = layers.Conv2D(64,(3,3),activation ='relu')\r\n        self.conv3 = layers.Conv2D(128,(3,3),activation='relu')\r\n        self.flatten = layers.Flatten()\r\n        self.dense64 = layers.Dense(64,activation='relu')\r\n        self.dense10 = layers.Dense(10,activation='relu')\r\n        self.dropout = layers.Dropout(0.25)\r\n    def call(self,inputs):\r\n        x = self.conv1(inputs)\r\n        x = self.conv2(x)\r\n        x = self.maxpool(x)\r\n        x = self.dropout(x)\r\n        x = self.flatten(x)\r\n        x = self.dense64(x)\r\n        x = self.dense10(x)\r\n        return x\r\n\r\n\r\n\r\nmodel = cnn_model()\r\n#result=tf.argmax(model(train_data[:10]))\r\nlabels = train_labels[:50]\r\nlogits = tf.argmax(model(train_data[:50]),axis=1)\r\n#print(labelss.shape)\r\nprint(logits.shape , ':', labels.shape)\r\nprint('logits->',logits)\r\nprint(\"=======\")\r\nprint('labels->',labels[1])\r\n#print(train_labels[:10])\r\n\r\n\r\ncnn_optimizer = tf.optimizers.Adam(1e-4)\r\n\r\n\r\ncheckpoint_dir = './training_checkpoints'\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\ncheckpoint = tf.train.Checkpoint(cnn_optimizer=cnn_optimizer)\r\n\r\n\r\n\r\n# Notice the use of `tf.function`\r\n# This annotation causes the function to be \"compiled\".\r\n@tf.function\r\ndef train_step(logits,labels):    \r\n    with tf.GradientTape() as cnn_tape:\r\n        print(\"train_step\")\r\n        loss = tf.losses.mean_squared_error(labels,logits)\r\n        #print(\"loss\",loss)\r\n        gradient_of_cnn = cnn_tape.gradient(loss,model.trainable_variables)\r\n        \r\n        cnn_optimizer.apply_gradients(zip(gradient_of_cnn,model.trainable_variables))\r\n        \r\n        #cnn_optimizer.minimize(loss,var_list=model.trainable_variables)\r\n\r\n\r\ndef train(train_dataset,train_labels_dataset,epochs):\r\n    for epoch in range(epochs):\r\n        start = time.time()\r\n        \r\n        for train_batch,label_batch in zip(train_dataset,train_labels_dataset):\r\n            print(train_batch.shape,label_batch.shape)\r\n            logits = tf.argmax(model(train_batch),axis=1)\r\n            labels = label_batch\r\n            print('logits->',logits.shape , 'labels->',labels.shape)\r\n            train_step(logits,labels)\r\n            \r\n        if (epoch + 1) % 15 == 0:\r\n            checkpoint.save(file_prefix = checkpoint_prefix)\r\n        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\r\n\r\n\r\n%%time\r\ntrain(train_dataset,train_labels_dataset, EPOCHS)\r\n```", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to depict the problem. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "I have also meet the problem, if I removed the `@tf.function` above the `train_step` function, the model ran successfully. However, when I added the `@tf.function` above the `train_step` function, error came. The error was: `ValueError: No gradients provided for any variable`.\r\n\r\nThis is my code:\r\n```\r\ndef train():\r\n    loss_fn = LOSS_DICT[ARGS.loss]\r\n    model, block_size = NETWORK_DICT[ARGS.network]()\r\n    dataset_train = read_data(\r\n        ARGS.batch_size_train, ARGS.label_file_train, True, ARGS.workers, block_size)\r\n\r\n    if not os.path.exists(ARGS.model_dir):\r\n        os.mkdir(ARGS.model_dir)\r\n    checkpoint = tf.train.latest_checkpoint(ARGS.model_dir)\r\n    begin_epoch = int(checkpoint.split('-')[-1])+1 if checkpoint else 1\r\n\r\n    logger = MyLogger(ARGS.record_path)\r\n    optimizer = tk.optimizers.SGD(learning_rate=ARGS.learning_rate)\r\n\r\n    @tf.function\r\n    def train_step(step, batch_data):\r\n        imgs, labels, imgs_width, labels_len = batch_data\r\n        with tf.GradientTape() as tape:\r\n            logits = model(imgs, training=True)\r\n            logits_len = tf.cast(tf.math.ceil(\r\n                tf.divide(imgs_width, block_size)), tf.int32)\r\n            loss, accuracy, prediction = loss_fn(\r\n                labels, logits, labels_len, logits_len, CLASSES)\r\n        grads = tape.gradient(loss, model.trainable_variables)\r\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n        loss_mean = np.mean(loss.numpy())\r\n        accuracy_mean = np.mean(accuracy.numpy())\r\n        print(f'Loss of step#{step+1} is {loss_mean} ...')\r\n        print(f'Accruacy of step#{step+1} is {accuracy_mean} ...')\r\n\r\n    for epoch in range(begin_epoch, ARGS.epoches+begin_epoch):\r\n        print(f'Epoch {epoch} start ...')\r\n        for step, batch_data in enumerate(dataset_train):\r\n            train_step(step, batch_data)\r\n\r\n        quit()\r\n        print(f'Epoch {epoch} finish ...')\r\n    logger.close()\r\n    # delete_not_best_models(ARGS.model_prefix, best_test_epoch)\r\n```\r\n\r\n@achandraa Could you please help me, thanks.\r\n", "@jiarenyf : Apology for delayed response. Please open a new issue if you are still stuck by following the [Template.](https://github.com/tensorflow/tensorflow/issues/new/choose) \r\n\r\nMeanwhile let me close this issue due to lack of recent activities. Thanks!"]}, {"number": 28835, "title": "No gradients provided for any variable: ['cnn_model_14/conv2d_42/kernel:0', 'cnn_model_14/conv2d_42/bias:0', 'cnn_model_14/conv2d_43/kernel:0', 'cnn_model_14/conv2d_43/bias:0', 'cnn_model_14/dense_28/kernel:0', 'cnn_model_14/dense_28/bias:0', 'cnn_model_14/dense_29/kernel:0', 'cnn_model_14/dense_29/bias:0'].", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@SlowMonk \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "@SlowMonk Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28834, "title": "[ROCm] Fix for the broken `--config=rocm` build.", "body": "This PR contains a fix for the broken `--config=rocm` build.\r\n\r\nCurrently the file `tensorflow/core/util/gpuLaunch_config.h` is not included in any code that is enabled for the `--config=rocm` build. Once that is included within ROCm enabled code, it will break the `--config=rocm` build, because that file currrently contains a couple of Cuda* names that should not be visible in the ROCm build. This commit/PR fixes that.\r\n\r\n-------------------\r\n\r\n@tatianashp , @whchung , @chsigg \r\n\r\nThis is a trivial change that only affects the ROCm build...please review and merge\r\n", "comments": ["The failures in the `Linux GPU` and other CI runs do not seem to be related to the changes in this commit (which are only applicable to the `--config=rocm` build)", "@chsigg a gentle ping on this PR as we do wish to ensure `--config=rocm` always build in TF mainline."]}, {"number": 28833, "title": "TypeError: cannot unpack non-iterable NoneType object", "body": "loss = tf.losses.mean_squared_error(labels,logits)\r\ncnn_optimizer = tf.keras.optimizers.Adam(1e-4)\r\n\r\n# Notice the use of `tf.function`\r\n# This annotation causes the function to be \"compiled\".\r\n#@tf.function\r\ndef train_step(logits,labels):    \r\n    with tf.GradientTape() as cnn_tape:\r\n        print(\"train_step\")\r\n        loss = tf.losses.mean_squared_error(labels,logits)\r\n        #print(\"loss\",loss)\r\n        gradient_of_cnn = cnn_tape.gradient(loss,model.trainable_variables)\r\n        cnn_optimizer.apply_gradients(gradient_of_cnn)\r\n\r\ndef train(train_dataset,train_labels_dataset,epochs):\r\n    for epoch in range(epochs):\r\n        start = time.time()\r\n        \r\n        for train_batch,label_batch in zip(train_dataset,train_labels_dataset):\r\n            print(train_batch.shape,label_batch.shape)\r\n            logits = tf.argmax(train_batch)\r\n            labels = label_batch\r\n            train_step(logits,labels)\r\n            \r\n        if (epoch + 1) % 15 == 0:\r\n            checkpoint.save(file_prefix = checkpoint_prefix)\r\n        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\r\n\r\n%%time\r\ntrain(train_dataset,train_labels_dataset, EPOCHS)\r\n\r\n\r\n\r\n(256, 28, 28, 1) (256,)\r\ntrain_step\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<timed eval> in <module>()\r\n\r\n<ipython-input-16-6e197a4fd13b> in train(train_dataset, train_labels_dataset, epochs)\r\n      7             logits = tf.argmax(train_batch)\r\n      8             labels = label_batch\r\n----> 9             train_step(logits,labels)\r\n     10 \r\n     11         if (epoch + 1) % 15 == 0:\r\n\r\n<ipython-input-15-c1d555ba6f91> in train_step(logits, labels)\r\n      8         #print(\"loss\",loss)\r\n      9         gradient_of_cnn = cnn_tape.gradient(loss,model.trainable_variables)\r\n---> 10         cnn_optimizer.apply_gradients(gradient_of_cnn)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name)\r\n    394       ValueError: If none of the variables have gradients.\r\n    395     \"\"\"\r\n--> 396     grads_and_vars = _filter_grads(grads_and_vars)\r\n    397     var_list = [v for (_, v) in grads_and_vars]\r\n    398 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)\r\n    914   filtered = []\r\n    915   vars_with_empty_grads = []\r\n--> 916   for grad, var in grads_and_vars:\r\n    917     if grad is None:\r\n    918       vars_with_empty_grads.append(var)\r\n\r\nTypeError: cannot unpack non-iterable NoneType object\r\n\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact code snippet to depict the problem. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "def gradient_descent(init_m,init_c,x,t,learning_rate,iterations,error_threshold):\r\n    m=init_m\r\n    c=init_c\r\n    error_values=list()\r\n    mc_values=list()\r\n    for i in range(iterations):\r\n        e=error(m,x,c,t)\r\n        if e<error_threshold:\r\n            print('ERROR')\r\n            break\r\n            error_values.append(e)\r\n            m,c=update(m,x,c,t,learning_rate)\r\n            mc_values.append((m,c))\r\n            return m,c,error_values,mc_values\r\ninit_m=0.9\r\ninit_c=0\r\nlearning_rate=0.001\r\niterations=100\r\nerror_threshold=0.001\r\n\r\nm,c,error_values,mc_values= gradient_descent(init_m, init_c, xtrain, ytrain,learning_rate,iterations,error_threshold)\r\n\r\n**I am getting this error please help**\r\n\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-26-b291d8eef5c0> in <module>\r\n      5 error_threshold=0.001\r\n      6 \r\n----> 7 m,c,error_values,mc_values= gradient_descent(init_m, init_c, xtrain, ytrain,learning_rate,iterations,error_threshold)\r\n\r\nTypeError: cannot unpack non-iterable NoneType object\r\n\r\n\r\n\r\n", "> def gradient_descent(init_m,init_c,x,t,learning_rate,iterations,error_threshold):\r\n> m=init_m\r\n> c=init_c\r\n> error_values=list()\r\n> mc_values=list()\r\n> for i in range(iterations):\r\n> e=error(m,x,c,t)\r\n> if e<error_threshold:\r\n> print('ERROR')\r\n> break\r\n> error_values.append(e)\r\n> m,c=update(m,x,c,t,learning_rate)\r\n> mc_values.append((m,c))\r\n> return m,c,error_values,mc_values\r\n> init_m=0.9\r\n> init_c=0\r\n> learning_rate=0.001\r\n> iterations=100\r\n> error_threshold=0.001\r\n> \r\n> m,c,error_values,mc_values= gradient_descent(init_m, init_c, xtrain, ytrain,learning_rate,iterations,error_threshold)\r\n> \r\n> **I am getting this error please help**\r\n> \r\n> TypeError Traceback (most recent call last)\r\n> in\r\n> 5 error_threshold=0.001\r\n> 6\r\n> ----> 7 m,c,error_values,mc_values= gradient_descent(init_m, init_c, xtrain, ytrain,learning_rate,iterations,error_threshold)\r\n> \r\n> TypeError: cannot unpack non-iterable NoneType object\r\n\r\nHi Anshika. I had the same problem. Just found out that my indents in defining the update and gradient descent functions were incorrect. Indents are the spaces at the start of a command. Usually, they're taking care of by python but make sure that the return function is properly indented. Tell me if this solves your problem."]}, {"number": 28832, "title": "bazel still report error: name 'http_archive' is not defined", "body": "I'm experiencing the same error described in issue #27189, when I try to build TF v1.12.2 using Bazel 0.25.2 on a Jetson TX2\r\n```\r\n$ export TF_NEED_CUDA=1\r\n$ export TF_CUDA_VERSION=9.0\r\n$ export CUDA_TOOLKIT_PATH=/usr/local/cuda\r\n$ export TF_CUDNN_VERSION=7.1.5\r\n$ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu/\r\n$ export TF_CUDA_COMPUTE_CAPABILITIES=6.2\r\n\r\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nERROR: /home/nvidia/git/tensorflow/WORKSPACE:3:1: name 'http_archive' is not defined\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package\r\nINFO: Elapsed time: 0.222s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```", "comments": ["@IeiuniumLux Can you please let us know if you have followed the steps mentioned in the tensorflow website [link](https://www.tensorflow.org/install/source#tested_build_configurations) ,also please install relevant bazel version as mentioned in the link and mention the OS platform details.", "@muddham Yes, those are the exact steps I'm following.  The link doesn't list the Bazel version for TF v1.12.2 though.  Are you saying that I should use Bazel 0.15.0 instead of 0.25.2?", "For v.12.2 you should use Bazel 0.15.0.\r\n\r\nRunning `./configure` before building is always a good idea as there are checks to ensure the toolchain you're using is compatible with the code.", "@mihaimaruseac I did run the ./configure before building it.  Anyhow, let's me try with Bazel 0.15.0 and report back the results. Thanks!", "@IeiuniumLux Please try installing relevant bazel 0.15.0 also refer the [link](https://github.com/tensorflow/tensorflow/issues/27189)", "With Bazel 0.15.0, the build got further.  However, a little bit after a hour building, it failed with this error.  Any suggestion?\r\n```\r\nERROR: /home/nvidia/git/tensorflow/tensorflow/contrib/lite/kernels/internal/BUILD:455:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels/internal:tensor_utils' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    LD_LIBRARY_PATH=/home/nvidia/ros_ws/devel/lib:/opt/ros/kinetic/lib:/opt/ros/kinetic/lib/aarch64-linux-gnu \\\r\n    PATH=/opt/ros/kinetic/bin:/usr/local/cuda-9.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.2 \\\r\n    TF_CUDA_VERSION=9.0 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NCCL_VERSION=1 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/arm-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.pic.d '-frandom-seed=bazel-out/arm-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/arm-opt/genfiles -iquote external/com_google_absl -iquote bazel-out/arm-opt/genfiles/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/arm-opt/genfiles/external/bazel_tools -iquote external/arm_neon_2_x86_sse -iquote bazel-out/arm-opt/genfiles/external/arm_neon_2_x86_sse -iquote external/gemmlowp -iquote bazel-out/arm-opt/genfiles/external/gemmlowp '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '-march=native' -O3 '-mfpu=neon' -c tensorflow/contrib/lite/kernels/internal/tensor_utils.cc -o bazel-out/arm-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.pic.o)\r\ngcc: error: unrecognized command line option '-mfpu=neon'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 5924.627s, Critical Path: 227.50s\r\nINFO: 5436 processes: 5436 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "What's the output of `gcc --version`?", "@mihaimaruseac \r\ngcc (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609", "I guess the option forward is to try removing `\"-mpfu=neon\"` lines from https://github.com/tensorflow/tensorflow/blob/6b634657d8ff1355132c3838271e4f569d1ffaba/tensorflow/contrib/lite/kernels/internal/BUILD#L21-L37\r\n", "Also, if you want, can you also post the output of `gcc -dumpmachine`? That tells you what GCC can build for.", "@mihaimaruseac \r\n\r\nnvidia@tegra-ubuntu:~$ gcc -dumpmachine\r\naarch64-linux-gnu\r\n\r\nI'm going to build it without the \"-mpfu=neon\" option. But before, let me ask a few questions. \r\n\r\nDo I need to run `./configure` before building it again?\r\n\r\nIs it normal for the build to take more than a hour to complete on a Nvidia TX2?\r\n\r\nFinally, is a [NCCL](https://docs.nvidia.com/deeplearning/sdk/nccl-install-guide/index.html) required by TF to build?\r\n\r\nBTW, thank you for your assistance with this.\r\n", "Ok, so for AArch64 `-mfpu=neon` is not needed as neon is mandatory and included by default. Still dubious that the compiler would complain instead of silently ignoring but not the topic to discuss that here.\r\n\r\nI think it is better to run `./configure` again as you changed some options. Though these seem to be in Bazel files, so probably there won't be any change. Better safe though, so I recommend rerunning.\r\n\r\nI got builds taking 4-5 hours at one point, it all depends on how many CPUs and how powerful they are. The codebase is quite big.\r\n\r\nYou can build without NCCL. In fact, many times I built without any GPU at all.", "With -mfpu=neon removed, the build ran longer, BUT after almost 6 hours running, then it failed with some kind of native TensorFlow runtime error:\r\n\r\n```\r\nERROR: /home/nvidia/git/tensorflow/tensorflow/BUILD:533:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash failed: error executing command \r\n  (cd /home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/home/nvidia/ros_ws/devel/lib:/opt/ros/kinetic/lib:/opt/ros/kinetic/lib/aarch64-linux-gnu \\\r\n    PATH=/opt/ros/kinetic/bin:/usr/local/cuda-9.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/host/genfiles/tensorflow_api/v1/ --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow._api.v1 bazel-out/host/genfiles/tensorflow/_api/v1/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/app/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/bitwise/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/compat/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/data/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/data/experimental/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/debugging/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/distributions/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/dtypes/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/errors/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/feature_column/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/gfile/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/graph_util/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/image/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/io/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/initializers/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/activations/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/applications/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/applications/densenet/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/applications/inception_resnet_v2/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/applications/inception_v3/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/applications/mobilenet/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/applications/mobilenet_v2/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/applications/nasnet/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/applications/resnet50/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/applications/vgg16/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/applications/vgg19/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/applications/xception/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/backend/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/callbacks/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/constraints/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/datasets/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/datasets/boston_housing/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/datasets/cifar10/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/datasets/cifar100/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/datasets/fashion_mnist/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/datasets/imdb/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/datasets/mnist/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/datasets/reuters/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/estimator/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/initializers/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/layers/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/losses/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/metrics/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/models/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/optimizers/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/preprocessing/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/preprocessing/image/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/preprocessing/sequence/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/preprocessing/text/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/regularizers/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/utils/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/wrappers/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/keras/wrappers/scikit_learn/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/layers/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/linalg/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/logging/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/losses/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/manip/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/math/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/metrics/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/nn/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/nn/rnn_cell/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/profiler/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/python_io/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/quantization/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/random/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/resource_loader/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/strings/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/saved_model/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/saved_model/builder/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/saved_model/constants/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/saved_model/loader/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/saved_model/main_op/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/saved_model/signature_constants/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/saved_model/signature_def_utils/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/saved_model/tag_constants/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/saved_model/utils/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/sets/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/sparse/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/spectral/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/summary/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/sysconfig/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/test/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/train/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/train/queue_runner/__init__.py bazel-out/host/genfiles/tensorflow/_api/v1/user_ops/__init__.py')\r\nTraceback (most recent call last):\r\n  File \"/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws11Environment6GetEnvB5cxx11EPKc\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws11Environment6GetEnvB5cxx11EPKc\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 25367.470s, Critical Path: 430.44s\r\nINFO: 8521 processes: 8521 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "At this point I would try recloning the branch on a different directory and recompiling again. Perhaps something else changed?", "Any updates? Did recloning and recompiling work?", "@mihaimaruseac,  I re-cloned the branch on a different directory and recompiled again.  Unfortunately, it crashed because it ran out of disk space after 6 hours into the build.\r\n\r\nI'm now going to build it on another TX2 system.  I'll post an update.\r\n\r\nBTW, should I build TF-1.13.1 with Bazel 0.19.2 instead TF-1.12.2 with Bazel 0.15.0?\r\n\r\nOR, should I save myself time and just install the version from Nvidia ?\r\n\r\nhttps://developer.nvidia.com/embedded/downloads#?search=tensorflow", "I'd recommend to use the newest version if you want to compile by yourself.\r\n\r\nIf you can use a precompiled binary, that's better", "@mihaimaruseac By newest version, do you mean TF-1.13.1 ?  \r\n\r\nAnd by a precompiled binary, do you mean install the version from [Nvidia](https://developer.nvidia.com/embedded/downloads#?search=tensorflow) and forget about building it myself?", "Yes to both of the questions. For the first one, you can also try master.", "@mihaimaruseac In that case, then I'm going to go with the TF-1.13.1 precompiled binary version from Nvidia and save myself from the trouble of spending 6+ hours building it from source.  Thank you for time and assistance with this."]}, {"number": 28831, "title": "tf-nightly-gpu 2.0 very slow on tape.gradient()", "body": "While migrating to tensorflow 2.0 I found a big performance issue.\r\n\r\n```\r\n@tf.function\r\ndef onTrainStep(self, data, training=True):\r\n\r\n    images, labels = data\r\n\r\n    with tf.GradientTape() as tape:\r\n        loss, predictions = self.seq2seq(images, labels, training)\r\n\r\n    # Calculate the total probability of the output string.\r\n    probability = tf.nn.softmax(predictions)\r\n\r\n    ggn = 0\r\n    if training:\r\n        params = self.seq2seq.trainable_variables\r\n\r\n        gradients = tape.gradient(loss, params)\r\n        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\r\n        ggn = tf.linalg.global_norm(gradients)\r\n\r\n        # update op - apply gradients\r\n        self.optimizer.apply_gradients(zip(gradients, params))\r\n\r\n    return loss, predictions, probability, ggn\r\n\r\n```\r\n### tensorflow-gpu 1.13.1: \r\ntakes 0.7 seconds\r\n### tensorflow-gpu 2.0 nightly\r\ntakes 2.4 seconds\r\n\r\n### removing `gradients = tape.gradient(loss, params)` and `ggn = tf.linalg.global_norm(gradients)`:\r\ntakes 0.3 seconds\r\n\r\nThat means `gradients = tape.gradient(loss, params)` is consuming most of the time\r\n\r\n", "comments": ["I also noticed that tensorflow2.0a is  much faster than the latest nightly build with keras when updating the weights.", "Can you add full instructions to reproduce?\r\n\r\nThis feels like a serious problem but it could be caused by many different things, and so having working code we can debug would be very helpful.", "Or if you don't want to share your code if you're willing to run a git bisect and share the results with us that'd also be great.", "@alextp, my full code is in this notebook:\r\nhttps://github.com/alew3/huia_experience/blob/master/02_train/train_huia_poses_keras_tf2.ipynb\r\n\r\nI timed this snippet of training as an example, it is running 3 times slower in the nightly. Visually the slow down seems to happen when it starts updating the weights.\r\n\r\nstart = datetime.now()\r\nmodel.fit(train_dataset, epochs=epochs,steps_per_epoch=steps_per_epoch,verbose=1,validation_data=test_dataset,\r\n          callbacks=callbacks)\r\nend = datetime.now()\r\nprint(f\"total time of {end-start} for {epochs} epochs, tensorflow version={tf.__version__}\")\r\n\r\n\r\nand run twice the code for each version of tensorflow:\r\n**TF2 Alpha**\r\ntotal time of **0:01:00.676783** for 5 epochs, tensorflow version=2.0.0-alpha0\r\ntotal time of **0:01:02.069855** for 5 epochs, tensorflow version=2.0.0-alpha0\r\n\r\n**TF2 Nightly**\r\ntotal time of **0:03:20.682476** for 5 epochs, tensorflow version=2.0.0-dev20190521\r\ntotal time of **0:03:21.514782** for 5 epochs, tensorflow version=2.0.0-dev20190521\r\n\r\n\r\n\r\n\r\n\r\n", "Can you give me a shorter example to reproduce? I can't quite run your notebook as I don't have the training data or the filesystem setup in the way you do.", "@alextp , I've cut down the code and included the training data in two different collabs to make things easier to reproduce, with the only change being the version of Tensorflow 2 used. Tensorflow Nightly is 3x slower.\r\n\r\n**TensorFlow 2 Alpha GPU**\r\nhttps://colab.research.google.com/drive/1AOhKl18zI-W1YZ2BPTHO_e0zezUgw6Jh#scrollTo=OaNzmAVbosq0\r\nrun 1. total time of **0:05:39.198438** for 5 epochs, tensorflow version=2.0.0-alpha0\r\nrun 2. total time of **0:05:30.281252** for 5 epochs, tensorflow version=2.0.0-alpha0\r\n\r\n**Tensorflow 2 Nightly GPU 2.0.0-dev20190522**\r\nhttps://colab.research.google.com/drive/1Z9M_ovwlUjHUiswW3a29aS67CsaOHjWo#scrollTo=3cVLAo9Mospi\r\nrun 1. total time of **0:15:31.170586** for 5 epochs, tensorflow version=2.0.0-dev20190522\r\n", "Enabling device logging during model.fit(), I see that it is mostly running on the GPU. But I'm not sure what **Executing op ExperimentalDatasetCardinality in device /job:localhost/replica:0/task:0/device:CPU:0** or\r\n**Executing op __inference_keras_scratch_graph_264441 in device unspecified** do, but they happen when things start to slow down.\r\n\r\n\r\nExecuting op CloseSummaryWriter in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nEpoch 1/20\r\nExecuting op ExpandDims in device /job:localhost/replica:0/task:0/device:GPU:0\r\nW0522 20:40:51.144862 140192919144256 deprecation.py:323] From /home/ale/anaconda3/envs/tensor20/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op __inference_keras_scratch_graph_13159 in device <unspecified>\r\n68/70 [============================>.] - ETA: 0s - loss: 0.8524 - accuracy: 0.7154**Executing op ExperimentalDatasetCardinality in device /job:localhost/replica:0/task:0/device:CPU:0**\r\n**Executing op __inference_keras_scratch_graph_14812 in device unspecified**\r\nExecuting op WriteScalarSummary in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op WriteScalarSummary in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op WriteHistogramSummary in device /job:localhost/replica:0/task:0/device:CPU:0\r\n70/70 [==============================] - 68s 973ms/step - loss: 0.8384 - accuracy: 0.7196 - val_loss: 1.9080 - val_accuracy: 0.3720", "@iganichev could this be related to placer changes?\r\n\r\n", "Unlikely. The new placer should have the same placement decisions (safe for some rare corner cases) for graphs without functions outputting resources on multiple devices. It is unlikely that keras produces such graphs. Also, Dataset ops have not been integrated into the recursive placement framework.", "> It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?\r\n\r\nThe issue persists. Very slow updating gradients on the nightly and beta compared to the alpha.", "The issue persists.", "@robieta , the notebook which is slow seems to be using model.fit. Do you know what part of it changed between alpha and nightly that could cause this?\r\n\r\n@alew3 this is still a very large example to reproduce, and the notebook you shared doesn't overlap at all in terms of code with the thing starting this post. It's quite hard to understand what the actual problem is.", "@alextp sorry, I tried to cut down the code to the essential. It is just a simple image classification training via transfer learning using mobilenet with keras and tf.data.dataset as recommended by Tensorflow guides. But the slow down is very noticeable on the newer versions of TF2 when it updates the weights. It is so slow, that makes it  impracticable for real work. I can't guarantee it is the same reason as the original poster's problem.", "For example, can you reproduce the slowdown with just a constant size numpy array? Can you reproduce with a smaller keras model?\r\n\r\nIf you can reproduce with a numpy array with random numbers instead of a real dataset this gets much easier for me to reproduce. Similarly, the smallest the model that is slow the easier it is for me to know what is going on.", "I would echo @alextp's point about synthetic data. It's often a source of much complexity in a model, and if an issue reproduces with a synthetic pipeline it drastically narrows down the possible causes. (And a similar line of reasoning goes for models. Complex topologies are hard to debug.) Other than that, however, I thought it was a very high quality repro and much appreciated. I've tweaked it to use a synthetic pipeline: https://colab.research.google.com/gist/robieta/0c8a007cf1ec2181872511b1367809e7/tf_issue_28831_repro.ipynb\r\n\r\nThe issue is this bit here:\r\n```\r\n# skip 20% validation images\r\ntest_dataset = ds.take(838) \r\ntrain_dataset = ds.skip(838)\r\n```\r\n\r\nThere are 4191 examples, hence `20% * 4191 = 838`. However at this time the dataset __is already batched__, so at a batch size of 48 that actually becomes `838 batches = 48 * 838 examples = 9.6 epochs`. So you are actually telling the dataset to process nearly 10 epochs before starting validation. (The results of skip are not used by test_dataset; they are simply discarded) This of course also has the unfortunate effect that it does not preserve the train / test split. Generally I would recommend making the split in the dataset as early as possible, as it tends to be both more performant and less error prone.\r\n\r\nThe reason that this is showing up now is that there have been some changes to how Model.fit interacts with datasets, so the dataset is making a new iterator each time. (As opposed to the alpha code where the inefficiency was only in the first epoch.) @tomerk Is iterator reuse across epochs via the `steps_per_epoch` argument on your radar?\r\n\r\nThat said, I think the original issue reported by @nicoosokhan seems to be different from the behavior observed by @alew3. @nicoosokhan if you can provide a minimal repro colab we would be very interested to see what's going on with GradientTape.", "@qlzh727 For the `steps_per_epoch` iterator reuse behavior as well.", "@robieta thanks for the feedback, I also tried running my code on multiple gpus with mirror distribution strategy and was running out of data, so that's probably related!\r\n\r\n", "I'm going to go ahead and close this out since it seems to be resolved. Feel free to reopen if I missed something.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28831\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28831\">No</a>\n"]}, {"number": 28830, "title": "Add Tensorflow Lite GPU Delegates for the desktop", "body": "**System information**\r\n- TensorFlow version (you are using): Tensorflow2.0alpha\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nTensorflow lite only has GPU delegates for iOS and Android devices.\r\n\r\nIn theory, Tensorflow lite should be able to use desktop Opengles apis through GLAD.\r\n\r\n**Will this change the current api? How?**\r\n\r\nThis will use the existing interfaces for gpu delegates.\r\n\r\n**Who will benefit with this feature?**\r\n\r\n1. Tensorflow lite will be able to be gpu accelerated on desktops\r\n1. Users of Tensorflow lite can have the same acceleration on the desktop and the mobile devices for ease of development.\r\n1. Nvidia Tegra kits are a desktop-like platform that isn't an Android or iOS device but is able to run compute shaders.\r\n1. Inference on the GPU may now run fast enough and now become suitable for real-time applications compared to the CPU.", "comments": ["@fire \r\n\r\nOh my apologies.  This issue somehow dropped from my radar, and I haven't responded for very long.\r\n\r\nYes, we have had successful running of the TFLite GPU on desktop with mesa.  It wasn't robust, e.g. I couldn't make it to run, but some of my colleagues were able to.\r\n\r\nOur team is pretty short staffed, and have all our forces on supporting mobile devices.  And even so, we have trouble fixing Mali performance (which is going on for over a month now).  I'm sure we can somehow make it work for desktop, but that goes with supporting the community, and we just don't have the resources to do so.\r\n\r\nFor now, I will close this issue as infeasible.", "I have here fixed gpu delegate to run on windows with angle library.\r\n", "Is it testable? Can you link to a git branch? I am interested.", "Can anyone give a short summary of how tflite runs on GPU using mesa as used in Mediapipe project?", "![image](https://user-images.githubusercontent.com/32321/80245575-cf678e80-861f-11ea-83a8-80da530f77ad.png)\r\n", "Haha good one.  MediaPipe uses\r\n\r\n`--copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11`\r\n\r\nto bypass the X11 Status issue."]}, {"number": 28829, "title": "FAILED: Build did NOT complete successfully", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.2**\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: git master, should be version 1.13.1\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: pip \r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0/7.5.1\r\n- GPU model and memory: Geforce 980M\r\n\r\n\r\n\r\n```console\r\nINFO: From Compiling tensorflow/core/grappler/utils/functions.cc [for host]:\r\nIn file included from tensorflow/core/grappler/utils/functions.cc:33:0:\r\n./tensorflow/core/grappler/utils.h: In function 'int tensorflow::grappler::NodePositionIfSameNode(const string&, const string&)':\r\n./tensorflow/core/grappler/utils.h:134:49: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       std::distance(input_it, input_name.end()) < node_name.size()) {\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/utils/functions.cc: In function 'tensorflow::Status tensorflow::grappler::ReplaceInputWithConst(const tensorflow::NodeDef&, int, tensorflow::grappler::GrapplerFunctionItem*)':\r\ntensorflow/core/grappler/utils/functions.cc:304:38: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (input_index < 0 || input_index >= item->input_size()) {\r\n                          ~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/utils/functions.cc: In function 'tensorflow::Status tensorflow::grappler::RemoveFunctionOutputs(const absl::flat_hash_set<int>&, tensorflow::grappler::GrapplerFunctionItem*, std::vector<std::pair<int, int> >*)':\r\ntensorflow/core/grappler/utils/functions.cc:344:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (remove_output < 0 || remove_output >= item->output_size()) {\r\n                              ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/grappler/utils/functions.cc:356:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < item->output_size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/core/lib/core/errors.h:21,\r\n                 from ./tensorflow/core/framework/tensor_shape.h:23,\r\n                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/function.h:22,\r\n                 from ./tensorflow/core/grappler/utils/functions.h:26,\r\n                 from tensorflow/core/grappler/utils/functions.cc:15:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:470:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:444:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:444:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nERROR: ....../tensorflow/tensorflow/core/BUILD:3467:1: C++ compilation of rule '//tensorflow/core:gpu_runtime_impl' failed (Exit 1)\r\nIn file included from tensorflow/core/common_runtime/gpu/gpu_device.cc:81:0:\r\nbazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/cuda_config.h:19:44: error: expected '}' before numeric constant\r\n #define TF_CUDA_CAPABILITIES \"CudaVersion(\"5.2\")\"\r\n                                            ^\r\nbazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/cuda_config.h:19:44: note: in definition of macro 'TF_CUDA_CAPABILITIES'\r\n #define TF_CUDA_CAPABILITIES \"CudaVersion(\"5.2\")\"\r\n                                            ^~~\r\nbazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/cuda_config.h:19:44: error: could not convert '{\"CudaVersion(\"}' from '<brace-enclosed initializer list>' to 'std::vector<tensorflow::{anonymous}::CudaVersion>'\r\n #define TF_CUDA_CAPABILITIES \"CudaVersion(\"5.2\")\"\r\n                                            ^\r\nbazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/cuda_config.h:19:44: note: in definition of macro 'TF_CUDA_CAPABILITIES'\r\n #define TF_CUDA_CAPABILITIES \"CudaVersion(\"5.2\")\"\r\n                                            ^~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1575:1: error: 'Status' does not name a type; did you mean 'static'?\r\n Status BaseGPUDeviceFactory::EnablePeerAccess(\r\n ^~~~~~\r\n static\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1619:1: error: 'Status' does not name a type; did you mean 'static'?\r\n Status BaseGPUDeviceFactory::GetValidDeviceIds(\r\n ^~~~~~\r\n static\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1759:1: error: 'uint64' does not name a type; did you mean 'uint4'?\r\n uint64 BaseGPUDevice::SafeAllocFrontier(uint64 old_value) {\r\n ^~~~~~\r\n uint4\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1767:5: error: 'BaseGPUDevice' has not been declared\r\n int BaseGPUDevice::PendingKernels() {\r\n     ^~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc: In function 'int PendingKernels()':\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1768:7: error: 'kernel_tracker_' was not declared in this scope\r\n   if (kernel_tracker_) {\r\n       ^~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc: At global scope:\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1774:1: error: 'uint64' does not name a type; did you mean 'uint4'?\r\n uint64 GPUKernelTracker::MaybeQueue(OpKernelContext* ctx) {\r\n ^~~~~~\r\n uint4\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1800:6: error: 'GPUKernelTracker' has not been declared\r\n void GPUKernelTracker::RecordQueued(uint64 queued_count, int weight) {\r\n      ^~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1800:37: error: variable or field 'RecordQueued' declared void\r\n void GPUKernelTracker::RecordQueued(uint64 queued_count, int weight) {\r\n                                     ^~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1800:37: error: 'uint64' was not declared in this scope\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1800:37: note: suggested alternatives:\r\nIn file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:46:0,\r\n                 from external/protobuf_archive/src/google/protobuf/io/coded_stream.h:135,\r\n                 from bazel-out/host/genfiles/tensorflow/core/lib/core/error_codes.pb.h:23,\r\n                 from ./tensorflow/core/lib/core/status.h:23,\r\n                 from ./tensorflow/core/common_runtime/device_factory.h:22,\r\n                 from tensorflow/core/common_runtime/gpu/gpu_device.cc:37:\r\nexternal/protobuf_archive/src/google/protobuf/stubs/port.h:156:18: note:   'google::protobuf::uint64'\r\n typedef uint64_t uint64;\r\n                  ^~~~~~\r\nIn file included from ./tensorflow/core/platform/types.h:29:0,\r\n                 from ./tensorflow/core/platform/default/logging.h:25,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/core/common_runtime/device_factory.h:22,\r\n                 from tensorflow/core/common_runtime/gpu/gpu_device.cc:37:\r\n./tensorflow/core/platform/default/integral_types.h:32:28: note:   'tensorflow::uint64'\r\n typedef unsigned long long uint64;\r\n                            ^~~~~~\r\n./tensorflow/core/platform/default/integral_types.h:32:28: note:   'tensorflow::uint64'\r\n./tensorflow/core/platform/default/integral_types.h:32:28: note:   'tensorflow::uint64'\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1800:58: error: expected primary-expression before 'int'\r\n void GPUKernelTracker::RecordQueued(uint64 queued_count, int weight) {\r\n                                                          ^~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1847:6: error: 'GPUKernelTracker' has not been declared\r\n void GPUKernelTracker::MaybeQueueProgressEvent() {\r\n      ^~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc: In function 'void MaybeQueueProgressEvent()':\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1848:3: error: 'mutex_lock' was not declared in this scope\r\n   mutex_lock l(mu_);\r\n   ^~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1848:3: note: suggested alternative:\r\nIn file included from ./tensorflow/core/platform/mutex.h:31:0,\r\n                 from ./tensorflow/core/platform/default/notification.h:24,\r\n                 from ./tensorflow/core/platform/notification.h:26,\r\n                 from ./tensorflow/core/lib/core/notification.h:21,\r\n                 from ./tensorflow/core/common_runtime/gpu/gpu_event_mgr.h:24,\r\n                 from ./tensorflow/core/common_runtime/gpu/gpu_device.h:30,\r\n                 from tensorflow/core/common_runtime/gpu/gpu_device.cc:38:\r\n./tensorflow/core/platform/default/mutex.h:63:23: note:   'tensorflow::mutex_lock'\r\n class SCOPED_LOCKABLE mutex_lock {\r\n                       ^~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1849:7: error: 'num_pending_' was not declared in this scope\r\n   if (num_pending_ == 0) {\r\n       ^~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1849:7: note: suggested alternative: 'sigpending'\r\n   if (num_pending_ == 0) {\r\n       ^~~~~~~~~~~~\r\n       sigpending\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1850:5: error: 'uint64' was not declared in this scope\r\n     uint64 new_count = timing_counter_->next();\r\n     ^~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1850:5: note: suggested alternatives:\r\nIn file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:46:0,\r\n                 from external/protobuf_archive/src/google/protobuf/io/coded_stream.h:135,\r\n                 from bazel-out/host/genfiles/tensorflow/core/lib/core/error_codes.pb.h:23,\r\n                 from ./tensorflow/core/lib/core/status.h:23,\r\n                 from ./tensorflow/core/common_runtime/device_factory.h:22,\r\n                 from tensorflow/core/common_runtime/gpu/gpu_device.cc:37:\r\nexternal/protobuf_archive/src/google/protobuf/stubs/port.h:156:18: note:   'google::protobuf::uint64'\r\n typedef uint64_t uint64;\r\n                  ^~~~~~\r\nIn file included from ./tensorflow/core/platform/types.h:29:0,\r\n                 from ./tensorflow/core/platform/default/logging.h:25,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/core/common_runtime/device_factory.h:22,\r\n                 from tensorflow/core/common_runtime/gpu/gpu_device.cc:37:\r\n./tensorflow/core/platform/default/integral_types.h:32:28: note:   'tensorflow::uint64'\r\n typedef unsigned long long uint64;\r\n                            ^~~~~~\r\n./tensorflow/core/platform/default/integral_types.h:32:28: note:   'tensorflow::uint64'\r\n./tensorflow/core/platform/default/integral_types.h:32:28: note:   'tensorflow::uint64'\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1851:18: error: 'new_count' was not declared in this scope\r\n     RecordQueued(new_count, 1);\r\n                  ^~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1851:5: error: 'RecordQueued' was not declared in this scope\r\n     RecordQueued(new_count, 1);\r\n     ^~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1852:5: error: 'em_' was not declared in this scope\r\n     em_->ThenExecute(stream_,\r\n     ^~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1852:22: error: 'stream_' was not declared in this scope\r\n     em_->ThenExecute(stream_,\r\n                      ^~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1852:22: note: suggested alternative: 'strncmp'\r\n     em_->ThenExecute(stream_,\r\n                      ^~~~~~~\r\n                      strncmp\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1853:23: error: invalid use of 'this' in non-member function\r\n                      [this, new_count]() { RecordTerminated(new_count); });\r\n                       ^~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc: In lambda function:\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1853:61: error: 'new_count' is not captured\r\n                      [this, new_count]() { RecordTerminated(new_count); });\r\n                                                             ^~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1853:38: note: the lambda has no capture-default\r\n                      [this, new_count]() { RecordTerminated(new_count); });\r\n                                      ^\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1851:18: note: '<typeprefixerror>new_count' declared here\r\n     RecordQueued(new_count, 1);\r\n                  ^~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1853:44: error: 'RecordTerminated' was not declared in this scope\r\n                      [this, new_count]() { RecordTerminated(new_count); });\r\n                                            ^~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc: At global scope:\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1857:6: error: 'GPUKernelTracker' has not been declared\r\n void GPUKernelTracker::RecordTerminated(uint64 queued_count) {\r\n      ^~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1857:41: error: variable or field 'RecordTerminated' declared void\r\n void GPUKernelTracker::RecordTerminated(uint64 queued_count) {\r\n                                         ^~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1857:41: error: 'uint64' was not declared in this scope\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1857:41: note: suggested alternatives:\r\nIn file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:46:0,\r\n                 from external/protobuf_archive/src/google/protobuf/io/coded_stream.h:135,\r\n                 from bazel-out/host/genfiles/tensorflow/core/lib/core/error_codes.pb.h:23,\r\n                 from ./tensorflow/core/lib/core/status.h:23,\r\n                 from ./tensorflow/core/common_runtime/device_factory.h:22,\r\n                 from tensorflow/core/common_runtime/gpu/gpu_device.cc:37:\r\nexternal/protobuf_archive/src/google/protobuf/stubs/port.h:156:18: note:   'google::protobuf::uint64'\r\n typedef uint64_t uint64;\r\n                  ^~~~~~\r\nIn file included from ./tensorflow/core/platform/types.h:29:0,\r\n                 from ./tensorflow/core/platform/default/logging.h:25,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/core/common_runtime/device_factory.h:22,\r\n                 from tensorflow/core/common_runtime/gpu/gpu_device.cc:37:\r\n./tensorflow/core/platform/default/integral_types.h:32:28: note:   'tensorflow::uint64'\r\n typedef unsigned long long uint64;\r\n                            ^~~~~~\r\n./tensorflow/core/platform/default/integral_types.h:32:28: note:   'tensorflow::uint64'\r\n./tensorflow/core/platform/default/integral_types.h:32:28: note:   'tensorflow::uint64'\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1917:1: error: expected declaration before '}' token\r\n }  // namespace tensorflow\r\n ^\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1544:26: warning: 'std::vector<tensorflow::{anonymous}::CudaVersion> tensorflow::GetSupportedCudaComputeCapabilities()' defined but not used [-Wunused-function]\r\n std::vector<CudaVersion> GetSupportedCudaComputeCapabilities() {\r\n                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1484:12: warning: 'int tensorflow::GetMinGPUMultiprocessorCount(stream_executor::Platform*, const std::vector<tensorflow::gtl::IntType<tensorflow::PlatformGpuId_tag_, int> >&)' defined but not used [-Wunused-function]\r\n static int GetMinGPUMultiprocessorCount(\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1118.884s, Critical Path: 99.33s\r\nINFO: 3435 processes: 3435 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nDid anybody meet the same issue?\r\n\r\nCheers\r\nPei", "comments": ["@jiapei100 Can you please let us know if you have followed the steps mentioned in the tensorflow website [link](https://www.tensorflow.org/install/source#tested_build_configurations), also please install relevant bazel version as mentioned in the link.", "I think these two commits fix this issue - 4ae881f056287465184844395a5c8c63e40ee3cd and 85bc3808aa5f3cda02b96f1d1896cb22e61bb36c ", "@jiapei100 Did you get chance to check the links suggested by @npanpaliya.", "@jiapei100 Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28828, "title": "module 'tensorflow._api.v1.keras.layers' has no attribute 'Lamdba'", "body": "Cannot build my code due to \"module 'tensorflow._api.v1.keras.layers' has no attribute 'Lamdba'\" heres the code thats related to it:\r\n\r\n`classifier_layer = layers.Lamdba(classifier_layer, input_shape = IMAGE_SIZE*[3])\r\nclassifier_model = tf.keras.Sequential([classifier_layer])\r\nclassifier_mode.summary()`\r\n\r\nIm currently just \"Copy pasting\" / writing it from the Tensorflow tutorial \r\n(https://www.tensorflow.org/tutorials/images/hub_with_keras) to understand how it works, im very unsure why its not working for me since i used every import they used on the page aswell, looked up if i need to install another package (had to do that some times already) but unfortunally i didnt found anything.", "comments": ["> Cannot build my code due to \"module 'tensorflow._api.v1.keras.layers' has no attribute 'Lamdba'\" heres the code thats related to it:\r\n> \r\n> `classifier_layer = layers.Lamdba(classifier_layer, input_shape = IMAGE_SIZE*[3]) classifier_model = tf.keras.Sequential([classifier_layer]) classifier_mode.summary()`\r\n> \r\n> Im currently just \"Copy pasting\" / writing it from the Tensorflow tutorial\r\n> (https://www.tensorflow.org/tutorials/images/hub_with_keras) to understand how it works, im very unsure why its not working for me since i used every import they used on the page aswell, looked up if i need to install another package (had to do that some times already) but unfortunally i didnt found anything.\r\n\r\nHi! Can you provide the Version of Tensorflow, OS Version,etc. you are using? It would be nice if you close this issue and open another issue using the issue template, provided.", "Hello, sorry for the late reply, im gonna open a new issue.."]}, {"number": 28827, "title": "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.", "body": "I'm working on a text analytics project and encountered this issue. I don't have much experience with this. Please assist. \r\n\r\n![image](https://user-images.githubusercontent.com/17949213/57971131-863aa380-79a7-11e9-90b8-ef8f2969aa1a.png)\r\n", "comments": ["Deprecation messages means a particular service/component will not be there in future version as for example contrib is not there in TensorFlow version 2.0. For more information and better support for such issues, you can go to [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and can provide help faster for such issues. Thanks!\r\n"]}, {"number": 28826, "title": "Break the reference between closure tape_grad_fn and flat_result", "body": "Addresses [#27492](https://github.com/tensorflow/tensorflow/issues/27492)\r\nLooks eager execution mode doesn't have such problem, so no modifications there.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28826) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28826) for more info**.\n\n<!-- ok -->"]}, {"number": 28825, "title": "plzz help", "body": "Use standard file APIs to check for files with this prefix.\r\n\r\n```\r\nWARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py:232: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nWARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\nTraceback (most recent call last):\r\n  File \"export_inference_graph.py\", line 156, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\HP\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"export_inference_graph.py\", line 152, in main\r\n    write_inference_graph=FLAGS.write_inference_graph)\r\n  File \"C:\\Users\\HP\\Desktop\\RE\\models\\research\\object_detection\\exporter.py\", line 465, in export_inference_graph\r\n    write_inference_graph=write_inference_graph)\r\n  File \"C:\\Users\\HP\\Desktop\\RE\\models\\research\\object_detection\\exporter.py\", line 421, in _export_inference_graph\r\n    placeholder_tensor, outputs)\r\n  File \"C:\\Users\\HP\\Desktop\\RE\\models\\research\\object_detection\\exporter.py\", line 264, in write_saved_model\r\n    builder = tf.saved_model.builder.SavedModelBuilder(saved_model_path)\r\n  File \"C:\\Users\\HP\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\saved_model\\builder_impl.py\", line 425, in __init__\r\n    super(SavedModelBuilder, self).__init__(export_dir=export_dir)\r\n  File \"C:\\Users\\HP\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\saved_model\\builder_impl.py\", line 100, in __init__\r\n    \"directory: %s\" % export_dir)\r\nAssertionError: Export directory already exists. Please specify a different export directory: inference_graph\\saved_model\r\n```", "comments": ["a advice, be more specific to your problem, like state it in more formal and broader way, with proper indentation in you code as well as your output received, then people may help you in better way :)", "@lakshitha1629 Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "`AssertionError: Export directory already exists. Please specify a different export directory: inference_graph\\saved_model` tells you the issue: you either delete the `inference_graph\\saved_model` or specify a different one when running.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28824, "title": "Error Loading Package @io_bazel_rules_docker when building Tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.12.2 / master branch\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): gcc version 8.2.0 (Rev3, Built by MSYS2 project)\r\n- CUDA/cuDNN version: Cuda v10.1 - cudnn-9.2-windows10-x64-v7.5.1.10\r\n- GPU model and memory: NVidia 1060\r\n\r\n\r\n**Describe the problem**\r\nBuild failing when running `bazel build --config=opt //tools/pip_package:build_pip_package`\r\nwith error:\r\n```\r\nERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories'\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed the tutorial here: https://medium.com/@amsokol.com/how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-d047d9342b44\r\n\r\n```\r\n> python configure.py\r\nPython Location (default): C:\\Users\\Arise\\Miniconda3\\envs\\tensorflow-v1.10\\python.exe\r\nPython Libraries Path (default): C:\\Users\\Arise\\Miniconda3\\envs\\tensorflow-v1.10\\lib\\site-packages\r\nXLA JIT support: No\r\nROCm support: No\r\nCUDA support: Yes\r\nCUDA compute capabilities (default): 3.5,7.0\r\nOptimization Flags Bazel (default): --config=opt\r\nOverride Eigen Strong Inline (default): Yes\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n(tensorflow-v1.10) C:\\Users\\Arise\\tensorflow-build\\tensorflow>bazel build\r\nStarting local Bazel server and connecting to it...\r\nWARNING: Usage: bazel build <options> <targets>.\r\nInvoke `bazel help build` for full description of usage and options.\r\nYour request is correct, but requested an empty set of targets. Nothing will be built.\r\nINFO: An error occurred during the fetch of repository 'io_bazel_rules_docker'\r\nINFO: Call stack for the definition of repository 'io_bazel_rules_docker':\r\n - C:/users/arise/_bazel_arise/ji3ak47x/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - C:/users/arise/tensorflow-build/tensorflow/WORKSPACE:29:1\r\nERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):\r\n        File \"C:/users/arise/_bazel_arise/ji3ak47x/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 234\r\n                _clone_or_update(ctx)\r\n        File \"C:/users/arise/_bazel_arise/ji3ak47x/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 74, in _clone_or_update\r\n                fail((\"error cloning %s:\\n%s\" % (ctx....)))\r\nerror cloning io_bazel_rules_docker:\r\n+ cd C:/users/arise/_bazel_arise/ji3ak47x/external\r\n+ rm -rf C:/users/arise/_bazel_arise/ji3ak47x/external/io_bazel_rules_docker C:/users/arise/_bazel_arise/ji3ak47x/external/io_bazel_rules_docker\r\n/usr/bin/bash: line 5: rm: command not found\r\nERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):\r\n        File \"C:/users/arise/_bazel_arise/ji3ak47x/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 234\r\n                _clone_or_update(ctx)\r\n        File \"C:/users/arise/_bazel_arise/ji3ak47x/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 74, in _clone_or_update\r\n                fail((\"error cloning %s:\\n%s\" % (ctx....)))\r\nerror cloning io_bazel_rules_docker:\r\n+ cd C:/users/arise/_bazel_arise/ji3ak47x/external\r\n+ rm -rf C:/users/arise/_bazel_arise/ji3ak47x/external/io_bazel_rules_docker C:/users/arise/_bazel_arise/ji3ak47x/external/io_bazel_rules_docker\r\n/usr/bin/bash: line 5: rm: command not found\r\nINFO: Elapsed time: 2.642s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n", "comments": ["I managed to fix this by adding the following to the top of my WORKSPACE file:\r\n```python\r\nhttp_archive(\r\n    name = \"io_bazel_rules_docker\",\r\n    sha256 = \"aed1c249d4ec8f703edddf35cbe9dfaca0b5f5ea6e4cd9e83e99f3b0d1136c3d\",\r\n    strip_prefix = \"rules_docker-0.7.0\",\r\n    urls = [\"https://github.com/bazelbuild/rules_docker/archive/v0.7.0.tar.gz\"],\r\n)\r\n```\r\nIf you do the same, you probably want to ensure you get the latest version here https://github.com/bazelbuild/rules_docker and copy the code snippet from the README instead of from here.", "Same failure, can someone help?\r\n```\r\nERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):\r\n        File \"C:/users/vcntwtt/_bazel_vcntwtt/cd6duifl/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 234\r\n                _clone_or_update(ctx)\r\n        File \"C:/users/vcntwtt/_bazel_vcntwtt/cd6duifl/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 74, in _clone_or_update\r\n                fail((\"error cloning %s:\\n%s\" % (ctx....)))\r\nerror cloning io_bazel_rules_docker:\r\njava.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(456): CreateProcessW(\"bash\" -c \"\r\ncd C:/users/vcntwtt/_bazel_vcntwtt/cd6duifl/external\r\nset -ex\r\n( cd C:/users/vcntwtt/_bazel_vcntwtt/cd6duifl/external &&\r\n    if ! ( cd 'C:/users/vcntwtt/_bazel_vcntwtt/cd6duifl/external(...)): The system cannot find the file specified.\r\n```", "@QuellaZhang Did you read my comment above? Did it not work?", "@Acidic9 : Looks like you were able to resolve this issue. Let us know. Thanks!", "Closing the issue since it is resolved. Thanks!", "@Acidic9 Thanks for the patch, I can build successfully with the patch. \r\n\r\nBut the final reason I found it is that the Python library installation is not complete, I reinstall the Python64 and  follow the build steps, then I can successfully build the latest Tensorflow source code without any patch.\r\n\r\nIn addition, I found that if we did not execute \"pacman -S git patch unzip\" after installing msys32, it will report similar errors.", "I got into the very same issue when I tried to compile `TensorFlow 2.0.0-beta1` from source with \r\n*Intel-MKL* (CPU only) support.\r\n\r\nI solved the issue by including the following directive on top of the `WORKSPACE` file:\r\n\r\n```python\r\n# Download the rules_docker repository at release v0.8.1\r\nhttp_archive(\r\n    name = \"io_bazel_rules_docker\",\r\n    sha256 = \"87fc6a2b128147a0a3039a2fd0b53cc1f2ed5adb8716f50756544a572999ae9a\",\r\n    strip_prefix = \"rules_docker-0.8.1\",\r\n    urls = [\"https://github.com/bazelbuild/rules_docker/archive/v0.8.1.tar.gz\"],\r\n)\r\n``` \r\n\r\nThe directive has been taken from here: https://github.com/bazelbuild/rules_docker\r\n\r\nHTH", "> \r\n> \r\n> I managed to fix this by adding the following to the top of my WORKSPACE file:\r\n> \r\n> ```python\r\n> http_archive(\r\n>     name = \"io_bazel_rules_docker\",\r\n>     sha256 = \"aed1c249d4ec8f703edddf35cbe9dfaca0b5f5ea6e4cd9e83e99f3b0d1136c3d\",\r\n>     strip_prefix = \"rules_docker-0.7.0\",\r\n>     urls = [\"https://github.com/bazelbuild/rules_docker/archive/v0.7.0.tar.gz\"],\r\n> )\r\n> ```\r\n> \r\n> If you do the same, you probably want to ensure you get the latest version here https://github.com/bazelbuild/rules_docker and copy the code snippet from the README instead of from here.\r\n\r\nhttps://github.com/bazelbuild/rules_docker/blob/master/WORKSPACE", "> ```python\r\n> http_archive(\r\n>     name = \"io_bazel_rules_docker\",\r\n>     sha256 = \"aed1c249d4ec8f703edddf35cbe9dfaca0b5f5ea6e4cd9e83e99f3b0d1136c3d\",\r\n>     strip_prefix = \"rules_docker-0.7.0\",\r\n>     urls = [\"https://github.com/bazelbuild/rules_docker/archive/v0.7.0.tar.gz\"],\r\n> )\r\n> ```\r\nSuccessful!!. but not on the top of workspace file. put these lines after loading http_archive ", "Hi @Acidic9 \r\n\r\n> ```python\r\n> http_archive(\r\n>     name = \"io_bazel_rules_docker\",\r\n>     sha256 = \"aed1c249d4ec8f703edddf35cbe9dfaca0b5f5ea6e4cd9e83e99f3b0d1136c3d\",\r\n>     strip_prefix = \"rules_docker-0.7.0\",\r\n>     urls = [\"https://github.com/bazelbuild/rules_docker/archive/v0.7.0.tar.gz\"],\r\n> )\r\n> ```\r\n\r\nto which WORKSPACE file did you added the code snippet above?The exact path would help me a lot. I'm sorry if it should be obvious.Thank you so much!", "> to which WORKSPACE file did you added the code snippet above?The exact path would help me a lot. I'm sorry if it should be obvious.Thank you so much!\r\n\r\n@Tameli the `WORKSPACE` file is in the root of the project.\r\n\r\n", "@Acidic9 Thank you!! "]}, {"number": 28823, "title": "[TF 2.0 alpha] Tutorial \"Using TFRecords and tf.Example\" can't run on tensorflow2.0 \u03b10 gpu", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nTensorFlow version: Tensorflow 2.0 alpha\r\nDoc Link: Using TFRecords and tf.Example https://www.tensorflow.org/alpha/tutorials/load_data/tf_records\r\nWindows 10 LTSC x64 Python3.6 Cuda 10.0 Cudnn 7.5\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIf I run the example code, I will have an error in the cell:\r\ntf_serialize_example(f0,f1,f2,f3)\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\n`UnknownError           Traceback (most recent call last)\r\n<ipython-input-21-406ee79a7f52> in <module>\r\n----> 1 tf_serialize_example(f0,f1,f2,f3)\r\n\r\n<ipython-input-20-3f3d0d83f6b2> in tf_serialize_example(f0, f1, f2, f3)\r\n      3     serialize_example,\r\n      4     (f0,f1,f2,f3),  # pass these args to the above function.\r\n----> 5     tf.string)      # the return type is `tf.string`.\r\n      6   return tf.reshape(tf_string, ()) # The result is a scalar\r\n\r\nF:\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py in eager_py_func(func, inp, Tout, name)\r\n    387     if `func` returns None.\r\n    388   \"\"\"\r\n--> 389   return _internal_py_func(func=func, inp=inp, Tout=Tout, eager=True, name=name)\r\n    390 \r\n    391 \r\n\r\nF:\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py in _internal_py_func(func, inp, Tout, stateful, eager, is_grad_func, name)\r\n    276   if eager:\r\n    277     result = gen_script_ops.eager_py_func(\r\n--> 278         input=inp, token=token, Tout=Tout, name=name)\r\n    279   else:\r\n    280     if stateful:\r\n\r\nF:\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py in eager_py_func(input, token, Tout, name)\r\n     64       else:\r\n     65         message = e.message\r\n---> 66       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n     67   # Add nodes to the TensorFlow graph.\r\n     68   token = _execute.make_str(token, \"token\")\r\n\r\nF:\\Anaconda3\\envs\\TF2\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nUnknownError: RuntimeError: Error copying tensor to device: CPU:0. Can't copy 35 bytes of a tensor into another with 32 bytes buffer.\r\nTraceback (most recent call last):\r\n\r\n  File \"F:\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 205, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"F:\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 107, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"<ipython-input-5-a40a581f0687>\", line 10, in serialize_example\r\n    'feature2': _bytes_feature(feature2),\r\n\r\n  File \"<ipython-input-1-0ab605f55efd>\", line 8, in _bytes_feature\r\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\r\n\r\n  File \"F:\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 732, in numpy\r\n    return self._cpu_nograd()._numpy()  # pylint: disable=protected-access\r\n\r\n  File \"F:\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 899, in _cpu_nograd\r\n    return self._copy_nograd(context.context(), \"CPU:0\")\r\n\r\n  File \"F:\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 847, in _copy_nograd\r\n    new_tensor = self._copy_to_device(context=ctx._handle, device=device_name)\r\n\r\nRuntimeError: Error copying tensor to device: CPU:0. Can't copy 35 bytes of a tensor into another with 32 bytes buffer.\r\n\r\n [Op:EagerPyFunc]\r\n`\r\n\r\n", "comments": ["If I write import numpy before def _bytes_feature(value): Problem solved.", "@Windaway I tried reproducing the issue through colab link provided but the code executed without any error. Can you try once again and let us know if that still gives error. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28822, "title": "Tensorflow loss increase during the training", "body": "I cloned a github project that using tensorflow :\r\n+ https://github.com/yinguobing/cnn-facial-landmark\r\n\r\nand i tried to training ibug dataset (300VW , 300W , Helen...) to get a trained tensorflow model \r\nbut when training starting , the result is good and the loss decreasing during training , but after 1000 steps , the loss value increase and decrease during the training , I don't have any idea about this changing of loss value is a problem or bug or not , \r\nHow can i fix that?\r\n\r\n![image](https://user-images.githubusercontent.com/19480228/57968022-700bf380-7965-11e9-8256-e130e12c5d97.png)\r\n\r\n\r\n**System information**\r\n- The source code : https://github.com/yinguobing/cnn-facial-landmark/blob/master/landmark.py\r\n- OS Platform and Distribution : Windows 10 x64\r\n- TensorFlow installed from : binary\r\n- TensorFlow version : 13.1.1 ( GPU )\r\n- Python version: 3.6\r\n- CUDA/cuDNN version:10.0/7.4\r\n- GPU model and memory:Nvidia Geforce 840m 4 Go\r\n\r\n\r\n", "comments": ["might be due to inconsistency in data set, check through it once, ", "i have ibug dataset that contains ( 300VW 300W , Helen..) => images and .json files (annotation)", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes , the problem is in the loss function but i didn't know how can i solve it?", "I am closing this issue as it is not related to Build/install or Bug/performance. \r\nPlease post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 28821, "title": "Large virtual memory usage for TF Lite 2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04 and 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0 and 1.10\r\n- Python version: 3.5.2 and 3.6.7\r\n- Bazel version (if compiling from source): 19.0 and 16.0\r\n- GCC/Compiler version (if compiling from source): 7.4 and 7.3\r\n- CUDA/cuDNN version: 10.1 and 9.1\r\n- GPU model and memory: Nvidia RTX 2080ti(11 GB) and Nvidia GTX 1050ti(4 GB)\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using valgrind massif on label_image TF Lite example, it reports 400 MB of virtual memory,\r\neven though the model's size is only 14 MB and I do not understand where this overhead is coming from.\r\nHere is the profiling information at peak:\r\n100.00% (413,081,600B) (page allocation syscalls) mmap/mremap/brk, --alloc-fns, etc.\r\n->89.83% (371,077,120B) 0x6B8E6B9: mmap (mmap.c:34)\r\n| ->48.74% (201,330,688B) 0x6B0A3CF: new_heap (arena.c:438)\r\n| | ->48.74% (201,330,688B) 0x6B0AC1F: arena_get2.part.3 (arena.c:646)\r\n| |   ->48.74% (201,330,688B) 0x6B11248: malloc (malloc.c:2911)\r\n| |     ->48.74% (201,330,688B) 0x657C1E6: operator new(unsigned long) (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.25)\r\n| |       ->32.49% (134,221,824B) 0x4BDD65: ??? (in /home/paul/.cache/bazel/_bazel_paul/3c9cebfe1f000a90a0a2bd1f5b50e543/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/examples/label_image/label_image)\r\n| |       | ->32.49% (134,221,824B) 0x4EE3B1: ??? (in /home/paul/.cache/bazel/_bazel_paul/3c9cebfe1f000a90a0a2bd1f5b50e543/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/examples/label_image/label_image)\r\n| |       |   ->32.49% (134,221,824B) 0x65A657D: ??? (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.25)\r\n| |       |     ->32.49% (134,221,824B) 0x5BBE6B8: start_thread (pthread_create.c:333)\r\n| |       |       ->32.49% (134,221,824B) 0x6B9441B: clone (clone.S:109)\r\n\r\n**Describe the expected behavior**\r\nI expected less memory usage from this lite tool.\r\n\r\n**Code to reproduce the issue**\r\nJust run label_image TF example and with this:\r\n`valgrind --tool=massif --pages-as-heap=yes --massif-out-file=massif.out bazel-bin/tensorflow/lite/examples/label_image/label_image -m mobilenet_v2\r\n_1.0_224.tflite -i grace_hopper.bmp; grep mem_heap_B massif.out | sed -e 's/mem_heap_B=\\(.*\\)/\\1/' | sort -g | tail -n 1`\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nLE:\r\nBy running with debugging symbols, it seems that EigenForTFLite is the culprit; is this really a wanted behavior?", "comments": ["If you have debug symbols, you are going to have much bigger .so's. These get mmapped which means you will have large VM memory usage. In virtual memory large vm usage is not necessarily a problem, because as long as most pages are not really needed, then you won't actually incur physcal memory  pressure. Also, try a release build, look at physical resident memory usage.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28821\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28821\">No</a>\n", "Thank you for your time."]}, {"number": 28820, "title": "Converting keras model file to tflite, got this error.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7 x64\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): 1.13.1\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: EXPAND_DIMS, FILL, FULLY_CONNECTED, PACK, RESHAPE, SHAPE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: CudnnRNN.\r\n2019-05-17 23:06:18.042831: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: CudnnRNN\r\n2019-05-17 23:06:18.042831: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"CPU\"') for unknown op: WrapDatasetVariant\r\n2019-05-17 23:06:18.042831: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: WrapDatasetVariant\r\n2019-05-17 23:06:18.042831: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"CPU\"') for unknown op: UnwrapDatasetVariant\r\n2019-05-17 23:06:18.042831: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: UnwrapDatasetVariant\r\n2019-05-17 23:06:18.042831: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: CudnnRNN\r\n2019-05-17 23:06:18.043831: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: CudnnRNN\r\n2019-05-17 23:06:18.043831: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: CudnnRNN\r\n2019-05-17 23:06:18.045831: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 134 operators, 271 arrays (0 quantized)\r\n2019-05-17 23:06:18.046831: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 134 operators, 271 arrays (0 quantized)\r\n2019-05-17 23:06:18.054832: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 17 operators, 43 arrays (0 quantized)\r\n2019-05-17 23:06:18.054832: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 17 operators, 43 arrays (0 quantized)\r\n2019-05-17 23:06:18.055832: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 1856 bytes, theoretical optimal value: 1856 bytes.\r\n2019-05-17 23:06:18.055832: E tensorflow/lite/toco/toco_tooling.cc:421] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\nand pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: EXPAND_DIMS, FILL, FULLY_CONNECTED, PACK, RESHAPE, SHAPE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: CudnnRNN.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@ShaneSmiskol  Request you to please refer the [link1](https://www.tensorflow.org/lite/guide/ops_select) , [link2](https://www.tensorflow.org/lite/guide/ops_custom) . Please try if that helps and let us know how it progresses."]}, {"number": 28819, "title": "Object detection: when training, GPU utilization started high, and became low/idle later on.", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Server 2016\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10/7.1\r\n- GPU model and memory: Tesla V100 - 16 GB\r\n\r\n**Describe the current behavior**\r\nI am trying to train a face detector by using the Object Detection API. Specifically, I am using 4000 images from the Wider Faces data set for training/validation, and using faster_rcnn_resnet101_coco for fine-tuning. I split the train set images into 5 shards, and in the .config file, I changed the `min_dimension` and `max_dimension` to 300 and 512, respectively, in hopes of faster training.\r\n\r\nAfter I kicked off training, I noticed that the GPU memory usage was about 95%. GPU volatile-utilization and CPU utilization on the other hand were behaving oddly. Specifically, during the very first ~3k steps, GPU volatile-utilization was between 60%  and 95%, and the CPU utilization was only around 8%. Global_step/sec essentially plateaued around 8 steps/sec within the first 3k steps. \r\n\r\nFrom ~3k steps and onward, CPU utilization increased to 80-100% whereas GPU volatile utilization oscillated between 0% to 30% and **for the most part, it was idle (0%)**. GPU memory usage continued to be high.\r\n\r\n**Describe the expected behavior**\r\nGiven the initial high GPU volatile utilization rate, I was expecting the high util-rate to persist, so it was a bit strange to see that after the initial 3k steps, the training process only occasionally used the GPU, and mostly relied on the CPU.\r\n\r\nAny pointers/help would be much appreciated. Thanks!\r\n\r\n\r\n\r\n\r\n", "comments": ["Just to add a bit more detail: after around 4000 steps, Global_step/sec dropped to around 1 step/sec\r\n\r\n![image](https://user-images.githubusercontent.com/3064882/58047790-d461bb00-7afd-11e9-9774-d6d25dd8539c.png)\r\n\r\n", "This issue belongs to object detection api handled by tensorflow models repo. Please post it in [TF models](https://github.com/tensorflow/models/issues). Thanks!"]}, {"number": 28818, "title": "RW4YQ2T67I2", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command or code snippet if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Seems like a spam issue"]}, {"number": 28817, "title": "Update backprop.py with better markdown", "body": "", "comments": ["@yifeif can you please help merge this PR,thank you.", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 28816, "title": "Formatted backprop.py", "body": "Formatted some markdown in the file", "comments": []}, {"number": 28815, "title": "ArrayIndexOutOfBoundsException: length=1; index=1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Android Studio 3.4.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nOnePlus 6\r\n\r\n**Describe the problem**\r\n\r\nI am using the tensorflow example object_detection app, it's working fine. But when I try plugging in my own model \"tiny-yolo-obj.lite\" (created with Tiny YOLO) and \"labelmap2.txt\" (first line being ???, other 36 lines my objects) it throws the following error:\r\n\r\n```\r\nProcess: org.tensorflow.lite.examples.detection, PID: 31738\r\n    java.lang.ArrayIndexOutOfBoundsException: length=1; index=1\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n**My model meta:**\r\n\r\n`{\"net\": {\"type\": \"[net]\", \"batch\": 64, \"subdivisions\": 8, \"width\": 416, \"height\": 416, \"channels\": 3, \"momentum\": 0.9, \"decay\": 0.0005, \"angle\": 0, \"saturation\": 1.5, \"exposure\": 1.5, \"hue\": 0.1, \"learning_rate\": 0.001, \"max_batches\": 40200, \"policy\": \"steps\", \"steps\": \"-1,100,20000,30000\", \"scales\": \".1,10,.1,.1\"}, \"type\": \"[region]\", \"anchors\": [1.08, 1.19, 3.42, 4.41, 6.63, 11.38, 9.42, 5.11, 16.62, 10.52], \"bias_match\": 1, \"classes\": 36, \"coords\": 4, \"num\": 5, \"softmax\": 1, \"jitter\": 0.2, \"rescore\": 1, \"object_scale\": 5, \"noobject_scale\": 1, \"class_scale\": 1, \"coord_scale\": 1, \"absolute\": 1, \"thresh\": 0.6, \"random\": 1, \"model\": \"cfg/tiny-yolo-obj.cfg\", \"inp_size\": [416, 416, 3], \"out_size\": [13, 13, 205], \"name\": \"tiny-yolo-obj\", \"labels\": [\"Rosen 6\", \"Rosen 7\", \"Rosen 8\", \"Rosen 9\", \"Rosen 10\", \"Rosen Under\", \"Rosen Ober\", \"Rosen K\\u00f6nig\", \"Rosen Ass\", \"Eichel 6\", \"Eichel 7\", \"Eichel 8\", \"Eichel 9\", \"Eichel 10\", \"Eichel Under\", \"Eichel Ober\", \"Eichel K\\u00f6nig\", \"Eichel Ass\", \"Schellen 6\", \"Schellen 7\", \"Schellen 8\", \"Schellen 9\", \"Schellen 10\", \"Schellen Under\", \"Schellen Ober\", \"Schellen K\\u00f6nig\", \"Schellen Ass\", \"Schilten 6\", \"Schilten 7\", \"Schilten 8\", \"Schilten 9\", \"Schilten 10\", \"Schilten Under\", \"Schilten Ober\", \"Schilten K\\u00f6nig\", \"Schilten Ass\"], \"colors\": [[254.0, 254.0, 254], [246.0625, 222.25, 127], [238.125, 190.5, 0], [230.1875, 158.75, -127], [222.25, 127.0, 254], [214.3125, 95.25, 127], [206.375, 63.5, 0], [198.4375, 31.75, -127], [190.5, 0.0, 254], [182.5625, -31.75, 127], [174.625, -63.5, 0], [166.6875, -95.25, -127], [158.75, -127.0, 254], [150.8125, -158.75, 127], [142.875, -190.5, 0], [134.9375, -222.25, -127], [127.0, 254.0, 254], [119.0625, 222.25, 127], [111.125, 190.5, 0], [103.1875, 158.75, -127], [95.25, 127.0, 254], [87.3125, 95.25, 127], [79.375, 63.5, 0], [71.4375, 31.75, -127], [63.5, 0.0, 254], [55.5625, -31.75, 127], [47.625, -63.5, 0], [39.6875, -95.25, -127], [31.75, -127.0, 254], [23.8125, -158.75, 127], [15.875, -190.5, 0], [7.9375, -222.25, -127], [0.0, 254.0, 254], [-7.9375, 222.25, 127], [-15.875, 190.5, 0], [-23.8125, 158.75, -127]]}`\r\n\r\n**My code in DetectorActivity**\r\n\r\n```\r\n  // Configuration values for the prepackaged SSD model.\r\n  private static final int TF_OD_API_INPUT_SIZE = 300;\r\n  private static final boolean TF_OD_API_IS_QUANTIZED = false; // maybe also true, dunno yet\r\n  private static final String TF_OD_API_MODEL_FILE = \"tiny-yolo-obj.lite\";\r\n  private static final String TF_OD_API_LABELS_FILE = \"labelmap2.txt\";\r\n  private static final DetectorMode MODE = DetectorMode.TF_OD_API;\r\n  // Minimum detection confidence to track a detection.\r\n  private static final float MINIMUM_CONFIDENCE_TF_OD_API = 0.5f;\r\n  private static final boolean MAINTAIN_ASPECT = false;\r\n  private static final Size DESIRED_PREVIEW_SIZE = new Size(640, 480);\r\n  private static final boolean SAVE_PREVIEW_BITMAP = false;\r\n  private static final float TEXT_SIZE_DIP = 10;\r\n```\r\n\r\nAny help would be greatly appreciated! Thanks in advance.", "comments": ["Issue resolved, I had to use DetectorMode.YOLO and specify other parameters for YOLO.", "Good to know the issue is resolved and thank you for mentioning the resolution", "Closing this issue since its resolved. Thanks!", "@TheSiebi   @ymodak  \r\nI'm integrating MakeMl nails model in android app and facing same issue.\r\n\r\nError Message: java.lang.ArrayIndexOutOfBoundsException: length=66049; index=66049\r\n\r\nModel tflite File: https://github.com/makeml-app/MakeML-Nails/tree/master/Segmentation%20Nails/Resources\r\n\r\nAndroid Code: https://github.com/RobinHan24/android-segmentation-app\r\n\r\nAny suggestion. How I resolved this issue ?\r\n"]}, {"number": 28814, "title": "Use TF_CUDA_PATHS to find cuBLAS if CUDA version >= 10.1, otherwise C\u2026", "body": "\u2026UDA_TOOLKIT_PATH.\r\n\r\nI need this patch to get TF1.14 to build with CUDA 10.1.\r\n\r\nPiperOrigin-RevId: 247743023", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28814) for more info**.\n\n<!-- need_sender_cla -->", "Note that I am just asking someone to merge f46e445c78 into r1.14."]}, {"number": 28813, "title": "Fix bazel rule for cuda bazel generation from templates", "body": "This PR fixes the issue in templated bazel file generation for cuda repositories. In clang compilation case, \r\n template replacement argument is passed as a list of strings instead of a single string. This PR adds necessary change to convert the list of strings to a string.", "comments": []}]