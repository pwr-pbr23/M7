[{"number": 28598, "title": "Tensorflow 2.0 rejection resample not producing the proper target distribution", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): from binary pip3\r\n- TensorFlow version (use command below): 2.0-alpha\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GTX Titan V 12GB\r\n\r\n**Issues**\r\nI am trying to test rejection sampling with the following code. \r\nThis is the result I got:\r\n\r\n    target_dist [0.5, 0.5] \r\n    initial distribution [0.8333333333333334, 0.16666666666666666]\r\n    result counts [1500, 600] \r\n    final dist 0.7142857142857143 0.2857142857142857\r\n\r\n**Expectation**\r\nThe final distribution does not reflect the target distribution I set. I feel like this is a bug in the original algorithm because I can't seem to find what I did wrong in my code as shown below:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n# everything is based on tensorflow 2.0\r\ntf.random.set_seed(2342)\r\n\r\n\r\ndef map2label(sample):\r\n    return tf.cast(tf.math.equal(sample, 2), tf.int32)\r\n\r\nnp_data = np.array([0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2])\r\ntarget_dist = [0.5, 0.5]\r\ninit_dist = [(np_data.shape[0]-3)/np_data.shape[0], 3/np_data.shape[0]]\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(np_data)\r\nrej = tf.data.experimental.rejection_resample(map2label, target_dist, init_dist, 2342)   # set seed explicitly\r\ndataset = dataset.apply(rej)\r\n\r\nbucket_counts = [0, 0]\r\nfor i in range(100):\r\n    for data in dataset:\r\n        class_id, data_content = data\r\n        bucket_counts[class_id.numpy()] += 1\r\n\r\nprint(\"This is your target_dist\", target_dist, \"This is your initial distribution\", init_dist)\r\nprint(\"This is your result counts\", bucket_counts,\r\n      \"This is your final dist\", bucket_counts[0] / np.sum(bucket_counts), bucket_counts[1] / np.sum(bucket_counts))\r\n\r\n```", "comments": ["@celinew1221 It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "Sure. Can I just edit this post? Looks like I am missing some system info. Done. Please read edited post.", "@celinew1221 Able to reproduce the issue with the provided code. In the template it is mentioned that it is not a custom written code, can you provide the link for this, to investigate further.\r\n\r\nThis is your target_dist [0.5, 0.5] This is your initial distribution [0.8333333333333334, 0.16666666666666666]\r\nThis is your result counts [1500, 600] This is your final dist 0.7142857142857143 0.2857142857142857", "Sorry, I meant that I didn't create a custom made rejection_resample function. Everything else is what I wrote. I'll edit that.", "Hi,\r\nI had a similar issue. Initially, I was using shuffle and repeat separately. This caused the rejection resample to produce incorrect distribution.  \r\n\r\nCode\r\n```python \r\n            self.data = self.data.shuffle(buffer_size=len(self.images)*2, seed= 1)\r\n            self.data = self.data.repeat(count = 1)\r\n            target_dist=[1.0/self.args.num_classes for cc in range(self.args.num_classes)]\r\n            rej = tf.data.experimental.rejection_resample(\r\n                          class_func=lambda _,__,___, y,____,_____:y,\r\n                          initial_dist=self.class_weight.astype(np.float32),\r\n                          target_dist=target_dist)\r\n            self.data = self.data.apply(rej).map(lambda _,b:(b))\r\n```\r\nThe incorrect result is provided below:\r\n```result\r\nWith seperate shuffle and repeat\r\n[0 0 3 3 3 0 1 0 3 0 3 0 0 0 0 0]\r\n```\r\n\r\nHowever, when I replaced the shuffle and repeat using the function given below, it works as expected.\r\n\r\n```python\r\nself.data = self.data.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=len(self.images)*2,count=1))\r\n```\r\nResult with correct distribution is given below:\r\n```result\r\nWith combined shuffle and repeat\r\n[3 1 3 1 3 3 0 1 3 0 1 4 2 4 3 0]\r\n```", "Issue is replicating with `Tf-nightly==2.2.0.dev20200311`,\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/0501e4610fd758d4ba6e72a95d35604b/untitled443.ipynb). Thanks!", "This issue should be fixed by https://github.com/tensorflow/tensorflow/commit/f313fdce45d4933938a89980f6ca9bb2c8cbd27a", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28598\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28598\">No</a>\n", "Well, I'm stil able to reproduce the issue by running the gist @gadagashwini provided using 2.2.0-rc3, and I'm also having a hard time getting it to properly resample using my own training scripts (which I don't think is important to provide since even for a simple script from that gist - it does not work).\r\n\r\n@celinew1221 - this Issue should probably be reopened?", "I still have this issue with my own data (TensorFlow 2.2) and I can reproduce the gist that is provided above even by version 2.4.0-dev20200717! it seems that the problem still exists.\r\nI think the issue needs to be reopened. @jsimsa  ", "I can reproduce the same gist using Tensorflow 2.4.3. This is very likely not fixed yet!", "This is the fix: https://github.com/tensorflow/tensorflow/commit/f15fbec84874d41592dd8622e784e455f5b580f4#diff-8c6fcb1b5540671afd6bfe9d0d10f84cb4a681b47929b49f016d7a8ee9c41a7c\r\n\r\nThe fix has been released as part of TF 2.6.\r\n\r\nTF 2.4 was released in December 2020 and the TensorFlow project does not backport fixes to patch releases of older minor releases (i.e. TF 2.4.x)."]}, {"number": 28597, "title": "tensorflow 2.0 rejection resample printing proportion of examples ... message", "body": "I am using tensorflow 2.0 CUDA 10.\r\n\r\nThe message comes from rejection sampling. I receive multiple lines similar to this:\r\n> Proportion of examples rejected by sampler is high: [0.935080409][0.935080409 0.0649196059][0 1]\r\n\r\nTurn the logging level to ERROR does not help. Is there a way to turn this off or direct it to a disk file so that it won't mess up my other progress messages? You can use the following snippet to reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n# everything is based on tensorflow 2.0\r\n\r\n# this is important or rejection sample won't work\r\n# I set this here so that all random process keeps the same order but you can\r\n# also set it at rejection_resample function\r\ntf.random.set_seed(2342)\r\n\r\n\r\ndef map2label(sample):\r\n    return tf.cast(tf.math.equal(sample, 2), tf.int32)\r\n\r\nnp_data = np.array([0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2])\r\ntarget_dist = [0.5, 0.5]\r\ninit_dist = [(np_data.shape[0]-3)/np_data.shape[0], 3/np_data.shape[0]]\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(np_data)\r\nrej = tf.data.experimental.rejection_resample(map2label, target_dist, init_dist, 20)\r\ndataset = dataset.apply(rej)\r\n\r\nbucket_counts = [0, 0]\r\nfor i in range(100):\r\n    for data in dataset:\r\n        class_id, data_content = data\r\n        bucket_counts[class_id.numpy()] += 1\r\nprint(\"This is your target_dist\", target_dist, \"This is your initial distribution\", init_dist)\r\nprint(\"This is your result counts\", bucket_counts,\r\n      \"This is your final dist\", bucket_counts[0] / np.sum(bucket_counts), bucket_counts[1] / np.sum(bucket_counts))\r\n```\r\n", "comments": ["@celinew1221 Could you provide more details about the issue and context? Please provide as many details as possible to resolve the issue faster. Thanks!", "I\u2019m confused. What other information do you need? The sample code is what I used to undersample data - using rejection sampling. However, with a highly unbalanced dataset, like the one I used here, it will spit out the information I quoted above, which is very annoying when I have other debug/info printed to screen. ", "@celinew1221 Can you try \r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \r\n#0 = all messages are logged (default behavior)\r\n#1 = INFO messages are not printed\r\n#2 = INFO and WARNING messages are not printed\r\n#3 = INFO, WARNING, and ERROR messages are not printed\r\n```\r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 28596, "title": "Keras code in colab from tensorflow.org is not showing tensorflow version details", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Able to reproduce the issue from this [link](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras.ipynb#scrollTo=7e1LPcXx0gR6) .\r\nPlease see the attached image.\r\n![image](https://user-images.githubusercontent.com/48215502/57529375-61648180-7352-11e9-8081-a9a452e73238.png)\r\n", "@muddham I don't see any issue. Please check the attached [GitHub gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/afd188f0434e01f55b7aac61667e0d08/keras.ipynb). Thanks!", "Seems to be working correctly for me. @muddham I'm unable to reproduce the error from the given [link](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras.ipynb#scrollTo=7e1LPcXx0gR6). Are you still facing the problem?\r\n![Screen Shot 2019-05-12 at 6 34 11 PM](https://user-images.githubusercontent.com/46882326/57582614-a28c9b00-74e4-11e9-946b-175066983dde.png)\r\n"]}, {"number": 28595, "title": " I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0 ", "body": "Hello , \r\nI need a help , when i try to execute program with tensorflow it frozen on this line:\r\n\r\n>  I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n\r\nI run this script :  \r\n`import tensorflow as tf\r\ntf.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None)`\r\n\r\n**System information**\r\n- Windows 10 x64\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6\r\n- Installed using pip\r\n- CUDA/cuDNN version:10.0/7.5\r\n- GPU model and memory : Nvidia Geforce 840m \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nAs you can see this is a screenshot of the bug :  \r\n![image](https://user-images.githubusercontent.com/19480228/57528912-d6748e80-7333-11e9-896b-b8840339303a.png)\r\n\r\n", "comments": []}, {"number": 28594, "title": "[doc] fix an error in the comment.", "body": "", "comments": []}, {"number": 28593, "title": "[lite/micro] fix various typos.", "body": "", "comments": ["@csukuangfj can you please resolve conflicts.", "@gbaned\r\nDone.", "@achowdhery Can you please review this PR? Thanks!"]}, {"number": 28592, "title": "About slim.nets.inception_v3", "body": "I have read the function Inception_v3_base, I have questions about the code of Mixed_5b, 5c, 5d. The paper replaces the 5x5 convolution with 1x1 3x3 3x3, but why replace the original 1x1 3x3 combination with 1x1 5x5 in the code.", "comments": ["not only this question, in line 242~251, the code named the scope as \"conv2d_1a_1x1\", but actually the code use 3x3 size", "@menghuanlater This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n\r\n"]}, {"number": 28591, "title": "TfLite stride_slice begin/end/strided supports int64", "body": "This PR supports stride_slice ops begin/end/strided params with int64 type.", "comments": ["@achowdhery Can you please review this PR ? Thanks!", "Can one of the admins verify this patch?", "@Dayananda-V Could you please resolve the conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28590, "title": "python3.exe crush after running python script with tensorflow", "body": "Hello ,\r\nI'm setting and cloned the this project :\r\nhttps://github.com/yinguobing/cnn-facial-landmark\r\nBut when i run: \r\n\r\n> python3 landmark.py \r\n\r\n\r\nI get this bug : \r\n\r\n![ce](https://user-images.githubusercontent.com/19480228/57519953-c8ffda00-731c-11e9-9e8e-21c9db0e0387.PNG)\r\n\r\nwhen i debug , i get this : \r\n\r\n![des](https://user-images.githubusercontent.com/19480228/57520070-13815680-731d-11e9-8b58-73e93587da41.PNG)\r\n\r\n\r\nNote : I test tensorflow with code mentionned in the readme and it work perfectly . \r\n\r\n![image](https://user-images.githubusercontent.com/19480228/57520025-fa78a580-731c-11e9-8432-a4171c6b956a.png)\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform: Windows 10 x64\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version (1.13)\r\n- Python version:(3.6)\r\n- CUDA/cuDNN version: 10.0/7.5\r\n- GPU model and memory: Nvidia Geforce 840m 4.00 Go\r\n\r\n\r\nHow can i solve this error ?", "comments": ["Can you give us some more info on the error you are facing ? Could not able to read the actual error message. Thanks!", "I think that the error will gone , i missed some tensorflow files ( train.record , test.record , valid.record )  that should be generated by Tensorflow , I follow this tuto :\r\n+ https://github.com/faust690226/cnn-facial-landmark-tutorial ", "Good to hear that. I will be closing this issue since it is resolved. Thanks!"]}, {"number": 28589, "title": "Build TensorFlow Lite for ARM64 boards failed", "body": "**System information**\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: r1.14\r\n\r\n\r\n\r\n**Describe the problem**\r\nI got build error: undefined reference to `NnApiImplementation()'\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed https://www.tensorflow.org/lite/guide/build_arm64 . Here are the exact sequence of commands:\r\n`git clone https://github.com/tensorflow/tensorflow && cd tensorflow`\r\n`git checkout -b r1.14 origin/r1.14`\r\n`./tensorflow/lite/tools/make/download_dependencies.sh` `./tensorflow/lite/tools/make/build_aarch64_lib.sh`\r\n\r\n**Any other info / logs**\r\nI got these error info when I execute `build_aarch64_lib.sh` :\r\n\r\n/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIAllocation::~NNAPIAllocation()':\r\nnnapi_delegate.cc:(.text+0x28): undefined reference to `NnApiImplementation()'\r\n/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIAllocation::NNAPIAllocation(char const*, tflite::ErrorReporter*)':\r\nnnapi_delegate.cc:(.text+0x1a4): undefined reference to `NnApiImplementation()'\r\n/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIDelegate::~NNAPIDelegate()':\r\nnnapi_delegate.cc:(.text+0x218): undefined reference to `NnApiImplementation()'\r\nnnapi_delegate.cc:(.text+0x234): undefined reference to `NnApiImplementation()'\r\n/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::addTensorOperands(tflite::Subgraph*, ANeuralNetworksModel*, unsigned int*, std::vector<long, std::allocator<long> >*)':\r\nnnapi_delegate.cc:(.text+0x2b8): undefined reference to `NnApiImplementation()'\r\n/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o):nnapi_delegate.cc:(.text+0x578): more undefined references to `NnApiImplementation()' follow\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:227: recipe for target '/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal' failed\r\nmake: *** [/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\n\r\n", "comments": ["This should be fixed on head. Note that the 1.14 branch is in a state of flux and will be updated with the fix. Please re-open if you find otherwise (testing against head)."]}, {"number": 28587, "title": "Erratum 843419 found and fixed at **", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  aarch64-linux-gnu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:  tensorflow-1.13.0-rc2\r\n- Python version: python 2.7\r\n- Installed using virtualenv? pip? conda?: native python 2.7\r\n- Bazel version (if compiling from source):  0.19.2- (@non-git)\r\n- GCC/Compiler version (if compiling from source):  gcc version 5.3.1 20160413 (Ubuntu/Linaro 5.3.1-14kord4)\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n\r\n\r\n**Describe the problem**\r\nthe detail info  is here [https://github.com/bazelbuild/bazel/issues/8285](https://github.com/bazelbuild/bazel/issues/8285)\r\n\r\ncould you help me? thanks.", "comments": []}, {"number": 28586, "title": "build error on aarch64", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  aarch64-linux-gnu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.0-rc2\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: native python 2.7\r\n- Bazel version (if compiling from source): compile from source   `bazel version` output:\r\nBuild label: 0.19.2- (@non-git)\r\nBuild target: bazel-out/aarch64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n...\r\n\r\n- GCC/Compiler version (if compiling from source): `gcc -v`\r\n\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/aarch64-linux-gnu/5/lto-wrapper\r\nTarget: aarch64-linux-gnu\r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 5.3.1-14kord4' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-arm64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-arm64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-arm64 --with-arch-directory=aarch64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu\r\nThread model: posix\r\n**gcc version 5.3.1 20160413 (Ubuntu/Linaro 5.3.1-14kord4)**\r\n\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n**first  have successfuly build bazel ,and can see bazel version info by `bazel version`,ad output :\r\nBuild label: 0.19.2- (@non-git)\r\n...**\r\n\r\n**then  run `bazel build --config=opt  //tensorflow:libtensorflow_cc.so`\r\nthere is all have compiled but when linking get an error below**\r\n\r\nERROR: /home/greatwall/recog_vote/tensorflow/tensorflow-1.13.0-rc2/tensorflow/BUILD:489:1: Linking of rule '//tensorflow:libtensorflow_cc.so' failed (Exit 1)\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/rpc/_objs/grpc_master_service_impl/grpc_master_service_impl.pic.o\", section 581, offset 0x00000180.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/rpc/eager/_objs/grpc_eager_service_impl/grpc_eager_service_impl.pic.o\", section 444, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/external/grpc/_objs/grpc/init.pic.o\", section 24, offset 0x00000050.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/external/grpc/_objs/grpc_client_channel/client_channel.pic.o\", section 192, offset 0x000002e8.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/external/grpc/_objs/grpc_transport_chttp2/hpack_parser.pic.o\", section 92, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/profiler/internal/_objs/tfprof_tensor/tfprof_tensor.pic.o\", section 72, offset 0x00000f60.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/depth_space_ops/spacetodepth_op.pic.o\", section 168, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/one_hot_op/one_hot_op.pic.o\", section 789, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/unpack_op/unpack_op.pic.o\", section 204, offset 0x0000003c.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op/dynamic_partition_op.pic.o\", section 261, offset 0x0000003c.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/matrix_inverse_op/matrix_inverse_op.pic.o\", section 112, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/summary_op/summary_op.pic.o\", section 203, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/batch_matmul_op_real.pic.o\", section 2108, offset 0x00000398.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_bool.pic.o\", section 274, offset 0x000000dc.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_complex128.pic.o\", section 299, offset 0x00000318.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_int32.pic.o\", section 246, offset 0x000000dc.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/reduction_ops/reduction_ops_min.pic.o\", section 1426, offset 0x00000d78.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/set_kernels/set_kernels.pic.o\", section 997, offset 0x00000100.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/scatter_nd_op_cpu_impl_1.pic.o\", section 857, offset 0x000001a0.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_add_1.pic.o\", section 1800, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_pow.pic.o\", section 1347, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_real.pic.o\", section 123, offset 0x00000360.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_tan.pic.o\", section 167, offset 0x00000588.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/grappler/clusters/_objs/virtual_cluster/virtual_cluster.pic.o\", section 150, offset 0x00000cfc.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/external/boringssl/_objs/crypto/bcm.pic.o\", section 1350, offset 0x0000026c.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/rpc/_objs/grpc_master_service_impl/grpc_master_service_impl.pic.o\", section 581, offset 0x00000180.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/rpc/eager/_objs/grpc_eager_service_impl/grpc_eager_service_impl.pic.o\", section 444, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/external/grpc/_objs/grpc/init.pic.o\", section 24, offset 0x00000050.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/external/grpc/_objs/grpc_client_channel/client_channel.pic.o\", section 192, offset 0x000002e8.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/external/grpc/_objs/grpc_transport_chttp2/hpack_parser.pic.o\", section 92, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/profiler/internal/_objs/tfprof_tensor/tfprof_tensor.pic.o\", section 72, offset 0x00000f60.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/depth_space_ops/spacetodepth_op.pic.o\", section 168, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/one_hot_op/one_hot_op.pic.o\", section 789, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/unpack_op/unpack_op.pic.o\", section 204, offset 0x0000003c.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op/dynamic_partition_op.pic.o\", section 261, offset 0x0000003c.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/matrix_inverse_op/matrix_inverse_op.pic.o\", section 112, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/summary_op/summary_op.pic.o\", section 203, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/batch_matmul_op_real.pic.o\", section 2108, offset 0x00000398.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_bool.pic.o\", section 274, offset 0x000000dc.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_complex128.pic.o\", section 299, offset 0x00000318.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_int32.pic.o\", section 246, offset 0x000000dc.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/reduction_ops/reduction_ops_min.pic.o\", section 1426, offset 0x00000d78.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/set_kernels/set_kernels.pic.o\", section 997, offset 0x00000100.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/scatter_nd_op_cpu_impl_1.pic.o\", section 857, offset 0x000001a0.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_add_1.pic.o\", section 1800, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_pow.pic.o\", section 1347, offset 0x00000000.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_real.pic.o\", section 123, offset 0x00000360.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_tan.pic.o\", section 167, offset 0x00000588.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/tensorflow/core/grappler/clusters/_objs/virtual_cluster/virtual_cluster.pic.o\", section 150, offset 0x00000cfc.\r\nErratum 843419 found and fixed at \"bazel-out/aarch64-opt/bin/external/boringssl/_objs/crypto/bcm.pic.o\", section 1350, offset 0x0000026c.\r\nbazel-out/aarch64-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::EnvironmentAWSCredentialsProvider::GetAWSCredentials(): error: undefined reference to 'Aws::Environment::GetEnv[abi:cxx11](char const*)'\r\nbazel-out/aarch64-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::EnvironmentAWSCredentialsProvider::GetAWSCredentials(): error: undefined reference to 'Aws::Environment::GetEnv[abi:cxx11](char const*)'\r\nbazel-out/aarch64-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::EnvironmentAWSCredentialsProvider::GetAWSCredentials(): error: undefined reference to 'Aws::Environment::GetEnv[abi:cxx11](char const*)'\r\nbazel-out/aarch64-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::ProfileConfigFileAWSCredentialsProvider::GetConfigProfileFilenameabi:cxx11: error: undefined reference to 'Aws::FileSystem::GetHomeDirectoryabi:cxx11'\r\nbazel-out/aarch64-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::ProfileConfigFileAWSCredentialsProvider::GetCredentialsProfileFilenameabi:cxx11: error: undefined reference to 'Aws::Environment::GetEnv[abi:cxx11](char const*)'\r\nbazel-out/aarch64-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::ProfileConfigFileAWSCredentialsProvider::GetCredentialsProfileFilenameabi:cxx11: error: undefined reference to 'Aws::FileSystem::GetHomeDirectoryabi:cxx11'\r\nbazel-out/aarch64-opt/bin/external/aws/_objs/aws/ClientConfiguration.pic.o:ClientConfiguration.cpp:function Aws::Client::ComputeUserAgentString(): error: undefined reference to 'Aws::OSVersionInfo::ComputeOSVersionStringabi:cxx11'\r\nbazel-out/aarch64-opt/bin/external/aws/_objs/aws/DateTimeCommon.pic.o:DateTimeCommon.cpp:function Aws::Utils::DateTime::ConvertTimestampStringToTimePoint(char const*, Aws::Utils::DateFormat): error: undefined reference to 'Aws::Time::TimeGM(tm*)'\r\nbazel-out/aarch64-opt/bin/external/aws/_objs/aws/DateTimeCommon.pic.o:DateTimeCommon.cpp:function Aws::Utils::DateTime::ConvertTimestampToLocalTimeStruct() const: error: undefined reference to 'Aws::Time::LocalTime(tm*, long)'\r\nbazel-out/aarch64-opt/bin/external/aws/_objs/aws/DateTimeCommon.pic.o:DateTimeCommon.cpp:function Aws::Utils::DateTime::ConvertTimestampToGmtStruct() const: error: undefined reference to 'Aws::Time::GMTime(tm*, long)'\r\nbazel-out/aarch64-opt/bin/external/aws/_objs/aws/TempFile.pic.o:TempFile.cpp:function Aws::Utils::TempFile::~TempFile(): error: undefined reference to 'Aws::FileSystem::RemoveFileIfExists(char const*)'\r\nbazel-out/aarch64-opt/bin/external/aws/_objs/aws/TempFile.pic.o:TempFile.cpp:function Aws::Utils::ComputeTempFileName(char const*, char const*): error: undefined reference to 'Aws::FileSystem::CreateTempFilePathabi:cxx11'\r\n\r\n**and the last output**\r\n\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 6534.318s, Critical Path: 428.70s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 1956 processes: 1956 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nthere is all of the error about\r\nErratum 843419 found and fixed at ***\r\n\r\nand bazel is compile all but the error for the last step of linking\r\n\r\n**here is some info:**\r\n\r\n- --enable-fix-cortex-a53-843419\r\n    https://github.com/gcc-mirror/gcc/blob/master/gcc/doc/install.texi#L3439 in this web , tell me add this option\r\n    could be ok, but i don't know how to add to bazel when building...\r\n- arm64: errata: Check for --fix-cortex-a53-843419 and --fix-cortex-a53\r\n    https://patchwork.kernel.org/patch/9406341/\r\n    about the last of this web, here look like some info for help.\r\n\r\n\r\ncould you help me? thanks.", "comments": []}, {"number": 28585, "title": "The package org.tensorflow.lite.nnapi  does not exist", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution\uff1aUbuntu 16.04:\r\n- Mobile device\uff1aPixel 2\r\n- TensorFlow installed from\uff1asource\r\n- TensorFlow version:1.13.1\r\n- Python version:3.6\r\n- Bazel version :0.24.1\r\n\r\n\r\n**Describe the current behavior**\r\nwhen i run the demo which in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo\r\nso ,I found the bug,don't have  org.tensorflow.lite.nnapi \r\n\r\n\r\n\r\n", "comments": ["Excuse me, how long will this nnapi package be completed in org.tensorflow:tensorflow-lite:0.0.0-nightly?", "@wcq19941215 Could you provide more details about the bug and context? Also, it would be great if you can provide any commands you followed. Please provide as many details as possible to resolve the issue faster. Thanks!", "@gadagashwini .The detail about bug in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java\r\n\r\nand the code is:\r\nimport org.tensorflow.lite.nnapi.NnApiDelegate;\r\n\r\nbut I can't find the pack in org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly or org.tensorflow:tensorflow-lite:0.1.2-nightly\r\nso ,i used bazel ,want to get a new tensorflowlite aar , of course ,i can get a org.tensorflow.lite.nnapi.NnApiDelegate but it don't work \r\nthe bazel command i used is:\r\nbazel build --cxxopt='--std=c++11' -c opt             \\\r\n  --config=android_arm --config=monolithic          \\\r\n  //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops\r\n\r\nThanks for your help.^v^\r\n\r\n\r\n", "Facing same issue in the tflite demo app (tflite-gpu-nightly)...", "@anilsathyan7 Have you solved this problem?", "Thanks for flagging the issue, this is an issue with how sources are aggregated in the aar that gets uploaded. We'll have it fixed in the night TFLite nightly (org.tensorflow:tensorflow-lite:0.0.0-nightly).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28585\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28585\">No</a>\n", "  @jdduke That \u2018means you will add org.tensorflow.lite.nnapi to  org.tensorflow:tensorflow-lite:0.0.0-nightly no long?Thanks for your help", "Facing same issue in the tflite demo app (tflite-gpu-nightly)...\r\nerror: org.tensorflow.lite.nnapi does not exist\r\n\u9519\u8bef: \u7a0b\u5e8f\u5305org.tensorflow.lite.nnapi\u4e0d\u5b58\u5728", "@chenjiaoAngel You can try to compile a local AAR yourself. I tried it and successfully compiled lite.nnapi aar , but it can't work. It should be caused by conflict with other packages.", "Hi, thank you!\r\nSO the final solution is ? And how to build the local AAR ?\r\nBest wishes,\r\nJiao\r\n--------------------------\r\nTel: +8618101358192\r\nEmail: chenjiao04@baidu.com<mailto:chenjiao04@baidu.com>\r\nBaiduHi: AngelCJ\r\n--------------------------\r\nBaidu\r\n\r\nSYS\r\n\r\n\r\n\r\n\u53d1\u4ef6\u4eba: wcq19941215 <notifications@github.com>\r\n\u7b54\u590d: tensorflow/tensorflow <reply@reply.github.com>\r\n\u65e5\u671f: 2019\u5e746\u670811\u65e5 \u661f\u671f\u4e8c \u4e0b\u53484:03\r\n\u6536\u4ef6\u4eba: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\n\u6284\u9001: \"Chen,Jiao(SYS)\" <chenjiao04@baidu.com>, Mention <mention@noreply.github.com>\r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] The package org.tensorflow.lite.nnapi does not exist (#28585)\r\n\r\n\r\n@chenjiaoAngel<https://github.com/chenjiaoAngel> You can try to compile a local AAR yourself. I tried it and successfully compiled lite.nnapi aar , but it can't work. It should be caused by conflict with other packages.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/28585?email_source=notifications&email_token=AJG4D2HRB4EY6JORJ72JBVTPZ5L5DA5CNFSM4HMASMUKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXMIUGY#issuecomment-500730395>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AJG4D2H3CL37SPBDGIIL22LPZ5L5DANCNFSM4HMASMUA>.\r\n", "@chenjiaoAngel if you want to compile a local AAR ,you will use bazel.Finally, I did not solve this problem because there are other things that have been delayed.But I think that the latest aar package should have solved this problem, just need to update it in the AS.", "The NNAPI delegate is in the latest TFLite nightly @ org.tensorflow:tensorflow-lite:0.0.0-nightly. The GPU delegate is in a separate .aar, and should be used *in addition to* the core TFLite nightly. Add org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly to your dependencies. Note that you might need to [clear your gradle cache](https://stackoverflow.com/questions/23025433/how-to-clear-gradle-cache) for the new class to be visible.", "@jdduke Thanks,It can work<v>", "Since gradle automatically caches a copy of it while downloading it, gradle actually reloads the local cache when clean project\r\n.SO the final solution is delete the \".gradle/caches/\" ,and rebuild project.\r\nOn Windows: %USER_HOME%\\.gradle/caches/\r\nOn Mac/Unix: $HOME/.gradle/caches/"]}, {"number": 28584, "title": "_decayed_lr do not cast self.iterations when scheduler is used", "body": "PiecewiseConstantDecay do not accept float step, other schedulers are casting step by themselves", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28584) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28584) for more info**.\n\n<!-- ok -->", "@SimplifyAll can you please check build failures", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@SimplifyAll Did you get a chance to look on build failures? Please let us know on the update. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28583, "title": "Fix a bug in algebraic simplification pass.", "body": "In the dot contracting dimensions reorder optimization, check if the reshape op squishes consecutive dimensions, and bail out if it is not the case.\r\n\r\nThe current algorithm to compute unsquished lhs contracting dims in reshape input could result in non-consecutive dimensions. For example for A = f32[5,6] reshape(f32[1,5,2,3] B), the unsquished lhs contracting dims = {0, 2, 3}. The check for transpose only permutes contracting dims won't catch this case. Later transformation assume the contracting dims are consecutive to compute and compose permutations.", "comments": ["Hi @jlebar, this PR is to fix issue [28579](https://github.com/tensorflow/tensorflow/issues/28579). Could you help to review? Thanks!"]}, {"number": 28582, "title": "tf-nightly-gpu 1.14.1 failed to get device attribute 13 for device 0: CUDA_ERROR_UNKNOWN: unknown error", "body": "My Python version is 3.6.8, the operating system is Windows 10, the CUDA version is cuda_10.0.130,the cudnn version is cudnn-10.0-windows10-x64-v7.5.1.10, the GPU is Nvidia GTX965M, and the tf-nightly-gpu-1.14.1is installed.\r\nWhen I run  \"import tensorflow as tf\"\r\nthere is no problem.\r\nWhen I run  \"python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\n\r\nAn error was reported\uff1a\r\n2019-05-10 12:33:26.728437: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: failed to get device attribute 13 for device 0: CUDA_ERROR_UNKNOWN: unknown error\r\n\r\nHow to solve this problem\uff1fThanks\uff01\uff01", "comments": ["@FiveSpeedFrog Did you install tensorflow through Anaconda or pip? When you install tf-nightly-gpu-1.14.1, eager mode is activated by default. Can you just run the following code and send the error trace? Thanks!\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.reduce_sum(tf.random_normal([1000, 1000])))\r\n```", "Hello, I have given up using tf-nightly-gpu-1.14.1.\r\nI am using tensorflow-gpu==1.12.0, and there are no errors.\r\nStill thank you for your willingness to help.\r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28582\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28582\">No</a>\n", "FWIW, I got this error right now on tf-nightly-gpu. Upgraded tf-estimator-nightly and tb-nightly and the problem went away.", "> FWIW, I got this error right now on tf-nightly-gpu. Upgraded tf-estimator-nightly and tb-nightly and the problem went away.\r\n\r\nHow exactly did you upgrade those? I am getting the same error with tf 1.15 and CUDA v10. Thanks!", "I run into this error with tensorflow 2.1.0 and CUDA 10. Not always, but most of the times.", "Same issue using 2.1.0", "@Yuki-Nagato Can you please open a new issue with full error trace so that community will get benefited. Also, when you open new issue, please fill the issue template to describe details of your system. thanks!", "@ShixiangWang Can you please open a new issue with full error trace so that community will get benefited. Also, when you open new issue, please fill the issue template to describe details of your system. thanks!", "Hi, I have the same problem too. My system is lenovo z500 ideapad on windows 10 with 'GeForce GT 740M' GPU.\r\nwhen I installed tf 2  with cuda 10.2 and cuDNN 7.6 I was able to run pytorch command torch.cuda.is_available() and get True while tf could not recognize my GPU. \r\nThen I deleted cuda 10.2 and installed 10.1 and was able to find my GPU. \r\nHowever now I face problem of getting the name of gpu with command: tf.test.gpu_device_name()\r\nsome times it works and sometimes not.\r\nAlso the main problem is that when I want to run model(x_train[:1]) I get error during tf.matmul operation:\r\nthe log file give this trace:\r\nInternalError: Blas GEMM launch failed : a.shape=(1, 784), b.shape=(784, 128), m=1, n=128, k=784 [Op:MatMul] in Jupiter notebook inside.\r\nand the Jupyter console gives this error:\r\ncoreClock: 1.0325GHz coreCount: 2 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 13.41GiB/s\r\n2020-03-13 00:25:01.698025: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-03-13 00:25:01.724783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-03-13 00:25:01.749689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-03-13 00:25:01.773207: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-03-13 00:25:01.804865: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-03-13 00:25:01.829288: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-03-13 00:25:01.864333: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-03-13 00:25:01.882774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-03-13 00:25:01.898071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GT 740M computeCapability: 3.5\r\ncoreClock: 1.0325GHz coreCount: 2 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 13.41GiB/s\r\n2020-03-13 00:25:01.928428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-03-13 00:25:01.945189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-03-13 00:25:01.961029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-03-13 00:25:01.976427: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-03-13 00:25:01.991357: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-03-13 00:25:02.014377: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-03-13 00:25:02.040688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-03-13 00:25:02.070685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-03-13 00:25:03.699888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-03-13 00:25:03.718350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-03-13 00:25:03.733347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-03-13 00:25:03.747231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1408 MB memory) -> physical GPU (device: 0, name: GeForce GT 740M, pci bus id: 0000:01:00.0, compute capability: 3.5)\r\n2020-03-13 00:25:16.059771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-03-13 00:25:16.082033: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-03-13 00:25:16.108546: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-03-13 00:25:16.130726: W tensorflow/stream_executor/stream.cc:2041] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n[I 00:25:20.764 NotebookApp] Saving file at /Desktop/2020/tensorflow/Keras/Text Classification.ipynb\r\n[I 00:26:38.414 NotebookApp] Saving file at /Desktop/2020/tensorflow/beginners.ipynb\r\n\r\nPS. cuda 10.1 cudnn 7.6.5.32 and nvidia driver 425.31\r\n\r\n\r\n\r\n\r\n\r\n\r\n "]}, {"number": 28581, "title": "absl.flags._exceptions.UnrecognizedFlagError: Unknown command line flag 'h'", "body": "@jmhodges @ry @bmabey @djones I want to look at my own parameters in command line. I used `'python main.py -h '`  . But this error occurred 'absl.flags._exceptions.UnrecognizedFlagError: Unknown command line flag 'h''. I use tensorflow-gpu == 1.10.0 . Can you give me some advice ?\r\n\r\n*Here is my code :*\r\n```python\r\nFlags = tf.app.flags\r\n\r\nFlags.DEFINE_integer('rand_seed', 1 , 'random seed' )\r\n\r\n# Directories\r\nFlags.DEFINE_string('input_dir_LR', None, 'The directory of the input resolution input data, for inference mode')\r\nFlags.DEFINE_string('input_dir_HR', None, 'The directory of the input resolution input data, for inference mode')\r\nFlags.DEFINE_string('mode', 'inference', 'train, or inference')\r\nFlags.DEFINE_string('output_dir', None, 'The output directory of the checkpoint')\r\nFlags.DEFINE_string('output_pre', 'images', 'The output pre of the images')\r\nFlags.DEFINE_string('output_ext', 'jpg', 'The format of the output when evaluating')\r\nFlags.DEFINE_string('summary_dir', None, 'The dirctory to output the summary')\r\n\r\n# Models\r\nFlags.DEFINE_string('checkpoint', None, 'If provided, the weight will be restored from the provided checkpoint')\r\nFlags.DEFINE_integer('num_resblock', 16, 'How many residual blocks are there in the generator')\r\n\r\n# Machine resources\r\nFlags.DEFINE_string('cudaID', '0', 'CUDA devices')\r\n\r\nFLAGS = Flags.FLAGS\r\n```\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). \r\nMake sure you also include the exact command if possible to produce the output included in your test case. You can refer this [link](https://github.com/tensorflow/tensorflow/issues/new/choose) to fill in the template. Thanks!", "@shliang0603 hey, I'm not a tensorflow maintainer or anything so please don't include me like this. It sends me noisy notifications that I can't do anything about", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\n", "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution:\r\nLinux Fedora 30\r\n- TensorFlow version:\r\n1.13.1\r\n- Python version:\r\n3.7.3\r\n\r\n**Describe the current behavior**\r\n\r\nDepending on how source code using tf.app.flags is written, the -h, -help, -helpshort, and -helpfull arguments may cause exceptions to be triggered as opposed to displaying flags, default values, and programmer-provided information.\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen any value of -h, -help, -helpshort, -helpfull are provided as arguments to scripts using tf.app.flags, the help list should appear.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n''' mwe.py: displays tensorflow --help argument issue '''\r\nfrom tensorflow import app\r\napp.flags.DEFINE_string('myflag', 'Default', 'Help output')\r\nFLAGS = app.flags.FLAGS\r\nglobal_string = \"This string causes absl to begin processing the {0} flag\".format(FLAGS.myflag)\r\ndef main(_):\r\n  print(\"This string causes TF to race the absl parsing process, which kills the help menu using string %s\" % FLAGS.myflag)\r\nif __name__ == '__main__':\r\n  app.run(main)\r\n```\r\n\r\n**Other info / logs**\r\nRun the above code as follows to see the difference in output. The first run will trigger an exception due to Tensorflow attempting to use its implicit parsing procedure. The second run will not because the script will explicitly wait for all flags to be fully parsed before proceeding.\r\n```bash\r\npython mwe.py --help\r\nsed -i 's/garbage/#garbage/g' mwe.py\r\npython mwe.py --help\r\n```"]}, {"number": 28580, "title": "keras.layers.BatchNormalization.call() with default training param does not reflect K.learning_phase() Tensor input when fed into feed_dict", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 update 1809\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.12.0\r\n- **Python version**: 3.6.8\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDAv9.0.176 / cuDNNv7.4.2\r\n- **GPU model and memory**: NVIDIA GeForce GTX 1060 6GB RAM\r\n\r\n## Describe the problem\r\nWhen using keras.layers.BatchNormalization, K.learning_phase() does not seem to have any affect during training when BatchNormalization.\\_\\_call__ is called with training as default param. However, if K.learning_phase() is explicitly passed in as BatchNormalization.\\_\\_call__(training=K.learning_phase()), the model is able to train batch norm's moving mean and moving variance ops.\r\n\r\n#### Expected Behavior\r\nKeras.layers.BatchNormalization is expected to fill in K.learning_phase() if training=None and ops are updated during training.\r\n\r\n## Exact command to reproduce\r\nRunning below code snippet for BatchNormalization.\\_\\_call__(input) with non-supplied training param:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Activation, Input\r\nfrom tensorflow.train import AdamOptimizer\r\nimport numpy as np\r\nfrom keras import backend as K\r\n\r\nshape = (100, 4)\r\ndata = np.random.random(size=shape)\r\n\r\ntf.reset_default_graph()\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n  is_training = K.learning_phase()\r\n  input_tensor = tf.placeholder(tf.float32, shape=shape)\r\n  x = Input(tensor=input_tensor)\r\n  layer = Dense(units=128, activation='relu')(x)\r\n  layer = Dense(units=32, activation='relu')(layer)\r\n  layer = Dense(units=4, activation='linear')(layer)\r\n  bn = BatchNormalization()\r\n  # line of interest\r\n  layer = bn(layer)\r\n  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, bn.updates)\r\n  logits = Activation('relu')(layer)\r\n  loss = tf.losses.mean_squared_error(logits, x)\r\n  opt = AdamOptimizer()\r\n  grads_and_vars = opt.compute_gradients(loss)\r\n  opt = opt.apply_gradients(grads_and_vars)\r\n  variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\r\n                          scope='batch_normalization')\r\n\r\nprint([var.name for var in variables])\r\nwith tf.Session(graph=graph) as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  for idx in range(50):\r\n    out = sess.run([opt] + bn.updates, feed_dict={input_tensor:np.array(data), is_training:1})\r\n    if not idx % 5: \r\n      outs = [sess.run(var, feed_dict={input_tensor:np.array(data), is_training:0}) for var in variables]\r\n      outs = np.linalg.norm(outs, axis=1)\r\n      print(f'Running {idx}: {outs}')\r\n```\r\nProduces with the following output (notice that moving_mean=0 and moving_variance=2 throughout) :\r\n```\r\n['batch_normalization_v1/gamma:0', 'batch_normalization_v1/beta:0', 'batch_normalization_v1/moving_mean:0', 'batch_normalization_v1/moving_variance:0', 'batch_normalization_v1/gamma/Adam:0', 'batch_normalization_v1/gamma/Adam_1:0', 'batch_normalization_v1/beta/Adam:0', 'batch_normalization_v1/beta/Adam_1:0']\r\nRunning 0: [2.0005007e+00 1.7320185e-03 0.0000000e+00 2.0000000e+00 6.5435906e-04\r\n 3.3886202e-08 1.7219670e-02 2.0985259e-05]\r\nRunning 5: [2.0065410e+00 8.4475391e-03 0.0000000e+00 2.0000000e+00 1.4017796e-02\r\n 3.9132146e-06 1.1373939e-01 2.7749091e-04]\r\nRunning 10: [2.01387644e+00 1.56976394e-02 0.00000000e+00 2.00000000e+00\r\n 2.52754204e-02 1.10197625e-05 1.15519769e-01 3.48794798e-04]\r\nRunning 15: [2.01952314e+00 2.05438156e-02 0.00000000e+00 2.00000000e+00\r\n 1.88015848e-02 1.29885275e-05 7.03223869e-02 3.55401076e-04]\r\nRunning 20: [2.0201447e+00 2.0984445e-02 0.0000000e+00 2.0000000e+00 1.1408665e-02\r\n 1.6400263e-05 3.1892959e-02 3.6953186e-04]\r\nRunning 25: [2.0176027e+00 1.9309264e-02 0.0000000e+00 2.0000000e+00 1.3336688e-02\r\n 1.8967023e-05 3.1269908e-02 3.8181755e-04]\r\nRunning 30: [2.0159297e+00 1.7820496e-02 0.0000000e+00 2.0000000e+00 6.4963060e-03\r\n 1.9346602e-05 2.3151563e-02 3.8251214e-04]\r\nRunning 35: [2.0171123e+00 1.7269025e-02 0.0000000e+00 2.0000000e+00 6.5930812e-03\r\n 2.0723272e-05 6.8678367e-03 3.8147590e-04]\r\nRunning 40: [2.0202377e+00 1.7495051e-02 0.0000000e+00 2.0000000e+00 1.2473603e-02\r\n 2.2621665e-05 7.9890825e-03 3.8188352e-04]\r\nRunning 45: [2.0230916e+00 1.8221466e-02 0.0000000e+00 2.0000000e+00 1.3377106e-02\r\n 2.3886721e-05 1.5546208e-02 3.8239476e-04]\r\n```\r\n\r\nHowever, after explicitly adding training parameter to \\_\\_call__ with K.learning_phase(), training seems to be updating batch norm ops: \r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Activation, Input\r\nfrom tensorflow.train import AdamOptimizer\r\nimport numpy as np\r\nfrom keras import backend as K\r\n\r\nshape = (100, 4)\r\ndata = np.random.random(size=shape)\r\n\r\ntf.reset_default_graph()\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n  is_training = K.learning_phase()\r\n  input_tensor = tf.placeholder(tf.float32, shape=shape)\r\n  x = Input(tensor=input_tensor)\r\n  layer = Dense(units=128, activation='relu')(x)\r\n  layer = Dense(units=32, activation='relu')(layer)\r\n  layer = Dense(units=4, activation='linear')(layer)\r\n  bn = BatchNormalization()\r\n  # line of interest\r\n  layer = bn(layer, training=is_training)\r\n  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, bn.updates)\r\n  logits = Activation('relu')(layer)\r\n  loss = tf.losses.mean_squared_error(logits, x)\r\n  opt = AdamOptimizer()\r\n  grads_and_vars = opt.compute_gradients(loss)\r\n  opt = opt.apply_gradients(grads_and_vars)\r\n  variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\r\n                          scope='batch_normalization')\r\n\r\nprint([var.name for var in variables])\r\nwith tf.Session(graph=graph) as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  for idx in range(50):\r\n    out = sess.run([opt] + bn.updates, feed_dict={input_tensor:np.array(data), is_training:1})\r\n    if not idx % 5: \r\n      outs = [sess.run(var, feed_dict={input_tensor:np.array(data), is_training:0}) for var in variables]\r\n      outs = np.linalg.norm(outs, axis=1)\r\n      print(f'Running {idx}: {outs}')\r\n```\r\nNow moving_mean and moving_variance both seem to be updating during training as expected.\r\n```\r\n['batch_normalization_v1/gamma:0', 'batch_normalization_v1/beta:0', 'batch_normalization_v1/moving_mean:0', 'batch_normalization_v1/moving_variance:0', 'batch_normalization_v1/gamma/Adam:0', 'batch_normalization_v1/gamma/Adam_1:0', 'batch_normalization_v1/beta/Adam:0', 'batch_normalization_v1/beta/Adam_1:0']\r\nRunning 0: [1.9980000e+00 1.9999903e-03 2.0461062e-03 1.9801079e+00 2.8676972e-02\r\n 4.9131999e-05 1.5716424e-02 1.4908865e-05]\r\nRunning 5: [1.9882594e+00 1.1786965e-02 1.1019228e-02 1.8835746e+00 9.9898919e-02\r\n 1.8135854e-04 6.1359003e-02 7.0034890e-05]\r\nRunning 10: [1.9793329e+00 2.0299187e-02 2.0462982e-02 1.7917910e+00 1.1684086e-01\r\n 2.4303599e-04 6.9242075e-02 9.5857220e-05]\r\nRunning 15: [1.9711298e+00 2.7165966e-02 2.9484417e-02 1.7045155e+00 1.1539625e-01\r\n 2.8652389e-04 6.3280165e-02 1.0698218e-04]\r\nRunning 20: [1.9639944e+00 3.2249879e-02 3.6651909e-02 1.6214770e+00 9.9982716e-02\r\n 3.0927311e-04 5.2701045e-02 1.1211485e-04]\r\nRunning 25: [1.95816207e+00 3.52966003e-02 4.23905924e-02 1.54244149e+00\r\n 8.03107098e-02 3.20362276e-04 3.72414254e-02 1.13687325e-04]\r\nRunning 30: [1.9536946e+00 3.6224511e-02 4.7054645e-02 1.4672419e+00 6.2270045e-02\r\n 3.2455754e-04 3.2511890e-02 1.1524249e-04]\r\nRunning 35: [1.9503421e+00 3.6185008e-02 5.0670151e-02 1.3957475e+00 5.0311577e-02\r\n 3.2821088e-04 3.6015689e-02 1.1753518e-04]\r\nRunning 40: [1.9479088e+00 3.5928987e-02 5.3476196e-02 1.3277912e+00 4.1749101e-02\r\n 3.3096061e-04 3.5941143e-02 1.1917475e-04]\r\nRunning 45: [1.9461093e+00 3.5778154e-02 5.5816881e-02 1.2631813e+00 3.3145458e-02\r\n 3.3147351e-04 3.4038719e-02 1.2038982e-04]\r\n```\r\n\r\n", "comments": ["@monkeypride Able to reproduce two kinds of  output as mentioned in the template with the provided code.", "Before I get to the central point of the issue, it should be clarified that `keras` and `tf.keras` are distinct libraries. Specifically, both implement the Keras API; however interchanging `keras.thing` and `tf.keras.thing` is not supported since they have different implementations.\r\n\r\nI took the liberty of replacing `keras` with `tf.keras`, and pruning the repro down to something that only exercised batch norm:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import BatchNormalization, Input\r\nimport numpy as np\r\nfrom tensorflow.keras import backend as K\r\n\r\nshape = (100, 1)\r\ndata = np.ones(shape=shape)\r\n\r\ntf.reset_default_graph()\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n  is_training = K.learning_phase()\r\n  input_tensor = tf.placeholder(tf.float32, shape=shape)\r\n  x = Input(tensor=input_tensor)\r\n\r\n  bn = BatchNormalization()\r\n  # line of interest\r\n  layer = bn(x)\r\n  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, bn.updates)\r\n  variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\r\n                                scope='batch_normalization')\r\n\r\nprint([var.name for var in variables])\r\nwith tf.Session(graph=graph) as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  for idx in range(100):\r\n    out = sess.run([layer] + bn.updates, feed_dict={input_tensor:np.array(data) * idx, is_training:1})\r\n    if not (idx + 1) % 5:\r\n      outs = [sess.run(var, feed_dict={input_tensor:np.array(data) * idx, is_training:0}) for var in variables]\r\n      outs = \" \".join(\"{:.2f}\".format(float(i)).ljust(8) for i in outs)\r\n      print('Running {idx}: {outs}'.format(idx=idx, outs=outs))\r\n``` \r\n\r\nThat said, I don't think this is a supported pattern. (Relying on high level library concepts like training templating, but not using `tf.keras.backend.function` or `model.fit/evaluate/predict/...`) But in the example above it does at least use `is_training`, so I'm going to close this as intended behavior.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28580\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28580\">No</a>\n", "Yeah this looks like it's just because you're feeding the `keras.backend.learning_phase()` placeholder rather thanthe `tf.keras.backend.learning_phase()` placeholder. \r\n\r\nIt works fine for me if I replace\r\n\r\n```from keras import backend as K```\r\n\r\nwith \r\n\r\n```from tensorflow.keras import backend as K```"]}, {"number": 28579, "title": "[XLA] AlgebraicSimplifierTest::DotContractingReorder_NoChangeInContractingDimsOrder is broken (reads out-of-bounds in an array)", "body": "https://github.com/tensorflow/tensorflow/commit/3eb4a44f0536888881d49df60c18f528b7122d25 fixed a bug in `AlgebraicSimplifierTest::DotContractingReorder_NoChangeInContractingDimsOrder` that was causing it not to run.\r\n\r\nWith that bug fixed, the test now crashes XLA!  STR, get rid of \"DISABLED_\" from the front of the test and run it.  It should crash with an out-of-bounds array access in `ComposePermutation`.\r\n\r\n@BinFan would you be willing to have a look at this?  I can't assign the bug to you because you're not in the github TF organization, so I've assigned it to myself.", "comments": ["@jlebar  Sure. I'll take a look.", "I believe this was fixed by the PR above."]}, {"number": 28578, "title": "tensorflow 2.0 from_generator", "body": "While using from_generator, tensorflow is giving me this message. Is there a way to turn this off?\r\n\r\n> W0509 19:06:38.972821 139709270456064 deprecation.py:323] From /home/celine/.environments/tensorflow_new/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py:410: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> tf.py_func is deprecated in TF V2. Instead, there are two\r\n>     options available in V2.\r\n>     - tf.py_function takes a python function which manipulates tf eager\r\n>     tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n>     an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n>     means `tf.py_function`s can use accelerators such as GPUs as well as\r\n>     being differentiable using a gradient tape.\r\n>     - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n>     (it is not differentiable, and manipulates numpy arrays). It drops the\r\n>     stateful argument making all functions stateful.\r\n> ", "comments": ["Did you get chance to have a look on #26348. Let us know if that solves the problem. Thanks!", "The problem is gone by using the following code in tensorflow 2.0. It will be great if we won't see these messages without explicitly setting the logger level by us. But happy this is solved. Thank you.\r\n```\r\ntflogger = tf.get_logger()\r\ntflogger.setLevel('ERROR')\r\n```", "@celinew1221 : Good to know it solved your issue. ", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28578)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28578)\r\n"]}, {"number": 28577, "title": "tensorflow 2.0 giving tf.print message while using rejection_resample", "body": "I am using rejection_resample from tensorflow v2.0, this is giving me the following message. While this is no big deal, it is kind of annoying. I don't think this is an issue on my end, I didn't use any tf.print. It will be great to turn this off. Thank you.\r\n\r\n> \r\n> Instructions for updating:\r\n> Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\r\n> ```python\r\n>     sess = tf.Session()\r\n>     with sess.as_default():\r\n>         tensor = tf.range(10)\r\n>         print_op = tf.print(tensor)\r\n>         with tf.control_dependencies([print_op]):\r\n>           out = tf.add(tensor, tensor)\r\n>         sess.run(out)\r\n>     ```\r\n> Additionally, to use tf.print in python 2.7, users must make sure to import\r\n> the following:\r\n> \r\n>   `from __future__ import print_function`\r\n> ", "comments": ["@celinew1221 I ran the code provided in TF GPU 2.0.0-alpha0. Have not received any error.\r\n![image](https://user-images.githubusercontent.com/48215502/57530948-fe74e980-7355-11e9-8ad6-736c082a5c01.png). Please check and let us know.\r\n", "The code provided was in the error message itself. I am using rejection sampling function, and that produces this message. But I managed to get rid of it with the following code:\r\n\r\n```\r\ntflogger = tf.get_logger()\r\ntflogger.setLevel('ERROR')\r\n```"]}, {"number": 28576, "title": "[INTEL ML] Code clean - Mkl Pooling Ops", "body": "Remove MKL ML implementation of forward Mkl Pooling Ops.\r\n\r\nMKL ML is not supported anymore. It has been replaced with MKL DNN.", "comments": []}, {"number": 28575, "title": "[INTEL MKL] Code clean - MklConvOp (fwd)", "body": "Remove MKL ML implementation of forward MklConvOp.\r\n\r\nMKL ML is not supported anymore. It has been replaced with MKL DNN.", "comments": []}, {"number": 28574, "title": "[INTEL MKL] Code clean - MklSliceOp", "body": "Remove MKL ML related #if/#else/#endif statements related to MklSliceOp\r\n\r\nMKL ML is not supported anymore. It has been replaced with MKL DNN.", "comments": []}, {"number": 28573, "title": "7 cherrypicks", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28573) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28573) for more info**.\n\n<!-- cla_yes -->", "Any blockers to this PR? We are waiting for one of these cherry-picks to land on r1.14.", "1.14 is being recut on Monday, May 13 at 1 PM PST so I don\u2019t think we need\nany of these cherrypicks - will close this PR.\n\n*From: *Bairen Yi <notifications@github.com>\n*Date: *Sat, May 11, 2019 at 1:55 PM\n*To: *tensorflow/tensorflow\n*Cc: *Sundeep Gottipati, Author\n\nAny blockers to this PR? We are waiting for one of these cherry-picks to\n> land on r1.14.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/28573#issuecomment-491542852>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKEVL2BY6GEUCNQ3RUJZH23PU4XDTANCNFSM4HL6KN7A>\n> .\n>\n"]}, {"number": 28572, "title": "Does the order matter for importing tensorflow and keras?", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04;\r\n- I create a conda virtual environment:\r\nconda create -n tf_gpu tensorflow-gpu\r\nconda activate tf_gpu\r\n- TensorFlow installed in conda virtual environment: \r\nconda install -c conda-forge/label/cf201901 \r\n- TensorFlow version: 1.14.1-dev20190424;\r\n- Python version: 3.6.7;\r\n\r\n**Problem (1)**\r\nI am running python in linux terminal and get errors if I change the order of importing keras and tensorflow. Is there any explanation for why the following happens? What is the dependency relation between tensorflow and keras?\r\n\r\nNo error if I import keras first and then tensorflow:\r\n![image](https://user-images.githubusercontent.com/13983910/57488299-50dfd880-7278-11e9-8b60-78ac24a38f63.png)\r\n\r\nGot errors if I import tensorflow first and then keras (error occurs for both importing tensorflow and keras):\r\n![image](https://user-images.githubusercontent.com/13983910/57488524-f2672a00-7278-11e9-9ca9-b69d54778af3.png)\r\n\r\n![image](https://user-images.githubusercontent.com/13983910/57488548-03b03680-7279-11e9-96fd-c71d4200fe7c.png)\r\n\r\n**Problem (2)**\r\n- I add the virtual environment to my jupyter kernel by:\r\n```\r\nsource activate tf_gpu\r\npython -m ipykernel install --user --name tf_gpu\r\n```\r\n- I create a jupyter notebook and run the commands:\r\n```\r\nimport keras \r\nimport tensorflow\r\n```\r\nThe order of importing keras and tensorflow doesn't matter in jupyter notebook.\r\n\r\n**My question is:** \r\nWhy does the order of importing keras and tensorflow matter when I am running python script, but the order doesn't matter when I am running a jupyter notebook?\r\n", "comments": ["No need to import Keras and TensorFlow independently. When we import tensorflow, keras will also get imported automatically no matter whether you use jupyter notebook or terminal. Let us know if that resolves the issue.Thanks!", "Note that `keras` can be used without a proper tensorflow install. In your first screenshot that's exactly what happens: `import keras` gives a warning that it is using the Theano backend. Then you `import tensorflow` and override the previous `keras` import.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28571, "title": "[ROCm] Forward device function name from gpu prefix to cuda prefix", "body": "This PR is a follow-up to the original PR #28343. The reviewer requested to break down the original large PR to a series of smaller ones. According to the plan here, this PR is the 3rd one in the whole series. It is dependent on the infrastructure provided from PR #28564\r\n\r\n@chsigg @whchung", "comments": ["nit picking: remove the empty line (line 111), between `GpuGridRangeX` and `CREATE_CUDA_DEVICE_FUNCTION_ALIAS`. github UI somehow prevents me commenting in-place.", "@whchung Sounds good. I'll do a force push to avoid additional commit. For your convenience, the diff is below:\r\n\r\n> @@ -108,7 +108,6 @@ __device__ detail::GpuGridRange<T> GpuGridRangeX(T count) {\r\n   return detail::GpuGridRange<T>(blockIdx.x * blockDim.x + threadIdx.x,\r\n                                   gridDim.x * blockDim.x, count);\r\n }\r\n \\-\r\nCREATE_CUDA_DEVICE_FUNCTION_ALIAS(GpuGridRangeX, CudaGridRangeX);\r\n// Helper to visit indices in the range 0 <= i < count using the y-coordinate.\r\n   @@ -467,7 +466,6 @@ __host__ __device__ T GpuLdg(const T* address) {\r\n    return *address;\r\n #endif\r\n }\r\n\\-CREATE_CUDA_DEVICE_FUNCTION_ALIAS(GpuLdg, CudaLdg);\r\n__host__ __device__ inline bool GpuLdg(const bool* address) {\r\n   return GpuLdg(reinterpret_cast<const char*>(address)) != 0;\r\n@@ -492,6 +490,7 @@ __host__ __device__ inline std::complex<double> GpuLdg(\r\n   return *address;\r\n #endif\r\n }\r\n+CREATE_CUDA_DEVICE_FUNCTION_ALIAS(GpuLdg, CudaLdg);\r\n", "@chsigg There was a point when I get your approval, but lost that due to the need to rebase to fix conflicts. Would you mind taking a second look at this PR, and re-approve is possible? I'm asking because this PR will starts blocking others in ROCm team if not merged. "]}, {"number": 28570, "title": "TF 1.13.1 SequenceExample & Dataset/Estimator Saved Model: No attr named 'Ncontext_sparse' in NodeDef", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS X 10.14\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nIn TF 1.13.1, using a Dataset API pipeline that parses a [SequenceExample](https://www.tensorflow.org/api_docs/python/tf/train/SequenceExample) with [parse_single_sequence_example](https://www.tensorflow.org/api_docs/python/tf/io/parse_single_sequence_example) exported as a SavedModel with `strip_default_attrs` set to `True` (which is the default behavior for Estimators) fails with the following error:\r\n```\r\n2019-05-09 16:55:22.618817: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at optimize_dataset_op.cc:67 : Not found: No attr named 'Ncontext_sparse' in NodeDef:\r\n         [[{{node ParseSingleSequenceExample/ParseSingleSequenceExample}}]]\r\n```\r\nSee the full stacktrace below.\r\n\r\n**Describe the expected behavior**\r\nThe above error does not occur in TF 1.12, perhaps due to `DatasetV2` changes with `OptimizeDataset`. After some experimentation, I found the error does not occur when `strip_default_attrs` is set to `False` when exporting the SavedModel. Nor does it occur when using [Example](https://www.tensorflow.org/api_docs/python/tf/train/Example) in a similar capacity.\r\n\r\nThough the example below is intentionally mundane, I've found the  Dataset API functions like `padded_batch` to be extremely nice and was disappointed when I found I couldn't immediately use 1.13.1 saved models as I had been in previous versions.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tempfile\r\n\r\nimport tensorflow as tf\r\n\r\n# Set to `False`, and the below script produces 'Hello, World!'\r\nstrip_default_attrs = True\r\n\r\n\r\n# function to deserialize SequenceExample proto\r\ndef _parse_proto(proto):\r\n    return tf.parse_single_sequence_example(\r\n        serialized=proto,\r\n        context_features={'f': tf.FixedLenFeature([], tf.dtypes.string)})[0]\r\n\r\n\r\n# create a minimal SequenceExample\r\nfeature = tf.train.Feature(\r\n    bytes_list=tf.train.BytesList(value=[bytes('Hello, World!', encoding='utf-8')]))\r\nexample = tf.train.SequenceExample(\r\n    context=tf.train.Features(feature={'f': feature})).SerializeToString()\r\n\r\nwith tempfile.TemporaryDirectory() as tmp:\r\n    export_dir = tmp + '/export'\r\n\r\n    # build and export saved model to a temporary directory\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n        dataset = tf.data.Dataset.from_tensor_slices(tf.constant([example]))\r\n        dataset = dataset.map(_parse_proto)\r\n        dataset.make_one_shot_iterator().get_next(name='hello')\r\n\r\n        builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\r\n\r\n        builder.add_meta_graph_and_variables(\r\n            sess,\r\n            tags=[tf.saved_model.tag_constants.SERVING],\r\n            strip_default_attrs=strip_default_attrs)\r\n\r\n        builder.save()\r\n\r\n    # load and apply saved model\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n        tf.saved_model.loader.load(sess,\r\n                                   tags=[tf.saved_model.tag_constants.SERVING],\r\n                                   export_dir=export_dir)\r\n        # get element from dataset\r\n        result = sess.graph.get_tensor_by_name('hello:0')\r\n\r\n        # if strip_default_attrs is True, fails with \"NotFoundError: No attr named 'Ncontext_sparse' in NodeDef...\" error\r\n        print(str(sess.run(result), 'utf-8'))  # otherwise, will print 'Hello, World!'\r\n```\r\n\r\n**Other info / logs**\r\nFull stacktrace:\r\n```\r\n2019-05-09 16:55:22.618817: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at optimize_dataset_op.cc:67 : Not found: No attr named 'Ncontext_sparse' in NodeDef:\r\n         [[{{node ParseSingleSequenceExample/ParseSingleSequenceExample}}]]\r\n2019-05-09 16:55:22.618916: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at iterator_ops.cc:1022 : Not found: No attr named 'Ncontext_sparse' in NodeDef:\r\n         [[{{node ParseSingleSequenceExample/ParseSingleSequenceExample}}]]\r\n         [[{{node OptimizeDataset}}]]\r\nTraceback (most recent call last):\r\n  File \"/Users/jgung/.venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/jgung/.venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/Users/jgung/.venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'Ncontext_sparse' in NodeDef:\r\n         [[{{node ParseSingleSequenceExample/ParseSingleSequenceExample}}]]\r\n         [[{{node OptimizeDataset}}]]\r\n         [[{{node OneShotIterator}}]]\r\n```\r\n", "comments": ["For anyone experiencing this issue, you can work around it in the Estimator API by exporting your saved model using [export_savedmodel](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_savedmodel) with `strip_default_attrs` set to `False`. This isn't ideal since it may hurt the forwards compatibility of your exported model.", "Testing in later versions (1.14/1.15), this issue seems to be fixed now. Closing the ticket, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28570\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28570\">No</a>\n"]}, {"number": 28569, "title": "8 Cherrypicks", "body": "", "comments": ["Covered by https://github.com/tensorflow/tensorflow/pull/28573"]}, {"number": 28568, "title": "[ROCm] Forward host function names from gpu prefix to cuda prefix", "body": "This PR is a follow-up to the original PR #28343. The reviewer requested to break down the original large PR to a series of smaller ones. According to the plan here, this PR is the 3rd one in the whole series. It is dependent on the infrastructure provided from PR #28564\r\n\r\n@chsigg @whchung", "comments": []}]