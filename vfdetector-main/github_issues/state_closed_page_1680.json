[{"number": 2503, "title": "Push changes from internal: 123201123", "body": "No conflicts to resolve.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "@tensorflow-jenkins test this please\n", "@vrv @martinwicke ready to merge. \n"]}, {"number": 2502, "title": "Method to check for GPU op support", "body": "So, given the basic word2vec example being bound to CPU (#514), we can see that `tf.nn.embedding_lookup` doesn't work on GPU. Therefore, ops that use `embedding_lookup` internally doesn't support GPU either (for example, `nce_loss`).\nCan we have explicit info in the documentation on which operations are currently GPU-capable and which are not?\nFor example, are `tf.gather` or `LogUniformCandidateSampler` GPU-capable?\n", "comments": ["@vrv is there a way to check CPU/GPU capabilities of an operation from python? Something like print out a list of available backends for each operation?\nBecause I could update the documentation myself, but I was wondering if there is a more efficient way than just digging sources one operation at the time.\n", "There doesn't seem to be a more efficient way. Kernel registrations are\ndone through C++ registry mechanism, so some non-trivial piping would need\nto be done to expose it in Python.\n", "BTW, when tracking capabilities through code, one thing to keep in mind is\nthat there are logical devices and \"physical\" devices that don't\nnecessarily coincide. For instance, the following\nfrom core/kernels/cwise_op_add.cc shows that \"Add\" for int32 is registered\nfor GPU logical device, but the actual implementation will run on CPU.\n(This was done to avoid crossing logical device boundary for efficiency\nreasons)\n\nREGISTER_KERNEL_BUILDER(Name(\"Add\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"x\")\n                            .HostMemory(\"y\")\n                            .HostMemory(\"z\")\n                            .TypeConstraint<int32>(\"T\"),\n                        BinaryOp<CPUDevice, functor::add<int32>>);\n", "The fact that word2vec is bound to CPU is historical: `tf.gather` works fine on GPU now.  I believe `LogUniformCandidateSampler` is still CPU-only, though.\n\nUnfortunately making the docs not depend on kernel registrations helps significantly, and we'd lose those advantages if we presented this information in the docs.  However, a runtime feature for checking which ops have which kernels would be great, so I'll mark this as contributions welcome.\n", "A tool that could produce an HTML table with an overview of\n- which ops exist\n- which dtypes are supported\n- which backends are supported\n\nwould be pretty amazing when starting out with tensorflow, and would also be useful for making sure that things are defined in a consistent way.\nThe table could then also be included in the documentation.\n(Or should this rather be a feature of TensorBoard?)\nThis could be built on top of the runtime checking that's the topic of this issue.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "It's been more than a year, but I can't find any information on the subject. How can one check if a tensorflow operation is running/supported on CPU or GPU? Thanks!", "I wrote such a function `supported_devices_for_op` [here](https://github.com/rwth-i6/returnn/blob/master/TFUtil.py) ([this commit](https://github.com/rwth-i6/returnn/commit/ed47af101e0f750232c81944be154ff980eb70d9)).\r\nBasically it uses the TF C++ function `tensorflow::KernelsRegisteredForOp` which provides such information.\r\n", "@dantkz another way to check where your ops are running is using:\r\n`tf.ConfigProto(log_device_placement=True)`"]}, {"number": 2501, "title": "nan values management in max_pool", "body": "Hi,\n\nI faced a strange behavior using the max_pool op, returning -3.40282347e+38 when Nan are provided.\nA simple example run on CPU is shown bellow:\n\n```\n\nimport tensorflow as tf\nimport numpy as np\nx = tf.placeholder(tf.float32, [None, 2, 1, 1])\npool = tf.nn.max_pool(x, ksize=[1, 2, 1, 1], strides=[1,2,1,1], padding='VALID')\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\ndata = np.empty((1, 2, 1, 1))\ndata[:] = np.NAN\nprint sess.run(pool, feed_dict={x: data})\n\n[[[[ -3.40282347e+38]]]]\n```\n\nIs there any reason for returning this arbitrary value? I feel much more comfortable with conv2D behavior which returns NaN in similar situations. I guess this behavior should be mentioned within the documentation.\n\nRegards,\n", "comments": ["Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 2500, "title": "Test fail: //tensorflow_serving/session_bundle:session_bundle_py_test", "body": "http://ci.bazel.io/job/TensorFlow_Serving/BAZEL_VERSION=HEAD,PLATFORM_NAME=linux-x86_64/115/console\n\n```\n____From Testing //tensorflow_serving/session_bundle:session_bundle_py_test: \n====================\nTest output for //tensorflow_serving/session_bundle:session_bundle_py_test: .E. \n====================================================================== \nERROR: \ntestBasic (__main__.SessionBundleLoadTest)\n ---------------------------------------------------------------------- \nTraceback (most recent call last): \nFile \"/home/ci/.cache/bazel/_bazel_ci/e409f7152fa45ddf3337f1660b5273b9/execroot/linux-x86_64/bazel-out/local-fastbuild/bin/tensorflow_serving/session_bundle/session_bundle_py_test.runfiles/tf_serving/tensorflow_serving/session_bundle/session_bundle_test.py\", line 39, in testBasic base_path, target=\"\", config=tf.ConfigProto(device_count={\"CPU\": 2})) \nFile \"/home/ci/.cache/bazel/_bazel_ci/e409f7152fa45ddf3337f1660b5273b9/execroot/linux-x86_64/bazel-out/local-fastbuild/bin/tensorflow_serving/session_bundle/session_bundle_py_test.runfiles/tf_serving/tensorflow_serving/session_bundle/session_bundle.py\", line 57, in LoadSessionBundleFromPath meta_graph_filename) \nRuntimeError: Expected meta graph file missing /home/ci/.cache/bazel/_bazel_ci/e409f7152fa45ddf3337f1660b5273b9/execroot/linux-x86_64/bazel-out/local-fastbuild/bin/tensorflow_serving/session_bundle/session_bundle_py_test.runfiles/tensorflow_serving/session_bundle/example/half_plus_two/00000123/export.meta\n ---------------------------------------------------------------------- \nRan 3 tests in 0.001s FAILED (errors=1) \n================================================================================\n```\n", "comments": ["This issue tracker is for core TensorFlow issues. Please reopen this issue on the [TensorFlow Serving issues page](https://github.com/tensorflow/serving/issues).\n", "Trigger update_date\n"]}, {"number": 2499, "title": "Fedora 23: build fails with missing libcudart.so.7.5 without `--genrule_strategy=standalone`", "body": "Hi, this is a continuation of Issue #2053, because that one was closed for no apparent reason.\n\nCompiling the GPU version of the `tutorials_example_trainer` target fails on Fedora 23 with the message:\n\n```\nbazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc: error while loading shared libraries: libcudart.so.7.5: cannot open shared object file: No such file or directory\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n```\n\nWhen using the command line\n\n```\npath/to/bazel/output/bazel build -c opt --config=cuda -j 4 //tensorflow/cc:tutorials_example_trainer\n```\n\nWhen adding `--genrule_strategy=standalone`, the compilation succeeds.\n\nI believe that either the Documentation should be updated to include this flag, or that the build system is changed to pass it automatically.\n", "comments": ["https://github.com/tensorflow/tensorflow/pull/2504  can you let me know if that solves the problem ?\n", "Thanks, as far as I can see this fixes the problem.\n"]}, {"number": 2498, "title": "Upstream changes from internal for 5/24/2016", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 2497, "title": "Setting up TensorFlow for Development broken on Linux", "body": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md\n\nTLDR;\ninstead of\n`ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/* .\n`\nI had to do\n`ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/__main__/* .\n`\nUbuntu 15.04, bazel-0.2.3, week-old TensorFlow from head\n\nOn MacOS, Bazel 0.2.1-homebrew, the command that works is `ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/* .`\n", "comments": ["cc @kchodorow \n", "Oops, I guess a week-old version is too old. I just synced to head and now the instructions are current\n", "Ugh, this is inconvenient:\n- Bazel 0.2.1 and lower, any TF: the path will be `build_pip_package.runfiles/*`\n- Bazel 0.2.2 with TensorFlow before 32fa42a98eb751ee05827535ac47535312b7fa76: `build_pip_package.runfiles/*`\n- Bazel 0.2.2, TF after 32fa42a98eb751ee05827535ac47535312b7fa76: `build_pip_package.runfiles/org_tensorflow/*`.\n- Bazel 0.2.3, TF before 32fa42a98eb751ee05827535ac47535312b7fa76: `build_pip_package.runfiles/__main__/*`\n- Bazel 0.2.3, TF after 32fa42a98eb751ee05827535ac47535312b7fa76: `build_pip_package.runfiles/org_tensorflow/*`\n", "@kchodorow btw, perhaps related, at HEAD I'm seeing following ominous-looking warnings\n\n```\n\nWARNING: /home/yaroslavvb/.cache/bazel/_bazel_yaroslavvb/4361dd5b218cfc4553165f848068d48a/external/highwayhash/WORKSPACE:1: Workspace name in /home/yaroslavvb/.cache/bazel/_bazel_yaroslavvb/4361dd5b218cfc4553165f848068d48a/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.\nWARNING: /home/yaroslavvb/.cache/bazel/_bazel_yaroslavvb/4361dd5b218cfc4553165f848068d48a/external/re2/WORKSPACE:1: Workspace name in /home/yaroslavvb/.cache/bazel/_bazel_yaroslavvb/4361dd5b218cfc4553165f848068d48a/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\n\n```\n", "BTW, current instructions for latest bazel are a slightly broken:\n`ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow* .\n`\n\nIt should say\n`ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/* .`\n", "Send a PR to fix?\n\nOn Sat, May 28, 2016, 2:16 PM Yaroslav Bulatov notifications@github.com\nwrote:\n\n> BTW, current instructions for latest bazel are a slightly broken:\n> ln -s\n> ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow*\n> .\n> \n> It should say\n> ln -s\n> ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/*\n> .\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2497#issuecomment-222330071,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAcTeaOcxUT9H3Bb4XylXeUfsKRGksdEks5qGLDBgaJpZM4ImKHo\n> .\n"]}, {"number": 2496, "title": "tf.pad docstring is missing description of the \"mode\" argument", "body": "The docstring gives examples of using \"CONSTANT\", \"REFLECT\" and \"SYMMETRIC\" modes, but never describes what they actually are.\n\nFor example:\n- What value is used to pad in constant mode? (The answer is 0.)\n- What exactly is the difference between REFLECT and SYMMETRIC? (I don't know.)\n", "comments": ["Cool, thanks for the report.  http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.pad.html is I think where we get these names from -- do you want to send us a PR to fix this?\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L922 would be where to add the documentation.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 2495, "title": "Truncate random seed to fit into int during protobuf serialization", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/2460\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "Looks like flakiness in matmul test, trying again.  @tensorflow-jenkins test this please\n"]}, {"number": 2494, "title": "Saver.save inconsistently throws InvalidArgumentError with Batch Normalization", "body": "Hi,\n\nFirst off - thanks for releasing TensorFlow. It is immensely helpful in many ways (e.g. getting it working on my GPU was a breeze). However, because of its novelty I'm running into a problem I cannot just Google.\n\nAfter adding batch normalization to the code for a variational autoencoder found here:\nhttps://github.com/tensorflow/tensorflow/issues/new\n\nsaver.save() starts crashing inconsistently (sometimes it will save, sometimes it will crash). The error is unhelpful, so I'm not sure what is wrong (see error logs below).\n\nI call the saver as follows:\n\n```\n    saver = tf.train.Saver()\n        with tf.Session(graph=vae.graph) as sess:\n\n            tf.train.SummaryWriter(\"/tmp/vae_logs\", sess.graph)\n            init = tf.initialize_all_variables()\n```\n### Environment info\n\nOperating System:\nUbuntu 15.10\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root   322936 aug 15  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 61453024 apr  3 10:09 /usr/local/cuda-7.5/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 61453024 apr  3 10:09 /usr/local/cuda-7.5/lib64/libcudnn.so.4\n-rwxr-xr-x 1 root root 61453024 apr  3 10:09 /usr/local/cuda-7.5/lib64/libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 apr  3 10:09 /usr/local/cuda-7.5/lib64/libcudnn_static.a\n\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\n```\npip freeze \ntensorflow==0.8.0\n```\n1. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n```\n print(tensorflow.__version__)\n0.8.0\n```\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. Adapted the code from https://github.com/arahuja/generative-tf/tree/master/generative-tf for a Variational Autoencoder for Gaussian decoders\n   (this works fine)\n2. Added batch normalization:\n   Added the following to variational_autoencoder.py in def **init**:\n\n```\nif self.batch_normalized:\n                self.scale_decoder = tf.Variable(tf.ones([hidden_dim], name=\"decoder_scale\"))\n                self.beta_decoder  = tf.Variable(tf.zeros([hidden_dim], name=\"decoder_beta\"))\n\n                self.scale_encoder = tf.Variable(tf.ones([hidden_dim], name=\"encoder_scale\"))\n                self.beta_encoder  = tf.Variable(tf.zeros([hidden_dim], name=\"encoder_beta\" ))\n```\n1. Added the following to the _generate and _encode functions (changing the different variables of course):\n\n```\nif self.batch_normalized:\n                layer_output = tf.matmul(z, self._decoder_W) + self._decoder_bias\n                batch_mean, batch_var = tf.nn.moments(layer_output, axes=[0])\n                epsilon = 1e-4\n                batch_normalized = tf.nn.batch_normalization(layer_output, batch_mean, batch_var, self.beta_decoder, self.scale_decoder, epsilon, name=\"normalization_decoder\")\n                h = self.activation_func(batch_normalized)\n\n```\n1. Now sometimes the saver throws a very cryptic 'InvalidArgumentError', but it is unclear to me why.\n### What have you tried?\n1. Changed the names that I store the models in (does not help)\n2. Turned off Batch normalization (helps)\n3. Went through source code trying to track the error; but got stuck (not sure what protobuf does).\n4. Tried using less or more epochs (does not help)\n\nIs this a known bug?\n\nBest regards\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n```\n\nTraceback (most recent call last):\n  File \"train_mnist_vae.py\", line 374, in <module>\n    batch_normalized=args.bn\n  File \"train_mnist_vae.py\", line 212, in train_test_mnist_vae\n    save_path = saver.save(sess, os.path.join(\"saved_models/\", model_path + \".ckpt\"))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1039, in save\n    {self.saver_def.filename_tensor_name: checkpoint_file})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: saved_models/bern=False_bn=True_bs=100_ds=frey_epochs=40000_ff=True_hd=300_iw=False_ld=2_lr=1e-05_mm=0_0_nt=2000_opt=rmsprop_wd=0_1.ckpt.tempstate10425989302811289116\n     [[Node: save/save = SaveSlices[T=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/save/tensor_names, save/save/shapes_and_slices, Variable/_48233, Variable/RMSProp/_48235, Variable/RMSProp_1/_48237, Variable_1/_48239, Variable_1/RMSProp/_48241, Variable_1/RMSProp_1/_48243, Variable_2/_48245, Variable_2/RMSProp/_48247, Variable_2/RMSProp_1/_48249, Variable_3/_48251, Variable_3/RMSProp/_48253, Variable_3/RMSProp_1/_48255, decoderB/_48257, decoderB/RMSProp/_48259, decoderB/RMSProp_1/_48261, decoderW/_48263, decoderW/RMSProp/_48265, decoderW/RMSProp_1/_48267, encoderB/_48269, encoderB/RMSProp/_48271, encoderB/RMSProp_1/_48273, encoderW/_48275, encoderW/RMSProp/_48277, encoderW/RMSProp_1/_48279, log_var_decoder/_48281, log_var_decoder/RMSProp/_48283, log_var_decoder/RMSProp_1/_48285, log_var_decoder_b/_48287, log_var_decoder_b/RMSProp/_48289, log_var_decoder_b/RMSProp_1/_48291, log_var_encoder/_48293, log_var_encoder/RMSProp/_48295, log_var_encoder/RMSProp_1/_48297, log_var_encoder_b/_48299, log_var_encoder_b/RMSProp/_48301, log_var_encoder_b/RMSProp_1/_48303, mean_decoder/_48305, mean_decoder/RMSProp/_48307, mean_decoder/RMSProp_1/_48309, mean_decoderB/_48311, mean_decoderB/RMSProp/_48313, mean_decoderB/RMSProp_1/_48315, mean_encoder/_48317, mean_encoder/RMSProp/_48319, mean_encoder/RMSProp_1/_48321, mean_encoderB/_48323, mean_encoderB/RMSProp/_48325, mean_encoderB/RMSProp_1/_48327)]]\nCaused by op u'save/save', defined at:\n  File \"train_mnist_vae.py\", line 374, in <module>\n    batch_normalized=args.bn\n  File \"train_mnist_vae.py\", line 81, in train_test_mnist_vae\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 833, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 501, in build\n    save_tensor = self._AddSaveOps(filename_tensor, vars_to_save)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 197, in _AddSaveOps\n    save = self.save_op(filename_tensor, vars_to_save)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 149, in save_op\n    tensor_slices=[vs.slice_spec for vs in vars_to_save])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py\", line 172, in _save\n    tensors, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 341, in _save_slices\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 661, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": ["@sherrym's out, so reassigning to @josh11b for now.\n", "I'm sorry, I have no idea about this.\n", "@sherrym Any update on this?\n", "No.\n\nOn Mon, Aug 15, 2016 at 4:36 PM, Suharsh Sivakumar <notifications@github.com\n\n> wrote:\n> \n> @sherrym https://github.com/sherrym Any update on this?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2494#issuecomment-239960502,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AMLa9WtX48CQQeOoBREmb1IVZJWtTuAiks5qgPgBgaJpZM4Il5DW\n> .\n", "@tensorfiend Are you still experiencing this issue?\n", "I think with v2 checkpoints, this issue may be obsolete.\r\nAlso no activity for a long time.\r\nClosing the issue.", "Facing the same issue, can this be re-opened?", "Are you using v2 checkpoints?\r\nIf not, could you try with v2 checkpoints, and if you are still facing the problem, please report a new issue as that will probably be a something new."]}, {"number": 2493, "title": "updated README file", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 2492, "title": "seq2seq decoding after training in the same script", "body": "According to [FAQ](https://www.tensorflow.org/versions/r0.8/resources/faq.html), when session is closed, the model should be removed. \n\nHowever, when trying to test the seq2seq model in TensorFlow 0.7.1 right after training (in a different session, but within the same python script), an error appears.\n\nSpecifically, [rnn/translate/translate.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py)\n\nI add `decode()` function after training in order to not re-run the script for decoding:\n\n```\ndef main(_):\n   train()\n   decode()\n```\n\nAfter training I get the following error message:\n\n>  ValueError: Over-sharing: Variable embedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding already exists, disallowed. Did you mean to set reuse=True in VarScope?\n\n[Log file with error](https://github.com/tensorflow/tensorflow/files/280063/error_translate.txt)\n\nWhen running decoding separately, this code works fine. So, I am just confused why closing training session and opening a new one for decoding makes the program stuck...does it mean some model variables are not removed when session closes?\n", "comments": ["@lukaszkaiser any ideas when or why this could happen?  This seems like a python-level issue, not a runtime one...\n", "Variables are stored with the graph, not session. So it looks like we're changing the session but the graph remains. I'm not 100% sure what's the intention, will try to construct a much smaller example tomorrow (it doesn't look like anything specific to seq2seq, but an interaction between session and variables).\n", "Indeed, I can confirm that variables are stored in the graph which is not reset with session. So this is behaving as intended. If you want to create a graph twice, separately, do it like this:\n\n  with tf.Graph().as_default():\n    train()  # or anything else...\n  with tf.Graph().as_default():\n    decode()  # or anything else...\n", "Thanks. This fixed the problem\n", "@gorinars Hi, I'm new to tensorflow. \r\nI have the same error.\r\nCould you please give me some details about how to modify the codes?\r\nThanks!", "@fighting41love not sure which code you are working on, but what helped was what @lukaszkaiser suggested above.\r\n\r\nYou can check [this example](https://github.com/cmusphinx/g2p-seq2seq/blob/master/g2p_seq2seq/app.py)", "Thanks! @gorinars  \r\nI'm working on this script: [translate.py (code-to-comment)](https://github.com/thaije/code-to-comment/blob/master/seq2seq/translate.py)\r\nIn function train() (line 224), we just remove the \"#\" from lines 277 and 278.\r\nIn this case, the code can train a model for every 5000 steps, then save the model, and then evaluate the performance.\r\n\r\nAn error appears as follows:\r\n\r\nTraceback (most recent call last):\r\n  File \"translate.py\", line 443, in <module>\r\n    tf.app.run()\r\n  File \"/anaconda/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv))\r\n  File \"translate.py\", line 440, in main\r\n    train()\r\n  File \"translate.py\", line 291, in train\r\n    translate_file(source_path=dev_code_file,target_path=translated_dev_code)\r\n  File \"translate.py\", line 149, in translate_file\r\n    model = create_model(sess, True)\r\n  File \"translate.py\", line 225, in create_model\r\n    forward_only=forward_only)\r\n  File \"/Users/yy/PycharmProjects/test_code2comment/seq2seq/seq2seq_model.py\", line 89, in __init__\r\n    w = tf.get_variable(\"proj_w\", [size, self.target_vocab_size])\r\n  File \"/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 339, in get_variable\r\n    collections=collections)\r\n  File \"/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 262, in get_variable\r\n    collections=collections, caching_device=caching_device)\r\n  File \"/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 118, in get_variable\r\n    name, \"\".join(traceback.format_list(tb))))\r\n**ValueError: Variable proj_w already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:**\r\n\r\n  File \"/Users/yy/PycharmProjects/test_code2comment/seq2seq/seq2seq_model.py\", line 89, in __init__\r\n    w = tf.get_variable(\"proj_w\", [size, self.target_vocab_size])\r\n  File \"translate.py\", line 225, in create_model\r\n    forward_only=forward_only)\r\n  File \"translate.py\", line 248, in train\r\n    model = create_model(sess, False)\r\n\r\nThe translate.py of code-to-comment is a little different from this [example](https://github.com/cmusphinx/g2p-seq2seq/blob/master/g2p_seq2seq/app.py).\r\n\r\nI think the problem is the same. But I don't know how to fix the error in my code...\r\n\r\nThanks!\r\n\r\n\r\n"]}, {"number": 2491, "title": "Buggy exit/termination after a couple of iterations ", "body": "Buggy execution after a couple of iterations. I am running python3.5 on a mac and the model quits. Not sure if this is a compatibility issue with the Py3.5 site packages. Help appreciated! Thanks \n\nAverage loss at step  100000 :  4.67189269698\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/kg/anaconda/lib/python3.5/site-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 699, in runfile\n execfile(filename, namespace)\nFile \"/Users/kg/anaconda/lib/python3.5/site-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 88, in execfile\n    exec(compile(open(filename, 'rb').read(), filename, 'exec'), namespace)\n  File \"/Users/kg/TF/trial_cnn.py\", line 243, in <module>\n    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n  File \"/Users/kg/anaconda/lib/python3.5/site-packages/sklearn/manifold/t_sne.py\", line 866, in fit_transform\n    embedding = self._fit(X)\n  File \"/Users/kg/anaconda/lib/python3.5/site-packages/sklearn/manifold/t_sne.py\", line 777, in _fit\n    skip_num_points=skip_num_points)\n  File \"/Users/kg/anaconda/lib/python3.5/site-packages/sklearn/manifold/t_sne.py\", line 832, in _tsne\n    params, error, it = _gradient_descent(obj_func, params, **opt_args)\n  File \"/Users/kg/anaconda/lib/python3.5/site-packages/sklearn/manifold/t_sne.py\", line 387, in _gradient_descent\n    grad_norm = linalg.norm(grad)\n  File \"/Users/kg/anaconda/lib/python3.5/site-packages/scipy/linalg/misc.py\", line 129, in norm\n    a = np.asarray_chkfinite(a)\n  File \"/Users/kg/anaconda/lib/python3.5/site-packages/numpy/lib/function_base.py\", line 1022, in asarray_chkfinite\n    \"array must not contain infs or NaNs\")\nValueError: array must not contain infs or NaNs\n", "comments": ["It looks like your embeddings have NaNs, most likely caused by your model training not being stable, which I suspect is not a TensorFlow bug.  You might get more help on StackOverflow for non-bugs like these (unless you can demonstrate to us that there's a bug here).\n"]}, {"number": 2490, "title": "Modify zero_state to work with MultiRNNCell when state_is_tuple is True", "body": "#2463\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@googlebot I signed a CLA.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks for the contribution, can you add a test for these changes so that we don't regress in the future?\n", "Implemented a test on my implementation.\nSince the current version of `testBasicLSTMCellWithStateTuple` in rnn_cell_test.py is not properly using state_is_tuple, (it is using state_is_tuple only on `MultiRNNCell`, not on `BasicLSTMCell`), I modified `testBasicLSTMCellWithStateTuple` to use state_is_tuple on both cells.\n", "Looks good!  Thanks.  @tensorflow-jenkins test this please\n", "Sure, will kick off a test:  @tensorflow-jenkins test this please\n\n@ebrevdo should still look at this though \n", "Some test failures -- you can test locally by running bazel test tensorflow/... to make sure you catch all failures.\n", "We have a change internally that should fix nested tuples.  It obsoletes\nthe rnn changes in this pr.  At our next public push you should rebase and\nwe can see about the ptb example updates.\nOn May 24, 2016 9:09 PM, \"Vijay Vasudevan\" notifications@github.com wrote:\n\n> Sure, will kick off a test: @tensorflow-jenkins\n> https://github.com/tensorflow-jenkins test this please\n> \n> @ebrevdo https://github.com/ebrevdo should still look at this though\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2490#issuecomment-221468342\n", "Can one of the admins verify this patch?\n", "Wow, the test works properly after completely reinstalling the whole library...\n\nI didn't know that this is internally fixed, so I will left only PTB examples.\n"]}, {"number": 2489, "title": "New added variables cannot be saved", "body": "I tested by running these 3 pieces of code respectively.\n\nFirst, init some variables and save.\n\n```\nimport tensorflow as tf\n\nsess = tf.InteractiveSession()\nv1 = tf.Variable(1,name=\"v1\")\nv2 = tf.Variable(2,name=\"v2\")\nsess.run(tf.initialize_all_variables())\nsaver = tf.train.Saver()\nsaver.save(sess,'v12.ckpt')\n```\n\nThen, restore the session, add one more variable, and save.\n\n```\nimport tensorflow as tf\n\nsess = tf.InteractiveSession()\nv1 = tf.Variable(1,name=\"v1\")\nv2 = tf.Variable(2,name=\"v2\")\nsaver = tf.train.Saver()\nsaver.restore(sess,'v12.ckpt')  #works fine here\n\nv3 = tf.Variable(3,name=\"v3\")\nsess.run(tf.initialize_variables([v3]))\n\nsaver.save(sess,'v123.ckpt')\n\nprint v3.eval() #show value without problem\n```\n\nThen, restore them.\n\n```\nimport tensorflow as tf\n\nsess = tf.InteractiveSession()\nv1 = tf.Variable(1,name=\"v1\")\nv2 = tf.Variable(2,name=\"v2\")\nv3 = tf.Variable(3,name=\"v3\")\n\nsaver = tf.train.Saver()\nsaver.restore(sess,'v123.ckpt') #error here\n```\n\nThis is the error:\n\n```\ntensorflow.python.framework.errors.NotFoundError: Tensor name \"v3\" not found in checkpoint files v123.ckpt [[Node: save/restore_slice_2 = RestoreSlice[dt=DT_INT32, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]] Caused by op u'save/restore_slice_2'\n```\n\nWhat is the problem here?\n\nI'm using version r0.8 on Ubuntu16.04\n", "comments": ["When you create a `tf.train.Saver` with no arguments, it will implicitly use the current set of variables _at the time of Saver construction_ when it saves and restores. If you add a new variable (e.g. `v3` in your second code block), you have to create a new `tf.train.Saver` to save it.\n\n``` python\nimport tensorflow as tf\n\nsess = tf.InteractiveSession()\nv1 = tf.Variable(1,name=\"v1\")\nv2 = tf.Variable(2,name=\"v2\")\nsaver = tf.train.Saver()\nsaver.restore(sess,'v12.ckpt')  #works fine here\n\nv3 = tf.Variable(3,name=\"v3\")\nsess.run(tf.initialize_variables([v3]))\n\nsaver_with_v3 = tf.train.Saver()\nsaver_with_v3.save(sess,'v123.ckpt')\n```\n"]}, {"number": 2488, "title": "change cudnn from v5 to v4", "body": "  Hi there,\nCurrently I'm using cuDNN v5, which is not supported for tensorflow via pip install.\n\nI dont want to install tf from source code.\n\nso is there any way that I could sort of down-grading cudnn from v5 to v4?\n\nThanks!\n", "comments": ["I assume just install cudnn v4 without changing anything ,right?\n", "right. install v4 will do the trick\n"]}, {"number": 2487, "title": "regression in skflow monitoring code", "body": "Using the version [pip_gpu version #100](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/100), I noted that a corrected issue by ilblackdragon ([#2063](https://github.com/tensorflow/tensorflow/issues/2063)) re-occurred and a PR from him self [Ref #2063](https://github.com/ilblackdragon/tensorflow/commit/534deda849e389ea625cc3abca632177c95b9534#diff-0ac50f1931e3fc2819c02d8767edb1d3R29) has disappeared in the master code.\n\nCould the issue be corrected and the enhancement reintegrated ?\nCordially \n", "comments": ["Let me look into this - we have refactored a large chunk of code, including monitoring code so may have missed some parts of functionality.\n", "FYI, got a fix, but after adding some more tests to check that `ValidationMonitor` actually works correctly got a problem with flaky test (result change slightly every time I run) - trying to debug what is happening there. \n\nWe will cherry pick that fix into the 0.9 release though.\n"]}, {"number": 2486, "title": "Problem in header file", "body": "On this [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/core/status.h#L22). \nThe file `error_codes.pb.h` is not present.\n", "comments": ["This file is automatically generated (using `protoc`) from [`error_codes.proto`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/core/error_codes.proto). If you build TensorFlow using Bazel, the file will be generated for you. Otherwise, you will need to run `protoc` on that file to generate the C++ implementation of that type (and the other protocol buffers that TensorFlow uses).\n"]}, {"number": 2485, "title": "Fix wrong bazel test target in ffmpeg integration_test", "body": "", "comments": []}, {"number": 2484, "title": "Importing tensorflow and PIL conflict.", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: \n cat /etc/*-release\nDISTRIB_ID=LinuxMint\nDISTRIB_RELEASE=17.2\nDISTRIB_CODENAME=rafaela\nDISTRIB_DESCRIPTION=\"Linux Mint 17.2 Rafaela\"\nNAME=\"Ubuntu\"\nVERSION=\"14.04.4 LTS, Trusty Tahr\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 14.04.4 LTS\"\nVERSION_ID=\"14.04\"\nHOME_URL=\"http://www.ubuntu.com/\"\nSUPPORT_URL=\"http://help.ubuntu.com/\"\nBUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"\ncat: /etc/upstream-release: Is a directory\n\nIf installed from binary pip package, provide:\n1. 1.5.4\n2. Tensorflow version: 0.8.0\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\nIn python.\n\nimport tensorflow as tf\nfrom PIL import Image\nimageis = Image.open(\"Image.jpeg\")\nimageis.load()\n\nproduces this error. \nIOError: broken data stream when reading image file\n### What have you tried?\n\nIn order to fix this error, you have to import PIL first. \nfrom PIL import Image\nimport tensorflow as tf\nimageis = Image.open(\"Image.jpeg\")\nimageis.load()\n\nand everything works fine. \n### Logs or other output that would be helpful\n\nI wanted to submit this issue just in case someone else is affected by it. It took me a few hours to solve as i was searching my system for corrupt dependencies.\n", "comments": ["This problem should have been fixed in the latest nightly version of TensorFlow. Can you try downloading the nightly PIP package for your platform and see if the problem persists?\n", "(Closing this due to lack of activity. The problem should be fixed at HEAD, but let us know if it isn't.)\n", "Trigger update_date\n"]}, {"number": 2483, "title": "I got a problem with Tensorflow installation mac", "body": "https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#on-macosx\n\nI followed the all instructions for mac installation\n![screen shot 2016-05-24 at 8 49 07 am](https://cloud.githubusercontent.com/assets/19249239/15487984/a0fe6182-218c-11e6-9f75-2697ff446ac5.png)\n\nAfter I typed \"import tensorflow as tf\" this shows some error but no idea what those lines mean..\n\nNeed help\n", "comments": ["I would step-by-step copy paste the instructions, since you've typed the installation instructions incorrectly.  Please comment further if you have evidence that our existing instructions are not enough.\n"]}, {"number": 2482, "title": "Tensor board installing issue with docker", "body": "Hi I'm a new tensor flow installer~\nI do not have any experienced with docker or tensor flow and i followed this web-cite \n\nhttp://www.netinstructions.com/how-to-install-and-run-tensorflow-on-a-windows-pc/\n\nhttps://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#requirements\n\n![1](https://cloud.githubusercontent.com/assets/19249239/15487824/30c4a332-218b-11e6-8017-6b51cc276439.PNG)\n\n[this screen stopped after showing this image]\n\n![2](https://cloud.githubusercontent.com/assets/19249239/15487829/398ded3e-218b-11e6-8ac8-3bb39792301a.PNG)\n\n[I do not know what they are doing in actual cmd line but it does not work]\n\nThose are the stops I got stock.  can anyone help?\n", "comments": ["After running `docker run -it ...`, it says the Jupyter notebook server is running at http://[your ip]:8888\n\nIf you navigate to that address, you should be able to get things running.  Please comment if that's not the case.\n", "hey #vrv thanks for your comment. I'm new to this so not quite followed what you are saying. \ncan you tell me more about your solution?  \n", "in your terminal, run\n\n`docker run -it gcr.io/tensorflow/tensorflow`\n\nOnce that finishes, in your browser go to http://0.0.0.0:8888\n", "In addition, you might get more help on StackOverflow, this is only for bug reports and feature requests.\n", "um.. i can not connect too  http://0.0.0.0:8888\n", "Alright, then I'm not really sure -- try asking on StackOverflow and you should get some better advice.\n", "I also have the same problem. Did you, @newbornking999 ever figure it out? \n", "I upgraded every version of my mac-os then problem solved\ntry to upgrade every mac-os\n\n2016-07-07 7:46 GMT+09:00 Tina Ryu notifications@github.com:\n\n> I also have the same problem. Did you, @newbornking999\n> https://github.com/newbornking999 ever figure it out?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2482#issuecomment-230929809,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ASW4V8kYFYEISBc40MAh1wMtDijjzzA0ks5qTDA9gaJpZM4Ik-o3\n> .\n"]}, {"number": 2481, "title": "Upstream internal changes for 5/23/2016", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2480, "title": "Building TensorFlow with CMake", "body": "I realize that building TensorFlow from CMake is not yet ready for prime time, but I am attempting to push through some of the challenges and issues getting that working on a Mac.\n### Environment info\n\nOperating System: Mac, OS X 10.11.5\nUsing CMake to create a make file (not interested in trying this with XCode yet)\nNot currently using CUDA\n\nInstalled from sources, commit hash: 787b97e6\n### Steps to reproduce\n\nThere are some issues with paths to eigen, json and re2 that I have been able to work around but I have run into a roadblock.\n\nSeveral .cc files in tensorflow/core/framework/ directory attempt to include files that do not exist. The files to be included all end with \".pb_text.h\". I assume that these files are (or should be) generated by the protocol buffer compiler. \n\nFor example, from tensorflow/core/framework/attr_value_util.cc:\n\n```\n#include \"tensorflow/core/framework/attr_value.pb_text.h\"\n#include \"tensorflow/core/framework/tensor.pb_text.h\"\n```\n\nAs a result I get \"file not found\" errors when building.\n### What have you tried?\n\nThere are some files in tensorflow/contrib/cmake/build/tensorflow/core/framework with similar names (like attr_value.pb.h and tensor.b.h) which I assume are generated by the protocol buffer compiler. I modified the .cc files which were causing trouble to include these files:\n\n```\n#include \"tensorflow/core/framework/attr_value.pb.h\"\n#include \"tensorflow/core/framework/tensor.pb.h\"\n```\n\nThat results in errors related to using undeclared identifiers: \n\n.../tensorflow/tensorflow/core/framework/attr_value_util.cc:44:28: error: use of undeclared\n      identifier 'ProtoShortDebugString'\n                           ProtoShortDebugString(tensor_proto), \">\");\n\n.../tensorflow/tensorflow/core/framework/attr_value_util.cc:62:14: error: use of undeclared\n      identifier 'EnumName_DataType'\n      return EnumName_DataType(attr_value.type());\n\n(and so on)\n", "comments": ["*.pb_text.h is generated by tools/tools/proto_text, *.pb.h is generated by protoc, they are not the same, so replacing one to the other won't work.\n", "@cg31 thanks for clarifying which tools generate which files. That was not clear to me. I figured that my attempt at replacing the files would not work, but tried anyway.\n\nI looked around and found that files I need are actually generated when I build TF from source. They are buried deep in the bazel-out directory. So as a complete hack I copied the files I need from there. Definitely not a long-term solution, but it got me past this issue for now.\n", "It may be helpful to look at the work I've been doing to get TensorFlow's C++ core building with a plain makefile:\nhttps://github.com/tensorflow/tensorflow/pull/2440\nI build the proto_text tool and use that to generate the needed files.\n", "Thanks @petewarden. I will give that a try.\n", "@petewarden @timbrucks What's the status of this?\n", "We don't plan on supporting cmake ourselves, we're putting our effort behind the makefile approach for the core runtime instead. Closing this one, since we're not going to support this method.\n"]}, {"number": 2479, "title": "Go bindings", "body": "", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "I merged this without CLA. We checked with all authors, and all are both ok with merging their commits and have CLAs in place.\n", "This was merged into the `go` branch. I'm guessing that was intentional. Is there another PR to merge the `go` branch into `master`?\n", "Hi @jmhodges , yes, this PR comes from this other one: https://github.com/tensorflow/tensorflow/pull/1771\nI think that is because the code still has to be integrated on Jenkins and reviewed a bit more. By the moment, I think that there is no other PR to `master`.\n", "There is no PR to merge it into master right now. More infrastructure work\nhas to happen before we can merge it to master (from where it would\nautomatically be sucked into a lot of systems that are not ready for it).\n\nOn Tue, May 24, 2016 at 1:42 PM Alonso Vidales notifications@github.com\nwrote:\n\n> Hi @jmhodges https://github.com/jmhodges , yes, this PR comes from this\n> other one: #1771 https://github.com/tensorflow/tensorflow/pull/1771\n> I think that is because the code still has to be integrated on Jenkins and\n> reviewed a bit more. By the moment, I think that there is no other PR to\n> master.\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2479#issuecomment-221384030\n", "@martinwicke it looks like this got merged with 1429abd -- did that lose commit history? I was looking for how much of this still was made up of my initial contribution but it looks like that history may have been lost.\n", "Yes. I am sorry, I did squash those commits. I should not have. You can\nstill look at #2479 for the history.\n\nOn Thu, Jun 9, 2016 at 4:18 PM Travis Cline notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke it looks like this got\n> merged with 1429abd\n> https://github.com/tensorflow/tensorflow/commit/1429abd4308b755d5c3a97f2788c55b5cd0f8230\n> -- did that lose commit history? I was looking for how much of this still\n> was made up of my initial contribution but it looks like that history may\n> have been lost.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2479#issuecomment-225054846,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_cp8s1DQd2F1cDTFGDBuyLYLRrvbks5qKJ9VgaJpZM4IkyNX\n> .\n"]}, {"number": 2478, "title": "Fix typo in the instruction", "body": "paramter => parameter\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 2477, "title": "python 3.5 compatibility", "body": "1. python 3.5 compatibility\n2. Correct the link to \"Laplacian pyramid\".\n", "comments": ["adding comment to see if it triggers the CLA bot\n", "@tensorflow-jenkins test this please\n\n@zhongzyd is a prior committer and has signed the CLA, so this is fine.\n"]}, {"number": 2476, "title": "Feature request: RMSPropOptimizer support for sparse gradients", "body": "I use RMSPropOptimizer to optimize parameters, I get NotImplementedError. **But when I change to use AdamOptimizer, it works fine.**\n\nSo I try to fix the problem and I find some key point, may be it can help.\nHere is my code:\n\n```\nself.x = tf.placeholder(tf.int32, [None, sequence_length])  \npoint = tf.get_variable([len(embedding_matrix)])\n............\n............\noutputs, states = rnn.rnn(lstm_cell, x, initial_state=initial_state, sequence_length=real_length)\nindex     =    self.x[:, 0]\nindex  = tf.reshape(index, [-1,1])\nindex_point  =    tf.gather(pointt, index)\noutput   =  tf.mul(outputs[-1] ,   index_point)\n\nscores = tf.nn.xw_plus_b(output, self.W, b)\nlosses = tf.nn.softmax_cross_entropy_with_logits(scores, self.input_y)\nself.loss = tf.reduce_mean(losses) \noptimizer = tf.train.RMSPropOptimizer(1e-3, decay = 0.9)\ngrads_and_vars = optimizer.compute_gradients(self.loss)\nself.train_op = optimizer.apply_gradients(grads_and_vars)\n```\n\nWhen I try to change the line `output   =  tf.mul(outputs[-1] ,   index_point)` to some others such as `output   =  tf.mul(outputs[-1] ,   2)`, **the error disappear**. And I try to change to use `bidirectional_rnn`, **the error also disappear.**\n", "comments": ["This is known: there is no implementation of RMSPropOptimizer for sparse gradients.  I'll retitle to a feature request.\n", "I'll work on it\n"]}, {"number": 2475, "title": "NotImplementedError when I use RMSPropOptimizer", "body": "I am try to use RMSPropOptimizer to optimize parameters and I get NotImplementedError. **But when I try AdamOptimizer, it works fine**. \n\n**BTW, when I try to find the problem, it seems I find the key point:**\nHere is part of my code:\n\n```\nself.x = tf.placeholder(tf.int32, [None, sequence_length])  \npoint = tf.get_variable( [vocabulary_size])\n........\n........\noutputs, states = rnn.rnn(lstm_cell, x_vec, initial_state=initial_state, sequence_length = self.real_length)\nindex     =    self.x[:, 0]\nindex  = tf.reshape(index, [-1,1])\nindex_point  =    tf.gather(point, index)\noutput   =  tf.mul(outputs[-1] ,   index_point)\n```\n\nThe key point is last line `output   =  tf.mul(outputs[-1] ,   index_point)`, when I change `index_point` to some others such as number `1`, **the error disappear**.\nAnd, when I try to use `bidirectional_rnn`, **the error also can disappear**.\n", "comments": ["Closing as duplicate of #2476.\n"]}, {"number": 2474, "title": "Issue with deep dream example", "body": "I get this error when creating the session in the Deep Dream notebook example without any modifications. Does anybody know what does exactly mean?\nInput: \n\n```\nsess = tf.InteractiveSession(graph=graph)\n```\n\nOutput\n\n```\nException AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x1128b5990>> ignored\nException AssertionError: AssertionError() in <generator object get_controller at 0x1128a5af0> ignored\n```\n\nThanks,\nManu\n", "comments": ["Thanks for raising this! We should have a fix coming out soon.\n", "Any idea what hte core issue was?  I'm getting the same with RNN.\n"]}]