[{"number": 15128, "title": "GPU memory increases in multiples of batch_size 64 with allow_growth=True", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.4.0-14-gb5df90f', '1.4.1')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0-6\r\n- **CUDA/cuDNN version**: V9.0.176\r\n- **GPU model and memory**: V100 AWS P3 8X large instance\r\n- ** Exact command to reproduce**: Inception V3 training as mentioned in `models/research/slim`. `python train_image_classifier.py`\r\n\r\n\r\n### Describe the problem\r\nGPU memory is increasing in multiples of batch_size of 64. Using 8.9 GB of GPU memory for `batch_size=16` or `batch_size=32` or `batch_size=64`. And using 15.6 GB of GPU memory for `batch_size=96` and `batch_size=128`. I'd like to use `batch_size=96` so allocator won't throw warnings during global_step as it will have 2-3 GB unused.\r\n\r\nIs this a feature? Can we change its functioning?", "comments": ["on the top of my head, I don't know that this is configurable.\r\n\r\n@vrv is there a knob somewhere?", "I don't think there is a way to configure this; https://github.com/tensorflow/tensorflow/commit/9fe7c29605a5ee1519cceba8e67a6d8413444fac changed the chunk splitting policy to be less wasteful though.\r\n\r\nIf you're running a release before this commit, you might want to try updating.  If you're already past this change, I don't know if there is a knob to select here.", "Thanks for clarifying. I'm running r1.4 and the above-mentioned commit https://github.com/tensorflow/tensorflow/commit/9fe7c29605a5ee1519cceba8e67a6d8413444fac is not part of r1.4. I'll give it a try.\r\n\r\nI'm closing this as there is no way to configure this. But can I know the reason why it's like that? Or can I open another issue as a feature request?"]}, {"number": 15127, "title": "OS X ld does not have icf", "body": "when using tflite_linkopts_unstripped() or tflite_linkopts() to build stuff, such as https://github.com/tensorflow/tensorflow/pull/15095, on OS X, identical code folding flag is not supported.", "comments": ["Can one of the admins verify this patch?", "@aselle any luck with this?", "ping @aselle ", "Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 170 days with no activity and the `awaiting review` label has been applied.", "the same one is in a64995e97e1. I'll close this."]}, {"number": 15126, "title": "Is their any java spark api available for using Tensor", "body": "", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "version is  not matter, i can configure as corresponding,\r\nand plz give some deeplearnig example too with spark and tensorflow.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15125, "title": "Improve the Windows Bazel build", "body": "In this PR, we are making following improvements:\r\n\r\n1. Remove unnecessary environment variables.\r\n    `BAZEL_VS`: Bazel can detect Visual C++ installation\r\n    `BAZEL_PYTHON`: Not needed anymore since we don't use wrapper scripts.\r\n    `NO_MSVC_WRAPPER`: Already using wrapper-free CROSSTOOL by default.\r\n    `USE_DYNAMIC_CRT`: Bazel already links to dynamic msvcrt by default.\r\n\r\n2. Remove the `$BUILD_OPT` variable in `bazel_test_lib.sh`. Add these options in `configure.py`, this way our users don't have to remember them every time.\r\n\r\n3. Remove `bazel clean --expunge`. With Bazel 0.8.0, I'm quite confident to remove this so that we can benefit from cache.\r\n\r\n4. When `--define=no_tensorflow_py_deps=true`, we let `py_test` depend on a marker file for all dependencies of the pip package. By doing this, even without `bazel clean`, `py_test` also gets to rerun as long as the pip package changes.\r\n\r\n5. Filter out `external/org_tensorflow` directory, when creating the pip package.", "comments": []}, {"number": 15124, "title": "XLA Reshape_test failing for 3rd party platforms", "body": "I have opened a discussion on the XLA dev mailing list, but I think that this counts as an issue since if it isn't fixed then potentially all XLA tests will migrate to being unusable for 3rd party devices.\r\n\r\nhttps://groups.google.com/forum/#!topic/xla-dev/9cY21Hi0s_s\r\n\r\nThe issue is that the code in reshape_test.cc assumes that any devices which are not CPU/CPU_PARALLEL/GPU will use the bfloat16 format.  This isn't true for the Graphcore device.\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi,\r\n\r\nNo problem.\r\n\r\nOS Platform and Distribution:\r\n- all platforms and distrubutions\r\n\r\nTensorflow installed from:\r\n- source\r\n\r\nTensorflow version:\r\n- master branch of repo\r\n\r\nBazel version:\r\n- unknown / not relevent\r\n\r\nCUDA/cuDNN version:\r\n- none\r\n\r\nGPU model and memory:\r\n- none\r\n\r\nExact command to reproduce:\r\n- bazel test --test_env TF_CPP_MIN_VLOG_LEVEL=1  --verbose_failures --test_output=all --nocache_test_results --config=opt tensorflow/compiler/xla/tests:reshape_test_poplar\r\n\r\nthis only works if you have the Graphcore extensions which you will not.  see issue on the XLA dev board.\r\n ", "Hi David, thanks for reporting this issue. We have committed a workaround to disable bfloat16 tests for unknown backends by default:\r\nhttps://github.com/tensorflow/tensorflow/commit/557e0ce1ca949abb0ffdcccc6ab0480e65ea9fe1", "Thakns @ukoxyz . @DavidNorman I think that should solve your problem. Feel free to reopen if it doesn't."]}, {"number": 15123, "title": "Fix typo", "body": "*fix typo", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 15122, "title": "tensorflow lite: error when convert frozen model to lite format", "body": "Hi,\r\nI build the freeze .pb with the guide at https://github.com/tensorflow/models/tree/master/research/slim \r\nwith the below step:\r\n \r\npython train_image_classifier.py \\\r\n--train_dir /home/ubuntu/train/ \\\r\n--dataset_dir /home/ubuntu/vegetables/ \\\r\n--dataset_name=vegetables \\\r\n--dataset_split_name=train  \\\r\n--num_clones=2 \\\r\n--clone_on_cpu=True \\\r\n--checkpoint_path=/home/ubuntu/check_point/mobilenet_v1_1.0_224.ckpt \\\r\n--max_number_of_steps=10 \\\r\n--checkpoint_exclude_scopes=MobilenetV1/Logits,MobilenetV1/AuxLogits \\\r\n--model_name=mobilenet_v1\r\n\r\n\r\npython tensorflow/python/tools/freeze_graph.py \\\r\n    --input_graph=/home/ubuntu/train/mobilenet_v1_224.pb \\\r\n    --input_checkpoint=/home/ubuntu/check_point/mobilenet_v1_1.0_224.ckpt \\\r\n    --input_binary=true \\\r\n    --output_graph=/home/ubuntu/train/frozen_mobilenet_v1_224.pb \\\r\n    --output_node_names=MobilenetV1/Predictions/Reshape_1\r\n\r\nNOTE: I download mobilenet_v1_1.0_224.ckpt from http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz\r\n\r\nBut when I convert to lite mode with \r\n\r\nubuntu@ip-172-31-27-248:~/tensorflow$ bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n   --input_format=TENSORFLOW_GRAPHDEF \\\r\n   --input_file=/home/ubuntu/mobilenet_v1_1.0_224/frozen_mobilenet_v1_224.pb \\\r\n   --output_format=TFLITE \\\r\n   --output_file=/tmp/mobilenet_v1_1.0_224.lite --inference_type=FLOAT \\\r\n   --inference_input_type=FLOAT \\\r\n   --input_arrays=input \\\r\n   --output_arrays=MobilenetV1/Predictions/Reshape_1 --input_shapes=1,224,224,3\r\n \r\n2017-12-05 09:53:56.604720: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 336 operators, 502 arrays (0 quantized)\r\n2017-12-05 09:53:56.627922: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)\r\n2017-12-05 09:53:56.628156: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)\r\n2017-12-05 09:53:56.628327: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 6422528 bytes, theoretical optimal value: 4816896 bytes.\r\n2017-12-05 09:53:56.628487: I tensorflow/contrib/lite/toco/toco_tooling.cc:268] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).\r\n2017-12-05 09:53:56.628653: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze\r\nAborted (core dumped)\r\n\r\nPls help me! \r\n\r\nmy other question is when I download the pretrain freeze pb from https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_float_2017_11_08.zip \r\nIt's work when I use toco tools. where I can find guide which generate these freeze pb.\r\n\r\n\r\nPls Help! \r\n\r\n\r\n \r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["I am not able to retrain and freeze using train_image_classifier.py.\r\n\r\nHave you tried something like the following?\r\n\r\n```\r\npython tensorflow/examples/image_retraining/retrain.py \\\r\n  --image_dir /home/ubuntu/flower_photos \\\r\n  --architecture mobilenet_1.0_224\r\n```\r\n\r\n```\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --input_file=/tmp/output_graph.pb \\\r\n  --output_format=TFLITE \\\r\n  --output_file=/tmp/mobilenet_v1_1.0_224.lite \\\r\n  --inference_type=FLOAT \\\r\n  --inference_input_type=FLOAT \\\r\n  --input_arrays=input \\\r\n  --output_arrays=final_result \\\r\n  --input_shapes=1,224,224,3\r\n```", "@andrehentz tks for you help!\r\nI try below:\r\npython tensorflow/examples/image_retraining/retrain.py \\\r\n    --image_dir /home/ubuntu/vegetables_photos/ \\\r\n    --output_graph /home/ubuntu/vegetables_out/vegetables.pb \\\r\n    --output_label /home/ubuntu/vegetables_out/labels.txt \\\r\n    --how_many_training_steps=60 \\\r\n    --architecture mobilenet_1.0_224\r\n\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --input_file=/home/ubuntu/vegetables_out/vegetables.pb \\\r\n  --output_format=TFLITE \\\r\n  --output_file=/home/ubuntu/vegetables_out/mobilenet_quant_v1_224.tflite \\\r\n  --inference_type=FLOAT \\\r\n  --inference_input_type=FLOAT \\\r\n  --input_arrays=input \\\r\n  --output_arrays=final_result \\\r\n  --input_shapes=1,224,224,3\r\nandroid I put the mobilenet_quant_v1_224.tflite into the TfLiteCameraDemo.apk assert dir,android run TfLiteCameraDemo.apk, but the apk throws error:\r\nCaused by: java.lang.IllegalArgumentException: Failed to get input dimensions. 0-th input should have 602112 bytes, but found 150528 bytes.\r\n\r\n\r\n\r\nhowever I find then TfLiteCameraDemo.apk need quant mode,so I try below:\r\npython tensorflow/examples/image_retraining/retrain.py \\\r\n    --image_dir /home/ubuntu/vegetables_photos/ \\\r\n    --output_graph /home/ubuntu/vegetables_out/vegetables.pb \\\r\n    --output_label /home/ubuntu/vegetables_out/labels.txt \\\r\n    --how_many_training_steps=60 \\\r\n    --architecture mobilenet_1.0_224_quantized\r\n\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --input_file=/home/ubuntu/vegetables_out/vegetables.pb \\\r\n  --output_format=TFLITE \\\r\n  --output_file=/home/ubuntu/vegetables_out/mobilenet_quant_v1_224.tflite \\\r\n  --input_shapes=1,224,224,3 \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_arrays=Placeholder \\\r\n  --output_arrays=final_result \\\r\n  --mean_values=128 \\\r\n  --std_values=127\r\nthen I got the error:\r\n017-12-06 09:21:10.165673: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1046] Converting unsupported operation: AssignSub\r\n2017-12-06 09:21:10.165698: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1046] Converting unsupported operation: AssignAdd\r\n2017-12-06 09:21:10.165729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1046] Converting unsupported operation: Pow\r\n2017-12-06 09:21:10.165757: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1046] Converting unsupported operation: AssignSub\r\n2017-12-06 09:21:10.165801: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1046] Converting unsupported operation: AssignSub\r\n2017-12-06 09:21:10.165823: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1046] Converting unsupported operation: AssignAdd\r\n2017-12-06 09:21:10.165853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1046] Converting unsupported operation: Pow\r\n2017-12-06 09:21:10.165890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1046] Converting unsupported operation: AssignSub\r\n2017-12-06 09:21:10.219204: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1433 operators, 2236 arrays (0 quantized)\r\n2017-12-06 09:21:10.623803: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 401 operators, 962 arrays (1 quantized)\r\n2017-12-06 09:21:10.630679: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 94 operators, 184 arrays (1 quantized)\r\n2017-12-06 09:21:10.631553: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 94 operators, 184 arrays (1 quantized)\r\n2017-12-06 09:21:10.632222: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 94 operators, 184 arrays (1 quantized)\r\n2017-12-06 09:21:10.632392: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 66 operators, 156 arrays (1 quantized)\r\n2017-12-06 09:21:10.632699: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 66 operators, 156 arrays (1 quantized)\r\n2017-12-06 09:21:10.671081: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:354] Unimplemented: this graph contains an operator of type (Unsupported TensorFlow op: Pow) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nAborted (core dumped)\r\n\r\nMy question is : \r\n How I can retrain model which like mobilenet_quant_v1_224.tflite in the TfLiteCameraDemo.apk\r\npls help! \r\n\r\n", "Retraining  mobilenet_1.0_224_quantized is something we are still working on and we hope to update our docs and code soon.\r\n\r\nMeanwhile, I have to suboptimal workarounds for you:\r\n  1) Change the demo app to work with floating-point numbers (it requires changes [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L75) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L157) at least)\r\n\r\n  2) Use \"approximate\" quantization when converting with toco:\r\n```\r\npython tensorflow/examples/image_retraining/retrain.py \\\r\n  --image_dir /home/ubuntu/flower_photos \\\r\n  --architecture mobilenet_1.0_224\r\n\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --input_file=/tmp/output_graph.pb \\\r\n  --output_format=TFLITE \\\r\n  --output_file=/tmp/mobilenet_v1_1.0_224.lite \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --input_arrays=input \\\r\n  --output_arrays=final_result \\\r\n  --input_shapes=1,224,224,3\\\r\n  --mean_values=128 \\\r\n  --std_values=128 \\\r\n  --default_ranges_min=0 \\\r\n  --default_ranges_max=6\r\n```\r\n", "/CC @aselle ", "@andrehentz  It's work for me,very thanks for your help.", "@andrehentz @aselle \r\nI face one problem like below,\r\nI retain my modes ,and test in pc:\r\nubuntu@ip-172-31-27-248:~/tensorflow$ python tensorflow/examples/label_image/label_image.py \\\r\n>   --graph=/home/ubuntu/vegetables_train/vegetables_out/vegetables.pb \\\r\n>   --labels=/home/ubuntu/vegetables_train/vegetables_out/labels.txt \\\r\n>   --image=/home/ubuntu/test_photos/3.jpg \\\r\n>   --input_layer=input \\\r\n>   --output_layer=final_result \\\r\n>   --input_mean=128 \\\r\n>   --input_std=128 \\\r\n>   --input_width=224 \\\r\n>   --input_height=224\r\nxi hong shi 0.99971\r\nxi qin 0.000140324\r\nhu luo bo 6.89621e-05\r\nshang hai qing 3.10927e-05\r\njian jiao 2.14989e-05\r\nthe reusult is very nice!\r\n\r\n\r\nbut after I quant the model with:\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --input_file=/home/ubuntu/vegetables_train/vegetables_out/vegetables.pb \\\r\n  --output_format=TFLITE \\\r\n  --output_file=/home/ubuntu/vegetables_train/vegetables_out/mobilenet_quant_v1_224.tflite \\\r\n  --input_shapes=1,224,224,3 \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --input_arrays=input \\\r\n  --output_arrays=final_result \\\r\n  --default_ranges_min=0 \\\r\n  --default_ranges_max=6 \\\r\n  --mean_values=128 \\\r\n  --std_values=128\r\nput the mobilenet_quant_v1_224.tflite into TfLiteCameraDemo.apk , run TfLiteCameraDemo.apk,it's very very diffcult to  recognition the vegetables,\r\nI think this because quant to UINT8.\r\nAnd now I will try FLOAT model ,My question is:How to modify TfLiteCameraDemo app code to support FLOAT lite mode?\r\npls help!", "Doing quantized requires specialized training. Your input of default ranges sets invalid ranges that are useful for testing the possible speedup due to quantization but will not result in correct classification. We plan to release a quantized training script to show how you might do this with your model. \r\n\r\nAs far as changing TfLiteCameraDemo to support floats:\r\nedit this\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java\r\n\r\nYou can change the convertBitmapToByteBuffer to make a float array instead of a byte array.\r\nThat's about it I think.\r\n", "@aselle any luck with the sample  quantized training script ? Thanks ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Hi, I covert the frozen graph:mobilenet_v1_224 to tflite, and put it in the tflitecamerademo app, but the regonization result is not correct. If I use the tflite file which download from https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip\r\n\r\nThe regonization result is correct, I don't know what steps is not correct when I covert the frozeon graph to tflite file, could you help me to review what steps is the wrong?\r\n\r\nI put the frozen graph, coverting tflite file and the regonized picture in the https://drive.google.com/drive/folders/12h9O2AtcnDuQZXogAdmexRSjxIQVc1Ej?usp=sharing\r\n\r\nThe correct result should be \"malamute\", but I use the my coverting tflite file, the result  is \"shower curtain\"\r\n\r\nI use the command to do the covert\r\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco -- '--input_file=/tmp/mobilenet_frozen_graph.pb' '--output_file=/tmp/mobilenet_quant_20180117.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--inference_input_type=QUANTIZED_UINT8' '--input_shapes=1,224,224,3' '--input_arrays=input' '--output_arrays=MobilenetV1/Predictions/Reshape_1' '--mean_values=128' '--std_values=128' '--default_ranges_min=0' '--default_ranges_max=6'\r\n\r\nThanks\r\n", "@andrehentz have you solve the issue for mobilenet_1.0_224_quantized retrain + toco .tflite ?", "@aselle @andrehentz do you have new ImageClassifier.java file which can support float? ", "Here is the example of use float for inference: [TensorFlow for Poets 2 - TF Lite Demo App](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/android/tflite/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java)", "While full quantized training is still in the works, retraining an existing quantized mobilenet (currently only  mobilenet_1.0_224_quantized) should work. You can follow the step in [TensorFlow for Poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/) which I reproduce (slightly modified) here:\r\n\r\n```\r\nARCHITECTURE=mobilenet_1.0_224_quantized                                                           \r\nDATA_DIR=~/tensorflow-for-poets-2/tf_files                                                         \r\nTRAINING_DIR=/tmp/tf_files                                                                         \r\n                                                                                                   \r\npython tensorflow/tensorflow/examples/image_retraining/retrain.py \\                                \r\n  --bottleneck_dir=$TRAINING_DIR/bottlenecks \\                                                     \r\n  --how_many_training_steps=500 \\                                                                  \r\n  --model_dir=$TRAINING_DIR/models \\                                                               \r\n  --summaries_dir=$TRAINING_DIR/training_summaries/\"${ARCHITECTURE}\" \\                             \r\n  --output_graph=$TRAINING_DIR/retrained_graph.pb \\                                                \r\n  --output_labels=$TRAINING_DIR/retrained_labels.txt \\                                             \r\n  --architecture=\"${ARCHITECTURE}\" \\                                                               \r\n  --image_dir=$DATA_DIR/flower_photos  \r\n\r\nrm -f /$TRAINING_DIR/${ARCHITECTURE}.tflite                              \r\n                        \r\ntensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \\                                           \r\n  --input_file=$TRAINING_DIR/retrained_graph.pb \\                                                  \r\n  --input_format=TENSORFLOW_GRAPHDEF \\                                                             \r\n  --output_format=TFLITE \\                                                                         \r\n  --output_file=/$TRAINING_DIR/${ARCHITECTURE}.tflite \\                                            \r\n  --inference_type=QUANTIZED_UINT8 \\                                                               \r\n  --input_array=Placeholder \\                                                                      \r\n  --output_array=final_result \\                                                                    \r\n  --input_shape=1,224,224,3 \\                                                                      \r\n  --mean_value=128 \\                                                                               \r\n  --std_value=128                                                                                  \r\n```\r\n\r\nThat should create two files:\r\n```\r\n/tmp/tf_files/mobilenet_1.0_224_quantized.tflite\r\n/tmp/tf_files/retrained_labels.txt\r\n```\r\nwhich you can use in the original demo app.\r\n\r\nFor floating-point inference the link given by @ecrasy should work.\r\n", "@andrehentz I'm trying to use your method but I'm not able to create the quantized.tflite. I'm using a docker with tf1.5 and in your second step with toco I received:\r\n`toco --input_file=tf_files/retrained_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=tf_files/model.tflite --inference_type=QUANTIZED_UINT8 --input_array=Placeholder --output_array=final_result --input_shape=1,224,224,3 --mean_value=128 --std_value=128`\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-02-12 14:30:20.126646: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Dequantize\r\n2018-02-12 14:30:20.126818: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Dequantize\r\n2018-02-12 14:30:20.127122: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Dequantize\r\n2018-02-12 14:30:20.127511: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Dequantize\r\n2018-02-12 14:30:20.127633: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Dequantize\r\n2018-02-12 14:30:20.128115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Dequantize\r\n2018-02-12 14:30:20.128356: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Dequantize\r\n2018-02-12 14:30:20.128786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Dequantize\r\n2018-02-12 14:30:20.129454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Dequantize\r\n2018-02-12 14:30:20.129759: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Dequantize\r\n2018-02-12 14:30:20.132161: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 145 operators, 239 arrays (0 quantized)\r\n2018-02-12 14:30:20.134624: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 62 operators, 152 arrays (0 quantized)\r\n2018-02-12 14:30:20.136101: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 62 operators, 153 arrays (0 quantized)\r\n2018-02-12 14:30:20.137529: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 61 operators, 151 arrays (0 quantized)\r\n2018-02-12 14:30:20.138834: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 61 operators, 151 arrays (0 quantized)\r\n2018-02-12 14:30:20.139734: F tensorflow/contrib/lite/toco/tooling_util.cc:1204] Array input, which is an input to the Conv operator producing the output array MobilenetV1/MobilenetV1/Conv2d_0/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\nAborted \r\n\r\nAm I doing something wrong?", "@igsoft you might want to open a separate issue for that. Could you show the command lines you are using?", "@zhang13850588961 \r\nyour models are trained in Mobilenet with Float. It's not quantized\r\n\r\nyou can use the following link to make it work on the android demo app\r\n\r\nhttps://stackoverflow.com/questions/47761514/tensorflow-lite-pretrained-model-does-not-work-in-android-demo/49271024#49271024", "I have trained a custom model using DNN and tried to use TF_Lite for using that in Android. I'm using the same method in the demo app for loading the asset file. But I'm getting following error.\r\n\r\n\r\n\r\n\r\n`E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                                     Process: revan.locationspeed, PID: 12072\r\n                                                                     java.lang.RuntimeException: Unable to start activity ComponentInfo{revan.locationspeed/thambu.locationspeed.MainActivity}: java.lang.IllegalArgumentException: Contents of /dnn_frozen_graph.tflite does not encode a valid TensorFlowLite model: Could not open '/dnn_frozen_graph.tflite'.The model is not a valid Flatbuffer file.\r\nCaused by: java.lang.IllegalArgumentException: Contents of /dnn_frozen_graph.tflite does not encode a valid TensorFlowLite model: Could not open '/dnn_frozen_graph.tflite'.The model is not a valid Flatbuffer file.` \r\n\r\nI used toco for conversion... Expecting guidance for succeeding this", "@Revan007 not sure if you already did this but, for tflite files, you should tell gradle not to compress it. Add this:\r\n```\r\naaptOptions {\r\n        noCompress \"tflite\"\r\n        noCompress \"lite\"\r\n}\r\n```\r\nYou can find this in the gradle files of the TF demo Android apps as well.", "@andrehentz @\r\nhow can I run iOS camera demo with float .lite file\uff1f", "Hi @CoCa520 , here is how I got the iOS camera app to work for the tutorial: https://github.com/nheidloff/watson-deep-learning-tensorflow-lite\r\nhttps://github.com/nheidloff/watson-deep-learning-tensorflow-lite/blob/master/ios-camera/CameraExampleViewController.mm#L256-L272", "@andrehentz **Sadly the approach you provided (as quoted in the end of this post) in the former comment does not work for me here...**\r\n\r\nI retrained my mobilenet_v1_1.0_224_quantized model, and then running with the following command, and it still reports the same error message as posted in the original question. It IS driving me crazy. Do you have any other ideas about this?\r\n\r\nHere's my toco command, where retrained_graph.pb is my retrained model **without freeze** (also, a frozen model after freeze_graph does not work as well, which by the way, really confuses me that **is freeze_graph necessary for converting to tflite?**):\r\n\r\n`toco \\\r\n  --graph_def_file=output_dir/retrained_graph.pb \\\r\n  --output_file=output_dir/optimized_graph.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --input_array=input \\\r\n  --input_shape=1,224,224,3 \\\r\n  --output_array=final_result \\\r\n  --mean_value=128 \\\r\n  --std_dev_values=128 \\\r\n  --default_ranges_min=0 \\\r\n  --default_ranges_max=6 \\\r\n  --logtostderr '--v=2'`\r\n\r\n> While full quantized training is still in the works, retraining an existing quantized mobilenet (currently only mobilenet_1.0_224_quantized) should work. You can follow the step in TensorFlow for Poets which I reproduce (slightly modified) here:\r\n> \r\n> ARCHITECTURE=mobilenet_1.0_224_quantized                                                           \r\n> DATA_DIR=~/tensorflow-for-poets-2/tf_files                                                         \r\n> TRAINING_DIR=/tmp/tf_files                                                                         \r\n>                                                                                                    \r\n> python tensorflow/tensorflow/examples/image_retraining/retrain.py \\                                \r\n>   --bottleneck_dir=$TRAINING_DIR/bottlenecks \\                                                     \r\n>   --how_many_training_steps=500 \\                                                                  \r\n>   --model_dir=$TRAINING_DIR/models \\                                                               \r\n>   --summaries_dir=$TRAINING_DIR/training_summaries/\"${ARCHITECTURE}\" \\                             \r\n>   --output_graph=$TRAINING_DIR/retrained_graph.pb \\                                                \r\n>   --output_labels=$TRAINING_DIR/retrained_labels.txt \\                                             \r\n>   --architecture=\"${ARCHITECTURE}\" \\                                                               \r\n>   --image_dir=$DATA_DIR/flower_photos  \r\n> \r\n> rm -f /$TRAINING_DIR/${ARCHITECTURE}.tflite                              \r\n>                         \r\n> tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \\                                           \r\n>   --input_file=$TRAINING_DIR/retrained_graph.pb \\                                                  \r\n>   --input_format=TENSORFLOW_GRAPHDEF \\                                                             \r\n>   --output_format=TFLITE \\                                                                         \r\n>   --output_file=/$TRAINING_DIR/${ARCHITECTURE}.tflite \\                                            \r\n>   --inference_type=QUANTIZED_UINT8 \\                                                               \r\n>   --input_array=Placeholder \\                                                                      \r\n>   --output_array=final_result \\                                                                    \r\n>   --input_shape=1,224,224,3 \\                                                                      \r\n>   --mean_value=128 \\                                                                               \r\n>   --std_value=128                                                                                  \r\n> \r\n> That should create two files:\r\n> \r\n> /tmp/tf_files/mobilenet_1.0_224_quantized.tflite\r\n> /tmp/tf_files/retrained_labels.txt\r\n> \r\n> which you can use in the original demo app.\r\n> \r\n> For floating-point inference the link given by @ecrasy should work.\r\n", "Update: \r\nAnd, I retrained the mobilenet_1.0_224 model, the un-quantized version, and using toco to convert it to a QUANTIZED_UINT8 tflite file, it works, and the .tflite file is much smaller in size, which is 4.3M.\r\nThe command I use is as follows:\r\n\r\n`IMAGE_SIZE=224`\r\n`OUTPUT_NODE_NAME='final_result'`\r\n`INPUT_TYPE=\"QUANTIZED_UINT8\"`\r\n`INPUT_FILE=\"retrained_graph.pb\"`\r\n`toco \\\r\n  --graph_def_file=$output_dir/$INPUT_FILE \\\r\n  --output_file=$output_dir/optimized_graph.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=$INPUT_TYPE \\\r\n  --inference_input_type=$INPUT_TYPE \\\r\n  --input_array=input \\\r\n  --input_shape=1,\"${IMAGE_SIZE}\",\"${IMAGE_SIZE}\",3 \\\r\n  --output_array=$OUTPUT_NODE_NAME \\\r\n  --mean_value=128 \\\r\n  --std_dev_values=128 \\\r\n  --default_ranges_min=0 \\\r\n  --default_ranges_max=6 \\\r\n  --logtostderr '--v=2'`", "For anyone wondering where the `--default_ranges_min=0` and `--default_ranges_max=6` values come from, see [this note](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#use-dummy-quantization-to-try-out-quantized-inference-on-a-float-graph-) about dummy quantization, from the TOCO Command Line Examples documentation.\r\n\r\nAs they describe there, I got pretty poor accuracy after quantizing a retrained float MobileNet (the retrained model from the TensorFlow for Poets codelab)", "converting unsupported operation decodejpeg"]}, {"number": 15121, "title": "improve py_func", "body": "See #14448\r\nImproved `py_func` to accept nested structures as input and as output like in `tf.data.Dataset.form_generator`\r\n\r\n - relable inp to args and Tout to output_types\r\n - add arguments kwargs and output_shapes\r\n - allow args/kwargs/output_types/output_shapes to be a nested structure\r\n\r\nOpen questions:\r\n - allow output_types to be callable? (Dynamic output_types inference from args/kwargs)\r\n - (new) argument names\r\n - backward compatibility for old names", "comments": ["Can one of the admins verify this patch?", "ping @alextp ", "I'd say contrib to start, so we can fix things if we find something wrong with the initial version.", "Where should the code be? I do not know how contrib is organized.", "contrib.framework seems to be the least-inappropriate existing package. Just move this definition to a file there and it should work I think", "ok, I moved now the code to `tensorflow/conrtib/framework/ops/session_ops.py`", "sorry that I forgot the import, I fixed that.\r\nThere are maybe some more such bugs because I have not figured out how to install tensorflow from binary with git in editable mode.", "Jenkins, test this please.", "Let's see if the tests catch those.\n\nOn Mon, Dec 11, 2017 at 4:14 PM, Christoph Boeddeker <\nnotifications@github.com> wrote:\n\n> sorry that I forgot the import, I fixed that.\n> There are maybe some more such bugs because I have not figured out how to\n> install tensorflow from binary with git in editable mode.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15121#issuecomment-350770858>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdQHql0ysoJKd93tNISiQwOd918Kks5s_VTcgaJpZM4Q2CO2>\n> .\n>\n\n\n\n-- \n - Alex\n", "@tensorflow-jenkins test this please", "@boeddeker do you mind fix the lint errors and test failures?", "@yifeif Where do I find the lint errors and test failures?\r\nSince I did not write a test, how could a test fail, that is related to this PR?\r\nI only found one lint error.", "Incidentally you should write a test, then.\n\nOn Thu, Dec 21, 2017 at 1:46 AM, Christoph Boeddeker <\nnotifications@github.com> wrote:\n\n> @yifeif <https://github.com/yifeif> Where do I find the lint errors and\n> test failures?\n> Since I did not write a test, how could a test fail, that is related to\n> this PR?\n> I only found one lint error.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15121#issuecomment-353305445>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxbxk3jbO5J7-pdn8ClBY_MIth8MWks5tCijxgaJpZM4Q2CO2>\n> .\n>\n\n\n\n-- \n - Alex\n", "This appears to make *a lot* of tests fail. Could you check?", "I am not familiar with the import machinery in tensorflow, can someone explain, why the import does not work?\r\n`from tensorflow.contrib.framework.python.ops.script_ops import *`\r\n\r\nI will start to write tests, when the source code is formally correct for tensorflow.", "The `@@py_func` callout needs to go into the docstring of `tensorflow/contrib/framework/__init__.py`, since it is there that remove_undocumented is called. \r\n\r\nOtherwise I think this *should* work.", "@martinwicke Right, contrib, good point.", "@boeddeker you're probably missing some dependency.\r\n\r\n```\r\nFAIL: //tensorflow/python/kernel_tests:rnn_test (shard 10 of 10) (see /tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/testlogs/tensorflow/python/kernel_tests/rnn_test/shard_10_of_10/test.log)\r\nINFO: From Testing //tensorflow/python/kernel_tests:rnn_test (shard 10 of 10):\r\n==================== Test output for //tensorflow/examples/image_retraining:retrain_test:\r\nTraceback (most recent call last):\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/examples/image_retraining/retrain_test.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain_test.py\", line 24, in <module>\r\n    from tensorflow.examples.image_retraining import retrain\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/examples/image_retraining/retrain_test.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 117, in <module>\r\n    from tensorflow.contrib.quantize.python import quant_ops\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/examples/image_retraining/retrain_test.runfiles/org_tensorflow/tensorflow/contrib/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib import bayesflow\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/examples/image_retraining/retrain_test.runfiles/org_tensorflow/tensorflow/contrib/bayesflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.contrib.bayesflow.python.ops import csiszar_divergence\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/examples/image_retraining/retrain_test.runfiles/org_tensorflow/tensorflow/contrib/bayesflow/python/ops/csiszar_divergence.py\", line 26, in <module>\r\n    from tensorflow.contrib.bayesflow.python.ops.csiszar_divergence_impl import *\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/examples/image_retraining/retrain_test.runfiles/org_tensorflow/tensorflow/contrib/bayesflow/python/ops/csiszar_divergence_impl.py\", line 43, in <module>\r\n    from tensorflow.contrib import framework as contrib_framework\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/examples/image_retraining/retrain_test.runfiles/org_tensorflow/tensorflow/contrib/framework/__init__.py\", line 93, in <module>\r\n    from tensorflow.contrib.framework.python.ops import *\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/examples/image_retraining/retrain_test.runfiles/org_tensorflow/tensorflow/contrib/framework/python/ops/__init__.py\", line 27, in <module>\r\n    from tensorflow.contrib.framework.python.ops.script_ops import *\r\nModuleNotFoundError: No module named 'tensorflow.contrib.framework.python.ops.script_ops'\r\n================================================================================```", "@tensorflow-jenkins test this please", "Do I have to add the new file to `tf_custom_op_py_library` inside the `BUILD` file? (I did this in my last commit). ", "Jenkins, test this please.", "Sorry, but I have no bazel and did not know that I have to add dependencies.", "@tensorflow-jenkins test this please"]}, {"number": 15120, "title": "[Feature Request] Please make Estimator.export_savedmodel support input types other than tf.Example", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.4.1\r\n- **Python version**: \r\n3.5.3\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have a model,  I want to export it into SavedModel format by using tf.estimator API, because I'm using this API for training. Unfortunately,tf.estimator.export.build_raw_serving_input_receiver_fn and tf.saved_model.signature_def_utils.classification_signature_def require the input features must be encoded in tf.Example format.\r\n\r\nIn order to use the \"export_savedmodel\" function for exporting, when writing a custom model_fn for Estimator, I must populate the export_outputs element of the tf.estimator.EstimatorSpec return value.  Each output value must be an ExportOutput object such as tf.estimator.export.ClassificationOutput, tf.estimator.export.RegressionOutput, or tf.estimator.export.PredictOutput. Those three output types only support tf.Example as the input. \r\n\r\n```python\r\n    if (classes is not None\r\n        and not (isinstance(classes, ops.Tensor)\r\n                 and dtypes.as_dtype(classes.dtype) == dtypes.string)):\r\n      raise ValueError('Classification classes must be a string Tensor; '\r\n                       'got {}'.format(classes))\r\n```\r\n\r\nBecause they will use signature_def_utils.classification_signature_def(or predict_signature_def,...) to build the signature.\r\n\r\nThough I may build a new output type by myself, it doesn't. \r\nAs the comment in tensorflow\\python\\estimator\\export\\export.py:build_all_signature_defs stated:\r\n\"the call to is_valid_signature here should not remove anything else.\"\r\nIf you really want to remove it, please show me a warning.\r\n\r\n### Source code / logs\r\n```python\r\n\"\"\"Convolutional Neural Network Estimator for MNIST, built with tf.layers.\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport os\r\nimport sys\r\n\r\nimport tensorflow as tf\r\n\r\nparser = argparse.ArgumentParser()\r\n\r\n# Basic model parameters.\r\nparser.add_argument('--batch_size', type=int, default=100,\r\n                    help='Number of images to process in a batch')\r\n\r\nparser.add_argument('--model_dir', type=str, default='/tmp/mnist_model',\r\n                    help='The directory where the model will be stored.')\r\n\r\n\r\nparser.add_argument('--data_format', type=str, default=None,\r\n    choices=['channels_first', 'channels_last'],\r\n    help='A flag to override the data format used in the model. channels_first '\r\n         'provides a performance boost on GPU but is not always compatible '\r\n         'with CPU. If left unspecified, the data format will be chosen '\r\n         'automatically based on whether TensorFlow was built for CPU or GPU.')\r\n\r\n_NUM_IMAGES = {\r\n    'train': 50000,\r\n    'validation': 10000,\r\n}\r\n\r\ndef mnist_model(inputs, mode, data_format):\r\n  \"\"\"Takes the MNIST inputs and mode and outputs a tensor of logits.\"\"\"\r\n  # Input Layer\r\n  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\r\n  # MNIST images are 28x28 pixels, and have one color channel\r\n  inputs = tf.reshape(inputs, [-1, 28, 28, 1])\r\n\r\n  if data_format is None:\r\n    data_format = ('channels_first' if tf.test.is_built_with_cuda() else\r\n                   'channels_last')\r\n\r\n  if data_format == 'channels_first':\r\n    inputs = tf.transpose(inputs, [0, 3, 1, 2])\r\n\r\n  conv1 = tf.layers.conv2d(inputs=inputs,\r\n      filters=32,\r\n      kernel_size=[5, 5],\r\n      padding='same',\r\n      activation=tf.nn.relu,\r\n      data_format=data_format)\r\n\r\n\r\n  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2,\r\n                                  data_format=data_format)\r\n\r\n\r\n  conv2 = tf.layers.conv2d(inputs=pool1,\r\n      filters=64,\r\n      kernel_size=[5, 5],\r\n      padding='same',\r\n      activation=tf.nn.relu,\r\n      data_format=data_format)\r\n\r\n  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2,\r\n                                  data_format=data_format)\r\n\r\n\r\n  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\r\n\r\n  # Dense Layer\r\n  # Densely connected layer with 1024 neurons\r\n  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\r\n  # Output Tensor Shape: [batch_size, 1024]\r\n  dense = tf.layers.dense(inputs=pool2_flat, units=1024,\r\n                          activation=tf.nn.relu)\r\n\r\n  # Add dropout operation; 0.6 probability that element will be kept\r\n  dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=(mode == tf.estimator.ModeKeys.TRAIN))\r\n\r\n  # Logits layer\r\n  # Input Tensor Shape: [batch_size, 1024]\r\n  # Output Tensor Shape: [batch_size, 10]\r\n  logits = tf.layers.dense(inputs=dropout, units=10)\r\n  return logits\r\n\r\n\r\ndef mnist_model_fn(features, labels, mode, params):\r\n  if isinstance(features,dict):\r\n    features = features['image_raw']\r\n  \"\"\"Model function for MNIST.\"\"\"\r\n  logits = mnist_model(features, mode, params['data_format'])\r\n\r\n  predictions = {\r\n      'classes': tf.argmax(input=logits, axis=1),\r\n      'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\r\n  }\r\n\r\n  if mode == tf.estimator.ModeKeys.PREDICT:\r\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions,\r\n                                      export_outputs={'class': tf.estimator.export.ClassificationOutput(classes=tf.as_string(predictions['classes']))})  \r\n\r\n  loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\r\n\r\n  # Configure the training op\r\n  if mode == tf.estimator.ModeKeys.TRAIN:\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\r\n    train_op = optimizer.minimize(loss, tf.train.get_or_create_global_step())\r\n  else:\r\n    train_op = None\r\n\r\n  accuracy = tf.metrics.accuracy(tf.argmax(labels, axis=1), predictions['classes'])\r\n  metrics = {'accuracy': accuracy}\r\n\r\n  # Create a tensor named train_accuracy for logging purposes\r\n  tf.identity(accuracy[1], name='train_accuracy')\r\n  tf.summary.scalar('train_accuracy', accuracy[1])\r\n\r\n  return tf.estimator.EstimatorSpec(mode=mode,\r\n      predictions=predictions,\r\n      loss=loss,\r\n      train_op=train_op,\r\n      eval_metric_ops=metrics)\r\n\r\ndef main(unused_argv):\r\n  # Create the Estimator\r\n  mnist_classifier = tf.estimator.Estimator(model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,\r\n      params={'data_format': FLAGS.data_format})\r\n  image = tf.placeholder(tf.float32,[None])\r\n  mnist_classifier.export_savedmodel(\"bb\", tf.estimator.export.build_raw_serving_input_receiver_fn({\"image_raw\":image}))  \r\n\r\nif __name__ == '__main__':\r\n  tf.logging.set_verbosity(tf.logging.INFO)\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```", "comments": ["Sorry, I found tf.estimator.export.PredictOutput satisfied my need. I was using the wrong output type."]}, {"number": 15119, "title": "a  small  problem   in  the  word2vec", "body": "In  the  web  url  \u201chttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\u201d\uff1b\r\n\r\nIn  the    code  line  123\uff0c\u201cbuffer[:] = data[:span]\u201d \uff1b\r\nwhen  I  run  the   wiki data  using  1000  examples\uff081000 rows\uff09\uff0c\r\nthe  program    says  \u201csequence index must be integer not slice\u201d\uff0c\r\nso  I  changed     \u201cbuffer[:] = data[:span]\u201d   to   \u201cbuffer.clear()\u201d and  \u201cbuffer.extend(data[:span])\u201d\uff0c\r\nand  it  works  well\u3002\r\n", "comments": []}, {"number": 15118, "title": "Building Tensorflow from source failed, compilation error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat Enterprise Linux Server release 6.9 (Santiago)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:  2.7.13\r\n- **Bazel version (if compiling from source)**:  0.6.1 \r\n- **GCC/Compiler version (if compiling from source)**: 4.4.7\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:  \r\nbazel build --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nThe following errors appears at the building stage when I tried to install Tensorflow for CPU from source.   \r\n\r\n### Source code / logs\r\n```\r\nWARNING: /home/localuser/tensorflow/tensorflow/core/BUILD:1813:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/localuser/tensorflow/tensorflow/tensorflow.bzl:1100:30.\r\nWARNING: /home/localuser/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/localuser/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Found 1 target...\r\nERROR: /home/localuser/tensorflow/tensorflow/core/grappler/costs/BUILD:112:1: C++ compilation of rule '//tensorflow/core/grappler/costs:robust_stats' failed (Exit 1): gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_localuser/88f96db14f5044a661b2d9ab97596b51/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/opt/jdk1.8.0_144/bin:/home/localuser/bazel-0.6.1/output:/usr/local/MATLAB/R2016a/bin:/Ansys/AnsysEM170/AnsysEM17.0/Linux64:/Ansys/AnsysEM170/LayoutIntegrations17.0/Linux64:/usr/local/bin:/usr/local/netscape:/usr/sbin:/usr/bin:/usr/lib:/bin:/sbin:/etc:/opt/lumerical/fdtd/bin:/lib:.:/root/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python2.7 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-march=native' '-std=c++0x' -MD -MF bazel-out/local-opt/bin/tensorflow/core/grappler/costs/_objs/robust_stats/tensorflow/core/grappler/costs/robust_stats.pic.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/core/grappler/costs/_objs/robust_stats/tensorflow/core/grappler/costs/robust_stats.pic.o' -fPIC -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/grappler/costs/robust_stats.cc -o bazel-out/local-opt/bin/tensorflow/core/grappler/costs/_objs/robust_stats/tensorflow/core/grappler/costs/robust_stats.pic.o).\r\ntensorflow/core/grappler/costs/robust_stats.cc: In function 'std::pair<double, double> tensorflow::grappler::ScaledMedianAbsoluteDeviation(const std::vector<double, std::allocator<double> >&)':\r\ntensorflow/core/grappler/costs/robust_stats.cc:71: error: expected initializer before ':' token\r\ntensorflow/core/grappler/costs/robust_stats.cc:75: error: expected primary-expression before 'return'\r\ntensorflow/core/grappler/costs/robust_stats.cc:75: error: expected ')' before 'return'\r\ntensorflow/core/grappler/costs/robust_stats.cc: In constructor 'tensorflow::grappler::RobustStats::RobustStats(const std::vector<double, std::allocator<double> >&)':\r\ntensorflow/core/grappler/costs/robust_stats.cc:79: error: type 'tensorflow::grappler::RobustStats' is not a direct base of 'tensorflow::grappler::RobustStats'\r\ntensorflow/core/grappler/costs/robust_stats.cc: In function 'double tensorflow::grappler::UpdateHuberMean(const std::vector<double, std::allocator<double> >&, double, double)':\r\ntensorflow/core/grappler/costs/robust_stats.cc:95: error: expected initializer before ':' token\r\ntensorflow/core/grappler/costs/robust_stats.cc:152: error: expected primary-expression at end of input\r\ntensorflow/core/grappler/costs/robust_stats.cc:152: error: expected ';' at end of input\r\ntensorflow/core/grappler/costs/robust_stats.cc:152: error: expected primary-expression at end of input\r\ntensorflow/core/grappler/costs/robust_stats.cc:152: error: expected ')' at end of input\r\ntensorflow/core/grappler/costs/robust_stats.cc:152: error: expected statement at end of input\r\ntensorflow/core/grappler/costs/robust_stats.cc:92: warning: unused variable 'num_within'\r\ntensorflow/core/grappler/costs/robust_stats.cc:93: warning: unused variable 'sum'\r\ntensorflow/core/grappler/costs/robust_stats.cc:152: error: expected '}' at end of input\r\ntensorflow/core/grappler/costs/robust_stats.cc:152: warning: no return statement in function returning non-void\r\ntensorflow/core/grappler/costs/robust_stats.cc: At global scope:\r\ntensorflow/core/grappler/costs/robust_stats.cc:152: error: expected '}' at end of input\r\ntensorflow/core/grappler/costs/robust_stats.cc:152: error: expected '}' at end of input\r\ntensorflow/core/grappler/costs/robust_stats.cc:63: warning: 'std::pair<double, double> tensorflow::grappler::ScaledMedianAbsoluteDeviation(const std::vector<double, std::allocator<double> >&)' defined but not used\r\ncc1plus: warning: unrecognized command line option \"-Wno-free-nonheap-object\"\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 10.342s, Critical Path: 0.72s\r\n```\r\n\r\n### Comments\r\nI checked that it's not a memory issue. \r\n\r\n### Related issues\r\n[#8642](https://github.com/tensorflow/tensorflow/issues/8462)\r\n", "comments": ["Not sure, without looking at the code, maybe gcc is too old?", "I have updated my gcc to 4.8.2. Same compilation error shows up. \r\nA similar error might have been reported [here](https://github.com/tensorflow/models/issues/1261#issuecomment-293288093). ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Was that fixed recently? I think I saw some PR that had parens around the initializer list!", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 15117, "title": "Fix the iOS makefile on tensorflow lite.", "body": "Just make it right.\r\nRef: https://github.com/tensorflow/tensorflow/issues/15074", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Hi, it seems that it's not this pull request failing the tests. Anything I could do to fix the errors?", "It build successfully after I downloaded Eigen from https://bitbucket.org/eigen/eigen/get/b6e6d0cf6a77.zip. Hope this helps.", "Closed. Similar pull request that has been merged: https://github.com/tensorflow/tensorflow/pull/15191"]}, {"number": 15116, "title": "Any other official way(not public in github issues) to report security bug of tensorflow?", "body": "Hi,\r\n\r\nI have found a security issue in Tensorflow. An attacker can easily execute arbitary code on victim machine through this issue. I think it has severe secure impact on tensorflow users. So it is not proper to disclosure this issue on github issues publicly before it is fixed.\r\n\r\nIs there any other official way to report security issue? I will explain the details.\r\nThanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Not a google developer, but how about this?\r\nhttps://www.google.com/about/appsecurity/", "@zhangbo5891001 Is it related with the saved model file? Maybe we could try to sign it if it is. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "sorry for the delay.\r\nThis issue has been reported to google tensorflow team. And We are discussing the solution."]}, {"number": 15115, "title": "Wrong result when computing accuracy using tf.metrics.accuracy", "body": "### System information\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 1709\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: Python 3.5.2 :: Anaconda custom (64-bit)\r\n\r\n### Describe the problem\r\n\r\nI found the result `tf.metrics.accuracy` returns is incorrect when I trained my model. To verify this I wrote a  simple program.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\nlabels = tf.placeholder(tf.int32)\r\npredictions = tf.placeholder(tf.int32)\r\nacc, _ = tf.metrics.accuracy(labels, predictions)\r\nmy_acc = tf.reduce_mean(tf.cast(tf.equal(labels, predictions), tf.float32))\r\n\r\nfeed_dict = {\r\n    labels: [1, 2, 3, 4, 5], \r\n    predictions: [1, 2, 3, 4, 5]\r\n}\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(tf.local_variables_initializer())\r\n\r\nsess.run(acc, feed_dict)  # 0.0\r\nsess.run(my_acc, feed_dict)  # 1.0\r\n```\r\n\r\nYou can see that `acc` and `my_acc` is different and acc is wrong. I double checked [the doc](https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy) and still confused. Is there anything I missed? Thank you.", "comments": ["But **the result in evaluation mode is correct while wrong in train mode**. I'm using `tf.estimator` and `tf.data.TFRecordDataset`. The following are relative codes.\r\n\r\n```python\r\ndef cifar_model_fn(features, labels, mode):\r\n    # some other codes\r\n\r\n    logits = tf.layers.dense(inputs=dropout, units=10)\r\n    predictions = {\r\n        'classes': tf.argmax(input=logits, axis=1, name='classes'),\r\n        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\r\n    }\r\n    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\r\n    loss = tf.losses.softmax_cross_entropy(onehot_labels, logits)\r\n    accuracy, update_op = tf.metrics.accuracy(labels=labels, predictions=predictions['classes'], name='accuracy')\r\n    # my way to compute accuracy, which gives correct result when training\r\n    my_acc = tf.reduce_mean(tf.cast(tf.equal(tf.cast(labels, tf.int64), predictions['classes']), tf.float32))\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        tensors_to_log = {\r\n            'Accuracy': accuracy,\r\n            'My accuracy': my_acc}\r\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=100)\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\r\n        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op, training_hooks=[logging_hook])\r\n\r\n    eval_metric_ops = {\r\n        'accuracy': (accuracy, update_op)\r\n    }\r\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n\r\ndef main():\r\n    # create dataset ....\r\n\r\n    cifar10_classifier = tf.estimator.Estimator(model_fn=cifar_model_fn, model_dir=FLAGS.model_dir)\r\n    cifar10_classifier.train(input_fn=train_input_fn)\r\n    eval_results = cifar10_classifier.evaluate(input_fn=eval_input_fn)\r\n```\r\n\r\nMy purpose is to log training accuracy when training. The following is a part of the console output. `Accuracy` is **always** 0.0.\r\n\r\n```\r\nINFO:tensorflow:loss = 1.08232, step = 7301 (9.312 sec)\r\nINFO:tensorflow:My accuracy = 0.59375, Accuracy = 0.0 (9.311 sec)\r\nINFO:tensorflow:global_step/sec: 10.8054\r\nINFO:tensorflow:loss = 1.29713, step = 7401 (9.257 sec)\r\nINFO:tensorflow:My accuracy = 0.484375, Accuracy = 0.0 (9.256 sec)\r\nINFO:tensorflow:global_step/sec: 11.4928\r\nINFO:tensorflow:loss = 1.31355, step = 7501 (8.693 sec)\r\nINFO:tensorflow:My accuracy = 0.578125, Accuracy = 0.0 (8.693 sec)\r\nINFO:tensorflow:global_step/sec: 12.1267\r\nINFO:tensorflow:loss = 1.25855, step = 7601 (8.252 sec)\r\nINFO:tensorflow:My accuracy = 0.515625, Accuracy = 0.0 (8.254 sec)\r\nINFO:tensorflow:global_step/sec: 12.0489\r\nINFO:tensorflow:loss = 1.21857, step = 7701 (8.303 sec)\r\nINFO:tensorflow:My accuracy = 0.5625, Accuracy = 0.0 (8.304 sec)\r\nINFO:tensorflow:global_step/sec: 12.1971\r\nINFO:tensorflow:loss = 0.983289, step = 7801 (8.196 sec)\r\nINFO:tensorflow:My accuracy = 0.6875, Accuracy = 0.0 (8.195 sec)\r\n```\r\n\r\n", "Beware that tf.metrics.accuracy is computed over an entire session run, not per batch. I just kicked it out because I did not get the utility of it. Instead, I'm doing the same thing as you with reduce_mean to have batch accuracy.", "As @jmaye said, `tf.metrics.accuracy` is not meant to compute the accuracy of a single batch. It returns both the accuracy and an update_op, and `update_op` is intended to be run every batch, which updates the accuracy. See #9498 for more discussion on this.", "@reedwm @jmaye Thank you for your reply. I read #9498 and understand that `update_op` is called every batch. But I still have one question: **When** the `accuracy` is computed?\r\n\r\n[The doc](https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy) says:\r\n\r\n> The accuracy function creates two local variables, `total` and `count` that are used to compute the frequency with which predictions matches labels. This frequency is ultimately returned as accuracy: an idempotent operation that simply divides `total` by `count`.\r\n\r\nbut doesn't say when the accuracy is computed, i.e. divides `total` by `count`.\r\n\r\nIn my codes above, the accuracy is computed when finishing evaluation progress. But is it supposed to computing the accuracy when finishing the entire training progress? ", "`accuracy` is recomputed everytime it is evaluated.\r\n\r\nHow `tf.metrics.accuracy` works in general is that it maintains two variables, `total` and `count`, each which starts at 0. Whenever `accuracy` is evaluated, it returns `total / count` (or 0 if count is 0), and does not modify `total` or `count`. `accuracy` also does not look `labels` or `predictions`. It just does a division.\r\n\r\nWhen `update_op` is evaluated, it uses `labels` and `predictions` to increase `total` and `count`. It increases `total` by the number of predictions that match the labels, and increases `count` by the total number of predictions. The idea is that you run `update_op` for every new batch of labels and predictions you get. Then, whenever you want to check to accuracy of all the batches you've seen so far, you evaluate `accuracy`, which will have the current accuracy of the batches seen so far. `accuracy` will change only when you run `update_op`, since only running `update_op` modifies `total` and `count`.\r\n\r\nIn your Estimators case, `update_op` is never run during training, only during evaluation. The Estimator will automatically call the `update_op` every batch when you return the EstimatorSpec with `eval_metric_ops` during evaluation, but it does not during training.", "@reedwm Thank you for your detailed explanation. I got the idea.", "@reedwm  @jmaye \r\n`if mode == tf.estimator.ModeKeys.EVAL:`\r\n`            a = tf.random.uniform(dtype=tf.int32, maxval=1000, shape=[])`\r\n`            eval_metric_ops = {`\r\n `               \"eval_mean_bert_loss\": tf.metrics.mean(total_loss_bert),`\r\n`                \"eval_mean_original_loss\": tf.metrics.mean(total_loss_original),`\r\n`                \"eval_mean_loss\": tf.metrics.mean(a)}`\r\n`            output_spec = tf.estimator.EstimatorSpec(`\r\n`                mode=mode,`\r\n`                loss=a,`\r\n`                eval_metric_ops=eval_metric_ops)`       \r\n        \r\n`eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, steps=100, start_delay_secs=0,\r\n                                      throttle_secs=120)`\r\n\r\n[output](https://imgur.com/a/lT92UoU)       \r\n`INFO:tensorflow:Saving dict for global step 2000: eval_mean_bert_loss = 4.9399266, eval_mean_loss = 483.38, eval_mean_original_loss = 4.982164, global_step = 2000, loss = 483.38\r\n`\r\nhow can `loss` and `eval_mean_loss` be exactly same? batch size is 1 here. Shouldn't `loss` be the value of `a` at 100th batch, and `eval_mean_loss` be the mean of the all 100 `a` ?\r\n\r\n", "@dchatterjee172, can you provide a self-contained, relatively short example I can run to reproduce? It does seem like `loss` should be different from `eval_mean_loss`. I am also unsure how loss can be a non-integer, since you passed an `int32` value for loss.", "@reedwm \r\n`tf.metrics.mean` will cast the `values` argument to float.\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/ops/metrics_impl.py#L392", "Even with the cast in `tf.metrics.mean`, you pass `loss=a`, which is an int. In any case, a self-contained example would make this easier to debug.", "> Beware that tf.metrics.accuracy is computed over an entire session run, not per batch. I just kicked it out because I did not get the utility of it. Instead, I'm doing the same thing as you with reduce_mean to have batch accuracy.\r\n\r\nIt can be helpful when there is a lot of oscillation in batch accuracy. The aggregated accuracy will be a lot smoother and so it's easier to observe learning from it's values.", "why the value returned by tf.metrics.accuracy is always 0? "]}, {"number": 15114, "title": "Compiling CPU version on windows X86", "body": "System Information:\r\nWIN 10\r\nVisual studio 2015\r\nSwigwin 3.0.12\r\nPython 3.5.3\r\nCmake 3.10.0\r\n\r\nQuestion description:\r\n\r\nI am a new one about tensorflow, I used cmake command in cmd just like this:\r\n\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/Programs/Python/Python35/python.exe -DPYTHON_LIBRARIES=D:/Programs/Python/Python35/libs/python35.lib -Dtensorflow_ENABLE_GPU=OFF\r\n\r\nand got the x64 vs project, but when I wanted to generate x86 version with similar commnd like:  \r\n\r\ncmake .. -A x86 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/Programs/Python/Python35/python.exe -DPYTHON_LIBRARIES=D:/Programs/Python/Python35/libs/python35.lib -Dtensorflow_ENABLE_GPU=OFF\r\n\r\nthen I got a error:\r\n\r\ncmake .. -A x86 -DCMAKE_BUILD_TYPE=Debug -DSWIG_EXECUTABLE=D:/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/Programs/Python/Python35/python.exe -DPYTHON_LIBRARIES=D:/Programs/Python/Python35/libs/python35.lib -Dtensorflow_ENABLE_GPU=OFF\r\nCMake Error at CMakeLists.txt:5 (project):\r\n  Failed to run MSBuild command:\r\n\r\n    C:/Program Files (x86)/MSBuild/14.0/bin/MSBuild.exe\r\n\r\n  to get the value of VCTargetsPath:\r\n\r\n\r\nNow I want to ask: does tensorflow support x86 windows? Did anyone build the x86 version once before?  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I mistook the command. When I used the command: cmake .. -A Win32 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/Programs/Python/Python35/python.exe -DPYTHON_LIBRARIES=D:/Programs/Python/Python35/libs/python35.lib -Dtensorflow_ENABLE_GPU=OFF, I got the right project."]}, {"number": 15113, "title": "compiling CPU version under windows X86", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 15112, "title": "variables.get_global_step() is deprecated", "body": "variables.get_global_step() is deprecated, use `training_util.get_global_step()` instead\r\n\r\nWARNING message:\r\n\r\n```\r\ntensorflow/contrib/timeseries/python/timeseries/head.py:63: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\n```", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 15111, "title": "add a new operator :`sparse_tile_like interface`", "body": "Hi, we are doing some research on graph networks which is very necessary for us to use sparse operators. we think there might be someone else who has the same demands will join us. Here we would continue to update those operators. Thanks.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I used different emails. I signed a new CLA."]}, {"number": 15110, "title": "Merge pull request #1 from tensorflow/master", "body": "Fetch from upstream", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?"]}, {"number": 15109, "title": "Add a setter method variable_scope.auto_reuse_variables() to enable AUTO_REUSE", "body": "Currently we can enable variable reuse in a scope with variable_scope.reuse_variables(). However, there is no handy method to enable auto reuse yet. This PR adds this functionality.", "comments": ["Can one of the admins verify this patch?", "(From API review)\r\n\r\nWe'd like to avoid this change as it makes the variable scope harder to reason about (harder than it is now :). We don't have a way to set `self._reuse = False` either.\r\n\r\nIf you have a strong use case for this, we'd love to hear it. Otherwise, we're tempted to drop this change.\r\n\r\nThanks.", "Thanks for the feedback @asimshankar. I understand the reason for not having an API to reset the reuse status.\r\n\r\nCurrently the only way to enable auto-reuse for a scope is at its creation (if I understand it correctly). This forces me to create an auxiliary scope purely for this purpose in multi-GPU training. Models created in the auxiliary scope inherits the scope name as prefix, which causes the parameter names to be different from those in the pre-trained models without this scope. Even though I can manually match the names, it would be much easier and elegant to go without the auxiliary scope.", "@bowang : Could you expand a bit on the code you're writing? `AUTO_REUSE` is a relatively new thing anyway. The idea is that you'd create variables once (so with `reuse=False`) and then create more operations (e.g., towers in a multi-GPU configuration) with `reuse=True`. Can you use that pattern in your code?", "@asimshankar Here are some example code with/without `AUTO_REUSE`\r\n\r\nWithout `AUTO_REUSE`\r\n```\r\nfor i in range(num_gpus):\r\n  with tf.device('/gpu:%d' % i):\r\n    with tf.variable_scope('gpu', reuse=(i>0)):\r\n      create_gpu_tower(i)\r\n```\r\n\r\nWith `AUTO_REUSE`\r\n```\r\ntf.get_variable_scope()._reuse = tf.AUTO_REUSE\r\nfor i in range(num_gpus):\r\n  with tf.device('/gpu:%d' % i):\r\n    create_gpu_tower(i)\r\n```\r\n\r\nBy using `AUTO_REUSE`, we can avoid create the auxiliary scope `gpu`, which makes it easier and compatible with models created for a single GPU.", "Hi @asimshankar , I know it has been a while. Does the use case in my previous comment makes sense?\r\nWith this feature we can get rid of the unnecessary variable scope when training on multiple GPUs and generate the same graph as we do on a single GPU.", "Nagging Assignee @caisq: It has been 202 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "As we make progress on TF 2.0, variable_scope becomes obsolete (and instead sharing is achieved by sharing the Python `Variable` object). Hence closing this PR. "]}, {"number": 15108, "title": "error: namespace \"Eigen::half_impl\" has no member \"__half_raw\" when building latest TensorFlow/CUDA 9.0", "body": "When trying to build latest TensorFlow for CUDA 9.0/CuDNN 7.0, from today's head\r\n\r\nComplete instructions to reproduce are [here](https://github.com/yaroslavvb/tf_build), but basically I followed steps in in [Dockerfile.devel-gpu](https://github.com/tensorflow/tensorflow/blob/c5c642e051f1a7876d099bfcd9f8a2ecaf7227b8/tensorflow/tools/docker/Dockerfile.devel-gpu) to install dependencies, and then did `bazel build -c opt --config=cuda`\r\n\r\nHere are the errors\r\n\r\n```\r\nINFO: From Compiling tensorflow/stream_executor/cuda/cuda_blas.cc:\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cudaDataType_t perftools::gputools::cuda::{anonymous}::CUDAComputationType(perftools::gputools::blas::ComputationType)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:527:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\nINFO: From Compiling tensorflow/core/kernels/dense_to_sparse_batch_dataset_op.cc:\r\ntensorflow/core/kernels/dense_to_sparse_batch_dataset_op.cc: In member function 'virtual void tensorflow::{anonymous}::DenseToSparseBatchDatasetOp::MakeDataset(tensorflow::OpKernelContext*, tensorflow::DatasetBase*, tensorflow::DatasetBase**)':\r\ntensorflow/core/kernels/dense_to_sparse_batch_dataset_op.cc:83:71: warning: 'batch_size' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n         : batch_size_(batch_size), row_shape_(row_shape), input_(input) {\r\n                                                                       ^\r\ntensorflow/core/kernels/dense_to_sparse_batch_dataset_op.cc:41:11: note: 'batch_size' was declared here\r\n     int64 batch_size;\r\n           ^\r\nINFO: From Compiling tensorflow/core/kernels/histogram_op_gpu.cu.cc:\r\n./tensorflow/core/util/cuda_kernel_helper.h(109): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(109): error: expected a \")\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(110): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(113): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(113): error: expected a \";\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(119): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(119): error: expected a \")\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(120): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(123): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(123): error: expected a \";\"\r\n\r\n10 errors detected in the compilation of \"/tmp/tmpxft_000137d9_00000000-6_histog\r\n\r\n```", "comments": ["https://github.com/tensorflow/tensorflow/pull/14770 should fix this issue.", "#14770 seems not fix this issue. Still get error from cuda_kernel_helper", "seconded, same issue.", "looking into the history the breaking code introduced yesterday by \r\nWrappers for CUDA 9 warp-synchronous intrinsics.", "Cc @zheng-xq", "Looks like I was wrong, the PR I mentioned did not fix this completely. However this is fixed internally.\r\n@caisq when are we planning the next push from internal to github?", "I'll push as soon as the ongoing pull is complete, @gunan", "I tried to build it, but it seems that the issue still exist", "FYI, Issue still there in case the push already happened.", "The push is open in PR https://github.com/tensorflow/tensorflow/pull/15174, awaiting approval.", "This should now be fixed.", "This is back again. Commit  1ad680d3a seems to break it again.", "just leaving some of the build error messages from a cuda9.1 build with openmpi\r\nINFO: From Compiling tensorflow/core/kernels/gather_functor_gpu.cu.cc:^M\r\n./tensorflow/core/util/cuda_kernel_helper.h(116): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(116): error: expected a \")\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(117): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(120): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(120): error: expected a \";\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(126): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(126): error: expected a \")\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(127): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(130): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(130): error: expected a \";\"\r\n\r\n10 errors detected in the compilation of \"/tmp/tmpxft_00004514_00000000-6_gather_functor_gpu.cu.cpp1.ii\".\r\nERROR: /home/levinth/tensorflow/tensorflow/core/kernels/BUILD:1139:1: output 'tensorflow/core/kernels/_objs/gather_functor_gpu/tensorflow/core/kernels/gather_functor_gpu.cu.pic.o' was not created", "@reedwm @zhengxq FYI.\r\n\r\nI think we have an internal fix, we are waiting for that to be pushed out.\r\n@reedwm, could you confirm?", "1ad680d3a36e9742229748d5c5d3bddb1d0a2578 has been rolled back internally, so this should be fixed internally. Thank you @samikama for identifying the specific commit that broke it.", "when will we see the fix externally?", "Should be at master now.\n\nOn Dec 23, 2017 9:26 AM, \"David Levinthal Ph.D.\" <notifications@github.com>\nwrote:\n\n> when will we see the fix externally?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15108#issuecomment-353738119>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOSprlp48-GpM_vnx1dbV9hLSnZ5qks5tDTfZgaJpZM4Q1e9q>\n> .\n>\n", "yup..fresh git clone and\nINFO: Elapsed time: 362.184s, Critical Path: 157.04s^M\nINFO: Build completed successfully, 2767 total actions^M\n\n\nOn Sat, Dec 23, 2017 at 11:56 AM, Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> Should be at master now.\n>\n> On Dec 23, 2017 9:26 AM, \"David Levinthal Ph.D.\" <notifications@github.com\n> >\n> wrote:\n>\n> > when will we see the fix externally?\n> >\n> > \u2014\n> > You are receiving this because you modified the open/close state.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> 15108#issuecomment-353738119>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/AHlCOSprlp48-GpM_\n> vnx1dbV9hLSnZ5qks5tDTfZgaJpZM4Q1e9q>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15108#issuecomment-353744906>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIUuT0U7Mwnejouvfo7Jk0_ZkPFbThrCks5tDVsBgaJpZM4Q1e9q>\n> .\n>\n", "Thanks for verifying!\r\nThen I am closing this issue."]}, {"number": 15107, "title": "Fix link to BUILD file in android readme.", "body": "Simple syntax error.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15106, "title": "MKL: Dockerfile Locked to Broadwell/AVX2 arch to work around Eigen Issues with AVX512", "body": "Eigen currently has issues when Tensorflow is compiled with -march=skylake, or the container is build on a skylake system with -march=native. The container is now compiled with -march=broadwell, which adds AVX2 instructions, which are supported on Skylake, Haswell, Broadwell, KNL and KNM architectures.", "comments": ["Can one of the admins verify this patch?", "Doesn't this break compatibility with older hardware? Probably not a good idea.", "@tensorflow-jenkins test this please", "ping @gunan Can you merge this?"]}, {"number": 15105, "title": "Using tfdbg with Monitored Session", "body": "### System information\r\n- I am using a modified version of  [CIFAR10 tutorial](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10) TensorFlow.\r\n- CentOS Linux release 7.3.1611\r\n- TensrFlow from Source\r\n- TensorFlow v1.3\r\n-  Python 2.7\r\n\r\n### Describe the problem\r\nHello,\r\n\r\nI see that I can not use tfdbg with Monitored session. That is if I try to wrap `mon_sess = tf_debug.LocalCLIDebugWrapperSession(mon_sess)` where `mon_sess` is `tf.train.MonitoredTrainingSession(` I get the following error:\r\n\r\n`TypeError: Expected type <class 'tensorflow.python.client.session.BaseSession'>; got type <class 'tensorflow.python.training.monitored_session.MonitoredSession'>`\r\n\r\nCan there be a support from Debugger with Monitored Sessions?", "comments": ["This has been fixed in dc58dcbea9beb30e0013f41e08786471ca4a6cb6. Try upgrading to TensorFlow 1.4.", "Hello,\r\n\r\nThanks for the info, I updated to v1.4 and `tfdbg` works for me.\r\n\r\nHowever, there seems to be some unsupported features. When I ran with `tfdbg> run -f has_inf_or_nan`, my model diverged with a NaN. \r\n\r\nHowever, I received the following error:\r\n\r\n`tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.\r\nException ignored in: <bound method LocalCLIDebugWrapperSession.__del__ of <tensorflow.python.debug.wrappers.local_cli_wrapper.LocalCLIDebugWrapperSession object at 0x7ff95630a6a0>>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.4/site-packages/tensorflow/python/debug/wrappers/framework.py\", line 705, in __del__\r\nAttributeError: 'MonitoredSession' object has no attribute '__del__' `.\r\n\r\nCan you please guide me through this?\r\n\r\n", "I can reprodce on master with the following program:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\nwith tf.train.MonitoredSession() as sess:\r\n  sess = tf_debug.LocalCLIDebugWrapperSession(sess)\r\ndel sess\r\n```\r\n\r\n@caisq, can you fix this?", "Thanks for letting me know. Will fix it soon."]}, {"number": 15104, "title": "Building tensorflow from the sourcefile", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRedhat\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.0\r\n- **Python version**: \r\n2.7.14\r\n- **Bazel version (if compiling from source)**:\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n4.8.5\r\n- **CUDA/cuDNN version**:\r\n8/5\r\n- **GPU model and memory**:\r\n\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nGetting the following error:\r\n```\r\nERROR: /home/amalik/tensorflow/tensorflow/tools/test/BUILD:81:1: Traceback (most recent call last):\r\n\tFile \"/home/amalik/tensorflow/tensorflow/tools/test/BUILD\", line 81\r\n\t\ttf_cc_logged_benchmark(name = \"cast_op_benchmark\", target...\")\r\n\tFile \"/home/amalik/tensorflow/tensorflow/tools/test/performance.bzl\", line 23, in tf_cc_logged_benchmark\r\n\t\tlist((set(tags) + set([\"benchmark-tes...\"])))\r\n\tFile \"/home/amalik/tensorflow/tensorflow/tools/test/performance.bzl\", line 23, in list\r\n\t\tset(tags)\r\nThe `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false.\r\nERROR: package contains errors: tensorflow/tools/test.\r\nERROR: /home/amalik/tensorflow/tensorflow/tools/test/BUILD:86:1: Traceback (most recent call last):\r\n\tFile \"/home/amalik/tensorflow/tensorflow/tools/test/BUILD\", line 86\r\n\t\ttf_py_logged_benchmark(name = \"rnn_op_benchmark\", target ...\")\r\n\tFile \"/home/amalik/tensorflow/tensorflow/tools/test/performance.bzl\", line 52, in tf_py_logged_benchmark\r\n\t\ttf_cc_logged_benchmark(name = name, target = target, benchm..., <2 more arguments>)\r\n\tFile \"/home/amalik/tensorflow/tensorflow/tools/test/performance.bzl\", line 23, in tf_cc_logged_benchmark\r\n\t\tlist((set(tags) + set([\"benchmark-tes...\"])))\r\n\tFile \"/home/amalik/tensorflow/tensorflow/tools/test/performance.bzl\", line 23, in list\r\n\t\tset(tags)\r\nThe `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false.\r\nERROR: /home/amalik/tensorflow/tensorflow/core/kernels/BUILD:58:14: Traceback (most recent call last):\r\n\tFile \"/home/amalik/tensorflow/tensorflow/core/kernels/BUILD\", line 53\r\n\t\tconfig_setting(name = \"xsmm_backward\", values = {...\"})\r\n\tFile \"/home/amalik/tensorflow/tensorflow/core/kernels/BUILD\", line 58, in config_setting\r\n\t\t{\"define\": \"tensorflow_xsmm=1\", \"define\": \"tensorflow_xsmm_backward=1\"}\r\nDuplicated key \"define\" when creating dictionary.\r\nERROR: package contains errors: tensorflow/core/kernels.\r\nERROR: error loading package 'tensorflow/core/kernels': Package 'tensorflow/core/kernels' contains errors.\r\n```\r\n\r\nNote: Tried other plateforms but could get any reply. I am not sure its a bug or else. I apologize if I am using the wrong forum. I am trying to install tensorflow from source so I can use MPI\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["What's your bazel version? That's very important", "It is 0.7.0. Thanks", "Which Tensorflow version should be best for MPI? I am using 1.0.", "It's too new. You should either upgrade your TF or downgrade your bazel.  You can get the matching bazel version in tensorflow/tools/ci_build/install/install_bazel.sh.\r\n\r\nPlease close this issue, as this is not a bug of tensorflow. ", "Thanks\r\n", "@gunan any idea?\r\n@abidmalik1967 what is the bazel version you are using?"]}, {"number": 15103, "title": "No GPU OpKernel for tf.exp() operation for Complex64", "body": "I am running tensorflow 1.4.0 from nightly build ('v1.3.0-rc1-5297-g4b7d79b6ea'  on ubuntu 16.04). I've had success working in eager mode (great job with this guys!) however I think I found a small bug:\r\n\r\nIt seems that there is no OpKernel on device='GPU'  for the tf.exp() operation applied to complex numbers in eager mode.  This can be reproduced with the below code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nwith tf.device('/gpu:0'):\r\n  g = tf.spectral.rfft(tf.ones(64))\r\n  \r\n  tf.exp(g)\r\n```\r\nwhich results in\r\n```NotFoundError: No registered 'Exp' OpKernel for GPU devices compatible with node Exp = Exp[T=DT_COMPLEX64](dummy_input)\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n [Op:Exp]\r\n```\r\n\r\na more practical example that would lead to this same error:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\r\n\r\nframe_length=256\r\nframe_step=64\r\nn_mels = 64\r\nsr=16000\r\nfilename = 'path/to/a.wav'\r\n\r\nsome_signal = tf.contrib.ffmpeg.decode_audio(tf.read_file(filename), \r\n                                     file_format='wav', \r\n                                     samples_per_second=16000, \r\n                                     channel_count=1)\r\n\r\nwith tf.device('/gpu:0'):\r\n  stft = tf.contrib.signal.stft(tf.transpose(some_signal), frame_length=frame_length, \r\n                                  frame_step=frame_step, fft_length=frame_length)\r\n\r\n  linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(\r\n      n_mels, 1+frame_length//2, sr)\r\n\r\n  magnitude_spectrograms = tf.abs(stft)\r\n  log_mel_spec = tf.log(1e-6+ tf.tensordot(magnitude_spectrograms,\r\n                                           linear_to_mel_weight_matrix, \r\n                                           axes = [[2], [0]]))\r\n\r\n  mfccs = tf.contrib.signal.mfccs_from_log_mel_spectrograms(log_mel_spec)\r\n```\r\n\r\nKeeping operations on CPU works just fine but I figured this would be easy to implement for GPU as well. Thanks\r\n", "comments": ["/CC @asimshankar", "Thanks for the report @VinceMarron . This behavior isn't an artifact of eager execution, just that we don't have GPU kernels (in eager or graph) for Complex types for the Exp operation (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_exp.cc)\r\n\r\nI don't believe we have any immediate plans for GPU kernels for exp for complex types, but contributions are welcome.\r\n\r\nThanks!", "Gotcha. Thanks for looking into this @asimshankar . My error on assuming it was related to eager. \r\n\r\nI looked into contributing a general resolution but I don't think I have the know-how in terms of Tensorflow structure and C. What I can provide is a framework for implementing it in terms of real (tf.float32) operations that are implemented on gpu - [cwise_op_cos](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_cos.cc) and [cwise_op_exp](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_exp.cc)\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n\r\na=tf.random_normal([1], dtype=tf.float32)\r\nb=tf.random_normal([1], dtype=tf.float32)\r\n\r\ncomplex_num = tf.complex(a,b)\r\ncpu_exp_complex_num = tf.exp(complex_num) #complex calc on cpu\r\ngpu_exp_complex_num = tf.complex(tf.exp(a)*tf.cos(b), tf.exp(a)*tf.sin(b)) #complex calc on gpu\r\n\r\nassert cpu_exp_complex_num.numpy() == gpu_exp_complex_num.numpy()\r\n```\r\n\r\nI'm not sure if that is helpful or already obvious but I'd be happy to assist an effort to implement this. ", "@VinceMarron in your STFT example, the Mel matrix is the only part that relies on Exp I believe. You can move the `linear_to_mel_weight_matrix` call outside of the GPU device assignment block, or use soft device placement (which should automatically put the Exp on CPU): https://www.tensorflow.org/tutorials/using_gpu#using_a_single_gpu_on_a_multi-gpu_system\r\n\r\n", "@rryan  -- Agreed. I may be mistaken but I think the tf.gather() portion of tf.contrib.signal.stft() must run on cpu, and cpu is probably better suited for that operation, whereas having to copy a tensor to cpu to run tf.exp() on complex64 seemed silly. It appears @facaiy has implemented a gpu kernel that fixes this. Thanks. ", "@VinceMarron -- Gather does have a GPU kernel (I'm one of its authors :)). \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/70e33e0f77cb4f92ddd70a8dd601873b178124fc/tensorflow/core/kernels/gather_op.cc#L153\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/70e33e0f77cb4f92ddd70a8dd601873b178124fc/tensorflow/core/kernels/gather_functor_gpu.cu.h", "Ah.. Awesome work on this @rryan. Clearly I'm the rookie here (but trying to learn..)\r\n\r\nI very well could be doing something else wrong but appears there is a glitch in 'gathering' from frame indices on gpu in eager mode. \r\n\r\nExample:\r\n\r\nIn standard graph mode this code runs just fine:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nframe_length=256\r\nframe_step=64\r\n\r\n\r\nsignal = tf.random_uniform((1,16000), minval=-1, maxval=1)\r\n\r\nwith tf.device('/gpu:0'):\r\n  frames = tf.contrib.signal.frame(signal, frame_length=frame_length, \r\n                                  frame_step=frame_step)\r\n  with tf.Session() as sess:\r\n    frames_out = sess.run(frames)\r\n    \r\nprint(frames_out[0, :2, :2], frames_out.shape)\r\n\r\n```\r\n\r\n> output:\r\n>[[ 0.64405823 -0.69103551]\r\n >[-0.37752914  0.50947428]] (1, 247, 256)\r\n\r\nThe equivalent code in eager mode, however, produces an error:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nframe_length=256\r\nframe_step=64\r\n\r\n\r\nsignal = tf.constant(tf.random_uniform((1,16000), minval=-1, maxval=1))\r\n\r\nwith tf.device('/gpu:0'):\r\n  frames = tf.contrib.signal.frame(signal, frame_length=frame_length, \r\n                                  frame_step=frame_step)\r\n    \r\nprint(frames[0, :2, :2], frames.shape)\r\n\r\n```\r\n\r\n> ---------------------------------------------------------------------------\r\n> InvalidArgumentError                      Traceback (most recent call last)\r\n> <ipython-input-1-6254609ba767> in <module>()\r\n>      12 with tf.device('/gpu:0'):\r\n>      13   frames = tf.contrib.signal.frame(signal, frame_length=frame_length, \r\n> ---> 14                                   frame_step=frame_step)\r\n>      15 \r\n>      16 print(frames[0, :2, :2], frames.shape)\r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/contrib/signal/python/ops/shape_ops.py in frame(signal, frame_length, frame_step, pad_end, pad_value, axis, name)\r\n>     114     axis = math_ops.range(signal_rank)[axis]\r\n>     115 \r\n> --> 116     signal_shape = array_ops.shape(signal)\r\n>     117     outer_dimensions, length_samples, inner_dimensions = array_ops.split(\r\n>     118         signal_shape, [axis, 1, signal_rank - 1 - axis])\r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py in shape(input, name, out_type)\r\n>     276     A `Tensor` of type `out_type`.\r\n>     277   \"\"\"\r\n> --> 278   return shape_internal(input, name, optimize=True, out_type=out_type)\r\n>     279 \r\n>     280 \r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py in shape_internal(input, name, optimize, out_type)\r\n>     304         if optimize and input_shape.is_fully_defined():\r\n>     305           return constant(input_shape.as_list(), out_type, name=name)\r\n> --> 306       return gen_array_ops.shape(input, name=name, out_type=out_type)\r\n>     307 \r\n>     308 \r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py in shape(input, out_type, name)\r\n>    4479     _attrs = (\"T\", _attr_T, \"out_type\", out_type)\r\n>    4480     _result = _execute.execute(b\"Shape\", 1, inputs=_inputs_flat, attrs=_attrs,\r\n> -> 4481                                ctx=_ctx, name=name)\r\n>    4482   _execute.record_gradient(\r\n>    4483       \"Shape\", _inputs_flat, _attrs, _result, name)\r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n>      64     else:\r\n>      65       message = e.message\r\n> ---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n>      67   # pylint: enable=protected-access\r\n>      68   return tensors\r\n> \r\n> ~/.local/lib/python3.5/site-packages/six.py in raise_from(value, from_value)\r\n> \r\n> InvalidArgumentError: Tensors on conflicting devices: cannot compute Shape as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu(), or transparently copied by using tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:Shape] name: frame/Shape/\r\n\r\n\r\nUsing  tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_SILENT) works just fine as a work around but I figure there is a small fix to be done to allow this to work entirely on gpu. \r\n", "@VinceMarron huh, so I haven't used Eager mode with a GPU, but I think that error is saying your `signal` tensor is not located on the GPU, while the op `Shape` (which is part of the subgraph created by `tf.contrib.signal.frame`) expected that it was. \r\n\r\nCan you try the advice in the error of putting `.gpu()` on the end of your definition of `signal` to explicitly copy it to the GPU?  Or maybe define `signal` within the `with tf.device('/gpu:0'):` block.\r\n", "Both of these result in error:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nframe_length=256\r\nframe_step=64\r\n\r\n\r\nsignal = tf.constant(tf.random_uniform((1,16000), minval=-1, maxval=1))\r\n\r\nwith tf.device('/gpu:0'):\r\n  frames = tf.contrib.signal.frame(signal.gpu(), frame_length=frame_length, \r\n                                  frame_step=frame_step)\r\n    \r\nprint(frames[0, :2, :2], frames.shape)\r\n```\r\n\r\n> InvalidArgumentError                      Traceback (most recent call last)\r\n> <ipython-input-1-0e43d1db52a4> in <module>()\r\n>      12 with tf.device('/gpu:0'):\r\n>      13   frames = tf.contrib.signal.frame(signal.gpu(), frame_length=frame_length, \r\n> ---> 14                                   frame_step=frame_step)\r\n>      15 \r\n>      16 print(frames[0, :2, :2], frames.shape)\r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/contrib/signal/python/ops/shape_ops.py in frame(signal, frame_length, frame_step, pad_end, pad_value, axis, name)\r\n>     183 \r\n>     184     frames = array_ops.reshape(\r\n> --> 185         array_ops.gather(subframes, selector, axis=axis),\r\n>     186         array_ops.concat([outer_dimensions, [num_frames, frame_length],\r\n>     187                           inner_dimensions], 0))\r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py in gather(params, indices, validate_indices, name, axis)\r\n>    2587         params, indices, validate_indices=validate_indices, name=name)\r\n>    2588   else:\r\n> -> 2589     return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n>    2590 \r\n>    2591 \r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py in gather_v2(params, indices, axis, name)\r\n>    2052               _attr_Taxis)\r\n>    2053     _result = _execute.execute(b\"GatherV2\", 1, inputs=_inputs_flat,\r\n> -> 2054                                attrs=_attrs, ctx=_ctx, name=name)\r\n>    2055   _execute.record_gradient(\r\n>    2056       \"GatherV2\", _inputs_flat, _attrs, _result, name)\r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n>      64     else:\r\n>      65       message = e.message\r\n> ---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n>      67   # pylint: enable=protected-access\r\n>      68   return tensors\r\n> \r\n> ~/.local/lib/python3.5/site-packages/six.py in raise_from(value, from_value)\r\n> \r\n> InvalidArgumentError: Tensors on conflicting devices: cannot compute GatherV2 as input #1 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu(), or transparently copied by using tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:GatherV2]\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nframe_length=256\r\nframe_step=64\r\n\r\n\r\nwith tf.device('/gpu:0'):\r\n  frames = tf.contrib.signal.frame(tf.constant(tf.random_uniform((1,16000), minval=-1, maxval=1)), \r\n                                   frame_length=frame_length, \r\n                                   frame_step=frame_step)\r\n    \r\nprint(frames[0, :2, :2], frames.shape)\r\n```\r\n\r\n> \r\n> InvalidArgumentError                      Traceback (most recent call last)\r\n> <ipython-input-1-629cf1159aad> in <module>()\r\n>      11   frames = tf.contrib.signal.frame(tf.constant(tf.random_uniform((1,16000), minval=-1, maxval=1)), \r\n>      12                                    frame_length=frame_length,\r\n> ---> 13                                    frame_step=frame_step)\r\n>      14 \r\n>      15 print(frames[0, :2, :2], frames.shape)\r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/contrib/signal/python/ops/shape_ops.py in frame(signal, frame_length, frame_step, pad_end, pad_value, axis, name)\r\n>     183 \r\n>     184     frames = array_ops.reshape(\r\n> --> 185         array_ops.gather(subframes, selector, axis=axis),\r\n>     186         array_ops.concat([outer_dimensions, [num_frames, frame_length],\r\n>     187                           inner_dimensions], 0))\r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py in gather(params, indices, validate_indices, name, axis)\r\n>    2587         params, indices, validate_indices=validate_indices, name=name)\r\n>    2588   else:\r\n> -> 2589     return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n>    2590 \r\n>    2591 \r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py in gather_v2(params, indices, axis, name)\r\n>    2052               _attr_Taxis)\r\n>    2053     _result = _execute.execute(b\"GatherV2\", 1, inputs=_inputs_flat,\r\n> -> 2054                                attrs=_attrs, ctx=_ctx, name=name)\r\n>    2055   _execute.record_gradient(\r\n>    2056       \"GatherV2\", _inputs_flat, _attrs, _result, name)\r\n> \r\n> ~/anaconda3/envs/tf_source/lib/python3.5/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n>      64     else:\r\n>      65       message = e.message\r\n> ---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n>      67   # pylint: enable=protected-access\r\n>      68   return tensors\r\n> \r\n> ~/.local/lib/python3.5/site-packages/six.py in raise_from(value, from_value)\r\n> \r\n> InvalidArgumentError: Tensors on conflicting devices: cannot compute GatherV2 as input #1 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu(), or transparently copied by using tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:GatherV2]\r\n\r\nSetting 'tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)' is an obvious and effective 'bandaid' but it would be nice to figure out the root of the problem.  Are the 'subframes' from [tf.contrib.signal.frame](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/signal/python/ops/shape_ops.py) being forced to CPU for some reason?", "@VinceMarron so sorry I didn't see your most recent post. I think whatever issue you're seeing might have been fixed, because I just ran your examples on TF 1.9.0 on a Colab w/ GPU and don't see the error.\r\n\r\n"]}, {"number": 15102, "title": "Documentation required for SetIsStateful() method in OP registration", "body": "There are some examples with custom ops that use SetIsStateful() method during registration, e.g. https://www.tensorflow.org/extend/new_data_formats\r\n\r\nBut there is no documentation about that method.", "comments": ["This is documented  in the underlying op_def.proto that REGISTER generates...\r\nhttps://github.com/tensorflow/tensorflow/blob/0b131503a04f1ebbe0967bebb2559dd1367baded/tensorflow/core/framework/op_def.proto#L125-L133\r\n", "Ok, \"mooving between devices\"...\r\n\r\nBut why string ops does not have such constraint https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/string_ops.cc\r\n\r\nIt's still not clear.\r\n\r\n", "@shkarupa-alex it looks like @aselle answered the question. Reading the doc, I think the doc is clear enough for a reasonably skilled tensorflow developer. That's a tradeoff for conciseness. Feel free to submit a PR if you find a better wording."]}, {"number": 15101, "title": "Update docs for `tf.contrib.losses` -> `tf.losses`", "body": "This fix updates the docs in `extend/estimators.md` and changes the reference from `tf.contrib.losses` to `tf.losses`, as the former has been deprecated.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 15100, "title": "Branch 177809511", "body": "", "comments": ["@tensorflow-jenkins test this please", "cc @asimshankar @agarwal-ashish "]}, {"number": 15099, "title": "[CMake] Re-Enable include", "body": "Issue #3996 (and referenced tickets) are already fixed.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@sb2nov Is this the #14877 PIP package issue again?\r\n`ImportError: No module named 'tensorflow.contrib.rnn.python.kernel_tests'`\r\n`ImportError: No module named 'tensorflow.contrib.data.python.kernel_tests'`", "@caisq Test failures are well-known and should be unrelated to this PR."]}]