[{"number": 36496, "title": "[INTEL MKL] MKL-DNN v1.0 integration with pooling ops", "body": "MKL-DNNL v1.0 integration with pooling ops (avg & max pooling, both fwd and bwd).", "comments": ["@penpornk Not sure if you are reviewing other DNN 1.0 PRs if yes can you also make some comments on these please or pull this in", "@penpornk Many has not been addressed. I am working on it. pushed a check in. More soon. Next commit will have most of the changes\r\nThank you!", "@nammbash Sorry about that! I'll wait until you say it's ready next time.", "@penpornk Thank you and sorry for pushing commits without fixing everything. I addressed all comments now. please let me know.\r\n@rthadur if you need to start any of the CI , please do so.", "@penpornk Can you let me know if there are any more changes to it. Other PR's are waiting on this apprently from your comments", "@penpornk Thank you! Will update the team internally for the changes to common files. Thanks for the review again!", "@nammbash That would be very helpful. Thank you! I'll try to get this PR merged by today (if there is no test failure) and hopefully merge other PRs tomorrow.", "@penpornk thank you. There is a GPU failure? unrelated to my PR!?", "@nammbash Don't worry about `Windows Bazel GPU`. It's unrelated. Only `Ubuntu CPU` and `Ubuntu Sanity` need to pass. It's fine if the failed ones are all unrelated.  ", "@penpornk @rthadur All passed except one which is unrelated to my PR. Please merge it. :) thank you.", "@nammbash I'm getting `clang-format` errors. Would you mind running `clang-format` on `mkl_util.h`? This is the diff I got:\r\n```\r\n--- //tensorflow/core/util/mkl_util.h\t(before formatting)\r\n+++ //tensorflow/core/util/mkl_util.h\t(after formatting)\r\n@@ -728,9 +728,9 @@\r\n     }\r\n     return Status::OK();\r\n   } catch (mkldnn::error& e) {\r\n-    string error_msg = \"Status: \" + std::to_string(e.status) + \", message: \" +\r\n-                       string(e.message) + \", in file \" + string(__FILE__) +\r\n-                       \":\" + std::to_string(__LINE__);\r\n+    string error_msg = \"Status: \" + std::to_string(e.status) +\r\n+                       \", message: \" + string(e.message) + \", in file \" +\r\n+                       string(__FILE__) + \":\" + std::to_string(__LINE__);\r\n     LOG(FATAL) << \"Operation received an exception: \" << error_msg;\r\n   }\r\n }\r\n@@ -1250,8 +1250,8 @@\r\n   } catch (mkldnn::error& e) {\r\n     return Status(error::Code::INTERNAL,\r\n                   tensorflow::strings::StrCat(\r\n-                      \"Failed to create blocked memory descriptor.\", \"Status: \",\r\n-                      e.status, \", message: \", e.message));\r\n+                      \"Failed to create blocked memory descriptor.\",\r\n+                      \"Status: \", e.status, \", message: \", e.message));\r\n   }\r\n #else\r\n   // We have to construct memory descriptor in a C style. This is not at all\r\n```", "@penpornk Really, I ran it for all the files with my last commit. Let me run it again and check.", "@penpornk Not seeing it from my side.\r\nCommand used on my side:\r\n> (fasterrcnnfpn) nammbash@aipg-ra-skx-193:/localdisk/niroop/tensorflow$ /opt/tensorflow/google-code-compliance/clang+llvm-3.9.0-x86_64-fedora23/bin/clang-format -style=Google tensorflow/core/util/mkl_util.h > ../temp_mkl_util.h\r\n> (fasterrcnnfpn) nammbash@aipg-ra-skx-193:/localdisk/niroop/tensorflow$ diff tensorflow/core/util/mkl_util.h ../temp_mkl_util.h\r\n> (fasterrcnnfpn) nammbash@aipg-ra-skx-193:/localdisk/niroop/tensorflow$\r\n", "@nammbash Would you mind formatting it manually for now? Otherwise, I can't merge. (If I make these formatting changes myself, I'll need someone else to review my changes internally, which will not happen tonight.)", "@penpornk looks like your clang is undoing my clang. \r\nI think we need to check both these versions. Anyways, let me try and undo the local changes I did for that file.\r\n", "@penpornk  done. please let m know your clang version so that we can update internally", "@nammbash Will email you the version."]}, {"number": 36495, "title": "Cannot install tensorflow go", "body": "This is a continuation of a similar issue (#35133 ) but the missing package is now different. It appears that the fix has involved substituting one go dependency for another -- however, the new package also cannot be found:\r\n\r\n`go get github.com/tensorflow/tensorflow/tensorflow/go\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\" in any of:\r\n\t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOROOT)\r\n       /Users/admin/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOPATH)`\r\n\r\n", "comments": ["Please follow the Go README:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md\r\n\r\nYou need to generate the proto files. That means you must use `go get -d` to download only and then use `go generate` to BUILD the proto .go files.", "@glarchev, Did you get a chance to check @jhseu's comment. Thanks", "@gadagashwini I did not. I may come back to it, but this solution is not ideal for me (see latest comments in #35133 for more detail).", "@gadagashwini I tried to go through the install process just to see if it works, and it failed on the `./configure` step because it expects bazel version 2.0.0, and I have 2.1.0. So it wants me to downgrade my bazel version, which I shouldn't have to do.", "Hi, I have the same problem when I go through the installation process. I first had this error message: `cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\"`.\r\n\r\nSo I followed the steps to builds things from source instead and had the error about the bazel version (I had bazel 2.2.0 instead of the request 2.0.0), but removing the .bazelversion file solved the problem.\r\n\r\nFollowing the remaining allowed tensorflow for go to be installed correctly, though it was annoying to have to build everything from source.", "@groud @glarchev fyi you can use https://github.com/bazelbuild/bazelisk bazelisk instead of install a specific installation of bazel.  This is a wrapper around bazel which will automatically install and use the correct version of bazel for your project.  To use I renamed bazelisk to bazel and placed  in my PATH.  You can use a particular version of bazel by placing a .bazelversion with the desired version in file in your project root folder.  Still figuring this one out.  Let me know if you get anywhere with this please.", "cd $GOPATH/src/github.com/tensorflow/tensorflow\r\n\r\ngit checkout v2.1.0", "@longkeyy thanks a lot! that works.", "@glarchev \r\nPlease move this issue to closed status if resolved.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36495\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36495\">No</a>\n"]}, {"number": 36494, "title": "Tensorflow not properly installing", "body": "\r\n\r\n**System information**\r\n- Windows 10 Pro Version\t10.0.17763 Build 17763\r\n\r\n- Windows I7 Desktop 16gb memory\r\n- TensorFlow version: 2: (2.1.0)\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: installed with latest pip\r\n\r\n\r\n- CUDA/cuDNN version:CUDA 10.1 ,cudnn64_7.dll\r\n- GPU model and memory: 1080, 16 gb(?)\r\n\r\n**Describe the problem**\r\n\r\nI have had tensorflow 2. running (no problem) and then attempted to get the gpu version going. No luck I have uninstalled the tensorflow-gpu version and reinstalled (pip install tensorflow), the current version.  \r\n\r\n**Installing collected packages: tensorflow\r\nSuccessfully installed tensorflow-2.1.0**\r\n\r\nIt appears tensorflow is successfully installed.\r\n\r\nHere is my python code: **import tensorflow as tf**\r\nThats it!\r\n\r\nPlease advise.  Thanks\r\n\r\n**Traceback :**\r\nC:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\python.exe \"C:/Users/BillS/AppData/Local/Programs/Python/Python36/Tensorflow test.py\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/BillS/AppData/Local/Programs/Python/Python36/Tensorflow test.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@Bstrum36 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\nMake sure you update environment path for cuda.Make sure if there is a library that is in a different location/not installed on your system that cannot be loaded.Also, please follow the instructions from [Tensorflow website](https://www.tensorflow.org/install/gpu#windows_setup).\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you. Thanks!", "Thanks Ravikyram,\r\n\r\nI just installed the the latest Visual Studio from your supplied link. That didn have any effect.\r\nA to CPU  Its an Intel I&-7700K 4.2 ghz.  I have 16 Gb memory on my PC and am running 64 bit Windows.  \r\n\r\nWhat should the environment path be for cuda?  Her is the path.  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA. Should I reset this?\r\n\r\nHow do  tell about AVX instructions? Tensorflow 2. worked before I treid to install the gpu version.\r\n\r\nThanks", "@Bstrum36 \r\n\r\nFor AVX instructions please go through the [link](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX) .To set to set cuda path , please refer this https://www.tensorflow.org/install/gpu#windows_setup . Thanks!", "Thanks again, No luck.\r\n\r\nHere are the paths copied directly for the environment variable tab:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\libnvvp\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\lib64\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\include\r\nC:\\tools\\cuda\\bin\r\n\r\nThe libnvxp entry was already there.\r\nI rebooted and still got exactly the the same errors.\r\nAlso I check my CPU and it is listed as AVX enabled.\r\n\r\nI have been successfully using tensor flow 2.0  the problem occured when trying to get the GP working by unistall ver 2.o and installing the - gpu version.\r\nI gave up on the gpu and uninstalled the version and have reinstalled 2.0 without errors. \r\n\r\nI suppose I could do a system restore and hope for the best but I'd rather not go through that mess.\r\n\r\nAny more ideas on how to proceed??\r\n\r\n\r\n\r\n", "As far as I can tell, you have a CPU with AVX, and CUDA seems to be installed properly.\r\n@chsigg @yifeif could this error be caused by a mismatching cudnn version?\r\nWhich cudnn version was 2.1 built against?", "I solved the problem.  A dll was not in the correct path.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36494\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36494\">No</a>\n"]}, {"number": 36493, "title": "tf.linalg.diag not working as expected", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.1.0-0-ge5bf8de410 2.1.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): 0.27.1\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX1070 8GB\r\n\r\n**Describe the current behavior**\r\nFunction tf.linalg.diag is not working as expected\r\n```\r\nimport tensorflow as tf\r\ntf.linalg.diag([1,1,1,1,1,1,1,1,1,1,1,1,1,1], k=2, num_rows=-1, num_cols=-1, padding_value=9)\r\n```\r\n\r\n<tf.Tensor: shape=(14, 14), dtype=int32, numpy=\r\narray([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\r\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\r\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\r\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\r\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\r\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\r\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\r\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\r\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int32)>\r\n\r\n**Describe the expected behavior**\r\nI would expect something like this:\r\n\r\n<tf.Tensor: shape=(16, 16), dtype=int32, numpy=\r\narray([[9, 9, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\r\n       [9, 9, 9, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\r\n       [9, 9, 9, 9, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\r\n       [9, 9, 9, 9, 9, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\r\n       [9, 9, 9, 9, 9, 9, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9],\r\n       [9, 9, 9, 9, 9, 9, 9, 1, 9, 9, 9, 9, 9, 9, 9, 9],\r\n       [9, 9, 9, 9, 9, 9, 9, 9, 1, 9, 9, 9, 9, 9, 9, 9],\r\n       [9, 9, 9, 9, 9, 9, 9, 9, 9, 1, 9, 9, 9, 9, 9, 9],\r\n       [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 1, 9, 9, 9, 9, 9],\r\n       [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 1, 9, 9, 9, 9],\r\n       [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 1, 9, 9, 9],\r\n       [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 1, 9, 9],\r\n       [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 1, 9],\r\n       [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 1],\r\n       [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\r\n       [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]], dtype=int32)>\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ntf.linalg.diag([1,1,1,1,1,1,1,1,1,1,1,1,1,1], k=2, num_rows=-1, num_cols=-1, padding_value=9)\r\n```\r\n\r\n**Other info / logs**\r\nRunning the below code explicitly produces the right results:\r\n```\r\nfrom tensorflow.python.ops.gen_array_ops import matrix_diag_v2\r\nmatrix_diag_v2([1,1,1,1,1,1,1,1,1,1,1,1,1,1], k=2, num_rows=-1, num_cols=-1, padding_value=9)\r\n```\r\n", "comments": ["@sgabor1, This issue is fixed in latest tf-nightly version. Please take a look at the [gist](https://colab.research.google.com/gist/gadagashwini/5488d4eb9a67b0a8201f1b46e476a699/untitled.ipynb). Thanks!", "It's fixed there indeed, I'll wait for the next release then, thanks!", "Is there a patch for tensorflow 1.x? I am running tensorflow1.15.2."]}, {"number": 36492, "title": "saving_utils.compile_args_from_training_config does not work with multiple loss fuctions", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):TF VERSION:  1.15.0-rc2\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nError when loading a saved keras.model that was compiled with a list of loss functions.\r\n### Stacktrace\r\n```\r\nTraceback (most recent call last):\r\n  File \"reproduce_issue_multiloss.py\", line 27, in <module>\r\n    model = keras.models.load_model(\"model.hdf5\")\r\n  File \"/home/X/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 143, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/X/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 178, in load_model_from_hdf5\r\n    training_config, custom_objects))\r\n  File \"/home/X/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/X/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 336, in compile\r\n    self.loss, self.output_names)\r\n  File \"/home/X/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 1355, in prepare_loss_functions\r\n    loss_functions = nest.map_structure(get_loss_function, loss)\r\n  File \"/home/X/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\", line 536, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/X/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\", line 536, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/X/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 1092, in get_loss_function\r\n    name=loss_fn.__name__,\r\nAttributeError: 'MeanSquaredError' object has no attribute '__name__'\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe model is loaded and compiled without error.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\n\r\nprint(\"TF VERSION: \", tf.__version__)\r\n\r\ninputs = keras.Input(2)\r\nd1 = keras.layers.Dense(4)\r\nd2 = keras.layers.Dense(4)\r\no1 = d1(inputs)\r\no2 = d2(inputs)\r\n\r\n# make a model with multiple outputs\r\nmodel = keras.Model(inputs=inputs, outputs=[o1, o2])\r\n\r\n# compile the model with multiple losses\r\nmodel.compile(loss=[keras.losses.MeanSquaredError(), keras.losses.MeanSquaredError()])\r\n\r\n# try to feed a batch through the model\r\nbatch = np.linspace(0, 9, 10).reshape(5, 2)\r\nouts = model.predict(batch)\r\nprint(outs)\r\n\r\n# save and load the model\r\nmodel.save(\"model.hdf5\")\r\n\r\nmodel = keras.models.load_model(\"model.hdf5\")\r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\nThe error is related to the code in function ` compile_args_from_training_config` from `/python/keras/saving/saving_utils.py`\r\n\r\n```\r\ndef compile_args_from_training_config(training_config, custom_objects=None):\r\n  \"\"\"Return model.compile arguments from training config.\"\"\"\r\n  if custom_objects is None:\r\n    custom_objects = {}\r\n\r\n  optimizer_config = training_config['optimizer_config']\r\n  optimizer = optimizers.deserialize(\r\n      optimizer_config, custom_objects=custom_objects)\r\n\r\n  # Recover loss functions and metrics.\r\n  loss_config = training_config['loss']  # Deserialize loss class.\r\n  if isinstance(loss_config, dict) and 'class_name' in loss_config:\r\n    loss_config = losses.get(loss_config)\r\n  loss = nest.map_structure(\r\n      lambda obj: custom_objects.get(obj, obj), loss_config)\r\n  metrics = nest.map_structure(\r\n      lambda obj: custom_objects.get(obj, obj), training_config['metrics'])\r\n  weighted_metrics = nest.map_structure(\r\n      lambda obj: custom_objects.get(obj, obj),\r\n      training_config.get('weighted_metrics', None))\r\n  sample_weight_mode = training_config['sample_weight_mode']\r\n  loss_weights = training_config['loss_weights']\r\n\r\n  return dict(\r\n      optimizer=optimizer,\r\n      loss=loss,\r\n      metrics=metrics,\r\n      weighted_metrics=weighted_metrics,\r\n      loss_weights=loss_weights,\r\n      sample_weight_mode=sample_weight_mode)\r\n```\r\nSince we have multiple losses `training_config['loss']` is a list: `[{'class_name': 'MeanSquaredError', 'config': {...}}, {'class_name': 'MeanSquaredError', 'config': {...}}]`\r\n\r\nThis case is not covered, therefore the loss configs in the list are not parsed to functions leading to errors further down the call stack.\r\n", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/d6555cb6c92eaa51aa0e6011d1abc244/36492.ipynb). Thanks!", "This is fixed with latest tf nightly version ''2.2.0-dev20200218''. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36492\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36492\">No</a>\n"]}, {"number": 36491, "title": "tf lite model error", "body": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED. Here is a list of operators for which you will need custom implementations: IdentityN.", "comments": ["Hey @prakHr \r\n\r\nI'm attempting to triage issues in order to speed up the debugging process. Can you please give an example of the operators you tried to run to see if this is a bug or an intentional exclusion? If this is not a bug it might be best labeled as type:feature and comp:lite just to get the right eyes on this project!", "@prakHr,\r\nIf you are using the [Python converter](https://www.tensorflow.org/lite/convert/python_api), with the latest version of TF/TFLite (nightly maybe?) could you try adding the following line pre-conversion:\r\n\r\n`converter.experimental_new_converter = True`\r\n\r\nIf that doesn't work, it looks like we don't support the op yet :-(. You could try using [Select TF ops](https://www.tensorflow.org/lite/guide/ops_select) if you are using our C++/Java APIs. Thanks!", "@prakHr, Is this still as issue ? ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36490, "title": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED. Here is a list of operators for which you will need custom implementations: IdentityN.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 36489, "title": "No more fixed link for Cuda Archive download link", "body": "Added a Dictionary mapping between CUDA Version and the appropriate CUDA archive link.", "comments": ["We no longer accept PRs to 1.14 or earlier branches as we no longer do releases for these versions."]}, {"number": 36488, "title": "mkl_dnn as syslib", "body": "With this patch ther mkl-dnn opensource library as\r\na system library. The advantage is that the right\r\noptimizations can the be chosen at install time and\r\nthere are not optimization artifacts in the mkl-dnn\r\nfrom the build system", "comments": ["I don't know much about `systemlibs`. Will defer to @gunan.", "Also tagging @TensorFlow-MKL for information.", "@perfinion could you review this change?\r\n", "Hi @mslacken. Your comments don't make it clear what this PR hopes to achieve. Can you provide more information?\r\n\r\n`the right optimizations can the be chosen at install time`\r\n\r\nWhat does this mean? With the current build process, MKL is built and linked into the tensorflow binary and there are no choices at install time. How would the installation process be different with this patch?\r\n\r\n`there are not optimization artifacts in the mkl-dnn from the build system`\r\n\r\nAgain, what does this mean? What optimization artifacts are you hoping to remove from the build system? If `--config=mkl` is not specified on the command line, AFAIK there are no mkl-dnn artifacts in the build system. \r\n\r\nAlso, without a `--config=mkl` test in the Review Report below, do you have test results that show this change doesn't break the community MKL-DNN build?", "Hello @claynerobison,\r\n> Hi @mslacken. Your comments don't make it clear what this PR hopes to achieve. Can you provide more information?\r\nI want to package tensorflow as an systempackage (rpm) to the openSUSE project. That means that I want to unbundle as many package, which are present in the distribution, as possible.\r\n> \r\n> `the right optimizations can the be chosen at install time`\r\n> \r\n> What does this mean? With the current build process, MKL is built and linked into the tensorflow binary and there are no choices at install time. How would the installation process be different with this patch?\r\nWith MKL as a system library and tensorflow as a rpm/deb package, zypper/apt could decide at installation time, what optimized mkl-dnn it should used on that specific target machine.\r\n> \r\n> `there are not optimization artifacts in the mkl-dnn from the build system`\r\n> \r\n> Again, what does this mean? What optimization artifacts are you hoping to remove from the build system? If `--config=mkl` is not specified on the command line, AFAIK there are no mkl-dnn artifacts in the build system.\r\n> \r\nIf mkl-dnn or any other libraries detects vector extensions (like AVX512) during build, it enables or disables this optimization depending on the architecture on the build host. This is then a build artifact.\r\n\r\n", "@mslacken mkl-dnn includes AVX512 instructions at `runtime` if the host supports it, not build-time. You can build TF with --march=broadwell and you'll still get AVX512 instructions from mkl-dnn if the resulting binary is run on a skylake or later system.", ".@claynerobison \r\nThanks for this information. Still building mkl as system lib is sensible as one may have want control over the used CPU engine and other build options. Getting information about the used build options and also some control is possible by reading through different workspace and BUILD files, but this is not convenient nor as good documented as the build options for mkl.", "@mslacken Could you please resolve the conflicts? Thanks!", "I don't see all of @perfinion's comments being addressed. Also, as this is tested against openSUSE while most of the users of TensorFlow are on Ubuntu (which has a totally different layout of the libs, based on the comments), I don't think we can safely pull this in", "Sorry I do not have the time to write a rule which includes ubuntu and openSUSE headers as this quite complicated with bazel. So I am closing this request, but thanks for your time and comments.\r\nIf you are interested in how this is used you can have a look at \r\nhttps://build.opensuse.org/package/show/science:machinelearning/tensorflow2", "I'd still very much like to see this merged as currently, it's quite weird that you can use mkl as a system dependency but not mkl-dnn."]}, {"number": 36487, "title": "fixed mkl sgemm call", "body": "gemm called used *float instead of Float\r\nsee: https://intel.github.io/mkl-dnn/group__dnnl__api__blas.html", "comments": ["@mslacken Could you please check failed build errors? Thanks!", "@gbaned Is it possible that the Ubuntu CPU tests has the proprietary Intel Math Kernel library installed. There the calls for dnnl_sgemm may differ.", "@alextp Can you please assist with the above query from @mslacken? Thanks!", "@penpornk do you know the answer?", "DNNL (MKL-DNN v1.x) is only used with TF-MKL build (--config=mkl). Stock TensorFlow is still using MKL-DNN v0.x, whose sgemm interface is incompatible with v1.x. There is nothing to fix here. (I'll fix them when we update stock TF to v1.x later after the 2.2 branch cut.)", "More details:\r\n\r\n- Stock TensorFlow currently uses MKL-DNN [v0.21.3](https://github.com/tensorflow/tensorflow/blob/dd662a9228b0f94ed2527f4117f2bef62e0aad00/tensorflow/workspace.bzl#L171) (`@mkl_dnn`).\r\n- [eigen_contraction_kernel](https://github.com/tensorflow/tensorflow/blob/dd662a9228b0f94ed2527f4117f2bef62e0aad00/tensorflow/core/kernels/BUILD#L824) depends on `@mkl_dnn`.\r\n- [conv_grad_input_ops](https://github.com/tensorflow/tensorflow/blob/dd662a9228b0f94ed2527f4117f2bef62e0aad00/tensorflow/core/kernels/BUILD#L4516) depends on `eigen_contraction_kernel`.\r\n- [Here](https://github.com/intel/mkl-dnn/blob/4f5d024fc84cff5995b440eeaee3fda6385dd970/include/mkldnn.h#L1797-L1801) is the sgemm interface in v0.21.3 (`const float* alpha` and `const float* beta`). \r\n\r\nSo the existing `sgemm` calls are correct and shouldn't be fixed.", "@penpornk Thanks for you clarification. I encountered this issue as I am using the system mkl-dnn which is at version 1.1.3.\r\nOne good news is that this calls seem the only incompatibility so far.", "@mslacken Got it. Thank you for the explanation! Stock TensorFlow only uses two routines from MKL-DNN: [mkldnn_sgemm](https://github.com/tensorflow/tensorflow/blob/a898220ce412b3ede459b7ed6c0bd0660aacb97a/tensorflow/core/kernels/eigen_contraction_kernel.h#L166-L168) and [mkldnn_s8u8s32](https://github.com/tensorflow/tensorflow/blob/a898220ce412b3ede459b7ed6c0bd0660aacb97a/tensorflow/core/kernels/eigen_contraction_kernel.h#L231-L238). So there can't be many incompatibilities to begin with.\r\n\r\nWe plan to also switch to MKL-DNN v1.2 for stock TF in the near future. If you'd like to use v1.x before that, please feel free to apply the fix locally in the meanwhile. Note that another difference between v0.x and v1.x is that v0.x assumes matrices are in column major order and v1.x assumes row-major. Please account for that too if you do make the changes. \r\n\r\nHowever, I'd suggest staying with v0.21.3 to save your time, because v1.2 doesn't have any noticeable performance improvements from v0.21.3 for `sgemm`. I'll ping this thread once stock TF also switches to v1.2.\r\n", "Hi, i'm compiling this but with --config=cuda. I was under the impression --config=mkl would use this, but if --config=cuda would not. ", "@muziker Yes, only --config=mkl would use MKL-DNN v1.x. Vanilla TF and --config=cuda (and all other configurations that run on non-mobile devices) are still using MKL-DNN v0.21.3."]}, {"number": 36486, "title": "import tensorflow (v2.1.0) fails", "body": "**System information**\r\n- Windows 10\r\n- TensorFlow installed from (pip install tensorflow):\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.1\r\n- Installed using virtualenv:\r\n\r\n**Describe the problem**\r\n```import tensorflow as tf``` fails\r\n\r\n```\r\n(tf2) D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2>python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\nTraceback (most recent call last):\r\n  File \"D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\igor-z\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\igor-z\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\igor-z\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\@Temp\\@Issues\\2020-02-04-tf\\tf2\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\igor-z\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\igor-z\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\ntf (v1.15.0) in other virtual environment shows next:\r\n```\r\n(tf1) D:\\@Temp\\@Issues\\2020-02-04-tf\\tf1>python\r\nPython 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-02-05 15:31:28.406892: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\r\n2020-02-05 15:31:28.411004: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>> tf.__version__\r\n'1.15.0'\r\n>>>\r\n```\r\n\r\n", "comments": ["@constructor-igor, Suspecting that you have installed tensorflow-gpu 2.1 version. Please check and install `pip install tensorflow==2.1`. Thanks!", "@gadagashwini \r\nI checked and found next installed \"tensorflow\" packages:\r\n```\r\nsix                  1.14.0\r\ntensorboard          2.1.0\r\ntensorflow           2.1.0\r\ntensorflow-estimator 2.1.0\r\ntermcolor            1.1.0\r\n```", "@constructor-igor,\r\nOpen ...\\Lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd use Dependency Walker, it will show you the \r\nDLL dependency tree, you will find which DLL cause the problem. You can then install the needed version, \r\nor add some path to system variable.\r\nDependency Walker link : http://www.dependencywalker.com/.\r\nThanks!", "@gadagashwini \r\nfolder ...\\Lib\\site-packages\\tensorflow doesn't contain any .pyd file.\r\n\r\n![image](https://user-images.githubusercontent.com/1849690/74111438-14d9fb00-4b9d-11ea-8267-d63137f134ba.png)\r\n\r\nI found .pyd files in ...\\Lib\\site-packages\\tensorflow_core\\python folder only.\r\n\r\n![image](https://user-images.githubusercontent.com/1849690/74111460-510d5b80-4b9d-11ea-8bfb-2b3d7666913e.png)\r\n\r\nDependency Walker shows next (see below).\r\nPlease, could you explain what should be installed for TF2?\r\n\r\n![image](https://user-images.githubusercontent.com/1849690/74111478-6bdfd000-4b9d-11ea-96df-1b05f47d7244.png)\r\n", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36486\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36486\">No</a>\n", "**solution**:\r\ndownload and install **Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019** package from \r\nhttps://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\r\n\r\nWindows 10 x64 requires **vc_redist.x64.exe**.\r\n"]}, {"number": 36485, "title": "Failed to load the native TensorFlow runtime.", "body": "**System information**\r\n- OS :windows 10 x64\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version:2.1\r\n- Python version:3.6.5\r\n- Installed using virtualenv? pip? conda?:installed in python3.6.5 anaconda virtualenv using pip\r\n- CUDA/cuDNN version:cuda10.1 with cudnn 7.6.5.32\r\n- GPU model and memory:nivida gtx 1060\r\n\r\nI have installed cuda and cudnn and add them to PATH,but when I try to import tensorflow \r\nI get the following error\r\n\r\n(kr) C:\\WINDOWS\\system32>python\r\nPython 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\anaconda\\envs\\kr\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nwould somebody kindly offer some help?", "comments": ["@AIChuY \r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\nMake sure you update environment path for cuda.Make sure if there is a library that is in a different location/not installed on your system that cannot be loaded.Also, please follow the instructions from [Tensorflow website]( https://www.tensorflow.org/install/gpu#windows_setup).\r\nPlease, check Your CPU/Python is on 32 bits.Please, refer #36167 and see if it helps you. Thanks!", "> @AIChuY\r\n> What is make/model of your cpu?\r\n> I suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\n> Make sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n> Make sure you update environment path for cuda.Make sure if there is a library that is in a different location/not installed on your system that cannot be loaded.Also, please follow the instructions from [Tensorflow website](https://www.tensorflow.org/install/gpu#windows_setup).\r\n> Please, check Your CPU/Python is on 32 bits.Please, refer #36167 and see if it helps you. Thanks!\r\n\r\nThanks for your helping\uff01I followed your instructions and now can successfully import tensorflow and I tried following code\r\n\r\nsess = tensorflow.compat.v1.Session(config=tensorflow.compat.v1.ConfigProto(log_device_placement=True))\r\n2020-02-06 17:00:49.540537: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-02-06 17:00:49.565232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-02-06 17:00:49.625527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1060 computeCapability: 6.1\r\ncoreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2020-02-06 17:00:49.629859: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-02-06 17:00:49.690104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-02-06 17:00:49.723887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-02-06 17:00:49.749089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-02-06 17:00:49.795990: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-02-06 17:00:49.833084: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-02-06 17:00:49.907455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-02-06 17:00:49.910498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-02-06 17:01:02.023834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-06 17:01:02.026382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-02-06 17:01:02.027971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-02-06 17:01:02.197690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4702 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\r\n2020-02-06 17:01:02.294332: I tensorflow/core/common_runtime/direct_session.cc:358] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\r\n\r\nIs everythin ok now\uff1f", "@AIChuY \r\nIt looks like you have successfully imported Tensorflow. Can you please confirm can we close this issue.Thanks!", "Closing the issue since its resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36485\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36485\">No</a>\n"]}, {"number": 36484, "title": "Gradient in keras model with respect to layer outputs is `None` when layer is a Model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos-release-7-6.1810.2.el7.centos.x86_64\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n\r\n- TensorFlow installed from (source or binary): Binary\r\n\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI want to obtain the gradients of a network's output with respect to an intermediate layer which is itself a keras `Model`. This returns `None`. \r\n\r\n**Describe the expected behavior**\r\nIt should return something other than `None`.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ninput = tf.keras.layers.Input(shape=(28,28))\r\n\r\nx = tf.keras.layers.Flatten()(input)\r\nx = tf.keras.layers.Dense(128)(x)\r\nx = tf.keras.layers.ReLU()(x)\r\nx = tf.keras.layers.Dense(10)(x)\r\n\r\nsmall_model = tf.keras.models.Model(inputs=[input], outputs=[x])\r\n\r\ny = small_model(input)\r\n\r\ny = tf.keras.layers.Dense(20)(y)\r\ny = tf.keras.layers.Dense(30)(y)\r\ny = tf.keras.layers.Dense(10)(y)\r\n\r\nbig_model = tf.keras.models.Model(inputs=[small_model.input], outputs=[y])\r\n\r\next_model = tf.keras.models.Model(inputs=[small_model.input], outputs=[big_model.output,  big_model.layers[1].output])\r\n\r\nimages = tf.random.uniform(shape=(32,28,28))\r\nlabels = tf.zeros(shape=(32,))\r\n\r\nwith tf.GradientTape() as tape:    \r\n    predictions, layer = ext_model(images)\r\n\r\nprint(\"grad = \", tape.gradient(predictions, layer))\r\n```\r\n\r\n**Other info / logs**\r\nChanging to ```ext_model = tf.keras.models.Model(inputs=[small_model.input], outputs=[big_model.output,  big_model.layers[2].output])``` outputs a gradient different from `None`, which indicates that the problem occurs only when the layer in question is a `Model` itself. But this is precisely the typical use case scenario, where one needs gradients with respect to the outputs of an intermediate layer of a pre-built `tf.keras.applications` model. \r\n\r\nThis issue is related (but not identical) to https://github.com/tensorflow/tensorflow/issues/33478 which doesn't seem to have been adequately addressed. ", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/a5327043e792ac7a79e5b78228c68977/36484.ipynb). Thanks!", "You can see the problem more clearly when you visualize the graph on TensorBoard, here's a [gist](https://colab.research.google.com/gist/n2cholas/5dbeaafc9c4d2bbce4e4185bccfe70f4/36484.ipynb). Specifically expand the `ext_model` node. In the `big_model.layers[1].output` case, the `predictions` and `layer` are in two different branches of the graph, so there is no gradient of `prediction` with respect to `layer`. In the `big_model.layers[2].output` case, there is a path between the two nodes, so there is a gradient. This has to do with how `tf.keras` copies the sub-model's ops when you create another model with it.\r\n\r\nI'm not sure how to resolve this issue but hopefully this sheds some light on it.", "@ksabr Is this still an issue? Please close the issue If this was already resolved. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 36483, "title": "Trying to install pytorch-neat - No such file or directory setup.py", "body": "I've attempted to install pytorch-neat from https://github.com/uber-research/PyTorch-NEAT.git.\r\nI cloned the repo which is now in C:\\Users\\username\\AppData\\Local\\Continuum\\anaconda3\\pkgs\\PyTorch-NEAT and the following command \r\npip install git+https://github.com/uber-research/PyTorch-NEAT.git\r\nwhich is giving the following error\r\n ERROR: Command errored out with exit status 1:\r\n     command: 'c:\\users\\townsond\\appdata\\local\\continuum\\anaconda3\\envs\\petchem_py36\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\townsond\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-0krj5_3e\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\townsond\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-0krj5_3e\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\townsond\\AppData\\Local\\Temp\\pip-req-build-0krj5_3e\\pip-egg-info'\r\n         cwd: C:\\Users\\townsond\\AppData\\Local\\Temp\\pip-req-build-0krj5_3e\\\r\n    Complete output (5 lines):\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"c:\\users\\townsond\\appdata\\local\\continuum\\anaconda3\\envs\\petchem_py36\\lib\\tokenize.py\", line 452, in open\r\n        buffer = _builtin_open(filename, 'rb')\r\n    FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\townsond\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-0krj5_3e\\\\setup.py'\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\r\n\r\nI can see pytorch-neat does not have a setup.py file and I'm unsure on how to proceed from here.\r\nI'm using Python 3.6 and have upgraded both setuptools and pip, and tried pip3 install also.\r\n\r\nAny help appreciated!", "comments": ["FWIW for now I can put the pytorch_neat file into my source and use it that way, but if there is a way of installing it I would like to know", "@DemiTownson26,\r\nLooks like you are facing issues with PyTorch. Could you please confirm if this issue is related to TensorFlow? Thanks!", "Any updates regarding this issue? Thanks!"]}, {"number": 36482, "title": "Print sys Info at the start of the Windows console", "body": "Collect and display system information when starting windows console.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36482) for more info**.\n\n<!-- need_sender_cla -->", "@Mohamed-94 thank you for your contribution, please sign CLA.", "@googlebot I signed it!\r\n\r\n> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n>  **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n>  **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36482) for more info**.\r\n\r\n@googlebot I signed it!\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36482) for more info**.\n\n<!-- ok -->", "@Mohamed-94 Can you please check reviewer comments and keep us posted. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 36481, "title": "Move and update CMSIS-NN readme file", "body": "kernels/cmsis-nn is a better location for the readme I think. I also updated the readme to take in to account some recent changes in the repo.", "comments": []}, {"number": 36480, "title": "Error when executing model.predict or model.evaluate even though Loading is successful", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Issue is not specific to OS. Could be reproduced in Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: Colab Version\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**: I have Saved a Model using the command, `cnn_model.save('CNN_MNIST_Sequential.h5')`. Now, when I loaded that Model using `tf.keras.models.load_model('CNN_MNIST_Sequential.h5')`, it is loading properly and is showing the Trained Parameters when I ran `model.summary()`. But when I ran the command, `cnn_model.evaluate(X_test_reshaped,  y_test, verbose=2)` or `cnn_model.predict(X_test_reshaped).shape`, it is resulting in the below error.\r\n\r\n> ValueError: When using data tensors as input to a model, you should specify the `steps` argument.\r\n\r\nReason for the error (not quite sure though) might be that the Model which is Saved (CNN_MNIST_Sequential.h5), was executed with `steps_per_epoch` and `validation_steps` as shown below,\r\n\r\n```\r\nhistory = cnn_model.fit(x = X_train_reshaped,\r\n                        y = y_train,\r\n                        batch_size = 512,\r\n                        epochs = 5,\r\n                        verbose = 1, validation_data = (X_test_reshaped, y_test),\r\n                        validation_steps = 10, steps_per_epoch=steps_per_epoch)\r\n```\r\n\r\n**Describe the expected behavior**: Commands, `model.evaluate` and `model.predict` should run without error, as the Model is Loading Succesfully\r\n\r\n**Code to reproduce the issue**: Please find Gist of my [Golab Colab](https://colab.sandbox.google.com/gist/rmothukuru/8b1639919d9e9eee2c8e95978fea8fc4/cnn_mnist_sequential_restored.ipynb).", "comments": ["Please find the Saved Model, CNN_MNIST_Sequential.h5, attached.\r\n\r\n[CNN_MNIST_Sequential.h5.zip](https://github.com/tensorflow/tensorflow/files/4157457/CNN_MNIST_Sequential.h5.zip)\r\n", "@rakeshmothukuru1, Please change the model.evaluate as\r\n```\r\n# Evaluate the restored model\r\nloss, acc = cnn_model.evaluate(X_test_reshaped,  y_test, verbose=2, steps=1)\r\nprint('Restored model, accuracy: {:5.2f}%'.format(100*acc))\r\n```\r\nOr make change in model.fit as steps_per_epoch =None\r\nPlease find the [gist](https://colab.sandbox.google.com/gist/gadagashwini/22f8fdb707a98f871bf48fe916f68332/cnn_mnist_sequential_restored.ipynb). Thanks!", "@gadagashwini,\r\nThank you for the workaround but it works only for `model.evaluate` and `model.predict` is still resulting in error. Is it like, we can't use `model.predict` on the Restored Model? ", "@rakeshmothukuru1, \r\n/model.fit  /model.evaluate and /model.predict will be provided with same input data. If any mismatch in the input data, ValueError will occur. Since in the `model.fit` verbose is set to 1, `model.predict` also expects verbose otherwise verbose default will be `None` in all the cases. \r\nChange the model.predict as \r\n`print(cnn_model.predict(X_test_reshaped, verbose=1, steps=1)) `\r\n\r\nPlease find the attached [gist](https://colab.research.google.com/gist/gadagashwini/005eb329625355452097978aa93a9513/untitled378.ipynb). \r\nFor more read [this](https://keras.io/models/sequential/) doc. Thanks!", "@rakeshmothukuru1, Are you happy to close this issue since its resolved. Thanks!", "Yes @gadagashwini . Thanks you."]}, {"number": 36479, "title": "Fix formatting of ops_version.md", "body": "This is alternative of PR #36145 which was closed because of CLA issues. @mihaimaruseac, You approved that PR, please approve this one so that it can be merged.", "comments": ["As it is identical to that one, we might need @settle to also approve of the 100% identical change.", "> As it is identical to that one, we might need @settle to also approve of the 100% identical change.\r\n\r\nAgree with you @mihaimaruseac . @settle, Please approve this PR if you think that the changes are identical to your previous PR #36145.", "@ashutosh1919 Could you update description of the commit to \"Fix formatting of ops_version.md\" ?", "> I guess you shouldn't include this merge commit.\r\n> [5e11aa9](https://github.com/tensorflow/tensorflow/commit/5e11aa9aadc38650670fb57c3b6961182e66f9c6)\r\n\r\nI have just synchronized it with the current master tf. How can it produce any kind of error? "]}, {"number": 36478, "title": "Upgrading the AWS SDK version", "body": "Also added some dependencies that the newer version requires", "comments": ["Actually @MihailSalnikov , I just requested review again, but please hold on before review. I'll comment when done.\r\n\r\nGoing to go through my bazel definitions once again. As although it builds fine on my local machine, it seems to fail on CI. ", "@mihaimaruseac I just wanted to say thanks a lot for your help with my multiple PRs. \r\nI think I've resolved the build issue here. Could you help give CI another shot at this PR?", "@rahul003 Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned Just pushed a commit to fix the license check issue. Please restart build", "The build failure doesn't seem related to my changes. Am I missing something\r\n\r\n```\r\nERROR: T:/tmp/bigvaudl/external/llvm-project/llvm/BUILD:3716:1: C++ compilation of rule '@llvm-project//llvm:support' failed (Exit 2)\r\nexternal/llvm-project/llvm/lib/Support/CrashRecoveryContext.cpp(218): error C2712: Cannot use __try in functions that require object unwinding\r\n```", "> The build failure doesn't seem related to my changes. Am I missing something\r\n> \r\n> ```\r\n> ERROR: T:/tmp/bigvaudl/external/llvm-project/llvm/BUILD:3716:1: C++ compilation of rule '@llvm-project//llvm:support' failed (Exit 2)\r\n> external/llvm-project/llvm/lib/Support/CrashRecoveryContext.cpp(218): error C2712: Cannot use __try in functions that require object unwinding\r\n> ```\r\n\r\nWe're broken by a LLVM integration as part of MLIR. It will have to wait until build turns back green (hopefully tomorrow)", "Can we retry now if the build has passed?\r\n"]}, {"number": 36477, "title": "[tf2.1] model saving error for model with Batch Normalization layer and under MirroredStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Redhat 8.1 (in Docker)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TF2.1.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: V100, 32GB\r\n\r\n**Describe the current behavior**\r\nIn TF2.1.0, if I build and compile a model with Batch Normalization layer under MirroredStrategy scope, model saving will throw some error like \r\n```\r\nKeyError: \"Failed to add concrete function b'__inference_sequential_layer_call_and_return_conditional_losses_869' to object based saved model as it captures tensor tf.Tensor(<unprintable>, shape=(), dtype=resource) which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\"\r\n```\r\nI have tested with tf2.0.0 and this seems to be fine .\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef build_and_compile_model():\r\n    \r\n    input = tf.keras.Input((20,))\r\n    x = tf.keras.layers.BatchNormalization()(input)\r\n    y = tf.keras.layers.Dense(2)(x)\r\n    \r\n    model = tf.keras.Model(inputs=input, outputs=y)\r\n    \r\n    model.compile(\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n        metrics=['accuracy'])\r\n    \r\n    return model\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    model = build_and_compile_model()\r\nmodel.save('test', save_format='tf')\r\n```\r\n\r\n**Other info / logs**\r\nTraceback logs as follows:\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n/tmp/site-packages/tensorflow_core/python/saved_model/function_serialization.py in serialize_concrete_function(concrete_function, node_ids, coder)\r\n     53     for capture in concrete_function.captured_inputs:\r\n---> 54       bound_inputs.append(node_ids[capture])\r\n     55   except KeyError:\r\n\r\n/tmp/site-packages/tensorflow_core/python/util/object_identity.py in __getitem__(self, key)\r\n    131   def __getitem__(self, key):\r\n--> 132     return self._storage[self._wrap_key(key)]\r\n    133 \r\n\r\nKeyError: <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-13-a7ec70c505d2> in <module>()\r\n----> 1 model.save('test10', save_format='tf')\r\n\r\n/tmp/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n   1006     \"\"\"\r\n   1007     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n-> 1008                     signatures, options)\r\n   1009 \r\n   1010   def save_weights(self, filepath, overwrite=True, save_format=None):\r\n\r\n/tmp/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    113   else:\r\n    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n--> 115                           signatures, options)\r\n    116 \r\n    117 \r\n\r\n/tmp/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n     76     # we use the default replica context here.\r\n     77     with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access\r\n---> 78       save_lib.save(model, filepath, signatures, options)\r\n     79 \r\n     80   if not include_optimizer:\r\n\r\n/tmp/site-packages/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    921       compat.as_str(constants.SAVED_MODEL_FILENAME_PB))\r\n    922   object_graph_proto = _serialize_object_graph(\r\n--> 923       saveable_view, asset_info.asset_index)\r\n    924   meta_graph_def.object_graph_def.CopyFrom(object_graph_proto)\r\n    925 \r\n\r\n/tmp/site-packages/tensorflow_core/python/saved_model/save.py in _serialize_object_graph(saveable_view, asset_file_def_index)\r\n    645   for concrete_function in saveable_view.concrete_functions:\r\n    646     serialized = function_serialization.serialize_concrete_function(\r\n--> 647         concrete_function, saveable_view.captured_tensor_node_ids, coder)\r\n    648     if serialized is not None:\r\n    649       proto.concrete_functions[concrete_function.name].CopyFrom(\r\n\r\n/tmp/site-packages/tensorflow_core/python/saved_model/function_serialization.py in serialize_concrete_function(concrete_function, node_ids, coder)\r\n     61         \"trackable object \"\r\n     62         \"(see SaveTest.test_captures_unreachable_variable).\"\r\n---> 63         % (concrete_function.name, capture))\r\n     64   concrete_function_proto = saved_object_graph_pb2.SavedConcreteFunction()\r\n     65   structured_outputs = func_graph_module.convert_structure_to_signature(\r\n\r\nKeyError: \"Failed to add concrete function b'__inference_sequential_1_layer_call_and_return_conditional_losses_1937' to object based saved model as it captures tensor tf.Tensor(<unprintable>, shape=(), dtype=resource) which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\"\r\n```\r\nThanks so much for your time!", "comments": ["Thanks for reporting the issue and providing a small reproducible example. I checked that the issue is a regression in 2.1, but is already fixed in nightly.", "I've tested the nightly docker image, but I still get the error: tensorflow/tensorflow:nightly-gpu-py3\r\n\r\n```\r\nKeyError: \"Failed to add concrete function b'__inference_model_layer_call_and_return_conditional_losses_2786225' \r\nto object based saved model as it captures tensor tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nwhich is unsupported or not reachable from root.  One reason could be that a stateful object or a\r\nvariable that the function depends on is not assigned to an attribute of the serialized trackable \r\nobject (see SaveTest.test_captures_unreachable_variable).\"\r\n\r\n```", "I got the same as well.\r\nI can model.save() using one gpu; but error comes under distributed training with mirrored stategy", "Yes, I have verified.\r\nTF2.0.0 could save the model\r\nTF2.1.0 has an error like the post above", "Hi, Yufeng\r\nPlease use TF2.2, it's a nightly version.", "> Hi, Yufeng\r\n> Please use TF2.2, it's a nightly version.\r\n\r\nYeah. I've tested that w/ the nightly version on colab and it is working. Thanks. @JiayuanSternLi \r\nBut I'm not sure why @jonyvp still got the error w/ the nightly version.", "I can confirm it works with the latest nightly version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36477\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36477\">No</a>\n"]}, {"number": 36476, "title": "tensorflow==1.15.2 package on PyPI cannot find GPU devices", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (in Docker)\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 1.15.2\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GTX 1080 Ti 12 GB\r\n\r\n**Describe the problem**\r\n\r\nWhen using the tensorflow package at version 1.15.2, Tensorflow does not find my GPU. The tensorflow-gpu package works, but I was under the impression that from 1.15.0 onward, the tensorflow package would be capable of running with or without a GPU.\r\n\r\nLooking at PyPI, I notice there isn't a tensorflow-cpu package released for 1.15.2. Has the project switched back to using the old model where only tensorflow-gpu has GPU support?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. `pip3 install tensorflow==1.15.2`\r\n2. Run the following script:\r\n    ```python3\r\n    from tensorflow.python.client import device_lib\r\n    print(device_lib.list_local_devices())\r\n    ```\r\n3. Observe that CPU devices are found, but GPU devices are not\r\n\r\n**Any other info / logs**\r\n\r\nI've been testing this in a Docker container that uses Nvidia's Cuda images. I have nvidia-docker2 installed and can confirm that the tensorflow 1.15.0 package is capable of finding the GPU device.\r\n\r\n```Dockerfile\r\nFROM nvidia/cuda:10.0-cudnn7-runtime-ubuntu18.04\r\n\r\nRUN apt-get update && apt-get install -y python3-pip\r\nRUN pip3 install --upgrade pip\r\nRUN pip3 install tensorflow==1.15.2\r\n\r\nENTRYPOINT [\"python3\", \"-c\", \"from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())\"]\r\n```\r\n\r\nI get the following logs as a result:\r\n\r\n```\r\n2020-02-04 22:29:15.945515: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-02-04 22:29:15.971253: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3393255000 Hz\r\n2020-02-04 22:29:15.972730: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x45f0770 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-02-04 22:29:15.972756: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 14247052760952852178\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 3258632196886034744\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n]\r\n```\r\n", "comments": ["Just found the same issue. ", "@velovix, Tensorflow 1.15.2 has both GPU and CPU support. Please take a look at [gist](https://colab.research.google.com/gist/gadagashwini/9f44beac087511b14e36a82ce7de2920/untitled374.ipynb). To see available GPU devices, use \r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n``` \r\nThanks!", "@gadagashwini \r\n\r\nUsing the gist you linked, if I restart the runtime and run all steps, I get no GPUs detected.\r\n\r\n```\r\nNum GPUs Available:  0\r\n```\r\n\r\nI don't know a lot about Colab, so I'm not sure why that would be.\r\n\r\nWhen testing locally using `list_physical_devices` instead of `list_local_devices`, I'm still not able to detect GPUs with 1.15.2.\r\n\r\nI still have a suspicion that tensorflow==1.15.2 was not built with GPU support though. Taking a look at the wheel file sizes seem suspect to me:\r\n\r\n| Wheel Name                                                | File Size |\r\n|-----------------------------------------------------------|-----------|\r\n| tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl     | 394M      |\r\n| tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl | 393M      |\r\n| tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl     | 106M      |\r\n| tensorflow_gpu-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl | 392M      |\r\n\r\nIf we compare file sizes between tensorflow==1.15.0 and tensorflow-gpu==1.15.0, they're very similar. However, you can see that tensorflow==1.15.2 is significantly smaller than tensorflow-gpu==1.15.2. This is admittedly unscientific, but does suggest to me that something is going on.", "@velovix, On colab we need to change the runtime type as GPU before using Tensorflow-gpu 1.15.\r\nSee the [tensorflow release](https://github.com/tensorflow/tensorflow/releases) note for 1.15.2, there is no change in the installation package. It is same as Tf 1.15. Please take a look at screenshot\r\n![Screenshot from 2020-02-06 12-26-10](https://user-images.githubusercontent.com/48476109/73913212-74ee4a00-48dc-11ea-8643-64d1211f0c2e.png)\r\nThanks\r\n\r\n", "Duplicate of #36347 \r\n\r\nSince 1.15 single package was done via a quick workaround, we were not able to replicate it for the patch release and we reverted to dual pips. Please see duplicate issue for more details.", "Thank you @mihaimaruseac for the pointer! We'll just use tensorflow-gpu instead going forward.\r\n\r\nWould it be reasonable to add this to the patch notes for the 1.15.2 release on Github? That's where I looked for breaking changes and didn't see anything about this.", "That makes sense. I have edited the release notes."]}, {"number": 36475, "title": "tf.keras.utils.get_file issue", "body": "I am using Google Colab with tensorflow and I was trying to download files from my google cloud storage. I was able to download the files to Colab but somehow they are corrupted because I can not read or view the files. Even if I after I downloaded them from Colab to my own PC, I still couldn't open/read the files. But if download them directly from Cloud Storage to my PC, they are fine. The files I am dealing with are .tif files (satellite images). Here' the code to download the files: \r\n`import tensorflow as tf\r\ntf.keras.utils.get_file(\"/root/image.tif\",\"https://storage.cloud.google.com/project/image.tif\")`\r\n", "comments": ["I tried uploading tif image to colab locally,then i am able to read the file.However by following the above commands I am able to download the files from google cloud storage but cannot read/view tif files.Thanks!", "@wenjieji86 Tensorflow does not support (decode) .tif images. Please take a look at the code [here](https://github.com/tensorflow/tensorflow/blob/cf7fcf164c9846502b21cebb7d3d5ccf6cb626e8/tensorflow/core/kernels/decode_image_op.cc#L36)\r\n\r\nIf you want to use TIFF images, you could use a library like PIL or Pillow which can read TIFF images and convert them into a numpy array to feed into TensorFlow.\r\n\r\nPlease take a look at this issue [here](https://stackoverflow.com/questions/44706220/retraining-inception-with-tiff-images) for alternate solutions. Thanks!\r\n", "@gowthamkpr  OK, thanks."]}, {"number": 36474, "title": "TFLite iOS benchmark app doesn't produce consistent result while using GPU delegate", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone XR (tested on iOS 13.3, and 12.3.1), iPhone Xs (13.1.2)\r\n- TensorFlow installed from (source or binary): installed from source.\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source): 4.2.1\r\n\r\n\r\n**Describe the current behavior**\r\nTFLite iOS benchmark app doesn't produce consistent result while using GPU delegate.\r\n\r\nTesting with Mobilenet_1.0_224(float) model on iOS benchmark app, while using the default parameter, I'm able to obtain similar performance with the benchmark provided [here:](https://www.tensorflow.org/lite/performance/benchmarks) (around 14.x ms). \r\n\r\nHowever, while adding the GPU delegate param according to the instruction (`\"use_gpu\" : \"1\"` and `\"gpu_wait_type\" : \"aggressive\"` options were also added to `benchmark_params.json`), the benchmark app still reports almost the same performance, instead of the 4x faster performance shown in the benchmark.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nSimilar performance for GPU delegate with the results provided in the iOS benchmark page.\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/tensorflow/tensorflow/blob/9901f967b11763726ae380273a24ee9b4fdae7f0/tensorflow/lite/tools/benchmark/ios/TFLiteBenchmark/TFLiteBenchmark/benchmark_data/benchmark_params.json\r\nwith \r\n`\"use_gpu\" : \"1\",\r\n    \"gpu_wait_type\" : \"aggressive\"` added.\r\n\r\n\r\n**Other info / logs**\r\n\r\n```\r\nMin num runs: [20]\r\nMin runs duration (seconds): [1]\r\nMax runs duration (seconds): [150]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [2]\r\nBenchmark name: [mobile_net_benchmark]\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/private/var/containers/Bundle/Application/93C7DE45-ADBA-4E5B-B64B-3F789A357080/TFLiteBenchmark.app/mobilenet_v1_1.0_224.tflite]\r\nInput layers: [input]\r\nInput shapes: [1,224,224,3]\r\nInput value ranges: []\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [0]\r\nMax profiling buffer entries: [1024]\r\nLoaded model /private/var/containers/Bundle/Application/93C7DE45-ADBA-4E5B-B64B-3F789A357080/TFLiteBenchmark.app/mobilenet_v1_1.0_224.tflite\r\n2020-02-04 16:04:27.856557-0500 TFLiteBenchmark[1156:379854] Initialized TensorFlow Lite runtime.\r\nThe input model file size (MB): 16.9008\r\nInitialized session in 15.791ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=31 first=79523 curr=14493 min=14015 max=79523 avg=16387.8 std=11545\r\n\r\nRunning benchmark for at least 20 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=68 first=14721 curr=14936 min=14055 max=15838 avg=14710.9 std=308\r\n\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=0 overall=0\r\n```\r\n\r\n", "comments": ["Thanks for flagging, and providing your console logs.\r\nIt was clear from your logs that the `use_gpu` and `gpu_wait_type` parameters weren't being correctly read, and I think I found what's causing this. Should be fixed soon.", "This should be fixed now. Let me know if you're still seeing the problem.", "Looking good now, thanks for the work!\r\n\r\nAlthough still not getting similar benchmark performance(around 8ms for the `Mobilenet_1.0_224`, instead of 3.4ms), but there's already a significant speed up compare to the cpu. \r\n", "@Richard-Yang-Bose That's a somewhat known issue, which happens when the low-power CPU cores (Note: iPhone Xs CPU has non-symmetric cores) are used for the CPU computation part. When the high-power core is used, you'll see the 3.4ms result. Try adjusting the `num_threads` value to 2, and run the benchmarks multiple times to see that.\r\n\r\nIt's unfortunate that there doesn't seem to be any good way to force the high-power cores to be used with TFLite. We might end up updating our benchmark results page to reflect this.", "Gotcha, thanks for the clarification! This is really helpful!\r\n\r\n"]}, {"number": 36473, "title": "Program won't execute because of warnings but no errors. ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0 using pip install tensorflow-gpu\r\n- Python version:3.6\r\n- CUDA/cuDNN version: 9.2, 7\r\n\r\n`+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 36%   62C    P2   170W / 250W |  11723MiB / 12196MiB |     43%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN X (Pascal)    Off  | 00000000:03:00.0 Off |                  N/A |\r\n| 39%   67C    P2   135W / 250W |  11723MiB / 12196MiB |     31%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  TITAN X (Pascal)    Off  | 00000000:81:00.0 Off |                  N/A |\r\n| 23%   23C    P8    15W / 250W |     10MiB / 12196MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  TITAN X (Pascal)    Off  | 00000000:82:00.0 Off |                  N/A |\r\n| 23%   23C    P8    16W / 250W |     10MiB / 12196MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n`\r\n\r\n`/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\n2020-02-04 15:57:41.054419: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2020-02-04 15:57:41.072997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:02:00.0\r\n2020-02-04 15:57:41.073864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:03:00.0\r\n2020-02-04 15:57:41.074920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:81:00.0\r\n2020-02-04 15:57:41.075917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:82:00.0\r\n2020-02-04 15:57:41.076061: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2020-02-04 15:57:41.076154: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2020-02-04 15:57:41.076229: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2020-02-04 15:57:41.076301: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2020-02-04 15:57:41.076385: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2020-02-04 15:57:41.076456: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2020-02-04 15:57:41.081626: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-04 15:57:41.081657: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...\r\n2020-02-04 15:57:41.082444: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-02-04 15:57:41.774150: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x207b860 executing computations on platform CUDA. Devices:\r\n2020-02-04 15:57:41.774201: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN X (Pascal), Compute Capability 6.1\r\n2020-02-04 15:57:41.774213: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): TITAN X (Pascal), Compute Capability 6.1\r\n2020-02-04 15:57:41.774223: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): TITAN X (Pascal), Compute Capability 6.1\r\n2020-02-04 15:57:41.774232: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): TITAN X (Pascal), Compute Capability 6.1\r\n2020-02-04 15:57:41.777758: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2094980000 Hz\r\n2020-02-04 15:57:41.782256: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20995e0 executing computations on platform Host. Devices:\r\n2020-02-04 15:57:41.782292: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-02-04 15:57:41.782402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-04 15:57:41.782419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      \r\n2020-02-04 15:57:41.838944: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\n2020-02-04 15:57:42.106202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-04 15:57:42.106305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187] `\r\n\r\nIn my opinion, these warnings should not stop the program execution. I am not sure which warning to tackle to allow program execution. \r\n", "comments": ["@himanshisyadav,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "I am using the examples for basic MLP in [this github](https://github.com/lloydwindrim/hyperspectral-autoencoders) repo. ", "@himanshisyadav,\r\nSorry for the delayed response.\r\nI tried to reproduce the issue on TF-GPU 1.15.2, but did not get the warnings mentioned above. You can take a look at the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/48764af7da92e616399ad8bf4d961a8c/36473.ipynb). Thanks!", "Thanks. It works with 1.15.2 version and I don't get the warnings. "]}, {"number": 36472, "title": "Allow listing an bucket for S3 Filesystem backend.", "body": "Currently, when mounting S3 with an custom backend (e.g minio), tf.io.gfile.listdir(\"s3://my_bucket\") won't work as it will throw an error on empty object name.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36472) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36472) for more info**.\n\n<!-- ok -->", "The windows build seems irrelevant, any way to trigger a re build?"]}, {"number": 36471, "title": "TF2+ is amazing(!!!) IDC what anyone says...", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Name one meaningful advantage Pytorch has over TF2+..."]}, {"number": 36470, "title": "Get network output during training.", "body": "Hello,\r\nIs it possible to run `tensor.eval()` in the loss function to get the network output and use it in an external file to compute the loss.\r\n \r\nThank you", "comments": ["@ilyes-hm, Can you elaborate the issue with sample code and/or any use case. Thanks!", "So I am trying to build a loss function that uses the network output and pass it to a command line function to create two matrices then i compute the loss based on these matrices, my problem i cant generate these matrices when I pass the network outputs using `tensor.eval()` to a string to run the command line.\r\n```python\r\ndef command(params):   \r\n     cmd = 'a command line'+str(params.eval())\r\n     mat = os.system(cmd)\r\n     return mat\r\ndef custom_loss(y_true, y_pred):\r\n    mat1 = command(y_true)\r\n    mat2 = command(y_pred)\r\n    loss = mean_squared_error(mat1,mat2)\r\n    return loss\r\n```  ", "@ilyes-hm, Could you provide full error log and complete code to analyze the issue. Thanks", "Let me reformulate my question, Is it possible to get the network outputs as numpy and compute the loss using a numpy function (which include a command line that takes the network output as argument)?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 36469, "title": "[INTEL MKL] Relu op MKL-DNN 1.x integration", "body": "", "comments": ["@penpornk Thank you for reviewing the PR. I have addressed your comments. Please have a look. ", "@penpornk Appreciate if you can prioritize the review/merge-in this PR and a couple of others related to DNNL1.x upgrade. Thanks. "]}, {"number": 36468, "title": "Add block cache for low level table library", "body": "This is part of a patch series aiming to improve the performance of on-disk dataset.cache() (CacheDatasetV2).\r\n\r\nCurrently CacheDataset uses core/util/tensor_bundle to cache dataset elements on disks. It uses sorted string table (SST) to index dataset elements. Unlike checkpoints which do not have a great number of tensors, caching a large dataset may incur a greater number of tensors as well as index blocks.\r\n\r\nIf the index block is present in an in-memory LRU block cache, fetching a dataset element only needs 1 round trip instead of 2. This is particularly useful when CacheDataset are read from remote file system at a higher latency such as HDFS and GCS.\r\n\r\nAlmost all code are imported from the LevelDB project, in particular the hash function to shard LRU cache. Currently using Hash32 in core/lib/hash fails the EvictionPolicy test.\r\n\r\nI only make 2 modifications to the original cache:\r\n\r\n1. Alias leveldb::Slice to tensorflow::StringPiece, which transitively aliases to absl::string_view.\r\n2. Switch to tensorflow::mutex for all mutexes.\r\n\r\nPing @jsimsa to review.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36468) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36468) for more info**.\n\n<!-- ok -->", "Gently ping @gbaned for review. \r\n\r\nIt seems we have r2.2 branch cut later this month and I would really like this PR to make it into the next release.", "@gbaned Hi, is there anything I could do to roll this PR forward?", "> @gbaned Hi, is there anything I could do to roll this PR forward?\r\n\r\n@byronyi Sorry for the slow response, it is processing internally. We will let you know if anything needed. Thank you.", "@gbaned I've fixed the review comment. Mind to kick off CI tests again?", "@gbaned mind to take a look? r2.2 branch cut draws near.", "I see this is being merged manually. Thanks! "]}, {"number": 36467, "title": "Update LICENSE year 2020.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36467) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36467) for more info**.\n\n<!-- ok -->", "Sorry, but year should not change. See https://github.com/tensorflow/tensorflow/issues/36200#issuecomment-578450784"]}]