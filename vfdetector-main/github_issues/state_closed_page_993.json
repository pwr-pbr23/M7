[{"number": 23603, "title": "Is tensorflow supported in python 2.7?", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution: **Linux Ubuntu 16.04**\r\n- Python version: **2.7-64bits**\r\n- Installed using pip: **pip version 9.0.3**\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 9.1\r\n\r\n**Describe the problem**\r\nHi:\r\nIm trying to install tensorflow with pip with this command:\r\n`pip install tensorflow-gpu`\r\nand the output says:\r\n`Collecting tensorflow-gpu\r\n  Could not find a version that satisfies the requirement tensorflow-gpu (from versions: )\r\nNo matching distribution found for tensorflow-gpu\r\nYou are using pip version 9.0.3, however version 18.1 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.`\r\n\r\nIs my python version compatible with tensorflow? I see that are instructions here [https://www.tensorflow.org/install/pip?lang=python2](url) to install tensorflow for python 2.7 but i have the same result.\r\nI want to use Python 2.7 because the other modules I need for my scripts are installed in python 2.7. I have seen in other posts and issues that tensorflow is not compatible with python 2.7, is that true? How can I solve this? Should I try to install the other modules in python 3.5.2?\r\nThanks for help.\r\n", "comments": ["@PabloDIGITS \r\nWhat about trying the command below?\r\n```\r\n$ sudo pip2 install pip --upgrade\r\n$ sudo -H pip2 install tensorflow-gpu==1.11.0\r\nor\r\n$ sudo -H pip2 install tensorflow-gpu==1.12.0\r\n```", "@PINTO0309 \r\nIt give me the same output. But I didnt upgrade pip, the last time I did it it gave me the error \r\n`Traceback (most recent call last):\r\n  File \"/usr/bin/pip\", line 9, in <module>\r\n    from pip import main\r\nImportError: cannot import name main`\r\n\r\nAnd i had to downgrade to the 9.0.3 version to work again. Do you think its a pip problem and not an incompatibilty with the python version?\r\n", "@PabloDIGITS\r\nMaybe it is.\r\nWhat about trying the command below?\r\n```\r\n$ which python2\r\n$ wget https://bootstrap.pypa.io/get-pip.py\r\n$ sudo python2 get-pip.py\r\n```\r\nIf you do not do this please try asking with Stackoverflow.\r\nI think that it is better to tell something about the hardware.", "@PINTO0309 \r\n`$ which python2` gives me:\r\n`$ /usr/bin/python2`\r\nThe last 2 commands seems to install pip 18.1, and I can execute pip list/freeze without any problem so my pip problem looks like resolved. Thanks for that help.\r\nBut the tensorflow problem is still there. I'm going to ask in Stackoveflow and see what happends.\r\n\r\n", "@PabloDIGITS \r\nAlthough it may have already been settled, I will contact you in case.\r\nIf unnecessary information, please ignore below.\r\nIf the device you are using is \"TX2\", you can install Tensorflow1.9 with .whl at the following URL.\r\n\r\n**Python2.7**\r\nhttp://developer.download.nvidia.com/compute/redist/jp33/tensorflow-gpu/tensorflow_gpu-1.9.0+nv18.8-cp27-cp27mu-linux_aarch64.whl\r\n```\r\n$ sudo -E pip2 install tensorflow-gpu/tensorflow_gpu-1.9.0+nv18.8-cp27-cp27mu-linux_aarch64.whl\r\n```\r\n\r\n**Python3.5**\r\nhttp://developer.download.nvidia.com/compute/redist/jp33/tensorflow-gpu/tensorflow_gpu-1.9.0+nv18.8-cp35-cp35m-linux_aarch64.whl\r\n```\r\n$ sudo -E pip3 install tensorflow-gpu/tensorflow_gpu-1.9.0+nv18.8-cp35-cp35m-linux_aarch64.whl\r\n```", "Ok, now it seems installed. I did pip list and tensorflow-gpu shows in the list. The problem was about the arch right? Im using a Jetson TX2.\r\nNow I have 2 questions:\r\n-- What is the -E arg does?\r\n--The fact that I use sudo (without sudo gives my an error of permissions) for installing will affect to acces the library with a script executed without sudo permissions or that other modules installed without sudo can acces to tensoflow?\r\n\r\nThank you for the help.", "@PabloDIGITS \r\n>The problem was about the arch right?\r\n\r\nYes. That's right.\r\n\r\n>-- What is the -E arg does?\r\n\r\nThe \"-E\" option inherits environment variables.\r\n\r\n>for installing will affect to acces the library with a script executed without sudo permissions\r\n>that other modules installed without sudo can acces to tensoflow?\r\n\r\nUse \"sudo\" only during installation.\r\nThe only caveat is that it will not work unless cuda 9.0 is installed with Jet Pack.\r\nI failed because I introduced Jet Pack 3.1 + cuda 8.0.", "@PINTO0309 \r\nThanks for the response. I did `import tensorflow` and `import clodsa` (my custom library for augmented data that needs tensorflow and doesnt show any error). I will see if I can use the library without any error.\r\nThanks again.\r\n\r\nPD: I have CUDA 9.1 I hope that difference dont cause me any error.", "> @PabloDIGITS\r\n> Although it may have already been settled, I will contact you in case.\r\n> If unnecessary information, please ignore below.\r\n> If the device you are using is \"TX2\", you can install Tensorflow1.9 with .whl at the following URL.\r\n> \r\n> **Python2.7**\r\n> http://developer.download.nvidia.com/compute/redist/jp33/tensorflow-gpu/tensorflow_gpu-1.9.0+nv18.8-cp27-cp27mu-linux_aarch64.whl\r\n> \r\n> ```\r\n> $ sudo -E pip2 install tensorflow-gpu/tensorflow_gpu-1.9.0+nv18.8-cp27-cp27mu-linux_aarch64.whl\r\n> ```\r\n> **Python3.5**\r\n> http://developer.download.nvidia.com/compute/redist/jp33/tensorflow-gpu/tensorflow_gpu-1.9.0+nv18.8-cp35-cp35m-linux_aarch64.whl\r\n> \r\n> ```\r\n> $ sudo -E pip3 install tensorflow-gpu/tensorflow_gpu-1.9.0+nv18.8-cp35-cp35m-linux_aarch64.whl\r\n> ```\r\n\r\nHi\uff0cDo you have a 2.7 version of the TensorFlow installation package for Windows operating system? I can't find it."]}, {"number": 23602, "title": "tensorflow-gpu import throw serveral exception", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution:**:win 10 gtx860\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:tensorflow-gpu      1.12.0\r\n- **Python version**:3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:cuda 8.0/cuDNN 6.0\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\ni have already installed cuda and cuDNN,and the location of cuDNN  below:\r\nC:\\Users\\wdh>where cudnn64_6.dll\r\nD:\\studyTool\\cuda\\bin\\cudnn64_6.dll\r\n\r\nand environment is :\r\n print(\"Environmental variable:\", os.environ[\"PATH\"])\r\nEnvironmental variable: D:\\Anaconda\\envs\\tensorflow\\bin;D:\\studyTool\\cuda\\bin;D:\\studyTool\\cuda\\libnvvp;\r\n\r\nhow can i solve this problem?\r\n\r\n### Source code / logs\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Anaconda\\envs\\tensorflow\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["Please refer below issues. \r\n#17393\r\n#10033\r\n", "> Please refer below issues.\r\n> #17393\r\n> #10033\r\n\r\n@winterwindwang  Any update ?", "In \"awaiting response\" status for more than 3 days. Hence closing this. Please post the updates here if any, we will reopen the issue. Thanks !", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23600, "title": "tensorflow lite AllocateTensors() error after ResizeInputTensor of mobilenet ssd models", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:qualcom 801 platform\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):1.11\r\n- Python version:\r\n- Bazel version (if compiling from source):1.18\r\n- GCC/Compiler version (if compiling from source):gcc 5.4\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\n\r\n**Describe the current behavior**\r\nI convert ssd_mobilenet_v1_0.75_quantize model to .tflite, I know the model's training input size is 300x300,  I want to use different input tensor size, such as 240x240, so I use the ResizeInputTensor() function,\r\n**But I got an error in AllocateTensors():**\r\ntensorflow/contrib/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (2700 != 4332)\r\nNode number 36 (RESHAPE) failed to prepare.\r\nFailed to allocate tensors!\r\n\r\nHow to fix it?\r\n\r\n**Describe the expected behavior**\r\nI want to change the input tensor size freely\r\n\r\n**Code to reproduce the issue**\r\n`\r\n\r\n    std::vector<int> sizes = {1, 240, 240, 3};\r\n\r\n    interpreter_->ResizeInputTensor(interpreter_->inputs()[0], sizes);\r\n\r\n    if (interpreter_->AllocateTensors() != kTfLiteOk) {\r\n        LOG(INFO) << \"Failed to allocate tensors!\" << \"\\n\";\r\n        return false;\r\n}`\r\n", "comments": ["We have been tracking this issue internally. Meanwhile, aside from rescaling your input image, the only other workaround is to reconvert the model using --input_shapes=1,240,240,3\r\n", "OK,I'll try it", "@zjbaby , were you able to convert the model?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hi @andrehentz \r\nwhen will you fix this?\r\n", "@srjoglekar246 @zjd1988 \r\nsorry, I haven't fix it", "meet the same error. have you solved it @srjoglekar246 @zjd1988 @zjbaby ", "@IvyGongoogle @zjbaby @zjd1988 Apologies for the late response on this. AFAIK, MobileNet SSD has a fixed architecture that only supports 300x300 images. As a result, its not possible to resize inputs for this graph. Your best bet would be to central-crop the image to a 1:1 ratio, and then resize it (maybe with bilinear interpolation). C++ code for this is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/stages/image_preprocessing_stage.cc#L44)", "@srjoglekar246 I find the `sizes` must be `{1, 300, 300, 3}` which is the same input size of ssd tflite model, and then `interpreter_->ResizeInputTensor(interpreter_->inputs()[0], sizes)` works.  But you said `1:1 ratio` is ok?", "You would need to crop 1:1 AND resize to (300, 300) :-)"]}, {"number": 23599, "title": "TFLite constant strided slice wrongly", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.11\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nAfter convert to tf lite, my model cannot get the correct output, then i realise it is actually the slicing output is wrong.\r\n\r\nCode to reproduce the error\r\n```\r\nclass TestModel2(object):\r\n    def __init__(self):\r\n        self.sess = tf.Session()\r\n\r\n        self.input_placeholder = tf.placeholder(tf.float32, [4, 2])\r\n\r\n        self.data = tf.constant([[0, 1, 2, 3],\r\n                                 [4, 5, 6, 7],\r\n                                 [8, 9, 10, 11],\r\n                                 [12, 13, 14, 15]], tf.float32, name='data')\r\n\r\n        self.data_slice = self.data[:, :2]\r\n\r\n        # self.output_node = self.input_placeholder * self.data_slice\r\n\r\n        self.sess.run(tf.global_variables_initializer())\r\n\r\n    def test_convert_tflite(self):\r\n        print('loading from session')\r\n        converter = tf.contrib.lite.TocoConverter.from_session(self.sess, [self.input_placeholder],\r\n                                                               [self.data_slice])\r\n        print('converting to tflite')\r\n        tflite_model = converter.convert()\r\n        open('dummy.tflite', 'wb').write(tflite_model)\r\n\r\n    def test_run(self, inputs):\r\n        output = self.sess.run([self.data_slice], feed_dict={self.input_placeholder: inputs})\r\n        print(output)\r\n        print('-')\r\n\r\nclass TestTFLite(object):\r\n    def __init__(self, path):\r\n        # Load TFLite model and allocate tensors.\r\n        interpreter = tf.contrib.lite.Interpreter(model_path=path)\r\n        interpreter.allocate_tensors()\r\n\r\n        # Get input and output tensors.\r\n        input_details = interpreter.get_input_details()\r\n        output_details = interpreter.get_output_details()\r\n\r\n        print('input tensor:', input_details[0])\r\n        print('output tensor:', output_details[0])\r\n\r\n        self.interpreter = interpreter\r\n        self.input_details = input_details\r\n        self.output_details = output_details\r\n\r\n    def test_run(self, inputs):\r\n        self.interpreter.set_tensor(self.input_details[0]['index'], inputs)\r\n        self.interpreter.invoke()\r\n\r\n        output = self.interpreter.get_tensor(self.output_details[0]['index'])\r\n        print(output)\r\n        # output = self.interpreter.get_tensor(self.output_details[1]['index'])\r\n        # print(output)\r\n\r\nif __name__ == '__main__':\r\n    np.random.seed(1000)\r\n    inputs = np.random.randn(4, 2).astype(np.float32)\r\n    tr = TestModel2()\r\n    tr.test_run(inputs)\r\n    tr.test_convert_tflite()\r\n\r\n    tl = TestTFLite('dummy.tflite')\r\n    tl.test_run(inputs)\r\n\r\n```\r\n\r\ntensorflow output\r\n```\r\narray([[ 0.,  1.],\r\n       [ 4.,  5.],\r\n       [ 8.,  9.],\r\n       [12., 13.]], dtype=float32)\r\n```\r\nTFLite output\r\n```\r\n[[ 0.  4.]\r\n [ 8. 12.]\r\n [ 1.  5.]\r\n [ 9. 13.]]\r\n```", "comments": ["This has been verified. We are working on a fix.", "Fix has been submitted. Please retry at head and let us know if there is an issue.\r\n\r\nThanks"]}, {"number": 23598, "title": "Wrong output of AveragePool operation, for a quantization TF lite model ", "body": "Hello, \r\n\r\nstep1: Train a TF model to be a quantization one by adding Fake Node.\r\nstep2: Convert the quantization TF model to be a TF Lite model with command: \r\n` ./bazel-bin/tensorflow/contrib/lite/toco/toco \r\n        --output_file=~/deeplab_mobilenetv2_quantized.tflite  \r\n        --input_file=~/frozen_inference_graph.pb  \r\n        --inference_type=QUANTIZED_UINT8 \r\n        --input_arrays=Placeholder \r\n        --output_arrays=ResizeBilinear_2 \r\n        --mean_values=0 \r\n        --std_dev_values=1 \r\n        --input_shapes=1,513,513,3`\r\n         \r\nstep3. Run the quantization TF lite model on an Android phone.\r\n\r\nI find, for thethe output of model,  there is a big difference between TF and quantization TF lite .\r\n\r\nSo, I compared the output of maily operations of model, one by one. And I find that:\r\n1> For conv operataion, the output difference are less than 1.\r\n2> But for a AveragePool operation,the output of TF lite seem to be completely wrong. After debug, I find the kernel implementation is running by calling the interface `TF_LITE_AVERAGE_POOL(optimized_ops)`(in file: tensorflow/lite/kernels/pooling.cc).\r\n\r\nBut when I use `TF_LITE_AVERAGE_POOL(reference_ops)` to replace it by modifying the \"pooling.cc\" in the \"AverageEvalQuantized(...const uint8* input_data...)\" as: \r\n     ` if (kernel_type == kReference) {`\r\n          `TF_LITE_AVERAGE_POOL(reference_ops);`\r\n     `} else {`\r\n         ` //TF_LITE_AVERAGE_POOL(optimized_ops);`\r\n         `TF_LITE_AVERAGE_POOL(reference_ops) ;`\r\n       `}`\r\nAfter build and run, find that the difference are also less than 1.\r\n\r\nIn all, the issue is:\r\nThe output of AveragePool operation seem to be wrong when use the interface  `TF_LITE_AVERAGE_POOL(optimized_ops)`.\r\n\r\nWould you like to enlighten me whether I make some mistakes when I use AveragePool.\r\n\r\nAnd would you like to do a test for AveragePool operation with:\r\n   input size: 1x33x33x320 (NxHxWxC)\r\n   input data type: int8, range at 100~255\r\n   pool_params->stride_width = 33;\r\n   pool_params->stride_height = 33;\r\n   pool_params->filter_height = 33;\r\n   pool_params->filter_width = 33;\r\n   pool_params->activation = kTfLiteActNone;\r\n   pool_params->padding = kTfLitePaddingValid;\r\n\r\nSystem info for your reference:\r\n   - OS Platform: Linux Ubuntu 16.04\r\n   - Mobile device:XiaoMi Note\r\n   - TensorFlow version: commit d1e9a1ed54cae9b0b10ab89c06d6d7f9b53af3a1\r\n   - Python version:2.7.6\r\n   - Bazel version:0.15.0\r\n", "comments": ["Hello, can you explain how to modify deeplab model to train  a quantization one ? Great thanks", "@fengsam, Add `tf.contrib.quantize.create_training_graph(quant_delay=0)`before           \r\n       \u201cslim.learning.train()\u201d ", "@tensorflowbutler, @suharshs, @liyunlu0618 \r\n\r\nIt has been 12 days with no activity.\r\n\r\nDid you checked that the operation of AveragePool have worked normally on quantized tflite compared by reference?", "Apologies for your troubles. This is due to the different overflow characteristics of the reference and optimized ops. The former uses an int32 accumulator while the latter is using uint16. With the optimized version you will start seeing issues when the image size is larger than 16x16. The reference ops supports images up to 2899x2899 before showing signs of overflow.\r\n\r\nWe will track this internally in order to mitigate the issue.", "@tensorflowbutler, @suharshs, @liyunlu0618\r\n\r\nt has been a long time with no activity.\r\n\r\nHow did your status for this issue?", "Got the same issue when implementing GAP in a deeplab-like quantized model.", "Still working on a fix internally. Stay tuned.", "The issue should have been fixed by https://github.com/tensorflow/tensorflow/commit/129ff2e6ea3ddd88ae3d71a0cb719e987b486dac\r\n\r\nPlease take a look!"]}, {"number": 23597, "title": "training translation in multiple gpu with large data", "body": "hello i am using the language translation with attention in tensorflow \r\nhttps://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\r\n\r\nnow i like to train the model into multiple gpu with huge dataset i got some code but i don't know how to combine this with that\r\nNUM_GPUS = 2\r\nstrategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS)\r\nconfig = tf.estimator.RunConfig(train_distribute=strategy)\r\nestimator = tf.keras.estimator.model_to_estimator(model, config=config)\r\n\r\nor any other ways to do that could you please help me\r\n\r\nthanks in advance", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23596, "title": "Moving batch_get_value out of the for loop", "body": "Following the issue posted [here](https://github.com/tensorflow/tensorflow/issues/23545), moving the extremely expensive batch_get_value call out of the for loop speeds up weight saving like a million times", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "This is trading off computational overhead for memory overhead. I'm not sure this is the right decision.", ">           This is trading off computational overhead for memory overhead. I'm not sure this is the right decision.\r\n\r\nwe could add an extra parameter \"fast_saving\" or something to \"save_weights\" and let the users choose what works the best for them", "As per our discussion in https://github.com/keras-team/keras/pull/11604, we will not be merging this change. Thank you!"]}, {"number": 23595, "title": "Running Keras CNN model give different results on GPU than CPU.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian\r\n- TensorFlow version (use command below): \r\nKeras, version 2.2.4 is used on Tensorflow CPU 1.5.0 \r\nKeras, version 2.2.4 is used on Tensorflow GPU 1.11.0\r\n- Python version: 3 \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 7.2.1\r\n- GPU model and memory: Telsa P4\r\n\r\n**Describe the current behaviour**\r\nI trained a CNN model using Keras over Tensorflow. The results showed that the model has nearly 10% better accuracy than running on CPU for the same code.\r\n\r\n**Describe the expected behaviour**\r\nI expected the same test accuracy on both GPU and CPU.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport keras\r\nfrom keras.datasets import cifar10,mnist,cifar100\r\nfrom keras import Sequential,optimizers\r\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten\r\nimport LoggerYN as YN\r\nimport numpy as np\r\nimport scipy.io as sio\r\nimport utilsYN as uYN\r\nimport datetime\r\nimport time\r\n\r\n\r\n\r\ndef initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\r\n    global Dataset    \r\n    global pbatchSize\r\n    global pnumClasses\r\n    global pEpochs\r\n    global pLearningRate\r\n    global pMomentum\r\n    global pWeightDecay\r\n    Dataset = dataset\r\n    pbatchSize = batchSize\r\n    pnumClasses = numClasses\r\n    pEpochs = epochs\r\n    pLearningRate = learningRate\r\n    pMomentum = momentum\r\n    pWeightDecay = weightDecay\r\n    \r\ndef NormalizeData(x_train,x_test):\r\n        x_train /= 255\r\n        x_test /= 255\r\n        return x_train, x_test\r\n\r\ndef CategorizeData(y_train,y_test,pnumClasses):\r\n    y_train = keras.utils.to_categorical(y_train, pnumClasses)\r\n    y_test = keras.utils.to_categorical(y_test, pnumClasses)\r\n    return y_train, y_test\r\n    \r\ndef loadData():\r\n\r\n    Dataset = cifar100\r\n    (x_train, y_train), (x_test, y_test) = Dataset.load_data(label_mode='fine') if fineFlag else Dataset.load_data()\r\n    \r\n    global imgRows\r\n    global imgCols\r\n    global inputShape\r\n    \r\n    imgRows = x_train.shape[1]\r\n    imgCols = x_train.shape[2]\r\n\r\n    try:\r\n        imgRGB_Dimensions = x_train.shape[3]\r\n    except Exception:\r\n        imgRGB_Dimensions = 1 #For Gray Scale Images\r\n\r\n    print(x_train.shape)\r\n    x_train = x_train.reshape(x_train.shape[0], imgRows, imgCols, imgRGB_Dimensions)\r\n    x_test = x_test.reshape(x_test.shape[0], imgRows, imgCols, imgRGB_Dimensions)\r\n    x_train = x_train.astype('float32')\r\n    x_test = x_test.astype('float32')\r\n    x_train, x_test = NormalizeData(x_train, x_test)\r\n    y_train, y_test = CategorizeData(y_train,y_test,pnumClasses)\r\n    inputShape = (imgRows, imgCols, imgRGB_Dimensions)\r\n    return x_train, y_train, x_test, y_test\r\n\r\n\r\ndef model_CIFAR100():\r\n    CIFAR_model = Sequential()\r\n    CIFAR_model.add(Conv2D(128, (3, 3), padding='same',strides=1,input_shape=inputShape))\r\n    CIFAR_model.add(Activation('relu'))\r\n    CIFAR_model.add(Conv2D(128, (3, 3)))\r\n    CIFAR_model.add(Activation('relu'))\r\n    CIFAR_model.add(MaxPooling2D(pool_size=(2, 2),strides=2,padding='valid'))\r\n    CIFAR_model.add(Dropout(0.1))\r\n    \r\n    CIFAR_model.add(Conv2D(256, (3, 3), padding='same',strides=1))\r\n    CIFAR_model.add(Activation('relu'))\r\n    CIFAR_model.add(Conv2D(256, (3, 3)))\r\n    CIFAR_model.add(Activation('relu'))\r\n    CIFAR_model.add(MaxPooling2D(pool_size=(2, 2),strides=2,padding='valid'))\r\n    CIFAR_model.add(Dropout(0.25))\r\n    \r\n    CIFAR_model.add(Conv2D(512, (3, 3), padding='same',strides=1))\r\n    CIFAR_model.add(Activation('relu'))\r\n    CIFAR_model.add(Conv2D(512, (3, 3)))\r\n    CIFAR_model.add(Activation('relu'))\r\n    CIFAR_model.add(MaxPooling2D(pool_size=(2, 2),strides=2,padding='valid'))\r\n    CIFAR_model.add(Dropout(0.5))\r\n    \r\n    \r\n    CIFAR_model.add(Flatten())\r\n    CIFAR_model.add(Dense(1024))\r\n    CIFAR_model.add(Activation('relu'))\r\n    CIFAR_model.add(Dropout(0.5))\r\n    CIFAR_model.add(Dense(pnumClasses))\r\n    CIFAR_model.add(Activation('softmax'))    \r\n    return CIFAR_model\r\n\r\n\r\ndef evaluateModel(model,x_test,y_test,verbose):\r\n    pLoss, pAcc = model.evaluate(x_test, y_test, verbose)\r\n    print(\"Test Loss\", pLoss)\r\n    print(\"Test Accuracy\", pAcc)\r\n     \r\n\r\n\r\ndef RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\r\n    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\r\n    x_train, y_train, x_test, y_test = loadData()\r\n    CIFAR_model = model_CIFAR100()\r\n    CIFAR_sgd = optimizers.SGD(lr=learningRate, decay=weightDecay, momentum=momentum, nesterov=False)\r\n    CIFAR_model.compile(loss='categorical_crossentropy',optimizer=CIFAR_sgd, metrics=['accuracy'])\r\n    CIFAR_model.fit(x_train, y_train,batch_size=batchSize,epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\r\n    evaluateModel(CIFAR_model,x_test, y_test, verbose=1)\r\n\r\n\r\ndef runModel(dataset,batchSize=128,numClasses=10,epochs=12,learningRate=0.01,momentum=0.5,weightDecay=1e-6):\r\n    RunCIFAR100(dataset,batchSize,numClasses=100,epochs=epochs,learningRate=learningRate,momentum=momentum,weightDecay=weightDecay)\r\n\r\ndef main():\r\n    runModel(\"cifar100\",epochs=200)\r\n```\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23595\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23595\">No</a>\n", "Closing out this issue because it hasn't been updated in the last year.  Please reopen if this is still relevant."]}, {"number": 23594, "title": "update rpi build path", "body": "lite folder is no longer under contrib folder, I found these two scripts need update", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@marine0131  Please sign CLA if you wish to contribute a PR. Thanks !", "I have signed CLA", "CLAs look good, thanks!\n\n<!-- ok -->", "@petewarden  Any update/comments ?", "Looks like these changes have been independently made: https://github.com/tensorflow/tensorflow/commit/b4a7f51584d60e26024bb7401e3167704f94d298", "Nagging Reviewer @petewarden, @miaout17: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied.", "Sorry for the delay, and thanks for the fix! We had another similar change come in that has solved the issue, so closing this PR."]}, {"number": 23593, "title": "Cannot install tensorflow-gpu; pip killed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 inside a Singularity image\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 9.0 + 9.1 + 9.2 / 7.3.1.20\r\n- GPU model and memory: GeForce GT 740 / 1999 MiB\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen I go to install `tensorflow-gpu`, pip reports a message `Killed`. No errors or other messages. `tensorflow` installs just fine.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\npip install --user tensorflow-gpu\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n```\r\nCollecting tensorflow-gpu\r\n  Downloading https://files.pythonhosted.org/packages/55/7e/bec4d62e9dc95e828922c6cec38acd9461af8abe749f7c9def25ec4b2fdb/tensorflow_gpu-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (281.7MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 281.7MB 9.4kB/s\r\nKilled\r\n```\r\n", "comments": ["I'm not sure why pip died but your GPU only supports Cuda compute capability 3.0.  TF 1.12 requires a compute capability of 3.5.  You'll need to switch to the non-gpu version (just tensorflow), go to an older version of TF or compile from source.  If you move to an older version be sure to check which cuda/cudnn version it supports.  There's specific compatibility tables listed in the TF docs and on Stack Overflow.  ", "BTW.. I was curious so I looked back to see what versions of TF support compute capability 3.0 and even back in mid-2015 it looks like they were compiling for CC 3.5 and 5.2, no support for 3.0.  Looks like you'll need to compile from source if you want to use the GPU.  I had to do this for my laptop's 860M and it now runs without issue.", "Thanks @bjascob, that was the problem. I tried it on a newer card and it worked. Closing."]}, {"number": 23592, "title": "Fix the output issue in MaxPool3DGrad", "body": "This PR fixes #23511 . \r\n`MaxPool3DGrad` requires output indices but [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/pywrap_tfe_src.cc#L1856) says no, so `op.outputs` for `_MaxPool3DGrad` will be `None`, which will cause `TypeError: 'NoneType' object has no attribute '__getitem__'` as reported by #23511. ", "comments": ["I think the fix is to remove the broken annotation that maxpool3dgrad will not require the output indices since it does seem to require.\r\n\r\nCan you add a test?", "@alextp Thanks for your comments! Yes, `MaxPool3DGrad` requires the output indices. The broken annotation has been fixed. A test case is added to `backprop_test.py` as well. Is `backprop_test.py` a proper place for this test? ", "Yes, the PR looks great now. Thanks!", "@alextp The test failures seem to be unrelated. Also, delete an empty test file, which was for experiment and was added unintentionally. ", "@feihugis I use the tensorflow1.8.0. I can find  tensorflow/python/ops/nn_grad.py. But I can not find tensorflow/python/eager/pywrap_tfe_src.cc in the tensorflow/python/eager directory. Is it because the version is wrong? Hope for your answer.\r\n\r\n", "1.8 predates eager, please switch to 1.14\n\nOn Tue, Jun 25, 2019 at 6:07 AM liubc17 <notifications@github.com> wrote:\n\n> @feihugis <https://github.com/feihugis> I use the tensorflow1.8.0. I can\n> find tensorflow/python/ops/nn_grad.py. But I can not find\n> tensorflow/python/eager/pywrap_tfe_src.cc in the tensorflow/python/eager\n> directory. Is it because the version is wrong? Hope for your answer.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/23592?email_source=notifications&email_token=AAABHRKF5K6II3Q7VIOBYTDP4IKA7A5CNFSM4GCPHGBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYQE4EI#issuecomment-505433617>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRP4GV7ZSCPHFZ2DIALP4IKA7ANCNFSM4GCPHGBA>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 23591, "title": "tf.Variable.load() raises NotImplementedError on TF1.11 but works okay in TF1.10 (RefVariable changes)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a \r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nccing @alextp as it seemed relevant.\r\n\r\nThere had been some changes to [variables.py](https://github.com/tensorflow/tensorflow/blob/e1fb7a248bb2d932a0bed6fc1d2d9e3d91b50e89/tensorflow/python/ops/variables.py#L473-L506) which separate RefVariable from default Variable (see [this commit](https://github.com/tensorflow/tensorflow/commit/e1fb7a248bb2d932a0bed6fc1d2d9e3d91b50e89) which was released to TF1.11 build). However, some of these changes seem to break backward compatibility.\r\n\r\nFor instance, if I create a variable using `tf.get_variable`, and within a session load a value into it using `.load(value, sess)` method, it worked fine in TF1.10. However with TF1.11 this results in `NotImplementedError`:\r\n\r\n```\r\n  File \"test.py\", line 41, in <module>\r\n    my_var.load(1.2, sess)\r\n  File \"/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 789, in load\r\n    raise NotImplementedError\r\nNotImplementedError\r\n```\r\n\r\nThe error points to line 789 in [variables.py](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/ops/variables.py#L789)\r\n\r\n**[UPDATE]**\r\nHere's a sample code snippet to reproduce the issue.\r\n```python\r\n# test.py\r\nimport tensorflow as tf\r\n\r\nclass RestoredVariable(tf.Variable):\r\n  \"\"\"\r\n  A variable restored from disk\r\n  \"\"\"\r\n  def __init__(self, name, trainable=True, collections=None, graph=None):\r\n    if graph is None:\r\n      graph = tf.get_default_graph()\r\n    if collections is None:\r\n      collections = [tf.GraphKeys.GLOBAL_VARIABLES]\r\n    if trainable and tf.GraphKeys.TRAINABLE_VARIABLES not in collections:\r\n      collections = collections + [tf.GraphKeys.TRAINABLE_VARIABLES]\r\n    self._variable = graph.as_graph_element(name).outputs[0]\r\n    self._snapshot = graph.as_graph_element(name + '/read').outputs[0]\r\n    self._initializer_op = graph.as_graph_element(name + '/Assign')\r\n    self._constraint = None\r\n    self._trainable = trainable\r\n    i_name = name + '/Initializer/'\r\n    keys = [ k for k in graph._nodes_by_name.keys() if k.startswith(i_name) and '/' not in k[len(i_name):] ]\r\n    if len(keys) != 1:\r\n      raise ValueError('Could not find initializer for variable', keys)\r\n    self._initial_value = graph.as_graph_element(keys[0]).outputs[0]\r\n    for key in collections:\r\n      graph.add_to_collection(key, self)\r\n    self._save_slice_info = None\r\n\r\n\r\n# Create a variable `my_var`\r\nvar_temp = tf.get_variable('my_var', shape=[], dtype=tf.float32)\r\n\r\n# Restore `my_var` as a `RestoredVariable` object which derives from `tf.Variable`.\r\n# This is useful when the python handle `var_temp` isn't available in current session\r\n# as may be the case when a loading from a serialized graph.pb or metagraph.\r\nmy_var = RestoredVariable('my_var', trainable=True)\r\n\r\n\r\nwith tf.Session() as sess:\r\n  my_var.load(1.2, sess)\r\n  print(sess.run(my_var))\r\n```\r\n\r\n**Output on TF1.10:**\r\n```\r\n(tensorflow) $ python test.py\r\n...\r\n1.2\r\n```\r\n\r\n**ERROR on TF1.11:**\r\n```\r\n(tensorflow) $ python test.py\r\n...\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 43, in <module>\r\n    print(var)\r\n  File \"/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1547, in __exit__\r\n    self._default_graph_context_manager.__exit__(exec_type, exec_value, exec_tb)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5227, in get_controller\r\n    yield g\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5035, in get_controller\r\n    yield default\r\n  File \"/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5227, in get_controller\r\n    yield g\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/eager/context.py\", line 357, in _mode\r\n    yield\r\n  File \"/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5227, in get_controller\r\n    yield g\r\n  File \"test.py\", line 41, in <module>\r\n    my_var.load(1.2, sess)\r\n  File \"/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 789, in load\r\n    raise NotImplementedError\r\nNotImplementedError\r\n```\r\n\r\nIs there a change in the method to load variables within a session? Or am I missing something here. Please help.", "comments": ["Weird, load() is implemented for both ref and resource variables. Can you provide a full snippet of code to reproduce this?", "Hi @alextp, thanks for your response. At the moment it is breaking our production code (works in TF1.10) which is quite dense, but let me create a simple example to reproduce this. Back to you shortly.\r\n\r\n**[UPDATE]**: I've added the code snippet above. Kindly take a look.", "Perhaps related to #22648 ?", "Thanks @facaiy for the pointer to [#22648](https://github.com/tensorflow/tensorflow/issues/22648). Yes definitely related, but I'm having trouble understanding the conclusion there. Was there a proposed solution/workaround to handle such inheritances of `tf.Variable` in TF1.11 onward?\r\n\r\nAs @leandro-gracia-gil [pointed out](https://github.com/tensorflow/community/pull/11/files#r215829467), we highly depend on restoring models which were serialized to graph_defs, without having the variable objects / python code that generated them. So it's crucial that any infrastructure changes made to variables in 1.xx (to support 2.0) don't break existing workflows in our production code. Please suggest a workaround that I may be able to test on our end.", "There is no public API in tf right now to subclass variables. If you want to continue depending on the private implementation details you can inherit from the internal RefVariable type instead of Variable, but it doesn't have a subclass API.\r\n\r\nI think you use-case can be addressed by Variable.from_proto, though.", "Thanks @alextp, that helps. To follow-up,\r\n1. Would it be possible to add the subclass API to `RefVariable` in the next 1.xx release?\r\n2. Is `tf.Variable.from_proto` going to be retained in 2.0? ([currently](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/ops/variables.py#L900-L904) it returns a `RefVariable` object, but can it be extended to `ResourceVariable` too?)\r\n\r\nIn the meantime let me give `tf.Variable.from_proto` a try; will update the thread with my findings.", "Re (1) I'd like to not add functionality to RefVariable since it'll go away in tf2.0.\r\n\r\nRe (2) tf.Variable.from_proto will be retained and works with both resource and ref variables (there's a field in the proto which controls which one to create)", "@alextp Thanks! Feel free to close.", "Per @alextp's suggestion above, I updated `RestoredVariable` definition to use `Variable.from_proto` instead of inheriting from `tf.Variable` with `__init__`. This seems to fix the issue with TF>=1.11.\r\n\r\nI wasn't sure how to obtain `variable_def` directly from `node_def` that is read from `graph_def`, so I created one and populated its proto fields. I think this is **not** the cleanest implementation. @alextp - what is a better way to do this?\r\n\r\n```python3\r\nimport tensorflow as tf \r\nfrom tensorflow.core.framework import variable_pb2\r\n\r\ndef restored_variable(name, trainable=True, collections=None, graph=None):\r\n  \"\"\"\r\n  A variable restored from disk. (FIX for TF>=1.11) See issue: \r\n  https://github.com/tensorflow/tensorflow/issues/23591\r\n\r\n  Example:\r\n  1) variable = restored_variable(node.name)\r\n  2) variables = [ restored_variable(node.name, trainable=True, collections=None, graph=g) for node in g.as_graph_def().node if 'Variable' in node.op ]\r\n  \"\"\"\r\n  variable_def = variable_pb2.VariableDef()\r\n  if graph is None:\r\n    graph = tf.get_default_graph()\r\n  if collections is None:\r\n    collections = [tf.GraphKeys.GLOBAL_VARIABLES]\r\n  if trainable and tf.GraphKeys.TRAINABLE_VARIABLES not in collections:\r\n    collections = collections + [tf.GraphKeys.TRAINABLE_VARIABLES]\r\n  variable_def.variable_name = graph.as_graph_element(name).outputs[0].name\r\n  variable_def.snapshot_name = graph.as_graph_element(name + '/read').outputs[0].name\r\n  variable_def.initializer_name = graph.as_graph_element(name + '/Assign').name\r\n  variable_def.trainable = trainable\r\n  i_name = name + '/Initializer/'\r\n  keys = [ k for k in graph._nodes_by_name.keys() if k.startswith(i_name) and '/' not in k[len(i_name):] ]\r\n  if len(keys) != 1:\r\n    raise ValueError('Could not find initializer for variable', keys)\r\n  variable_def.initial_value_name = graph.as_graph_element(keys[0]).outputs[0].name\r\n  var = tf.Variable.from_proto(variable_def)\r\n  for key in collections:\r\n    graph.add_to_collection(key, var)\r\n  return var\r\n```", "The graph and nodedef do not have enough information to reconstruct the Python variable objects. Those get serialized in the metagraphdef if you export a metagraph (which is done by default when you save a checkpoint with tf.Saver)", "Okay. And after importing the metagraph in a session, how does one grab a `variable_def` using its variable name? Can you please provide a short example?\r\n\r\n**Import metagraph:**\r\n```python3\r\nwith tf.Session() as sess:\r\n  new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')\r\n  new_saver.restore(sess, 'my-save-dir/my-model-10000')\r\n```\r\n**Restore variable python object:**\r\n```python3\r\nvariable_name = \"my_var\"\r\nvariable_def = ??\r\nmy_var = tf.Variable.from_proto(variable_def)\r\n```", "After tf.train.import_meta_graph the variables will be in the collections,\nso [v for v in\ntf.get_default_graph().get_colection(tf.GraphKeys.GLOBAL_VARIABLES) if\nv.name == my_variable_name]\n\nOn Thu, Nov 15, 2018 at 4:35 PM Sambhav Jain <notifications@github.com>\nwrote:\n\n> Okay. And after importing the metagraph in a session, how does one grab a\n> variable_def using its variable name? Can you please provide a short\n> example?\n>\n> *Import metagraph:*\n>\n> with tf.Session() as sess:\n>   new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')\n>   new_saver.restore(sess, 'my-save-dir/my-model-10000')\n>\n> *Restore variable python object:*\n>\n> variable_name = \"my_var\"\n> variable_def = ??\n> my_var = tf.Variable.from_proto(variable_def)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23591#issuecomment-439239866>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVjkPpU-_q-cTow5mlpiReozUubTks5uvgg1gaJpZM4YTzTj>\n> .\n>\n\n\n-- \n - Alex\n", "Aah makes sense. Thanks @alextp. I guess to conclude we can say:\r\nThe `from_proto` method is not as useful when metagraph is available (which already contains collections of variables to be restored). The real use-case is when just the graphdef (.pb) is available (no metagraph). In this case, the (seemingly) only way to restore a variable is to create an empty `variable_def`, populate its proto fields, and use `tf.Variable.from_proto(variable_def)`. Correct?", "Yes, correct.\n\nOn Thu, Nov 15, 2018 at 5:36 PM Sambhav Jain <notifications@github.com>\nwrote:\n\n> Aah makes sense. Thanks @alextp <https://github.com/alextp>. I guess to\n> conclude we can say:\n> The from_proto method is not as useful when metagraph is available (which\n> already contains collections of variables to be restored). The real\n> use-case is when just the graphdef (.pb) is available (no metagraph). In\n> this case, the (seemingly) only way to restore a variable is to create an\n> empty variable_def, populate its proto fields, and use\n> tf.Variable.from_proto(variable_def). Correct?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23591#issuecomment-439251187>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVmVyxHq8O5UOAaOWCLmuTBYsjuIks5uvhaZgaJpZM4YTzTj>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 23590, "title": "tf.manip.roll executes on CPU instead of GPU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below):1.8\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:9/7.1\r\n- GPU model and memory:1080ti/11gb\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\ntf.manip.roll executes on CPU and is very slow\r\n\r\n**Describe the expected behavior**\r\n![image](https://user-images.githubusercontent.com/4759327/48167964-70a43d00-e2e5-11e8-9c7a-59e6bd50e306.png)\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dhingratul  In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "@harshini-gadige Here you go\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport collections\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\nslim = tf.contrib.slim\r\nimport timeit\r\nfrom tensorflow.python.client import timeline\r\n\r\n\r\ndef _repr(inputs, num_groups, scope=None):\r\n    if num_groups == 1:\r\n        return inputs\r\n    with tf.variable_scope(scope, 'channel_shift', [inputs]) as sc:\r\n        depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\r\n        assert depth_in % num_groups == 0, (\r\n            \"depth_in=%d is not divisible by num_groups=%d\" %\r\n            (depth_in, num_groups))\r\n        # group size, depth = g * n\r\n        group_size = depth_in // num_groups\r\n        net = tf.manip.roll(inputs, group_size, axis=3)\r\n\r\n        return net\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    batch = 5\r\n    mini_batch = 1\r\n    h = 704\r\n    w = 800\r\n    c = 6\r\n    # _inputs = tf.placeholder(tf.float32, shape=[1, h, w, c], name='Inputs')\r\n    _inputs = tf.Variable(tf.random_normal([1, h, w, c], stddev=0.1), name=\"Inputs\")\r\n    def create_mini_batch(mini_batch):\r\n        np.random.seed(i)\r\n        inputs = np.random.randn(mini_batch, h, w, c)\r\n        return inputs\r\n\r\n    # Network\r\n    net = _repr(_inputs, 2)\r\n\r\n    init = tf.global_variables_initializer()\r\n    times = []\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        summary_writer = tf.summary.FileWriter(\"logs_viz_unit\",graph=tf.get_default_graph())\r\n        for i in range(batch//mini_batch):\r\n            options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n            run_metadata = tf.RunMetadata()\r\n            print(\"Step {}/{}\".format(i,batch//mini_batch))\r\n            inputs = create_mini_batch(mini_batch)\r\n            for j in range(mini_batch):\r\n                start_time = timeit.default_timer()\r\n                a = sess.run([net], feed_dict={_inputs:np.expand_dims(inputs[j,:,:,:], 0)}, options=options, run_metadata=run_metadata)\r\n                elapsed = timeit.default_timer() - start_time\r\n                times.append(elapsed)\r\n            fetched_timeline = timeline.Timeline(run_metadata.step_stats)\r\n            fid = \"timeline_{}.json\".format(i)\r\n            chrome_trace = fetched_timeline.generate_chrome_trace_format()\r\n            with open(fid, 'w') as f:\r\n                f.write(chrome_trace)\r\n        print(\"Inference times(ms): {}\".format((sum(times[1:])/(batch - 1)) * 1000))\r\n```", "@azaks2 there's currently no GPU kernel for roll. Can someone on your team decide the priority of this?", "> Can someone on your team decide the priority of this?\r\nWe do not have bandwidth now, but would be happy to review PR.", "Any updates on a GPU version of tf.manip.roll? It becomes a huge bottleneck on training on big clusters of GPUs.\r\n\r\nBW, Gavin", "I'm reopening this as contributions welcome because we currently do not have the bandwidth to implement this ourselves but would be happy to review a pull request.", "@alextp, I would like to try implementing its GPU kernel, is it okay?", "@musikisomorphie yes! Let me know if you need any pointers. I encourage you to look at the GPU code in tensorflow/core/kernels, specially the .cu.h and .cu.cc files, which show the extra code that needs to be added to support kernels on the GPU.\r\n\r\nYou can try implementing this with eigen or directly with cuda, it's not clear to me which way will be simplest. I'd start by seeing how is the CPU roll kernel written, and if it's written in terms of eigen a similar implementation will likely work on the GPU.\r\n\r\nAnyway, lmk if you need more pointers.", "@alextp, sounds good. I am going to get myself familiar with this op first, I will let you know if I need some pointers.", "Any success? :) @musikisomorphie ", "@gavinlive, i was studying the codes implemented in the kernel last weekend. Today, I will try to implement it myself. ", "> @gavinlive, i was studying the codes implemented in the kernel last weekend. Today, I will try to implement it myself.\r\n\r\nAwesome! Let me know how you get on. I tried something but it didn't end up working but I'm happy to put it on GitHub.", "@gavinlive, this week I have a deadline.   But I will keep working on it simultaneously, once I make some progress I will let you know. It would be great if you can put your code on GitHub so I can get some inspiration.", "Hi @alextp, I may have a stupid question. I implemented roll_op.h roll_op_gpu.cu.cc, and I copy them in the kernels/ folder. When I compile tensorflow, it seems that the compiling process simply ignores roll_op_gpu.cu.cc. This is confirmed by the time when I tried to run some tests. Can you tell me what should I do? Thx.", "You need to add a new rule to tensorflow/core/kernels/BUILD for how to\nbuild these files. I recommend you take a look at existing rules and copy\nthem, as it can be a bit tricky to get the right incantations.\n\nOn Sun, Mar 24, 2019 at 11:42 AM musikisomorphie <notifications@github.com>\nwrote:\n\n> Hi @alextp <https://github.com/alextp>, I may have a stupid question. I\n> implemented roll_op.h roll_op_gpu.cu.cc, and I copy them in the kernels/\n> folder. When I compile tensorflow, it seems that the compiling process\n> simply ignores roll_op_gpu.cu.cc. This is confirmed by the time when I\n> tried to run some tests. Can you tell me what should I do? Thx.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23590#issuecomment-475981798>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxTNHcx9ljKIfpxmkBVI9A6-RdCQ_ks5vZ7kPgaJpZM4YTtgw>\n> .\n>\n\n\n-- \n - Alex\n", "> Hi @alextp, I may have a stupid question. I implemented roll_op.h roll_op_gpu.cu.cc, and I copy them in the kernels/ folder. When I compile tensorflow, it seems that the compiling process simply ignores roll_op_gpu.cu.cc. This is confirmed by the time when I tried to run some tests. Can you tell me what should I do? Thx.\r\n\r\nOh wow! Is that a successful implementation? :)", "@gavinlive, I am afraid i need more time to implement it, maybe one or two weeks. Because I spent a lot of time to figure out how to effectively debug the cuda code, still every time it takes me half an hour to compile the code and run some tests, which slows down my progress.  ", "@gavinlive I implemented the cuda code and it worked. But I still need some time to clean it, If you need it I can send you my current version.", "> @gavinlive I implemented the cuda code and it worked. But I still need some time to clean it, If you need it I can send you my current version.\r\n\r\nHi Can you send me your current version\r\n\r\n> @gavinlive I implemented the cuda code and it worked. But I still need some time to clean it, If you need it I can send you my current version.\r\n\r\nHi I would really appreciate if you could send me your gpu version of tf.roll. My email is grekavdel@hotmail.com ", "Seems like it is merged already:\r\nhttps://github.com/tensorflow/tensorflow/pull/27389", "Hi, are there any updates on this @alextp ?\r\n\r\nI am still having a hard time with the `fftshift` operators (as in they are very slow, as you can see [here](https://stackoverflow.com/questions/61060650/why-is-the-tensorflow-fftsthift-roll-op-so-slow)), which use the `roll` op under the hood.", "I am no longer working on TensorFlow, so someone else will have to pick this up. @aselle can you help triage?", "`tf.roll` gpu implementation is now supported. See colab [gist](https://colab.sandbox.google.com/gist/ymodak/67af7152a7e84b823ac1064299b7150b/gh_issue_23590.ipynb) executed with gpu runtime.\r\nAlso see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/roll_op_gpu.cu.cc\r\nThanks!\r\n"]}, {"number": 23589, "title": "[XLA] Add proto representation of U16/S16 literals", "body": "U16/S16 were already handled in most methods of Literal, except\r\nfor the serialization and deserialization of protos and a few\r\nother places. Add those, to make allocating U16 literals work\r\nvia xrt.", "comments": ["This looks good to me, but please wait till @meheffernan can take a look as well."]}, {"number": 23588, "title": "Support the new cuDNN RNN padding mask operation ", "body": "We added code to support the new cuDNN RNN padding mask operation, which can accept a batch of sequences with variable lengths. We added a new parameter of \"sequence_lengths\" to the old API, meaning only if it is provided, we will call the new operation.", "comments": ["Attention @azaks2", "This is a very useful PR. I'll start looking at it (but it would take some time.) ", "Thanks for your comments @protoget. I will take a look at how we improve the codes. ", "> Thanks for your comments @protoget. I will take a look at how we improve the codes.\r\n\r\n@houtoms  Any update ?", "@harshini-gadige Thanks for your reply. I am working on these issues pointed by @protoget . I hope to finish them these two days.", "I have made many changes according to the previous feedback. \r\n\r\nI am still working on designing a new kernel test for the ops based on the recently updated cudnn_rnn_ops_test.py. (I tried the new version of it, but I can't successfully run it in our container. I met many ' 'CudnnGRUSaveable' object has no attribute 'format_converter'' errors)", "Hi @protoget and @harshini-gadige , I just pushed an update to fix the problems you mentioned in the \r\nprevious conversation. Please take a look at it when you are available.", "Yes, I have just updated the codes. I fixed codes for some comments and I also had some questions on others. I have put my questions on corresponding comments.", "Hi @protoget, any new comments/feedback? ", "Hi @protoget, thanks for your comments. I have updated the codes accordingly.", "Hi @protoget, I fixed the problems you mentioned previously. \r\n\r\nFor the kernel_tests/cudnn_rnn_ops_test.py, since the TF docs mention that dynamic_rnn will \"zero-out outputs when past a batch element's sequence length\" and we explicitly set paddingFill as 0.0f when using cuDNN, I think we can rely on the test for now. ", "Hi @protoget, the changes have been made. Please take a look at them. Thanks.", "@protoget I fixed a syntax issue in cudnn_rnn_ops_test.cc. Also fixed some formatting issues in the python scripts.", "@harshini-gadige @protoget , Hi guys, do I need to do anything else? It seems the process is stuck on the \"interna safe review\".", "I'm waiting for the tests to run one more time then I'd approve the change internally.", "Hi @protoget, there is one problem from the \"GPU CC\" testing results: it cannot recognize the `cudnnRNNDataDescriptor_t` in the RNNDataDescriptorDeleter. I added a macro check on it.", "Got this error\r\n```\r\n/tensorflow/stream_executor/cuda/cuda_dnn.cc:1229:9: error: field 'rnn_data_handle_' will be initialized after field 'handle_' [-Werror,-Wreorder]\r\n        rnn_data_handle_(std::move(data_handle)),\r\n        ^\r\n1 error generated.\r\n```\r\n\r\nYou probably need to move `rnn_data_handle_` field under `handle_` in the private members section.", "Hi, @protoget , `rnn_data_handle_` field is already under `handle_` in the private members session.\r\n\r\n(I think the error you mentioned above is related to what I just solved. Please check my latest commits.)", "The error stays. I *guess* The init order in the constructor needs to be fixed to align with the declaration order in the member section.", "I see. I just changed the init order to be same with the declaration order. Please check it again. Thanks.", "@protoget  Do we need to rerun kokoro checks ? Or can be proceeded to merge ?", "Still got numerous errors:\r\n```\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:421:21: error: 'cudnnRNNDataStruct' was not declared in this scope\r\n     std::unique_ptr<cudnnRNNDataStruct, RNNDataDescriptorDeleter>;\r\n                     ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:421:65: error: template argument 1 is invalid\r\n     std::unique_ptr<cudnnRNNDataStruct, RNNDataDescriptorDeleter>;\r\n                                                                 ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:443:1: error: 'RNNDataDescriptor' does not name a type\r\n RNNDataDescriptor CreateRNNDataDescriptor() {\r\n ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1224:36: error: 'RNNDataDescriptor' has not been declared\r\n                                    RNNDataDescriptor data_handle,\r\n                                    ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1290:9: error: 'cudnnRNNDataDescriptor_t' does not name a type\r\n   const cudnnRNNDataDescriptor_t data_handle() const {\r\n         ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1306:3: error: 'RNNDataDescriptor' does not name a type\r\n   RNNDataDescriptor rnn_data_handle_;\r\n   ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In constructor 'stream_executor::cuda::CudnnRnnSequenceTensorDescriptor::CudnnRnnSequenceTensorDescriptor(stream_executor::cuda::CUDAExecutor*, int, int, int, cudnnDataType_t, int, stream_executor::cuda::{anonymous}::TensorDescriptor)':\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1232:9: error: class 'stream_executor::cuda::CudnnRnnSequenceTensorDescriptor' does not have any field named 'rnn_data_handle_'\r\n         rnn_data_handle_(std::move(data_handle)),\r\n         ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In static member function 'static stream_executor::port::StatusOr<stream_executor::cuda::CudnnRnnSequenceTensorDescriptor> stream_executor::cuda::CudnnRnnSequenceTensorDescriptor::Create(stream_executor::cuda::CUDAExecutor*, int, int, int, cudnnDataType_t)':\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1252:67: error: no matching function for call to 'stream_executor::cuda::CudnnRnnSequenceTensorDescriptor::CudnnRnnSequenceTensorDescriptor(stream_executor::cuda::CUDAExecutor*&, int&, int&, int&, cudnnDataType_t&, std::nullptr_t, std::remove_reference<std::unique_ptr<cudnnTensorStruct, stream_executor::cuda::{anonymous}::TensorDescriptorDeleter>&>::type)'\r\n                                             std::move(tensor_desc));\r\n                                                                   ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1252:67: note: candidates are:\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1236:3: note: stream_executor::cuda::CudnnRnnSequenceTensorDescriptor::CudnnRnnSequenceTensorDescriptor(stream_executor::cuda::CudnnRnnSequenceTensorDescriptor&&)\r\n   CudnnRnnSequenceTensorDescriptor(CudnnRnnSequenceTensorDescriptor&&) =\r\n   ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1236:3: note:   candidate expects 1 argument, 7 provided\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1221:3: note: stream_executor::cuda::CudnnRnnSequenceTensorDescriptor::CudnnRnnSequenceTensorDescriptor(stream_executor::cuda::CUDAExecutor*, int, int, int, cudnnDataType_t, int, stream_executor::cuda::{anonymous}::TensorDescriptor)\r\n   CudnnRnnSequenceTensorDescriptor(CUDAExecutor* parent, int max_seq_length,\r\n   ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1221:3: note:   no known conversion for argument 6 from 'std::nullptr_t' to 'int'\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'bool stream_executor::cuda::CudnnRnnSequenceTensorDescriptor::is_var_seq_lengths() const':\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1297:44: error: 'rnn_data_handle_' was not declared in this scope\r\n   bool is_var_seq_lengths() const { return rnn_data_handle_ != nullptr; }\r\n```", "`cudnnRNNDataStruct` is defined in `cudnn.h` for version 7.2.1 or later. It seems I need to add macro checks around all functions and parameters of `RNNDataDescriptor`, which needs to use `cudnnRNNDataStruct`. Any other idea?", "@protoget, Can we do something like this to create a dummy RNNDataDescriptor? (Update: maybe use void is more elegant. Can we do this?) (Update again: sorry, `void` should be `void*`)\r\n```\r\n#if CUDNN_VERSION >= 7201\r\nusing RNNDataDescriptor=std::unique_ptr<cudnnRNNDataStruct, RNNDataDescriptorDeleter>;  \r\n#else\r\n// dummy RNNDataDescriptor\r\nusing RNNDataDescriptor=void*;\r\n#endif \r\n```", "Then, I think I would add macros to avoid all RNNDataDescriptor if version check indicates cudnn < 7.2.1.", "Forget this constructor also needs to be changed.", "@protoget Any updates?", "```\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1302:9: error: 'cudnnRNNDataDescriptor_t' does not name a type\r\n   const cudnnRNNDataDescriptor_t data_handle() const {\r\n         ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'bool stream_executor::cuda::CudnnRnnSequenceTensorDescriptor::is_var_seq_lengths() const':\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1309:44: error: 'rnn_data_handle_' was not declared in this scope\r\n   bool is_var_seq_lengths() const { return rnn_data_handle_ != nullptr; }\r\n```\r\n\r\nCould you check passing all tests?", "Do we need to skip the tests on the new API if cudnn version < 7.2.1? If yes, can you point me to any example that I can use to write this?", "> Do we need to skip the tests on the new API if cudnn version < 7.2.1? If yes, can you point me to any example that I can use to write this?\r\n\r\nAll logic depending on cu 7.2 should be in enclosed in a macro.", "Yes. I mean for the `cudnn_rnn_ops_test.py`, if older cudnn is used, all the tests on the new API would fail and get the invalid argument error. Is this OK?", "AFAIK we run tests against only one cudnn version. I could be wrong. Let's wait to see the test results.", "Hi @protoget, any updates on the internal tests?", "I think it's getting some merge conflicts. Would you rebase all the commits on top of HEAD?", "Yes, I rebased the commits on the top of the HEAD. Please check. Thanks.", "@protoget, some tests failed, but it seems they are not caused by my changes (except the clang format one). Could you help check?", "> @protoget, some tests failed, but it seems they are not caused by my changes (except the clang format one). Could you help check?\r\n\r\nYes there're some internal build failures. The team is fixing it, but I don't have an ETA. Let's just wait."]}, {"number": 23587, "title": "how to implement fftshift on a tensor?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf this is a feature request, please fill the template. Thanks !"]}, {"number": 23586, "title": "Allowing GPU memory growth command does not work ", "body": "Hi, i have a memory problem.\r\nI am running a training on a server. I have the following print out.\r\n\r\n2018-11-05 21:08:07.907464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-11-05 21:08:07.908090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:\r\nname: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.2405\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.87GiB\r\n2018-11-05 21:08:07.908116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:02:00.0, compute capability: 5.2)\r\n\r\nAs you can see the total memory is higher than the free memory. Actually, running the code i get an \"Out of memory\" message.\r\nSo i applied the wrote code at the beginning of my script:\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3, allow_growth=True)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n\r\nUnfortunately the memory usage:\r\ntotalMemory: 3.95GiB freeMemory: 3.87GiB\r\n\r\ndoes not change at all. What is the problem?\r\n\r\nThanks and best regards,\r\nGiovanni", "comments": ["Sorry for the long delay here but actual issue and the expected behavior is not clear based the issue description.\r\n \r\nPlease mention all the relevant fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md) or leave them as N/A if is not applicable?", "Closing this to no recent activity. Let us know if this is still an issue."]}, {"number": 23585, "title": "tf.nn.Softmax() crashes if tensor is not two-dimensional ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04 \"nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04\" docker image and nvidia-docker\r\n- TensorFlow installed from (source or binary): binary (conda install tensorflow-gpu)\r\n- TensorFlow version (use command below): 1.10\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 9.2, CuDNN 7.2.1\r\n- GPU model and memory: GeForce GTX 1080 Ti, 11264 MB \r\n\r\n**Describe the current behavior**\r\nI'm working on image segmentation, with 5 classes. After computing logits, I want to compute softmax activations : \r\n`probabilities = tf.nn.softmax(logits)`\r\nwith logits of shape [-1,586,586,5], which shouldn't be a problem since softmax internally reshapes it to a 2D tensor. However, I get the following error.\r\n> Check failed: NDIMS == dims() (2 vs. 4)Asking for tensor of 2 dimensions from a tensor of 4 dimensions\r\n\r\n\r\nI cannot reproduce the issue outside of docker on ubuntu18.04 and tensorflow 1.11, everything works as expected. This happens at inference time, using a frozen model.\r\n\r\n**Describe the expected behavior**\r\ntensor should be reshaped internally and crash should't occur. \r\n\r\n**Quick fix**\r\n` flat_logits = tf.reshape(logits,shape=[-1,n_classes])`\r\n` probabilities = tf.nn.softmax(flat_logits)`\r\nReshaping the tensor myself prevents the crash from happening. ", "comments": ["@vincentleon Could you post an example code for reproducing the problem?", "@vincentleon In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.  If this is no longer an issue, feel free to close.\r\n"]}, {"number": 23584, "title": "Java does not need operations like'& 0xff'in the process of strong rotation. Reducing this operation can reduce many calculations and improve performance.", "body": "Java does not need operations like'& 0xff'in the process of strong rotation. Reducing this operation can reduce many calculations and improve performance.\r\n\r\nBefore that, every pixel in the picture had to be operated on three times, which would reduce the speed relatively. Because Java has the feature of high-to-low and discards high-to-low directly, high-to-low does not require manual'& 0xff'to be strong.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@petewarden  Any update ?", "Nagging Reviewer @petewarden: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied.", "Sorry for the delay in reviewing. The TensorFlow mobile examples are deprecated and no longer being actively maintained, so closing this one."]}, {"number": 23583, "title": "Explicitly import tools/bazel.rc", "body": "To fix build with Bazel 0.19.0 or later and it won't break build with old version of Bazel\r\nFixes https://github.com/tensorflow/tensorflow/issues/23398", "comments": ["/cc @yifeif ", "@yifeif Can you help merging this?", "For sure! Importing this now.", "Thank you!!!"]}, {"number": 23582, "title": "Is it ok for C++ code to link with _pywrap_tensorflow_internal.so?", "body": "I have a Python module written in C++, in which tensorflow serving API is used to load saved model and inference. Unfortunately, some part of my Python application has to use tensorflow python module directly. Because there are some common symbols between libtensorflow_cc.so and _pywrap_tensorflow_internal.so, there will be CHECK failure if the Python module is linked with libtensorflow_cc.so. \r\nThe only feasible option seems to link the Python module with _pywrap_tensorflow_internal.so. Is that OK?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "@tianyapiaozi did you resolve this issue ? were you able to link to it ?"]}, {"number": 23581, "title": "Simplest WindowDataset usage not working even for the documented examples", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): tensorflow/tensorflow:1.12.0 docker image, macOS 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n**Describe the current behavior**\r\n\r\n`Dataset.window` documentation has a few examples of which all crash in the process of graph construction (i.e. this example `tf.data.Dataset.range(7).window(2)`).\r\n\r\n**Describe the expected behavior**\r\n\r\nI would like it not to crash.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ndataset = tf.data.Dataset.range(7).window(2)\r\n\r\nit = dataset.make_one_shot_iterator()\r\n\r\n```\r\n\r\nThe same error occurs when eager execution is disabled.\r\n\r\n**Other info / logs**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-4495120d82f9> in <module>()\r\n      7 dataset = tf.data.Dataset.range(7).window(2)\r\n      8 \r\n----> 9 it = dataset.make_one_shot_iterator()\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/dataset_ops.pyc in make_one_shot_iterator(self)\r\n    178     \"\"\"\r\n    179     if context.executing_eagerly():\r\n--> 180       return iterator_ops.EagerIterator(self)\r\n    181     # NOTE(mrry): We capture by value here to ensure that `_make_dataset()` is\r\n    182     # a 0-argument function.\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/iterator_ops.pyc in __init__(self, dataset)\r\n    531         self._resource = gen_dataset_ops.anonymous_iterator(\r\n    532             output_types=self._flat_output_types,\r\n--> 533             output_shapes=self._flat_output_shapes)\r\n    534         gen_dataset_ops.make_iterator(ds_variant, self._resource)\r\n    535         # Delete the resource when this object is deleted\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.pyc in anonymous_iterator(output_types, output_shapes, name)\r\n     71       return anonymous_iterator_eager_fallback(\r\n     72           output_types=output_types, output_shapes=output_shapes, name=name,\r\n---> 73           ctx=_ctx)\r\n     74     except _core._NotOkStatusException as e:\r\n     75       if name is not None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.pyc in anonymous_iterator_eager_fallback(output_types, output_shapes, name, ctx)\r\n     89         \"Expected list for 'output_types' argument to \"\r\n     90         \"'anonymous_iterator' Op, not %r.\" % output_types)\r\n---> 91   output_types = [_execute.make_type(_t, \"output_types\") for _t in output_types]\r\n     92   if not isinstance(output_shapes, (list, tuple)):\r\n     93     raise TypeError(\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/execute.pyc in make_type(v, arg_name)\r\n    124   except TypeError:\r\n    125     raise TypeError(\"Expected DataType for argument '%s' not %s.\" %\r\n--> 126                     (arg_name, repr(v)))\r\n    127   i = v.as_datatype_enum\r\n    128   return i\r\n\r\nTypeError: Expected DataType for argument 'output_types' not <tensorflow.python.data.ops.dataset_ops._NestedDatasetComponent object at 0x7feadf71f9d0>.\r\n```", "comments": ["I am able to execute your script by using latest tf-nightly build. (1.13.0) Can you please confirm it?", "@antifriz your interpretation of the examples in the `window` dataset documentation is incorrect.\r\n\r\nThe result of applying the `window` transformation is a nested dataset, which needs to be flattened before it can be evaluated. One such mechanism for flattening a dataset is `flat_map`.\r\n\r\nFor instance,\r\n\r\n```\r\ndataset = tf.data.Dataset.range(7).window(2).flat_map(lambda x: x.skip(1))\r\n```\r\n\r\nproduces a dataset that skips all even elements of the original range.", "@jsimsa So how can we group adjacent elements during sequence into windows using tf.data? Maybe like what tf.contrib.signal.frame does.", "If by grouping, you mean creating a higher-dimensional element that groups input elements, then the simplest use case can be addressed by using `batch`. For instance, `tf.data.Dataset.range(4).batch(2)` produces `{[0, 1], [2, 3]}`. \r\n\r\nIf you would like to customize the shift and stride used for grouping the elements, you can do so by the virtue of `window(size, shift, stride, drop_remainder).flat_map(lambda x: x.batch(size))`. For instance, `tf.data.Dataset.range(4).window(2, 1, 2, True).flat_map(lambda x: x.batch(2))` produces `{[0, 2], [1, 3]}`.\r\n\r\nIf I misunderstood your question, please provide an example of the input sequence and the desired output sequence.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to inactivity.", "@jsimsa Hey Jiri, thanks for the answers above. But I'm getting an error when I'm trying to do the same on a dictionary-like dataset:\r\n```\r\ntf.data.Dataset.from_tensor_slices({\"a\": [1,2,3,4]}).window(3).flat_map(lambda x: x.batch(3))\r\n```\r\n\r\nIt'll give me the error of ```AttributeError: 'dict' object has no attribute 'batch'```\r\n\r\nDo you know if there's a way to do it on a dictionary-like tf.data.Dataset?\r\n\r\nThanks", "For a dataset of nested elements `window` creates a dataset of nested datasets of flat elements, not a dataset of datasets of nested elements.\r\n\r\nIn other words, the `x` in your `lambda` will be a dictionary mapping keys to datasets and if you wish to flatten it to a single dataset you will need to do:\r\n\r\n```\r\ndef map_fn(x):\r\n  result = {}\r\n  for key in x.keys():\r\n    result[key] = tf.data.experimental.get_single_element(x[key].batch(3))\r\n  return result\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices({\"a\": [1,2,3,4]}).window(3).map(map_fn)\r\n```\r\n", "@jsimsa Hey Jiri, thank you very much for the prompt reply. But I still got the same error even with \"tf.data.experimental.get_single_element\" function:\r\n\r\nI'm using tensorflow 1.14 and python3.\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 9, in <module>\r\n    tf.data.Dataset.from_tensor_slices({\"a\": [1,2,3,4]}).window(3).flat_map(map_fn)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1817, in flat_map\r\n    return DatasetV1Adapter(super(DatasetV1, self).flat_map(map_func))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1160, in flat_map\r\n    return FlatMapDataset(self, map_func)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3264, in __init__\r\n    type(self._map_func.output_structure)))\r\nTypeError: `map_func` must return a `Dataset` object. Got <class 'tensorflow.python.data.util.structure.NestedStructure'>\r\n\r\n```", "Try again, I have made changes to the example code and made sure it runs.", "@jsimsa Ah, got you. It works now with the tf.data.Dataset.map() function. Thanks for your help Jiri!"]}, {"number": 23580, "title": "RendezvousMgr & CancellationMgr are already aborted, shouldn't send R\u2026", "body": "\u2026PC call any more\r\n\r\nWhen RendezvousMgr & CancellationMgr are already aborted, following RPC couldn't handled\r\nby CancellationMgr. At the moment, if remote service is already closed, client would hang here.", "comments": ["@mrry  GPU check has been failed. Do you want me to proceed with \"ready to pull\" or this check is required to proceed ? Please keep me posted. ", "@harshini-gadige Do you have a link to the test failures?", "> @harshini-gadige Do you have a link to the test failures?\r\n\r\nPlease try if the below link helps to check the failure.\r\n\r\n[Target log](https://source.cloud.google.com/results/invocations/39a642ff-5ab0-4890-bbdd-a044febb3981/targets/%2F%2Ftensorflow%2Ftools%2Fci_build%2Fbuilds:gen_win_out/log)", "@harshini-gadige That failure looks unrelated, so this is ready to pull."]}, {"number": 23579, "title": "RendezvousMgr & CancellationMgr are already aborted, shouldn't send R\u2026", "body": "\u2026PC call any more\r\n\r\nWhen RendezvousMgr & CancellationMgr are already aborted, following RPC couldn't handled\r\nby CancellationMgr. At the moment, if remote service is already closed, client would hang here.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 23578, "title": "tensorflow performance bottleneck o IteratorGetNext", "body": "", "comments": []}, {"number": 23577, "title": "Install abseil headers to cmake shared library build", "body": " * Since commit 5f004516 tensorflow::StringPiece is replaced by\r\n   absl::string_view, so abseil headers should be installed to shared\r\n   library build to fix compilation error for out-of-source build.\r\n * To cleanly copy abseil headers, disable in source build for abseil to\r\n   avoid src tree been polluted by cmake generated files.\r\n * Meanwhile, remove _build suffix from abseil_cpp product name since\r\n   it's confusing.", "comments": []}, {"number": 23576, "title": "TOCO Runtime error: Check failed: array_copy_size[0] != 0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.11\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.15\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9\r\n- GPU model and memory: Titan XP\r\n\r\nConvert frozen graph pb file to tflite, but failed due to some low level problem.\r\n\r\n** Error Message **\r\n\r\nRuntimeError: TOCO failed see console for info.\r\nb'2018-11-07 13:26:38.105273: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 283 operators, 446 arrays (0 quantized)\r\n2018-11-07 13:26:38.111622: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 283 operators, 446 arrays (0 quantized)\r\n2018-11-07 13:26:38.431863: F tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:53] Check failed: array_copy_size[0] != 0 (0 vs. 0)\r\nAborted (core dumped)'\r\n\r\nI am not sure how this happens, I am following all tutorials to convert, the pb file can be read and run. However I am failed to convert pb to tflite.", "comments": ["Can you please point me to the tutorial you are trying?", "freeze graph: https://gist.github.com/omimo/5d393ed5b64d2ca0c591e4da04af6009\r\n\r\ntflite: https://stackoverflow.com/questions/49474549/can-i-convert-the-tensorflow-inception-pb-model-to-tflite-model\r\n\r\nthen i come out with below code to save the graph in pb format, when i restore, the pb file works perfectly\r\n```\r\nwith self.sess.as_default():\r\n\r\n            frozen_graph_def = tf.graph_util.convert_variables_to_constants(\r\n                self.sess,\r\n                self.graph.as_graph_def(),\r\n                ['output']\r\n            )\r\n\r\n            for node in frozen_graph_def.node:\r\n                print(type(node), node.name)\r\n            with open(filepath, 'wb') as f:\r\n                f.write(frozen_graph_def.SerializeToString())\r\n```\r\n\r\nhowever, when i convert to tflite, the error happens, the below code is my way to convert to tflite\r\n\r\n```\r\nimport os\r\n\r\nimport tensorflow as tf\r\n\r\ngraph_def_file = \"frozen_graph.pb\"\r\ninput_arrays = [\"input\"]\r\noutput_arrays = [\"output\"]\r\noutput_name = '{}.tflite'.format(os.path.basename(graph_def_file).split('.')[0])\r\n\r\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(\r\n  graph_def_file, input_arrays, output_arrays)\r\nconverter.allow_custom_ops = True\r\ntflite_model = converter.convert()\r\n\r\nopen(output_name, \"wb\").write(tflite_model)\r\n```\r\nI thought there is something wrong with the python code, so I try command as well\r\n```\r\n#!/usr/bin/env bash\r\n\r\ntoco --input_file=frozen_graph.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=frozen_graph.tflite \\\r\n--inference_type=FLOAT \\\r\n--input_type=FLOAT \\\r\n--input_arrays='input' \\\r\n--output_arrays='output' \\\r\n--input_shapes=1,400,400,3\r\n```\r\n\r\nboth way return the same error", "I debug my code one line by one line by checking which tensor have error output, then i found out this\r\n\r\n```\r\n# conf_data is [1, num_priors, 2] -> [1, 42082, 2]\r\nconf_preds = tf.reshape(conf_data, (num_priors, self.num_classes)) # -> [42082, 2]\r\nprint(conf_preds)\r\nconf_preds = tf.transpose(conf_preds, (1, 0)) -> [2, 42082]\r\nprint(conf_preds)\r\nconf_preds = tf.nn.softmax(conf_preds, 0) \r\nreturn tf.identify(conf_preds, name='output')\r\n# ...more codes...\r\n```\r\nthen i get `Check failed: array_copy_size[0] != 0 (0 vs. 0)`\r\n\r\nwhen i change axis of softmax to 1 `tf.nn.softmax(conf_preds, 1)` , then i get `Check failed: axis < input_shape.dimensions_count() (97288360 vs. 3)`\r\n\r\nwhen i remove the softmax line, then i get `axis < input_shape.dimensions_count() (71013160 vs. 3)`\r\n\r\nI don't really understand why removing softmax will have different dimensions_count (which output is `tf.transpose`)\r\n\r\nwhen i remove the transpose line (which output is `tf.reshape`)\r\n\r\nthen tflite convert successful.\r\n\r\nhope my information can help debugging.\r\n\r\ncode to reproduce the error\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass TestModel(object):\r\n    def __init__(self):\r\n        self.conv1 = tf.layers.Conv2D(64, 3, padding='same', activation='relu')  # 32, 32\r\n        self.maxpool1 = tf.layers.MaxPooling2D(2, 2)  # 16, 16\r\n\r\n        self.conv2 = tf.layers.Conv2D(64, 3, padding='same', activation='relu')  # 16, 16\r\n        self.maxpool2 = tf.layers.MaxPooling2D(2, 2)  # 8, 8\r\n\r\n        self.conv3 = tf.layers.Conv2D(64, 3, padding='same', activation='relu')  # 8, 8\r\n        self.maxpool3 = tf.layers.MaxPooling2D(2, 2)  # 4, 4\r\n\r\n        self.flatten = tf.layers.Flatten()\r\n        self.fc1 = tf.layers.Dense(10)\r\n\r\n    def __call__(self, input_placeholder):\r\n        out = self.conv1(input_placeholder)\r\n        out = self.maxpool1(out)\r\n        out = self.conv2(out)\r\n        out = self.maxpool2(out)\r\n        out = self.conv3(out)\r\n        out = self.maxpool3(out)\r\n        out = self.flatten(out)\r\n        out = self.fc1(out)\r\n        out = tf.concat((out, out), 1)\r\n        print(out)\r\n        out = tf.reshape(out, (10, 2))\r\n        print(out)\r\n        out = tf.transpose(out, (1, 0))\r\n        print(out)\r\n        out = tf.nn.softmax(out, 0)\r\n        print(out)\r\n        out = tf.identity(out, name='output')\r\n        return out\r\n\r\n\r\nclass TestRunner(object):\r\n    def __init__(self):\r\n        self.graph = tf.Graph()\r\n        self.sess = tf.Session(graph=self.graph)\r\n\r\n        with self.graph.as_default():\r\n            with self.sess.as_default():\r\n                self.model = TestModel()\r\n                self.input_placeholder = tf.placeholder(tf.float32, (1, 32, 32, 3), name='input')\r\n                self.output_node = self.model(self.input_placeholder)\r\n\r\n                self.sess.run(tf.global_variables_initializer())\r\n\r\n    def test_convert_tflite(self):\r\n        print('loading from session')\r\n        converter = tf.contrib.lite.TocoConverter.from_session(self.sess, [self.input_placeholder], [self.output_node])\r\n        print('converting to tflite')\r\n        tflite_model = converter.convert()\r\n        # print(tflite_model)\r\n\r\n\r\n    def test_run(self):\r\n        inputs = np.random.randn(1, 32, 32, 3)\r\n        outputs = self.sess.run(self.output_node, feed_dict={self.input_placeholder: inputs})\r\n        print(outputs.shape)\r\n\r\n\r\nif __name__ == '__main__':\r\n    tr = TestRunner()\r\n    tr.test_run()\r\n    tr.test_convert_tflite()\r\n```\r\noutput\r\n\r\nRuntimeError: TOCO failed see console for info.\r\nb'2018-11-08 17:12:34.536423: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 45 operators, 79 arrays (0 quantized)\\n2018-11-08 17:12:34.536829: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 45 operators, 79 arrays (0 quantized)\\n2018-11-08 17:12:34.537445: F tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:53] Check failed: array_copy_size[0] != 0 (0 vs. 0)\\nAborted (core dumped)\\n'\r\nNone\r\n", "seems like softmax with axis input will cause the error\r\n\r\nI try to put softmax before transpose, tflite convert without errors\r\n\r\n```\r\n    def __call__(self, input_placeholder):\r\n        out = self.conv1(input_placeholder)\r\n        out = self.maxpool1(out)\r\n        out = self.conv2(out)\r\n        out = self.maxpool2(out)\r\n        out = self.conv3(out)\r\n        out = self.maxpool3(out)\r\n        out = self.flatten(out)\r\n        out = self.fc1(out)\r\n        out = tf.concat((out, out), 1)\r\n        print(out)\r\n        out = tf.reshape(out, (10, 2))\r\n        print(out)\r\n        out = tf.nn.softmax(out)\r\n        print(out)\r\n        out = tf.transpose(out, (1, 0))\r\n        print(out)\r\n        out = tf.identity(out, name='output')\r\n        return out\r\n```\r\n", "Yes, most likely softmax is causing the issue. Please refer to: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tf_ops_compatibility.md\r\n\r\n> \"tf.nn.softmax - as long as tensors are 2D and axis is the last dimension\"\r\n\r\nPlease reopen if that's not the cause.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23575, "title": " The  compile error is to buid tensorflow1.4 source code in vs15", "body": " Cmake tensorflow1.4 source code,but the error is unable to open file  to tensorflow.def  in vs15 windows\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23574, "title": "Ubuntu 18.04 bazel build from source fails", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution:   \r\nDescription:\tUbuntu 18.04.1 LTS\r\nRelease:\t18.04\r\nCodename:\tbionic\r\n\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): bazel release 0.18.1\r\n- GCC/Compiler version (if compiling from source): gcc-6\r\n- CUDA/cuDNN version: CUDA-9.1, cuDNN-7.1\r\n- GPU model and memory: NVIDIA Corporation GP106M [GeForce GTX 1060 Mobile] (rev a1)\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuilding from source, fails on executing bazel build instruction.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI'm trying to build tensorflow from source on my system, following https://medium.com/@asmello/how-to-install-tensorflow-cuda-9-1-into-ubuntu-18-04-b645e769f01d as my OS is ubuntu 18.04, following these steps I ran the ./configure and then \"bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\" to build.\r\nbuild goes on for a couple hours, everything seems fine until the very end when I get this error\r\n\r\n**Any other info / logs**\r\nERROR: /home/armaan/software/tensorflow/tensorflow/BUILD:561:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN6icu_608ByteSinkE\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN6icu_608ByteSinkE\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 6680.706s, Critical Path: 297.19s\r\nINFO: 14471 processes: 14471 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["I believe the TF team is compiling 1.12.0 for Cuda 9.2.  Maybe there's some incompatibility with TF1.12 and Cuda 9.1??  I'm not sure your specific issue but I have a similar setup and I'm using Cuda 10.0 (and the associated cuDNN 7.3.1) installed from the official NVidia download, see [Cuda 10](https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804).  I've had issues in the past installing earlier gcc versions to overcome the Cuda compatibility issues.  This package is setup for 18.04 and so it works with gcc 7.0 (the default compiler).  No need to install gcc 6.\r\n\r\nFor my Ubuntu 18.04 (python3 and bazel 18) with Cuda 10.0 with TF 1.12.0 compiling for source worked great.", "@ArmaanBhullar  -  Please use Bazel 0.15.0 and GCC 4.8. Feel free to close if this issue no longer exists. Thanks !\r\n\r\n\r\n\r\n\r\n", "@harshini-gadige  thanks, I'll retry using Bazel 0.15.0 and GCC 4.8, and close this issues if it works :)\r\nbut just curious, is there any documentation which says to use these particular Bazel and GCC versions?", "@harshini-gadige  I tried with the gcc and bazel versions you specified but faced the following error:\r\nERROR: /home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/external/com_google_absl/absl/base/BUILD.bazel:58:1: C++ compilation of rule '@com_google_absl//absl/base:dynamic_annotations' failed (Exit 1)\r\ngcc-4.8: error trying to exec 'cc1plus': execvp: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.", "@gunan  PTAL", "Here is a ubuntu 18.10 build (kernal 4.18), tensorflow r1.12 branch, python 3.6, CUDA 10.0, cuDNN 7.4.1.5, NCCL 2.3.7, tensorRT 5.0, compute capability 7.5, 6.1, for my RTX 2080 TI.\r\n[tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl](https://app.box.com/s/jcw8uq6jannpg4myzighl166bq1zg07e)\r\n\r\nIt took me around 2 full day to get it built, so many issue among bazel version (I used 0.18.1 it works), python (3.6.7), gcc g++(I used 7.3), etc too many", "@MingyaoLiu  Great and thanks for your support.\r\n\r\n@ArmaanBhullar  -  Any update after using @MingyaoLiu  build ?", "In \"awaiting response\" status for more than 3 days. Hence closing this. Please post the updates here if any, we will reopen the issue. Thanks !", "@ArmaanBhullar did you ever resolve your issue?   I have a very similar setup to yours, and the @MingyaoLiu build isn't an option. (In fact, your initial setup is almost identical to \r\nmine now, one I can't stray too far away from)\r\n\r\nBecause I need a very specific driver version for my GTX 1060, I can not go beyond CUDA 9.1, which also fixes the cuDNN I'm able to use at 7.1", "I got the similar issue:\r\n```\r\nERROR: /home/charles/tensorflow-installation/tensorflow/tensorflow/BUILD:533:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/charles/.cache/bazel/_bazel_charles/f8def8956ae05eead7ef4aca6059934c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/charles/.cache/bazel/_bazel_charles/f8def8956ae05eead7ef4aca6059934c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/charles/.cache/bazel/_bazel_charles/f8def8956ae05eead7ef4aca6059934c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/opt/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/opt/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/charles/.cache/bazel/_bazel_charles/f8def8956ae05eead7ef4aca6059934c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTVN6icu_609ErrorCodeE\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/charles/.cache/bazel/_bazel_charles/f8def8956ae05eead7ef4aca6059934c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/charles/.cache/bazel/_bazel_charles/f8def8956ae05eead7ef4aca6059934c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/charles/.cache/bazel/_bazel_charles/f8def8956ae05eead7ef4aca6059934c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/charles/.cache/bazel/_bazel_charles/f8def8956ae05eead7ef4aca6059934c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/charles/.cache/bazel/_bazel_charles/f8def8956ae05eead7ef4aca6059934c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/charles/.cache/bazel/_bazel_charles/f8def8956ae05eead7ef4aca6059934c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/opt/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/opt/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/charles/.cache/bazel/_bazel_charles/f8def8956ae05eead7ef4aca6059934c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTVN6icu_609ErrorCodeE\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 17185.854s, Critical Path: 1003.10s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 13895 processes: 13895 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nI am using Bazel 0.15, GCC 4.8 to build TF 1.12.2. I have tried many times, but the problem remains.\r\nIt is frustrating. Did you ever solve the problem? @nathanielhobbs @nathanielhobbs "]}, {"number": 23573, "title": "How can I fix \"adding visible gpu devises: 0\"", "body": "Just fixed one problem and now it is refusing to acknowledge that I have a gpu or smth.\r\nOn top of that it is not doing anything. Any fixes?\r\n\r\nUsing Windows 10\r\nCuda 9\r\ncuDNN 7\r\nGPU - gtx 1080, 8gb\r\neGPU - razer core x\r\nPython 3.5.3\r\n\r\n\r\nUsing TensorFlow backend.\r\n2018-11-06 22:16:20.031851: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-11-06 22:16:20.718059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:07:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.59GiB\r\n2018-11-06 22:16:20.718927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n\r\n\r\nSolutions tried: \r\n1. uninstall tensorflow-gpu\r\n   install tensorflow-gpu\r\n\r\n2.uninstall tensorflow-gpu\r\n   uninstall protobuf\r\n   install tensorflow-gpu\r\n3. Repeat the previous two couple more times (got desperate) ", "comments": ["Its simply stating that it has found 1 available GPU. It is acknowledging the presence of your gpu.", "Anyways, thanks for your help and quick responses @ymodak", "i added below codes(refer to https://docs.google.com/presentation/d/1iO_bBL_5REuDQ7RJ2F35vH2BxAiGMocLC6t_N-6eXaE/edit#slide=id.g1df700e686_0_13), this phenomenon seems disappeared, i do not know the reason \r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}]