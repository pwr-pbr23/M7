[{"number": 22771, "title": "Use of bias term in tensorflow.keras conv3Dtranspose layer breaks if layer size is not defined", "body": "### System information\r\n\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, Colab\r\n    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n    TensorFlow installed from (source or binary): binary\r\n    TensorFlow version (use command below): 1.10 / 1.11\r\n    Python version: 3.6.x\r\n    Bazel version (if compiling from source): NA\r\n    GCC/Compiler version (if compiling from source): NA\r\n    CUDA/cuDNN version: n/a\r\n    GPU model and memory: n/a\r\n    Exact command to reproduce: see code below\r\n\r\n### Describe the problem\r\nIf the input layer size is None for a conv3Dtranspose layer the calculation of the bias term emits an error because it reshapes the tensor to be multiplied by the bias, even when just initializing with no input.  Note that this is supported for the 2D layer.\r\n\r\n### Source code / logs\r\nhttps://colab.research.google.com/gist/str4w/d60705ac54c5574f67ff2ec15cf89b42/conv_transpose_problem.ipynb\r\n\r\n```\r\n# use these to show tensorflow.keras issue\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.layers as kl\r\nimport tensorflow.keras.models as km\r\nimport tensorflow.keras.backend as K\r\nprint(tf.__version__)\r\nprint(keras.__version__)\r\n\r\nimport numpy as np\r\n\r\n\r\n# In 2d, the conv2dtranspose layer works fine with bias.\r\n\r\ninp=kl.Input(shape=(None,None,1))\r\nx=kl.Conv2D(1,kernel_size=5,padding='same')(inp)\r\nx=kl.MaxPooling2D(pool_size=2)(x)\r\nx=tf.layers.Conv2DTranspose(1,kernel_size=1,strides=2,use_bias=True)(x)\r\n#x=kl.Conv2DTranspose(1,kernel_size=1,strides=2)(x)\r\nmodel=km.Model(inputs=inp,outputs=x)\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,\r\n              optimizer=keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\nmodel.summary()\r\ndata2D=np.random.random_sample((1,42,74,1))\r\nZ2=model.predict(data2D)\r\nassert(Z2.shape == data2D.shape)\r\n\r\n\r\n# in 3d, the bias term has issues\r\n# Setting use_bias to false will allow this to pass\r\ninp=kl.Input(shape=(None,None,None,1))\r\nx=kl.Conv3D(1,kernel_size=5,padding='same')(inp)\r\nx=kl.MaxPooling3D(pool_size=2)(x)\r\nx=kl.Conv3DTranspose(1,kernel_size=1,strides=2,use_bias=True)(x)\r\n#x=kl.Conv3DTranspose(1,kernel_size=1,strides=2)(x)\r\nmodel=km.Model(inputs=inp,outputs=x)\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,\r\n              optimizer=keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\nmodel.summary()\r\n\r\ndata3D=np.random.random_sample((1,42,74,34,1))\r\nZ3=model.predict(data3D)\r\nassert(Z3.shape == data3D.shape)\r\n```\r\nOutput:\r\n```\r\n1.11.0\r\n2.1.6-tf\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, None, None, 1)     0         \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, None, None, 1)     26        \r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, None, None, 1)     0         \r\n_________________________________________________________________\r\nconv2d_transpose_1 (Conv2DTr (None, None, None, 1)     2         \r\n=================================================================\r\nTotal params: 28\r\nTrainable params: 28\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-ff267c7f58e9> in <module>()\r\n     32 x=kl.Conv3D(1,kernel_size=5,padding='same')(inp)\r\n     33 x=kl.MaxPooling3D(pool_size=2)(x)\r\n---> 34 x=kl.Conv3DTranspose(1,kernel_size=1,strides=2,use_bias=True)(x)\r\n     35 #x=kl.Conv3DTranspose(1,kernel_size=1,strides=2)(x)\r\n     36 model=km.Model(inputs=inp,outputs=x)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    767 \r\n    768       if not in_deferred_mode:\r\n--> 769         outputs = self.call(inputs, *args, **kwargs)\r\n    770         if outputs is None:\r\n    771           raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)\r\n   1064       else:\r\n   1065         outputs_4d = array_ops.reshape(outputs, [\r\n-> 1066             outputs_shape[0], outputs_shape[1] * outputs_shape[2],\r\n   1067             outputs_shape[3], outputs_shape[4]\r\n   1068         ])\r\n\r\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'\r\n```\r\n\r\nPinging: @nuance-research", "comments": ["@yongtang I think we can fix the issue easily after #22127 is merged . How do you think?", "@facaiy Yes I rebased #22127 to resolve the merge conflict. Hopefully it will be merged soon.", "I think the issue has been fixed in #23004. Feel free to reopen it if have any additional questions. Thanks.", "Hi, I have a got a new error like this:\r\nMy codes are:\r\ndef autoencoder1(file_length):\r\n\r\n\r\n    input_img = keras.layers.Input( shape=( None, len(params2), len(params2), 1 ) ) # adapt this if using `channels_first` image data format\r\n\r\n    conv1 = keras.layers.Conv3D(filters=32, kernel_size=(1, 3, 3), strides=(1, 1, 1), activation='selu', padding='same', data_format='channels_last', name='conv1')(input_img)\r\n    conv2 = keras.layers.Conv3D(filters=64, kernel_size=(1, 3, 3), strides=(1, 2, 2), activation='selu', padding='same', data_format='channels_last', name='conv2')(conv1)\r\n    conv3 = keras.layers.Conv3D(filters=128,kernel_size=(1, 3, 3), strides=(1, 2, 2), activation='selu', padding='same', data_format='channels_last', name='conv3')(conv2)\r\n    conv4 = keras.layers.Conv3D(filters=256,kernel_size=(1, 3, 3), strides=(1, 2, 2), activation='selu', padding='same', data_format='channels_last', name='conv4')(conv3)\r\n\r\n    convlstm1 = keras.layers.ConvLSTM2D(filters=32, return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm1')(conv1)\r\n    convlstm2 = keras.layers.ConvLSTM2D(filters=64, return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm2')(conv2)\r\n    convlstm3 = keras.layers.ConvLSTM2D(filters=128,return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm3')(conv3)\r\n    convlstm4 = keras.layers.ConvLSTM2D(filters=256,return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm4')(conv4)\r\n    \r\n    deconv4 = keras.layers.Conv3DTranspose(filters=128, kernel_size=(1, 2, 2), strides=(1, 2, 2), activation='selu', padding='valid', output_padding=(0, -1, -1), data_format='channels_last', name='deconv4')(convlstm4)\r\n    concat4 = keras.layers.Concatenate(axis=4, name='concat4')([convlstm3, deconv4])\r\n    deconv3 = keras.layers.Conv3DTranspose(filters=64 , kernel_size=(1, 2, 2), strides=(1, 2, 2), activation='selu', padding='valid', output_padding=(0, -1, -1), data_format='channels_last', name='deconv3')(concat4)\r\n    concat3 = keras.layers.Concatenate(axis=4, name='concat3')([convlstm2, deconv3])\r\n    deconv2 = keras.layers.Conv3DTranspose(filters=32 , kernel_size=(1, 3, 3), strides=(1, 2, 2), activation='selu', padding='same' , data_format='channels_last', name='deconv2')(concat3)\r\n    concat2 = keras.layers.Concatenate(axis=4, name='concat2')([convlstm1, deconv2])\r\n    deconv1 = keras.layers.Conv3DTranspose(filters=1  , kernel_size=(1, 3, 3), strides=(1, 1, 1), activation='selu', padding='same' , data_format='channels_last', name='deconv1')(concat2)\r\n\r\n    autoencoder = keras.models.Model(input_img, deconv1)\r\n    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\r\n    autoencoder.summary()\r\n    \r\n    return autoencoder\r\n\r\nautoencoder1(1700)\r\n\r\nand here's the error message:\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-152-83622ca7a77b> in <module>\r\n     28     return autoencoder\r\n     29 \r\n---> 30 autoencoder1(1700)\r\n\r\n<ipython-input-152-83622ca7a77b> in autoencoder1(file_length)\r\n     14     convlstm4 = keras.layers.ConvLSTM2D(filters=256,return_sequences=True, kernel_size=(3, 3), strides=(1, 1), activation='selu', padding='same', data_format='channels_last', name='convlstm4')(conv4)\r\n     15 \r\n---> 16     deconv4 = keras.layers.Conv3DTranspose(filters=128, kernel_size=(1, 2, 2), strides=(1, 2, 2), activation='selu', padding='valid', output_padding=(0, -1, -1), data_format='channels_last', name='deconv4')(convlstm4)\r\n     17     concat4 = keras.layers.Concatenate(axis=4, name='concat4')([convlstm3, deconv4])\r\n     18     deconv3 = keras.layers.Conv3DTranspose(filters=64 , kernel_size=(1, 2, 2), strides=(1, 2, 2), activation='selu', padding='valid', output_padding=(0, -1, -1), data_format='channels_last', name='deconv3')(concat4)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    552             # In graph mode, failure to build the layer's graph\r\n    553             # implies a user-side bug. We don't catch exceptions.\r\n--> 554             outputs = self.call(inputs, *args, **kwargs)\r\n    555           else:\r\n    556             try:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)\r\n   1138       else:\r\n   1139         outputs_4d = array_ops.reshape(outputs, [\r\n-> 1140             outputs_shape[0], outputs_shape[1] * outputs_shape[2],\r\n   1141             outputs_shape[3], outputs_shape[4]\r\n   1142         ])\r\n\r\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'int'\r\n\r\nIt's April 2019 and I think there are still problems in this way.", "Hi, please create a new issue and fill out all necessary information?"]}, {"number": 22769, "title": "Different weights initializations for different values of `alignment_history` in `tf.contrib.seq2seq.AttentionWrapper` (fixed graph random seed)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -\r\n- **TensorFlow installed from (source or binary)**: binary (wheel)\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 2.7.15\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: tested on CPU\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: code below\r\n\r\n### Describe the problem\r\nThe initialization of the weights of the tensors in the decoder change when\r\nchanging the `alignment_history` parameter in `tf.contrib.seq2seq.AttentionWrapper`\r\n(`True` or `False`). This occurs even if the graph's random seed is fixed\r\n(`tf.set_random_seed(1)`).\r\n\r\n### Source code / logs\r\nBelow, `create_graph` creates the graph consisting of a decoder with Bahdanau\r\nattention and `test_attention_wrapper` creates a session in order to check the\r\nvalues of the weight matrices at initialization.\r\n```python\r\n#! /usr/bin/env\r\n# encoding: utf-8\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef create_graph(memory,\r\n                 memory_sequence_length,\r\n                 batch_size=1,\r\n                 vocab_size=30,\r\n                 num_units=16,\r\n                 attention_layer_size=16,\r\n                 attention_alignment_history=False,\r\n                 name=\"decoder\"):\r\n\r\n    with tf.variable_scope(name) as scope:\r\n        decoder_cell = tf.contrib.rnn.LSTMCell(num_units, name=\"lstm\")\r\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\r\n            attention_layer_size,\r\n            memory,\r\n            memory_sequence_length)\r\n        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n            decoder_cell,\r\n            attention_mechanism,\r\n            attention_layer_size=attention_layer_size,\r\n            alignment_history=attention_alignment_history,\r\n            name=\"attention\")\r\n\r\n        decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32)\r\n\r\n        sos_id = tf.cast(vocab_size-2, tf.int32)\r\n        # End token id\r\n        eos_id = tf.cast(vocab_size-1, tf.int32)\r\n\r\n        # Start tokens for the batch\r\n        start_tokens = tf.fill([batch_size], sos_id)\r\n        end_token = eos_id\r\n\r\n        embedding = lambda ids: tf.one_hot(ids, vocab_size)\r\n        # Helper for the decoder (performs sampling)\r\n        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\r\n            embedding,\r\n            start_tokens,\r\n            end_token\r\n            )\r\n\r\n        # Decoder\r\n        decoder = tf.contrib.seq2seq.BasicDecoder(\r\n            cell=decoder_cell,\r\n            helper=helper,\r\n            initial_state=decoder_initial_state,\r\n            output_layer=None\r\n            )\r\n\r\n        final_outputs, final_state, _ = tf.contrib.seq2seq.dynamic_decode(\r\n            decoder,\r\n            scope=scope\r\n            )\r\n\r\n\r\ndef test_attention_wrapper(batch, max_time, num_units, memory_arr,\r\n                           memory_sequence_length_arr,\r\n                           attention_alignment_history=False,\r\n                           name=\"decoder\"):\r\n    # Set the random graph seed\r\n    tf.set_random_seed(1)\r\n    with tf.Session() as sess:\r\n        memory = tf.placeholder(tf.float32, [batch, max_time, num_units],\r\n                                name=\"memory\")\r\n        memory_sequence_length = tf.placeholder(tf.int32, [batch],\r\n                                                name=\"memory_sequence_length\")\r\n        create_graph(memory, memory_sequence_length,\r\n                     attention_alignment_history=attention_alignment_history)\r\n        # Initialize the variables\r\n        variables = tf.VariableScope(None, name=\"decoder\").global_variables()\r\n        sess.run([v.initializer for v in variables])\r\n\r\n        variables_out = sess.run(\r\n            variables,\r\n            feed_dict={memory: memory_arr,\r\n                       memory_sequence_length: memory_sequence_length_arr}\r\n            )\r\n\r\n    return variables_out, variables\r\n```\r\n\r\nThe full code used for reproducing the issue can be found below:\r\n\r\n```python\r\n#! /usr/bin/env\r\n# encoding: utf-8\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Parameters of the graph\r\nbatch = 1\r\nmax_time = 3\r\nnum_units = 1\r\nmemory_arr = np.ones((batch, max_time, num_units))\r\nmemory_sequence_length_arr = max_time * np.ones(batch)\r\n\r\n# Create two graphs (with and without attention history)\r\ntf.reset_default_graph()\r\nvariables_no, variables = test_attention_wrapper(\r\n    batch, max_time, num_units, memory_arr,\r\n    memory_sequence_length_arr,\r\n    attention_alignment_history=False)\r\ntf.reset_default_graph()\r\nvariables_yes, variables = test_attention_wrapper(\r\n    batch, max_time, num_units, memory_arr,\r\n    memory_sequence_length_arr,\r\n    attention_alignment_history=True)\r\n\r\nfor i, (var_out_no, var_out_yes) in enumerate(zip(variables_no, variables_yes)):\r\n    var_name = variables[i].name\r\n    print(var_name)\r\n    print(\"-\" * len(var_name))\r\n#     print \"NO:\", var_out_no\r\n#     print \"YES:\", var_out_yes\r\n\r\n    print \"--> The arrays are equal:\", np.array_equal(var_out_no, var_out_yes)\r\n    print(\"\\n\")\r\n```\r\n\r\nThe output obtained is the following:\r\n\r\n```\r\ndecoder/memory_layer/kernel:0\r\n-----------------------------\r\n--> The arrays are equal: True\r\n\r\n\r\ndecoder/attention/lstm/kernel:0\r\n-------------------------------\r\n--> The arrays are equal: False\r\n\r\n\r\ndecoder/attention/lstm/bias:0\r\n-----------------------------\r\n--> The arrays are equal: True\r\n\r\n\r\ndecoder/attention/bahdanau_attention/query_layer/kernel:0\r\n---------------------------------------------------------\r\n--> The arrays are equal: False\r\n\r\n\r\ndecoder/attention/bahdanau_attention/attention_v:0\r\n--------------------------------------------------\r\n--> The arrays are equal: False\r\n\r\n\r\ndecoder/attention/attention_layer/kernel:0\r\n------------------------------------------\r\n--> The arrays are equal: False\r\n\r\n```\r\n\r\nMore precisely, here are the two different weights for the LSTM cell (`LAS/decoder/attention/multi_rnn_cell/cell_0/lstm_cell/kernel:0`) obtained in each of the two cases for `alignment_history`:\r\n\r\n- `alignment_history = False`:\r\n\r\n```\r\n[[  1.93134502e-01   5.19581586e-02  -2.18084604e-01 ...,   1.71998128e-01\r\n    2.81669050e-02  -1.70363069e-01]\r\n [  1.95658550e-01   7.72098750e-02   2.17303529e-01 ...,   2.07259521e-01\r\n    5.78315109e-02  -1.17801599e-01]\r\n [ -6.88283443e-02  -1.35597080e-01   1.65620014e-01 ...,  -1.32969454e-01\r\n   -2.11523965e-01  -1.86820269e-01]\r\n ..., \r\n [ -1.15171000e-01  -8.01331848e-02   1.30112335e-01 ...,   2.73928046e-04\r\n    1.80437252e-01   1.90824643e-01]\r\n [ -7.53998458e-02   1.86289057e-01   1.80155411e-01 ...,  -1.64348409e-01\r\n    2.10424170e-01  -1.46689758e-01]\r\n [ -2.08476633e-02   7.97681957e-02  -2.03553244e-01 ...,  -1.91280752e-01\r\n    8.57728869e-02   1.46612525e-04]]\r\n```\r\n\r\n- `alignment_history = True`:\r\n\r\n```\r\n[[-0.18967885 -0.20777287  0.03707646 ...,  0.17251725  0.20710425\r\n  -0.03941825]\r\n [-0.13857554 -0.05786179  0.17680736 ...,  0.01303713  0.05177127\r\n  -0.12967519]\r\n [-0.18074478 -0.09467114  0.09963275 ..., -0.11447592  0.19544493\r\n  -0.19714527]\r\n ..., \r\n [ 0.16114177  0.14009587  0.11265792 ..., -0.11863185 -0.08480376\r\n  -0.19559079]\r\n [-0.17472327  0.11717187  0.21214487 ...,  0.17373656 -0.15397248\r\n   0.04700263]\r\n [-0.05792974  0.18947266  0.06573398 ..., -0.0308952   0.18018191\r\n  -0.20467089]]\r\n```", "comments": ["@lenassero My understanding is that `alignment_history` is integrated into seq2seq model with attention, \r\n`tf.contrib.seq2seq.BahdanauAttention` to be specific in this case. Therefore it would impact the output of `tf.contrib.seq2seq.AttentionWrapper` which is supposed to be the alignments for each step.", "Hi @wt-huang, thank you for your response! \r\n\r\nWell, the `alignment_history` does change the `decoder_cell` used by the decoder but it should not impact the weights to my knowledge. One can find in the `call` method of `AttentionWrapper`:\r\n\r\n```python\r\n...\r\n    next_state = AttentionWrapperState(\r\n        time=state.time + 1,\r\n        cell_state=next_cell_state,\r\n        attention=attention,\r\n        attention_state=self._item_or_tuple(all_attention_states),\r\n        alignments=self._item_or_tuple(all_alignments),\r\n        alignment_history=self._item_or_tuple(maybe_all_histories))\r\n\r\n    if self._output_attention:\r\n      return attention, next_state\r\n    else:\r\n      return cell_output, next_state\r\n```\r\n\r\nBasically, `alignment_history` in the named tuple `next_state` is either empty (`alignment_history = False`) or it stores the history of alignments (`alignment_history = True`). To me, this should not change the way the graph is initialized, and thus the weights at initialization as I mentioned..", "@lenassero There are several variables in `docoder` as shown in the output of the code snippet. The first variable `decoder/memory_layer/kernel:0` is identical between `alignment_history=True` and `alignment_history=False`. The weights should be set by default values which leads to identical results in memory. However, for `attention` variables and layers, the values are different as decoder cell is different. Note that `decoder/attention/lstm/bias:0` are set to 0 in most cases.\r\n\r\n", "Closing, feel free to reopen if additional issues surface."]}, {"number": 22768, "title": "remove some blank lines in README.md", "body": "remove the ending blank lines in readme.md", "comments": ["LGTM, but would you be willing to update the commit message to be more descriptive?  For example, \"Remove trailing whitespace in compiler/plugin/README.md.\"", "sorry for that.i was in dream when found this error.", "/retest"]}, {"number": 22767, "title": "Feature request: Support tf.slice in contrib.receptive_field", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11\r\n- **Python version**: n/a\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\nThe `contrib.receptive_field` does not support `tf.slice` and this would be useful. Furthermore, it seems asymmetric to support `tf.pad` but not `tf.slice`.\r\n\r\nMy use case is: in ResNet, if I use 'VALID' padding in the blocks, I need to crop some edge pixels at the residual connection to maintain the alignment of the receptive fields. This voids the ability to compute the receptive field.", "comments": ["Please feel free to send PR to add this feature and we will review it.\r\nThanks!", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22766, "title": "nvcc error: string_view.h: constexpr function return is non-constant", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: This is what the bug is about, see below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: none\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 8.0, 9.0, 9.1\r\n- **GPU model and memory**: doesn't matter\r\n- **Exact command to reproduce**: See below.\r\n\r\n### Describe the problem\r\n\r\nCompiling some custom native op with `nvcc` fails, with basically this error:\r\n\r\n    absl/strings/string_view.h(501): error: constexpr function return is non-constant\r\n\r\nCompiling the same code with `g++` does not have this issue. This seems specifically related to the CUDA frontent `cudafe`.\r\n\r\n### Source code / logs\r\n\r\nExample code `test.cpp`:\r\n\r\n```\r\n// For Eigen::GpuDevice.\r\n#define EIGEN_USE_GPU 1\r\n\r\n// For Eigen::ThreadPoolDevice.\r\n#define EIGEN_USE_THREADS 1\r\n\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/common_runtime/device.h\"\r\n\r\n#include <cuda.h>\r\n#include <cuda_runtime.h>\r\n#include <cublas_v2.h>\r\n#include <math_constants.h>\r\n\r\n#include \"tensorflow/core/platform/stream_executor.h\"\r\n```\r\n\r\nCompile command:\r\n`/usr/local/cuda-8.0/bin/nvcc -shared -O2 -std=c++11 -I /u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/include -I /u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/include/external/nsync/public -I /usr/local/cuda-8.0/include -L /usr/local/cuda-8.0/lib64 -x cu -DGOOGLE_CUDA=1 -Xcompiler -fPIC -D_GLIBCXX_USE_CXX11_ABI=0 -g test.cpp -o test.so -lblas -lf77blas -L/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/numpy/.libs -lopenblasp-r0-8dca6697.3.0.dev -L/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow -ltensorflow_framework -v -Xcompiler -v`\r\n\r\nThe full compile output (including some more warnings) can be seen [here in the StackOverflow question](https://stackoverflow.com/questions/52665441/nvcc-error-string-view-h-constexpr-function-return-is-non-constant).\r\n", "comments": ["A workaround is to add the flag `-DNDEBUG` .", "Hm, yea, although I would want that my `assert`s will fail.\r\nMaybe `-Dconstexpr=` would be another option (did not try), although not sure if that has other drawbacks.\r\n", "Same error compiling a custom op.\r\n\r\nEnvironment:\r\nUbuntu 16.04\r\nGCC 5.5.0\r\nCUDA 9.0 9.2 / cuDNN 7.1\r\nPython 2.7.12\r\nTensorflow 1.11.0\r\n\r\nFixed by adding the flag `-DNDEBUG`.\r\n", "@albertz Did the workaround help to solve your issue?\r\n\r\n> A workaround is to add the flag `-DNDEBUG` .\r\n\r\n", "@ymodak As I said:\r\n\r\nI want that my asserts will fail, so `-DNDEBUG` is not an option for me (and anyway, this is an ugly workaround, and doesn't really solve the problem itself).\r\n\r\nMaybe -Dconstexpr= would be another option (did not try; not sure if *less* ugly), although not sure if that has other drawbacks.\r\n", "I second @albertz: the problem can be masked as suggested by @ppwwyyxx, but it's not addressing the source of the issue. \r\n\r\nIn my environment, I can get past this error by replacing the call to `ABSL_ASSERT()` in `tensorflow/include/absl/strings/string_view.h:501` by `0` (but it is arguably just as ugly as `-DNDEBUG`).", "Also seeing this on Ubuntu 18.04, GCC 7.3, Python 3.5, CUDA 9.2.\r\n\r\nI worked around by just commenting out the offending line in `tensorflow/include/absl/strings/string_view.h:501`.", "Another hack is to remove the `ABSL_ASSERT` from the offending line. wfm, ymmv.", "I'm also affected by this issue and similar problems when attempting to compile in debug mode (normal compilation works fine). #27744\r\n\r\nIt seems to me like the TF community has found some workarounds like defining NDEBUG and commenting out a string_view line and making it work.\r\n\r\nBut what is the root cause of this issue? #28091 seems to suggest to just quit CUDA debugging altogether. Looking at this thread and related issues  it looks like there's a large amount of people still affected by this.", "I am also affected by this issue cuda 10.2, and the -DNDEBUG flag workaround seems to work for me. Possibly the last change fixed it, but I just haven't pulled the changes. ", "> Another hack is to remove the `ABSL_ASSERT` from the offending line. wfm, ymmv.\r\n\r\nHello, I'm compiling cuda code using NVCC command, and also met this issue, simply add `-DNDEBUG` doesn't slove all related errors(I've 5 errors related). Just curious when is the `ABSL_ASSERT`? Can you explicitly tell me?", "> > Another hack is to remove the `ABSL_ASSERT` from the offending line. wfm, ymmv.\r\n> \r\n> Hello, I'm compiling cuda code using NVCC command, and also met this issue, simply add `-DNDEBUG` doesn't slove all related errors(I've 5 errors related). Just curious when is the `ABSL_ASSERT`? Can you explicitly tell me?\r\n\r\nLook at the error:\r\n\r\n    absl/strings/string_view.h(501): error: constexpr function return is non-constant\r\n\r\nIt says, in file `absl/strings/string_view.h`, in line 501, there is some error.\r\nWhen you look into that file, in this line, you will find `ABSL_ASSERT`. The suggestion was to just remove that.\r\n\r\nBut as you say, `-DNDEBUG` doesn't work for you, I suspect that you actually have some different error. You should write what error you get exactly (and what CUDA version, what TF version).\r\n", "> > > Another hack is to remove the `ABSL_ASSERT` from the offending line. wfm, ymmv.\r\n> > \r\n> > \r\n> > Hello, I'm compiling cuda code using NVCC command, and also met this issue, simply add `-DNDEBUG` doesn't slove all related errors(I've 5 errors related). Just curious when is the `ABSL_ASSERT`? Can you explicitly tell me?\r\n> \r\n> Look at the error:\r\n> \r\n> ```\r\n> absl/strings/string_view.h(501): error: constexpr function return is non-constant\r\n> ```\r\n> \r\n> It says, in file `absl/strings/string_view.h`, in line 501, there is some error.\r\n> When you look into that file, in this line, you will find `ABSL_ASSERT`. The suggestion was to just remove that.\r\n> \r\n> But as you say, `-DNDEBUG` doesn't work for you, I suspect that you actually have some different error. You should write what error you get exactly (and what CUDA version, what TF version).\r\n\r\nThank you. Well, my CUDA=8.0, TF=1.15.0, \r\none of my error says:\r\n`site-packages/tensorflow_core/include/absl/strings/string_view.h(495): error: constexpr function return is non-constant`,\r\nSo I checked it out.\r\n```\r\nstatic constexpr size_type CheckLengthInternal(size_type len) {\r\n    return ABSL_ASSERT(len <= kMaxSize), len;\r\n  }\r\n```\r\nSo, should I change this block of code to:\r\n```\r\nstatic constexpr size_type CheckLengthInternal(size_type len) {\r\n    return 0, len;\r\n  }\r\n```\r\n? 0 or 1?", "> > > Another hack is to remove the `ABSL_ASSERT` from the offending line. wfm, ymmv.\r\n> > \r\n> > \r\n> > Hello, I'm compiling cuda code using NVCC command, and also met this issue, simply add `-DNDEBUG` doesn't slove all related errors(I've 5 errors related). Just curious when is the `ABSL_ASSERT`? Can you explicitly tell me?\r\n> \r\n> Look at the error:\r\n> \r\n> ```\r\n> absl/strings/string_view.h(501): error: constexpr function return is non-constant\r\n> ```\r\n> \r\n> It says, in file `absl/strings/string_view.h`, in line 501, there is some error.\r\n> When you look into that file, in this line, you will find `ABSL_ASSERT`. The suggestion was to just remove that.\r\n> \r\n> But as you say, `-DNDEBUG` doesn't work for you, I suspect that you actually have some different error. You should write what error you get exactly (and what CUDA version, what TF version).\r\n\r\nAnother error which appears twice is:\r\n\r\n```\r\n/include/absl/strings/str_cat.h(269): error: expression must have a constant value\r\n```\r\nI go to that file, and found that it's an template, how to comment this out, I have no idea, can you help me to check it out?\r\nHere is the 269 line:\r\n```\r\n// Normal enums are already handled by the integer formatters.\r\n  // This overload matches only scoped enums.\r\n  template <typename T,\r\n            typename = typename std::enable_if<\r\n                std::is_enum<T>{} && !std::is_convertible<T, int>{}>::type>\r\n  AlphaNum(T e)  // NOLINT(runtime/explicit)\r\n      : AlphaNum(static_cast<typename std::underlying_type<T>::type>(e)) {}\r\n```", "`return len;` should do the trick, no zero or one needed.\r\n\r\n\r\nAs for your second question, you could try to remove the `enable_if`:\r\n\r\n`template <typename T> AlphaNum ...`", "@albertz We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. please check https://github.com/tensorflow/tensorflow/issues/22766#issuecomment-679909377  Thanks!", "I just tried with GCC 9.3.0, CUDA 11.0 and TF 2.3.1 and I don't see the problem anymore.\r\n\r\nI'm not exactly sure since what (TF?) version that problem is gone.", "@albertz  if it is resolved then please feel free to move this issue to close status ? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22766\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22766\">No</a>\n"]}, {"number": 22765, "title": "[Performance Optimization] Improve the performance of tf.clip_by_norm when the input is IndexedSlices", "body": "Currently, the `tf.clip_by_norm` will convert the input to dense tensor and do the clipping. However, the gradients may be `IndexedSlices` type when encounters with `tf.nn.embedding_lookup` in forward pass. It may both consume large memory and make computation performance worse.\r\n\r\nIn this pull request, the clipping function will deal with `IndexedSlices` directly instead of converting to dense tensor.", "comments": ["Any code review comments are welcome. Thanks. @jhseu @alextp ", "@wangsiyu please address the linter errors here:\r\nhttps://source.cloud.google.com/results/invocations/a7086d3b-2b1e-448f-8c1d-37282e599faf/log\r\n", "> @wangsiyu please address the linter errors here:\r\n> https://source.cloud.google.com/results/invocations/a7086d3b-2b1e-448f-8c1d-37282e599faf/log\r\n\r\nI have refine the code according to the pylint feedback. Please review again. Thanks", "Any update for me\uff1f", "@wangsiyu It seems there are still a few pylint issues. Please check.", "@wt-huang I'm sorry. Could you show me more details? The lint check of `clip_ops.py` and `clip_ops_test.py` seems ok locally.  Thanks very much.", "@wt-huang Ci build test shows all lint passed. ", "@wangsiyu good that all pylint issues are fixed. Thanks!", "@wt-huang Thanks very much. But lack of an approval here @rmlarsen ."]}, {"number": 22764, "title": "Ios lite simple app improvment", "body": "add photo selection and image preview for tensorflow lite simple app", "comments": ["@gorBaghdasaryan your PR contains a bunch of XCode files. Did you submit those by mistake?", "@rmlarsen no, not by chance. all these changes are needed", "Sorry for the slow response on this. It looks like a good improvement to the sample app, but one of our goals is to keep the code for the example as simple as possible, so it might be best as a separate github project. If you do create one, we'd be happy to link to it from documentation."]}, {"number": 22763, "title": "AdamWOptimizer and learning rate decay", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04 \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**1.11:\r\n- **Python version**:3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI think I have found that the contrib.opt.AdamWOptimizer and associated decoupled weight decay optimizers do not function correctly when using learning rate decay without explicitly applying the decay to the weight_decay parameter. \r\n\r\nIn the original paper, as seen in algorithm 2, the schedule multiplier is factored out and applied to the whole expression, the current interface means you have to do `AdamWOptimizer(weight_decay=wd*decay, learning_rate=lr*decay)` to achieve parity with the paper, this is not in its self an issue, but I think the documentation should reflect this difference. Alternatively the API could give a schedule multiplier parameter and then fixed lr and wd parameters used. \r\n\r\nHappy to submit a PR if someone can advise whether a documentation or interface update is the desired approach.\r\n\r\n", "comments": ["I think the optimizer is introduced by #17438.\r\n\r\n@PhilJd @alextp Could you please comment ? ", "You're right, the current implementation requires to schedule the weight decay manually. I also thought about introducing a `schedule_multiplier` argument but decided against it to keep the interface similar to the existing optimizers, which also require manual scheduling of the learning rate. This makes switching optimizers during hyperparameter search a lot easier.\r\nI definitely agree that the documentation should be improved in this point ;) \r\nAn alternative could be to introduce a parameter that specifies the `ratio weight_decay/learning_rate`, which could be used to compute `weight_decay = learning_rate * ratio`.\r\nPersonally, I think this is less readable compared to explicit wd scheduling and improving the documentation is my preferred option but I'm happy to get convinced of the contrary ;)", "I agree improving the documentation is the best way to proceed at this point.\r\n\r\nDo you want to send us a pull request for that?", "Hello, I am a student at Seneca College, participating in Hacktoberfest. I've read through this issue, and I can improve the documentation to reflect the necessary changes, and can send the pull request. Please let me know if this would be helpful, thanks!", "@JoshuaRM Contribution is always welcome. Please link your PR to the issue :-)", "Ah, I was wondering why when my learning rate decays all my weights go to 0 exploding my loss. This was definitely the issue. ", ">   Note: when applying a decay to the learning rate, be sure to manually apply\r\n>   the decay to the `weight_decay` as well. For example:\r\n>    ```python\r\n>     decay = tf.train.piecewise_constant(tf.train.get_global_step(), \r\n>                                         [10000, 15000], [1e-1, 1e-2, 1e-3])\r\n>     lr = 1*decay\r\n>     wd = 1e-4*decay\r\n>      # ...\r\n>     optimizer = tf.contrib.opt.MomentumWOptimizer(learning_rate=lr,\r\n>                                                   weight_decay=wd,\r\n>                                                   momentum=0.9,\r\n>                                                   use_nesterov=True)\r\n>   ```\r\n\r\nI can add this snippet to the other weight decay optimizers as well if this looks good.", "Note that tf.train.piecewise_constant will in tf 2.0 return a function which, when called, returns a tensor (to make things a little safer wrt to multiple calls to optimizer.minimize in a single session.run and eager execution) so you'd have to do something like `wd = lambda: 1e-4 * decay()`.", "When using warm up, do we need to warm up weight_decay together with learning_rate?", "I guess it's very reasonable to do so. If the weight decay is too large compared to the optimizer update (which would be the case for learning warm up without warmed up decay) your weights are likely to become zero. ", "It's still confusing for me how should I declare decay in AdamWOptimizer? I'm using TF 1.14 because is the most recent runtime in Google AI which is the platform to train my models. \r\n\r\nIn the repo below a schedule is created according to the number of iterations and batchsize, https://github.com/PhilJd/AdamW_benchmark/blob/4e9bc846f3b913f727f7585389b59fe46aed0056/model/resnet.py\r\n\r\nI would appreciate any advice how to set **first_decay_steps** in particular in a hyperparameter optimization task in which the batch size is part of the search grid. \r\n\r\n\r\n", "Hi @benleetownsend !\r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more.  Attaching relevant threads for Reference. Ref [1](https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/AdamW), [2](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/exponential_decay) . Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 22762, "title": "New configure script alters .bazelrc in the source tree", "body": "### System information\r\n- Issue is when building from source \r\n- Linux Ubuntu 18\r\n- HEAD of master\r\n- python 3\r\n- bazel 0.16.1\r\n\r\n### Describe the problem\r\nThe issue is that the configure script now updates the contents of the .bazelrc file in place - thus changing the source code of the repo.  This is very unhelpful for change management.  There is a comment in the .bazelrc file which seems to suggest that the changes which are made should not be checked in. \r\n\r\n", "comments": ["```\r\ndiff --git a/.bazelrc b/.bazelrc\r\nindex d5d20309df..c27c68ee02 100644\r\n--- a/.bazelrc\r\n+++ b/.bazelrc\r\n@@ -86,3 +86,4 @@ build --define=LIBDIR=$(PREFIX)/lib\r\n build --define=INCLUDEDIR=$(PREFIX)/include\r\n \r\n # Do not commit the tf_configure.bazelrc line\r\n+import %workspace%/.tf_configure.bazelrc\r\n```\r\n\r\nThe bottom line is being added by the configure script and causes the .bazelrc to be permanently modified.\r\n", "The  line containing `.bazelrc` was removed from the `.gitignore` - which is a recent change (post branching of `r1.12`) and just need to be reverted. ", "Cc @perfinion.  This makes TensorFlow development awkward for people outside Google.  What was the motivation?", "@girving Bazel-0.18.0 changes which files it reads. It originally dropped reading tools/bazel.rc so we had to move everything into .bazelrc. it will support a try-import so i'm gonna unconditionally add that to bazelrc and then the file wont change with and without configuring. Bazel-0.18.0 is due out on monday so I was gonna bump the minimum version and fix it then. Its pretty awkward yeah but it wont be for that long so hopefully not too bad."]}, {"number": 22761, "title": "Error building on Windows, patch command failing", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: 0.17.2\r\n- **GCC/Compiler version (if compiling from source)**: Visual C++ Build Tools 2015\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Follow build instructions on https://www.tensorflow.org/install/source_windows\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen following the build instructions on https://www.tensorflow.org/install/source_windows I am unable to build tensorflow from source. I'm running on a clean Windows 10 install (nothing else installed but the requirements listed on the homepage). \r\n\r\nWhen running the bazel build step (CPU) with the command: $ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package I get two different errors, both when the build script tries to patch some files. The commands that fail are:\r\n\r\n`C:\\msys64\\usr\\bin\\bash.exe -l -c patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/kafka -i C:/tensorflow/third_party/kafka/config.patch\r\nC:\\msys64\\usr\\bin\\bash.exe -l -c patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/png_archive -i C:/tensorflow/third_party/png_fix_rpi.patch`\r\n\r\nCopying the patch commands into the cmd.exe prompt makes the patch command \"hang\" until I manually terminate it. If I add ' ' around the command part of the bash invocation (e.g. bash -c 'patch ...') it works just fine. \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n`$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nDEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@kafka//': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 99\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 67, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/kafka -i C:/tensorflow/third_party/kafka/config.patch':\r\nStdout:\r\nStderr: Timed out\r\nINFO: Elapsed time: 93,541s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (242 packages loaded)\r\n\r\n\r\ncm@tbuilder  /c/tensorflow\r\n$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nDEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@png_archive//': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 99\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 67, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/png_archive -i C:/tensorflow/third_party/png_fix_rpi.patch':\r\nStdout:\r\nStderr: Timed out\r\nINFO: Elapsed time: 33,880s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)`\r\n", "comments": ["@meteorcloudy ", "@madsdk I also just ran into this. In another similar bug (lost track of which one and can't find it now) it was suggested that you rerun `pacman -S patch` under msys2 (as opposed to cmd.exe).  Doing that got me unblocked.  (fwiw, seems like there is some issue here with the instructions, but I'm too unfamiliar with msys to say exactly what. just sharing this workaround in case it may prove of use.)", "> \r\n> \r\n> @madsdk I also just ran into this. In another similar bug (lost track of which one and can't find it now) it was suggested that you rerun `pacman -S patch` under msys2 (as opposed to cmd.exe). Doing that got me unblocked. (fwiw, seems like there is some issue here with the instructions, but I'm too unfamiliar with msys to say exactly what. just sharing this workaround in case it may prove of use.)\r\n\r\nThank you for your suggestion. I tried this (running bash from the cmd.exe and then removing and re-installing patch), but unfortunately it doesn't seem to work for me. Did you run the bazel build command from msys2 bash also?", "@madsdk Can you try to run `bazel clean --expunge` command then retry the build? See if it helps.\r\nIt's not the first I see this issue, but I couldn't reproduce it,", "> \r\n> \r\n> @madsdk Can you try to run `bazel clean --expunge` command then retry the build? See if it helps.\r\n> It's not the first I see this issue, but I couldn't reproduce it,\r\n\r\nThank you for your answer. I've tried calling clean before, but I just tried it again and got the following result:\r\n\r\nc:\\tensorflow>bazel clean --expunge\r\nStarting local Bazel server and connecting to it...\r\nINFO: Starting clean.\r\n\r\nc:\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nDEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/cm/_bazel_cm/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@kafka//': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 99\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 67, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/kafka -i C:/tensorflow/third_party/kafka/config.patch':\r\nStdout:\r\nStderr: Timed out\r\nINFO: Elapsed time: 98,260s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (243 packages loaded)", "Can you show me your `PATH` environment variable and where is `patch` installed?\r\n\r\nShould be the output of `echo %PATH%` and `which patch`", "c:\\tensorflow>echo %PATH%\r\nC:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\ProgramData\\chocolatey\\bin;C:\\Users\\cm\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\;C:\\Users\\cm\\AppData\\Local\\Programs\\Python\\Python36\\;C:\\Users\\cm\\AppData\\Local\\Microsoft\\WindowsApps;c:\\bazel;C:\\msys64\\usr\\bin;\r\n\r\nc:\\tensorflow>which patch\r\n/usr/bin/patch\r\n\r\nc:\\tensorflow>", "I am also having the same issue. Have exact the same setting like the original post except I am using Python 3.5.2 (Downgraded from 3.6.6 as I read somewhere tensorflow wasn't supporting 3.6. But probably outdated claim)", "Having the same issue. \r\nSame/exact configuration as @madsdk  \r\n", "I'm seeing the same behavior, but if the error message is correct, and the -c argument to bash.exe isn't being quoted, the behavior is what I'd expect. That's because bash would be running patch without arguments, so would be hanging waiting for STDIN, which leads to the timeout that kills the job:\r\n\r\n```\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c patch -p1 -d C:/users/bjeps/_bazel_bjeps/jcnpsgii/external/png_archive -i C:/users/bjeps/desktop/src/tensorflow/third_party/png_fix_rpi.patch':\r\nStdout:\r\nStderr: Timed out\r\nINFO: Elapsed time: 153.569s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (242 packages loaded)\r\n```", "@bjepson - In repo.bzl, you can try changing code in function: _wrap_bash_cmd\r\nChange the line:\r\n`\r\ncmd = [bazel_sh, \"-l\", \"-c\", \" \".join(cmd)]\r\n`\r\nTo be:\r\n`\r\ncmd = [bazel_sh, \"-l\", \"-c\", '\"' + \" \".join(cmd) + '\"']\r\n`\r\n\r\nI still ran into different issue anyway. May be what we see in output log is not accurate?\r\n\r\nThe modification that seems to unblock my build is simply increasing timeout in function: _execute_and_check_ret_code\r\nFrom:\r\n`\r\n    result = repo_ctx.execute(cmd_and_args, timeout = 10)\r\n`\r\nTo be:\r\n`\r\n    result = repo_ctx.execute(cmd_and_args, timeout = 100)\r\n`\r\n\r\nSeem  like bazel is utilizing machine resource and shell execution cannot complete task on time in some machines. Increasing the timeout seems to help.\r\n", "I just added the suggested changes to repo.bzl (thanks @twilightdema) but I still get an error with either the kafka or the png_archive package.\r\n\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@kafka//': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 99\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 67, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(127) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c \"patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/kafka -i C:/tensorflow/third_party/kafka/config.patch\"':\r\nStdout:\r\nStderr: /usr/bin/bash: patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/kafka -i C:/tensorflow/third_party/kafka/config.patch: No such file or directory\r\nINFO: Elapsed time: 111,261s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (264 packages loaded)\r\n\r\nThe return code is new though - 127 instead of 256 :)", "Copying the command line that it says fails, works perfectly now that the patch command has been put within ' ':\r\n\r\nc:\\tensorflow>C:\\msys64\\usr\\bin\\bash.exe -l -c 'patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/kafka -i C:/tensorflow/third_party/kafka/config.patch'\r\npatching file config.h\r\n", "@madsdk - Seeing from you log:\r\n`\r\nNon-zero return code(127) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c \"patch -p1 -d C:/users/cm/_bazel_cm/xv6zejqw/external/kafka -i C:/tensorflow/third_party/kafka/config.patch\"'\r\n`\r\nPlease make sure that you don't add single quote or double quote to \"repo.bzl \".\r\n**The only necessary change seems to be Timeout value.**\r\nAdding quote/double quote will generate 'No such file or directory' error.\r\nFrom what I found, the original line below is correct:\r\n`\r\ncmd = [bazel_sh, \"-l\", \"-c\", \" \".join(cmd)]\r\n`\r\n", "@twilightdema thank you for your message. You are indeed right, if I remove the added quotes the build process actually starts now! :) Only problem is that it breaks down a few minutes later now... I'll try some things and perhaps post a new log soon. Thank you \ud83d\udc4d ", "@twilightdema Thank you for the proposed solution.  Problem solved when changing the timeout value ! ", "> Please make sure that you don't add single quote or double quote to \"repo.bzl \".\r\n> **The only necessary change seems to be Timeout value.**\r\n> Adding quote/double quote will generate 'No such file or directory' error.\r\n> From what I found, the original line below is correct:\r\n> `cmd = [bazel_sh, \"-l\", \"-c\", \" \".join(cmd)]`\r\n\r\n@twilightdema Thank you! I just tried on a faster computer than the one I was building on, and it works, so that confirms what you said. \r\n\r\nThe arguments to bazel_sh are quoted fine, contrary to what I had thought before. It seems that building on a slower computer can lead to this error. I had compiled it on the same slow computer under the Windows Subsystem for Linux, and hadn't encountered this error, but when running using Visual C and MSYS, I received the timeout error.", "@twilightdema Thanks for the solution! I'll send a PR to increase the timeout!\r\n", "Still having timeout after setting up the time out. Adding double quotes or simply running patch instead of bash patch works. I fixed it by commenting then line where cmd is wraped with bash to run patch directly from msys2.", "> Still having timeout after setting up the time out. Adding double quotes or simply running patch instead of bash patch works. I fixed it by commenting then line where cmd is wraped with bash to run patch directly from msys2.\r\n\r\n@arunmandal53 - Can you try adding more timeout? I saw in patch that timeout was increased to be 60.. Anyway, I built tensorflow on Surface 3 (Non-Pro) (CPU ATOM), I have to use like 200 in order to not getting Timeout.", "When i ran bash -c patch -v it will take too much time so need to stop forcefully. But when i just use patch -v it works instantly. Also bash -c \"patch -v\" works instantly.", "> When i ran bash -c patch -v it will take too much time so need to stop forcefully. But when i just use patch -v it works instantly. Also bash -c \"patch -v\" works instantly.\r\n\r\n\r\n\r\n> Still having timeout after setting up the time out. Adding double quotes or simply running patch instead of bash patch works. I fixed it by commenting then line where cmd is wraped with bash to run patch directly from msys2.\r\n\r\ncan you explain more clearly?", "**TL;DR**: Triple-check that your MSYS2 installation is correct and updated (`pacman -Syu` returns \"your system is already up-to-date\")\r\n\r\nJust adding a comment for who lands here because of error 127 ( @madsdk ): after plenty of <s>swearing</s> researching, I finally figured it out: Error 127 is raised by bash when the program to run is not found **or one of its dependencies is missing**. Turns out, my **MSYS2 installation was not complete**.\r\n\r\nFollowing Bazel's website, I chose the Chocolately installation, which also installs dependencies (msys2 and python2), however, it seems that the setup script does NOT update the msys2 setup after the installation completes. Following the instructions on mysy2's homepage, I ran `pacman -Syu`, closed and reopened the shell, ran again `pacman -Su` and now it stopped raising the error on `patch`."]}, {"number": 22760, "title": "Feature request for tf.contrib.image.translate", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.4\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Not applicable\r\n- **TensorFlow installed from (source or binary)**: Don't know\r\n- **TensorFlow version (use command below)**: 1.18.0\r\n- **Python version**: 2.7.15\r\n- **Bazel version (if compiling from source)**: Don't know\r\n- **GCC/Compiler version (if compiling from source)**: Don't know\r\n- **CUDA/cuDNN version**: CUDA 7.5.17\r\n- **GPU model and memory**: GeForce GTX 1080 Ti, 11GB\r\n- **Exact command to reproduce**: tf.contrib.image.translate(img, output_of_network)\r\n\r\nHi, is there a way that the feature for computing gradients through the translation in `tf.contrib.image.translate` could be added? It does not involve any interpolation like in `tf.contrib.image.transform` or `tf.contrib.image.rotate` as in issue  #18649 \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nExact command to reproduce\nMobile device", "Hi @tensorflowbutler, I have updated the details, Please look into it.", "@gautamsreekumar `tf.contrib.image.translate` has the same issue as `tf.contrib.image.transform` since the gradient for bilinear interpolation can't be calculated from the existing projective transformation. OpenCV or MATLAB would be a better option.", "> @gautamsreekumar `tf.contrib.image.translate` has the same issue as `tf.contrib.image.transform` since the gradient for bilinear interpolation can't be calculated from the existing projective transformation. OpenCV or MATLAB would be a better option.\r\n\r\n@wt-huang  Do you mean we can implement gradient through image translation/rotation by OpenCV/MATLAB? If so, can you briefly explain how? As I am facing similar issue. Thank you!", "I needed this as well, so for posterity: if all you care about is translation, this can be accomplished with some effort using padding and slicing via `map_fn`. That keeps the operation differentiable. ([Code example](https://gist.github.com/skosch/e43260f3ae65ddb319b43ce880349175), [SO question](https://stackoverflow.com/questions/55767069))", "I asked this question on StackOverflow and I got an answer which showed how I could use `tf.gradients` to manually calculate the gradients. The answer can be found [here](https://stackoverflow.com/a/52779070/6211109)"]}, {"number": 22759, "title": "Building custom OP with bazel in MacOs fails when upgrading TF from 1.8 to 1.11", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**: source by bazel \"http_archive\"\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 10.0.0 (clang-1000.11.45.2)\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**:  \r\nclone branch https://github.com/shkarupa-alex/tfunicode/tree/feature/tf-1.11.0\r\nexport PYTHON_BIN_PATH=`which python2.7`\r\nbazel clean --expunge\r\nbazel build //tfunicode\r\n\r\n### Describe the problem\r\nI've got custom OPs library. I build it from source with bazel (for generating python wrappers).\r\nIt successfully builds with TF 1.8.0.\r\nBut when i try to upgrade TF to 1.11.0, building fails with such error:\r\n> ld: can't open -exported_symbols_list file: -filelist\r\n> clang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nFile provided to clang with failed option is pywrap_tensorflow_internal_versionscript.lds (exists).\r\n\r\n### Source code / logs\r\nFull error log [err.txt](https://github.com/tensorflow/tensorflow/files/2449241/err.txt)\r\n", "comments": ["While debugging i found that error occurs because of a space character in failed command invocations from earlier attached error log.\r\nRight here \r\n![semtech____hdd_develop_semtech__-_____tfunicode_x_sh__semtech_](https://user-images.githubusercontent.com/1289725/46523325-0899ab80-c88e-11e8-9960-cfd8e3c86497.png)\r\n", "Hi, I've been building from **master** without any custom code with XLA configured. After applying this diff:\r\n```diff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl\r\nindex df15914233..8de9c7cfdd 100644\r\n--- a/tensorflow/tensorflow.bzl\r\n+++ b/tensorflow/tensorflow.bzl\r\n@@ -1636,8 +1636,7 @@ def tf_py_wrap_cc(\r\n     )\r\n     extra_linkopts = select({\r\n         \"@local_config_cuda//cuda:darwin\": [\r\n-            \"-Wl,-exported_symbols_list\",\r\n-            \"$(location %s.lds)\" % vscriptname,\r\n+            \"-Wl,-exported_symbols_list,$(location %s.lds)\" % vscriptname,\r\n         ],\r\n         clean_dep(\"//tensorflow:windows\"): [],\r\n         \"//conditions:default\": [\r\n```\r\n\r\nand running `bazel test --test_output=all --nocache_test_results --config=opt tensorflow/compiler/tests:cpu_tests`\r\n\r\nI get lots of warnings during linking:\r\n```\r\nld: warning: cannot export hidden symbol std::__1::shared_ptr<tensorflow::AWSSha256HMACOpenSSLImpl>::__enable_weak_this(...) from bazel-out/darwin-opt/bin/tensorflow/core/platform/s3/libaws_crypto.lo(aws_crypto.o)\r\nld: warning: cannot export hidden symbol std::__1::shared_ptr<tensorflow::AWSLogSystem>::__enable_weak_this(...) from bazel-out/darwin-opt/bin/tensorflow/core/platform/s3/libaws_logging.lo(aws_logging.o)\r\nld: warning: cannot export hidden symbol std::__1::shared_ptr<tensorflow::Notification>::__enable_weak_this(...) from bazel-out/darwin-opt/bin/tensorflow/core/grappler/libutils.a(utils.o)\r\n```\r\n\r\nAnd then when the tests are ran I get:\r\n\r\n```\r\n2018-10-11 13:02:47.136859: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>\r\n*** Received signal 10 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n0   libtensorflow_framework.so          0x00000001200870a7 _ZN10tensorflow7testingL17StacktraceHandlerEiP9__siginfoPv + 183\r\n1   libsystem_platform.dylib            0x00007fff66049b3d _sigtramp + 29\r\n2   ???                                 0x0000000000000000 0x0 + 0\r\n3   _pywrap_tensorflow_internal.so      0x00000001143583a9 _ZNSt3__110__function6__funcIZN3xla3cpu13CpuExecutable24ExecuteAsyncOnStreamImplEPKNS2_27ServiceExecutableRunOptionsEN4absl4SpanIKPKNS2_12ShapedBufferEEEPNS2_19HloExecutionProfileEE12AsyncRunTaskNS_9allocatorISH_EEFvvEEclEv + 73\r\n4   libtensorflow_framework.so          0x000000012032bd09 _ZNSt3__110__function6__funcIZN15stream_executor4host10HostStream11EnqueueTaskENS_8functionIFvvEEEE12NotifiedTaskNS_9allocatorIS8_EES6_EclEv + 25\r\n5   libtensorflow_framework.so          0x00000001200641da _ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi + 618\r\n6   libtensorflow_framework.so          0x0000000120063e6f _ZNSt3__110__function6__funcIZN10tensorflow6thread16EigenEnvironment12CreateThreadENS_8functionIFvvEEEEUlvE_NS_9allocatorIS8_EES6_EclEv + 47\r\n7   libtensorflow_framework.so          0x0000000120088a30 _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEENS_8functionIFvvEEEEEEEEPvSB_ + 48\r\n8   libsystem_pthread.dylib             0x00007fff6605233d _pthread_body + 126\r\n9   libsystem_pthread.dylib             0x00007fff660552a7 _pthread_start + 70\r\n10  libsystem_pthread.dylib             0x00007fff66051425 thread_start + 13\r\n*** END MANGLED STACK TRACE ***\r\n\r\n*** Begin stack trace ***\r\n\ttensorflow::CurrentStackTrace()\r\n\ttensorflow::testing::StacktraceHandler(int, __siginfo*, void*)\r\n\t_sigtramp\r\n\r\n\tstd::__1::__function::__func<xla::cpu::CpuExecutable::ExecuteAsyncOnStreamImpl(xla::ServiceExecutableRunOptions const*, absl::Span<xla::ShapedBuffer const* const>, xla::HloExecutionProfile*)::AsyncRunTask, std::__1::allocator<xla::cpu::CpuExecutable::ExecuteAsyncOnStreamImpl(xla::ServiceExecutableRunOptions const*, absl::Span<xla::ShapedBuffer const* const>, xla::HloExecutionProfile*)::AsyncRunTask>, void ()>::operator()()\r\n\tstd::__1::__function::__func<stream_executor::host::HostStream::EnqueueTask(std::__1::function<void ()>)::NotifiedTask, std::__1::allocator<stream_executor::host::HostStream::EnqueueTask(std::__1::function<void ()>)::NotifiedTask>, void ()>::operator()()\r\n\tEigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\r\n\tstd::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()()\r\n\tvoid* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*)\r\n\t_pthread_body\r\n\t_pthread_start\r\n\tthread_start\r\n*** End stack trace ***\r\n```\r\n\r\nUsing \r\n```\r\nclang -v\r\nApple LLVM version 10.0.0 (clang-1000.11.45.2)\r\n```", "I found that correction good too.\r\nSo, when build fails i manually replace all 2-string occurences of \"exported_symbols_list\" with one-string with comma (in bazel-out/external/...).\r\n\r\nAfter that repeated build command finished success.", "@shkarupa-alex Just wanted to confirm whether you were able to solve your issue?", "> @shkarupa-alex Just wanted to confirm whether you were able to solve your issue?\r\n\r\nYes, when i made manual replacements.", "For others who may find this, it looks like it may be a duplicate of #22902, which is still open at this moment."]}, {"number": 22758, "title": "Tensorflow CPU 1.5 VS 1.6 on Windows 7 - 64 bits - Bug on 1.6 version", "body": "### System information\r\n-  **script provided in TensorFlow :  import tensorflow as tf**.\r\n- **OS Platform and Distribution Windows 7 64 bits**:\r\n- **TensorFlow installed from binary**:\r\n- **TensorFlow version 1.6**:\r\n- **Python 3.6**:\r\n- **Bazel version : no bazel**:\r\n- **GCC/Compiler version : no compiler**:\r\n- **CUDA/cuDNN version : CPU, not GPU**:\r\n- **GPU model and memory: no GPU**:\r\n- **compiling a python program with Pycharm**:\r\n\r\n\r\n### Describe the problem\r\nI got a bug when using Version 1.6.\r\nVersion 1.5 works fine on the same environment. But I need version Tensorflow 1.6 to run Mozilla's deepspeech tool.\r\n\r\n\r\n\r\n### Source code / logs\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 566, in run_nodebug\r\n  File \"<azegrir1>\", line 1, in <module>\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n", "comments": ["Possible duplicate of https://github.com/tensorflow/tensorflow/issues/22512\r\n\r\nCan you try installing the *Microsoft Visual C++ 2015 Redistributable Update 3* listed in the Windows [pip install](https://www.tensorflow.org/install/pip) instructions?", "@belkacem77 Have you tried installing Microsoft Visual C++ as said by @lamberta  above.", "@harshini-gadige  @lamberta \r\nYes I installed MVC++ 2015 Update 3. I will reinstall again from scratch today, even the OS, and will be back.", "Sorry I changed the OS version. Windows 10. It works fine."]}, {"number": 22757, "title": "keras.model.predict also needs two inputs (one for sample, and one for label) when used with tf.data together", "body": "I use tf.data.TFRecordDataset as the input of the tf.keras.model. The training and evaluation are OK. When I used the model for inference, I call the function tf.keras.model.prediction. However, it shows me an error \r\n\r\n> Please provide model inputs as a list or tuple of 2 elements: input and target pair.\r\n\r\nI check the source code, and found that the function \"_standardize_user_data\" (line 988) in \"./tensorflow/python/keras/engine/training.py\" check the input number as the training and evaluation. I think it may be a bug, because when I does prediction, I don't know the true label.\r\n\r\ntensorflow version: 1.11.0 gpu.\r\n\r\nThanks for your notice. I am looking forward your reply.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@monkeyCv To make inference, use `tf.keras.model.predict_generation()` instead. `_standardize_user_data `does require you to feed the true label but it is only used during training.", "@wt-huang Thanks very much. I will try to use `tf.keras.model.predict_generation()`. I am not sure whether the API can recieve the argument `tf.data`, and the document doesnot mention it.\r\nHere I think `tf.keras.model.predict()` calls `_standardize_user_data`, becuase I feed the label, it works well. If I donnot feed the label, it shows an erreor like the figure shows.\r\n![image](https://user-images.githubusercontent.com/38521537/46566548-53c9c200-c953-11e8-8665-0356ce8dffba.png)\r\n\r\nmy test code:   \r\n```\r\ndef test(test_tfrecord,norm_mean,norm_std):\r\n    test_dataset=tf.data.TFRecordDataset(test_tfrecord,num_parallel_reads=4)\r\n    def test_parse_function(example_proto):\r\n        features={'image_raw':tf.FixedLenFeature((),tf.string,default_value='')}\r\n        parsed_features=tf.parse_single_example(example_proto,features)\r\n        image=tf.decode_raw(parsed_features['image_raw'],tf.uint8)\r\n        image=tf.reshape(image,shape=(height,width,channels))\r\n        image=tf.cast(image,tf.float32)\r\n        image=image/255.0\r\n        image=image-norm_mean\r\n        image=image/norm_std\r\n        return image\r\n    test_dataset=test_dataset.map(test_parse_function)\r\n    test_dataset=test_dataset.batch(1)\r\n    \r\n    input_layer=keras.Input(shape=(height,width,channels))\r\n    base_net=myResnet50.ResNet50(input_tensor=input_layer,\r\n                                                  include_top=False,weights='imagenet')\r\n    x=base_net.output\r\n    x=keras.layers.Flatten()(x)\r\n    predictions=keras.layers.Dense(classes,activation='softmax')(x)\r\n    net=keras.models.Model(inputs=input_layer,outputs=predictions)\r\n    net.compile(optimizer=keras.optimizers.Adam(),loss='categorical_crossentropy',metrics=['accuracy'])\r\n    ckpt = tf.train.get_checkpoint_state('../model')\r\n    if not ckpt or   not ckpt.model_checkpoint_path:\r\n        print(\"No model! Start to train the model!\")\r\n    else:\r\n        net.load_weights('../model/mine')\r\n        print(\"Model is loaded!\")\r\n    rst=net.predict(test_dataset,steps=440)\r\n    return rst\r\n\r\n```\r\nmy platform: win10, anaconda 1.8.7, spyder 3.2.3, tensorflow 1.11.0 gpu, cuda 9 and cudnn 7.3.", "I have met the same problem when I use model.predict(tf.data), do you solve this problem?\r\nI think it must be a bug after I check the source code,training and evaluate are both ok because we have two input(image and label),but when we do prediction,we only have one input and it goes wrong", "> I have met the same problem when I use model.predict(tf.data), do you solve this problem?\r\n> I think it must be a bug after I check the source code,training and evaluate are both ok because we have two input(image and label),but when we do prediction,we only have one input and it goes wrong\r\n\r\nNo. I just input a label like placeholder without any meaning to call the predict program.", "The labels can be omitted when using the model.predict() method. You should be able to use tf.data with model.predict() without getting errors. Alternatively, you can work with numpy array. Make sure to initialize model properly and invoke model.fit() to feed inputs and label.\r\n\r\n", "Closing as this is resolved, free to reopen if problem persists."]}, {"number": 22756, "title": "C++ compilation error in rdma_mgr.cc", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:1.11 and master\r\n- **Python version**:2.7,3.5,3.6\r\n- **Bazel version (if compiling from source)**:0.16\r\n- **GCC/Compiler version (if compiling from source)**:4.8\r\n- **CUDA/cuDNN version**:9.0/9.2,7.2\r\n- **GPU model and memory**:various\r\n- **Exact command to reproduce**: compile with verbs\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n**It seems this static member functions (static void tensorflow::RdmaMgr::RegMemVisitors()) try to directly access non-static member (RdmaAdapter* rdma_adapter_;)**\r\n\r\nERROR: /tensorflow/tensorflow/contrib/verbs/BUILD:105:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_mgr' failed (Exit 1)\r\nIn file included from tensorflow/contrib/verbs/rdma_mgr.cc:18:0:\r\n./tensorflow/contrib/verbs/rdma_mgr.h: In static member function 'static void tensorflow::RdmaMgr::RegMemVisitors()':\r\n./tensorflow/contrib/verbs/rdma_mgr.h:50:16: error: invalid use of member 'tensorflow::RdmaMgr::rdma_adapter_' in static member function\r\nRdmaAdapter* rdma_adapter_;\r\n^\r\ntensorflow/contrib/verbs/rdma_mgr.cc:282:40: error: from this location\r\nint32_t bus_id = TryToReadNumaNode(rdma_adapter_->context_->device) + 1;\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "the current master branch C++ code is clearly incorrect, as I pointed out. It would not compile on any platform. Detailed info updated.", "Duplicated with #22455.", "Nagging Assignee @reedwm: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue, as it's a duplicate of #22455"]}, {"number": 22755, "title": "Will Tensorflow 2.0 support synchronizing batchnorm statistics across multiple GPUs?", "body": "Have I written custom code: N/A\r\nOS Platform and Distribution: cpe:/o:centos:centos:7\r\nTensorFlow installed from: pip install \r\nTensorFlow version: 1.10\r\nBazel version: N/A\r\nCUDA/cuDNN version: Cuda 9.0\r\nGPU model and memory: 1080 Ti, 11G\r\nExact command to reproduce: N/A\r\nMobile device: N/A\r\n\r\nI checked the [roadmap](https://www.tensorflow.org/community/roadmap) says the new feature:\r\n\r\n```\r\nEager execution\r\n\r\n    Use DistributionStrategy to utilize multiple GPUs and multiple TPU cores.\r\n    Distributed training support (multi-machine).\r\n    Performance improvements.\r\n    Simpler export to a GraphDef/SavedModel.\r\n```\r\n\r\nI am wondering whether tf 2.0 will consider synchronizing batchnorm statistics across multiple GPUs which is extremely helpful for segmentation and other task.\r\n\r\nSimilar issue as [Synchronized BatchNorm statistics across GPUs](https://github.com/tensorflow/tensorflow/issues/18222)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Hang Zhang has an implementation of sync bn in pytorch:\r\n\r\n [Implementing Synchronized Multi-GPU Batch Normalization](https://hangzhang.org/PyTorch-Encoding/notes/syncbn.html)\r\n\r\nCode in cuda:\r\n\r\n[syncbn_kernel.cu](https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/lib/gpu/syncbn_kernel.cu)\r\n\r\n@isaprykin Not sure whether it would help.", "MxNet also provides a version for sync bc layer:\r\n\r\n[mxnet.gluon.contrib.nn.SyncBatchNorm](http://mxnet.incubator.apache.org/versions/master/api/python/gluon/contrib.html?highlight=syncbatchnorm#mxnet.gluon.contrib.nn.SyncBatchNorm)", "There is an open issue about this (as you have linked to in your post), with some working solutions already. So it's better to leave this issue closed and marked as duplication, to make the discussion more focused.", "@ppwwyyxx Yeah, I see you have implemented in tensorpack. But I wish sync bn could at least be provided in `tf.contrib`, since generally the original framework has better maintenance especially when new version comes out.\r\n\r\nSince Tensorflow 2.0 claims that it will support \"Use DistributionStrategy to utilize multiple GPUs and multiple TPU cores\", and `tf.contrib` will be deprecated, so that I am wondering whether sync bn will be provided in TF 2.0.\r\n\r\nI don't think it is a duplication, at least currently sync bn is not provided in Tensorflow. As you mentioned, there do exist a lot of working solutions, the one in pytorch, in tensorpack, but they are not in official Tensorflow or Pytorch package, once newer and newer version comes out, there might be some error due to update and the largest percentage of users do use the original Tensorflow framework and they need the sync bn feature be a Tensorflow official function.\r\n\r\nThis kind of issue has been keeping coming out on Tensorflow feature request. @holyseven use list of inputs as a solution, [Batch Normalization for Multi-GPU / Data Parallelism](https://github.com/tensorflow/tensorflow/issues/7439). You also implement it in tensorpack as your solution. But they are not solutions to the majority of users. Tensorflow 2.0 seems to be a big change, and if the sync bn could not be provided in TF 2.0, a lot of Tensorflow users might need to wait for another year for this feature to come out. ", "I agree it's best if the feature exists in tensorflow. And that's actually exactly what the existing issue is about and the reason why the issue remains open.", "Let\u2019s just close this issue. It\u2019s duplicated.\r\n\r\nTo be more constructive, please submitting PR to support new features, not duplicated issues. ", "Sorry I didn't consider well about my words. Please ignore this issue if it is not clear.", "Please follow #18222 since this is duplicate. ", "I just implement this function https://github.com/jianlong-yuan/syncbn-tensorflow\r\nYou can easily use it. ", "@jianlong-yuan Thanks for your work! I am wondering whether it follows the similar way in cifar multi gpu example, especially when combining different towers' gradient results like:\r\n\r\n```\r\ndef average_gradients(tower_grads):\r\n    \"\"\"Calculate the average gradient for each shared variable across all towers.\r\n      Note that this function provides a synchronization point across all towers.\r\n\r\n    Args:\r\n        tower_grads: List of lists of (gradient, variable) tuples. The outer list\r\n        is over individual gradients. The inner list is over the gradient\r\n        calculation for each tower.\r\n    Returns:\r\n        List of pairs of (gradient, variable) where the gradient has been averaged\r\n        across all towers.\r\n    \"\"\"\r\n    average_grads = []\r\n    for grads_and_vars in zip(*tower_grads):\r\n        # Each grad_and_vars looks like the following:\r\n        # ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\r\n        grads = [g for g,_ in grads_and_vars]\r\n        if grads[0] is not None: # Gradient can be None\r\n            # Average the grads\r\n            grad = tf.reduce_mean(tf.stack(grads, 0), 0)\r\n            # The Variables are redundant because they are shared across towers\r\n            v = grads_and_vars[0][1]\r\n            grad_and_var = (grad, v)\r\n        else:\r\n            grad_and_var = grads_and_vars[0]\r\n\r\n        average_grads += [grad_and_var]\r\n    return average_grads\r\n```", "FYI, TF just added Sync BN: adf769043f0c48a44c05c5a24aac14f0b4951896"]}, {"number": 22754, "title": "1.12-rc0 cherry-pick request: Disable tensorrt:unary_test in OSS since it crashes with SEGV.", "body": "PiperOrigin-RevId: 215814732", "comments": []}, {"number": 22753, "title": "1.12-rc0 cherry-pick request: Add xla library into contrib_py", "body": "PiperOrigin-RevId: 215774158", "comments": ["Windows failure is unrelated."]}, {"number": 22752, "title": "[INTEL MKL] Update default TF_BUILD_VERSION to r1.11 and add packages", "body": "Signed-off-by: Cong Xu <cong.xu@intel.com>", "comments": ["Sorry for bothering you, but the PR seems to be stuck in \"ready to pull\" state for 10 days. What should I do to have it merged. Thanks a lot!", "@yifeif any idea why we see the copybara error?"]}, {"number": 22751, "title": "Take ALL TESTS PASSED in ticks for good formatting", "body": "", "comments": []}, {"number": 22750, "title": "Segmentation fault with small repro", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: archlinux\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:n/a\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:b'v1.9.0-rc2-5276-ge57874169f' 1.12.0-dev20181004\r\n- **Python version**:3.6\r\n- **Bazel version (if compiling from source)**:n/a\r\n- **GCC/Compiler version (if compiling from source)**:n/a\r\n- **CUDA/cuDNN version**:9.0\r\n- **GPU model and memory**:1080Ti\r\n- **Exact command to reproduce**:below\r\n\r\nThis code:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef f(boxes, scores):\r\n    def f(X):\r\n        prob, box = X\r\n        output_shape = tf.shape(prob)\r\n        ids = tf.reshape(tf.where(prob > 0.05), [-1])\r\n        prob = tf.gather(prob, ids)\r\n        box = tf.gather(box, ids)\r\n        # prob = tf.Print(prob, [box, prob], summarize=100, message='boxandprob')\r\n        selection = tf.image.non_max_suppression(box, prob, 100, 0.5)\r\n        selection = tf.to_int32(tf.gather(ids, selection))\r\n        selection = tf.Print(selection, [ids, selection], summarize=100, message='ids_selection_2')\r\n        sorted_selection = -tf.nn.top_k(-selection, k=tf.size(selection))[0]\r\n        mask = tf.sparse_to_dense(\r\n            sparse_indices=sorted_selection,\r\n            output_shape=output_shape,\r\n            sparse_values=True,\r\n            default_value=False)\r\n        return mask\r\n\r\n    masks = tf.map_fn(f, (scores, boxes), dtype=tf.bool, parallel_iterations=10)     # #cat x N\r\n    return masks\r\n\r\nwith tf.device('/gpu:0'):\r\n    boxes = tf.placeholder(tf.float32, (80, None, 4), name='boxes')\r\n    scores = tf.placeholder(tf.float32, (80, None), name='scores')\r\n    outs = f(boxes, scores)\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.allow_soft_placement = True\r\nsess = tf.Session(config=config)\r\ndata = dict(np.load('debug.npz'))\r\nfor k in range(1000):\r\n    sess.run(outs, feed_dict={boxes: data['boxes'].transpose(1, 0, 2)[1:, :, :], scores: data['scores'][:, 1:].T})\r\n    print(k)\r\n```\r\ncauses segmentation fault on tf-nightly-gpu, as well as tensorflow-gpu==1.11.0. It works on 1.10.\r\nIt needs the data file `debug.npz` here: \r\n[debug.zip](https://github.com/tensorflow/tensorflow/files/2448247/debug.zip)\r\n\r\nNote:\r\n1. I tested on two machines, an error happen in >90% runs.\r\n2. The code was distilled from the bug report about MaskRCNN evaluation [here](https://github.com/tensorpack/tensorpack/issues/919). The original bug report does not always segfault, but occasionally crash with other __different__ unreasonable TF internal errors, such as:\r\n```\r\nInvalidArgumentError (see above for traceback): scores has incompatible shape\r\n         [[node map/while/non_max_suppression/NonMaxSuppressionV3 (defined at bug.py:15)  = NonMaxSuppressionV3[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](map/while/Gather\r\nV2_1/_29, map/while/GatherV2/_31, map/while/non_max_suppression/NonMaxSuppressionV3/max_output_size/_33, map/while/non_max_suppression/iou_threshold/_35, map/while/non_max_suppression/score_thresh\r\nold/_37)]]\r\n``` \r\n```\r\n2018-10-04 14:59:14.736180: F tensorflow/core/common_runtime/bfc_allocator.cc:458] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum)                                                     \r\n```\r\n```\r\n2018-10-04 14:59:49.523436: F tensorflow/core/common_runtime/bfc_allocator.cc:380] Check failed: h != kInvalidChunkHandle \r\n```\r\n```\r\n2018-10-05 00:12:03.720295: F ./tensorflow/core/framework/tensor.h:643] Check failed: new_num_elements == NumElements() (39 vs. 0)\r\n```\r\n```\r\n\r\nInvalidArgumentError (see above for traceback): indices[1] = [0] is repeated\r\n         [[{{node map/while/SparseToDense}} = SparseToDense[T=DT_BOOL, Tindices=DT_INT32, _class=[\"loc:@map/while/TensorArrayWrite/TensorArrayWriteV3\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](map/while/Neg_1/_51, map/while/Shape/_53, map/while/SparseToDense/sparse_values/_55, map/while/SparseToDense/default_value/_57)]]\r\n         [[{{node map/while/SparseToDense/sparse_values/_54}} = _Send[T=DT_BOOL, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_111_map/while/SparseToDense/sparse_values\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](map/while/SparseToDense/sparse_values)]]\r\n```\r\nAfter distilled to this small repro, it seems to mostly do segfault. But the above error messages might help. Seems like a memory corruption.", "comments": ["@tayo \r\nI believe your commit https://github.com/tensorflow/tensorflow/commit/8566d9e6fa7dbe3660339befe8b0a3344d24ef2b#diff-6731fe0e9dae6d68dca55b2d50d32c06R320 about NMS op causes this bug.\r\n\r\nThe input `Tensor`s of a `OpKernel::Compute` should not be stored as members of the `OpKernel`. This effectively make `OpKernel::Compute` not thread-safe and crash with my sample code above. ", "Hi Yuxin,\n\nI was just made aware of this issue recently.  I am working on a fix and\nwill push this out soon.  Thanks for bringing this to attention!\n\n-Tayo\n\nOn Fri, Oct 5, 2018 at 2:58 PM Yuxin Wu <notifications@github.com> wrote:\n\n> @tayo <https://github.com/tayo>\n> I believe your commit 8566d9e#diff-6731fe0e9dae6d68dca55b2d50d32c06R320\n> <https://github.com/tensorflow/tensorflow/commit/8566d9e6fa7dbe3660339befe8b0a3344d24ef2b#diff-6731fe0e9dae6d68dca55b2d50d32c06R320>\n> about NMS op causes this bug.\n>\n> The input Tensors of a OpKernel::Compute should not be stored as members\n> of the OpKernel. This effectively make OpKernel::Compute not thread-safe\n> and crash with my sample code above.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22750#issuecomment-427509033>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKa_OWIc5QFzYKU6NC4ZAyj1WgTB342vks5uh9YKgaJpZM4XJAFi>\n> .\n>\n", "@tayo Hi, any update on the fix ? Could you please share the PR# here.", "Fix is under review.", "@tayo I think the fix is not correct and does not fix the problem. (It may avoid the crash, but may end up giving wrong results).\r\nThe bug still exists: `Compute()` should be thread-safe. It may be called concurrently by many callers, as quoted from tensorflow's source code:\r\nhttps://github.com/tensorflow/tensorflow/blob/4a51abb1f935818b6a130f71b340b29379e52a07/tensorflow/core/framework/op_kernel.h#L90-L93\r\n\r\nWhen you store a pointer to a Tensor in `OpKernel::Compute`, the `Compute` method become not thread-safe. If called concurrently by many threads, thread A set pointer to input A, then thread B might overwrite the pointer to input B, then thread A will access the wrong input B. ", "It does not avoid the crash either. After compiling TF with latest master from github, the same code above still segfault.", "Thanks. Issue reopened to look more closely.", "@ppwwyyxx Thanks for pointing this out.", "Thanks for the fix!", "Hi, I still run into this problem with the most recent version.\r\nCode stopped by this issue when I run around 1000 examples\r\n```\r\nimport time\r\nfrom model import *\r\nfrom utils import *\r\nimport DMaskRCNN as model\r\nimport coco\r\nfrom extract import parse_annotation, resize_bbox\r\nimport skimage.io\r\n\r\n# Root directory of the project\r\nroot_dir = os.getcwd()\r\n\r\n# Directory to save logs and trained model\r\nmodel_dir = os.path.join(root_dir, \"domain_logs\")\r\n\r\n# Local path to trained weights file\r\nmodel_path = os.path.join(root_dir, \"mask_rcnn_domain.h5\")\r\n\r\ntest_image_dir = os.path.join(root_dir, \"RTTS/JPEGImages\")\r\ntest_9411_dir = os.path.join(root_dir, \"RTTS/9411result\")\r\ntest_result_dir = os.path.join(root_dir, \"RTTS/result\")\r\ntest_aod_dir = os.path.join(root_dir, \"RTTS/AOD_Dehaze\")\r\n# test_MSCC_dir = os.path.join(root_dir, \"RTTS/MSCCdehaze\")\r\ntest_file_dir = os.path.join(root_dir, \"RTTS/Annotations\")\r\ntest_image_names = next(os.walk(test_image_dir))[2]\r\ntest_image_names.sort()\r\n\r\nCOCO_DIR = \"COCO\"\r\nconfig = coco.CocoConfig()\r\n\r\n\r\n# Override the training configurations with a few\r\n# changes for inferencing.\r\nclass InferenceConfig(config.__class__):\r\n    # Run detection on one image at a time\r\n    GPU_COUNT = 1\r\n    IMAGES_PER_GPU = 1\r\n\r\n\r\nconfig = InferenceConfig()\r\n# config.display()\r\n\r\n# Get the first available GPU\r\nDEVICE_ID_LIST = GPUtil.getFirstAvailable()\r\nDEVICE_ID = DEVICE_ID_LIST[0]  # grab first element from list\r\n\r\n# Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first available device id\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_ID)\r\n\r\n\r\n# Training dataset. Use the training set and 35K from the\r\n# validation set, as as in the Mask RCNN paper.\r\n\r\n\r\ndef mAP_test(image_dir, file_dir):\r\n    APs = []\r\n    t1 = time.time()\r\n    image_names = next(os.walk(image_dir))[2]\r\n    image_names.sort()\r\n    print(len(image_names))\r\n    i = 0\r\n    for image_name in image_names:\r\n        # print(image_name)\r\n        try:\r\n            image = skimage.io.imread(os.path.join(image_dir, image_name))\r\n            image, window, scale, padding = utils.resize_image(\r\n                image,\r\n                min_dim=config.IMAGE_MIN_DIM,\r\n                max_dim=config.IMAGE_MAX_DIM,\r\n                padding=config.IMAGE_PADDING)\r\n            if image_name.find('AOD') != -1:\r\n                image_name = image_name.replace('_AOD-Net', '')\r\n            if image_name.find('dehaze') != -1:\r\n                image_name = image_name.replace('_dehazed', '')\r\n            gt_class_id, ori_gt_bbox = parse_annotation(image_name, file_dir)\r\n            gt_bbox = resize_bbox(ori_gt_bbox, scale, padding)\r\n            results = domain_model.detect([image], verbose=0)\r\n            r = results[0]\r\n            AP, precisions, recalls, overlaps = \\\r\n                utils.compute_box_ap(gt_bbox, gt_class_id,\r\n                                     r[\"rois\"], r[\"class_ids\"], r[\"scores\"])\r\n            APs.append(AP)\r\n        except:\r\n            continue\r\n        if i % 100 == 0:\r\n            print(\"current i:\", i)\r\n        i = i+1\r\n        if i > 4322:\r\n            print(\"overflow\")\r\n            break\r\n\r\n    t2 = time.time() - t1\r\n    print(\"using time: \", t2)\r\n    print(\"mAP: \", np.mean(APs))\r\n    return np.mean(APs)\r\n\r\n\r\nprint('Build the mask rcnn model')\r\ndomain_model = model.Domain_MaskRCNN(mode=\"inference\", model_dir=model_dir,\r\n                                     config=config)\r\nprint('loading weights from: ', model_path)\r\ndomain_model.load_weights(model_path, by_name=True)\r\n\r\nprint('calculating the mAP from dehazed MSCC dataset')\r\ncur_mAP2 = mAP_test(test_result_dir, test_file_dir)\r\nprint('current mAP is:', cur_mAP2)\r\n\r\ndel domain_model\r\nK.clear_session()\r\n```", "@boyuangong This is surprising. Can you post your exact error message?  And do you have a small reproducer for the issue?", "I ran into the same issue with TF 1.12. It is not deterministic and fails in about 60% of cases.\r\n\r\nThis is my error message:\r\n```\r\nF tensorflow/core/common_runtime/bfc_allocator.cc:458] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum) \r\n```\r\n", "The bug has not been fixed in TF 1.12 and has been fixed in later versions.", "> The bug has not been fixed in TF 1.12 and has been fixed in later versions.\r\n\r\nAh yes, apparently this was not pulled into 1.12.  If this issue is blocking anyone, one suggestion is to try TF nightly, pending the next TF release.", "@tayo has the fix been pulled into version 1.13 and above?", "It is fixed in the latest release (1.13rc0).", "> It is fixed in the latest release (1.13rc0).\r\n\r\nThanks a lot. Will upgrade to 1.13rc0 and test it out. \r\n\r\n`2018-10-04 14:59:14.736180: F tensorflow/core/common_runtime/bfc_allocator.cc:458] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum)\r\n`\r\nI am getting the same exception in tensorflow serving and was considering compiling the nightly myself.", "Is it working,I get this error when running faster rcnn in Multiprocessing environment,it runs smoothly in the beginning but fails after 5 seconds\r\n\r\nCheck failed: c->in_use() && (c->bin_num == kInvalidBinNum)\r\n\r\n", "I noticed that this issue does not occur with tf-nightly-gpu `1.13.0.dev20190208` when compared to `1.12`.", "Also can confirm that pulling the 1.13.0 tf-nightly-gpu build solved this issue for me", "https://github.com/tensorflow/tensorflow/commit/1845bf763b4c1c54425d9bb8b1554db79759f567 and https://github.com/tensorflow/tensorflow/commit/76e7804409ccd76c7ce08e66eb739544cd5cda68 solved this issue for me on tf-1.12.0"]}, {"number": 22749, "title": "1.12-rc0 cherry-pick request: Add TF_BUILD_TEST_TIMEOUT to ci_parameterized_build.sh", "body": "PiperOrigin-RevId: 215793932", "comments": []}, {"number": 22748, "title": "[tf.data] Fix noisy warning.", "body": "PiperOrigin-RevId: 215607171", "comments": ["FYI this was merged via https://github.com/tensorflow/tensorflow/pull/22778\r\n\r\nFor future reference, make sure to prefix the PR Title with \"1.12-rc0 cherry-pick request: \" to make the nature of the PR clear."]}, {"number": 22747, "title": "[ROCm] Updated terminology changes in bazel scripts ", "body": "Note:  This is a replacement/updated version of PR #20996.  Based on community feedback, it is now simplified and backwards compatible.  \r\n\r\nMany functions in the TF bazel scripts can be used on both CUDA and ROCm platforms, so the goal here is to use more generic terminology. In general, this PR changes the following terminology in the bazel scripts: \r\n- tf_cuda_* -> tf_gpu_*\r\n- cuda_py_* -> gpu_py_*\r\n\r\nIn addition, the tf_cuda* and cuda_py* entry-points still exist for backward compatibility.  \r\n\r\nAuthors:\r\n- Jeff Poznanovic:  jeffrey.poznanovic@amd.com\r\n- Jack Chung:  jack.chung@amd.com", "comments": ["Our internal testing of `ci_build/linux/gpu/run_py3_core.sh` showed a single failing unit test:  \r\n```\r\nExecuted 833 out of 833 tests: 832 tests pass and 1 fails locally\r\n```\r\n\r\nLooks unrelated to this PR's mods.  Here's the failure:\r\n```\r\n//tensorflow/tools/docs:build_docs_test                                 FAILED in 8.2s\r\n```", "@parallelo Thanks for the cleaned up PR!", "@rmlarsen - Sure, happy to help.  Please let us know if it needs anything further.  ", "@parallelo it looks like you need to resolve a conflict.", "Ah, okay.  Looks like there was a commit to master that was rolled back about 20min ago.  \r\n\r\nI'll take care of it.  ", "Should be resolved now.  ", "@parallelo thanks!", "@rmlarsen - Looks like this PR was generally successful but failed the internal Windows builds.  However, I can't see any details for why those failed.  Anything we should be doing on our side to investigate?  ", "Thanks all.  Now looks like `All checks have passed`.", "Can we merge this PR before we get any conflicts?  Or are there some other blockers that I can help with?  ", "It got stuck on an internal tool failure. @yifeif @gunan what is the fastest way to proceed?", "Sorry for the delay. Is this still relevant? If so could we try to resolve the conflict and merge it again?", "Hi @rmlarsen - Yes, will take care of it.  ", "@rmlarsen Should be all set now -- merged + fixed conflicts.  Internal tests show 0 failures.  \r\n```\r\nExecuted 977 out of 977 tests: 977 tests pass.\r\nINFO: Build completed successfully, 24100 total actions\r\n```", "@rmlarsen errors in Mac and Windows test targets seem to be irrelevant to this PR. Could you take another look at this PR? Thanks", "@rmlarsen - Any thoughts on next steps for how to get this PR merged?  (currently unclear why it would fail MacOS and Windows)", "Hi @rmlarsen and @gunan - Please let us know if we need to do anything on our side for the (unrelated?) failing MacOS & Windows builds.  ", "@rmlarsen, @parallelo This PR has been stuck for a while. Is there anything the authors need to do to move it forward? Or, is it on our side?", "Hi @tatianashp - We did a few rounds of conflict fixes + merges, and most of the CI builds were successful.  However, the (unrelated?) Windows & MacOS builds always seemed to fail.  On our side, it was never clear why that was happening.  \r\n\r\nWhat's the best way to make forward progress on this PR?  \r\n\r\nThanks!", "@parallelo I think another rebase is probably needed. those failures on Windows and MacOS don't really matter. I've seen quite a few PRs get merged even when there are failures on Windows and MacOS targets.", "Yep, I'm working on it now", "I see this failure in the tests which had blocked the merge before\r\n```\r\n\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/compiler/jit/BUILD:727:1: Label '//tensorflow/core:gpu_runtime' is duplicated in the 'deps' attribute of rule 'xla_fusion_optimizer_test_gpu'\r\n```\r\nCould you take a look?", "Yep, will do.  Thanks for the heads-up.  ", "Hi @gunan - Looks like 14 checks in the list were successful.  \r\n\r\n*Ubuntu Python3 PIP* is reporting 5 unit test failures out of 1102 in total, but they seem unrelated to this PR. \r\n Thoughts?  \r\n```\r\n//bazel_pip/tensorflow/python:image_grad_test\r\n//bazel_pip/tensorflow/python:image_ops_test\r\n//bazel_pip/tensorflow/python:sparse_ops_test\r\n//bazel_pip/tensorflow/python/keras:convolutional_test\r\n//bazel_pip/tensorflow/python/kernel_tests:rnn_test\r\n```", "@gunan - Good suggestion.  Just pushed out those mods.  Thx!", "Just wanted to check if there was a blocker on merging this?  It is marked as `approved` and `ready to pull`.  Hoping we can squash+merge and avoid a stale PR.  ", "We have started the process, but we see some failing tests internally.\nI will try to look into them today.\n\nOn Mon, Mar 11, 2019 at 8:44 AM Jeff Poznanovic <notifications@github.com>\nwrote:\n\n> Just wanted to check if there was a blocker on merging this? It is marked\n> as approved and ready to pull. Hoping we can squash+merge and avoid a\n> stale PR.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/22747#issuecomment-471593998>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOVJgCniWamxvak9WYhD5dTl1Rvh9ks5vVnnSgaJpZM4XI7ZT>\n> .\n>\n"]}, {"number": 22746, "title": "Cannot compile with C++17 because std::random_shuffle is removed", "body": "std::random_shuffle has been deprecated since C++14 and [removed](https://meetingcpp.com/blog/items/stdrandom_shuffle-is-deprecated.html) from C++17. Unfortunately this means it's not possible to compile tensorflow with C++17.\r\n\r\nFortunately, there are not many [usages](https://github.com/tensorflow/tensorflow/search?l=C%2B%2B&q=random_shuffle\r\n) present, and the [suggested replacement](https://meetingcpp.com/blog/items/stdrandom_shuffle-is-deprecated.html) is not very complicated.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code: no\r\nOS Platform and Distribution: all\r\nTensorFlow installed from: n/a\r\nTensorFlow version: master\r\nBazel version: 16\r\nCUDA/cuDNN version: n/a\r\nGPU model and memory: n/a\r\nExact command to reproduce: n/a\r\nMobile device: no", "@FelixDuvalletKodiak TensorFlow 1.11.0 uses bazel and gcc 4.8, which is only compatible with c++14. c++17 definitely has nifty and handy features, but is not yet backward compatible with all the libraries used for TensorFlow. Good that you found replacements. We will note this down."]}, {"number": 22745, "title": "Trying to install tensorflow 1.11.0 on python 3.4.2", "body": "Have I written custom code: No\r\nOS Platform and Distribution: Mac Sierra\r\nTensorFlow installed from: running on pyenv, currently on local pyenv of python 3.4.2, installed via `pip`\r\nTensorFlow version: 1.11.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: `pip install tensorflow`\r\nMobile device: N/A\r\n\r\nThis is the error I get,\r\n\r\n> tensorflow 1.11.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.4.3 which is incompatible.\r\n\r\nuninstllling and reinstalling setuptools to required version fixes it but ideally it should work with the default verision that comes with python 3.4.2", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "updated ", "What is the pip version you are using?\r\n", "I guess this is related to https://github.com/tensorflow/tensorflow/issues/22567?", "Yes this issue is related to #22567. As mentioned in that thread, This issue can be resolved once 1.12 rc0 builds. \r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22744, "title": "[Feature request] tfcompile AOT gonna support tensorflow_transform?", "body": "Transform is out side of the graph. It could be great to have this support so that people do not have to re-implement this part in c++ while serve in c++ environment. \r\n\r\nThe other way I can think of to solve this is: is it possible to make arbitrary feature transform a part of the tensorflow graph by modifying it afterwards using graph transform tools?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@sanjoy Hi, could you please look into this issue ?", "I'm not familiar with tensorflow_transform but if it produces a TF graph that XLA can compile then it should be okay to use it with tfcompile.  tfcompile compiles a TF graph to machine code via XLA; so any TF graph that XLA supports is fine as input, it doesn't matter whether the TF graph was produced via tensorflow_transform or some other means.\r\n\r\nPerhaps you could try compiling a graph produced by tensorflow_transform using tfcompile?", "@ranqizhu  Is this still an issue ?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22743, "title": "CUDA 10 tensorflow installation", "body": "Hi,\r\nI have NVDIA gtx 1080 and succesfully installed CUDA 10. I am having the following error.\r\nTensorflow installation does n ot warn me with anything but trying to import tensorflow I am getting the folllowing error.\r\n\r\n\r\n`Python 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n`", "comments": ["@Ayshine, you need to build from sources to use cuda 10. If you want to use pip installation, you need to have cuda9.0 and cudnn7 installed. Please see installation instructions", "Thanks, @samikama!\r\nNow, I am trying to build from sources. I'll update this thread as soon as I can.", "See my posts at https://github.com/tensorflow/tensorflow/issues/22715", "Thanks, @joseortiz3. Trying to follow through your solution now. I have a little different configuration so I hope this would also work for me. I will get back here and update the issue after I completed my process. \r\n\r\n**~$ python --version**\r\nPython 3.6.5 :: Anaconda, Inc.\r\n**~$ nvcc --version**\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Nov__3_21:07:56_CDT_2017\r\nCuda compilation tools, release 9.1, V9.1.85\r\n**~$ cat /usr/local/cuda/version.txt**\r\nCUDA Version 10.0.130\r\n", "ok wh'le running basel got this \r\n\r\nayshine@ayshine:~/tensorflow$ ./configure\r\n/home/ayshine/bin/bazel: line 88: /home/ayshine/.bazel/bin/bazel-real: cannot execute binary file: Exec format error\r\n/home/ayshine/bin/bazel: line 88: /home/ayshine/.bazel/bin/bazel-real: Success\r\nTraceback (most recent call last):\r\n  File \"./configure.py\", line 1592, in <module>\r\n    main()\r\n  File \"./configure.py\", line 1458, in main\r\n    check_bazel_version('0.15.0')\r\n  File \"./configure.py\", line 450, in check_bazel_version\r\n    curr_version = run_shell(['bazel', '--batch', '--bazelrc=/dev/null', 'version'])\r\n  File \"./configure.py\", line 141, in run_shell\r\n    output = subprocess.check_output(cmd)\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/subprocess.py\", line 336, in check_output\r\n    **kwargs).stdout\r\n  File \"/home/ayshine/anaconda3/lib/python3.6/subprocess.py\", line 418, in run\r\n    output=stdout, stderr=stderr)\r\nsubprocess.CalledProcessError: Command '['bazel', '--batch', '--bazelrc=/dev/null', 'version']' returned non-zero exit status 1.\r\n", "@samikama @joseortiz3 thanks for the help. I am going to downgrade from CUDA 10 to CUDA 9. Some other libraries I want to install does not have CUDA 10 version. But, I will keep in mind to come back installing tensorflow on CUDA 10 and make it work :) "]}, {"number": 22742, "title": "Add a separator between shape and dtype in cache key encoding.", "body": "It was possible that we could mix shapes and types (T111 could mean a tensor of dtype 1 and shape (1, 1) or a tensor of dtype 11 and shape (1)).\r\n\r\nPiperOrigin-RevId: 215777629", "comments": ["Yup, this change seems low-risk and worth taking.  I'll merge it after the tests re-run.", "Thanks!"]}, {"number": 22741, "title": "TensorFlow build error possibly caused by renamed zlib license file", "body": "I get a build failure building tensorflow using bazel, due to the fact that the following file apparently no longer exists: https://docs.python.org/2.7/_sources/license.txt\r\n\r\nThis file is specified in workspace.bzl (master):\r\n\r\n  filegroup_external(\r\n      name = \"org_python_license\",\r\n      licenses = [\"notice\"],  # Python 2.0\r\n      sha256_urls = {\r\n          \"b5556e921715ddb9242c076cae3963f483aa47266c5e37ea4c187f77cc79501c\": [\r\n              \"https://mirror.bazel.build/docs.python.org/2.7/_sources/license.txt\",\r\n              \"https://docs.python.org/2.7/_sources/license.txt\",\r\n          ],\r\n      },\r\n  )\r\n\r\nIt appears that the file has been replaced by one named license.rst.txt.\r\n\r\nThere is a second file which bazel can't download, for no obvious reason: zlib-1.2.11.tar.gz. I am able to manually download the file from the file's URLs in workspace.bzl.\r\n\r\nBazel successfully downloaded all other files.\r\n\r\nAs a workaround, I have acquired the two problematical files, and manually placed them in the bazel cache, verifying that they have the expected file hashes. However, when I run \"bazel fetch //tensorflow/tools/pip_package:build_pip_packages\", I get the following errors:\r\n\r\n   ERROR: /home/cmesse02/.cache/bazel/_bazel_cmesse02/f4cce9ac06677e7da5b9b027126f0db6/external/org_python_pypi_backports_weakref/BUILD:17:1: no such package '@org_python_license//': java.io.IOException: Error downloading [https://mirror.bazel.build/docs.python.org/2.7/_sources/license.txt, https://docs.python.org/2.7/_sources/license.txt] to /home/cmesse02/.cache/bazel/_bazel_cmesse02/f4cce9ac06677e7da5b9b027126f0db6/external/org_python_license/license.txt: All mirrors are down: [GET returned 404 Not Found, sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target] and referenced by '@org_python_pypi_backports_weakref//:license'\r\n   ERROR: /home/cmesse02/.cache/bazel/_bazel_cmesse02/f4cce9ac06677e7da5b9b027126f0db6/external/org_python_pypi_backports_weakref/BUILD:17:1: no such package '@org_python_license//': java.io.IOException: Error downloading [https://mirror.bazel.build/docs.python.org/2.7/_sources/license.txt, https://docs.python.org/2.7/_sources/license.txt] to /home/cmesse02/.cache/bazel/_bazel_cmesse02/f4cce9ac06677e7da5b9b027126f0db6/external/org_python_license/license.txt: All mirrors are down: [GET returned 404 Not Found, sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target] and referenced by '@org_python_pypi_backports_weakref//:license'\r\n   ERROR: /home/cmesse02/local/tensorflow/tensorflow/tools/pip_package/BUILD:106:1: no such package '@zlib_archive//': java.io.IOException: Error downloading [https://mirror.bazel.build/zlib.net/zlib-1.2.11.tar.gz, https://zlib.net/zlib-1.2.11.tar.gz] to /home/cmesse02/.cache/bazel/_bazel_cmesse02/f4cce9ac06677e7da5b9b027126f0db6/external/zlib_archive/zlib-1.2.11.tar.gz: All mirrors are down: [sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target] and referenced by '//tensorflow/tools/pip_package:licenses'\r\n   ERROR: Evaluation of query \"deps(//tensorflow/tools/pip_package:build_pip_package)\" failed: errors were encountered while computing transitive closure\r\n   Building: no action\r\n\r\nDuring the command, bazel removes the two files from its cache.\r\n\r\nThe argument could be made that this is a bazel issue. However, I think it might also be a tensorflow issue, at least for the license.txt file, because that file name is apparently wrong (it should be license.rst.txt).", "comments": ["I have found a workaround. I created a local directory named manual-download, and copied into it 3 files which bazel had been unable to download:\r\n\r\ncurl-7.49.1.tar.gz  license.txt  zlib-1.2.11.tar.gz\r\n\r\nI verified that the file hashes were consistent with workspace.bzl. I then ran:\r\n\r\n  bazel fetch --experimental_distdir manual-download //tensorflow/tools/pip_package:build_pip_package\r\n\r\nThis worked. After that, I was able to build tensorflow with:\r\n\r\n  bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nNote that getting license.txt was itself a problem. The path to the file, from workspace.bzl, is:\r\n\r\n  https://docs.python.org/2.7/_sources/license.txt\r\n\r\nHowever, there is no such filename - you get a 404 error. Instead, download this:\r\n\r\n  https://docs.python.org/2.7/_sources/license.rst.txt\r\n\r\nSo, download the file and rename it to license.txt. The next problem you'll see is that the file hash is wrong - it doesn't match the file hash in workspace.bzl. The issue is that you have to edit the file, and change the year from 2018 to 2017.", "This is interesting. Our CI does not run into this problem.\r\nCould you fill in the issue template to help us see what is different in your setup?", "I suspect the issue is that I'm behind a corporate firewall, which does \"interesting\" things with https, I get hit with this problem, under various guises, regularly.\r\n\r\nAt first, it looked to me like the issue was that the Python 2.7 license filename had changed from license.txt to license.rst.txt, and the file hash was wrong. For more explanation, see my earlier comment, which I've expanded up to explain this aspect of workaround. It seems to me you're saying the Python 2.7 license file isn't an issue for you. I still don't understand why!\r\n\r\nIf you don't see any issue, then this bug should be resolved. Still, I imagine other people, building Tensorflow behind other \"interesting\" corporate firewalls, will hit the same kind of issue. If so, my workaround may prove useful for them.", "How about this theory?\r\nOur builds currently rely on `mirrors.bazel.build` hosting a mirror for each and every external dependency we have. This site is hosted on GCP.\r\nIf your corporate firewall blocked or limited access to GCP, you will fall back to the original URL. That, combined with the underlying change that happened in the license files can explain this problem.\r\n\r\nCan you confirm if your access to GCP is blocked by running:\r\n```\r\ncurl https://mirror.bazel.build/docs.python.org/2.7/_sources/license.txt\r\n```\r\n\r\nRegardless, we should update this license dependency, maybe even use a version from git for the original license to avoid such disruptions.", "Results with curl. First, for reference, I test a file which works, on 3 different machines (machines A, B, C):\r\n\r\n  curl http://ftp.exim.org/pub/pcre/pcre-8.39.tar.gz | sha256sum\r\n\r\nFile hash checks out on all machines. Next, on the Python 2.7 license file, from the bazel mirror:\r\n\r\n  curl https://mirror.bazel.build/docs.python.org/2.7/_sources/license.txt | sha256sum\r\n\r\nOn machines A and B, this fails with:\r\n\r\n  curl: (60) SSL certificate problem: unable to get local issuer certificate\r\n\r\nOn machine C, this works. Machine C did not have any problems building Tensorflow - no workaround was needed.\r\n\r\nOn all 3 machines, I was able to download this file using wget:\r\n\r\n  wget https://mirror.bazel.build/docs.python.org/2.7/_sources/license.txt\r\n\r\nSo, it would seem that curl is more discriminating than wget, somehow, when it comes to checking certificates. That explains part of what has been puzzling me: why is it that I can download the problematical files with no problem, while bazel can't.  It seems it is because I use wget, while bazel uses curl.", "the python license issue should be resolved.\r\nI think the zlib issue is due to access to the resources.", "I have the same issue with bazel **0.11.0** (`Error downloading [https://mirror.bazel.build/docs.python.org/2.7/_sources/license.rst.txt)`\r\nI tried creating a manual-download folder and this command: `bazel fetch --experimental_distdir manual-download //tensorflow/tools/pip_package:build_pip_package`, but --experimental_distdir option is not recognized. \r\ndo you know if --experimental_repository_cache is the replacement?\r\n\r\nIs there a known solution for this issue?\r\n"]}]