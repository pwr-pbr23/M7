[{"number": 25886, "title": "[tf c++ api] Does `GraphExecutionState::OptimizeGraph` take up too much cpu during repeated `session->Run()`?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  3.18.6-2.el7.centos.x86_64\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.9.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): bazel-0.10.0\r\n- GCC/Compiler version (if compiling from source): gcc-4.8.5\r\n\r\n**Describe the current behavior**\r\nI load tf model(protobuf) into c++ to do inference. The code roughly is:\r\n```cpp\r\nint TensorflowInfer::LoadModel() {\r\n  tensorflow::Status session_status = tensorflow::NewSession(tensorflow::SessionOptions(), &session_pointer);\r\n  infer_sess_.reset(session_pointer);\r\n\r\n  //Read the pb file into the grapgdef\r\n  tensorflow::GraphDef graphdef;\r\n  tensorflow::Status status_load =\r\n      tensorflow::ReadBinaryProto(tensorflow::Env::Default(), model_path, &graphdef);\r\n\r\n  tensorflow::graph::SetDefaultDevice(device, &graphdef);\r\n\r\n  // Add the graph to the session\r\n  tensorflow::Status status_create = infer_sess_->Create(graphdef);\r\n  return 0;\r\n}\r\n\r\nint TensorflowInfer::Infer(\r\n    const tensorflow::Tensor &input_tensor,\r\n    tensorflow::Tensor &output_tensor) {\r\n  std::vector<tensorflow::Tensor> outputs;\r\n\r\n  tensorflow::Status status = infer_sess_->Run({\"input\", input_tensor}, {\"output\"}, {}, &outputs);\r\n\r\n  output_tensor = outputs[0];\r\n\r\n  return 0;\r\n}\r\n```\r\nThen i run `TensorflowInfer::Infer()` with many different `input_tensor`, and make a profiling using `valgrind`, the result call graph is:\r\n![image](https://user-images.githubusercontent.com/705582/53008720-f4d59700-3474-11e9-90d1-724002973865.png)\r\n\r\nAs this figure shown, while the intensive eigen computaion(left part in this figure) takes 22.64% cpu, the `GraphExecutionState::OptimizeGraph` takes as much as 9.96% cpu. \r\n\r\nThe problem is:\r\n1. Is graph optimization need take that much cpu? \r\n2. Is it possible that the session reuse one `OptimizeGraph` during many infer() runs?", "comments": ["Here is my thought:\r\nI run the program locally, The `session` will be a `DirectSession`. And The `Run()` for a `DirectSession` does have some reuse mechanism https://github.com/tensorflow/tensorflow/blob/5f561241785462a7d866d6c92265f5d7ab63120d/tensorflow/core/common_runtime/direct_session.cc#L751-L753\r\nI wonder why the OptimizeGraph still takes that much cpu circles, and how to avoid it?", "I make a new profiling with much more data, and the `OptimizeGraph` step seems to negligible. Thus, the problem is due to the test date is not enough. I close the issue."]}, {"number": 25885, "title": "Ability to hook Iterator initialisation while using estimators", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI find it hard to track the epochs number while training with Estimators.\r\nOne possibility would be to hook the Dataset Iterator initialisation between epoch\r\ncombining the use of dataset.repeat(num_epochs) - (Assuming the Estimator is initialising the iterator between epochs) \r\n \r\n**Will this change the current api? How?**\r\nNo - just add another training hook.\r\n\r\n**Who will benefit with this feature?**\r\nEvery developer / researcher that would like to track the number of training epochs in addition to\r\nthe training steps\r\n\r\n**Any Other info.**\r\nOther ways to track the epoch number should be fine as well", "comments": ["@keotic, \r\nSorry for the delayed response. In the **`Tensorflow Version 2.x`**, since we use [TF Keras](https://www.tensorflow.org/api_docs/python/tf/keras) predominantly and don't use [Estimators](https://www.tensorflow.org/guide/estimator) much, can you please let us know if this Feature is still relevant? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25884, "title": "TF compilation warning removed", "body": "Variables are initialised and warnings in the compilation of tensorflow are removed. ", "comments": ["@PariksheetPinjari909 can you please resolve conflicts.", "@rthadur resolved the conflict\r\n@saeta Can you please approve again. TIA"]}, {"number": 25883, "title": "When the TF 2.0 stable edition releases \uff1f", "body": "I really want to know\uff01 Anyone could tell me\uff1f Thank You\uff01", "comments": ["@BigBigWolf-AI Team is working very hard on TF2.0. As of now, release date is not announced yet. \r\n\r\nI am closing the issue as it is not related to build/install or bug/performance. Thanks!", "OK! I have known that! Thanks!"]}, {"number": 25882, "title": "tf.image.random_jpeg_quality only products images of single jpeg quality", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09\r\n- Python version: 2.7.13\r\n- CUDA/cuDNN version: 9.0 / 7.0.3\r\n- GPU model and memory: GeForce GTX 1080 Ti, 10405 MB\r\n\r\n**Describe the current behavior**\r\ntf.image.random_jpeg_quality generates random jpeg quality on graph creation, which is then fixed.\r\n\r\n**Describe the expected behavior**\r\ntf.image.random_jpeg_quality generates random jpeg quality for each image/batch of images passed through it.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimg = np.random.randint(0, 256, (100, 200, 3), dtype=np.uint8)\r\n\r\ntf_img = tf.placeholder(tf.uint8)\r\njpeg_augment = tf.image.random_jpeg_quality(tf_img,\r\n                                            min_jpeg_quality=20,\r\n                                            max_jpeg_quality=90)\r\n\r\nsess_config = tf.ConfigProto()\r\nsess_config.gpu_options.allow_growth = True\r\n\r\nwith tf.Session(config=sess_config) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    results = []\r\n    for i in range(25):\r\n        augmented = sess.run([jpeg_augment], feed_dict={tf_img: img})[0]\r\n        results.append(augmented)\r\n\r\nresults = np.array(results)\r\nsame_as_first = results == results[0, ...]\r\nall_equal = np.all(same_as_first)\r\nprint('all_equal: {}'.format(all_equal))\r\nassert not all_equal\r\n```\r\n\r\nThe code causing this is located at:\r\nhttps://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/ops/image_ops_impl.py#L1634\r\n", "comments": ["@drpngx Can you answer the issue or reassign?", "I think this is more of a \"type:bug\" than \"type:feature\"", "The root cause of this problem is that the `EncodeJpeg` op's `quality` parameter is a fixed `Attr`, not a variable `Input`. Fixing `random_jpeg_quality` will necessitate a new version of `EncodeJpeg` with a different signature. @drpngx I'd be happy to put in a PR for this issue if that's ok.", "Sounds like a good idea. @frreiss are you familiar with the op replacement process? You have to:\r\n\r\n1. Create a new op, call it v2.\r\n2. Check it in, with tests etc and wait for two weeks.\r\n3. Replace the call from v1 to v2.\r\n\r\nThe v2 version should be a strict superset of v1. Ideally it should behave exactly like v1 with the default options.", "@drpngx yes, I've done an op replacement in the past with the `accumulate_n` op.\r\n\r\nI'll break this work down into a few smaller PRs to make the diff size more manageable:\r\n1. Add a new C++ op, `EncodeJpegV2`, along with C++ regression tests.\r\n2. Modify the `random_jpeg_quality` Python API to use `EncodeJpegV2` and also fix issue #25882.\r\n3. Change the other Python code in `image_ops_impl.py` that uses `gen_image_ops.encode_jpeg` to use `gen_image_ops.encode_jpeg_v2`. \r\n\r\nRegarding the first step: I see that `EncodeJpeg` has a number of other arguments that are currently static attributes:\r\n```c++\r\nREGISTER_OP(\"EncodeJpeg\")\r\n    .Input(\"image: uint8\")\r\n    .Attr(\"format: {'', 'grayscale', 'rgb'} = ''\")\r\n    .Attr(\"quality: int = 95\")\r\n    .Attr(\"progressive: bool = false\")\r\n    .Attr(\"optimize_size: bool = false\")\r\n    .Attr(\"chroma_downsampling: bool = true\")\r\n    .Attr(\"density_unit: {'in', 'cm'} = 'in'\")\r\n    .Attr(\"x_density: int = 300\")\r\n    .Attr(\"y_density: int = 300\")\r\n    .Attr(\"xmp_metadata: string = ''\")\r\n    .Output(\"contents: string\")\r\n\r\n```\r\nDo you have any preference about which arguments I should turn into 0-D tensors in the new op? ", "I will let the API reviewer comment. My general intuition is that numbers can be changed, and everything else, in the worst case put in the graph. So `quality`, `{x,y}_density` would be input tensors.", "(For API owners) This might be more appropriate for the [TensorFlow Addons](https://github.com/tensorflow/addons) repository, which has a number of special use ops like this. In either case, in the op registration code above, the Attrs should be Inputs. (Inputs are more flexible.)", "I was about to start work on this, but it looks like an anonymous Googler has already committed a patch. Someone should close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25882\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25882\">No</a>\n"]}, {"number": 25881, "title": "can't split error", "body": "Traceback (most recent call last):\r\n  File \"train.py\", line 82, in <module>\r\n    tf.app.run(main)\r\n  File \"/home/madono/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 52, in main\r\n    datasets = mnist.load_data()[:10000]\r\nTypeError: 'ConditionalDataset' object is not subscriptable\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25880, "title": "Migration Guide from Tensorflow Android to Tensorflow Lite", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow Android 1.12.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI'm currently using a custom frozen model in .pb format for real-time face detection with Tensorflow Android. Everything is working fine except that the model is quite heavy and my app stops working from time to time. Logcat shows that the ActivityManager force closes my app, which I think is because it is using too many resources despite running in a different thread. Tensorflow Lite seems to be a good solution with continuous support and better performance. I want to migrate my existing solution to TFLite but I can't find any clear documentation on how to do so. The TensorFlowInferenceInterface methods and parameters are very different from Interpreter in TFLite. \r\n\r\nI followed the TFLiteObjectDetectionAPIModel samples for both Tensorflow Android and TFLite but I can't follow on the differences as someone who is not familiar with Tensorflow. Please provide documentation on how to migrate especially when you are deprecating Tensorflow Android already.\r\n\r\n**Will this change the current api? How?**\r\nI think no if there is proper documentation.\r\n\r\n**Who will benefit with this feature?**\r\nDevelopers currently using Tensorflow Android with performance issues\r\n\r\n**Any Other info.**\r\nNone", "comments": ["@karloberas Are these the documents you were looking for?\r\nhttps://www.tensorflow.org/lite/convert\r\nhttps://www.tensorflow.org/lite/convert/cmdline_examples", "Is there any tflite demo on Android using C++ API rather than Java API? I got different result and inference time with tflite-nightly `implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'` after I rebuild my project", "Can you be more specific about the result/inference differences you're observing using the Java APIs? One thing to note is that using multi-dimensional Java arrays can yield surprisingly poor performance results, which is why we recommend using flat ByteBuffer input/output types.", "I just converted our internal project by following the changes on the examples, and the migration is still failing because of either one of the issues:\r\n1. `Cannot convert between a TensorFlowLite buffer with 30420 bytes and a ByteBuffer with 2076672 bytes.`\r\n2. `Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/pad.cc:74 SizeOfDimension(op_context->paddings, 0) != op_context->dims (4 != 1)Node number 0 (PAD) failed to prepare.`\r\n\r\nis there any clear documentation of differences between android and lite and its java part migration?\r\nThe documents posted here describe how to convert the models, not the projects and basically the calls from:\r\n\r\n```\r\n        // Copy the input data into TensorFlow.\r\n        inferenceInterface.feed(INPUT_NAME, processBitmap(bitmap), 1, INPUT_SIZE, INPUT_SIZE, 3);\r\n\r\n        // Run the inference call.\r\n        inferenceInterface.run(new String[]{OUTPUT_NAME});\r\n\r\n        // Copy the output Tensor back into the output array.\r\n        inferenceInterface.fetch(OUTPUT_NAME, tfOutput);\r\n```\r\n\r\nto:\r\n```\r\n      tfLite.run(tfInput, tfOutput);\r\n      // or\r\n      tfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n```\r\n\r\n\r\n", "If you need to resize the input tensor, you should first call Interpreter.resizeInputTensor(`inputIndex`, `newShape`). Otherwise, when you feed the interpreter a flat ByteBuffer, it will try to fit that buffer into the existing tensor's buffer.\r\n\r\nWhat shape did you use during conversion? And what shape are you using for your input data?", "Good point @jdduke. We have been feeding in a long float array with only one dimension (since it worked that way with tensorflow android) but the expected shape should be different.\r\nWe figured we can obtain the current output shape that way:\r\n``` \r\nint[] shape = recognizer.tfLite.getOutputTensor(0).shape();\r\nfloat[][][][]  tfOutput = new float[shape[0]][shape[1]][shape[2]][shape[3]];\r\n``` \r\n\r\nnow it works :)", "Now - where its running with unquantized model, we try to switch to quantized model and are facing the old issue: \r\n```  \r\njava.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 2076672 bytes and a ByteBuffer with 519168 bytes.\r\n```\r\nWe are not using `runForMultipleInputsOutputs` but could this be an issue here?\r\nPS. \r\nWe switch numBytesPerChannel accordingly and still the obtained source from the tensor looks the same after loading the quantized model.\r\n\r\nThanks for any advice.", "@karloberas,\r\nCan you please refer to [TF Lite Android Guide](https://www.tensorflow.org/lite/guide/android) and let us know if it helps? Also, please refer to [TF Lite Guide](https://www.tensorflow.org/lite/guide) for more detailed documentation on different aspects of `TF Lite`. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25879, "title": "DOC: Fix typos in readme.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nGooglers can find more info about SignCLA and this PR by [following this link](go/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25879).\n\n<!-- need_sender_cla -->", "@csukuangfj please sign CLA", "I signed it!", "@csukuangfj thank you but our system still shows there is no CLA signed , make sure you give all necessary information , ex: email ", "Please check again. I really signed it.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25879) for more info**.\n\n<!-- ok -->"]}, {"number": 25878, "title": "Invalid argument: No OpKernel was registered to support Op 'ListDiff' with these attrs.", "body": "", "comments": []}, {"number": 25877, "title": "AttributeError: 'NoneType' object has no attribute 'get_file'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy]\r\nwhen i run densenet, i got stuck into this bug.\r\n`  File \"D:/Documents/GitHub/DenseNet-master/imagenet_inference.py\", line 26, in <module>\r\n    print('Predicted:', decode_predictions(preds))\r\n  File \"D:\\Applications\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras_applications\\imagenet_utils.py\", line 224, in decode_predictions\r\n    fpath = keras_utils.get_file(\r\nAttributeError: 'NoneType' object has no attribute 'get_file'\r\n`\r\n**System information**\r\n- Win10\r\n- Python version:3.6", "comments": ["@YibaYan Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!"]}, {"number": 25876, "title": "Does ConditionalAccumulator have any other reduction_type?", "body": "![image](https://user-images.githubusercontent.com/29396595/52999122-ee88f000-345f-11e9-94bf-72550d222b1d.png)\r\n\r\nWhen I use SyncReplicasOptimizer, I want to sum the gradients but not to mean them. The motivation to do this is, I want to train a big model with big batch_size other than mini batch_size. So SyncReplicasOptimizer has any other reduction_types? (For example sum, max...) ", "comments": ["You can try other loss reduction types such as https://www.tensorflow.org/api_docs/python/tf/losses/Reduction\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25876\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25876\">No</a>\n"]}, {"number": 25875, "title": "R1.10", "body": "ddd", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nGooglers can find more info about SignCLA and this PR by [following this link](go/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25875).\n\n<!-- need_sender_cla -->", "@BingoInGithub please sign CLA "]}, {"number": 25874, "title": "Passing variable length sentences to Tensorflow LSTM", "body": "\r\nI have a tensorflow LSTM model for predicting the sentiment. I build the model with the maximum sequence length 150. (Maximum number of words) While making predictions, i have written the code as below:\r\n\r\n    batchSize = 32\r\n    maxSeqLength = 150\r\n\r\n    def getSentenceMatrix(sentence):\r\n        arr = np.zeros([batchSize, maxSeqLength])\r\n        sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\r\n        cleanedSentence = cleanSentences(sentence)\r\n        cleanedSentence = ' '.join(cleanedSentence.split()[:150])\r\n        split = cleanedSentence.split()\r\n        for indexCounter,word in enumerate(split):\r\n            try:\r\n                sentenceMatrix[0,indexCounter] = wordsList.index(word)\r\n            except ValueError:\r\n                sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\r\n        return sentenceMatrix\r\n\r\n    input_text = \"example data\"\r\n    inputMatrix = getSentenceMatrix(input_text)\\\r\n\r\nIn the code i'm truncating my input text to 150 words and ignoring remaining data.Due to this my predictions are wrong.\r\n\r\n    cleanedSentence = ' '.join(cleanedSentence.split()[:150])\r\nI know that if we have lesser length than sequence length we can pad with zero's. What we need to do if we have more length.\r\nCan anyone suggest me the best way to do this. Thanks in advance.\r\n", "comments": ["Please take a look at similar questions answered on [SO](https://stackoverflow.com/questions/34670112/how-to-deal-with-batches-with-variable-length-sequences-in-tensorflow).\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as a minimal code snippet to reproduce the issue reported. Thanks!\r\n", "I know it's not a bug or feature request, but i couldn't found anywhere how to solve this problem, So i posted in stack overflow also,but i didn't get any response.\r\n[here](https://stackoverflow.com/questions/54421451/issue-with-maximum-sequence-length-while-making-predictions-using-lstm-model)"]}, {"number": 25873, "title": "Update abseil_cpp.cmake", "body": "no such file \"absl_internal_malloc_internal.lib\"", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nGooglers can find more info about SignCLA and this PR by [following this link](go/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25873).\n\n<!-- need_sender_cla -->", "@vzack2001 please sign CLA"]}, {"number": 25872, "title": "Remove duplicated LD_LIBRARY_PATH", "body": "`LD_LIBRARY_PATH` already added at head, remove the one at tail.", "comments": []}, {"number": 25871, "title": "TF Lite delegate_data_test warnings fix", "body": "Warning fixed.", "comments": ["@nutsiepully  \r\n\r\nPlease review the changes, thanks.", "@nutsiepully \r\n\r\nyeah fixed the noticed problem, please have look once again and thanks."]}, {"number": 25870, "title": "Error installing/running Tensorflow-GPU version", "body": "Unable to install/run tensorflow-gpu in my system.  Below error am getting.  I tried with Python 3.7, then downgraded it to py 3.6.8 and then to 3.6.5, but the end result is the same.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): installed using pip3\r\n- TensorFlow version: GPU version 1.12.0\r\n- Python version: 3.7.x, 3.6.8, 3.6.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: Quadro M1200\r\n- GPU model and memory: NVIDIA 4 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nTraceback (most recent call last):\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in \r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\nreturn _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"chatbot.py\", line 4, in \r\nimport tensorflow as tf\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow__init__.py\", line 24, in \r\nfrom tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python__init__.py\", line 49, in \r\nfrom tensorflow.python import pywrap_tensorflow\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in \r\nraise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in \r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\nreturn _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@vinodk333 Please check [tested build config](https://www.tensorflow.org/install/source_windows#gpu). For your version you need to downgrade some modules, build and install the TF1.12 version. Thanks!", "@vinodk333 Please also check there is a known issue with windows10 on character [limit](https://www.howtogeek.com/266621/how-to-make-windows-10-accept-file-paths-over-260-characters/). Thanks!", "I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!"]}, {"number": 25869, "title": "Remove duplicated LD_LIBRARY_PATH in `CONTRIBUTING.md`", "body": "non-functional, fixing a readme typo.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\nGooglers can find more info about SignCLA and this PR by [following this link](go/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25869).\n\n<!-- unknown_author -->"]}, {"number": 25868, "title": "TF Lite nnapi_delegate_test update test", "body": "Test updated.\r\n\r\nRef PiperOrigin-RevId: 227039711", "comments": ["@nutsiepully  \r\n\r\nPlease review the changes, thanks."]}, {"number": 25867, "title": "TF Lite nnapi_delegate non-trivial designated initializers not suppor\u2026", "body": "Bazel test case run nnapi_delegate_test non-trivial designated initializers not supported fix", "comments": ["@aselle \r\n\r\nPlease review the changes, thanks.", "@Dayananda-V please resolve conflicts", "@Dayananda-V gentle ping to resolve the conflicts.", "Latest code doesn't have this issue, so closing PR."]}, {"number": 25866, "title": "CUDA-related crash when training simple network", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, also tested on Ubuntu 18.04 on same machine\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0 and also 1.13.0rc2\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0 and also 10.0, cudnn 7.2 (appropriate cudnn versions for 9.0 and 10.0 respectively)\r\n- GPU model and memory: GTX 1080 TI 11gb\r\n\r\n**Describe the current behavior**\r\n\r\nDepending on the batch size, during training there is eventually (After 1000 or so steps, sometimes a lot more steps with smaller batches) a CUDA-related crash, with one of a number of error messages. When the crash happens, sometimes the entire display on my computer goes black and resets.\r\n\r\nThis happens despite:\r\n- No OOM errors on startup\r\n- Using a bare-bones example CNN\r\n- Trying with and without limiting the available memory with the option `tf.GPUOptions(per_process_gpu_memory_fraction=0.1, allow_growth=False)`\r\n- Restarting the machine\r\n- Reinstalling latest NVIDIA drivers\r\n- Reinstalling CUDA\r\n- Trying CUDA 9 (TF 1.12.0, conda env) and CUDA 10 (TF 1.13.0rc2, manual installation)\r\n\r\nI'm beginning to suspect that this is a hardware issue but the GPU has never had any problems and can still run high intensity games for hours with no problems. There are also no abnormal temperature or other sensor readings in the diagnostics when this happens.\r\n\r\nIs there anything I can do to narrow down whether this is a hardware problem?\r\n\r\n**Describe the expected behavior**\r\n\r\nNo crash\r\n\r\n**Code to reproduce the issue**\r\nThis code is taken from the [CNN tutorial](https://www.tensorflow.org/tutorials/estimators/cnn) with the exception of a line added to try limiting the GPU memory usage. The crash happens with or without this line.\r\n\r\nFor this particular example, a batch size of 50 was enough to consistently reproduce the error. The crash happens earlier with larger batch sizes however.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ndef cnn_model_fn(features, labels, mode):\r\n\t\"\"\"Model function for CNN.\"\"\"\r\n\t# Input Layer\r\n\tinput_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\r\n\r\n\t# Convolutional Layer #1\r\n\tconv1 = tf.layers.conv2d(\r\n\t  inputs=input_layer,\r\n\t  filters=32,\r\n\t  kernel_size=[5, 5],\r\n\t  padding=\"same\",\r\n\t  activation=tf.nn.relu)\r\n\r\n\t# Pooling Layer #1\r\n\tpool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\r\n\r\n\t# Convolutional Layer #2 and Pooling Layer #2\r\n\tconv2 = tf.layers.conv2d(\r\n\t  inputs=pool1,\r\n\t  filters=64,\r\n\t  kernel_size=[5, 5],\r\n\t  padding=\"same\",\r\n\t  activation=tf.nn.relu)\r\n\tpool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\r\n\r\n\t# Dense Layer\r\n\tpool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\r\n\tdense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\r\n\tdropout = tf.layers.dropout(\r\n\t  inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\r\n\r\n\t# Logits Layer\r\n\tlogits = tf.layers.dense(inputs=dropout, units=10)\r\n\r\n\tpredictions = {\r\n\t  # Generate predictions (for PREDICT and EVAL mode)\r\n\t  \"classes\": tf.argmax(input=logits, axis=1),\r\n\t  # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\r\n\t  # `logging_hook`.\r\n\t  \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\r\n\t}\r\n\r\n\tif mode == tf.estimator.ModeKeys.PREDICT:\r\n\t\treturn tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\r\n\r\n\t# Calculate Loss (for both TRAIN and EVAL modes)\r\n\tloss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n\r\n\t# Configure the Training Op (for TRAIN mode)\r\n\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\toptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\r\n\t\ttrain_op = optimizer.minimize(\r\n\t\t\tloss=loss,\r\n\t\t\tglobal_step=tf.train.get_global_step())\r\n\t\treturn tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\n\t# Add evaluation metrics (for EVAL mode)\r\n\teval_metric_ops = {\r\n\t  \"accuracy\": tf.metrics.accuracy(\r\n\t\t  labels=labels, predictions=predictions[\"classes\"])\r\n\t}\r\n\treturn tf.estimator.EstimatorSpec(\r\n\t  mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n\t  \r\n# Load training and eval data\r\n((train_data, train_labels),\r\n (eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\r\n\r\ntrain_data = train_data/np.float32(255)\r\ntrain_labels = train_labels.astype(np.int32)  # not required\r\n\r\neval_data = eval_data/np.float32(255)\r\neval_labels = eval_labels.astype(np.int32)  # not required\r\n\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1, allow_growth=False)\r\nsession_config = tf.ConfigProto(gpu_options=gpu_options)\r\nrun_config = tf.estimator.RunConfig(session_config=session_config)\t\r\n\r\n# Create the Estimator\r\nmnist_classifier = tf.estimator.Estimator(\r\n    model_fn=cnn_model_fn, model_dir=\"./run\", config=run_config)\r\n\t\r\n# Set up logging for predictions\r\ntensors_to_log = {\"probabilities\": \"softmax_tensor\"}\r\n\r\n# Train the model\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n    x={\"x\": train_data},\r\n    y=train_labels,\r\n    batch_size=50,\r\n    num_epochs=None,\r\n    shuffle=True)\r\n\r\n# train one step and display the probabilties\r\nmnist_classifier.train(\r\n    input_fn=train_input_fn,\r\n    steps=10000000)\r\n\r\n```\r\n\r\n**Other info / logs**\r\n\r\n\r\n*Example error message 1*\r\n```\r\n2019-02-21 16:43:26.973428: E tensorflow/stream_executor/cuda/cuda_driver.cc:1011] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered ::\r\n2019-02-21 16:43:26.973442: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-02-21 16:43:26.984939: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1\r\n```\r\n\r\n*Example error message 2*\r\n```\r\n2019-02-19 12:32:46.833634: E tensorflow/stream_executor/cuda/cuda_driver.cc:1011] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure ::\r\n2019-02-19 12:32:46.833656: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-02-19 12:32:46.849082: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1\r\n```\r\n\r\n*Example error message 3*\r\n```\r\n2019-02-19 13:19:11.936586: E tensorflow/stream_executor/cuda/cuda_driver.cc:1011] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure ::\r\n2019-02-19 13:19:12.200064: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-02-19 13:19:12.204540: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-02-19 13:19:12.208523: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-02-19 13:19:12.213922: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-02-19 13:19:12.218206: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-02-19 13:19:12.223482: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-02-19 13:19:12.228027: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-02-19 13:19:12.233217: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-02-19 13:19:12.237655: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: GPU sync failed\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"cnn.py\", line 102, in <module>\r\n    steps=10000000)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 358, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1124, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1158, in _train_model_default\r\n    saving_listeners)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1407, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 676, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1171, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1270, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\six.py\", line 693, in reraise\r\n    raise value\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1255, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1327, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1091, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: GPU sync failed\r\n```\r\n\r\n*Example Output and Error Message when running under cuda-memcheck on Ubuntu*\r\n```\r\n========= CUDA-MEMCHECK\r\nINFO:tensorflow:Using config: {'_model_dir': './run', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff649578710>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nWARNING:tensorflow:From /home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:From /home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2019-02-20 21:57:36.622307: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-02-20 21:57:36.717892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-02-20 21:57:36.719031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:06:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.27GiB\r\n2019-02-20 21:57:36.719054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-02-20 21:57:38.167083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-20 21:57:38.167111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2019-02-20 21:57:38.167117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2019-02-20 21:57:38.167550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9930 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nWARNING:tensorflow:From /home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nINFO:tensorflow:Saving checkpoints for 0 into ./run/model.ckpt.\r\nINFO:tensorflow:loss = 2.3112628, step = 0\r\nINFO:tensorflow:global_step/sec: 1.52266\r\nINFO:tensorflow:loss = 2.2921445, step = 100 (65.674 sec)\r\nINFO:tensorflow:global_step/sec: 1.52365\r\nINFO:tensorflow:loss = 2.2698283, step = 200 (65.633 sec)\r\nINFO:tensorflow:global_step/sec: 1.52262\r\nINFO:tensorflow:loss = 2.2548892, step = 300 (65.676 sec)\r\nINFO:tensorflow:global_step/sec: 1.51757\r\nINFO:tensorflow:loss = 2.2342505, step = 400 (65.895 sec)\r\n2019-02-20 22:02:30.594297: E tensorflow/stream_executor/cuda/cuda_driver.cc:684] failed to enqueue async memset operation: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-02-20 22:02:30.618259: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x55f0692849d0: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-02-20 22:02:30.618293: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x55f0692849d0: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-02-20 22:02:30.618303: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x55f0692849d0: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\nTraceback (most recent call last):\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: cuDNN Backward Filter function launch failure : input shape([500,1,28,28]) filter shape([5,5,1,32])\r\n\t [[{{node gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter}} = Conv2DBackpropFilter[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, ConstantFolding/gradients/conv2d/Conv2D_grad/ShapeN-matshapes-1, gradients/conv2d/Relu_grad/ReluGrad, ^gradients/conv2d/BiasAdd_grad/BiasAddGrad)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"cnn.py\", line 102, in <module>\r\n    steps=10000000)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1207, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1241, in _train_model_default\r\n    saving_listeners)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1471, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 671, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1156, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1255, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1240, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1312, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1076, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: cuDNN Backward Filter function launch failure : input shape([500,1,28,28]) filter shape([5,5,1,32])\r\n\t [[node gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter (defined at cnn.py:59)  = Conv2DBackpropFilter[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, ConstantFolding/gradients/conv2d/Conv2D_grad/ShapeN-matshapes-1, gradients/conv2d/Relu_grad/ReluGrad, ^gradients/conv2d/BiasAdd_grad/BiasAddGrad)]]\r\n\r\nCaused by op 'gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter', defined at:\r\n  File \"cnn.py\", line 102, in <module>\r\n    steps=10000000)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1207, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1237, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"cnn.py\", line 59, in cnn_model_fn\r\n    global_step=tf.train.get_global_step())\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 400, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 519, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 630, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 814, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 408, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 814, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\", line 526, in _Conv2DGrad\r\n    data_format=data_format)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1092, in conv2d_backprop_filter\r\n    dilations=dilations, name=name)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n...which was originally created as op 'conv2d/Conv2D', defined at:\r\n  File \"cnn.py\", line 102, in <module>\r\n    steps=10000000)\r\n[elided 3 identical lines from previous traceback]\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"cnn.py\", line 17, in cnn_model_fn\r\n    activation=tf.nn.relu)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 417, in conv2d\r\n    return layer.apply(inputs)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 817, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 374, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 757, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 194, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 868, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 520, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 204, in __call__\r\n    name=self.name)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 957, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n\r\nInternalError (see above for traceback): cuDNN Backward Filter function launch failure : input shape([500,1,28,28]) filter shape([5,5,1,32])\r\n\t [[node gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter (defined at cnn.py:59)  = Conv2DBackpropFilter[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, ConstantFolding/gradients/conv2d/Conv2D_grad/ShapeN-matshapes-1, gradients/conv2d/Relu_grad/ReluGrad, ^gradients/conv2d/BiasAdd_grad/BiasAddGrad)]]\r\n\r\n========= ERROR SUMMARY: 0 errors\r\n```", "comments": ["Your code is slightly different than the one in the tutorial. Can you please make following changes and execute again,\r\n```python\r\nmnist_classifier.train(input_fn=train_input_fn, steps=1000) # currently it has steps=10000000\r\n```", "@ymodak \r\n\r\nIf the batch size is large enough, the crash happens with a smaller number of steps than 1000 to be sure. So yes, when executing again the issue still happens.\r\n\r\nI tried running furmark (a GPU stress test) on the machine and after a similar amount of time I am seeing severe artifacts across the entire screen.\r\n\r\n![img_20190222_132907235](https://user-images.githubusercontent.com/11645696/53219710-25a40f00-36b5-11e9-81af-aa7ae8feb174.jpg)\r\n\r\nSo I think we can be fairly confident that this is an issue with the graphics card hardware. If you feel that this makes it an issue not worth looking into, feel free to close this bug.\r\n", "Yes, Its fairly evident that it is more of a hardware issue. Thanks for sharing your investigation. I will close this issue, feel free to reopen if anything else comes up. Thanks!", "HI, I want to reopen this because I encounter simlilar issue. \r\nI've run a custom model for more than 200 experimentation on 2 month and some scanning parameter training for 3 days without any issue.\r\n\r\nPreviously NO ISSUES was on driver 4.10 (from PPA), CUDA 9.0, Cudnn 7.3.1 \r\nUbuntu 16.04 \r\nTensorflow install from binary \r\nTensorFlow version (use command below): 1.12.0\r\nPython 3.6\r\nGPU model and memory: GTX 1080 TI 11gb \r\n\r\nFor Now, ISSUES with: \r\nNvidia driver 418.43\r\nTensorFlow installed from: source\r\nTensorFlow version: 1.13.0\r\nPython version: 3.6\r\nBazel version (if compiling from source): 0.21.0 (higher version block building with a conveniant message that we must use 0.21.0) \r\nGCC/Compiler version (if compiling from source): GCC 5.4.0, recommended one 4.8.5 was failing on build.\r\nCUDA/cuDNN version: 10.1 (tried 10.0 too)\r\ncudnn 7.4.2 (tried 7.5.1)\r\nGPU model and memory: GTX 1080 TI 11gb  \r\n\r\n```\r\n tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-03-19 23:31:22.049885: E tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-03-19 23:31:22.049943: E tensorflow/stream_executor/stream.cc:328] Error recording event in stream: error recording CUDA event on stream 0x5f6f9f0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.\r\n2019-03-19 23:31:22.049953: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2019-03-19 23:31:22.049964: I tensorflow/stream_executor/stream.cc:5027] [stream=0x5f6f950,impl=0x5f6f130] did not memcpy host-to-device; source: 0x120b2480\r\n2019-03-19 23:31:22.049886: E tensorflow/stream_executor/cuda/cuda_driver.cc:1131] failed to enqueue async memcpy from host to device: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure; GPU dst: 0x7f98f3649d00; host src: 0x7f988c0075c0; size: 19267584=0x1260000\r\n2019-03-19 23:31:22.049972: I tensorflow/stream_executor/stream.cc:2079] [stream=0x5f6f950,impl=0x5f6f130] did not wait for [stream=0x5f6f710,impl=0x5f08840]\r\n2019-03-19 23:31:22.049978: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-03-19 23:31:22.049989: I tensorflow/stream_executor/stream.cc:2079] [stream=0x5f6f950,impl=0x5f6f130] did not wait for [stream=0x5f6f710,impl=0x5f08840]\r\n2019-03-19 23:31:22.050006: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1\r\n2019-03-19 23:31:22.050002: I tensorflow/stream_executor/stream.cc:5027] [stream=0x5f6f950,impl=0x5f6f130] did not memcpy host-to-device; source: 0x7f9b57c00b00\r\n2019-03-19 23:31:22.050006: I tensorflow/stream_executor/stream.cc:2079] [stream=0x5f6f950,impl=0x5f6f130] did not wait for [stream=0x5f6f710,impl=0x5f08840]\r\n2019-03-19 23:31:22.050085: I tensorflow/stream_executor/stream.cc:5027] [stream=0x5f6f950,impl=0x5f6f130] did not memcpy host-to-device; source: 0x7f9b57c01600\u00a0```\r\n\r\nI run models on my 1080ti weekly since january 2018 and never encounter a single issue. Nothing has change on my workstation hardware. ", "@macfly1202 \r\n\r\nSadly it looks very similar to my issue which was definitely a hardware issue. My problem also started happening without any hardware changes etc - I guess the GPU just died spontaneously. After I returned the card and got a replacement, the issue went away entirely.\r\n\r\nMaybe try running https://www.geeks3d.com/gputest/ for a period of time (mine took 5 minutes or so) to see if you can replicate the artifacts or other graphical bugs. ", "@JustASquid thanks for answer. I run gputest / pixmark Piano in full screen during 40 minutes without any issue: GPU0 - core temp: 68 \u00b0C, core usage 100%\r\nGPU0 - core temp: 68 \u00b0C, core usage 99%.\r\n\r\nI will try compiled version to check especially the conda install tensorflow which embed the cuda package directly to check if errors occurs again. ", "[SOLVED] Just if someone have this issue, I use ubuntu 16.04 and reinstall driver from linux proprietary drivers repo now  according to a tutorial with CUDA 10.0 and Cudnn 7.3.1 (https://www.pytorials.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-ubuntu/). I didn't have issue anymore with this install. \r\nSo there is something that cause issue with drivers  418.43 and cudnn 7.4.1. I had previously encounter issue with install tensorflow from PIP, conda and try built from source with same result in each case: failure reference here above.\r\nNow, hapilly, issue is solve.   ", "I'm running Windows, no issues with my GPU. I'm also getting the problem with my NVidia 1080.", "> @ymodak\r\n> \r\n> If the batch size is large enough, the crash happens with a smaller number of steps than 1000 to be sure. So yes, when executing again the issue still happens.\r\n> \r\n> I tried running furmark (a GPU stress test) on the machine and after a similar amount of time I am seeing severe artifacts across the entire screen.\r\n> \r\n> ![img_20190222_132907235](https://user-images.githubusercontent.com/11645696/53219710-25a40f00-36b5-11e9-81af-aa7ae8feb174.jpg)\r\n> \r\n> So I think we can be fairly confident that this is an issue with the graphics card hardware. If you feel that this makes it an issue not worth looking into, feel free to close this bug.\r\n\r\nHi!\r\nDid you find any solution for this?", "@ymodak : please read my comment above ; it solve my issue ; ```[SOLVED] Just if someone have this issue, I use ubuntu 16.04 and reinstall driver from linux proprietary drivers repo now according to a tutorial with CUDA 10.0 and Cudnn 7.3.1 (https://www.pytorials.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-ubuntu/). I didn't have issue anymore with this install.\r\nSo there is something that cause issue with drivers 418.43 and cudnn 7.4.1. I had previously encounter issue with install tensorflow from PIP, conda and try built from source with same result in each case: failure reference here above.\r\nNow, hapilly, issue is solve.```", "For what it's worth, I ran into this exact issue when interfacing with an NVIDIA tesla K80 via a docker container after the card had overheated. It was hard to debug as the `docker-build` process etc was working as expected. ", "i ran this error on windows 10 and GTX 1050\r\n- Restarting PC doesnt work\r\n- Kill GPU, flush memory didnt work\r\n- Windows + Ctrl + Shift + B . doesnt work\r\n-  I've make sure nothing change in the code... before this error comes, i've run training for several times w/o error\r\n- make sure adding this snippet code, before any tensorflow calculation\r\n `physical_devices = tf.config.list_physical_devices('GPU')\r\ntry:\r\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n    print(\"Setup memory growth\")\r\nexcept:\r\n    print(\"Error gpu setup memory\")` \r\n\r\nit doesn't work...\r\n- nvidia-smi running without any problem\r\n\r\nnow I'm deadly stuck, only god knows what happen and what the solution.", "I just saw a discussion that may help you.\r\n\r\nhttps://forum.faceswap.dev/viewtopic.php?t=1106"]}, {"number": 25865, "title": "linking libcuda.so.1 in a Docker image at build time", "body": "**System information**\r\n- Host system Ubuntu 18.04\r\n- TensorFlow r1.13 branch\r\n- Installing from `nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04` base docker image\r\n- Bazel 0.21.0\r\n- GCC/Compiler version (if compiling from source):\r\n- Cuda 10.0, cuDNN 7.4\r\n- RTX 2080 Ti\r\n\r\n**Describe the problem**\r\nWhen building `//tensorflow/libtensorflow.so` in a RUN step of my Dockerfile, I get this error:\r\n\r\n```\r\nERROR: /tensorflow/tensorflow/cc/BUILD:477:1: Linking of rule '//tensorflow/cc:ops/data_flow_ops_gen_cc' failed (Exit 1)\r\n/usr/bin/ld: warning: libcuda.so.1, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scc_Cops_Sdata_Uflow_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)\r\n<snip>\r\nTarget //tensorflow:libtensorflow.so failed to build\r\nERROR: /tensorflow/tensorflow/cc/BUILD:477:1 Linking of rule '//tensorflow/cc:ops/nn_ops_gen_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/bin:/usr/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/cc/ops/nn_ops_gen_cc '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U_S_Stensorflow_Scc_Cops_Snn_Uops_Ugen_Ucc___Utensorflow' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scc_Cops_Snn_Uops_Ugen_Ucc___Utensorflow -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Wl,-ldl '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..' -pthread -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -Wl,-S -Wl,-no-as-needed -pie -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/host/bin/tensorflow/cc/ops/nn_ops_gen_cc-2.params)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nINFO: Elapsed time: 229.527s, Critical Path: 139.75s\r\nINFO: 2250 processes: 2250 local.\r\n```\r\n\r\nThis only occurs when trying to build **in a RUN step**. If skip buliding and shell into the container, it builds to completion. There are several similar issues on github, two of the most similar are:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/16184\r\nhttps://github.com/tensorflow/tensorflow/issues/14573\r\nParticularly https://github.com/tensorflow/tensorflow/issues/14573#issuecomment-362424509\r\n\r\nI have tried every variation of the advice given in that thread, as well as cribbed lines from this Dockerfile that is doing a similar thing:\r\nhttps://github.com/gunan/tensorflow-docker/blob/master/gpu-devel/Dockerfile.ubuntu#L64\r\n\r\nYet, I'm still getting this message. \r\n\r\nI don't know what exactly nvidia is doing with its cuda libraries, but I think they are not in all the same places at buildtime and runtime. I discovered this discrepancy accidentally with `RUN find / | grep libcuda.so.1` in my Dockerfile:\r\n\r\n```\r\n/usr/local/cuda-10.0/compat/libcuda.so.1\r\n```\r\nThis is the only place that libcuda.so.1 exists at build time. However, even if I symlink from `/usr/local/cuda-10.0/compat/libcuda.so.1\r\n` to  a place that is definitely in `LD_LIBRARY_PATH`, _and_ make sure to pass `action_env=LD_LIBRARY_PATH=${LD_LIBRARY_PATH}`, I _still_ get this error. \r\n\r\nI'm completely stumped at this point.\r\n\r\nTo reproduce this you can use the dockerfile attached at the bottom of this post.\r\n\r\nBuild in the container - works:\r\n\r\n```\r\ndocker build -f test.dockerfile -t masontest . && docker run --vm=nvidia --rm masontest \r\ncd /tensorflow\r\nRUN bazel build \\\r\n --verbose_failures \\\r\n --spawn_strategy=standalone \\\r\n    --genrule_strategy=standalone \\\r\n    --action_env=LD_LIBRARY_PATH=${LD_LIBRARY_PATH} \\\r\n    //tensorflow:libtensorflow.so\r\n```\r\nUncomment the bazel command - the image fails to build with the linker error above.\r\n\r\n```\r\nFROM nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04\r\n# TF docker (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile)\r\n# uses cuda:10.0-base-ubuntu16.04, but we need full sources \r\n# because later in this docker we are going to compile Tensorflow from scratch\r\n# using bazel.\r\n\r\n# so we aren't prompted to set up the keyboard and other nonsense\r\nENV DEBIAN_FRONTEND noninteractive \r\n\r\nRUN set -eux; \\\r\n    apt-get update && apt-get install -y --no-install-recommends \\\r\n    autoconf \\\r\n    automake \\\r\n    build-essential \\\r\n    ca-certificates \\\r\n    g++ \\\r\n    gcc \\\r\n    git \\\r\n    # these were needed by the mnistCUDNN demo\r\n    libfreeimage3 \\\r\n    libfreeimage-dev \\\r\n    make \\\r\n    # bazel needs python2 https://github.com/tensorflow/tensorflow/issues/15618\r\n    python \\\r\n    # tf needs swig\r\n    swig \\\r\n    unzip \\\r\n    wget \\\r\n    zlib1g-dev; \\\r\n    apt-get clean; \\\r\n    rm -rf /var/lib/apt/lists/*\r\n\r\n# Tensorflow pukes when you try this with 0.22.0\r\nENV BAZEL_VERSION 0.21.0\r\nWORKDIR /\r\nRUN mkdir /bazel && \\\r\n    cd /bazel && \\\r\n    wget https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \\\r\n    # curl -fSsL -o /bazel/LICENSE.txt https://raw.githubusercontent.com/bazelbuild/bazel/master/LICENSE.txt && \\\r\n    chmod +x bazel-*.sh && \\\r\n    ./bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \\\r\n    cd / && \\\r\n    rm -f /bazel/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\r\n\r\nRUN echo $(g++ --version)\r\nRUN echo $(bazel version)\r\n\r\n### TENSORFLOW \r\nENV TENSORFLOW_VERSION r1.13\r\nRUN git clone https://github.com/tensorflow/tensorflow.git && \\\r\n    cd tensorflow && \\\r\n    git checkout ${TENSORFLOW_VERSION}\r\n\r\nENV PYTHON_LIB_PATH /usr/local/lib/python2.7/dist-packages\r\nENV PYTHONPATH /tensorflow/lib\r\nENV PYTHON_ARG /tensorflow/lib\r\n\r\nENV GCC_HOST_COMPILER_PATH /usr/bin/gcc\r\nENV CC_OPT_FLAGS=\"-march=native\"\r\nENV CUDA_TOOLKIT_PATH /usr/local/cuda\r\nENV CUDNN_INSTALL_PATH /usr/lib/x86_64-linux-gnu\r\nENV TF_CUDA_COMPUTE_CAPABILITIES 7.0 \r\nENV TF_CUDNN_VERSION 7\r\nENV TF_CUDA_VERSION 10.0\r\nENV TF_NEED_GCP 0\r\nENV TF_ENABLE_XLA 0\r\nENV TF_NEED_OPENCL 0\r\nENV TF_NEED_CUDA 1\r\nENV TF_CUDA_VERSION 10.0.0\r\nENV TF_NEED_HDFS 0\r\nENV TF_NEED_TENSORRT 0\r\nENV TF_NCCL_VERSION 2\r\nENV TF_BUILD_BRANCH r1.13\r\nENV TF_NEED_ROCM 0\r\nENV TF_NEED_OPENCL_SYCL 0\r\nENV TMP /tmp/tensorflow\r\nRUN echo \"startup --batch\" >>/etc/bazel.bazelrc\r\n\r\nENV LD_LIBRARY_PATH=\"/usr/local/lib:/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:${LD_LIBRARY_PATH}\"\r\n\r\nRUN find / | grep  libcuda.so.1\r\nWORKDIR /tensorflow\r\nRUN yes '' | ./configure\r\n# https://github.com/gunan/tensorflow-docker/blob/master/gpu-devel/Dockerfile.ubuntu#L64\r\n# Is this ln -s is necessary when building? Doesn't seem to help, and the actual libcuda ... https://github.com/tensorflow/tensorflow/issues/14573#issuecomment-362424509\r\n# Similarly, we need to workaround sandboxing issues: https://github.com/bazelbuild/bazel/issues/418\r\nRUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1\r\n# RUN bazel build \\\r\n#  --verbose_failures \\\r\n#  --spawn_strategy=standalone \\\r\n#     --genrule_strategy=standalone \\\r\n#     --action_env=LD_LIBRARY_PATH=${LD_LIBRARY_PATH} \\\r\n#     //tensorflow:libtensorflow.so\r\n```", "comments": ["Building TF inside of a Docker image succeeded at head, presumably because https://github.com/tensorflow/tensorflow/issues/16184 switched to dynamic loading of libcuda. It sure would be nice to have a working development Docker with the latest stable release but I understand that there are a lot of things that would be nice to have. I can't imagine that this is only broken for me - I'm just not doing anything special.", "Related issue - original incidence of this issue and the originally proposed fix. Will try it tonight.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/13399", "Also https://github.com/NVIDIA/nvidia-docker/issues/775", "I've marked this as \"Community Support\" because the TF team doesn't officially support building inside a Docker build process.\r\n\r\n@masonk, do the official [Docker images](https://hub.docker.com/r/tensorflow/tensorflow) not meet your use case? We provide images with the latest stable releases and with the nightly packages pre-installed with Nvidia GPU support.", "@angersson \r\n\r\nI don't _think_ that the official images meet my use-case, because I am building a Rust app that uses the rust-tensorflow (bindings to the C API), and it needs to link against libtensorflow, which I did not find in the official images.\r\n\r\nI was able to build GPU-enabled libtensorflow v1.12.0 from scratch using a custom Dockerfile, developed with great pain over a course of days. The nice thing is that, once written, it continues to work. I will reproduce that Dockerfile here:  \r\n\r\n```\r\nFROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04\r\n# TF docker (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile)\r\n# uses cuda:10.0-base-ubuntu16.04, but we need full sources \r\n# because later in this docker we are going to compile Tensorflow from scratch\r\n# using bazel.\r\n# so we aren't prompted to set up the keyboard and other nonsense\r\nENV DEBIAN_FRONTEND noninteractive \r\n\r\nRUN set -eux; \\\r\n    apt-get update && apt-get install -y --no-install-recommends \\\r\n    autoconf \\\r\n    automake \\\r\n    build-essential \\\r\n    ca-certificates \\\r\n    g++ \\\r\n    gcc \\\r\n    git \\\r\n    less \\\r\n    # these were needed by the mnistCUDNN demo\r\n    libfreeimage3 \\\r\n    libfreeimage-dev \\\r\n    # These two packages allow a TF to be built that run on multiple GPUs\r\n    # libnccl2 libnccl-dev \\\r\n    # Seraphim depends on libssl via the openssl-sys crate (which I think is a transitive dep of ctrlc)\r\n    libssl-dev \\ \r\n    lldb-7 \\\r\n    gdbserver \\\r\n    make \\\r\n    # Rust libssl crate uses pkg-config to find the openssl system headers\r\n    pkg-config \\\r\n    # bazel needs python2 https://github.com/tensorflow/tensorflow/issues/15618\r\n    python \\\r\n    # tf needs swig\r\n    swig \\\r\n    #just for my own convenience, I like this package\r\n    tree \\ \r\n    unzip \\\r\n    wget \\\r\n    zlib1g-dev; \\\r\n    apt-get clean; \\\r\n    rm -rf /var/lib/apt/lists/*\r\n\r\n# This version has to correspond to the appropriate bazel version\r\n# r1.12 can't be built with bazel 0.19 https://github.com/tensorflow/tensorflow/issues/23401#issuecomment-434681778\r\n# It also can't be built with any higher version of bazel that I tried\r\n# Note to self for the future: r1.13 can be built with bazel 0.21, but not 0.22\r\nENV BAZEL_VERSION 0.18.0\r\nWORKDIR /\r\nRUN mkdir /bazel && \\\r\n    cd /bazel && \\\r\n    wget https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \\\r\n    # curl -fSsL -o /bazel/LICENSE.txt https://raw.githubusercontent.com/bazelbuild/bazel/master/LICENSE.txt && \\\r\n    chmod +x bazel-*.sh && \\\r\n    ./bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \\\r\n    cd / && \\\r\n    rm -f /bazel/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\r\n\r\nRUN echo $(g++ --version)\r\nRUN echo $(bazel version)\r\n\r\n### TENSORFLOW \r\n# rust-tensorflow works with r1.12\r\n# https://github.com/tensorflow/tensorflow/issues/25865\r\nENV TENSORFLOW_VERSION r1.12\r\nRUN git clone https://github.com/tensorflow/tensorflow.git && \\\r\n    cd tensorflow && \\\r\n    git checkout ${TENSORFLOW_VERSION}\r\n\r\nENV TF_NEED_CUDA 1\r\n\r\nWORKDIR /tensorflow\r\nRUN yes '' | ./configure\r\n# https://github.com/gunan/tensorflow-docker/blob/master/gpu-devel/Dockerfile.ubuntu#L64\r\n# This ln-s is necessary when building ... https://github.com/tensorflow/tensorflow/issues/14573#issuecomment-362424509\r\n# Similarly, we need to workaround sandboxing issues:\r\n#   https://github.com/bazelbuild/bazel/issues/418\r\n\r\n# Context for this elaborate bazel invocation is here https://github.com/tensorflow/tensorflow/issues/25865\r\n# TLDR: tf <=1.13 links to libcuda.so.1 at build time, but those headers aren't available in the nvidia docker\r\n# at build time. (They are mounted in the running container, but not at build time). So we link to\r\n# stubs to satisfy the build-time linker.\r\nRUN export LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:$LD_LIBRARY_PATH && \\\r\n    ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 && \\\r\n    bazel build \\\r\n    -c opt \\\r\n    --config=\"opt\" \\\r\n    --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 \\\r\n    --action_env=LD_LIBRARY_PATH=${LD_LIBRARY_PATH} \\\r\n    --verbose_failures \\\r\n    --spawn_strategy=standalone \\\r\n    --genrule_strategy=standalone \\\r\n    //tensorflow:libtensorflow.so && \\\r\n    rm /usr/local/cuda/lib64/stubs/libcuda.so.1\r\n\r\nRUN cp bazel-bin/tensorflow/libtensorflow.so /usr/local/lib\r\n\r\n\r\n```"]}, {"number": 25864, "title": "TF Lite operator_test non-trivial designated initializers not support\u2026", "body": "Bazel test case run operator_test non-trivial designated initializers not supported fix", "comments": ["This is duplicate of #25836, so closing the current PR."]}, {"number": 25863, "title": "fix absl import issue #25282", "body": "fix issue when using:\r\n\r\n```\r\nfrom tensorflow.python.platform.googletest import mock\r\n```\r\nthere is code in `tensorflow/python/platform/googletest.py`\r\n\r\n```\r\nfrom absl.testing.absltest import *\r\n```\r\nbut absl.testing.absltest have `mock` object after absl>=0.4.0\r\n\r\nthis case is occur after commit 87cc788e1cc\r\n@revan \r\n87cc788e1cc (Revan Sopher           2019-01-23 13:14:20 -0800\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/25282", "comments": ["Thanks for the fix! I think we should be able to bump this all the way to 0.7.0, after this commit: https://github.com/tensorflow/tensorflow/commit/661103ed87cdf0f5a792100b543006b71d62697a\r\n\r\nCan you try 0.7.0?", "@revan OK,I have rebased this commit to `absl-py==0.7.0`", "@revan Hello,is this PR in progress", "Hi @zjjott, I've retriggered the tests and am waiting for a green signal.", "@rthadur submit failed due to unrelated linter error, can we force?", "> @rthadur submit failed due to unrelated linter error, can we force?\r\n\r\n@revan we will wait until it gets merged internally."]}, {"number": 25862, "title": "Can't detected GPU using pycharm CUDA_ERROR_NO_DEVICE", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version: 1.12.0\r\n- Python version:3.6.8\r\n- Installed using virtualenv? pip? conda?:pip\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: 9.0   7.4.2\r\n- GPU model and memory: GeForce GTX 1070 8gb\r\n\r\nWhen running on command line, it seems my graphics card can be detected, but when running in pycharm, i'm getting failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n\r\n![image](https://user-images.githubusercontent.com/35086049/52987994-242bcb80-33c3-11e9-9f5d-6c791f7525f4.png)\r\n\r\n![image](https://user-images.githubusercontent.com/35086049/52988006-3443ab00-33c3-11e9-8327-aa1db2145a85.png)\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/35086049/52988031-56d5c400-33c3-11e9-87b8-31df048c4bcd.png)\r\n\r\nI've searched the internet and can't find a solution. Thanks for helping!", "comments": ["Can you try setting cuda,cupti, cudnn paths to your environment variables in PyCharm?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I have the same problem too...\r\n## My system information:\r\n* OS Platform and Distribution: Windows 10\r\n* Pytorch version: 1.0.1\r\n* Python version:3.6.5\r\n* Installed using virtualenv? pip? conda?:pip\r\n* CUDA/cuDNN version: 9.0 7.4.2\r\n* GPU model and memory: GeForce RTX 2070 8gb and GeForce GTX 1050 8gb\r\nHere's the result in the terminal \r\n![](https://user-images.githubusercontent.com/13173313/56232298-03c27980-60b3-11e9-846e-b47396067e74.png)\r\nAnd the result from Pycharm with the same code:\r\n![](https://user-images.githubusercontent.com/13173313/56232603-c14d6c80-60b3-11e9-8d50-7df50a6a87c3.png)\r\nIn Pycharm, it can only detect 1 GPU(the GeForce GTX 1050)"]}, {"number": 25861, "title": "fix absl import issue #25282", "body": "fix issue when using:\r\n```\r\nfrom tensorflow.python.platform.googletest import mock\r\n```\r\n\r\nthere is code in `tensorflow/python/platform/googletest.py`\r\n```\r\nfrom absl.testing.absltest import *\r\n```\r\n\r\nbut absl.testing.absltest have `mock` object after absl>=0.4.0", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\nGooglers can find more info about SignCLA and this PR by [following this link](go/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25861).\n\n<!-- need_author_cla -->"]}, {"number": 25860, "title": "Fix endpoint of string_split", "body": "It looks like at the moment, `string_split` is expose as `@tf_export(\"string_split\")` which supports both v1 and v2.\r\n\r\nAnd `string_split_v2` is exposed as `@tf_export(\"strings.split\")` which support both v1 and v2 as well.\r\n\r\nI think it makes sense to expose `@tf_export(\"strings.split\")` in v1 and v2 for\r\n`string_split_v2`.\r\n\r\nBut `@tf_export(\"string_split\")` should really be v1 only.\r\n\r\nThis fix changes `@tf_export(\"string_split\")` to `@tf_export(v1=[\"string_split\"])`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Also /cc @martinwicke ", "The PR has been updated to fix the api compatibility test. The tf_upgrade_v2.py has also been updated so that v1's `tf.string_split` points to v2's `tf.compat.v1.string_split` (the v2's `tf.strings.split` behaves differently) so rename is needed.", "Also ping @rthadur, any update on this PR?", "Ping @rthadur, just rebased to fix the merge conflict. Would hope this PR could land in 2.0 as it addresses the incorrect api endpoint issue. Otherwise it will be really hard to make a change after 2.0 is out."]}, {"number": 25859, "title": "Add StringLower/StringUpper op to support lower/upper() for string operations", "body": "This fix tries to address the issue raised in #25857 where there is no lower() or upper() for string operations yet.\r\n\r\nThis fix adds StringLower/StringUpper to the kernel to support this op.\r\n\r\nThe endpoints are exposed as `strings.lower` and `strings.upper`.\r\n\r\nThis fix fixes #25857.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @alextp. I take a look at TF 2.0 API: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/strings\r\n\r\nIt looks like unicode operations are separated with `unicode_` prefixes. Wondering if it make sense to create similar `unicode_upper` and `unicode_lower`? Or we could also consolidate unicode operations into part of the existing strings operation?", "I'd prefer to only have unicode versions of these operations since the\nunicode version can have a fast path for ascii strings.\n\nOn Wed, Feb 27, 2019 at 11:27 AM Yong Tang <notifications@github.com> wrote:\n\n> Thanks @alextp <https://github.com/alextp>. I take a look at TF 2.0 API:\n> https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/strings\n>\n> It looks like unicode operations are separated with unicode_ prefixes.\n> Wondering if it make sense to create similar unicode_upper and\n> unicode_lower? Or we could also consolidate unicode operations into part\n> of the existing strings operation?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25859#issuecomment-467995636>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXhiy0zOTZgvhryYcutoGz6f3tUUks5vRtwsgaJpZM4bCAT4>\n> .\n>\n\n\n-- \n - Alex\n", "Thanks @alextp. I will update the PR to add the unicode option on top of `lower/upper`.", "@alextp Thanks for the review and sorry for the delay. The PR has been updated to add an additional arg of `encoding=''` to allow unicode support. At the moment utf-8 is supported but additional encoding is possible if needed. Please take a look.", "Sweet", "@alextp Thank you for reviewing! Sorry that I tagged `API review` early. I wasn't sure if it could be tagged at any time or just after the PR is approved so I kind of tagged it early. Will only tag after PR is approved next time.", "Oh I remove the API review tag because the API owners discussed and are\nfine with it.\n\nOn Thu, Apr 25, 2019 at 3:10 PM Penporn Koanantakool <\nnotifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> Thank you for reviewing! Sorry that I\n> tagged API review early. I wasn't sure if it could be tagged at any time\n> or just after the PR is approved so I kind of tagged it early. Will only\n> tag after PR is approved next time.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25859#issuecomment-486854212>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHROC3MEWJ4WKZX4EBFLPSIT6LANCNFSM4GYIAT4A>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp Oh, I meant you ended up having to review the whole PR instead because I tagged `API review` early. Sorry about that. :)", "@yongtang Could you please take a look at test failures? \r\nYou might want to start with [Ubuntu Python3](https://source.cloud.google.com/results/invocations/7293872e-0dd7-4cf6-84e0-10db72130f96/targets)'s `//tensorflow/python/kernel_tests:string_lower_op_test` and `//tensorflow/python/kernel_tests:string_upper_op_test` ", "@penpornk Thanks for the review. The python 3 string error has been fixed and the PR has been updated. There are other test failures but it looks like they are unrelated. Please take a look and let me know if there are any issues.", "@rthadur @penpornk Looks like copybara is failing:\r\n```\r\nfeedback/copybara \u2014 Google internal checks FAILED for runs with \\\r\n     create time 2019-04-26T23:57:23.564146795Z,2019-04-26T23:55:48.976711363Z.\r\n```\r\n\r\nWondering if additional update is needed or anything else I need to do?"]}, {"number": 25858, "title": "Tensorflow Parameter Server Hangs when doing distributed training with Estimator", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:1.5, 1.8, 1.12 all the same results\r\n- Doc Link:  https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig , https://www.tensorflow.org/guide/distribute_strategy#example_with_estimator_api\r\n\r\n\r\n**Describe the documentation issue**\r\nIs parameter server expected to be killed after the training is done when using tf.estimator.Estimator for distributed training ? or it is expected behavior that `ps` hangs forever ?\r\n\r\nI am trying simple mnist example on localhost to try to get distributed training with estimator work but not able to do so.\r\n\r\nHere is the complete code (code I downloaded and modified from https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_custom_estimator.py). \r\n```python\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\n\r\nimport simplejson \r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ntf.app.flags.DEFINE_integer('steps', 100, 'The number of steps to train a model')\r\ntf.app.flags.DEFINE_string('job_name', 'master', 'job_name')\r\ntf.app.flags.DEFINE_string('task_index', '0', 'task_index')\r\ntf.app.flags.DEFINE_string('model_dir', './models/ckpt/', 'Dir to save a model and checkpoints')\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\nINPUT_FEATURE = 'image'\r\nNUM_CLASSES = 10\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n    # Input Layer\r\n    input_layer = features[INPUT_FEATURE]\r\n    # Logits layer\r\n    logits = tf.layers.dense(inputs=input_layer, units=NUM_CLASSES)\r\n\r\n    predictions = {\r\n        # Generate predictions (for PREDICT and EVAL mode)\r\n        \"classes\": tf.argmax(input=logits, axis=1),\r\n        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\r\n        # `logging_hook`.\r\n        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\r\n    }\r\n\r\n    # PREDICT mode\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            predictions=predictions,\r\n            export_outputs={\r\n                'predict': tf.estimator.export.PredictOutput(predictions)\r\n            })\r\n\r\n    # Calculate Loss (for both TRAIN and EVAL modes)\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n\r\n    # Configure the Training Op (for TRAIN mode)\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.AdamOptimizer()\r\n        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\n    # Add evaluation metrics (for EVAL mode)\r\n    eval_metric_ops = {\r\n        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\r\n    }\r\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n\r\n\r\ndef main(_):\r\n    # Load training and eval data\r\n    mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\r\n    train_data = mnist.train.images  # Returns np.array\r\n    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\r\n    eval_data = mnist.test.images  # Returns np.array\r\n    eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\r\n\r\n    # reshape images\r\n    # To have input as an image, we reshape images beforehand.\r\n    train_data = train_data.reshape(train_data.shape[0], 28 * 28)\r\n    eval_data = eval_data.reshape(eval_data.shape[0], 28 * 28)\r\n\r\n    # Create the Estimator\r\n    training_config = tf.estimator.RunConfig(\r\n        model_dir=FLAGS.model_dir,\r\n        save_summary_steps=20,\r\n        save_checkpoints_steps=20)\r\n    classifier = tf.estimator.Estimator(\r\n        model_fn=model_fn,\r\n        model_dir=FLAGS.model_dir,\r\n        config=training_config)\r\n\r\n    # Set up logging for predictions\r\n    # Log the values in the \"Softmax\" tensor with label \"probabilities\"\r\n    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\r\n    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\r\n\r\n    # Train the model\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={INPUT_FEATURE: train_data},\r\n        y=train_labels,\r\n        batch_size=FLAGS.steps,\r\n        num_epochs=1,\r\n        shuffle=True)\r\n\r\n    # Evaluate the model and print results\r\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={INPUT_FEATURE: eval_data},\r\n        y=eval_labels,\r\n        num_epochs=1,\r\n        shuffle=False)\r\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\r\n    # setup eval spec evaluating ever n seconds\r\n    eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn)\r\n    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\r\ndef make_tf_training_config(args):\r\n    \"\"\"\r\n    Returns TF_CONFIG that can be used to set the environment variable necessary for distributed training\r\n    See https://github.com/clusterone/clusterone-tutorials/blob/master/tf-estimator/mnist.py\r\n    \"\"\"\r\n    worker_hosts = ['localhost:2222']\r\n    ps_hosts = ['localhost:2224']\r\n    tf_config = {\r\n        'task': {\r\n            'type': FLAGS.job_name,\r\n            'index': FLAGS.task_index\r\n        },\r\n        'cluster': {\r\n            'master': [worker_hosts[0]],\r\n            'ps': ps_hosts\r\n        },\r\n        'environment': 'cloud'\r\n    }\r\n\r\n    return tf_config\r\n\r\nimport os\r\nif __name__ == \"__main__\":\r\n    print(\"@@@ Version: {}\".format(tf.__version__))\r\n    tf_config = make_tf_training_config(None)\r\n    os.environ['TF_CONFIG'] = simplejson.dumps(tf_config)\r\n    tf.app.run()\r\n```\r\n\r\nHere is how I launch one master job and one ps job:\r\n\r\n```bash\r\n# launch master worker job\r\npython test.py \r\n```\r\n\r\n```bash\r\n# launch ps job\r\npython test.py --job_name ps\r\n```\r\n\r\nHere is the job log for master worker job (only last few lines as this job succeeded and exit):\r\n```bash\r\nINFO:tensorflow:Finished evaluation at 2019-02-19-01:40:18\r\nINFO:tensorflow:Saving dict for global step 3301: accuracy = 0.9918, global_step = 3301, loss = 0.025573652\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 3301: ./models/ckpt/model.ckpt-3301\r\nINFO:tensorflow:Loss for final step: 0.02294134.\r\n```\r\nHere is the complete job log for ps job:\r\n\r\n```bash\r\n@@@ Version: 1.12.0\r\nWARNING:tensorflow:From test.py:118: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data.\r\nWARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST-data/train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST-data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST-data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST-data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nINFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'localhost:2224'], u'master': [u'localhost:2222']}, u'task': {u'index': u'0', u'type': u'ps'}}\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': device_filters: \"/job:ps\"\r\ndevice_filters: \"/job:worker\"\r\ndevice_filters: \"/job:master\"\r\nallow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_task_type': u'ps', '_train_distribute': None, '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x127a47550>, '_model_dir': './models/ckpt/', '_protocol': None, '_save_checkpoints_steps': 20, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_save_summary_steps': 20, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 1, '_master': u'grpc://localhost:2224'}\r\nINFO:tensorflow:Not using Distribute Coordinator.\r\nINFO:tensorflow:Start Tensorflow server.\r\n2019-02-18 17:38:15.844761: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-02-18 17:38:15.846148: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job master -> {0 -> localhost:2222}\r\n2019-02-18 17:38:15.846171: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2224}\r\n2019-02-18 17:38:15.846704: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:2224\r\n\r\n```\r\nAnd the ps hangs forever. \r\n\r\nI would expect estimator, being such a high-level API should handle closing the parameter server when training is done. If that is not the case, it should be clear in the documentation. \r\n\r\nThanks !\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["For 1.x tensorflow, can you follow this instruction to set up parameter server and worker?\r\nhttps://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md\r\nI think the server.join() should get terminated once job is done.", "@tanzhenyu server.join() blocks forever based on `tensorflow/python/training/server_lib.py`:\r\n\r\n```\r\n   def join(self):\r\n    Blocks until the server has shut down.\r\n\r\n    This method currently blocks forever.\r\n\r\n    Raises:\r\n      tf.errors.OpError: Or one of its subclasses if an error occurs while\r\n        joining the TensorFlow server.\r\n    \r\n    with errors.raise_exception_on_not_ok_status() as status:\r\n      pywrap_tensorflow.PyServer_Join(self._server, status)\r\n```", "Oh then you probably have to ps aux and then kill it manually. Seems this has been an issue for quite some time. (Contributions welcome)", "The cluster orchestrator (Yarn, K8s, or the poor programmer himself) is expected to terminate PS processes when the job is done, along with handling failing workers by respawning processes. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25858\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25858\">No</a>\n"]}, {"number": 25857, "title": "Add lower() to string operations", "body": "**System information**\r\n- TensorFlow version (you are using):  1.10 and 1.12\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAdd the string `lower()` function to the tf.string class.\r\nCurrent state has no way to lower case input strings. This requires it to be done at dataset build time which reduces dataset flexibility\r\n\r\n**Will this change the current api? How?**\r\nWould add extra functionality\r\n\r\n**Who will benefit with this feature?**\r\nAll users doing natural language processing\r\n\r\n**Any Other info.**\r\n", "comments": ["Added a PR #25859 with `strings.lower()` and `strings.upper()`."]}]