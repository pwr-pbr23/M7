[{"number": 40246, "title": "(Embedding python into c) ImportError: numpy.core.multiarray failed to import", "body": "I am working on a project which is related to object detection and ocr.  I am using Ubuntu 18.04 and python 2.7, yolov3 for object detection and tesseract 5.0.0-alpha-692-g62ea for ocr. I have to embed python code that is for ocr into the c code which is for object detection. I wrote this C code and compile at the terminal by \"  sudo gcc code.c -lpython2.7  \"\r\n\r\n#include </usr/include/python2.7/Python.h>\r\n\r\nint main() {\r\nPy_Initialize();\r\nPyRun_SimpleString(\"import sys; sys.path.append('.')\");\r\nPyRun_SimpleString(\"import ocr3;\");\r\nPyRun_SimpleString(\"print ocr3.myabs(2.0)\");\r\nPy_Finalize();\r\n\r\nreturn 0;\r\n}\r\n\r\nbut it gives an error while running a.out:\r\n\r\nImportError: numpy.core.multiarray failed to import\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"./ocr3.py\", line 1, in <module>\r\n    import cv2\r\n  File \"/home/asus/.local/lib/python2.7/site-packages/cv2/__init__.py\", line 3, in <module>\r\n    from .cv2 import *\r\nImportError: numpy.core.multiarray failed to import\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nNameError: name 'ocr3' is not defined\r\n\r\n\r\nso, I tried another C code:\r\n\r\n#include <stdio.h>\r\n#include <ncurses.h>\r\n#include </usr/local/include/python2.7/Python.h>\r\n\r\nint main()\r\n{\r\n\tchar filename[] = \"ocr3.py\";\r\n\tFILE* fp;\r\n\r\n\tPy_Initialize();\r\n\r\n\tfp = _Py_fopen(filename, \"r\");\r\n\tPyRun_SimpleFile(fp, filename);\r\n\r\n\tPy_Finalize();\r\n\treturn 0;\r\n}\r\nbut again I have an issue while running a.out: \" code.c:(.text+0x3e): undefined reference to `_Py_fopen'  \"\r\n\r\nUnfortunately, I could not find any solution. Can anyone help about the fix one of the these problems?", "comments": ["@bsrc,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/ask) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!", "@amahendrakar you are right actually, but I have asked on StackOverflow, too. Thank you for your recommendation. \r\n\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 40245, "title": "Tensorflow v2.2 build fails with cuda 10.2 TensorRT 7.0.0.11-1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source build\r\n- TensorFlow version: master branch\r\n- Python version: 3.7.7\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 3.0.0\r\n- GCC/Compiler version (if compiling from source): 8.4.0\r\n- CUDA/cuDNN version: CUDA 10.2 / cuDNN: 7.6.5\r\n- GPU model and memory: Nvidia TITAN Xp\r\n\r\n**Describe the problem**\r\nBuilding TensorFlow master branch with option ``` bazel build --verbose_failures --config=opt --config=nonccl //tensorflow/tools/pip_package:build_pip_package ``` fails with the message shown below. I would very much appreciate any help in building this.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nwithin the tensorflow master branch source directory, after standard ./configure (CUDA support enabled), below is the configuration.\r\n\r\n```\r\nYou have bazel 3.0.0 installed.\r\nPlease specify the location of python. [Default is /import/home/xxxxx/Desktop/tf/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /import/home/xxxxxx/Desktop/tf/lib/python3.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/import/home/xxxxx/Desktop/tf/lib/python3.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.2 in:\r\n    /usr/local/cuda-10.2/targets/x86_64-linux/lib\r\n    /usr/local/cuda-10.2/targets/x86_64-linux/include\r\nFound cuDNN 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\nFound TensorRT 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include/x86_64-linux-gnu\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1,6.1]: 6.1\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-8\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nINFO: From ProtoCompile tensorflow/core/framework/tensor.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/types.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/example/example_parser_configuration.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/reader_base.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/profiler/profiler_options.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/named_tensor.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/summary.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/util/event.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/node_def.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/tensor_slice.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/util/saved_tensor_slice.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/versions.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/util/memmapped_file_system.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/saver.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/transport_options.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/trackable_object_graph.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/data/experimental/snapshot.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/api_def.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/control_flow.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/debug.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/cluster.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/meta_graph.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/tensor_bundle.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/tensorflow_server.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/struct.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/saved_object_graph.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/debug_event.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/rewriter_config.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/verifier_config.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/remote_tensor_handle.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/config.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/queue_runner.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/device_properties.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/graph_debug_info.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/device_filters.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/saved_model.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/bfc_memory_map.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/allocation_description.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/lib/core/error_codes.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/device_attributes.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/attr_value.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/function.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/cost_graph.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/graph_transfer_info.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/log_memory.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/variable.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/op_def.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/step_stats.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/kernel_def.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/graph.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/remote_fused_graph_execute_info.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/tensor_description.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/util/test_log.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/profiler/protobuf/xplane.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/stream_executor/dnn.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data.pb.h [for host]:\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nERROR: /import/home/xxxxx/Desktop/tensorflow/tensorflow/core/util/proto/BUILD:64:1: C++ compilation of rule '//tensorflow/core/util/proto:proto_utils' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /import/home/xxxxx/.cache/bazel/_bazel_xxxxx/b8f3678e231ee59114f75ff5566fbc57/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/lib64 \\\r\n    PATH=/home/xxxxx/bin:/home/xxxxx/.local/bin:/import/home/xxxxx/Desktop/tf/bin:/opt/CD-adapco/13.04.011-R8/STAR-View+13.04.011/bin:/opt/CD-adapco/13.04.011-R8/STAR-CCM+13.04.011-R8/star/bin:/opt/CD-adapco/13.04.011-R8/STAR-View+13.04.011/bin:/opt/CD-adapco/13.04.011-R8/STAR-CCM+13.04.011-R8/star/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda/bin:/work/star/13.04.011-R8/STAR-CCM+13.04.011-R8/star/bin:/snap/bin:/home/xxxxx/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/util/proto/_objs/proto_utils/proto_utils.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/util/proto/_objs/proto_utils/proto_utils.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 '-std=c++14' -c tensorflow/core/util/proto/proto_utils.cc -o bazel-out/host/bin/tensorflow/core/util/proto/_objs/proto_utils/proto_utils.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from /usr/include/c++/8/cmath:45,\r\n                 from external/com_google_absl/absl/time/time.h:78,\r\n                 from ./tensorflow/core/util/proto/proto_utils.h:21,\r\n                 from tensorflow/core/util/proto/proto_utils.cc:16:\r\n/usr/include/x86_64-linux-gnu/bits/mathcalls.h:289:1: internal compiler error: Segmentation fault\r\n __MATHCALL (rint,, (_Mdouble_ __x));\r\n ^~~~~~~~~~\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-8/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /import/home/xxxxx/Desktop/tensorflow/tensorflow/python/tools/BUILD:226:1 C++ compilation of rule '//tensorflow/core/util/proto:proto_utils' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /import/home/xxxxx/.cache/bazel/_bazel_xxxxx/b8f3678e231ee59114f75ff5566fbc57/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/lib64 \\\r\n    PATH=/home/xxxxx/bin:/home/xxxxx/.local/bin:/import/home/xxxxx/Desktop/tf/bin:/opt/CD-adapco/13.04.011-R8/STAR-View+13.04.011/bin:/opt/CD-adapco/13.04.011-R8/STAR-CCM+13.04.011-R8/star/bin:/opt/CD-adapco/13.04.011-R8/STAR-View+13.04.011/bin:/opt/CD-adapco/13.04.011-R8/STAR-CCM+13.04.011-R8/star/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda/bin:/work/star/13.04.011-R8/STAR-CCM+13.04.011-R8/star/bin:/snap/bin:/home/xxxxx/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/util/proto/_objs/proto_utils/proto_utils.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/util/proto/_objs/proto_utils/proto_utils.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 '-std=c++14' -c tensorflow/core/util/proto/proto_utils.cc -o bazel-out/host/bin/tensorflow/core/util/proto/_objs/proto_utils/proto_utils.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nINFO: Elapsed time: 727.625s, Critical Path: 23.83s\r\nINFO: 641 processes: 641 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["@khatrishubham88 \r\nCan you refer tot here links and let us know if it helps:\r\n\r\n#35917 #38194 #34759 ", "@Saduf2019 \r\n#38194 is closest to my problem but as you can see in my case the problem isn't that the configure cannot find the CUDA 10.2 but it is with the compilation of couple of kernels in the tf. Unfortunately, I am building this on remote machine which does not have CUDA 10.1 which is why I have to build it using 10.2 only. \r\nSince I am new to CUDA, I am quite unsure about the cause of this.\r\nI have tried the steps mentioned in issue #35917 but that doesn't solve the problem.", "any updates?", "@khatrishubham88,\r\nIs this still an issue?\r\n\r\nCould you please try building TensorFlow v2.4.1 as per [this guide](https://www.tensorflow.org/install/source) and let us know if you are facing the same error. Thanks!", "@amahendrakar Thanks but my issue was concerning TF2.2. The issue does not exist in later versions.", "@khatrishubham88,\r\nThank you for the update.\r\n\r\nIn this case, can we close this issue as the error is fixed in the latest version of TensorFlow?", "@amahendrakar yes, most certainly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40245\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40245\">No</a>\n"]}, {"number": 40244, "title": "TF2.2: MultiWorkerMirroredStrategy doesn't assign workers correctly and training doesn't start", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary via PIP\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: 960M/4GB\r\n\r\n**Describe the current behavior**\r\nHave custom training script set up with strategy MultiWorkerMirroredStrategy, in this case trying on 2 separate workers using a docker image.\r\nThe script crashes on both workers:\r\nworker 0:\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation Identity: {{node Identity}} was explicitly assigned to /job:localhost/replica:0/task:0/device:CPU:0 but available devices are [ /job:worker/replica:0/task:0/device:CPU:0, /job:worker/replica:0/task:0/device:GPU:0, /job:worker/replica:0/task:0/device:XLA_CPU:0, /job:worker/replica:0/task:0/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[Identity]] [Op:__inference_distributed_train_step_14920]\r\n2020-06-07 16:08:53.189763: W tensorflow/core/common_runtime/eager/context.cc:447] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n```\r\n\r\nworker 1:\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation Identity: {{node Identity}} was explicitly assigned to /job:localhost/replica:0/task:0/device:CPU:0 but available devices are [ /job:worker/replica:0/task:1/device:CPU:0, /job:worker/replica:0/task:1/device:GPU:0, /job:worker/replica:0/task:1/device:XLA_CPU:0, /job:worker/replica:0/task:1/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[Identity]] [Op:__inference_distributed_train_step_14761]\r\n2020-06-07 16:08:53.186955: W tensorflow/core/common_runtime/eager/context.cc:447] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe script starts training on all workers.\r\n\r\n**Standalone code to reproduce the issue**\r\nI don't have stand-alone code, but I've attached my training script. It was built following documentation and start command includes the TF_CONFIG environment set, for example on worker 0:\r\n```\r\nTF_CONFIG='{\"cluster\": {\"worker\": [\"192.168.1.130:12345\", \"192.168.1.131:12345\"]}, \"task\":{\"index\": 0, \"type\": \"worker\"}}' python3 multi_train.py --data_root /datasets --config configs/v53.cfg\r\n```\r\n\r\n**Other info / logs**\r\ntraining script - [multi_train_py.txt](https://github.com/tensorflow/tensorflow/files/4742432/multi_train_py.txt)\r\ncomplete log of worker 0 (similar to worker 1) - [complete_log.txt](https://github.com/tensorflow/tensorflow/files/4742437/complete_log.txt)\r\n\r\n", "comments": ["Hi @alexrogozea can you confirm that your code runs when training on a single machine?", "Hi Nikita, it doesn't. Running only with worker 0 on the machine I get the\nsame error.\n\nOn Tue, 9 Jun 2020 at 00:08, Nikita Namjoshi <notifications@github.com>\nwrote:\n\n> Hi @alexrogozea <https://github.com/alexrogozea> can you confirm that\n> your code runs on a single machine?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40244#issuecomment-640933433>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AF2CGXE4IGOGAXTKLDS5WRTRVVVPNANCNFSM4NXMOLJQ>\n> .\n>\n\n\n-- \nAlex Rogozea\n\n+44 7783 509 193\nLondon, United Kingdom\n", "I've spent some time on Horovod following the lack of success with TF's solution, and I'd like to know how similar the pipeline requirements are. I did make Horovod train the model on 2 workers.\r\n\r\nIt's worth noting that my script doesn't run successfully on a normal, single host, getting the same error as above.", "@alexrogozea We've recently fixed this issue. can you try using tf-nightly? ", "@anj-s Using tf-nightly-gpu and with tf-nightly builds I get the following exception:\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\r\n2020-06-09 20:41:36.344375: W tensorflow/core/common_runtime/eager/context.cc:553] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n```\r\nI'm not running other python/TF processes in the background, and the GPU is not being used by anything other than Xorg and 'compiz'.\r\nMore relevant, this steps stops the process before anything so I can't tell if it would work with the initial problem.", "@alexrogozea Looks like this might be a separate issue. can you share the complete stack trace? Also is this error something you see on a single host?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40244\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40244\">No</a>\n", "@anj-s for reference: Do you happen to know which commit fixed this issue? We have users wanting to still use TF 2.2 and are looking into backporting this"]}, {"number": 40243, "title": "Update steps numbers from 13. 12 was duplicate", "body": "", "comments": []}, {"number": 40242, "title": "Tensorflow 2.x and tf.compat issues", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary):Binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory: gtx 1060\r\n\r\n\r\nThe Tensorflow's latest version is a complete tragedy. It just breaks each and every functions that has been written with the older version of tensorflow. The eager execution is the pain in the neck. It just throws the countless meaningless error. I got `tf.function-decorated Value error`. I fixed it and now I'm getting the following error,\r\n\r\n```\r\ntensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'Conv2DBackpropInput_1:0' shape=(1, 13, 13, 1280) dtype=float32>]\r\n2020-06-07 16:49:58.682705: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n\t [[{{node PyFunc}}]]\r\n\r\n```\r\n\r\n```\r\nDisabling eager execution throws the following error,\r\n\r\nValueError: Operation name: \"AssignAddVariableOp\"\r\nop: \"AssignAddVariableOp\"\r\ninput: \"AssignAddVariableOp/Variable\"\r\ninput: \"Const\"\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\n is not an element of this graph.\r\n```\r\n\r\n**Please regularize the code base before even trying to issue a vendor certificate aka TensorFlow certified developer certificate**\r\n", "comments": ["@Zumbalamambo,\r\nCould you please share the code you're trying to run, so that we can look into it and fix the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "+1", "> It just breaks each and every functions that has been written with the older version of tensorflow. The eager execution is the pain in the neck. It just throws the countless meaningless error.\r\n\r\n@Zumbalamambo,\r\nCould you please share the code you are running to that we can help you with the errors. Thanks!", "@Zumbalamambo,\r\nThe issue tracker is for Tensorflow feature requests or bug reports. Please refrain from posting inappropriate comments. If any functionality or feature does not work to your satisfaction, please let us know, as each feedback is valuable to us. Tensorflow team will be happy to address them. Kindly share the information related to code as requested by @amahendrakar. Let us know how we can help."]}, {"number": 40241, "title": "xrange() was removed from Python on 1/1/2020", "body": "", "comments": []}, {"number": 40240, "title": "Fix SyntaxWarnings on Python >= 3.8", "body": "Avoid [SyntaxWarnings on Python >= 3.8](https://docs.python.org/3/whatsnew/3.8.html#porting-to-python-3-8).\r\n\r\n% `python3.8`\r\n```\r\n>>> 0 is 0\r\n<stdin>:1: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n>>> () is ()\r\n<stdin>:1: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n```", "comments": ["Thanks, I'll try to get this merged.", "@cclauss  Can you please address Ubuntu Sanity errors? Thanks!\r\n", "I'll fix it in the merge.", "Thanks. Sorry about the delay here. This is hitting some issue on some internal tests.\r\n\r\n```\r\nusing a `tf.Tensor` as a Python `bool` is not allowed\r\n```\r\n\r\nI may have to revert `random_ops.py` for this to get merged without a deep dive.", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 40239, "title": "Tensorflow Lite Converter Raise Issue", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.2.0\r\n\r\n**Failure details**\r\nI try to convert TF model to TFlite but converter raise ERROR\r\n Here is my code :\r\n\r\nmodel = tf.keras.models.load_model(load_path, custom_objects={'loss_dice_coefficient_error': loss_dice_coefficient_error,\r\n                                             'dice_coefficient': dice_coefficient,\r\n                                             'jaccard_coef': jaccard_coef})\r\n    \r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.experimental_new_converter = True\r\n    converter.allow_custom_ops = True\r\n    \r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\n        \r\n    tflite_model = converter.convert()\r\n\r\n\r\n\r\n**Any other info / logs**\r\nTraceback (most recent call last):\r\n  File \"G:\\AI_library_3\\Create_Lite_Model.py\", line 89, in <module>\r\n    load_all_models()\r\n  File \"G:\\AI_library_3\\Create_Lite_Model.py\", line 81, in load_all_models\r\n    load_model_from_path_convert_to_lite(models_root, \"LV_ED\", lite_models_root)\r\n  File \"G:\\AI_library_3\\Create_Lite_Model.py\", line 73, in load_model_from_path_convert_to_lite\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 518, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 496, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 227, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-06-07 17:30:36.154029: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-06-07 17:30:36.154352: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-06-07 17:30:40.516629: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.\r\n2020-06-07 17:30:40.516872: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.\r\n2020-06-07 17:30:40.566746: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-06-07 17:30:40.628297: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b479ed4240 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-07 17:30:40.628652: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-07 17:30:40.642470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-06-07 17:30:40.691199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GT 610 computeCapability: 2.1\r\ncoreClock: 1.62GHz coreCount: 1 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 9.94GiB/s\r\n2020-06-07 17:30:40.692419: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-06-07 17:30:40.693284: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\r\n2020-06-07 17:30:40.694129: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n2020-06-07 17:30:40.694962: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n2020-06-07 17:30:40.695842: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-06-07 17:30:40.696682: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\r\n2020-06-07 17:30:40.697563: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found\r\n2020-06-07 17:30:40.697849: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-06-07 17:30:40.744757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-07 17:30:40.745082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-06-07 17:30:40.745282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-06-07 17:30:40.756749: I tensorflow/compiler/xla/service/platform_util.cc:139] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 2.1\r\n2020-06-07 17:30:40.757309: I tensorflow/compiler/jit/xla_gpu_device.cc:161] Ignoring visible XLA_GPU_JIT device. Device number is 0, reason: Internal: no supported devices found for platform CUDA\r\nloc(callsite(\"model_1/conv2d_transpose_1/atrous_conv2d_transpose/Conv2DBackpropInput\"(\"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\":865:0) at callsite(\"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\":959:0 at callsite(\"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\":435:0 at callsite(\"G:\\AI_library_3\\Create_Lite_Model.py\":66:0 at callsite(\"G:\\AI_library_3\\Create_Lite_Model.py\":81:0 at callsite(\"G:\\AI_library_3\\Create_Lite_Model.py\":89:0 at callsite(\"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\idlelib\\run.py\":474:0 at callsite(\"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\idlelib\\run.py\":144:0 at \"<string>\":1:0))))))))): **error: 'tfl.transpose_conv' op expect output type tensor<9x29x38x64xf32>, got tensor<?x?x?x?xf32>**\r\nTraceback (most recent call last):\r\n\r\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n\r\n    \"__main__\", mod_spec)\r\n\r\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\r\n\r\n    exec(code, run_globals)\r\n\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\toco_from_protos.exe\\__main__.py\", line 9, in <module>\r\n\r\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n\r\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n\r\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n\r\n    _run_main(main, args)\r\n\r\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n\r\n    sys.exit(main(argv))\r\n\r\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n\r\n    enable_mlir_converter)\r\n\r\nException: C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:865:9: error: 'tfl.transpose_conv' op expect output type tensor<9x29x38x64xf32>, got tensor<?x?x?x?xf32>\r\n\r\n        self._initialize(args, kwargs, add_initializers_to=initializers)\r\n\r\n        ^\r\n\r\nC:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:959:5: note: called from\r\n\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n\r\n    ^\r\n\r\nC:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:435:5: note: called from\r\n\r\n    concrete_func = func.get_concrete_function()\r\n\r\n    ^\r\n\r\nG:\\AI_library_3\\Create_Lite_Model.py:66:5: note: called from\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n    ^\r\n\r\nG:\\AI_library_3\\Create_Lite_Model.py:81:5: note: called from\r\n\r\n    load_model_from_path_convert_to_lite(models_root, \"LV_ED\", lite_models_root)\r\n\r\n    ^\r\n\r\nG:\\AI_library_3\\Create_Lite_Model.py:89:1: note: called from\r\n\r\nload_all_models()    \r\n\r\n^\r\n\r\nC:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\idlelib\\run.py:474:17: note: called from\r\n\r\n                exec(code, self.locals)\r\n\r\n                ^\r\n\r\nC:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\idlelib\\run.py:144:17: note: called from\r\n\r\n                ret = method(*args, **kwargs)\r\n\r\n                ^\r\n\r\n<string>: note: called from\r\n\r\n", "comments": ["I submitted a fix. Could you try the conversion with the next day of tf-nightly ?", "Dear @abattery \r\nI update tf-nightly to version 2.3.0.dev20200607 and it was fixed.\r\n\r\nThank you so much"]}, {"number": 40238, "title": "Unhandled Rejection (Error): No backend found in registry in a React App", "body": "Hi, \r\n\r\n System Specifications:\r\n- Fedora 32\r\n- React JS \r\n- npm\r\n- Tensorflow-mobilenet\r\n\r\nBehavior:\r\n\r\nThis basic React App using mobilenet from @tensorflow-models was working fine a few months ago. Now I am getting Unhandled Rejection error.\r\n\r\n```\r\nimport * as mobilenet from '@tensorflow-models/mobilenet';\r\nimport photo from './assets/file.jpg';\r\nimport { loadImage } from 'canvas';\r\nimport React, { useState } from 'react';\r\n\r\nconst App = () => {\r\n\tconst [res, handleRes] = useState([]);\r\n\tconst myPhoto = () => {\r\n\t\treturn <img src={photo} alt=\"foto \"></img>;\r\n\t};\r\n\tconst myPrediction = async () => {\r\n\t\tconst loadModel = await mobilenet.load();\r\n\t\tconst pic = await loadImage(photo);\r\n\t\tconst pred = await loadModel.classify(pic);\r\n\t\tconsole.log(pred);\r\n\t\thandleRes(pred);\r\n\t};\r\n\r\n\treturn (\r\n\t\t<div className=\"App\">\r\n\t\t\t{myPhoto()}\r\n\t\t\t<button onClick={(e) => myPrediction(e)}>Predict</button>\r\n\t\t\t{res.map((e, k) => (\r\n\t\t\t\t<li key={k}>\r\n\t\t\t\t\t<h1>{e.className + ': ' + Math.round(e.probability * 100) + '%'}</h1>\r\n\t\t\t\t</li>\r\n\t\t\t))}\r\n\t\t</div>\r\n\t);\r\n};\r\n\r\nexport default App;\r\n```\r\nThis basic example should make a prediction on the uploaded image.\r\n\r\nI made a post here https://programandoconro.wordpress.com/2019/12/30/react-app-para-clasificacion-de-imagenes-con-machine-learning/\r\n\r\nFull App code is here https://github.com/programandoconro/Image-Classification-ML-App (not working either).\r\n![file](https://user-images.githubusercontent.com/50117686/83968945-8f552600-a8c4-11ea-8928-131356e1637b.jpg)\r\n\r\nFortunately,  deployed App still working  https://programandoconro.github.io/Image-Classification-ML-App/\r\n\r\nSample image attached.\r\n\r\nThank you, ", "comments": ["@programandoconro Please post this issue in [tensorflow/tfjs](https://github.com/tensorflow/tfjs/issues) as this issue is apt to that repo. Thanks!", "@gowthamkpr Thank you, I will. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40238\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40238\">No</a>\n"]}, {"number": 40237, "title": "Machine translation using Seq2Seq with attention by using LSTM instead of GRU", "body": "Hello,\r\nI am using the template in tutorial 'Machine translation using Seq2Seq with attention'. However, seq2seq is built on GRU.(The link is https://www.tensorflow.org/tutorials/text/nmt_with_attention) \r\n\r\nCould you please provide a template that Machine translation using Seq2Seq with attention using LSTM instead of GRU?\r\n\r\n", "comments": ["@yanglei-github Please take a look at this [doc](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt) where LSTM has been user for Neural Machine Translation. Thanks!", "Thanks a lot!  Also, do you have some tutorials for implementing copy mechanism(pointer-generator or copynet) codes in TensorFlow version 2? Because I think by far TensorFlow doesn't have a build-in API for using copy mechanism in TensorFlow.", "And it is really a great tutorial for machine translation. If it is possible that add some evaluation metrics such as bleu or meteor to evaluate the result ", "@yanglei-github If its a document feature request can you create a new issue to track this. Thank you!", "OK! Sure!"]}, {"number": 40236, "title": "ModuleNotFoundError: No module named 'tensorflow.contrib'", "body": "I want to train from tensorflow, but I always get error on this bug.\r\nfrom tensorflow.contrib import data as tf_data\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\nI have try install lower version of tensorflow, but still didn't work.\r\nenvironment:macos python3.7 tensorflow2.2.0", "comments": ["@ryankuang0618 \r\n\r\ntensorflow.contrib does not exist in TF 2.x. You'll have to use TF 1.x to use tensorflow.contrib\r\nFor Python version compatibility. Please check the tested build configurations [here](https://www.tensorflow.org/install/source#linux).\r\n\r\nyou may also refer to similar issues:\r\n#39065 #30794  #38882 #38739 #31350 #36269\r\nThanks!", "> @ryankuang0618 \n> \n> \n> \n> tensorflow.contrib does not exist in TF 2.x. You'll have to use TF 1.x to use tensorflow.contrib\n> \n> For Python version compatibility. Please check the tested build configurations [here](https://www.tensorflow.org/install/source#linux).\n> \n> \n> \n> you may also refer to similar issues:\n> \n> #39065 #30794  #38882 #38739 #31350 #36269\n> \n> Thanks!\n\nThanks, I solved the error by install tensorflow version to 1.14.0"]}, {"number": 40235, "title": "Undefined name: import sys for line 56", "body": "`sys` is neither defined nor imported which leads to an _undefined name_ which has the potential to raise NameError at runtime.", "comments": []}, {"number": 40234, "title": "error with tf.strings.to_number()", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["when I use tf.strings.to_number(keys) it gives me error \r\nInvalidArgumentError: StringToNumberOp could not correctly convert string: 00000000000000000000000100000000000000100010000000000000010000000100000000010100100101010000000111101101011101110100011011110111110011111110101111111011111111110111110 [Op:StringToNumber]\r\n", "@Ahmed-fub \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also include your TensorFlow version.\r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Please fill in issue template and provide minimal code to reproduce.", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40233, "title": "Encountered fatal Python error with TFLite Experimental Converter", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `tensorflow/tensorflow:latest-gpu` Docker image on Ubuntu 20.04 host on GCP Compute Engine VM\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.2 (Using Docker TensorFlow GPU image)\r\n- GPU model and memory: Tesla V100-SXM2-16GB\r\n\r\n**Describe the current behavior**\r\nI'm using the new QAT API demonstrated in the docs [here](https://www.tensorflow.org/model_optimization/guide/quantization/training_example). Then I'm taking the resulting model and converting it to TFLite, again referencing the docs. Everything works as advertised until converting the model, where I encounter an error (see logs below).\r\n\r\n**Describe the expected behavior**\r\nI expected to get a converted model I could save as a `tflite` binary. I was only able to do this when explicitly disabling the experimental converter.\r\n\r\n**Standalone code to reproduce the issue**\r\nI don't have this available at the moment- I will try to provide this when able. In the meantime, this is the code snippet surrounding the problem. Let me know if this is insufficient.\r\n\r\n```\r\nmodel = tfmot.quantization.keras.quantize_model(model)\r\nmodel.compile(optimizer=optimizer, loss=loss)\r\nhistory = model.fit(train_dataset, epochs=FLAGS.epochs, callbacks=callbacks, validation_data=val_dataset)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n# converter.experimental_new_converter = False # Must uncomment this line to convert successfully\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\nwith open(\"output.tflite\", 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\nOur model is made up of the following Keras layers: Add, Concatenate, Conv2D, Input, Lambda, LeakyReLU, MaxPool2D, UpSampling2D, ZeroPadding2D, and BatchNormalization organized into several sub-models. Each submodel is quantized on its own because the `tfmot` API does not have native support for submodels. [This ](https://github.com/tensorflow/model-optimization/issues/377#issuecomment-623586866) was the recommended workaround until that is added.\r\n\r\n**Other info / logs**\r\nIt's a long one, here is what was spit out. All lines following line 59 were output together at the end. From my point of view the console didn't update from line 59 for several minutes until it all appeared together. I checked `nvidia-smi` while I waited and noticed GPU VRAM usage was at 100% (almost 16GB was being used by the Python process running this script) but GPU utilization was at 0%. Checking `htop` showed RAM and CPU usage very low at the time.\r\n\r\nhttps://pastebin.com/LMNefb3V\r\n", "comments": ["@willbattel,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code or the `model` file you are using in the code. Thanks!", "Sure, I'll try to bring that code over. I need to clean it up first.", "> Sure, I'll try to bring that code over. I need to clean it up first.\r\n\r\n@willbattel,\r\nAny updates regarding this? Thanks!", "> Any updates regarding this? Thanks!\r\n\r\nNot yet. I've been delayed by other work that is higher priority because this issue wasn't blocking for us. Because it works if you disable the experimental converter, I just wanted to submit the issue for your reference. If I find time to submit a reproducible sample, I'll add that in this thread- I can't just drop in what we ran because it is littered with clutter, proprietary code, etc.... In the mean time, if you would like to close this issue you can do so. We could re-open it later, or I could always submit a new issue if need-be.", "@willbattel,\r\nThank you for the update. Marking the issue as close, please feel free to reopen if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40233\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40233\">No</a>\n"]}, {"number": 40232, "title": "Request to include tf.depth_to_space", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary):  \r\n- TensorFlow version (or github SHA if from source): 1.14\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nValueError: Didn't find custom op for name 'DEPTH_TO_SPACE' with version 1\r\nRegistration failed.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nimport tensorflow as tf\r\ngraph_def_file = \"frozen_model.pb\"\r\ninput_arrays = [\"Placeholder\"]\r\noutput_arrays = [\"DepthToSpace\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n  graph_def_file, \r\n  input_arrays, \r\n  output_arrays, \r\n  input_shapes={'Placeholder':[200,200,200,4]}\r\n  )\r\nconverter.allow_custom_ops = True\r\ntflite_model = converter.convert()\r\nopen(\"fix_shape.tflite\", \"wb\").write(tflite_model)\r\n\r\nLink to the model : https://drive.google.com/file/d/1i9q1XtMVMe31KRGRhy1vbtiEL1hy0x_Z/view?usp=sharing\r\n\r\n**Any other info / logs**\r\nI tried testing with tensorflow versions >=2.0 and also tf.nightly \r\nIt did not work out", "comments": ["@purva98 \r\nIs there any particular reason to be using an older version of tensorflow ,can you please try on later versions, also we are unable to preview the drive shared. if possible please share a colab gist for us to analyse the error.\r\nPlease refer to these links and let us know if it helps:\r\n[link](https://github.com/google-coral/tflite/issues/2) [link1](https://github.com/nnstreamer/nnstreamer/issues/916) [link2](https://medium.com/@bsramasubramanian/running-a-tensorflow-lite-model-in-python-with-custom-ops-9b2b46efd355) ", "I used tf 1.14 because , when I try with tf 2.2.0\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('saved_model')\r\ntflite_model = converter.convert()\r\n\r\nI get this error : \r\n\"This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\"\r\n\r\nThanks for the links, I will share the colab gist and update you. \r\n\r\n", "@purva98 \r\nPlease refer to these links for the mentioned error:\r\n#34350  #35031", "Yes, I ll follow these tutorials to build the custom operator,\r\nThanks"]}, {"number": 40231, "title": "tf.linalg.expm enters infinite loop when input may cause reduce_sum to have inf", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0 & v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.linalg.expm` may enter an infinite loop when certain input can cause `math_ops.reduce_sum` to return `inf`, so `l1_norm`, `squarings`, and `max_squarings` will all become `inf` and the `while_loop`'s condition will never be false.\r\n\r\nPart of this behavior comes from how `reduce_sum` deals with overflow with certain dtype, because for example, if input is `float16`, it's very easy to have input to cause `reduce_sum` to have `inf` in the result. On the other hand, if input is `float32`, `inf` may not occur easily because it's more difficult for `reduce_sum` to have overflow with `float32`\r\n**Describe the expected behavior**\r\nTensorflow should be able to detect this infinite loop and then throw an exception to warn the user and stop the execution.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# an input big enough to cause reduce_sum to return inf\r\nin_tensor = (np.random.rand(1000, 1000) * 10000).astype('float16')\r\n\r\ntf.linalg.expm(in_tensor) # will not terminate\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe problem seems to be this call: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg/linalg_impl.py#L274-L278\r\n```python\r\n    l1_norm = math_ops.reduce_max(\r\n        math_ops.reduce_sum(\r\n            math_ops.abs(matrix),\r\n            axis=array_ops.size(array_ops.shape(matrix)) - 2),\r\n        axis=-1)[..., array_ops.newaxis, array_ops.newaxis]\r\n```\r\nwhere `math_ops.reduce_sum` would have `inf` value in result, so `l1_norm` would also contain `inf`. Then, eventually `max_squarings` would just be `inf` due to error propagation, so the while_loop condition `c = lambda i, r: math_ops.less(i, max_squarings)` would never evaluate to be false, causing the infinite loop.", "comments": ["I have tried in colab with TF version 2.1, 2.2, nightly version (`2.3.0-dev20200605`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/24db9e4fa3b191408143bea8243a0ce2/untitled961.ipynb).Thanks!", "@leeyeetonn Looks like this was resolved in recent `tf-nightly`.  Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/26cdb711b0c60ed21ac8cc4d74e45745/untitled961.ipynb). Thanks!\r\n\r\nI also checked in `!pip install tensorflow==2.4rc3` and there is no issue.  So, if you want to use stable version, then please wait for few more days for the release of `TF2.4` in near future. \r\n\r\nI am closing this issue as this was resolved in recent `tf-nightly`. Please feel free to reopen if I am mistaken. thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40231\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40231\">No</a>\n"]}, {"number": 40230, "title": "the result of each execution is very different.", "body": "https://colab.research.google.com/drive/1_YW_Afi0yZS5wJQxJbHXddezIppZFUL_\r\n\r\nAs you can see in colab, the result of each execution is very different.\r\n\r\nI understand that the results of the run change because the weights change, but the difference is too great.\r\n\r\nReduced to reduce loss. Why does this happen and how can I reduce the difference in execution results to increase the accuracy and reliability of predictions?", "comments": ["@Lay4U,\r\nI do not have permission to view the Colab gist you have linked. \r\n\r\nCould you please save the Gist using the following method. 'File' -> 'Save a copy as Github Gist', and share the link of the new window. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40229, "title": "tf.function-decorated  Value error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.3\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory: GTX 1050\r\n\r\nI'm using the following code to create a custom loss,\r\n\r\n    \r\n```\r\n@tf.function\r\ndef custom_loss(self, y_true, y_pred):\r\n        mask_shape = tf.shape(y_true)[:4]\r\n        \r\n        cell_x = tf.cast(tf.reshape(tf.tile(tf.range(self.grid_w), [self.grid_h]), (1, self.grid_h, self.grid_w, 1, 1)),dtype=tf.float32)\r\n        cell_y = tf.transpose(cell_x, (0,2,1,3,4))\r\n\r\n        cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [self.batch_size, 1, 1, self.nb_box, 1])\r\n        \r\n        coord_mask = tf.zeros(mask_shape)\r\n        conf_mask  = tf.zeros(mask_shape)\r\n        class_mask = tf.zeros(mask_shape)\r\n        \r\n        seen = tf.Variable(0.)\r\n        total_recall = tf.Variable(0.)\r\n```\r\n\r\n\r\nBut it throws the following error,\r\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\r\n\r\n", "comments": ["@Zumbalamambo \r\nI ran the code shared by you on tf_nightly and tf 2.2 but do not face any error, please have a look at the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/66417df2cb20cbc9d33fc21806d23951/untitled217.ipynb)", "with the nightly build, I'm getting the following error, :(\r\n\r\n`RuntimeError: The layer has never been called and thus has no defined output shape.\r\n`\r\n\r\nThis error happens when I try to get the output shape\r\n        \r\n```\r\nfeature_extractor = Model(input_image, x)  \r\nfeature_extractor.get_output_shape_at(-1)[1:3]\r\n```\r\n\r\n", "@Zumbalamambo please share the minimal amount of relevant code so we can reproduce the RuntimeError you are seeing. With just the function you have provided, I am unable to reproduce either the ValueError or the RuntimeError.\r\n\r\nHowever, on first glance, the issue you are facing is likely from the creation of the variables within your tf.function. I recommend checking out the Variables section of the [documentation](https://www.tensorflow.org/guide/function#variables) for tf.function, which provides some context on the ValueFunction error you are seeing. In general, avoid creating variables inside of tf.function. Happy to provide more help if you share a minimal reproducible example.", "@Zumbalamambo\r\nPlease update as per above comment.", "this is my tf function @Saduf2019 @nikitamaia \r\n\r\n```\r\n@tf.function\r\n    def custom_loss(self, y_true, y_pred):\r\n        mask_shape = tf.shape(y_true)[:4]\r\n        \r\n        cell_x = tf.cast(tf.reshape(tf.tile(tf.range(self.grid_w), [self.grid_h]), (1, self.grid_h, self.grid_w, 1, 1)),dtype=tf.float32)\r\n        cell_y = tf.transpose(cell_x, (0,2,1,3,4))\r\n\r\n        cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [self.batch_size, 1, 1, self.nb_box, 1])\r\n        \r\n        coord_mask = tf.zeros(mask_shape)\r\n        conf_mask  = tf.zeros(mask_shape)\r\n        class_mask = tf.zeros(mask_shape)\r\n\r\n        if self.seen  is None:\r\n            self.seen  = tf.Variable(0.)\r\n            self.total_recall = tf.Variable(0.)\r\n        \r\n        \"\"\"\r\n        Adjust prediction\r\n        \"\"\"\r\n        ### adjust x and y      \r\n        pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\r\n        \r\n        ### adjust w and h\r\n        pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(self.anchors, [1,1,1,self.nb_box,2])\r\n        \r\n        ### adjust confidence\r\n        pred_box_conf = tf.sigmoid(y_pred[..., 4])\r\n        \r\n        ### adjust class probabilities\r\n        pred_box_class = y_pred[..., 5:]\r\n        \r\n        \"\"\"\r\n        Adjust ground truth\r\n        \"\"\"\r\n        ### adjust x and y\r\n        true_box_xy = y_true[..., 0:2] # relative position to the containing cell\r\n        \r\n        ### adjust w and h\r\n        true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\r\n        \r\n        ### adjust confidence\r\n        true_wh_half = true_box_wh / 2.\r\n        true_mins    = true_box_xy - true_wh_half\r\n        true_maxes   = true_box_xy + true_wh_half\r\n        \r\n        pred_wh_half = pred_box_wh / 2.\r\n        pred_mins    = pred_box_xy - pred_wh_half\r\n        pred_maxes   = pred_box_xy + pred_wh_half       \r\n        \r\n        intersect_mins  = tf.maximum(pred_mins,  true_mins)\r\n        intersect_maxes = tf.minimum(pred_maxes, true_maxes)\r\n        intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\r\n        intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\r\n        \r\n        true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\r\n        pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\r\n\r\n        union_areas = pred_areas + true_areas - intersect_areas\r\n        iou_scores  = tf.truediv(intersect_areas, union_areas)\r\n        \r\n        true_box_conf = iou_scores * y_true[..., 4]\r\n        \r\n        ### adjust class probabilities\r\n        true_box_class = tf.argmax(y_true[..., 5:], -1)\r\n        \r\n        \"\"\"\r\n        Determine the masks\r\n        \"\"\"\r\n        ### coordinate mask: simply the position of the ground truth boxes (the predictors)\r\n        coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * self.coord_scale\r\n        \r\n        ### confidence mask: penelize predictors + penalize boxes with low IOU\r\n        # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\r\n        true_xy = self.true_boxes[..., 0:2]\r\n        true_wh = self.true_boxes[..., 2:4]\r\n        \r\n        true_wh_half = true_wh / 2.\r\n        true_mins    = true_xy - true_wh_half\r\n        true_maxes   = true_xy + true_wh_half\r\n        \r\n        pred_xy = tf.expand_dims(pred_box_xy, 4)\r\n        pred_wh = tf.expand_dims(pred_box_wh, 4)\r\n        \r\n        pred_wh_half = pred_wh / 2.\r\n        pred_mins    = pred_xy - pred_wh_half\r\n        pred_maxes   = pred_xy + pred_wh_half    \r\n        \r\n        intersect_mins  = tf.maximum(pred_mins,  true_mins)\r\n        intersect_maxes = tf.minimum(pred_maxes, true_maxes)\r\n        intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\r\n        intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\r\n        \r\n        true_areas = true_wh[..., 0] * true_wh[..., 1]\r\n        pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\r\n\r\n        union_areas = pred_areas + true_areas - intersect_areas\r\n        iou_scores  = tf.truediv(intersect_areas, union_areas)\r\n\r\n        best_ious = tf.reduce_max(iou_scores, axis=4)\r\n        conf_mask = conf_mask + tf.cast(best_ious < 0.6,dtype=tf.float32) * (1 - y_true[..., 4]) * self.no_object_scale\r\n        \r\n        # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\r\n        conf_mask = conf_mask + y_true[..., 4] * self.object_scale\r\n        \r\n        ### class mask: simply the position of the ground truth boxes (the predictors)\r\n        class_mask = y_true[..., 4] * tf.gather(self.class_wt, true_box_class) * self.class_scale       \r\n        \r\n        \"\"\"\r\n        Warm-up training\r\n        \"\"\"\r\n        no_boxes_mask = tf.cast(coord_mask < self.coord_scale/2.,dtype=tf.float32)\r\n        self.seen  = tf.compat.v1.assign_add(self.seen , 1.)\r\n        \r\n        true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(self.seen , self.warmup_batches+1),\r\n                              lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \r\n                                       true_box_wh + tf.ones_like(true_box_wh) * \\\r\n                                       np.reshape(self.anchors, [1,1,1,self.nb_box,2]) * \\\r\n                                       no_boxes_mask, \r\n                                       tf.ones_like(coord_mask)],\r\n                              lambda: [true_box_xy, \r\n                                       true_box_wh,\r\n                                       coord_mask])\r\n        \r\n        \"\"\"\r\n        Finalize the loss\r\n        \"\"\"\r\n        nb_coord_box = tf.reduce_sum(tf.cast(coord_mask > 0.0,dtype=tf.float32))\r\n        nb_conf_box  = tf.reduce_sum(tf.cast(conf_mask  > 0.0,dtype=tf.float32))\r\n        nb_class_box = tf.reduce_sum(tf.cast(class_mask > 0.0,dtype=tf.float32))\r\n        \r\n        loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\r\n        loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\r\n        loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\r\n        loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\r\n        loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\r\n        \r\n        loss = tf.cond(tf.less(self.seen , self.warmup_batches+1),\r\n                      lambda: loss_xy + loss_wh + loss_conf + loss_class + 10,\r\n                      lambda: loss_xy + loss_wh + loss_conf + loss_class)\r\n        \r\n\r\n        return loss\r\n\r\n```", "Thanks @Zumbalamambo. Can you share a minimal example that can be reproduced? Also please provide the error you are seeing. Is it still the ValueError mentioned in your original post, or something else?\r\n", "@Zumbalamambo I think it might be because in your if-statement for `tf.Variable`s you have `self.total_recall = tf.Variable(0.)`. Try moving it into its own if-statement. ", "Hi @Zumbalamambo, were you able to resolve this issue with the suggestion from @sumanthratna? Essentially you want to make sure your variables are only created the first time your tf.function is executed (ie do not create a new variable with each call), or you'll see the ValueError.", "@Zumbalamambo \r\nPlease update a s per above comment."]}, {"number": 40227, "title": "CUDA Toolkit 11.0 RC", "body": "NVIDIA released their `CUDA 11.0 RC Toolkit`, and was wondering if there is an existing tensorflow-nightly build that can pull it or do I have to build from source which takes around 7 hours?\r\n", "comments": ["We haven't yet switched to CUDA 11 support", "+1 for a \u201cprerelease\u201d build  with CUDA 11 + CUDNN 8 support..\r\n", "I tried to build the `master` branch but got and patch issue.\r\n\r\nSetup:\r\n* Fedora 32\r\n* Nvidia GTX 1080 Ti\r\n* Cuda 11 + Cudnn 8\r\n* Bazelisk + gcc 10.1.1\r\n\r\n```\r\n$ bazel version                                                                                                        \r\nBazelisk version: v1.5.0\r\nBuild label: 3.1.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Apr 22 10:32:27 2020 (1587551547)\r\nBuild timestamp: 1587551547\r\nBuild timestamp as int: 1587551547\r\n```\r\n\r\nMain issue for building TensorFlow manually is that I'm unable to install any of those Cuda versions 10.1/10.2. [forums.developer.nvidia.com](https://forums.developer.nvidia.com/t/cuda-10-1-2-x86-64-run-file-installation-issue/127653)\r\n\r\n```\r\nINFO: Repository com_google_protobuf instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule tf_http_archive defined at:\r\n  /\u2026/tensorflow/third_party/repo.bzl:134:19: in <toplevel>\r\nINFO: Repository 'com_google_protobuf' used the following cache hits instead of downloading the corresponding file.\r\n * Hash 'cfcba2df10feec52a84208693937c17a4b5df7775e1635c1e3baffc487b24c9b' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/v3.9.2.zip\r\nIf the definition of 'com_google_protobuf' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'com_google_protobuf':\r\n   Traceback (most recent call last):\r\n        File \"/\u2026/tensorflow/third_party/repo.bzl\", line 110\r\n                _apply_patch(ctx, <1 more arguments>)\r\n        File \"/\u2026/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, <1 more arguments>)\r\n        File \"/\u2026/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(<1 more arguments>)\r\nNon-zero return code(1) when executing 'patch -p1 -d /HOME/.cache/bazel/_bazel_fife/0e7cc962cbb913e6cc324ac59e6b080c/external/com_google_protobuf -i /\u2026/tensorflow/third_party/protobuf/protobuf.patch':\r\nStdout: \r\nStderr: src/main/tools/process-wrapper-legacy.cc:58: \"execvp(patch, ...)\": No such file or directory\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@com_google_protobuf//': Traceback (most recent call last):\r\n        File \"/\u2026/tensorflow/third_party/repo.bzl\", line 110\r\n                _apply_patch(ctx, <1 more arguments>)\r\n        File \"/\u2026/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, <1 more arguments>)\r\n        File \"/\u2026/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(<1 more arguments>)\r\nNon-zero return code(1) when executing 'patch -p1 -d /HOME/.cache/bazel/_bazel_fife/0e7cc962cbb913e6cc324ac59e6b080c/external/com_google_protobuf -i /\u2026/tensorflow/third_party/protobuf/protobuf.patch':\r\nStdout: \r\nStderr: src/main/tools/process-wrapper-legacy.cc:58: \"execvp(patch, ...)\": No such file or directory\r\n```\r\n\r\nProtobuf is further available as pip package:\r\n```\r\n$ pip show protobuf                                                                                               \r\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\r\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\r\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\r\nName: protobuf\r\nVersion: 3.12.2\r\nSummary: Protocol Buffers\r\nHome-page: https://developers.google.com/protocol-buffers/\r\nAuthor: None\r\nAuthor-email: None\r\nLicense: 3-Clause BSD License\r\nLocation: /HOME/.local/lib/python3.8/site-packages\r\nRequires: setuptools, six\r\nRequired-by: \r\n```\r\n", "@mihaimaruseac \r\n\r\ndoes 2.3.0-rc.0 support CUDA 11RC ?", "No, it's being slated for 2.4", "We will wait until 2.4 hopefully nvidia won't comeout with CUDA 12 by then.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40227\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40227\">No</a>\n", "Unfortunately we cannot simply support all versions of CUDA. Our CI runs on a specific architecture which is shared by other projects at Google too.\r\n\r\nHowever, TensorFlow is a community product and we welcome any PR which moves us towards newer CUDA toolkits, adds new CI (to run on external host, see SIG Build), while still maintaining compatibility with the internal requirements (which in this case are support for CUDA 10.1).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40227\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40227\">No</a>\n", "+1 for a \u201cprerelease\u201d build with CUDA 11 + CUDNN 8 Support", "Surprise Surprise!\r\n[tf-nightly-gpu](https://pypi.org/project/tf-nightly-gpu/) released an hour ago!\r\n![Screenshot from 2020-08-20 23-01-57](https://user-images.githubusercontent.com/10144957/90805568-6b3ca900-e339-11ea-8ed4-94d4a34dfc67.png)\r\n"]}, {"number": 40226, "title": "Eigen download can fail due to Gitlab captcha", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.8.2003\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: n/a\r\n- Python version: n/a\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): GCC 4.8.5\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\nDownloading eigen from the gitlab mirror sometimes fails due to captcha and doesn't have a helpful error message\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n[root@9a53b50ad80e tensorflow]# ./tensorflow/lite/tools/make/download_dependencies.sh\r\ndownloading https://gitlab.com/libeigen/eigen/-/archive/c2ab36f47a34e572f37e3dd556ac8a04ab769277/eigen-c2ab36f47a34e572f37e3dd556ac8a04ab769277.tar.gz\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100  5511    0  5511    0     0  41050      0 --:--:-- --:--:-- --:--:-- 40822\r\nchecking sha256 of tensorflow/lite/tools/make/downloads/eigen\r\n/tmp/tmp.y8lTsk6u19/eigen-c2ab36f47a34e572f37e3dd556ac8a04ab769277.tar.gz: FAILED\r\nsha256sum: WARNING: 1 computed checksum did NOT match\r\n[root@9a53b50ad80e tensorflow]# head /tmp/tmp.uZlok4zSME/eigen-c2ab36f47a34e572f37e3dd556ac8a04ab769277.tar.gz\r\n<!DOCTYPE html>\r\n<html>\r\n<head>\r\n  <meta content=\"width=device-width, initial-scale=1, maximum-scale=1\" name=\"viewport\">\r\n  <title>Captcha Challenge</title>\r\n  <style>body{color:#666;text-align:center;font-family:Helvetica Neue,Helvetica,Arial,sans-serif;margin:auto;font-size:14px;display:flex;flex-direction:column;align-items:center;justify-content:center}hr{max-width:800px;margin:18px auto;border:0;border-top:1px solid #eee;border-bottom:1px solid #fff}img{max-width:40vw}.container{margin:auto 20px}.cferror_details{list-style-type:none}.cf-error-details h1{color:#456;font-size:20px;font-weight:400;line-height:28px}</style>\r\n</head>\r\n\r\n<body>\r\n  <div class=\"header\">\r\n```\r\n**Any other info / logs**\r\nIn https://github.com/tensorflow/tensorflow/pull/29017 , the tensorflow mirror was filtered out, but I'm not sure if this mirror is more stable now. Options to resolve include using or falling back to the tensorflow mirror or checking for the `403` status from Gitlab to present a more descriptive error message. I didn't get this error on my own computer but did when running inside centos7 docker on https://labs.play-with-docker.com/\r\n", "comments": ["@dtrodrigues \r\nPlease update the system information, for us to analyse the issue faced.", "I've updated the system information but this looks to be a platform-agnostic issue. It's not currently happening but as shown in the log above, there are cases where Gitlab requests a captcha when downloading and the `download_dependencies.sh` script fails and only shows that the there was a download mismatch instead of something more indicative of the issue or attempting to use the other mirror.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40226\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40226\">No</a>\n"]}, {"number": 40225, "title": "how could tf.image.extract_patches with dynamic kernel size?", "body": "The `sizes` parameter in `tf.image.extract_patches` can only be integers. However, I want to let the `kernel_size = sentence_length` which is variable. The `sentence_length` comes from `tf.shape(input)[1]`, in which `input` is `tf.keras.Input()` of size `[None, None, embedding_size]`.", "comments": ["@godweiyang \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "> @godweiyang\r\n> \r\n> Do you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!\r\n\r\nhere is a demo that will cause this error and I run in tensorflow 2.2.0:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nfeature_dim = 1024\r\n\r\n# placeholder of input sentence of size: [batch_size, sentence_length, feature_dim]\r\nx = tf.keras.layers.Input((None, feature_dim), dtype=\"float32\", name=\"features\")\r\n\r\n# we need to let kernel_size=sentence_length in our model\r\nkernel_size = tf.shape(x)[1]\r\n\r\n# expand x from 1d to 2d\r\nx = tf.expand_dims(x, axis=1)\r\n\r\n# this will cause ERROR: TypeError: Expected int for argument 'ksizes' not <tf.Tensor 'strided_slice:0' shape=() dtype=int32>.\r\nx_unfold = tf.image.extract_patches(x, sizes=[1, 1, kernel_size, 1], strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding='VALID')\r\n```", "@godweiyang,\r\nSorry for the delayed response. Can you please refer this [Stack Overflow Answer](https://stackoverflow.com/a/43576298/11530462) where the similar error has been resolved? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@godweiyang the short answer is that this is not supported. Please see #36621. I think the documentation is wrong since it states that sizes and strides are 1D tensors. I am not sure anyway, let see if someone else can confirm it."]}, {"number": 40224, "title": "'fused' argument of BatchNormalization is not saved in the model", "body": "Batch normalization layer's `fused` argument is not part of the saved model h5/json.   \r\n\r\nTested with TF version: 2.1.0 (CPU), Python 3.7.7\r\n\r\n```\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import Model\r\n\r\nx = layers.Input((32, 32))\r\nm = Model(x, layers.BatchNormalization(fused=False)(x))\r\nprint('fused' in m.to_json())\r\n```\r\nOutput: `False`.\r\nExpected output is `True`.\r\n\r\nI found this issue when trying to get equal predictions in different versions of tensorflow.\r\n\r\nI made a model with batch normalization layers. I tested the model in TF 1.12 and TF 2.1.0 (both are CPU versions, if that matters) with the same input. I got almost same predictions but differing in 4 or 5th decimal. Difference may become larger if the model is deeper. Once I manually added `\"fused\" : false` in the model json, the predictions became exactly same, at least after batch norm layer. \r\n\r\nFixing this issue can help during sanity check of the model, for the developers like me who work with different TF versions simultaneously.\r\n", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/097019fc33cd11a9261eb368b0db4377/40224.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/5ff8e0e7695d09abc816b7d2b4b812e1/40224-tf-nightly.ipynb). Please find the attached gist. Thanks!", "any updates on this issue?", "@sandeepjana For your code snippet, the `fused` arg has been set as `False` in the layer, why is it expected to be `True`?", "@sandeepjana  Could you please check the above comment and let us know if the issue persists still.\r\n\r\nClosing this issue as of now since it was the intended behavior. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40224\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40224\">No</a>\n", "Hi, this issue is not solved.\r\n\r\nWhen the input is a 4D tensor, serializing and deserializing a `BatchNormalization` layer with `fused=False` will give a layer with `fused=True`. The serialization seems to be the problem. It doesn't save the state of the `fused` parameter. Adding it manually works, as the following snippet (which builds on the snippet posted by @sandeepjana) shows.\r\nNote that unlike the original snippet (3D input, if we count the batch dimension), I added an extra dimension to the input (4D input).\r\n\r\n```\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import Model\r\n\r\nx = layers.Input((32, 32, 1))\r\nm = Model(x, layers.BatchNormalization(fused=False)(x))\r\nprint('fused' in m.to_json())\r\n\r\n# Load from JSON\r\nfrom tensorflow.keras.models import model_from_json\r\nm_loaded = model_from_json(m.to_json())\r\nprint(m_loaded.layers[1].fused)\r\n\r\n# Correct JSON\r\nimport json\r\njson_corrected = json.loads(m.to_json())\r\njson_corrected['config']['layers'][1]['config']['fused'] = False\r\njson_corrected = json.dumps(json_corrected)\r\n\r\n# Load from corrected JSON\r\nm_loaded = model_from_json(json_corrected)\r\nprint(m_loaded.layers[1].fused)\r\n```\r\n\r\nThis prints:\r\n```\r\nFalse\r\nTrue\r\nFalse\r\n```\r\nwhich means that:\r\n- the `fused` parameter isn't found in the serialized model\r\n- after deserialization, the BN layer has the parameter `fused = True`, which is incorrect\r\n- after correction of the JSON and deserialization, the BN layer once again has the parameter `fused = False`, which would be the expected behavior.\r\n\r\nThis was tested in Tensorflow 2.3.0."]}, {"number": 40223, "title": "[grappler] Convert identity ConjugateTranspose to Conj instead of removing it", "body": "This pr fixes the bug in #27500 where the identity conjugate transpose op was wrongly removed.\r\n\r\nThank you for your time on reviewing this pr.", "comments": ["@gbaned \r\nThis pr has been waiting for reviewing for a while... Maybe we can add someone else to review it? I believe it is a pretty simple change and won't take too much time...", "@ezhulenev @gbaned \r\nCould you help to merge this pr? Thank you! :).", "@zhuzilin thanks for the fix!"]}, {"number": 40221, "title": "Can not convert tf.map_fn during TFLite Conversion (With Minimum Example)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Ubuntu 2004 over WSL2\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (or github SHA if from source): 2.2 Stable\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,\r\n    tf.lite.OpsSet.SELECT_TF_OPS,\r\n]\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-06-05 22:27:19.003670: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-05 22:27:19.003971: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-06-05 22:27:19.004272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-06-05 22:27:19.004567: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-06-05 22:27:19.004842: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-06-05 22:27:19.005144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-06-05 22:27:19.005465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-05 22:27:19.006000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-06-05 22:27:19.006294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-05 22:27:19.006536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0\r\n2020-06-05 22:27:19.006735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N\r\n2020-06-05 22:27:19.007280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4831 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-06-05 22:27:19.301593: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-06-05 22:27:19.301969: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 1282 nodes (966), 1999 edges (1683), time = 33.269ms.\r\n2020-06-05 22:27:19.302215: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 1282 nodes (0), 1999 edges (0), time = 15.255ms.\r\n2020-06-05 22:27:19.302403: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: map_while_body_33337\r\n2020-06-05 22:27:19.302750: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 39 nodes (0), 42 edges (0), time = 1.062ms.\r\n2020-06-05 22:27:19.303076: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 39 nodes (0), 42 edges (0), time = 0.9ms.\r\n2020-06-05 22:27:19.303388: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_png_false_33387\r\n2020-06-05 22:27:19.303619: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 8 nodes (0), 8 edges (0), time = 0.331ms.\r\n2020-06-05 22:27:19.303812: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 8 nodes (0), 8 edges (0), time = 0.34ms.\r\n2020-06-05 22:27:19.304065: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_png_true_33386\r\n2020-06-05 22:27:19.304353: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 22:27:19.304665: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 22:27:19.304962: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: map_1_while_cond_34702\r\n2020-06-05 22:27:19.305318: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 22:27:19.305696: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 22:27:19.306091: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: decode_image_cond_jpeg_false_33368\r\n2020-06-05 22:27:19.306474: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 11 nodes (0), 12 edges (0), time = 0.448ms.\r\n2020-06-05 22:27:19.306808: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 11 nodes (0), 12 edges (0), time = 0.489ms.\r\n2020-06-05 22:27:19.307104: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_gif_false_33398\r\n2020-06-05 22:27:19.307397: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 22:27:19.307741: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 22:27:19.308084: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: decode_image_cond_jpeg_true_33367\r\n2020-06-05 22:27:19.308466: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 22:27:19.308762: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 22:27:19.309035: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: map_1_while_body_34703\r\n2020-06-05 22:27:19.309325: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 22:27:19.309541: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 22:27:19.309842: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: map_while_cond_33336\r\n2020-06-05 22:27:19.310108: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-05 22:27:19.310376: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-05 22:27:19.310642: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_gif_true_33397\r\n2020-06-05 22:27:19.310783: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 22:27:19.310998: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 22:27:21.532247: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-06-05 22:27:21.532847: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-05 22:27:21.535655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1060 computeCapability: 6.1\r\ncoreClock: 1.733GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2020-06-05 22:27:21.536315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-05 22:27:21.536688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-06-05 22:27:21.537057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-06-05 22:27:21.537401: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-06-05 22:27:21.537814: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-06-05 22:27:21.538133: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-06-05 22:27:21.538423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-05 22:27:21.538987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-06-05 22:27:21.539329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-05 22:27:21.539617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0\r\n2020-06-05 22:27:21.540042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N\r\n2020-06-05 22:27:21.540561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4831 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-06-05 22:27:21.867862: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-06-05 22:27:21.868148: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 931 nodes (-351), 1333 edges (-666), time = 84.162ms.\r\n2020-06-05 22:27:21.868426: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 931 nodes (0), 1333 edges (0), time = 20.041ms.\r\n2020-06-05 22:27:21.868694: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: map_1_while_body_34703_frozen\r\n2020-06-05 22:27:21.868934: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 30 nodes (0), 27 edges (0), time = 0.558ms.\r\n2020-06-05 22:27:21.869323: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 30 nodes (0), 27 edges (0), time = 0.322ms.\r\n2020-06-05 22:27:21.869599: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: map_while_cond_33336_frozen\r\n2020-06-05 22:27:21.869891: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 11 nodes (0), 8 edges (0), time = 0.256ms.\r\n2020-06-05 22:27:21.870168: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 11 nodes (0), 8 edges (0), time = 0.111ms.\r\n2020-06-05 22:27:21.870434: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: decode_image_cond_jpeg_false_33368_frozen\r\n2020-06-05 22:27:21.870756: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 11 nodes (0), 12 edges (0), time = 0.415ms.\r\n2020-06-05 22:27:21.871043: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 11 nodes (0), 12 edges (0), time = 0.195ms.\r\n2020-06-05 22:27:21.871326: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_png_true_33386_frozen\r\n2020-06-05 22:27:21.871605: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 6 nodes (0), 4 edges (0), time = 0.199ms.\r\n2020-06-05 22:27:21.871895: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 6 nodes (0), 4 edges (0), time = 0.067ms.\r\n2020-06-05 22:27:21.872182: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_gif_true_33397_frozen\r\n2020-06-05 22:27:21.872445: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 8 nodes (-7), 8 edges (-6), time = 0.434ms.\r\n2020-06-05 22:27:21.872715: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 8 nodes (0), 8 edges (0), time = 0.093ms.\r\n2020-06-05 22:27:21.872937: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_gif_false_33398_frozen\r\n2020-06-05 22:27:21.873215: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 15 nodes (-4), 18 edges (-2), time = 0.527ms.\r\n2020-06-05 22:27:21.873520: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 15 nodes (0), 18 edges (0), time = 0.164ms.\r\n2020-06-05 22:27:21.873774: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_png_false_33387_frozen\r\n2020-06-05 22:27:21.874025: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 8 nodes (0), 8 edges (0), time = 0.264ms.\r\n2020-06-05 22:27:21.874282: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 8 nodes (0), 8 edges (0), time = 0.129ms.\r\n2020-06-05 22:27:21.874542: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: decode_image_cond_jpeg_true_33367_frozen\r\n2020-06-05 22:27:21.874782: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 9 nodes (-3), 8 edges (-2), time = 0.378ms.\r\n2020-06-05 22:27:21.875115: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 9 nodes (0), 8 edges (0), time = 0.109ms.\r\n2020-06-05 22:27:21.875358: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: map_1_while_cond_34702_frozen\r\n2020-06-05 22:27:21.875596: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 11 nodes (0), 8 edges (0), time = 0.255ms.\r\n2020-06-05 22:27:21.875888: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 11 nodes (0), 8 edges (0), time = 0.107ms.\r\n2020-06-05 22:27:21.876138: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: map_while_body_33337_frozen\r\n2020-06-05 22:27:21.876413: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 39 nodes (0), 42 edges (0), time = 1.324ms.\r\n2020-06-05 22:27:21.876736: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 39 nodes (0), 42 edges (0), time = 0.491ms.\r\nTraceback (most recent call last):\r\n  File \"deploy.py\", line 122, in <module>\r\n    generator = run(model.generator, tflite_path)\r\n  File \"deploy.py\", line 88, in run\r\n    tflite = convert(wrapped)\r\n  File \"deploy.py\", line 63, in convert\r\n    tflite_model = converter.convert()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 514, in convert\r\n    result = _toco_convert_impl(\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 491, in toco_convert_impl\r\n    data = toco_convert_protos(\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 227, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-06-05 22:27:28.323077: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-05 22:27:31.116166: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.\r\n2020-06-05 22:27:31.116208: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.\r\nloc(fused[callsite(\"map/TensorArrayV2_1@__inference_call_34750\"(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\ops\\map_fn.py\":417:0) at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\":574:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\deblurrer-1.0.0-py3.8.egg\\deblurrer\\model\\wrapper.py\":42:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\":957:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":3299:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\":441:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\":981:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":2657:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":2777:0 at \"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":2446:0))))))))), \"image_byte_wrapper/StatefulPartitionedCall/map/TensorArrayV2_1\"]): error: requires element_shape to be 1D tensor during TF Lite transformation pass\r\nloc(fused[callsite(\"map/TensorArrayV2_1@__inference_call_34750\"(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\ops\\map_fn.py\":417:0) at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\":574:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\deblurrer-1.0.0-py3.8.egg\\deblurrer\\model\\wrapper.py\":42:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\":957:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":3299:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\":441:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\":981:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":2657:0 at callsite(\"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":2777:0 at \"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":2446:0))))))))), \"image_byte_wrapper/StatefulPartitionedCall/map/TensorArrayV2_1\"]): error: failed to legalize operation 'tf.TensorListReserve'\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\Scripts\\toco_from_protos-script.py\", line 11, in <module>\r\n    load_entry_point('tensorflow==2.2.0', 'console_scripts', 'toco_from_protos')()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 50, in execute\r\n    output_str = _pywrap_toco_api.TocoConvert(\r\nException: C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\ops\\map_fn.py:417:3: error: requires element_shape to be 1D tensor during TF Lite transformation pass\r\n  return map_fn(\r\n  ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:574:7: note: called from\r\n      return func(*args, **kwargs)\r\n      ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\deblurrer-1.0.0-py3.8.egg\\deblurrer\\model\\wrapper.py:42:9: note: called from\r\n        images = tf.map_fn(pre_input, inputs, dtype=tf.float32)\r\n        ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:957:13: note: called from\r\n            return autograph.converted_call(\r\n            ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3299:5: note: called from\r\n    return wrapped_fn(*args, **kwargs)\r\n    ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:441:9: note: called from\r\n        return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n        ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:981:7: note: called from\r\n      func_outputs = python_func(*func_args, **func_kwargs)\r\n      ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2657:9: note: called from\r\n        func_graph_module.func_graph_from_py_func(\r\n        ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2777:7: note: called from\r\n      graph_function = self._create_graph_function(args, kwargs)\r\n      ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2446:7: note: called from\r\n      graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n      ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\ops\\map_fn.py:417:3: error: failed to legalize operation 'tf.TensorListReserve'\r\n  return map_fn(\r\n  ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:574:7: note: called from\r\n      return func(*args, **kwargs)\r\n      ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\deblurrer-1.0.0-py3.8.egg\\deblurrer\\model\\wrapper.py:42:9: note: called from\r\n        images = tf.map_fn(pre_input, inputs, dtype=tf.float32)\r\n        ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:957:13: note: called from\r\n            return autograph.converted_call(\r\n            ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3299:5: note: called from\r\n    return wrapped_fn(*args, **kwargs)\r\n    ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:441:9: note: called from\r\n        return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n        ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:981:7: note: called from\r\n      func_outputs = python_func(*func_args, **func_kwargs)\r\n      ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2657:9: note: called from\r\n        func_graph_module.func_graph_from_py_func(\r\n        ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2777:7: note: called from\r\n      graph_function = self._create_graph_function(args, kwargs)\r\n      ^\r\nC:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2446:7: note: called from\r\n      graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n```\r\n\r\n**Failure details**\r\nWhen trying to convert tf.map_fn op tflite the above error trace arises. The function is the following:\r\n```\r\ndef pre_input(image):\r\n            image = tf.io.decode_image(image[0])\r\n            image = tf.cast(image, dtype=tf.float32)\r\n            image = (image - 127.0) / 128.0\r\n            return image\r\n\r\nimages = tf.map_fn(pre_input, inputs, dtype=tf.float32)\r\n```\r\nWith ```inputs``` on line 7 being a string tensor of shape [batch, 1], and outputs a float tensor with shape [batch, h, w, 3].\r\n\r\nSpecial care to the line ```Exception: C:\\ProgramData\\Anaconda3\\envs\\deblurring-model\\lib\\site-packages\\tensorflow\\python\\ops\\map_fn.py:417:3: error: requires element_shape to be 1D tensor during TF Lite transformation pass```\r\n\r\nI tried different shapes for inputs, like [batch, 1, 1] or [batch], this changes the fn input elements accordingly, but no luck, seems to be a problem with the shape of the inner TensorArray elements, but i dont know how i can have control over that.\r\n\r\nThanks in advance!\r\n", "comments": ["@ravikyram here is a minimum example:\r\n```\r\nimport tensorflow as tf\r\n\r\nclass ImageByteWrapper(tf.keras.Model):\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 1], dtype=tf.string)])\r\n    def call(self, inputs):\r\n        def pre_input(image):\r\n            image = tf.io.decode_image(image[0])\r\n            image = tf.cast(image, dtype=tf.float32)\r\n            image = (image - 127.0) / 128.0\r\n            return image\r\n\r\n        images = tf.map_fn(pre_input, inputs, dtype=tf.float32)\r\n\r\n        return images\r\n\r\ndef convert(model):\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    #converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n    #converter.target_spec.supported_ops = [\r\n        #tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        #tf.lite.OpsSet.SELECT_TF_OPS,\r\n    #]\r\n    tflite_model = converter.convert()\r\n\r\n    return tflite_model\r\n\r\nmodel = ImageByteWrapper()\r\n\r\ntest_input = tf.random.uniform(shape=[64, 64, 3], minval=0, maxval=255, dtype=tf.int32)\r\ntest_input = tf.cast(test_input, dtype=tf.uint8)\r\ntest_input = tf.io.encode_jpeg(test_input)\r\ntest_input = tf.stack([test_input, test_input])\r\ntest_input = tf.reshape(test_input, [-1, 1])\r\n\r\nwith tf.device('/cpu:0'):\r\n    test_output = model(test_input)\r\n\r\ntflite = convert(model)\r\n```", "@jvishnuvardhan any updates on this? i truly need the support, thanks!", "Hello there! @ravikyram @jvishnuvardhan can you give me some support? Thanks!", "I can reproduce the issue with `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/1ce6ce5c36801b149fb4f1ffd751ffef/40221.ipynb) is the gist for our reference. Thanks!", "Haoliang, Can you please have a look.\r\n\r\nThanks", "Seeing a same error", "run into the same error, tried tf-nightly with latest build `tf-nightly==2.4.0.dev20200707`, I can avoid this problem by downgrade to `tensorflow=2.0.0`, but tf 2.0.0 got its own issue[https://github.com/tensorflow/tensorflow/pull/28410](url), hope to have a solution on this", "Hi,\r\n\r\nSorry for the late response on this issue. The cause for the failure is that the TensorArray ops doesn't pass a valid element shape inside the map_fn implementation. This will result in an error when converting to TF Lite. One way you can walk-around the issue is to specify 'fn_output_signature=tf.TensorSpec(shape, dtype)' when calling tf.map_fn in your code. I modified your program with something like this:\r\n    images = tf.map_fn(pre_input, inputs, dtype=tf.float32, fn_output_signature=tf.TensorSpec((64, 64, 3), dtype=tf.float32))\r\n\r\n(not sure if the shape i specified here is correct, but it gives you an idea on how to modify)\r\nAnd the error goes away (however it raises another error unrelated to this).\r\n\r\nCould you give a try? Thanks.", "> Hi,\r\n> \r\n> Sorry for the late response on this issue. The cause for the failure is that the TensorArray ops doesn't pass a valid element shape inside the map_fn implementation. This will result in an error when converting to TF Lite. One way you can walk-around the issue is to specify 'fn_output_signature=tf.TensorSpec(shape, dtype)' when calling tf.map_fn in your code. I modified your program with something like this:\r\n> images = tf.map_fn(pre_input, inputs, dtype=tf.float32, fn_output_signature=tf.TensorSpec((64, 64, 3), dtype=tf.float32))\r\n> \r\n> (not sure if the shape i specified here is correct, but it gives you an idea on how to modify)\r\n> And the error goes away (however it raises another error unrelated to this).\r\n> \r\n> Could you give a try? Thanks.\r\n\r\nproblem solved, thanks", "@ElPapi42 \r\n\r\nIs this still an issue?\r\nPlease, close this thread if your issue was resolved.Thanks!", "@ravikyram sorry for the no response, i will test out today, and maybe open an issue fir the other error", "I can confirm this solution works, the unrelated derived error is close to  https://github.com/tensorflow/tensorflow/issues/40370, i will close this issue.\r\n\r\nPD: Looking forward to a fix for the linked PR, @thaink mention a refactor for implement b64encoding/decoding and image encoding/decoding in tflite", "@ElPapi42 \r\n\r\nPlease, close this thread as your issue was resolved.Thanks!"]}, {"number": 40220, "title": "Windows Python 3.7, Pycharm and tensorflow problem on import tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (Windows 10):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.7\r\n- Installed using pip:\r\n\r\nIm traying to run a script with tensorflow imported, it works when use spyder but with pycharm gives me this traceback:\r\n\r\n`Traceback (most recent call last):\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/YOURDATA/Desktop/SamuelFernandes/ML/Object_detection_Ualg - pycharm/main.py\", line 3, in <module>\r\n    import generate_tfrecord\r\n  File \"C:\\Users\\YOURDATA\\Desktop\\SamuelFernandes\\ML\\Object_detection_Ualg - pycharm\\generate_tfrecord.py\", line 25, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'`\r\n\r\nIm traying to run a script with tensorflow imported, it works when use spyder but with pycharm gives me this traceback:\r\n\r\n    Traceback (most recent call last):\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/YOURDATA/Desktop/SamuelFernandes/ML/Object_detection_Ualg - pycharm/main.py\", line 3, in <module>\r\n    import generate_tfrecord\r\n  File \"C:\\Users\\YOURDATA\\Desktop\\SamuelFernandes\\ML\\Object_detection_Ualg - pycharm\\generate_tfrecord.py\", line 25, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\YOURDATA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nI'm new to python and know it as a loot of compatibility issues, can someane help me see what is happening? Thanks", "comments": ["@SIPFernandes,\r\nIs there any specific reason you are trying to install TensorFlow 1.12? \r\nTensorFlow 1.x is not actively supported, instead could you please install the latest version of TensorFlow i.e. v2.2 and let us know if you are facing the same issue. \r\n\r\nFor more information, please check [this](https://www.tensorflow.org/install/pip#system-requirements) installation guide. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40220\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40220\">No</a>\n"]}, {"number": 40219, "title": "Import util issue", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@Sahil2712  \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.  please provide a code snippet to reproduce the issue reported here. \r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n\r\nPlease refer to below issues related with util errors.\r\n#39175  #39732 #40112 #39960 #38969 #39845 #37432 #36809 #30758 #39196 #36501 #24268", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Saduf,\nI'm sorry.\nI was busy due to exams. I'm using windows 10,i5 processor and in jupyter\nnotebook,tenserflow 2.0 version.\nIf I run it in tf 1.9 I'm getting output but not in 2.0.\nThanks,\nSahil Naik\n\nOn Sun, Jun 14, 2020, 4:39 PM tensorflow-butler[bot] <\nnotifications@github.com wrote:\n\n> This issue has been automatically marked as stale because it has not had\n> recent activity. It will be closed if no further activity occurs. Thank you.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40219#issuecomment-643751735>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AMYYDM23DSLSMT7V6CMXGXLRWSVWRANCNFSM4NV4WVFQ>\n> .\n>\n", "@Sahil2712\r\nPlease refer to the issues shared with similar error."]}, {"number": 40218, "title": "tensorflow/core/public/session.h Reset is not work?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@lyfpeter \r\n\r\nRequest you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease, share colab ink or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Nothing useful provided in the template."]}, {"number": 40217, "title": "MobileNetV3", "body": "**System information**\r\n- TensorFlow version (you are using): tf-nightly 2.2.0.dev20200508 \r\n- Are you willing to contribute it (Yes/No): Would be willing to but I've never contributed code to an open sourced project.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI want to do transfer learning using MobileNetV3. I see on [here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet_v3.py) that MobileNetV3 is available, but I don't see it either in the [Keras website](https://keras.io/applications/) or on [TF.](https://www.tensorflow.org/api_docs/python/tf/keras/applications?version=nightly) Why is this? \r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nEveryone", "comments": ["It seems that the code was added to keras-team/keras, but hasn't been port to tf/keras yet. We will fix it and make the pre-trained weight available.", "Hi, it's been almost 2 months since the last post. Are there any updates? \r\n\r\nI would really, really appreciate if you could port it to tf/keras because I need it for my experimentation as soon as possible. ", "> Hi, it's been almost 2 months since the last post. Are there any updates?\r\n> \r\n> I would really, really appreciate if you could port it to tf/keras because I need it for my experimentation as soon as possible.\r\n\r\nThis has been done just now, please try out in tf-nightly", "Hi, thank you very much for the commit and for the response! \r\n\r\nI upgraded tf-nightly and am now in version 2.4.0.dev20200809, but sadly when I tried it out I got: \r\n\r\n\"AttributeError: module 'tensorflow.keras.applications' has no attribute 'mobilenet_v3' \"\r\n\r\n", "> Hi, thank you very much for the commit and for the response!\r\n> \r\n> I upgraded tf-nightly and am now in version 2.4.0.dev20200809, but sadly when I tried it out I got:\r\n> \r\n> \"AttributeError: module 'tensorflow.keras.applications' has no attribute 'mobilenet_v3' \"\r\n\r\nIt's in 2.4.0-dev20200810. Please try again. ", "Thank you, now it's working! \r\n\r\nI still have 3 points to address: \r\n\r\n1. Using the exact same parameters, dataset and identical conditions (other than the model itself used) I trained a minimalistic MobileNetV3 with alpha of 1.0 using the imagenet weights, freezing the feature extractor layers, I get a around 53% accuracy while with MobileNetV2 with alpha 1.0 I get around 86% accuracy. Have you also faced similar issues?\r\n2. When I try to run the network with lower alphas I get the following error: \r\n\"ValueError: If imagenet weights are being loaded, alpha can be one of `0.75`, `1.0` for non minimalistic or `1.0` for minimalistic only.\"\r\nWhy is this limitation present? Would be curious to understand that a bit more in depth.\r\n3. The function \"tf.keras.applications.MobileNetV3Samll\" is misspelled. ", "> Thank you, now it's working!\r\n> \r\n> I still have 3 points to address:\r\n> \r\n> 1. Using the exact same parameters, dataset and identical conditions (other than the model itself used) I trained a minimalistic MobileNetV3 with alpha of 1.0 using the imagenet weights, freezing the feature extractor layers, I get a around 53% accuracy while with MobileNetV2 with alpha 1.0 I get around 86% accuracy. Have you also faced similar issues?\r\n\r\nFrom https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md, it seems that minimalistic small is achieving 61.9 TOP1 acc. Not sure which model you are using/training. \r\n\r\n> 2. When I try to run the network with lower alphas I get the following error:\r\n>    \"ValueError: If imagenet weights are being loaded, alpha can be one of `0.75`, `1.0` for non minimalistic or `1.0` for minimalistic only.\"\r\n>    Why is this limitation present? Would be curious to understand that a bit more in depth.\r\n\r\nThis is expected since the pretrained weights from original implementation only has 0.75 and 1.0 provided. You need to set the \"weight\" param to None if you want to use other value of alpha.\r\n\r\n> 3. The function \"tf.keras.applications.MobileNetV3Samll\" is misspelled.\r\n\r\nIt has been updated yesterday, should be available in the latest nightly release.\r\n"]}, {"number": 40216, "title": "Use new param log_all in CSVLogger to log all elements.", "body": "@rchao Hi, can you please review the following. Reference to #40114 .\r\n\r\nNew feature. Use new param \"log_all\" in CSVLogger to log all elements in training even if some epochs don't contain the same elements. Example of this is when validation_freq is set to > 1, and some epochs won't contain validation accuracy/loss. CSV file header will contains all elements in all epochs.", "comments": ["@gbaned hi any update on this?", "@PiyushDatta  Can you please check @rchao's comments and keep us posted ? Thanks!", "Will get to this during the weekend.", "**Validation**\r\n\r\n```pylint --rcfile=tensorflow/tools/ci_build/pylintrc tensorflow/python/keras/callbacks.py```\r\nYour code has been rated at 9.75/10 (previous run: 9.75/10, +0.00)\r\n\r\n`python tensorflow/python/keras/callbacks_test.py KerasCallbacksTest.test_CSVLogger`\r\n\r\nINFO:tensorflow:time(__main__.KerasCallbacksTest.test_CSVLogger): 1.08s\r\nI0502 17:29:58.183570 32040 test_util.py:2102] time(__main__.KerasCallbacksTest.test_CSVLogger): 1.08s\r\n[       OK ] KerasCallbacksTest.test_CSVLogger\r\nRan 1 test in 1.082s\r\n**OK**", "New approach:\r\n- Write to csv file on each epoch\r\n- If we have log_all enabled and we hit a point where the current epoch has more keys/columns than previous epochs and rows we:\r\n       1. Create a backup file of current csv file (in case something goes wrong in this process)\r\n       2. Create a copy of current csv file as a temp file (we will make this current csv file at end)\r\n       3. Include new keys/columns in `self.keys`\r\n       4. Write new columns/adjustments in to temp file and read from current csv file\r\n       4. Go to header position in temp file (first header we wrote in this file) and write out new header with new columns\r\n       5. Go to starting position in both temp file and csv file (first row we inserted during this training)\r\n       6. For each row from csv file, read the row, add new columns with empty string, and write into temp file and flush\r\n       8. Repeat for all rows until current row\r\n       9. Close temp file, rename temp file as current file, and remove back-up file\r\n      10. Now the temp file is renamed to current csv file, we now write the current epoch data into the file and go on with regular operations.\r\n   \r\nIf we don't have log_all enabled or log_all is enabled but all epochs have same # of keys/columns then we keep original design/process.", "@rchao please review new revision. @gbaned fyi.", "@PiyushDatta Can you please resolve conflicts? Thanks!", "Will resolve conflicts this weekend", "@gbaned fixed merge conflicts, good to review now", "This should be moved to the Keras repo now with the split.\r\n\r\nCC @qlzh727 ", "Thanks for the PR. Keras code has been moved to keras-team/keras repository, and the old code in tensorflow/python/keras will be removed soon. Please send your PR to the new repo and we will take it from there. Thanks."]}, {"number": 40215, "title": "keras layers LSTM uses inconsistent dropout approach", "body": "**System information**\r\n- TensorFlow version (use command below): 2.2\r\n\r\n**Describe the current behavior**\r\nThe input dropout mask is the same for the respective input, forget, update, and output computation. \r\n\r\nfrom the call method in the LSTM class in [recurrent_v2.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent_v2.py)\r\n```python\r\n    dropout_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\r\n    if dropout_mask is not None:\r\n        inputs = inputs * dropout_mask[0]\r\n```\r\n\r\nHowever, the recurrent dropout mask is unique for the input, forget, update, and output computation\r\n```python\r\n        h_tm1_i = h_tm1 * rec_dp_mask[0]\r\n        h_tm1_f = h_tm1 * rec_dp_mask[1]\r\n        h_tm1_c = h_tm1 * rec_dp_mask[2]\r\n        h_tm1_o = h_tm1 * rec_dp_mask[3]\r\n```\r\n\r\nIf the input dropout mask approach was the intended behavior can someone link me a reference paper that explains why and shouldn't we at least set the count=1. To me it seems like we are attempting to follow the approach from \"[A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf)\"\r\n\r\n**Describe the expected behavior**\r\nI'd have thought the input dropout mask would be unique to the input, forget, update, and output computation\r\n\r\n", "comments": ["@zacwellmer \r\nPlease share a simple stand alone code such that we can replicate the issue faced, or is possible share a colab gist for us to analyse the error faced.", "I don't think stand alone code would help to describe this issue since the issue is heavily embedded into the LSTM implementation. I'll write the equations which are being performed and what I expect to occur.\r\n\r\n# Current Behavior\r\nfor each sequence the masks $\\mathbf{m}\\_{x}$, $\\mathbf{m}\\_{hi}$, $\\mathbf{m}\\_{hf}$, $\\mathbf{m}\\_{ha}$, and $\\mathbf{m}\\_{ho}$ are sampled then used in the following LSTM update\r\n![image](https://user-images.githubusercontent.com/9603276/83980653-54ef8700-a8cc-11ea-9094-8404ac483da3.png)\r\n![image](https://user-images.githubusercontent.com/9603276/83980476-11e0e400-a8cb-11ea-84b1-a33f5c9641d8.png)\r\n![image](https://user-images.githubusercontent.com/9603276/83980483-245b1d80-a8cb-11ea-8ef9-c8c5a9792cd5.png)\r\n\r\n# Expected Behavior\r\nI would expect $\\mathbf{m}\\_{xi}$, $\\mathbf{m}\\_{xf}$, $\\mathbf{m}\\_{xa}$, and $\\mathbf{m}\\_{xo}$ to be different for each gate\r\n![image](https://user-images.githubusercontent.com/9603276/83980529-83b92d80-a8cb-11ea-86ce-fbae1b700e4a.png)\r\n\r\nI'm guessing this is not currently being done b/c there is a desire to use the gpu implementation and that doesn't seem to allow 4 different masked ```inputs``` values\r\n\r\n", "@qlzh727 is there any updates on this? If there's a paper that the current convention was modeled after I'd love to check it out. If not, I'm happy to help out with a PR, though it seems like this would change _could_use_gpu_kernel to False for input dropout", "@qlzh727 @haozha111 do you folks have any thoughts on this? I'm open to make the PR to fix this but would prefer to know if it's a bad idea before spending time on it", "Thanks for reporting the issue. Let me check the change history and related paper/math, and I will post some updates.", "@qlzh727 is there any updates on this? I'm still open to helping ", "@qlzh727 FYI looks there is also discrepancies in how the input mask is handled between when _could_use_gpu_kernel is True and False. When it is True it looks as though we properly apply dropout (uniquely to i, f, a, and o) ([here](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/layers/recurrent.py#L2388)).\r\n```python\r\n        inputs_i = inputs * dp_mask[0]\r\n        inputs_f = inputs * dp_mask[1]\r\n        inputs_c = inputs * dp_mask[2]\r\n        inputs_o = inputs * dp_mask[3]\r\n```\r\n\r\nSeems like the simplest fix for this is to add an extra condition on ```self._could_use_gpu_kernel``` requiring that ```dropout==0.0```. Then the following lines could be dropped ([here](https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/layers/recurrent_v2.py#L1146))\r\n```\r\nself.reset_dropout_mask()\r\n      dropout_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\r\n      if dropout_mask is not None:\r\n        inputs = inputs * dropout_mask[0]\r\n```", "Very sorry for the long wait. \r\n\r\nI checked RNN code history in recurrent.py. Historically we have 2 implementation, which is branched by flag \"implementation\" in https://github.com/tensorflow/tensorflow/blob/8c80b0433f6f09a720a4fc2fbe55b64b4948965c/tensorflow/python/keras/layers/recurrent.py#L2429.\r\n\r\nIn the implementation 1, the input/recurrent input to IFCO gates are dropout via different mask, whereas in implementation 2, they are dropout via same mask. See https://github.com/tensorflow/tensorflow/blob/8c80b0433f6f09a720a4fc2fbe55b64b4948965c/tensorflow/python/keras/layers/recurrent.py#L2468. The recurrent_v2 is using implementation 2 as the basic and mimic it behavior. Historically, we create 4 dropout mask so that both implementation can use the same field, rather than branching the mask creation logic based on \"implementation\" version.\r\n\r\nHope this answers your question.", "No worries and thanks for circling back! \r\n\r\nI understand what it is doing but I don't understand why this would be the desired behavior. I don't understand why a different variant of input dropout would silently be used from an arbitrary combination of flags (ex: ```dropout>0``` and ```recurrent_dropout=0``` vs. ```dropout>0``` and ```recurrent_dropout>0``` ).\r\n\r\nThe only case I could see for sharing the IFCO dropout masks is if the user preferred the code to be faster (using the cudnn implementation) at the cost of questionable performance (I'm not aware of any works studying the impact of fixing a mask for all gates). But even still why would we only offer this speed/performance trade-off for the input mask but not the hidden state mask?", "The reason we split the logic between dropout = 0 and recurrent_dropout = 0 in recurrent_v2 implementation is that cudnn kernel is a fused op and we can't inject the recurrent dropout for hidden state in each timestep. On the other hand, the input dropout can be done by dropping out the overall input sequence before feeding the input to the cudnn kernel.\r\n\r\nNote that in the implementation 2, the behavior between recurrent.py and recurrent_v2.py should be the same. The same dropout mask is used for all 4 gates, and I think that's intended. We could certainly change recurrent_v2.py to use 4 different mask for input IFCO, but I am not sure if that's intended behavior. I would expect the mask to randomly drop the same feature for all IFCO, which mimic the behavior that some certain feature is totally hide from the network within the batch. This is aligned with the concept of the variational dropout, which use the same mask for all timesteps. \r\n\r\nIt seems to me that using different mask for 4 gates will introduce noise to the network, but I don't have the math proof for it.", "ah gotcha, thanks for the explanation. I'm not sure how the cudnn implementation works but the reasoning sounds like it makes sense \r\n\r\n> I would expect the mask to randomly drop the same feature for all IFCO, which mimic the behavior that some certain feature is totally hide from the network within the batch. This is aligned with the concept of the variational dropout, which use the same mask for all timesteps.\r\n\r\nThis approach is called \"tied-weights LSTM\". I agree that the tied-weights approach seems to closely follow traditional uses of dropout (dropping a specific subset of units). However, dropout can also be interpreted as dropping rows of the weight matrix. Through the lens of the second approach it seems intuitive that the mask should differ between gates. \r\n\r\nIt turns out the untied-weights LSTM performs better. From [section 4.1](http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf) and referring to the trade-off from tied-weights\r\n\r\n> With the second parametrisations we would place a distribution over the single matrix W. This leads to a faster forward-pass, but with slightly diminished results as we will see in the experiments section\r\n\r\nFrom Table 1\r\n![image](https://user-images.githubusercontent.com/9603276/92803715-89139180-f36c-11ea-87d3-64eecaa4ca16.png)\r\n\r\n> It seems to me that using different mask for 4 gates will introduce noise to the network\r\n\r\nyeah, the entropy of q(\u03c9) would be larger in the untied weights approach\r\n", "Thanks for the detailed reference. \r\n\r\nIf I understand the \"MC\" correctly, I don't think keras is currently using MC dropout at test time. Basically dropout is disabled at eval/predict time. And if we remove the item with MC, then the tied weights are actually out performing the untied weights (lower error and higher WPS).", "that's a great point, thanks for taking a closer look at that table than I did. On second thought I'm not sure how helpful that table is for us since it doesn't tell us which performs better when dropout is only applied to the input gates (implementation=1).\r\n\r\nStill seems odd to me that we follow the tied weights approach when ```dropout>0``` and ```recurrent_dropout=0``` but follow the untied approach when ```dropout>0``` and ```recurrent_dropout>0```. But if this is the desired behavior, I guess we could at least update the docs ", "I don't think we perform differently between recurrent_dropout=0 and recurrent_dropout>0, since the default implementation in TF2 is implementation=2, which only use the first mask out of 4 to mask the input. The implementation=1 is only used in TF1 and we can't change its behavior since it is suppose to be frozen.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8c80b0433f6f09a720a4fc2fbe55b64b4948965c/tensorflow/python/keras/layers/recurrent.py#L2469\r\n\r\nIf there is no other questions, I will close this bug as working as intended. Feel free to reopen it if there is any other issue.\r\n\r\nSorry again for the long wait and thank you the bring up this question to us.\r\n", "> I don't think we perform differently between recurrent_dropout=0 and recurrent_dropout>0, since the default implementation in TF2 is implementation=2, which only use the first mask out of 4 to mask the input.\r\n\r\nI don't think this is true. ```implementation=1``` is still used when ```dropout>0``` and ```recurrent_dropout=0```. I couldn't figure out a good way to fix the generated dropout masks (fixing the random seed doesn't work) between both approaches. You will need to follow this hacky approach with pdb. In short, the old approach is still used in [recurrent_v2.py](https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/layers/recurrent_v2.py#L1149) \r\n\r\nHere's a [colab notebook](https://colab.research.google.com/drive/1o7DWrvkPZxiu2Ovww-XeCRGbWlsB-xhP?usp=sharing) to demonstrate\r\n![image](https://user-images.githubusercontent.com/9603276/92985048-ed654c80-f463-11ea-9a26-5f5c4754ac85.png)\r\n", "> > I don't think we perform differently between recurrent_dropout=0 and recurrent_dropout>0, since the default implementation in TF2 is implementation=2, which only use the first mask out of 4 to mask the input.\r\n> \r\n> I don't think this is true. `implementation=1` is still used when `dropout>0` and `recurrent_dropout=0`. I couldn't figure out a good way to fix the generated dropout masks (fixing the random seed doesn't work) between both approaches. You will need to follow this hacky approach with pdb. In short, the old approach is still used in [recurrent_v2.py](https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/layers/recurrent_v2.py#L1149)\r\n> \r\n> Here's a [colab notebook](https://colab.research.google.com/drive/1o7DWrvkPZxiu2Ovww-XeCRGbWlsB-xhP?usp=sharing) to demonstrate\r\n> ![image](https://user-images.githubusercontent.com/9603276/92985048-ed654c80-f463-11ea-9a26-5f5c4754ac85.png)\r\n\r\nI don't think so.. when dropout>0 and recurrent_dropout=0, it will fallback to recurrent.py, but both LSTM and LSTM Cell are created with implementation=2 (you can verify that by print out the layer.implementation). When implementation=2, it will use tied dropout, whereas implementation=1 will use untied dropout.\r\n\r\n", "Regardless of what the user passes input for ```implementation```, if ```self._could_use_gpu_kernel``` is true, then the tied weights approach is used for inputs.\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/layers/recurrent_v2.py#L1091-L1095\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/layers/recurrent_v2.py#L1120-L1149\r\n\r\n> it will fallback to recurrent.py\r\n\r\nMaybe I'm misunderstanding something. By \"fallback to recurrent.py\" do you mean that recurrent_v2.py is not used? b/c that example in the colab notebook shows the tied weights approach being used (unless the output is thrown-away after?)\r\n\r\n", "Correct, when self._could_use_gpu_kernel is True, then the tied dropout approach is used and all the logic is in recurrent_v2.py. When self._could_use_gpu_kernel is False, it will fallback to https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/layers/recurrent_v2.py#L1120-L114, and the logic of self.cell.call() is the actual logic gets triggered which lives in recurrent.py. When implementation=2, then the tied dropout will be used in that case.", "The user provided input of the implementation flag doesn't seem to matter since it's overridden anytime the recurrent dropout rate is greater than 0. \r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/layers/recurrent.py#L2300-L2302\r\n\r\nThe tied weights dropout looks like it is _only_ used when ```implementation=2```, ```dropout>0```, and ```recurrent_dropout=0```. In the current implementation, it would be impossible to use the tied weights approach to apply dropout to the hidden state of the LSTM.\r\n\r\nWhy wouldn\u2019t we just offer a tied weights flag instead of silently changing based on an arbitrary set of parameters which the user is unaware of? "]}]