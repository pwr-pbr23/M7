[{"number": 50818, "title": "C API methods to get attribute names", "body": "Adds C API methods to get the attribute names of graph operations.", "comments": ["Thanks for the PR. Could you please explain the motivation for adding these APIs?", "There is currently no good way to read *all* attributes from a graph operation without using something like `TF_OperationToNodeDef`, which is less than ideal since it serializes the entire thing instead of allowing iterating (i.e. filtering based on type or other metadata), and also https://github.com/tensorflow/tensorflow/pull/50816#issuecomment-882860268.  I'm trying to implement graph -> eager raising for tensorflow/java, and that and a few other uses need a way to see all the attributes an op has."]}, {"number": 50817, "title": "TF2 \u2018saved_model.pb\u2019 from \u2018exporter_main_v2.py\u2019 is different than \u2018saved_model.pb\u2019 from official TF2 Zoo", "body": "Please make sure that this is a bug. As per our\r\nGitHub Policy,\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template\r\n\r\nSystem information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\nTensorFlow installed from (source or binary): pip\r\nTensorFlow version (use command below): 2.3.0\r\nPython version: 3.6.9\r\nBazel version (if compiling from source): -\r\nGCC/Compiler version (if compiling from source): -\r\nCUDA/cuDNN version: CUDA 11.4\r\nGPU model and memory: GeForce RTX 3090\r\nI am trying to convert a re-trained TF2 object detection SSD MobilenetV2 model to a proprietary framework. I have successfully re-trained the network and it runs properly. However, I am having trouble with converting the saved_model.pb to the other framework. The conversion script from the SDK I am working with performs optimization on the saved_model.pb, using 'meta_optimizer.cc', which returns an empty graph after running through my re-trained model. I used 'exporter_main_v2.py' to export my re-trained checkpoint to the saved_model.pb which I am having trouble with.\r\n\r\nThe issue is not with my training or checkpoints, but with the exporting process from checkpoint to a saved_model.pb using 'exporter_main_v2.py'. I know this because I downloaded the SSD MobilenetV2 model from the TF2 Zoo to test with it. I have no issue converting the official saved_model.pb file found in the official repo, but when I try to convert the official checkpoints found in the repo to a saved_model.pb using 'exporter_main_v2.py', I face the same issue trying to convert the newly produced saved_model.pb file to the proprietary framework. This means that something wrong is happening when executing the 'exporter_main_v2.py' script.\r\n\r\nDescribe the expected behavior\r\nThe exported saved_model.pb file should not be different than the official saved_model.pb file found in the official repo.\r\n\r\nThe following is what I get, showing 0 nodes and 0 edges grappler_empty_graph\r\n![AAEA8FD8-1609-4BAC-AE9B-53918E23EA20](https://user-images.githubusercontent.com/84783887/126050092-3161c0b0-ce22-4cb5-a054-8fcb08c4933e.png)\r\n\r\n\r\nStandalone code to reproduce the issue\r\nThe model I downloaded is: http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\r\n\r\nThe command I used to export the official checkpoint to a saved_model.pb is: python ~/models/research/object_detection/exporter_main_v2.py --input_type image_tensor --pipeline_config_path pipeline.config --trained_checkpoint_dir checkpoint/ --output_directory exported_model/", "comments": ["I have had some discussion about this on the tensorflow/models repo and they suggested that I ask for https://github.com/tensorflow/models/issues/10120 assistance here. Refer to for details. ", "Have you tried to `sha256sum` the savedmodel in the official `tar.gz` with the one exported yourself with TF 2.4.0 from that same `tar.gz`?", "I have not heard of `sha256sum` before to be honest and not sure how to use it. Any guidance/details you could help with? What would I be looking for exactly?", "E.g. on Ubuntu https://help.ubuntu.com/community/HowToSHA256SUM\n\nIf the process is determinstic you could have the same hash in the end with 2.4.0.", "Thank you for the suggestion! I do not have access to my computer at the moment but I will try it within the coming 2 days and I will keep you updated. ", "I hope that now you can understand my comment at https://github.com/tensorflow/models/issues/10120#issuecomment-881955457\n\nSo If the process is determinstic also the model garden team could reproduce exaclty the same hash of the savedmodel in `tar.gz` with a new export using a specific fixed TF and model garden version (exaclty the one that produced the saved model in the `tar.gz`).", "I am closing this issue since it has been resolved on the original issue post (https://github.com/tensorflow/models/issues/10120) ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50817\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50817\">No</a>\n"]}, {"number": 50816, "title": "Adds header for TFE_OpSetAttrValueProto", "body": "`TFE_OpSetAttrValueProto` was added to the C api, but no header was added for it.  This fixes that.\r\n\r\nThis would be nice to have in 2.6 if possible (for SIG JVM), I'm not sure what the criteria are for that.  It is kind of a bug fix.", "comments": ["The test error does not seem to have anything to do with my changes:\r\n```\r\n\u001b[32m[23,808 / 23,872]\u001b[0m 868 / 1246 tests, \u001b[31m\u001b[1m1 failed\u001b[0m;\u001b[0m Testing //tensorflow/python/kernel_tests/linalg/sparse:csr_sparse_matrix_ops_test [36s (2 actions)] ... (32 actions, 31 running)\r\n\u001b[31m\u001b[1mERROR: \u001b[0m/workspace/tensorflow/core/tpu/kernels/BUILD:1071:11: Couldn't build file tensorflow/core/tpu/kernels/_objs/sharding_util_ops_test/sharding_util_ops_test.o: C++ compilation of rule '//tensorflow/core/tpu/kernels:sharding_util_ops_test' failed (Exit 1): gcc failed: error executing command \r\n  (cd /home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-mkl-linux-cpu-pr/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH='' \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_ENABLE_ONEDNN_OPTS=1 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/tpu/kernels/_objs/sharding_util_ops_test/sharding_util_ops_test.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/tpu/kernels/_objs/sharding_util_ops_test/sharding_util_ops_test.o' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL '-DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote. -iquotebazel-out/k8-opt/bin -iquoteexternal/com_google_absl -iquotebazel-out/k8-opt/bin/external/com_google_absl -iquoteexternal/nsync -iquotebazel-out/k8-opt/bin/external/nsync -iquoteexternal/eigen_archive -iquotebazel-out/k8-opt/bin/external/eigen_archive -iquoteexternal/gif -iquotebazel-out/k8-opt/bin/external/gif -iquoteexternal/libjpeg_turbo -iquotebazel-out/k8-opt/bin/external/libjpeg_turbo -iquoteexternal/com_google_protobuf -iquotebazel-out/k8-opt/bin/external/com_google_protobuf -iquoteexternal/com_googlesource_code_re2 -iquotebazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquoteexternal/farmhash_archive -iquotebazel-out/k8-opt/bin/external/farmhash_archive -iquoteexternal/fft2d -iquotebazel-out/k8-opt/bin/external/fft2d -iquoteexternal/highwayhash -iquotebazel-out/k8-opt/bin/external/highwayhash -iquoteexternal/zlib -iquotebazel-out/k8-opt/bin/external/zlib -iquoteexternal/double_conversion -iquotebazel-out/k8-opt/bin/external/double_conversion -iquoteexternal/snappy -iquotebazel-out/k8-opt/bin/external/snappy -iquoteexternal/curl -iquotebazel-out/k8-opt/bin/external/curl -iquoteexternal/boringssl -iquotebazel-out/k8-opt/bin/external/boringssl -iquoteexternal/jsoncpp_git -iquotebazel-out/k8-opt/bin/external/jsoncpp_git -iquoteexternal/com_google_googletest -iquotebazel-out/k8-opt/bin/external/com_google_googletest -iquoteexternal/local_config_cuda -iquotebazel-out/k8-opt/bin/external/local_config_cuda -iquoteexternal/local_config_rocm -iquotebazel-out/k8-opt/bin/external/local_config_rocm -iquoteexternal/local_config_tensorrt -iquotebazel-out/k8-opt/bin/external/local_config_tensorrt -iquoteexternal/mkl_dnn_v1 -iquotebazel-out/k8-opt/bin/external/mkl_dnn_v1 -iquoteexternal/bazel_tools -iquotebazel-out/k8-opt/bin/external/bazel_tools -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem external/com_google_googletest/googlemock -isystem bazel-out/k8-opt/bin/external/com_google_googletest/googlemock -isystem external/com_google_googletest/googlemock/include -isystem bazel-out/k8-opt/bin/external/com_google_googletest/googlemock/include -isystem external/com_google_googletest/googletest -isystem bazel-out/k8-opt/bin/external/com_google_googletest/googletest -isystem external/com_google_googletest/googletest/include -isystem bazel-out/k8-opt/bin/external/com_google_googletest/googletest/include -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/mkl_dnn_v1/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -msse3 -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/tpu/kernels/sharding_util_ops_test.cc -o bazel-out/k8-opt/bin/tensorflow/core/tpu/kernels/_objs/sharding_util_ops_test/sharding_util_ops_test.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from external/com_google_googletest/googletest/include/gtest/internal/gtest-internal.h:40:0,\r\n                 from external/com_google_googletest/googletest/include/gtest/gtest.h:60,\r\n                 from ./tensorflow/core/platform/test.h:26,\r\n                 from ./tensorflow/core/framework/tensor_testutil.h:24,\r\n                 from tensorflow/core/tpu/kernels/sharding_util_ops_test.cc:28:\r\nexternal/com_google_googletest/googletest/include/gtest/internal/gtest-port.h: In instantiation of 'struct testing::internal::IteratorTraits<tensorflow::{anonymous}::XlaSplitNDTestParam>':\r\nexternal/com_google_googletest/googletest/include/gtest/gtest-param-test.h:301:1:   required by substitution of 'template<class ForwardIterator> testing::internal::ParamGenerator<typename testing::internal::IteratorTraits<Iterator>::value_type> testing::ValuesIn(ForwardIterator, ForwardIterator) [with ForwardIterator = tensorflow::{anonymous}::XlaSplitNDTestParam]'\r\ntensorflow/core/tpu/kernels/sharding_util_ops_test.cc:515:1:   required from here\r\nexternal/com_google_googletest/googletest/include/gtest/internal/gtest-port.h:1967:41: error: no type named 'value_type' in 'struct tensorflow::{anonymous}::XlaSplitNDTestParam'\r\n   typedef typename Iterator::value_type value_type;\r\n                                         ^~~~~~~~~~\r\nexternal/com_google_googletest/googletest/include/gtest/internal/gtest-port.h: In instantiation of 'struct testing::internal::IteratorTraits<tensorflow::{anonymous}::RankedXlaSplitNDTestParam>':\r\nexternal/com_google_googletest/googletest/include/gtest/gtest-param-test.h:301:1:   required by substitution of 'template<class ForwardIterator> testing::internal::ParamGenerator<typename testing::internal::IteratorTraits<Iterator>::value_type> testing::ValuesIn(ForwardIterator, ForwardIterator) [with ForwardIterator = tensorflow::{anonymous}::RankedXlaSplitNDTestParam]'\r\ntensorflow/core/tpu/kernels/sharding_util_ops_test.cc:560:1:   required from here\r\nexternal/com_google_googletest/googletest/include/gtest/internal/gtest-port.h:1967:41: error: no type named 'value_type' in 'struct tensorflow::{anonymous}::RankedXlaSplitNDTestParam'\r\nexternal/com_google_googletest/googletest/include/gtest/internal/gtest-port.h: In instantiation of 'struct testing::internal::IteratorTraits<tensorflow::{anonymous}::XlaConcatNDTestParam>':\r\nexternal/com_google_googletest/googletest/include/gtest/gtest-param-test.h:301:1:   required by substitution of 'template<class ForwardIterator> testing::internal::ParamGenerator<typename testing::internal::IteratorTraits<Iterator>::value_type> testing::ValuesIn(ForwardIterator, ForwardIterator) [with ForwardIterator = tensorflow::{anonymous}::XlaConcatNDTestParam]'\r\ntensorflow/core/tpu/kernels/sharding_util_ops_test.cc:1139:1:   required from here\r\nexternal/com_google_googletest/googletest/include/gtest/internal/gtest-port.h:1967:41: error: no type named 'value_type' in 'struct tensorflow::{anonymous}::XlaConcatNDTestParam'\r\nexternal/com_google_googletest/googletest/include/gtest/internal/gtest-port.h: In instantiation of 'struct testing::internal::IteratorTraits<tensorflow::{anonymous}::RankedXlaConcatNDTestParam>':\r\nexternal/com_google_googletest/googletest/include/gtest/gtest-param-test.h:301:1:   required by substitution of 'template<class ForwardIterator> testing::internal::ParamGenerator<typename testing::internal::IteratorTraits<Iterator>::value_type> testing::ValuesIn(ForwardIterator, ForwardIterator) [with ForwardIterator = tensorflow::{anonymous}::RankedXlaConcatNDTestParam]'\r\ntensorflow/core/tpu/kernels/sharding_util_ops_test.cc:1186:1:   required from here\r\nexternal/com_google_googletest/googletest/include/gtest/internal/gtest-port.h:1967:41: error: no type named 'value_type' in 'struct tensorflow::{anonymous}::RankedXlaConcatNDTestParam'\r\nexternal/com_google_googletest/googletest/include/gtest/internal/gtest-port.h: In instantiation of 'struct testing::internal::IteratorTraits<tensorflow::{anonymous}::RoundtripXlaSplitConcatNDTestParam>':\r\nexternal/com_google_googletest/googletest/include/gtest/gtest-param-test.h:301:1:   required by substitution of 'template<class ForwardIterator> testing::internal::ParamGenerator<typename testing::internal::IteratorTraits<Iterator>::value_type> testing::ValuesIn(ForwardIterator, ForwardIterator) [with ForwardIterator = tensorflow::{anonymous}::RoundtripXlaSplitConcatNDTestParam]'\r\ntensorflow/core/tpu/kernels/sharding_util_ops_test.cc:1422:1:   required from here\r\nexternal/com_google_googletest/googletest/include/gtest/internal/gtest-port.h:1967:41: error: no type named 'value_type' in 'struct tensorflow::{anonymous}::RoundtripXlaSplitConcatNDTestParam'\r\n\u001b[32mINFO: \u001b[0mElapsed time: 4992.945s, Critical Path: 587.16s\r\n\u001b[32mINFO: \u001b[0m25728 processes: 8052 internal, 17676 local.\r\n\u001b[31m\u001b[1mFAILED:\u001b[0m Build did NOT complete successfully\r\n//tensorflow/core/tpu/kernels:sharding_util_ops_test            \u001b[0m\u001b[31m\u001b[1mFAILED TO BUILD\u001b[0m\r\n```", "Hi, this was added to `tensorflow/c/eager/c_api_experimental.h` https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/c/eager/c_api_experimental.h#L430\r\n\r\n@allenlavoie fyi", "How experimental are they?  I.e. should we be wary of using them?  Is there a process for moving it to non-experimental?  As best I can tell it's about a year and half old.", "TFE_OpAttrsSerialize and TFE_OpSetAttrValueProto are experimental mostly because we'd rather not keep using protos for attributes in the future (e.g. because of a migration to TFRT/MLIR/etc.). The former I've seen uses of, but the latter maybe didn't need to be added in the first place (based just on usage now).\r\n\r\nIs there a reason Java is building up attr value protos instead of using the typed attribute setters? The latter is what Python does, which seems to work pretty well.\r\n\r\nIf there's a strong use-case I don't think making these non-experimental is a huge constraint, we would only be promising a compatibility layer between AttrValue protos, not that this is the internal representation. But I'd like to see multiple real uses, not just the symbol sitting there for a year.", "We're using it is when getting attributes of unknown type (i.e. when listing attrs) from the C API or getting or setting them en masse, like copying ops (graph -> graph and graph -> eager).  Even for those, it's possible to not use them, but we'd need a lot of switch statements that duplicate what is in the C api, or some abstraction that is almost identical to the proto.  It's definitely something we can get rid of if necessary.\r\n\r\nI think the only place we *need* the proto setter is for setting function lists on graph ops since there's no typed method for that.", "I see, yes I agree that when doing transfers graph->eager this would be useful. The use-cases I have seen for TFE_OpAttrsSerialize are eager->graph. Maybe we can avoid the need for protos by having a way to transfer these between ops directly in the eager/graph agnostic C API (CC @saxenasaurabh). Even if we could technically support protos long-term with a compatibility layer, it would be nicer to keep protos out of the (non-experimental) API.\r\n\r\nBut I don't think there's much danger of these going away in the near future, so if you have a use-case just keep in mind that there may be a migration in the future. If there's testing against nightlies you should be able to quickly ask for a rollback of a change that forgets about you?\r\n\r\nWe should add a graph op function list setter. Definitely prefer that to using protos. Maybe file a bug, or a PR if you want it done soon :)."]}, {"number": 50815, "title": "Update .bazelrc", "body": "build:xla had a typo in --define=with_xla_supprt (sic), updated to --define=with_xla_support", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50815) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 50814, "title": "AttributeError: module 'tensorflow' has no attribute 'report_uninitialized_variables'", "body": "I'm using Tensorflow version 2 and get this issue when i run this code\r\n```\r\nuninitialized_variables = set([i.decode('ascii') for i in tf.report_uninitialized_variables()])\r\n\r\ninit_op = tf.variables_initializer(\r\n    [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\r\n)\r\n\r\ninit_op\r\n```\r\nError : AttributeError: module 'tensorflow' has no attribute 'report_uninitialized_variables' ", "comments": []}, {"number": 50813, "title": "FIx random gamma and space to depth tests.", "body": "Fix random gamma tests to use deprecated v1 graph mode so\r\nthat test is executed in both CPU and accelerator because in eager\r\nmode, same device will be be only selected irrespective of use_gpu\r\nflag. Remove loop of use_gpu where not required.\r\nSpace to depth testBatchSize0 verification fixed with common\r\ncode.\r\n\r\nSigned-off-by: puneeshkhanna <puneesh.khanna83@gmail.com>", "comments": ["Please review !", "Also on a separate note, can someone guide me to correct forum or people with whom I can discuss about the usage of use_gpu flag in tensorflow/python/framework/test_util.py. ", "Any updates on review ? Can this be merged ?", "Any updates ?", "Regarding `use_gpu`, I recommend not using it at all (that means you modifications look good here) - the test suites run on CPU and GPU separately, and in general the same code should produce identical results, irrespective of what device it runs on.", "Regarding use_gpu, I recommend not using it at all (that means you modifications look good here) - the test suites run on CPU and GPU separately, and in general the same code should produce identical results, irrespective of what device it runs on.            ->thanks for clearing this. There are so many test cases where a loop of use_gpu of False, True is executed unnecessarily. We can avoid those loops in so many other places un less there is a requirement to compare CPU results vs GPU results. I can start by updating this random gamma test file. I will send updated patch soon. Also I think that it does make sense to have a test comparing random operators behavior between CPU and any other device because random number generator algorithm can be different.", "Hi @mdanatg - Updated files ready for review. Avoided unnecessary loop of use_gpu where not required and the current code will run all the tests on whatever device is connected. \r\nRegarding testCPUGPUMatch specific test - for now lets have @test_util.run_deprecated_v1 for now because graph mode will correctly constrain devices between CPU (when use_gpu is False) and any other device (when use_gpu is True). Moving to tf function seems to be breaking other tests. But will try to take that up in another pull request in near future.", "Btw on same lines, I will try to raise some more pull requests soon where we can avoid unnecessary loops of use_gpu of False, True. It will further speed up the execution time overall.", "Any updates ? Can this be merged now ? Locally ran all the tests on CPU too and they are passing after all the changes.", "> Sorry for the delay, this slipped through the cracks.\r\n> \r\n> I'm curious about the error you get when using tf.function - happy to help debug it down. But no need to block this PR on that. I forgot to double check, are you using e.g. `self.evaluate(diff)` instead of `diff.eval()`. The former is expected to work.\r\n\r\nYes got that but didn't want to confuse tf function with the intend of this pull request. Will surely take it up soon. Hope you saw the changes which will make the execution faster too of the tests by ensuring to not run the test twice where not required. How will this be merged now ?", "@mdanatg  - I got it what needs to be done for tf function. But didn't want to mix those changes with the intend of this pull request. Will surely take it up in near future. Hope you saw the changes to avoid execution of same test twice and test execution time will be further faster now. How will this be merged now ?", "Thanks, we just need to wait for the merge process to complete now.", "Some checks were not successful\r\n13 successful and 1 errored checks\r\n@mdanatg  - can you please help ? I am not sure what is this error.", "Looks like a tool failure, I started a retry - @gbaned might be able to help getting it unstuck.", "> Looks like a tool failure, I started a retry - @gbaned might be able to help getting it unstuck.\r\n\r\n@mdanatg, @puneeshkhanna  This PR is merged. Thank you. "]}, {"number": 50812, "title": "TensorFlow Certificate Network - region - australiasia ", "body": "https://developers.google.com/certification/directory/tensorflow\r\n\r\nselect australia for region is not available due to incorrect name shown as australiasia.\r\n\r\nselecting australiasia shows noting nether australia or Asia. \r\n\r\nI just passed Tensorflow exam and want to show my profile correct in australia.\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/24234662/126030411-4bf4f5e5-917e-46d9-bc13-9c025d357c2f.png)\r\n", "comments": ["@BrianHwang \r\nKindly open a tf discussion forum issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50811, "title": "Adding mhlo Einsum to LinalgGeneric Lowering", "body": "", "comments": []}, {"number": 50810, "title": "r2.6 cherry-pick request: Downgrade oneDNN version from v2.3 back to v2.3-rc2", "body": "Intel found that oneDNN v2.3 has a performance degradation in int8 AVX-512 convolution kernels. \r\nWe recently cherry-picked PR [#50708](https://github.com/tensorflow/tensorflow/pull/50708) into r2.6 to upgrade oneDNN from v2.3-rc2 to v2.3. This PR reverts it back to using oneDNN v2.3-rc2.\r\n\r\n(The aarch64 backend is fine since the issue is specific to AVX-512).\r\n\r\nThe corresponding rollback commit for master went in this afternoon: https://github.com/tensorflow/tensorflow/commit/797721dc79d9972214821673c98e823bfce70f62\r\n\r\ncc: @nammbash @agramesh1 \r\n", "comments": []}, {"number": 50809, "title": "I'm working in colab and  changed `tf.gfile.GFile` to `tf.io.gfile.GFile` but still get this error", "body": "@mihaimaruseac I'm working in colab and  changed `tf.gfile.GFile` to `tf.io.gfile.GFile` but still get this error\r\n`AttributeError: module 'tensorflow' has no attribute 'gfile'\r\n`\r\n\r\n_Originally posted by @mitramir55 in https://github.com/tensorflow/tensorflow/issues/31315#issuecomment-643277101_", "comments": ["I tried the way mentioned but still it is not working\r\n", "I tried https://www.tensorflow.org/io/tutorials/avro tutorial which shows [tf.io.gfile.GFile](https://www.tensorflow.org/api_docs/python/tf/io/gfile/GFile) usage example. I didn't see AttributeError raised.\r\n> schema = tf.io.gfile.GFile('train.avsc').read()\r\n\r\nPerhaps you can confirm at your end. Thanks!", "Please fill in issue template. Please make sure you are using TF 2.x. Please post full error and code sample.\r\n\r\nWithout a reproducer we cannot do anything here.", "@ishkabir1401  If you are using TF 1.x, change import tensorflow as tf to import tensorflow.compat.v1 as tf and it works.\r\n \r\nAs suggested by @mihaimaruseac, Please fill the issue template. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50809\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50809\">No</a>\n"]}, {"number": 50808, "title": "Regarding Scroll for the nearest points", "body": "When we make a  projector using the vectors and select the 100 nearest points, then for the name of points we can see in the bottom left is not scrollable to see more nearest points.\r\n\r\nThe area can be see  in the attached file.\r\n![image](https://user-images.githubusercontent.com/37951124/125999818-e7cdec52-f761-4c67-9465-13a5b2b78282.png)\r\n", "comments": ["@Srkshaikh5 \r\n\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abug&template=00-bug-issue.md) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced] .\r\n\r\nand also refer the similar [issue](https://github.com/tensorflow/tensorflow/issues/17867)\r\n\r\nIf this is only related to the scroll option post this in tensorboard [repo](https://github.com/tensorflow/tensorboard/issues).Thanks\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50808\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50808\">No</a>\n"]}, {"number": 50806, "title": "Issue running code on linux server remotely", "body": "I am doing ssh to my university sever in order to run my code which makes use of large dataset, however when i run my code i am getting bellow error after feature extraction in the code, Can someone please help as i am doing my final year project and should rectify this within 2 days\r\n\r\n2021-07-10 09:35:48.234847: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-07-10 09:35:48.238831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-07-10 09:35:49.584522: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Not found: No algorithm worked!\r\nTraceback (most recent call last):\r\nFile \"copy_of_copy_of_copy_of_untitled9.py\", line 157, in\r\ntrain_validate_features = extract_features(image_dataset_path, train_validate_images)\r\nFile \"copy_of_copy_of_copy_of_untitled9.py\", line 143, in extract_features\r\nfeature = model.predict(image, verbose=0)\r\nFile \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\", line 1629, in predict\r\ntmp_batch_outputs = self.predict_function(iterator)\r\nFile \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in call\r\nresult = self._call(*args, **kwds)\r\nFile \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 894, in _call\r\nreturn self._concrete_stateful_fn._call_flat(\r\nFile \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 1918, in _call_flat\r\nreturn self._build_call_outputs(self._inference_function.call(\r\nFile \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 555, in call\r\noutputs = execute.execute(\r\nFile \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\ntensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked!\r\n[[node model/block1_conv1/Relu (defined at copy_of_copy_of_copy_of_untitled9.py:143) ]] [Op:__inference_predict_function_610]\r\n\r\nFunction call stack:\r\npredict_function\r\n", "comments": ["@Neha10252018 It looks like your Issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. As ([previously announced](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999/)), all future development of Keras is expected to happen in the [github.com/keras-team/keras](github.com/keras-team/keras) repository. If your issue lies with the TF-Core area please comment back with your explanation and we can look into it further. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50805, "title": "Fix TF_DEVICE_MIN_SYS_MEMORY_IN_MB for big size", "body": "This fix some casting issue that show up when using: `TF_DEVICE_MIN_SYS_MEMORY_IN_MB=2048`.", "comments": []}, {"number": 50804, "title": "Saving data as SequenceExample and reading TFRecords for LSTM", "body": "Let's say I have a 3dimentional dataset with shape (sample, timesteps, features) and I want to save it with suitable tensroflow dataset format, e.g. using SequenceExample for further reading with TFRecord and using in LSTM. There is a dataset:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport numexpr\r\n\r\ndata = np.array(\r\n[\r\n    [\r\n        [1 , 10, ],\r\n        [2 , 11, ],\r\n    ],\r\n    [\r\n        [2 , 11, ],\r\n        [3 , 12, ],\r\n    ]\r\n], dtype=np.float32)\r\ny = np.array([101., 202.], dtype=np.float32)\r\n```\r\n\r\nThere is a model:\r\n\r\n```\r\ninputs= tf.keras.layers.Input(\r\nshape=(2, 2),\r\nname='input',\r\n)\r\nmodel = tf.keras.layers.LSTM(\r\n    units=data.shape[2],\r\n    return_sequences=False,\r\n    return_state=False,\r\n    name='lstm',\r\n)(inputs)\r\nmodel = tf.keras.layers.Dense(\r\n    units=1,\r\n    name='dense',\r\n)(model)\r\noutputs = model\r\nloss = tf.keras.losses.MSE\r\nmodel = tf.keras.Model(\r\n    inputs=inputs,\r\n    outputs=outputs,\r\n    name='model',\r\n)\r\nmodel.compile(\r\n    optimizer='rmsprop',\r\n    loss='mse',\r\n    metrics='mse',\r\n)\r\nmodel.summary()\r\nmodel.fit(\r\n    x=data,\r\n    y=y,\r\n    batch_size=1,\r\n)\r\n```\r\n\r\nLet's try to save and read dataset using tensorflow API:\r\n\r\n```\r\ndef _float_feature(value):\r\n  \"\"\"Returns a float_list from a float / double.\"\"\"\r\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n\r\n# writer\r\noptions = tf.io.TFRecordOptions(\r\n    compression_type='ZLIB',\r\n    flush_mode=None,\r\n    input_buffer_size=None,\r\n    output_buffer_size=None,\r\n    window_bits=None,\r\n    compression_level=0,\r\n    compression_method=None,\r\n    mem_level=None,\r\n    compression_strategy=None,\r\n)\r\nwriter = tf.io.TFRecordWriter(\r\n    path=r'test.tfrecord',\r\n    options=options,\r\n)\r\n# iterate over each row\r\nfor i in range(data.shape[0]):\r\n    # set example id\r\n    sample_dict = {\r\n        'index': tf.train.Feature(int64_list=tf.train.Int64List(value=[i]))\r\n    }\r\n    features_list = {}\r\n    # iterate over each feature\r\n    for c in range(data[0].shape[1]):\r\n        feature_values = [\r\n            _float_feature(v) for v in data[i][:, c]\r\n        ]\r\n        features_list[str(c)] = tf.train.FeatureList(feature=feature_values)\r\n    # set example\r\n    example = tf.train.SequenceExample(\r\n        context=tf.train.Features(feature=sample_dict),\r\n        feature_lists=tf.train.FeatureLists(feature_list=features_list)\r\n    )\r\n    # write\r\n    writer.write(example.SerializeToString())\r\nwriter.close()\r\n\r\n# read raw\r\ndata_raw = tf.data.TFRecordDataset(\r\n    filenames=[r'test.tfrecord'],\r\n    compression_type='ZLIB',\r\n    buffer_size=10*1024, # 10MB\r\n    num_parallel_reads=numexpr.detect_number_of_cores()-1,\r\n)\r\n# parse real\r\nschema = dict(\r\n    zip(\r\n        [str(s) for s in range(data[0].shape[1])],\r\n        [tf.io.FixedLenSequenceFeature([], dtype=tf.float32)] * data[0].shape[1]\r\n    )\r\n)\r\ndef decode_fn(record_bytes):\r\n    context, features = tf.io.parse_single_sequence_example(\r\n        serialized=record_bytes,\r\n        context_features={'index': tf.io.FixedLenFeature([], dtype=tf.int64)},\r\n        sequence_features=schema,\r\n    )\r\n    return features\r\n# read real\r\nfor r in data_raw.map(decode_fn):\r\n    print(r, '\\n')\r\n```\r\n\r\nWhen I'm trying to fit model with tensorflow dataset it's getting me an error\r\n\r\n```\r\nmodel.fit(\r\n    data_raw,\r\n    batch_size=1,\r\n)\r\nValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=0. Full shape received: []\r\n```\r\n\r\nI understand that I did not add label to tensorflow dataset but it does not matter in this case cause dataset has no incomplitable shape. Can anybody help me to understand why and where am I wrong in the code?", "comments": ["@dishkakrauch \r\n\r\nCould you please share the colab gist,it will help us analyse better.\r\n\r\nand also refer similar issues  [link1](https://github.com/keras-team/keras/issues/7403) , [link2](https://ai-pool.com/d/input-0-is-incompatible-with-layer-lstm--expected-ndim-3--found-ndim-2-in-keras), and let us know if it helps.Thanks\r\n", "@UsharaniPagadala hey, your are not related to my problem with TFRecords.\r\nHave you read all description of my question?\r\nHere is colab link - https://colab.research.google.com/drive/1uOT4Mx7oP44HRqvKok9HVWjEBvcLSJuj?usp=sharing\r\n", "@dishkakrauch \r\n\r\nIt looks like your Issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](https://github.com/keras-team/keras/issues) repository instead. As [previously announced](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) all future development of Keras is expected to happen in the keras-team/keras repository. If your issue lies with the TF-Core area please comment back with your explanation and we can look into it further. Thanks!", "@UsharaniPagadala \r\n\r\nI can't see relation between problem with TFReocrds and keras library. I've attached code and colab notebook. I hope that you read it from the scratch and understand that this issue 100% related to tensorflow python library.\r\nLet me shortly describe what am I trying to do is saveing 3dimentional time series data with TFRecords and using it with LSTM net.\r\nHope this helps.", "I've added decode function and mapped it to tfrecord dataset. This fixed everything. Thx."]}, {"number": 50803, "title": "make tf.rsqrt available!", "body": "**System information**\r\n- TensorFlow version: 2.5.0\r\n- Are you willing to contribute it: Not Sure\r\n\r\nSince TensorFlow 2.0.0, why `tf.rsqrt` is unavailable while `tf.square`, `tf.reduce_sum` and etc are not?\r\n\r\nFrom my opinion, TensorFlow should acts as a math calculating framework, so it should provide some shortcuts for `tf.math` calculation. Like PyTorch, every calculation can be carried out via top-level module.\r\n\r\nWhy you guys remove them from `tensorflow` module?\r\n\r\n```python\r\ntf.sqrt  # <function tensorflow.python.ops.math_ops.sqrt(x, name=None)>\r\n\r\ntf.rsqrt # AttributeError: module 'tensorflow' has no attribute 'rsqrt'\r\n```\r\n\r\nI think not only `tf.rsqrt`, there are also some other functions should be bring back. Or, you should provide some reasonable explanation, especially:\r\n  - Why `tf.sqrt` are available while `tf.rsqrt` are not?\r\n  - Why some math functions are removed from top-level module since 2.0.0?", "comments": ["@codingpy,\r\nThe **`Tensorflow API`** for **`rsqrt`** is available as [tf.math.rsqrt](https://www.tensorflow.org/api_docs/python/tf/math/rsqrt?hl=en). Is this what you are looking for? Thanks!", "Yes, I know. But I also wanna know that:\r\n  - Why tf.sqrt are available while tf.rsqrt are not?\r\n  - Why some math functions are removed from top-level module since 2.0.0?\r\n\r\nWhat's your judgement that some functions should be in top-level module while others shouldn't?\r\n\r\nThank you for your reply!", "@codingpy the move was for better modularization.\r\n\r\nThe `sqrt` function is also now under `tf.math`, and is not documented as being top-level anymore (although the example in the documentation does use it at the top level... that should be updated).  In fact, all the math ops were moved - we just happen to export some at the top-level still, mainly for backward compatibility for some existing models.\r\n\r\n"]}, {"number": 50802, "title": "tf.compat.v1.profiler.profile produces wrong flops on Conv2D", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Pip\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: - \r\n- GPU model and memory: -\r\n\r\n\r\nFlops calculation for a Convolution2D Layer via tf.compat.v1.profiler.profile seems to be wrong. More specifically, it seems to only calculate flops for one application of the kernel and leave out the input size.\r\n\r\nI use the following to calculate flops of a model containing only one conv layer:\r\n\r\n`\r\nfrom typing import Callable, Any\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n\r\ndef calculate_flops(model_generating_function: Callable, *args: Any, **kwargs: Any) -> int:\r\n    \"\"\"\r\n    Calculates the flops of a keras model. For implementation reasons, which include creating and tearing down the\r\n    tensorflow graph, the function does not take a model as input, but a function that returns said model.\r\n    Adapted from answer of ch271828n on\r\n    https://stackoverflow.com/questions/49525776/how-to-calculate-a-mobilenet-flops-in-keras.\r\n    :param model_generating_function: a function that returns a keras model.\r\n    :param args: positional arguments for model_generating_function\r\n    :param kwargs: keyword arguments for model_generating_function\r\n    :return: number of float point operations\r\n    \"\"\"\r\n    tf.compat.v1.reset_default_graph()\r\n    session = tf.compat.v1.Session()\r\n    graph = tf.compat.v1.get_default_graph()\r\n\r\n    with graph.as_default():\r\n        with session.as_default():\r\n            _ = model_generating_function(*args, **kwargs)\r\n\r\n            run_meta = tf.compat.v1.RunMetadata()\r\n            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n\r\n            # We use the Keras session graph in the call to the profiler.\r\n            flops = tf.compat.v1.profiler.profile(\r\n                graph=graph,\r\n                run_meta=run_meta,\r\n                cmd='op',\r\n                options=opts\r\n            )\r\n    # necessary to not clog up graph and add flops over multiple calls to this function.\r\n    tf.compat.v1.reset_default_graph()\r\n\r\n    return flops.total_float_ops\r\n\r\n\r\ndef get_model(input_size: int, n_channels: int, n_filters: int, kernel_size: int) -> keras.Sequential:\r\n\r\n    model = keras.Sequential(\r\n        [\r\n            keras.Input(shape=(input_size, input_size, n_channels)),\r\n            layers.Conv2D(n_filters, kernel_size=(kernel_size, kernel_size), activation=None, padding=\"same\", use_bias=False)\r\n        ]\r\n    )\r\n\r\n    return model\r\n\r\n\r\nassert calculate_flops(get_model, input_size=16, n_channels=1, n_filters=1, kernel_size=3) == 2 * 3 * 3 +1\r\n`\r\n\r\nwill return 19 = 2 * kernel_size * kernel_size + 1\r\n\r\n**Describe the expected behavior**\r\nI would have expected something around 16 * 16 *  actual_output\r\n", "comments": ["@SvenWarnke  It looks like your Issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. As ([previously announced](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999/)), all future development of Keras is expected to happen in the [github.com/keras-team/keras](github.com/keras-team/keras) repository. If your issue lies with the TF-Core area please comment back with your explanation and we can look into it further. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50802\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50802\">No</a>\n"]}, {"number": 50801, "title": "Adding some Boosted tree ops to the 'allowed' list", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):2.4\r\n- Are you willing to contribute it (Yes/No):Yes/will need some help\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.** \r\n\r\nCurrently, we cant convert tensorflow boosted tree model to tensorflow lite using tf.lite.TFLiteConverter.from_saved_model even after having \r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS\r\n    # enable TensorFlow ops.\r\n]\r\nI am getting this error,\r\n\r\n`ConverterError: <unknown>:0: error: loc(\"boosted_trees\"): 'tf.BoostedTreesEnsembleResourceHandleOp' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(\"boosted_trees/BoostedTreesPredict\"): 'tf.BoostedTreesPredict' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(\"boosted_trees/head/predictions/str_classes\"): 'tf.AsString' op is neither a custom op nor a flex op\r\n<unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\ttf.AsString {device = \"\", fill = \"\", precision = -1 : i64, scientific = false, shortest = false, width = -1 : i64}\r\n\ttf.BoostedTreesEnsembleResourceHandleOp {container = \"\", device = \"\", shared_name = \"boosted_trees/\"}\r\n\ttf.BoostedTreesPredict {device = \"\", logits_dimension = 7 : i64, num_bucketized_features = 18 : i64}\r\n\r\n`\r\n**Will this change the current api? How?**\r\nNot sure\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who want to use boosted tree tensorflow lite model will be benifted.\r\n\r\n**Any Other info.**\r\nThanks to @MeghnaNatraj and @abattery for responding to case #50667 .After referring this [https://www.tensorflow.org/lite/guide/op_select_allowlist#add_tensorflow_core_operators_to_the_allowed_list], i have raised this feature request to add the unsupported ops.\r\n\r\n\r\n", "comments": ["In order to do code changes i have tried to build tensorflow from source in my mac , But it gave me an error in the last step where we install the wheel file of tensorflow through pip, i have raised the following case for this #50829 .Please let me know how i can proceed.", "Hello, i was able to build from source in a linux machine(Ubuntu 18.04.5). However i am getting this errror after i add the necessary ops required in the code.\r\n\r\n`Hello from TensorFlow C library version 2.6.0-rc1\r\nThe elapsed time is 0.000060 seconds\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2021-07-28 11:52:42.296825: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nINFO: TfLiteFlexDelegate delegate: 37 nodes delegated out of 117 nodes with 4 partitions.\r\n\r\n2021-07-28 11:52:42.331907: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at quantile_ops.cc:461 : Not found: Container localhost does not exist. (Could not find resource: localhost/boosted_trees/QuantileAccumulator/)\r\nERROR: Container localhost does not exist. (Could not find resource: localhost/boosted_trees/QuantileAccumulator/)\r\n(while executing 'BoostedTreesQuantileStreamResourceGetBucketBoundaries' via Eager)\r\nERROR: Node number 117 (TfLiteFlexDelegate) failed to invoke.`\r\n\r\n\r\nAttaching the diff file here : [tf_diff.txt](https://github.com/tensorflow/tensorflow/files/6892539/tf_diff.txt)\r\n\r\nPS: I am using Tensorflow c api and the code is attached here: [boosted_tree_tflite.c](https://github.com/tensorflow/tensorflow/files/6900356/nn_tflite.txt)\r\n\r\n\r\nThank you, \r\nKoushik\r\n", "Here are a couple of suggested solutions:\r\n1. [#28287#comment](https://github.com/tensorflow/tensorflow/issues/28287#issuecomment-495005162)\r\n2. (related to 1.)[StackOverflow: Solution A](https://stackoverflow.com/questions/56513905/not-found-container-localhost-does-not-exist-when-i-load-model-with-tensorflow) and [StackOverflow: Solution B](https://stackoverflow.com/questions/54772549/container-localhost-does-not-exist-error-when-using-keras-flask-blueprints)\r\n3. [discusses possible cause](https://github.com/tensorflow/tensorflow/issues/16481)\r\n\r\nLet me know if this works.", "Hi Meghna, Thanks for the response.\r\nI don't understand how i can use the 1 and 2 points you have mentioned as i am using a C API whereas they are using Python API for inference.\r\nAbout the 3rd link, i did not understand the following comment by @mrry :\r\n\r\n>Thanks for tracking that down! I think this is a legitimate bug, introduced in 9f4118d. That change modifies most iterators to use the same Device, FunctionLibraryRuntime, and ResourceMgr as the op that created them, which enables the resource-capturing logic to be simplified, because handles are valid in both the caller and the callee.\r\n\r\nDoes it mean i have to essentially train the model in the same device and then load the model in the same device for it to work?\r\n", "I think the environment is fine, but the Boosted trees implementation may be using streaming mechanisms that isn't supported by TFLite on-device. Looks like #41226 faces a similar issue. I'll look into this further and get back to you.\r\n\r\n", "@Koushik667 could you share the reproducible steps for creating the above TensorFlow model to debug?", "Sure,\r\n\r\nI have created a sample program for the titanic dataset which takes only 2 input numerical features for simplicity.\r\nAttaching codes as .txt files as githhub is not allowing .c or .py attachments.\r\n\r\nDataset csv files:\r\n[train.csv](https://github.com/tensorflow/tensorflow/files/6928824/train.csv)\r\n[eval.csv](https://github.com/tensorflow/tensorflow/files/6928826/eval.csv)\r\n\r\nHere is the python program to create the tensorflow model which uses above csv files\r\n[BoostedTree.txt](https://github.com/tensorflow/tensorflow/files/6928771/BoostedTree.txt)\r\n\r\nHere is the converter code which converts TensorFlow model generated to TensorFlow lite :[converter_code.txt](https://github.com/tensorflow/tensorflow/files/6928786/converter_code.txt)\r\n\r\nHere is the diff file which applied after building from source : [tf_diff.txt](https://github.com/tensorflow/tensorflow/files/6928793/tf_diff.txt)\r\n\r\nHere is the C code which loads and runs the tensorflow lite model : \r\n[test.txt](https://github.com/tensorflow/tensorflow/files/6928835/test.txt)\r\n\r\nI run this code using this command:\r\n gcc -I../tensorflow/ test.c -Wl,-rpath=/home/luser/tensorflow/bazel-bin/tensorflow/lite/c/  -L/home/luser/tensorflow/bazel-bin/tensorflow/lite/c/ -ltensorflowlite_c -o test.o\r\n\r\nAlso attaching tensorflow and tensorflow lite models  in zip file :\r\n[Archive.zip](https://github.com/tensorflow/tensorflow/files/6928809/Archive.zip)\r\n\r\n\r\n", "Could you format the above script in a format of the CoLab if possible?", "Sure,\r\nHere is the link: https://colab.research.google.com/drive/1w0zWDFQMljfOSzlSEtvFtLkAINu8UxFd?usp=sharing", "@abattery Please let me know if you need anything from my side.\r\n", "Hi @abattery @MeghnaNatraj , Can you please let me know if this issue is solvable or not? or if it is a work in progress? ", "@renjie-liu  Any comments on this issue?", "Adding Karim who may have a better idea", "@karimnosseir Any updates on this issue?", "Hi @MeghnaNatraj, Can you please comment on what can be done for this issue?", "These ops were removed from TF so TFLite doesn't support it either."]}, {"number": 50800, "title": "Problem in adjust gamma function (tf.image.adjust_gamma)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Not applicable (NA)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): NA\r\n- TensorFlow version (use command below):  2.5.0\r\n- Python version: NA\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nPixels values are manipulated using this equation: Out = gain * In**gamma\r\n\r\n**Describe the expected behavior**\r\nThis function should preserve range of pixel values by first converting them between 0 and 1 and then again rescaling back to original range or atleast provide an argument for preserving range.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):  yes\r\n", "comments": ["@tejal567 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.Thanks!", "You can take any image and call tf.image.adjust_gamma on it. You will see that range of image pixel values is changed. This should not happen. Suppose a person calls this function on RGB images where pixel value of each channel is between 0 and 255. After using this function it will no longer remain between 0 and 255. We can provide an argument in this function to preserve range.", "@tejal567 ,\r\n\r\nWithout the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a minimal code snippet you are using.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50800\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50800\">No</a>\n"]}, {"number": 50799, "title": "add device_vendor to the GpuDevice in JAX runtime", "body": "With this PR, device vendor will be exposed from JAX GpuDevice into Python\r\n\r\n@hawkinsp ", "comments": ["One of our internal presubmits reminded me that you also need to update the Python type stub for pybind code:\r\nhttps://github.com/tensorflow/tensorflow/blob/0c13207eeda65754532bab5888cc33693fb06834/tensorflow/compiler/xla/python/xla_extension/__init__.pyi#L248\r\nto add `device_vendor`", "Our presubmits are still unhappy, with this error:\r\n```\r\nerror: field 'device_vendor_' will be initialized after base 'xla::PjRtStreamExecutorDevice' [-Werror,-Wreorder-ctor]\r\n```"]}, {"number": 50798, "title": "Word Embeddings not Accuracte", "body": "I am trying to build my own word2vec model using the code provided here\r\nLink: - https://www.tensorflow.org/tutorials/text/word2vec\r\n\r\nSo i have even tried to increase the data as well for training the word embedding and i am able to achieve a good model accuracy but when i plot the word vectors on the Embedding Projector the distance between words or the word similarity is really bad, if i even use the cosine distance formula between very similar words the result is bad.\r\n\r\nWhereas if the same data is used to train own embeddings using the Gensim library ( not pre-trained) the results of distance and similarity are way better, even on the Embedding Projector as well.\r\n\r\nPlease can someone help me regarding this, i want to use the Word2Vec code only which is provided by TensorFlow but i am not able to get good results for word distance and word similarity.\r\n", "comments": ["duplicate #https://github.com/tensorflow/tensorflow/issues/50645"]}, {"number": 50795, "title": "Installed successfully the TensorFlow-gpu==1.15 but got error tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible resource devices: /device:CPU:0 vs /device:GPU:0. The edge src node is input_producer , and the dst node is ReaderReadV2", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS BigSur\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: tensorflow 2.5.0\r\n- Python version: 3.9.6\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: TensorFlow-gpu==1.15 \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nEnvironment and Configuration needed to run the evaluation code.\r\nPython 3.6.4 --> version 3.9.6\r\nTensorflow-gpu 1.8.0 (pip install tensorflow-gpu==1.8.0) --> not available instead installed the TensorFlow-gpu==1.15.\r\nTensorpack (pip install tensorpack) version - \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nExecuted code :\r\n_user@Olens-MacBook-Air SELFIE % python3 -m pip3 install tensorflow-gpu==1.15\r\n/Library/Frameworks/Python.framework/Versions/3.9/bin/python3: No module named pip3\r\nuser@Olens-MacBook-Air SELFIE % python3 -m pip3 install tensorflow-gpu==1.8.0\r\n/Library/Frameworks/Python.framework/Versions/3.9/bin/python3: No module named pip3\r\nuser@Olens-MacBook-Air SELFIE % python3 main.py  0  CIFAR-10  DenseNet-10-12  SELFIE  pair  0.05  /Users/user/Desktop/Summer\\ 2021/log_\r\n['main.py', '0', 'CIFAR-10', 'DenseNet-10-12', 'SELFIE', 'pair', '0.05', '/Users/olenbaduria/Desktop/Summer 2021/log']\r\n------------------------------------------------------------------------\r\nThis code trains Densnet(L={10,25,40}, k=12) using SELFIE in tensorflow-gpu environment.\r\n\r\nDescription -----------------------------------------------------------\r\nPlease download datasets from our github before running command.\r\nFor SELFIE, the hyperparameter was set to be uncertainty threshold = 0.05 and  history length=15.\r\nFor Training, we follow the same configuration in our paper\r\nFor Training, training_epoch = 100, batch = 128, initial_learning rate = 0.1 (decayed 50% and 75% of total number of epochs), use momentum of 0.9, warm_up=25, restart=2, ...\r\nYou can easily change the value in main.py\r\nDataset exists in  /Users/user/Desktop/Summer 2021/SELFIE/SELFIE/dataset/CIFAR-10\r\n2021-07-15 17:50:10.016830: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nNow read following files.\r\n['/Users/user/Desktop/Summer 2021/SELFIE/SELFIE/dataset/CIFAR-10/data_batch_1.bin', '/Users/user/Desktop/Summer 2021/SELFIE/SELFIE/dataset/CIFAR-10/data_batch_2.bin', '/Users/user/Desktop/Summer 2021/SELFIE/SELFIE/dataset/CIFAR-10/data_batch_3.bin', '/Users/user/Desktop/Summer 2021/SELFIE/SELFIE/dataset/CIFAR-10/data_batch_4.bin', '/Users/user/Desktop/Summer 2021/SELFIE/SELFIE/dataset/CIFAR-10/data_batch_5.bin']\r\nFilling queue with 20000 data before starting to train. This will take a few minutes.\r\nNow read following files.\r\n['/Users/user/Desktop/Summer 2021/SELFIE/SELFIE/dataset/CIFAR-10/test_batch.bin']\r\nFilling queue with 4000 data before starting to train. This will take a few minutes.\r\n/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n  warnings.warn('`layer.apply` is deprecated and '\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/conv0': [?, 32, 32, 3] --> [?, 32, 32, 16]\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/block1/dense_layer.0/conv1': [?, 32, 32, 16] --> [?, 32, 32, 12]\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/block1/dense_layer.1/conv1': [?, 32, 32, 28] --> [?, 32, 32, 12]\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/block1/transition1/conv1': [?, 32, 32, 40] --> [?, 32, 32, 40]\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/block1/transition1/pool': [?, 32, 32, 40] --> [?, 16, 16, 40]\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/block2/dense_layer.0/conv1': [?, 16, 16, 40] --> [?, 16, 16, 12]\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/block2/dense_layer.1/conv1': [?, 16, 16, 52] --> [?, 16, 16, 12]\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/block2/transition2/conv1': [?, 16, 16, 64] --> [?, 16, 16, 64]\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/block2/transition2/pool': [?, 16, 16, 64] --> [?, 8, 8, 64]\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/block3/dense_layer.0/conv1': [?, 8, 8, 64] --> [?, 8, 8, 12]\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/block3/dense_layer.1/conv1': [?, 8, 8, 76] --> [?, 8, 8, 12]\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/gap': [?, 8, 8, 88] --> [?, 88]\r\n[0715 17:50:10 @registry.py:90] 'DenseNet/linear': [?, 88] --> [?, 10]\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1358, in _run_fn\r\n    self._extend_graph()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1398, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible resource devices: /device:CPU:0 vs /device:GPU:0. The edge src node is input_producer , and the dst node is ReaderReadV2\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/user/Desktop/Summer 2021/SELFIE/SELFIE/main.py\", line 108, in <module>\r\n    main()\r\n  File \"/Users/user/Desktop/Summer 2021/SELFIE/SELFIE/main.py\", line 104, in main\r\n    selfie(gpu_id, input_reader, model_name, total_epochs, batch_size, lr_boundaries, lr_values, optimizer, noise_rate, noise_type, warm_up, threshold, queue_size, restart=restart, log_dir=log_dir)\r\n  File \"/Users/user/Desktop/Summer 2021/SELFIE/SELFIE/algorithm/selfie.py\", line 167, in selfie\r\n    train_batch_patcher.bulk_load_in_memory(sess, train_ids, train_images, train_labels)\r\n  File \"/Users/user/Desktop/Summer 2021/SELFIE/SELFIE/reader/batch_patcher.py\", line 40, in bulk_load_in_memory\r\n    mini_ids, mini_images, mini_labels = sess.run([ids, images, labels])\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 967, in run\r\n    result = self._run(None, fetches, feed_dict, options_ptr,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1190, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1368, in _do_run\r\n    return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1394, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: **_Cannot place the graph because a reference or resource edge connects colocation groups with incompatible resource devices: /device:CPU:0 vs /device:GPU:0. The edge src node is input_producer , and the dst node is ReaderReadV2_**\r\nuser@Olens-MacBook-Air SELFIE % python3 --version\r\nPython 3.9.6\r\n", "comments": ["@jocelynbaduria \r\nPlease refer to [this issue](https://github.com/tensorflow/tensorflow/issues/30748) with similar error, and let us know.\r\nAs there is no active support for tf 1.x, please upgrade your version and let us know.\r\nplease follow: https://www.tensorflow.org/install/source_windows#gpu", "@Saduf2019 I upgraded the version to tensorflow==2.5.0 still got an error.\r\n\r\nsee error below, running the code main.py in Colab.\r\n\r\n2021-07-18 18:22:40.196041: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n['/content/drive/Shareddrives/Eranti-Vijay-Su21-2/code/Updated_SELFIE/SELFIE/SELFIE/main.py', '0', 'CIFAR-100', 'DenseNet-25-12', 'SELFIE', 'pair', '0.4', 'log/CIFAR-100/SELFIE']\r\n------------------------------------------------------------------------\r\nThis code trains Densnet(L={10,25,40}, k=12) using SELFIE in tensorflow-gpu environment.\r\n\r\nDescription -----------------------------------------------------------\r\nPlease download datasets from our github before running command.\r\nFor SELFIE, the hyperparameter was set to be uncertainty threshold = 0.05 and  history length=15.\r\nFor Training, we follow the same configuration in our paper\r\nFor Training, training_epoch = 100, batch = 128, initial_learning rate = 0.1 (decayed 50% and 75% of total number of epochs), use momentum of 0.9, warm_up=25, restart=2, ...\r\nYou can easily change the value in main.py\r\nDataset exists in  /content/drive/Shareddrives/Eranti-Vijay-Su21-2/code/Updated_SELFIE/SELFIE/SELFIE/dataset/CIFAR-100\r\n2021-07-18 18:22:41.999102: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-07-18 18:22:42.000147: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\n2021-07-18 18:22:42.026919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-18 18:22:42.027498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2021-07-18 18:22:42.027562: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-18 18:22:42.037067: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n2021-07-18 18:22:42.037147: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\n2021-07-18 18:22:42.038817: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\r\n2021-07-18 18:22:42.039187: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\r\n2021-07-18 18:22:42.039639: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\r\n2021-07-18 18:22:42.040275: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\r\n2021-07-18 18:22:42.040462: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n2021-07-18 18:22:42.040489: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2021-07-18 18:22:42.239995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-07-18 18:22:42.240038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \r\n2021-07-18 18:22:42.240054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \r\nNow read following files.\r\n['/content/drive/Shareddrives/Eranti-Vijay-Su21-2/code/Updated_SELFIE/SELFIE/SELFIE/dataset/CIFAR-100/data_batch_1.bin']\r\nFilling queue with 20000 data before starting to train. This will take a few minutes.\r\nNow read following files.\r\n['/content/drive/Shareddrives/Eranti-Vijay-Su21-2/code/Updated_SELFIE/SELFIE/SELFIE/dataset/CIFAR-100/test_batch.bin']\r\nFilling queue with 4000 data before starting to train. This will take a few minutes.\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n  warnings.warn('`layer.apply` is deprecated and '\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/conv0': [?, 32, 32, 3] --> [?, 32, 32, 16]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block1/dense_layer.0/conv1': [?, 32, 32, 16] --> [?, 32, 32, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block1/dense_layer.1/conv1': [?, 32, 32, 28] --> [?, 32, 32, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block1/dense_layer.2/conv1': [?, 32, 32, 40] --> [?, 32, 32, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block1/dense_layer.3/conv1': [?, 32, 32, 52] --> [?, 32, 32, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block1/dense_layer.4/conv1': [?, 32, 32, 64] --> [?, 32, 32, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block1/dense_layer.5/conv1': [?, 32, 32, 76] --> [?, 32, 32, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block1/dense_layer.6/conv1': [?, 32, 32, 88] --> [?, 32, 32, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block1/transition1/conv1': [?, 32, 32, 100] --> [?, 32, 32, 100]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block1/transition1/pool': [?, 32, 32, 100] --> [?, 16, 16, 100]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block2/dense_layer.0/conv1': [?, 16, 16, 100] --> [?, 16, 16, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block2/dense_layer.1/conv1': [?, 16, 16, 112] --> [?, 16, 16, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block2/dense_layer.2/conv1': [?, 16, 16, 124] --> [?, 16, 16, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block2/dense_layer.3/conv1': [?, 16, 16, 136] --> [?, 16, 16, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block2/dense_layer.4/conv1': [?, 16, 16, 148] --> [?, 16, 16, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block2/dense_layer.5/conv1': [?, 16, 16, 160] --> [?, 16, 16, 12]\r\n[0718 18:22:42 @registry.py:90] 'DenseNet/block2/dense_layer.6/conv1': [?, 16, 16, 172] --> [?, 16, 16, 12]\r\n[0718 18:22:43 @registry.py:90] 'DenseNet/block2/transition2/conv1': [?, 16, 16, 184] --> [?, 16, 16, 184]\r\n[0718 18:22:43 @registry.py:90] 'DenseNet/block2/transition2/pool': [?, 16, 16, 184] --> [?, 8, 8, 184]\r\n[0718 18:22:43 @registry.py:90] 'DenseNet/block3/dense_layer.0/conv1': [?, 8, 8, 184] --> [?, 8, 8, 12]\r\n[0718 18:22:43 @registry.py:90] 'DenseNet/block3/dense_layer.1/conv1': [?, 8, 8, 196] --> [?, 8, 8, 12]\r\n[0718 18:22:43 @registry.py:90] 'DenseNet/block3/dense_layer.2/conv1': [?, 8, 8, 208] --> [?, 8, 8, 12]\r\n[0718 18:22:43 @registry.py:90] 'DenseNet/block3/dense_layer.3/conv1': [?, 8, 8, 220] --> [?, 8, 8, 12]\r\n[0718 18:22:43 @registry.py:90] 'DenseNet/block3/dense_layer.4/conv1': [?, 8, 8, 232] --> [?, 8, 8, 12]\r\n[0718 18:22:43 @registry.py:90] 'DenseNet/block3/dense_layer.5/conv1': [?, 8, 8, 244] --> [?, 8, 8, 12]\r\n[0718 18:22:43 @registry.py:90] 'DenseNet/block3/dense_layer.6/conv1': [?, 8, 8, 256] --> [?, 8, 8, 12]\r\n[0718 18:22:43 @registry.py:90] 'DenseNet/gap': [?, 8, 8, 268] --> [?, 268]\r\n[0718 18:22:43 @registry.py:90] 'DenseNet/linear': [?, 268] --> [?, 100]\r\n2021-07-18 18:22:43.507170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-18 18:22:43.507770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2021-07-18 18:22:43.507799: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1358, in _run_fn\r\n    self._extend_graph()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1398, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible resource devices: /device:CPU:0 vs /device:GPU:0. The edge src node is input_producer , and the dst node is ReaderReadV2\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/content/drive/Shareddrives/Eranti-Vijay-Su21-2/code/Updated_SELFIE/SELFIE/SELFIE/main.py\", line 108, in <module>\r\n    main()\r\n  File \"/content/drive/Shareddrives/Eranti-Vijay-Su21-2/code/Updated_SELFIE/SELFIE/SELFIE/main.py\", line 104, in main\r\n    selfie(gpu_id, input_reader, model_name, total_epochs, batch_size, lr_boundaries, lr_values, optimizer, noise_rate, noise_type, warm_up, threshold, queue_size, restart=restart, log_dir=log_dir)\r\n  File \"/content/drive/Shareddrives/Eranti-Vijay-Su21-2/code/Updated_SELFIE/SELFIE/SELFIE/algorithm/selfie.py\", line 170, in selfie\r\n    train_batch_patcher.bulk_load_in_memory(sess, train_ids, train_images, train_labels)\r\n  File \"/content/drive/Shareddrives/Eranti-Vijay-Su21-2/code/Updated_SELFIE/SELFIE/SELFIE/reader/batch_patcher.py\", line 40, in bulk_load_in_memory\r\n    mini_ids, mini_images, mini_labels = sess.run([ids, images, labels])\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 968, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1191, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1369, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1394, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible resource devices: /device:CPU:0 vs /device:GPU:0. The edge src node is input_producer , and the dst node is ReaderReadV2\r\n", "@jocelynbaduria \r\nCan you please share the code your running, or try it on colab and share a gist with us.", "HI @Saduf2019 \r\n\r\nIt worked now using the version 2.0.0.\r\n\r\n#!pip install tensorflow-gpu==2.5.0 - does not work.\r\n!pip install tensorflow-gpu==2.0.0\r\n\r\nThanks"]}, {"number": 50794, "title": "[tf.data] Fix a performance regression for input pipelines that creat\u2026", "body": "\u2026e iterators for window datasets and add missing initialization of datasets that are not created through op kernel execution.\r\n\r\n**Reasons to cherry-pick this into branch 2.6**\r\n1) This commit fixes a performance regression\r\n2) Without this commit, all users of [group_by_window](https://www.tensorflow.org/api_docs/python/tf/data/experimental/group_by_window) will see error logs along the lines of `Failed precondition: Cannot compute input sources for dataset of type PaddedBatchDatasetV2, because sources could not be computed for input dataset of type Window`. This issue was reported by @guillaumekln  in https://github.com/tensorflow/tensorflow/issues/50693\r\n\r\n@jsimsa FYI\r\n\r\nPiperOrigin-RevId: 382427031\r\nChange-Id: I5ced4692659101ad09dc65a3fc71c036ad272f7a", "comments": []}, {"number": 50793, "title": "Serialize tf.keras Preprocessing layers", "body": "I created some keras preprocessing layers using the following:\r\nThis example is for categorical columns but I could have done this for any type of preprocessing as shown in [Module: tf.keras.layers.experimental.preprocessing](Module: tf.keras.layers.experimental.preprocessing)\r\n```\r\ncategorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\r\n\r\nindex = preprocessing.StringLookup(max_tokens=max_tokens)\r\nindex.adapt(feature_ds)\r\nencoder = preprocessing.CategoryEncoding(num_tokens=index.vocabulary_size())\r\nencoding_layer = lambda feature: encoder(index(feature))\r\n```\r\nNow if I want to serialize this `encoding_layer` or the `categorical_col` there is no way to do this.\r\n\r\nI tried pickle dump and got the following error:\r\n`pickle.dumps(encoding_layer)`\r\n```\r\n--------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-15-da3341eebce4> in <module>\r\n      1 import pickle\r\n----> 2 pickle.dumps(tfp.encoded_features[0])\r\n\r\nAttributeError: Can't pickle local object 'PreprocessingLayer.make_adapt_function.<locals>.adapt_step'\r\n```\r\nI also tried to use the tf.keras serializer and I got the following error:\r\n`tf.keras.layers.serialize(encoding_layer)`\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-16-c9875a7f3e69> in <module>\r\n----> 1 tf.keras.layers.serialize(tfp.encoded_features[0])\r\n\r\n~/Dev-Stage/armet/env/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py in serialize(layer)\r\n    141 @keras_export('keras.layers.serialize')\r\n    142 def serialize(layer):\r\n--> 143   return generic_utils.serialize_keras_object(layer)\r\n    144 \r\n    145 \r\n\r\n~/Dev-Stage/armet/env/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in serialize_keras_object(instance)\r\n    528   if hasattr(instance, '__name__'):\r\n    529     return get_registered_name(instance)\r\n--> 530   raise ValueError('Cannot serialize', instance)\r\n    531 \r\n    532 \r\n\r\nValueError: ('Cannot serialize', <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'normalization')>)\r\n```\r\nIs there a way to serialize the keras preprocessing layers? I am not sure why this serialization is not working.", "comments": ["@sidhartha-roy \r\n\r\nwe see the issue [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abug&template=00-bug-issue.md) has not been filled please do so,  and provide the colab gist, it will helps us to analyse better, \r\n\r\nalso refer [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/generic_utils.py#L515), and let us know if it helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50793\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50793\">No</a>\n"]}, {"number": 50792, "title": "Add missing dep on MLIRMhloUtils", "body": "The LmhloPasses makes use of `mlir::codegen_utils` functions, part of\r\nthe MLIRMhloUtils target. This adds the missing dep on MLIRMhloUtils.", "comments": []}, {"number": 50791, "title": "tf.keras.models.clone_model ignores input_tensors argument ", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution. Linux Ubuntu 20.04:\r\n- TensorFlow installed from binary:\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: NVIDIA Quadro RTX 8000\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.keras.models.clone_model` ignores `input_tensors` argument and reuses the inputs of the original model for functional models. It works for sequential models.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nIf I provide `input_tensors`, it should use it.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n\r\nI think that the error is in the following [line](https://github.com/tensorflow/tensorflow/blob/383f3a58a682aaf8e460170ccd76f216db375f9f/tensorflow/python/keras/models.py#L187)\r\n\r\nI would replace this line:\r\n`new_input_layers[original_input_layer] = original_input_layer`\r\nby this line:\r\n`new_input_layers[original_input_layer] = input_tensor._keras_history.layer`\r\n\r\nThis would be similar to the [line in sequential models](https://github.com/tensorflow/tensorflow/blob/383f3a58a682aaf8e460170ccd76f216db375f9f/tensorflow/python/keras/models.py#L344)\r\n\r\nIt works for me, but I'm not 100% sure if that is the intended behavior.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Colab](https://colab.research.google.com/drive/1ljevVbdSon7Vkz21lA9Bd0-dYw1tkuES?usp=sharing)\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\ninputs_small = layers.Input((64, 64, 3), name=\"small\")\r\noutputs = layers.Conv2D(32, 1)(inputs_small)\r\nmodel_small = tf.keras.models.Model(inputs=inputs_small, outputs=outputs)\r\n\r\ninputs_large = layers.Input((128, 128, 3), name=\"large\")\r\nmodel_large = tf.keras.models.clone_model(model_small, input_tensors=inputs_large)\r\nmodel_large.summary()\r\n```\r\nThis results into:\r\n```\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nsmall (InputLayer)           [(None, 64, 64, 3)]       0         \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 64, 64, 32)        128       \r\n=================================================================\r\nTotal params: 128\r\nTrainable params: 128\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\nI like to see this behavior:\r\n```\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nlarge (InputLayer)           [(None, 128, 128, 3)]     0         \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 128, 128, 32)      128       \r\n=================================================================\r\nTotal params: 128\r\nTrainable params: 128\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\nThe same happens if the new input tensor just has a different name than the original one", "comments": ["@LeonhardFeiner ,\r\n\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n", "I created an issue there: https://github.com/keras-team/keras/issues/14937", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50791\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50791\">No</a>\n"]}, {"number": 50789, "title": "Import error when multiple versions of tensorflow are in python path", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)\r\n- TensorFlow installed from (source or binary): via pip\r\n- TensorFlow version (use command below): 2.4.1 and 2.5.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: Environment without GPU\r\n- GPU model and memory: Environment without GPU\r\n\r\n**Initial environment**\r\nI have empty virtual environment without system libraries, created via command `virtualenv --system-site-packages`. Outside of the environment I have installed a set of common data-science libraries , including `tensorflow==2.4.1`. My python path looks like this: \r\n```\r\n['/usr/local/lib/python37.zip',\r\n '/usr/local/lib/python3.7',\r\n '/usr/local/lib/python3.7/lib-dynload',\r\n '/root/venv/lib/python3.7/site-packages',\r\n '/usr/local/lib/python3.7/site-packages',\r\n '/shared-libs/python3.7/py/lib/python3.7/site-packages',\r\n '/deepnote-config/ipython',\r\n '/root/work']\r\n```\r\n\r\nwhere `/shared-libs/python3.7/py/lib/python3.7/site-packages` contains tensorflow==2.4.1.\r\n\r\n**Describe the current behavior**\r\nWhen I import tensorflow in this environment, everything works ok. Problem appears when I install newer `tensorflow` to my env via command `pip install tensorflow==2.5.0`. After succesfull installation I get this error while importing tensorflow:\r\n`NotFoundError: /shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow6thread10ThreadPool26TransformRangeConcurrentlyExxRKSt8functionIFvxxEE `. \r\n\r\nWhy is tensorflow trying  to load `libtfkernel_sobol_op.so` module form last path in python path `/shared-libs/python3.7/py`. Just to clarify, `cat /root/venv/lib/python3.7/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so` file exists.\r\n\r\nWhen I remove  '/shared-libs/python3.7/py/lib/python3.7/site-packages' (which contains tf 2.4.1) from python path, the problem seems to disappear. Therefore it seems like presence of different version of tf later in the path causes the issue, which is unexpected. \r\n\r\n**Describe the expected behavior**\r\nI expect that python will search tensorflow according to the order of the python path. In other words, I expect that tensorflow 2.5.0 will be imported from  '/root/venv/lib/python3.7/site-packages' and tensorflow outside of my venv will be ignored '/shared-libs/python3.7/py/lib/python3.7/site-packages'. \r\n\r\n**Other info / logs** \r\nI found this similar error https://github.com/tensorflow/tensorflow/issues/42978 . I tried to run logic for for sorting site_packages dirs:\r\n\r\n```python\r\nimport six as _six\r\nimport sys as _sys\r\nimport site as _site\r\n\r\n# Get sitepackages directories for the python installation.\r\n_site_packages_dirs = []\r\nif _site.ENABLE_USER_SITE and _site.USER_SITE is not None:\r\n  _site_packages_dirs += [_site.USER_SITE]\r\n  _site_packages_dirs += [_p for _p in _sys.path if 'site-packages' in _p]\r\n  print(_site_packages_dirs)\r\n```\r\nOutput:\r\n```\r\n[ '/root/venv/lib/python3.7/site-packages', '/usr/local/lib/python3.7/site-packages', '/shared-libs/python3.7/py/lib/python3.7/site-packages']\r\n```\r\nHappy to provide any assistance in debugging and I am also happy to provide a fix if anyone can point me in the right direction. ", "comments": ["@chudyandrej \r\n\r\nCould you please refer this [comment](https://stackoverflow.com/questions/65405705/undefined-symbol-zn10tensorflow8opkernel11tracestringepns-15opkernelcontexteb/65467331), and also refer the [documentation,](https://www.tensorflow.org/install/pip#virtual-environment-install) and let us know if it helps.Thanks", "Hi @UsharaniPagadala, thanks for your comment. I tried to update TF outside of my virtual environment to the latest version 2.5.0. Now when i install any other version of TF (i tried **2.4.1**, **2.4.2**, **2.6.0rc0**) in a virtual environment I getting the same error.\r\n\r\n```\r\nNotFoundError: /shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeEN4absl14lts_2020_09_2311string_viewEOSt6vectorINS_10StackFrameESaIS7_EE\r\n```\r\n\r\nI was using this setup for quite a long time. For example when TF outside of venv is **2.0.1** and the installed version of TF inside of venv is  **2.0.0** everything works just fine. This problem appeared with version **2.5.0**. \r\nThanks for your time. ", "@chudyandrej \r\nCan you please remove all the installations and start from scratch so that way the correct files can be in correct path.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50789\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50789\">No</a>\n"]}, {"number": 50788, "title": "ValueError: destinations can not be empty", "body": "I'm having this bug when I try to: get features at the layer before last layer, compute loss with expected features using cosine similarity, then propagate back to input (I update input, not network's weight) with optimizer.minimine(loss,var_list = input). I cannot find any information about this bug. Please give me advice", "comments": ["@Yurushia1998 \r\n\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abug&template=00-bug-issue.md) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks\r\n", "> @Yurushia1998\r\n> \r\n> We see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abug&template=00-bug-issue.md) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks\r\n\r\nTF Version: 2.3.0, but I use disable_v2_behaviour\r\nOS Ubuntu 20.04, PC\r\nPython 3.6.13\r\ngcc  10.2.0\r\nCUDA 10, cudnn 7.6.5\r\nGPU memory 11019MB\r\n\r\n A simplified version of code would be:\r\n\r\nclass Model\r\n  def init(some_network):\r\n    self.probe_images_weight = tf.get_variable(\"probe_images_weight\",  initializer=tf.ones([100,32,32,3]), trainable = True)\r\n    self.net = some_network\r\n    self.optimizer_probe = tf.train.GradientDescentOptimizer(\r\n          learning_rate=0.005)\r\n  def update_probe_operator(self):\r\n    \r\n      def step_update_probe():\r\n        \"\"\"Step functon.\r\n\r\n        Args:\r\n          inputs: inputs from data iterator\r\n\r\n        Returns:\r\n          a set of variables want to observe in Tensorboard\r\n        \"\"\"\r\n        net = self.net\r\n        logits,features = net(self.probe_images_weight, name='model', reuse=tf.AUTO_REUSE, training=True)\r\n        probe_loss = tf.keras.losses.cosine_similarity(self.expect_features, features)\r\n        probe_loss = tf.reduce_mean(probe_loss)\r\n        minimizer_op = self.optimizer_probe.minimize(probe_loss, var_list= [self.probe_images_weight])\r\n        with tf.control_dependencies([minimizer_op]):      #Error appear here when I update the input \r\n              return tf.identity(probe_loss)\r\n      # end of parallel\r\n      probe_loss = self.strategy.run(\r\n           step_update_probe)\r\n  \r\n      probe_loss = self.strategy.unwrap(probe_loss)    \r\n      return [probe_loss]", "@Yurushia1998 \r\n\r\n I am facing indentation errors.Could you please share the colab gist with clear details.Thanks", "> @Yurushia1998\r\n> \r\n> I am facing indentation errors.Could you please share the colab gist with clear details.Thanks\r\n\r\nHi, can you acces this link: https://drive.google.com/drive/folders/1dpBhVutc15sputAIlFymLdGZ8FTqZP0y?usp=sharing\r\nTo run the code, please install\r\n!pip install --upgrade tensorflow-gpu\r\n!pip install torch torchvision\r\n!pip install tqdm numpy absl-py sklearn gast tensorflow_datasets tensorflow_addons Pillow \r\n!pip install git+https://github.com/qubvel/classification_models.git\r\n!pip install scikit-learn-extra keras\r\n\r\non a virtual environment, then run by command:\r\nbash ieg/run_cifar10_diversity_and_balance_only_clean_balance_mix_cosine_real_features_0.006_resnet29.sh\r\n", "@Yurushia1998 \r\n\r\n Could you please provide the colab gist rather than a script,it will help me to analyse the issue better.Thanks", "> @Yurushia1998\r\n> \r\n> Could you please provide the colab gist rather than a script,it will help me to analyse the issue better.Thanks\r\n\r\nIs this what you want: https://gist.github.com/Yurushia1998/404fc4bd55909768d1d90f2ffeb54c1a? Please let me know if you want something else", "@rmothukuru \r\n\r\nI was able to  replicate the issue reported here.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/c2b9c6fc826c90737e1e9a5125ace181/ieg.ipynb#scrollTo=E3n3TIi_Fo93). Thanks", "This question is better asked on [TensorFlow Forum](https://discuss.tensorflow.org/) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50788\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50788\">No</a>\n"]}, {"number": 50787, "title": "Library using libtensorflow crashes with the TensorFlow packege in Python", "body": "First, thanks for attending this issue. As I couldn't find a precise category for this problem, I considered that this one was the better suited. Please let me know if this is not appropriate and I should direct my question somewhere else. \r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 20.04.2 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.8/3.9\r\n- Installed using virtualenv? pip? conda?: pip and the C API\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\n\r\n**Describe the problem**\r\n[Essentia](https://essentia.upf.edu/) is a C++ music and sound analysis library with Python bindings that uses the C TensorFlow API for machine learning inference. When Essentia is linked against [libtensorflow](https://www.tensorflow.org/install/lang_c), and we import both  `essentia` and `tensorflow`  in a Python session, a segmentation fault happens. However, they can work perfectly in separate sessions. We are opening this issue as we believe that a similar problem can happen to other projects with a Python interface relying on `libtensorflow`.\r\n\r\nAs pointed out in https://github.com/tensorflow/tensorflow/issues/22810#issuecomment-433225466, using the same TensorFlow library should fix the problem. However, even if we `pip install tensorflow==2.5.0` and we link Essentia against `libtensorflow 2.5.0` the problem persists. Maybe this is because the way TensorFlow is shipped inside the Python wheel is not through `libtensorflow` so it is not recognized as the same library?\r\n\r\nWe have found two ways to overcome this issue:\r\n- **With a static build of TensorFlow**. However, this method was only achieved with the [contrib Makefile](https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/contrib/makefile/Makefile) for TensorFlow 1.15. The main problem is that this approach requires substantial patching to the Makefile, and each time it is more difficult to keep track with the updates of the library. Also, this solution makes the final build larger, and we would prefer to avoid it. \r\n- **Linking against `libtensorflow_framework` inside the Python wheel**. While this approach was working with Tensorflow 1.X, it is not working for Tensorflow 2.X. Also, while this approach was working (and it is our current way to create [wheels](https://pypi.org/project/essentia/)) it feels a bit hacky, and we would prefer to use `libtensorflow`. \r\n\r\nOur question is, is there a way in which we could link our library against `libtensorflow` without crashing with the official TensorFlow library in Python?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nWe have Python [wheels](https://drive.google.com/drive/folders/1Ipl48pcZAvdX-ogYk0cx2gwwkDE58rbX?usp=sharing) linked against `libtensorflow` that can be installed offline. After this:\r\n```\r\nimport tensorflow\r\nimport essentia.standard\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nThe lines above produce:\r\n```\r\n2021-07-15 10:15:35.710295: F tensorflow/core/framework/variant_op_registry.cc:66] Check failed: existing == nullptr (0x563975228328 vs. nullptr)Unary VariantDecodeFn for type_name: tensorflow::data::WrappedDatasetVariant already registered\r\nAborted (core dumped)\r\n```\r\n", "comments": ["We solved this issue by making our own TensorFlow build with Bazel with the `monolithic` option.\r\nWe found that this way `libtensorflow` is only optionally dependent on `libtensorflow_framework`.\r\nBy patching the pkg-config file generated by TensorFlow to skip `libtensorflow_framework`  our Python bindings do not clash with TensorFlow anymore:\r\n\r\n```bash\r\nsed -i 's/ -ltensorflow_framework//' tensorflow.pc\r\n```\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50787\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50787\">No</a>\n"]}, {"number": 50785, "title": "About pb model quantification", "body": "Hello, I have a `float32 pb` model, and I want to convert it to an `int8 pb` model, how should I operate it?", "comments": ["@Zengyf-CVer \r\n\r\nCould you please let us know are you Converting from pb to tflite. If yes Please refer the [documentation](https://www.tensorflow.org/lite/performance/post_training_quantization) and also look at this [article](https://medium.com/axinc-ai/convert-keras-models-to-tensorflow-lite-e654994fb93c) ,hope it helps.Thanks\r\n", "@UsharaniPagadala \r\nInstead of converting to tflite, but converting float32 type pb model to int8 type pb model, in other words, `(float32)pb=>(int8)pb`", "@Zengyf-CVer \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) / TF [Forum](https://discuss.tensorflow.org/) . There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. \r\nand move this to closed status.Thanks\r\n", "@UsharaniPagadala \r\nThank you very much for your reply."]}, {"number": 50784, "title": "NotImplementedError: Cannot convert a symbolic Tensor (lstm_20/strided_slice:0) to a numpy array...", "body": "My code:\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(LSTM(50, return_sequences=True, input_shape=(50, 1)))\r\n\r\nmodel.add(LSTM(64, return_sequences=False))\r\n\r\nmodel.add(Dense(1, activation='linear'))\r\n\r\nmodel.compile(loss='mse', optimizer='rmsprop')\r\n\r\nmodel.summary()\r\n\r\n-------------------------------------------------------------------------\r\n\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-111-b51fee4336b3> in <module>\r\n      1 model = Sequential()\r\n      2 \r\n----> 3 model.add(LSTM(50, return_sequences=True, input_shape=(50, 1)))\r\n      4 \r\n      5 model.add(LSTM(64, return_sequences=False))\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    455         self._checkpoint.restore_saveables(tensor_saveables, python_saveables))\r\n    456     return restore_ops\r\n--> 457 \r\n    458   @property\r\n    459   def checkpoint(self):\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py in add(self, layer)\r\n    204               batch_shape=batch_shape, dtype=dtype, name=layer.name + '_input')\r\n    205           # This will build the current layer\r\n--> 206           # and create the node connecting the current layer\r\n    207           # to the input layer we just created.\r\n    208           layer(x)\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)\r\n    661 \r\n    662     # If any of `initial_state` or `constants` are specified and are Keras\r\n--> 663     # tensors, then add them to the inputs and temporarily modify the\r\n    664     # input_spec to include them.\r\n    665 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\r\n    923       ValueError: if the layer's `call` method returns None (an invalid value).\r\n    924       RuntimeError: if `super().__init__()` was not called in the constructor.\r\n--> 925     \"\"\"\r\n    926     if not hasattr(self, '_thread_local'):\r\n    927       raise RuntimeError(\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n   1115         layer=self, inputs=inputs, build_graph=True, training=training_value):\r\n   1116       # Symbolic execution on symbolic tensors. We will attempt to build\r\n-> 1117       # the corresponding TF subgraph inside `backend.get_graph()`\r\n   1118       # TODO(reedwm): We should assert input compatibility after the inputs\r\n   1119       # are casted, not before.\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py in call(self, inputs, mask, training, initial_state)\r\n   1106         recurrent_activation=recurrent_activation,\r\n   1107         use_bias=use_bias,\r\n-> 1108         kernel_initializer=kernel_initializer,\r\n   1109         recurrent_initializer=recurrent_initializer,\r\n   1110         bias_initializer=bias_initializer,\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py in _process_inputs(self, inputs, initial_state, constants)\r\n    860 \r\n    861     if len(initial_state) != len(self.states):\r\n--> 862       raise ValueError('Layer has ' + str(len(self.states)) +\r\n    863                        ' states but was passed ' + str(len(initial_state)) +\r\n    864                        ' initial states.')\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py in get_initial_state(self, inputs)\r\n    643           inputs=None, batch_size=batch_size, dtype=dtype)\r\n    644     else:\r\n--> 645       init_state = _generate_zero_filled_state(batch_size, self.cell.state_size,\r\n    646                                                dtype)\r\n    647     # Keras RNN expect the states in a list, even if it's a single state tensor.\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py in get_initial_state(self, inputs, batch_size, dtype)\r\n   2521   \"We find that LSTM augmented by 'peephole connections' from its internal\r\n   2522   cells to its multiplicative gates can learn the fine distinction between\r\n-> 2523   sequences of spikes spaced either 50 or 49 time steps apart without the help\r\n   2524   of any short training exemplars.\"\r\n   2525 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py in _generate_zero_filled_state_for_cell(cell, inputs, batch_size, dtype)\r\n   2966       return x\r\n   2967     if isinstance(x, tuple):\r\n-> 2968       return list(x)\r\n   2969     return [x]\r\n   2970 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py in _generate_zero_filled_state(batch_size_tensor, state_size, dtype)\r\n   2982 \r\n   2983 def _generate_zero_filled_state_for_cell(cell, inputs, batch_size, dtype):\r\n-> 2984   if inputs is not None:\r\n   2985     batch_size = array_ops.shape(inputs)[0]\r\n   2986     dtype = inputs.dtype\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py in map_structure(func, *structure, **kwargs)\r\n    633     ValueError: If no structure is provided or if the structures do not match\r\n    634       each other by type.\r\n--> 635     ValueError: If wrong keyword arguments are provided.\r\n    636   \"\"\"\r\n    637   if not callable(func):\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py in <listcomp>(.0)\r\n    633     ValueError: If no structure is provided or if the structures do not match\r\n    634       each other by type.\r\n--> 635     ValueError: If wrong keyword arguments are provided.\r\n    636   \"\"\"\r\n    637   if not callable(func):\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py in create_zeros(unnested_state_size)\r\n   2979   return (hasattr(state_size, '__len__') and\r\n   2980           not isinstance(state_size, tensor_shape.TensorShape))\r\n-> 2981 \r\n   2982 \r\n   2983 def _generate_zero_filled_state_for_cell(cell, inputs, batch_size, dtype):\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py in wrapped(*args, **kwargs)\r\n   2745                          [1, 2, 3],\r\n   2746                          [0, 4, 5]],\r\n-> 2747                         [[1, 2, 0],\r\n   2748                          [5, 6, 4],\r\n   2749                          [6, 1, 2],\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py in zeros(shape, dtype, name)\r\n   2792       the left (right-pads the row). It is the packing format LAPACK uses.\r\n   2793       cuSPARSE uses \"LEFT_RIGHT\", which is the opposite alignment.\r\n-> 2794   \"\"\"\r\n   2795   return gen_array_ops.matrix_set_diag_v3(\r\n   2796       input=input, diagonal=diagonal, k=k, align=align, name=name)\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py in _constant_if_small(value, shape, dtype, name)\r\n   2730           [7, 5, 7, 7],\r\n   2731           [7, 7, 6, 7]]]\r\n-> 2732 \r\n   2733   # A superdiagonal (per batch).\r\n   2734   tf.matrix_set_diag(input, diagonal, k = 1)\r\n\r\n<__array_function__ internals> in prod(*args, **kwargs)\r\n\r\n~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py in prod(a, axis, dtype, out, keepdims, initial, where)\r\n   3028         but the type of the resulting values will be cast if necessary.\r\n   3029 \r\n-> 3030     Returns\r\n   3031     -------\r\n   3032     cumprod : ndarray\r\n\r\n~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)\r\n     85                 return reduction(axis=axis, out=out, **passkwargs)\r\n     86 \r\n---> 87     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n     88 \r\n     89 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in __array__(self)\r\n    843   # operators to run when the left operand is an ndarray, because it\r\n    844   # accords the Tensor class higher priority than an ndarray, or a\r\n--> 845   # numpy matrix.\r\n    846   # TODO(mrry): Convert this to using numpy's __numpy_ufunc__\r\n    847   # mechanism, which allows more control over how Tensors interact\r\n\r\nNotImplementedError: Cannot convert a symbolic Tensor (lstm_20/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n\r\n\r\n-----------------------------------------------------------------------------------------------\r\n\r\nTensorflow : 2.4.1\r\nnumpy : 1.19.5\r\n\r\nNumPy version 1.20 or later has been reported to be a problem, so I changed it to 1.19.5 but it is the same.\r\nEven if I downgrade TensorFlow to 2.2, it's the same.\r\nWhat action should I take?", "comments": ["@kimwoo0 \r\n\r\nTensorflow 2.4.1 requires numpy~=1.19.2  is compatible version,Please check the same you have\r\n\r\nCould you please provide the colab gist with all the dependencies.And also refer similar issues [link1](https://stackoverflow.com/questions/66207609/notimplementederror-cannot-convert-a-symbolic-tensor-lstm-2-strided-slice0-t) and [link2](https://www.reddit.com/r/tensorflow/comments/lgcgby/numpyrelated_error_when_building_model/)     ,let us know if it helps.\r\nRefer this [comment](https://groups.google.com/g/machine-learning-for-physicists/c/-ofwo29YgpY) as well.  Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]