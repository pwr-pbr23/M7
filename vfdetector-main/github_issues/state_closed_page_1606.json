[{"number": 4732, "title": "Input ops fed networks operate considerably slower than direct feed ones", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nhttp://stackoverflow.com/questions/39794149/when-introducing-the-optimizer-variables-under-variable-scope-get-recreated-twic\n### Environment info\n\nTried both OS X and Linux (Ubuntu 16)\nOn both utilizing CPU only.\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nAny simple operation (i.e. calculating logits with a 3 layer deep network with simple regression in each layer) in which the data was fed with either a `parse_example`, `parse_single_example`, building a `CustomRunner` that feeds a `RandomShuffleQueue` or utilizing `QueueRunner`.  In all of those the operation and time to complete 1 epoch took considerably more than if I were to save the data in a bumpy array and feed it during a call to `sess.run()` with the `feed_dict`.\n### What other attempted solutions have you tried?\n\nTried to score the web for solutions and ran into a couple blog posts describing the problem but can't seem to find solutions.\n### Logs or other output that would be helpful\n\nTried this on both TF 0.10 and the latest RC for 0.11.\n", "comments": ["@ebrevdo might have some thoughts on this.\n", "I looked at some timelines in https://github.com/tensorflow/tensorflow/issues/4740 and [here](http://stackoverflow.com/questions/39840323/benchmark-of-howto-reading-data).\n\nOn official MNIST examples QueueDequeueMany is the bottleneck, whether it's reading using `parse_single_example` or using `slice_input_producer`, with or without the GPU, taking 10x longer than entire duration of step when using `feed_dict` with same data size\n- examples/how_tos/reading_data/fully_connected_reader.py\n- examples/how_tos/reading_data/fully_connected_preloaded.py\n- examples/how_tos/reading_data/fully_connected_preloaded_var.py\n", "(see #4740 for an attached timeline)\n", "`QueueDequeueMany` takes a long time when it's waiting for enough stuff to get put on the queue.  It's not computation cycles that are slowing it down.\n\nYou need multiple reader threads all feeding the queue in order to unblock it.  If you're using `tf.train.batch` or its friends, in TensorBoard you'll see that the queue is never full (and it should be).  Increase the number of threads you use; this is a simple argument in `tf.train.batch` and friends.\n", "Ah, good point, increasing capacity, number of threads, and adding `time.sleep` in main thread to preload the examples makes `preloaded_reader` example as fast as feed_dict one.\n\nhttps://github.com/yaroslavvb/stuff/commit/226099cd883df5aa5b08483581d66960ada90e22\n\nNote that simply increasing capacity/number of threads didn't help. I suspect Python just ran the main queue until it ran out of examples, and went back to slow behavior with blocking. Possibly this is specific to tiny models like MNIST -- when the main computation loop takes 1-2ms, then GIL is only released for 2ms, and  there's not enough time for the data-preloading threads to pre-empt the computation thread, so that thread will always be starved of examples\n\nAlso, seems that there's no way to increase number of threads in `slice_input_producer`\n", "@ebrevdo in my case, when producing timelines for with and without input ops, vs, feed_dict, it seems that the majority of the time is spent on a matmul operation in my `hidden_layer_1_1` which is the shared scope of my test.  (I obviously share my weights and biases in the hidden layers between my training data and my test data).\n\nThat operation, when used with input ops variables, takes 743ms!!!!!! (this is on a CPU/macbook air with TF 0.10)\n", "It appears that the MatMul operation takes a really long time since my test data size is longer than the training data batch size. (Training data is 100 example at each run while test is 3800 examples)\n\nThe question is why is sharing variables causing this size of a test data run to take so long as compared to utilizing place holders and not using variables scope sharing. \n\nAlso important to note that overall with the smaller test batch size and variable sharing I'm still seeing about 100ms per run compared to 75ms per run with placeholders and a longer test size batch (3800 examples). \n\nBut that's not as bad as my main problem which is 1000ms vs about 100ms \n", "The situation is even worse when utilizing parse_single_example vs a QueueRunner with data preloaded and fed that way into the QueueRunner. \n\nWith MatMul still taking a long time (for the reason stated in the above comment) and now in addition to that QueueDequeueMany taking a good 360ms. And that's with shuffle_batch, 4 threads, batch size of 100, capacity of 1000 and min dequeue of 200. \n", "Just to add some info, my features tensor is about 32,000 features.  So the shape of the test set is (3800,32000) and the training data is a batch of shape (100,32000).\n\nLike i said, doing the above utilizing `feed_dict` takes about 80ms per `sess.run()` call and utilizing a QueueRunner with directly fed data items from numpy array (only for the training data, the test data is a const created by `tf.convert_to_tensor()`) takes about 1000ms total with `MatMul` taking most of the time.  And finally utilizing two `tfrecord` files, one for the training data and one for the test data, and feeding with `parse_single_example` for the training data and `parse_example` for the test data, `QueueDequeueMany` (<- of the test data, so the one using `parse_example`) and `Matmul` are the two highest time consumers bringing the `sess.run()` up to around 1800ms. \n", "@danielschonfeld \nIt sounds a bit suspicious that matmul itself would take different time depending on `feed_dict` vs `Queue`. Are you getting MatMul timing from the Timeline? Are you sure that matmul sizes are the same? Unlike with `QueueDequeue` many, IO time should not be included in the timeline timing, so if size is the same, the timing should also be the same\n\nHere's [how](https://github.com/yaroslavvb/stuff/commit/aa886026ca3c48e27f776269548b40a2e2bb89ea) you can generate text file with timing information. In that file you can see duration of matmul as well as total output size for sanity checking\n\n```\n    node_stats {\n      node_name: \"hidden1/MatMul\"\n      all_start_micros: 1475724885315055\n      op_start_rel_micros: 1\n      op_end_rel_micros: 314\n      all_end_rel_micros: 318\n      memory {\n        allocator_name: \"cpu\"\n        total_bytes: 51200\n      }\n\n```\n\nIf the this timing information shows same total_bytes but 10x difference in timing, the way to get this triaged would be to provide a minimal self-contained reproducible test case\n", "@yaroslavvb i'm certain about the size being the same.  But the difference between the two runs, except for being `feed_dict` vs `Queue` is that the matmul operation that takes long utilizing `Queue` is happening on the shared variable op node, an op that didn't exist (since it wasn't needed) on the `feed_dict` run.\n\nI got the time from timeline, yes.  And i assume the time is what's called `wall time`. \n\nFinally, I'll go ahead and run it again, and save the raw RunMetadata this time.  I'll follow up with another comment shortly with the results of that.\n", "@yaroslavvb so doing that revealed something I didn't realize before and that now i'm not sure how to proceed.  \n\nFirst things first, there's no problem with `Matmul`.  It produces the same amount of execution time for the same amount of bytes. I recorded only the first example from the two below, which in the feed_dict run didn't include the test data run.  I found that out by looking at the dim size and the output bytes as you suggested.\n\nBasically I do something like the following:\n`sess.run([optimizer, cost, summeryMerge])`\nAnd later on I do something like `accuracy.eval()`\n\nIn the `Queue` based run, the value of accuracy is calculated at every run, even without me calling `accuracy.eval()` - I'm assuming because of the shared variables being updated since the graph has them built in.  That in turn is why my runs are considerably slower overall compared to `feed_dict`.  The operation takes the same amount of time, but is happening at every batch now instead of once per epoch.  I'm assuming that's also why the `Queue` that holds the test data gets depleted fast and takes a long while to continue producing sample data, causing `QueueDequeueMany` to appear slow.\n\nMy question then is how does one go about using `Queues` and feeding in test data but have it computed only once per epoch instead of at every batch run.  From reading `Sharing Variables`, I thought thats the way to go with training/test data.  So I have a structure similar to the following code:\n\n```\ndef neural_network_model(x):\n    with tf.variable_scope(\"hidden_layer_1\"):\n        relu1 = hl_relu(x, [max_words_len, n_nodes_hl1], [n_nodes_hl1]) #hl_relu utilizes get_variable for weights and biases\n```\n\nAnd later on i'd do something like this:\n\n```\ndef train_neural_network(x_batch, y_batch, test_x, test_y):\n    with tf.device(\"/cpu:0\"):\n        with tf.variable_scope(\"dnn\") as scope:\n            logits = neural_network_model(x_batch)\n            scope.reuse_variables()\n            test_logits = neural_network_model(test_x)\n```\n\nI'd use `logits` to compute the cost with `softmax_cross_entropy_with_logits`, as follows:\n\n```\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_batch), name=\"cost\")\n```\n\nand `test_logits` to compute the accuracy in a three step operation looking something like this:\n\n```\nprediction = tf.nn.softmax(test_logits)\ncorrect = tf.equal(tf.argmax(prediction, 1), tf.argmax(test_y, 1), name=\"correct\")\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n```\n\nHow does one do it the 'right' way? My newbieness shows unfortunately\n", "Maybe i should just simplify my question.  How do I train and test in the same process, using shared variables with 2 tfrecord files, one designated as train data and one designated as test data. (I'm willing to do it in 1 tfrecords file and just designate a percentage of the data as test data too... it doesn't really matter to me)\n\nAs mentioned briefly in on the last line of [https://www.tensorflow.org/versions/r0.11/how_tos/reading_data/index.html#multiple-input-pipelines](url)\n", "I'm confused why matmul timing would change unless the shapes changed, or\nperhaps if it for some reason started running on a CPU device where before\nit was running on a GPU device.  Please provide the runmetadata.\n\nOn Wed, Oct 5, 2016 at 8:11 PM, Daniel Schonfeld notifications@github.com\nwrote:\n\n> Just to add some info, my features tensor is about 32,000 features. So the\n> shape of the test set is (3800,32000) and the training data is usually of\n> shape (100,32000).\n> \n> Like i said, doing the above utilizing feed_dict takes about 80ms per\n> sess.run() call and utilizing a QueueRunner with directly fed data items\n> from numpy array (only for the training data, the test data is a const with\n> tf.convert_to_tensor()) takes about 1000ms total with that matmul taking\n> most of the time. And finally utilizing a two tfrecord files, one for the\n> training data and one for the test data, and feeding with\n> parse_single_example for the training data and parse_example for the test\n> data, QueueDequeueMany (<- of the test data, so the one using\n> parse_example) and Matmul are the two highest time consumers bringing the\n> sess.run() up to around 1800ms.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4732#issuecomment-251856881,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim3mQAeLWGCZCOdnkF1K4ylL5ey5sks5qxGbNgaJpZM4KMz--\n> .\n", "@ebrevdo the shape changed... it was my mistake.  My timing comparison was incorrect.  When compared with the same shape the timing was identical.\n\nSo now I'm just left wondering, what I asked above.  How can I do both train and eval in the same process using input ops?\n", "It's atypical to do this.  Folks usually build two graphs: one for\ntraining, and a separate one for eval.  (you can use a common function to\nbuild the \"common\" subgraph).  You then run the train and eval graphs\neither in separate threads or in different processes.  At the very least,\nyou use different sessions for training and eval.\n\nOn Thu, Oct 6, 2016 at 9:08 AM, Daniel Schonfeld notifications@github.com\nwrote:\n\n> @ebrevdo https://github.com/ebrevdo the shape changed... it was my\n> mistake. My timing comparison was incorrect. When compared with the same\n> shape the timing was identical.\n> \n> So now I'm just left wondering, what I asked above. How can I do both\n> train and eval in the same process using input ops?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4732#issuecomment-252010220,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim2jldBDZ9pE6HgkqTgkQ96TVIcwCks5qxR0bgaJpZM4KMz--\n> .\n", "@ebrevdo I see... so i should adopt more of like the cifar-10 example model where one process trains and one monitors for checkpoint files and loads and tries the eval stuff against?\n", "Exactly. Then you keep the best checkpoint.\n\nOn Oct 6, 2016 9:25 AM, \"Daniel Schonfeld\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo I see... so i should adopt more of\n> like the cifar-10 example model where one process trains and one monitors\n> for checkpoint files and loads and tries the eval stuff against?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4732#issuecomment-252014914,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimw_dRn0cy_ytHS9OTOP9p_IeJVNRks5qxSD4gaJpZM4KMz--\n> .\n", "now if only i can figure out why my raw usage of tensorflow only achieves 60% accuracy compared to using the SciKit module of TF `DNNClassifier` which archives like 99% pretty fast.\n\nPerhaps the usage of embedded lookup? We can probably close this issue for now, unless someone else wants to keep it alive?\n", "Going through the issues mentioning r0.11.\nLooks like the initial issue is resolved here. Closing.\n"]}, {"number": 4731, "title": "Use scope name instead of scope tensor in output collections", "body": "Start from version r0.11, because of the change in variable_scope function, it breaks the collect_named_output function. Instead of using scope tensor, use scope.orignal_name_scope.\n", "comments": ["Can one of the admins verify this patch?\n", "@haleuh, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @martinwicke and @lukaszkaiser to be potential reviewers.\n"]}, {"number": 4730, "title": "Add KL divergence for Categorical and Beta distributions.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@admcrae, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @ebrevdo and @jhseu to be potential reviewers.\n", "I have cleaned up the code a bit along with adding another sanity check to the tests. I can confirm that running the tests 500 times results in no failures.\n", "Jenkins, test this please.\n", "@tensorflow-jenkins test this please\n", "Please resolve conflicts.\n", "Done.\n", "Jenkins, test this please.\n", "(maybe docker bug?)\n\nJenkins, test this please.\n"]}, {"number": 4729, "title": "IOS #include \"unsupported/Eigen/CXX11/Tensor\"", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System: Mac OS X EI captain 10.11.6\n\nInstalled version of CUDA and cuDNN: \nNo\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n   I install tensorflow from source following this site: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   But It seems i cant get the tensorflow version command, there is no module named tensorflow; I also installed tensorflow with docker on my Mac.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`) \n   6218ac2be3cc530da866ec32da4cb86c6ac5bb85\n2. The output of `bazel version`\n   Build label: 0.3.1-homebrew\n   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Thu Aug 4 09:58:27 2016 (1470304707)\n   Build timestamp: 1470304707\n   Build timestamp as int: 1470304707\n### What other attempted solutions have you tried?\n\nSearch on stackoverflow\n\nI followed the ios_example from here, https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples\nI successfully installed the static library containing the core code of tensorflow;\nI downloaded the incetption v1, and I have xcode 7.3.1 installed on my Mac;\nWhen I launch xcode from tf_ios_makefile_example.xcodeproj in the subfolder tensorflow/tensorflow/contrib/ios_examples/simple, I got un error:\n# include \"unsupported/Eigen/CXX11/Tensor\"\n\nWhat did I miss? the version of eigen is not right? I do install eigen.\n\nAny suggestion will be appreciated\nThanks\n", "comments": ["This seems to be a duplicate of #4680.  Please follow the development there, thanks.\n"]}, {"number": 4728, "title": "Eigen errors.", "body": "/apps/.GCC/6.2.0/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=0' -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -g0 -DNDEBUG -pipe -DMKL_LP64 -I/usr/include/ncurses -I/apps/INTEL/2017.0-035/mkl/include -I/apps/PYTHON/3.5.2_ML/include/python3.5m -m64 '-march=native' '-mtune=native' -Ofast -s -fmath-errno -fno-unsafe-math-optimizations -fno-finite-math-only -fno-cx-limited-range -freciprocal-math -ftree-vectorize -fomit-frame-pointer -fno-stack-protector -ffunction-sections -fdata-sections -flto -fuse-linker-plugin -Wl,--as-needed -L/apps/.GLIBC/2.24/lib -L/apps/INTEL/2017.0-035/mkl/lib/intel64 -lmkl_rt -lpthread -lm -ldl -Wl,--gc-sections -Wl,-rpath,/apps/.GLIBC/2.24/lib -Wl,-rpath,/apps/.GCC/6.2.0/lib64 -Wl,-rpath,/apps/.GCC/6.2.0/lib -Wl,-rpath,/apps/INTEL/2017.0-035/mkl/lib/intel64 -Wl,--dynamic-linker,/apps/.GLIBC/2.24/lib/ld-linux-x86-64.so.2 '-std=c++11' -MD -MF bazel-out/local_mn-py3-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/tensorflow/core/kernels/batch_matmul_op_real.pic.d '-frandom-seed=bazel-out/local_mn-py3-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/tensorflow/core/kernels/batch_matmul_op_real.pic.o' -fPIC -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/local_mn-py3-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_mn-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/eigen_archive -iquote external/protobuf -iquote bazel-out/local_mn-py3-opt/genfiles/external/protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_mn-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/farmhash_archive -iquote external/gif_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/gif_archive -iquote external/highwayhash -iquote bazel-out/local_mn-py3-opt/genfiles/external/highwayhash -iquote external/jpeg_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/zlib_archive -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/eigen_archive -isystem external/protobuf/src -isystem bazel-out/local_mn-py3-opt/genfiles/external/protobuf/src -isystem external/farmhash_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/farmhash_archive -isystem external/gif_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/gif_archive -isystem external/highwayhash -isystem bazel-out/local_mn-py3-opt/genfiles/external/highwayhash -isystem external/jpeg_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/jpeg_archive -isystem external/png_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/zlib_archive -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread '--sysroot=/apps/.GLIBC/2.24' -c tensorflow/core/kernels/batch_matmul_op_real.cc -o bazel-out/local_mn-py3-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/tensorflow/core/kernels/batch_matmul_op_real.pic.o\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:331:0,\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n                 from ./tensorflow/core/kernels/batch_matmul_op_impl.h:21,\n                 from tensorflow/core/kernels/batch_matmul_op_real.cc:16:\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of \u2018typename Eigen::internal::unpacket_traits<T>::type Eigen::internal::predux(const Packet&) [with Packet = Eigen::internal::Packet8h; typename Eigen::internal::unpacket_traits<T>::type = Eigen::half]\u2019:\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralMatrixVector.h:548:23:   required from \u2018static void Eigen::internal::general_matrix_vector_product<Index, LhsScalar, LhsMapper, 1, ConjugateLhs, RhsScalar, RhsMapper, ConjugateRhs, Version>::run(Index, Index, const LhsMapper&, const RhsMapper&, Eigen::internal::general_matrix_vector_product<Index, LhsScalar, LhsMapper, 1, ConjugateLhs, RhsScalar, RhsMapper, ConjugateRhs, Version>::ResScalar_, Index, Eigen::internal::general_matrix_vector_product<Index, LhsScalar, LhsMapper, 1, ConjugateLhs, RhsScalar, RhsMapper, ConjugateRhs, Version>::ResScalar) [with Index = long int; LhsScalar = Eigen::half; LhsMapper = Eigen::internal::const_blas_data_mapper<Eigen::half, long int, 1>; bool ConjugateLhs = false; RhsScalar = Eigen::half; RhsMapper = Eigen::internal::const_blas_data_mapper<Eigen::half, long int, 0>; bool ConjugateRhs = false; int Version = 0; Eigen::internal::general_matrix_vector_product<Index, LhsScalar, LhsMapper, 1, ConjugateLhs, RhsScalar, RhsMapper, ConjugateRhs, Version>::ResScalar = Eigen::half]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GeneralProduct.h:318:132:   required from \u2018static void Eigen::internal::gemv_dense_selector<2, 1, true>::run(const Lhs&, const Rhs&, Dest&, const typename Dest::Scalar&) [with Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Dest = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; typename Dest::Scalar = Eigen::half]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:367:34:   required from \u2018static void Eigen::internal::generic_product_impl<Lhs, Rhs, Eigen::DenseShape, Eigen::DenseShape, 7>::scaleAndAddTo(Dest&, const Lhs&, const Rhs&, const Scalar&) [with Dest = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Eigen::internal::generic_product_impl<Lhs, Rhs, Eigen::DenseShape, Eigen::DenseShape, 7>::Scalar = Eigen::half]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:349:27:   required from \u2018static void Eigen::internal::generic_product_impl_base<Lhs, Rhs, Derived>::scaleAndAddTo(Dst&, const Lhs&, const Rhs&, const Scalar&) [with Dst = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Derived = Eigen::internal::generic_product_impl<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>, Eigen::DenseShape, Eigen::DenseShape, 7>; Eigen::internal::generic_product_impl_base<Lhs, Rhs, Derived>::Scalar = Eigen::half]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:337:33:   required from \u2018static void Eigen::internal::generic_product_impl_base<Lhs, Rhs, Derived>::evalTo(Dst&, const Lhs&, const Rhs&) [with Dst = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Derived = Eigen::internal::generic_product_impl<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>, Eigen::DenseShape, Eigen::DenseShape, 7>]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:144:43:   [ skipping 4 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\n./tensorflow/core/kernels/batch_matmul_op_impl.h:181:17:   required from \u2018static void tensorflow::{anonymous}::SequentialMatMulKernel<Scalar>::Run(const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_, int, int) [with Scalar = Eigen::half]\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:238:50:   required from \u2018tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext_, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_)::<lambda(int, int)> [with Scalar = Eigen::half]\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:237:42:   required from \u2018struct tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext_, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_) [with Scalar = Eigen::half]::<lambda(int, int)>\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:235:12:   required from \u2018static void tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext_, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_) [with Scalar = Eigen::half]\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:427:46:   required from \u2018void tensorflow::BatchMatMul<Device, Scalar>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; Scalar = Eigen::half]\u2019\ntensorflow/core/kernels/batch_matmul_op_real.cc:33:1:   required from here\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:324:10: error: could not convert \u2018a\u2019 from \u2018const Eigen::internal::Packet8h\u2019 to \u2018Eigen::internal::unpacket_traitsEigen::internal::Packet8h::type {aka Eigen::half}\u2019\n { return a; }\n          ^\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:442:0,\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n                 from ./tensorflow/core/kernels/batch_matmul_op_impl.h:21,\n                 from tensorflow/core/kernels/batch_matmul_op_real.cc:16:\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h: In instantiation of \u2018static void Eigen::internal::etor_product_packet_impl<1, -1, Lhs, Rhs, Packet, LoadMode>::run(Eigen::Index, Eigen::Index, const Lhs&, const Rhs&, Eigen::Index, Packet&) [with Lhs = Eigen::internal::evaluator<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Rhs = Eigen::internal::evaluator<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Packet = Eigen::internal::Packet8h; int LoadMode = 0; Eigen::Index = long int]\u2019:\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:545:20:   required from \u2018const PacketType Eigen::internal::product_evaluator<Eigen::Product<Lhs, Rhs, 1>, ProductTag, Eigen::DenseShape, Eigen::DenseShape>::packet(Eigen::Index, Eigen::Index) const [with int LoadMode = 0; PacketType = Eigen::internal::Packet8h; Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; int ProductTag = 8; typename Eigen::internal::traits<typename Eigen::Product<Lhs, Rhs, 1>::Rhs>::Scalar = Eigen::half; typename Eigen::internal::traits<typename Eigen::Product<Lhs, Rhs, 1>::Lhs>::Scalar = Eigen::half; Eigen::Index = long int]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/AssignEvaluator.h:652:5:   required from \u2018void Eigen::internal::generic_dense_assignment_kernel<DstEvaluatorTypeT, SrcEvaluatorTypeT, Functor, Version>::assignPacket(Eigen::Index, Eigen::Index) [with int StoreMode = 16; int LoadMode = 0; PacketType = Eigen::internal::Packet8h; DstEvaluatorTypeT = Eigen::internal::evaluator<Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; SrcEvaluatorTypeT = Eigen::internal::evaluator<Eigen::Product<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 1> >; Functor = Eigen::internal::assign_op<Eigen::half, Eigen::half>; int Version = 0; Eigen::Index = long int]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/AssignEvaluator.h:666:48:   required from \u2018void Eigen::internal::generic_dense_assignment_kernel<DstEvaluatorTypeT, SrcEvaluatorTypeT, Functor, Version>::assignPacketByOuterInner(Eigen::Index, Eigen::Index) [with int StoreMode = 16; int LoadMode = 0; PacketType = Eigen::internal::Packet8h; DstEvaluatorTypeT = Eigen::internal::evaluator<Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; SrcEvaluatorTypeT = Eigen::internal::evaluator<Eigen::Product<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 1> >; Functor = Eigen::internal::assign_op<Eigen::half, Eigen::half>; int Version = 0; Eigen::Index = long int]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/AssignEvaluator.h:551:9:   required from \u2018static void Eigen::internal::dense_assignment_loop<Kernel, 4, 0>::run(Kernel&) [with Kernel = Eigen::internal::generic_dense_assignment_kernel<Eigen::internal::evaluator<Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >, Eigen::internal::evaluator<Eigen::Product<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 1> >, Eigen::internal::assign_op<Eigen::half, Eigen::half>, 0>]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/AssignEvaluator.h:713:37:   required from \u2018void Eigen::internal::call_dense_assignment_loop(const DstXprType&, const SrcXprType&, const Functor&) [with DstXprType = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; SrcXprType = Eigen::Product<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 1>; Functor = Eigen::internal::assign_op<Eigen::half, Eigen::half>]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/AssignEvaluator.h:862:31:   [ skipping 8 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\n./tensorflow/core/kernels/batch_matmul_op_impl.h:185:17:   required from \u2018static void tensorflow::{anonymous}::SequentialMatMulKernel<Scalar>::Run(const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_, int, int) [with Scalar = Eigen::half]\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:238:50:   required from \u2018tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext_, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_)::<lambda(int, int)> [with Scalar = Eigen::half]\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:237:42:   required from \u2018struct tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext_, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_) [with Scalar = Eigen::half]::<lambda(int, int)>\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:235:12:   required from \u2018static void tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext_, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor_) [with Scalar = Eigen::half]\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:427:46:   required from \u2018void tensorflow::BatchMatMul<Device, Scalar>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; Scalar = Eigen::half]\u2019\ntensorflow/core/kernels/batch_matmul_op_real.cc:33:1:   required from here\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:648:24: error: no matching function for call to \u2018pset1(int)\u2019\n     res = pset1<Packet>(0);\n           ~~~~~~~~~~~~~^~~\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:331:0,\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n                 from ./tensorflow/core/kernels/batch_matmul_op_impl.h:21,\n                 from tensorflow/core/kernels/batch_matmul_op_real.cc:16:\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:222:1: note: candidate: template<class Packet> Packet Eigen::internal::pset1(const typename Eigen::internal::unpacket_traits<Packet>::type&)\n pset1(const typename unpacket_traits<Packet>::type& a) { return a; }\n ^~~~~\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:222:1: note:   template argument deduction/substitution failed:\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:442:0,\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n                 from ./tensorflow/core/kernels/batch_matmul_op_impl.h:21,\n                 from tensorflow/core/kernels/batch_matmul_op_real.cc:16:\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:648:24: note:   cannot convert \u20180\u2019 (type \u2018int\u2019) to type \u2018const type& {aka const Eigen::half&}\u2019\n     res = pset1<Packet>(0);\n", "comments": ["Can you post the answers to the questions shown in the New Issue template?\n\nAs a quick sanity check, what's your build command?  Did it include `--copt=-mavx`?\n", "Hi @concretevitamin and @benoitsteiner,\n\nthe **bazel** command was:\n\n`bazel build --nofetch --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --host_crosstool_top=@bazel_tools//tools/cpp:toolchain-mn --host_cpu=mn --crosstool_top=@bazel_tools//tools/cpp:toolchain-mn --cpu=mn -c opt --strip=always //tensorflow/tools/pip_package:build_pip_package`\n\nI'm using a custom tool-chain, so as to set max performance flags for our HW, but I did not use `--copt=-mavx`. Shall I have added? Is this flag enabling something I need?\n\nI guess the error was introduced between Friday and Monday (yesterday) 'cause on Friday I compiled another TF and went fine. I've checked Eigen version for both TF compilations, and it's the same. I think it's a TF source change which breaks the compilation then.\n\nLet me some time to complete the whole New Issue template, yesterday was a bad day. My apologises.\n\nRegards.\n", "Answers to the New Issue template:\n\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI haven't found any information after googling a lot.\n\n### Operating System:\n\nSLES 11.3 (x86_64)\n\n### Installed version of CUDA and cuDNN:\n\nNo GPUs on this cluster. No GPU libs installed.\n\n### Compilation, not a PIP package.\n\n### The commit hash (`git rev-parse HEAD`)\n\n3d35376a66cde4f3e614c746d3c8708d15caa1b5\n\n### The output of `bazel version`\n\nBuild label: 0.3.1- (@non-git)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: lun oct 3 07:41:14 2016 (1475480474)\nBuild timestamp: 1475480474\nBuild timestamp as int: 1475480474\n\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nThe compilation line I pasted to open this issue.\n\n### What other attempted solutions have you tried?\n\nI've tried to found the \"bug\", without luck.\n\n### Logs or other output that would be helpful\n\nAt this time no. I think all the necessary info has been provided.\n", "Adding `--copt=-mavx` did not work. Same error.\n", "Seems like this commit is breaking the compilation: https://github.com/tensorflow/tensorflow/commit/3057e97bf5d8079f6e139212c6812fd07e989390#diff-8157d290f5c74391c3c97d5694f001c2\n\nAFAIK, the problem is that this commit adds new Eigen stuff, and it's not working. I think it is because at some point wants to cast a couple of things (var a and literal 0), and cannot do it.\n", "It may be an error coming from the recently implemented Eigen's half type, but I'm unable to find a fix.\n", "I'm testing a change that could solve this compilation error. There is no guarantee though since I don't have access to your custom toolchain to validate the fix. I'll update this issue once the fix is available.\n", "Ok no problem, just give me this patch and I'll test it in my toolchain asap. Thanks!!\n", "@benoitsteiner, how the patch is going? Something I can help?\n", "Any news. Thanks!!\n", "@tripiana The change is in: https://github.com/tensorflow/tensorflow/commit/60afe603d1dbd580dff9c60e6d4868b3e2c734e4. Please try it and let me know if that fixes your compilation error.\n", "This Eigen patch fixes half the problem, so I think you are going in the right direction. Solves the error:\n\n```\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:648:24: error: no matching function for call to \u2018pset1(int)\u2019\nres = pset1(0);\n```\n\nbut not the error:\n\n```\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:324:10: error: could not convert 'a' from 'const Eigen::internal::Packet8h' to 'Eigen::internal::unpacket_traits<Eigen::internal::Packet8h>::type {aka Eigen::half}'\n { return a; }\n```\n\nI'm trying to fix this but I'm unable to find a solutions after few hours.\n\nBest regards.\n", "@benoitsteiner this is the complete compilation error:\n\n```\n/apps/.GCC/6.2.0/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=0' -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -g0 -DNDEBUG -pipe -DMKL_LP64 -DEIGEN_USE_MKL_ALL -I/usr/include/ncurses -I/apps/INTEL/2017.0-035/mkl/include -I/apps/PYTHON/3.5.2_ML/include/python3.5m -m64 '-march=native' '-mtune=native' -Ofast -s -fmath-errno -fno-unsafe-math-optimizations -fno-finite-math-only -fno-cx-limited-range -freciprocal-math -ftree-vectorize -fomit-frame-pointer -fno-stack-protector -ffunction-sections -fdata-sections -flto -fuse-linker-plugin -Wl,--as-needed -L/apps/.GLIBC/2.24/lib -L/apps/INTEL/2017.0-035/mkl/lib/intel64 -lmkl_rt -lpthread -lm -ldl -Wl,--gc-sections -Wl,-rpath,/apps/.GLIBC/2.24/lib -Wl,-rpath,/apps/.GCC/6.2.0/lib64 -Wl,-rpath,/apps/.GCC/6.2.0/lib -Wl,-rpath,/apps/INTEL/2017.0-035/mkl/lib/intel64 -Wl,--dynamic-linker,/apps/.GLIBC/2.24/lib/ld-linux-x86-64.so.2 '-std=c++11' -MD -MF bazel-out/local_mn-py3-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/tensorflow/core/kernels/batch_matmul_op_real.pic.d '-frandom-seed=bazel-out/local_mn-py3-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/tensorflow/core/kernels/batch_matmul_op_real.pic.o' -fPIC -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/local_mn-py3-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_mn-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/eigen_archive -iquote external/protobuf -iquote bazel-out/local_mn-py3-opt/genfiles/external/protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_mn-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/farmhash_archive -iquote external/gif_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/gif_archive -iquote external/highwayhash -iquote bazel-out/local_mn-py3-opt/genfiles/external/highwayhash -iquote external/jpeg_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_mn-py3-opt/genfiles/external/zlib_archive -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/eigen_archive -isystem external/protobuf/src -isystem bazel-out/local_mn-py3-opt/genfiles/external/protobuf/src -isystem external/farmhash_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/farmhash_archive -isystem external/gif_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/gif_archive -isystem external/highwayhash -isystem bazel-out/local_mn-py3-opt/genfiles/external/highwayhash -isystem external/jpeg_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/jpeg_archive -isystem external/png_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_mn-py3-opt/genfiles/external/zlib_archive -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread '--sysroot=/apps/.GLIBC/2.24' -c tensorflow/core/kernels/batch_matmul_op_real.cc -o bazel-out/local_mn-py3-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/tensorflow/core/kernels/batch_matmul_op_real.pic.o -ftemplate-backtrace-limit=0\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:331:0,\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n                 from ./tensorflow/core/kernels/batch_matmul_op_impl.h:21,\n                 from tensorflow/core/kernels/batch_matmul_op_real.cc:16:\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of \u2018typename Eigen::internal::unpacket_traits<T>::type Eigen::internal::predux(const Packet&) [with Packet = Eigen::internal::Packet8h; typename Eigen::internal::unpacket_traits<T>::type = Eigen::half]\u2019:\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralMatrixVector.h:548:23:   required from \u2018static void Eigen::internal::general_matrix_vector_product<Index, LhsScalar, LhsMapper, 1, ConjugateLhs, RhsScalar, RhsMapper, ConjugateRhs, Version>::run(Index, Index, const LhsMapper&, const RhsMapper&, Eigen::internal::general_matrix_vector_product<Index, LhsScalar, LhsMapper, 1, ConjugateLhs, RhsScalar, RhsMapper, ConjugateRhs, Version>::ResScalar*, Index, Eigen::internal::general_matrix_vector_product<Index, LhsScalar, LhsMapper, 1, ConjugateLhs, RhsScalar, RhsMapper, ConjugateRhs, Version>::ResScalar) [with Index = long int; LhsScalar = Eigen::half; LhsMapper = Eigen::internal::const_blas_data_mapper<Eigen::half, long int, 1>; bool ConjugateLhs = false; RhsScalar = Eigen::half; RhsMapper = Eigen::internal::const_blas_data_mapper<Eigen::half, long int, 0>; bool ConjugateRhs = false; int Version = 0; Eigen::internal::general_matrix_vector_product<Index, LhsScalar, LhsMapper, 1, ConjugateLhs, RhsScalar, RhsMapper, ConjugateRhs, Version>::ResScalar = Eigen::half]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GeneralProduct.h:318:132:   required from \u2018static void Eigen::internal::gemv_dense_selector<2, 1, true>::run(const Lhs&, const Rhs&, Dest&, const typename Dest::Scalar&) [with Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Dest = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; typename Dest::Scalar = Eigen::half]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:367:34:   required from \u2018static void Eigen::internal::generic_product_impl<Lhs, Rhs, Eigen::DenseShape, Eigen::DenseShape, 7>::scaleAndAddTo(Dest&, const Lhs&, const Rhs&, const Scalar&) [with Dest = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Eigen::internal::generic_product_impl<Lhs, Rhs, Eigen::DenseShape, Eigen::DenseShape, 7>::Scalar = Eigen::half]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:349:27:   required from \u2018static void Eigen::internal::generic_product_impl_base<Lhs, Rhs, Derived>::scaleAndAddTo(Dst&, const Lhs&, const Rhs&, const Scalar&) [with Dst = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Derived = Eigen::internal::generic_product_impl<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>, Eigen::DenseShape, Eigen::DenseShape, 7>; Eigen::internal::generic_product_impl_base<Lhs, Rhs, Derived>::Scalar = Eigen::half]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:337:33:   required from \u2018static void Eigen::internal::generic_product_impl_base<Lhs, Rhs, Derived>::evalTo(Dst&, const Lhs&, const Rhs&) [with Dst = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Derived = Eigen::internal::generic_product_impl<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>, Eigen::DenseShape, Eigen::DenseShape, 7>]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/ProductEvaluators.h:144:43:   required from \u2018static void Eigen::internal::Assignment<DstXprType, Eigen::Product<Lhs, Rhs, Options>, Eigen::internal::assign_op<Scalar, Scalar>, Eigen::internal::Dense2Dense, typename Eigen::internal::enable_if<((Options == DefaultProduct) || (Options == AliasFreeProduct))>::type>::run(DstXprType&, const SrcXprType&, const Eigen::internal::assign_op<Scalar, Scalar>&) [with DstXprType = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Lhs = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Rhs = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; int Options = 0; Scalar = Eigen::half; Eigen::internal::Assignment<DstXprType, Eigen::Product<Lhs, Rhs, Options>, Eigen::internal::assign_op<Scalar, Scalar>, Eigen::internal::Dense2Dense, typename Eigen::internal::enable_if<((Options == DefaultProduct) || (Options == AliasFreeProduct))>::type>::SrcXprType = Eigen::Product<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>, 0>]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/AssignEvaluator.h:813:49:   required from \u2018void Eigen::internal::call_assignment_no_alias(Dst&, const Src&, const Func&) [with Dst = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Src = Eigen::Product<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>, 0>; Func = Eigen::internal::assign_op<Eigen::half, Eigen::half>]\u2019\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NoAlias.h:42:31:   required from \u2018ExpressionType& Eigen::NoAlias<ExpressionType, StorageBase>::operator=(const StorageBase<OtherDerived>&) [with OtherDerived = Eigen::Product<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>, 0>; ExpressionType = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; StorageBase = Eigen::MatrixBase]\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:131:19:   required from \u2018void tensorflow::{anonymous}::Multiply(bool, bool, Tx, Ty, Tz) [with Tx = Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; Ty = Eigen::Block<Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, 1, false>; Tz = Eigen::Map<Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >]\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:181:17:   required from \u2018static void tensorflow::{anonymous}::SequentialMatMulKernel<Scalar>::Run(const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor*, int, int) [with Scalar = Eigen::half]\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:238:50:   required from \u2018tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor*)::<lambda(int, int)> [with Scalar = Eigen::half]\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:237:42:   required from \u2018struct tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor*) [with Scalar = Eigen::half]::<lambda(int, int)>\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:235:12:   required from \u2018static void tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Launch(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, bool, bool, tensorflow::Tensor*) [with Scalar = Eigen::half]\u2019\n./tensorflow/core/kernels/batch_matmul_op_impl.h:427:46:   required from \u2018void tensorflow::BatchMatMul<Device, Scalar>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; Scalar = Eigen::half]\u2019\ntensorflow/core/kernels/batch_matmul_op_real.cc:33:1:   required from here\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:324:10: error: could not convert \u2018a\u2019 from \u2018const Eigen::internal::Packet8h\u2019 to \u2018Eigen::internal::unpacket_traits<Eigen::internal::Packet8h>::type {aka Eigen::half}\u2019\n { return a; }\n```\n\nIt is very interesting that there is no predux for Packet8h nor for Half types available... The fact that this is CUDA stuff and I only want to compile CPU tensorflow is very confusing, but I guess it is for some reason.\n\nHope you can help me.\n\nBest regards.\n", "Can you try to patch https://github.com/tensorflow/tensorflow/pull/4845 and let me know if that fixes the problem ?\n", "@tripiana https://github.com/tensorflow/tensorflow/pull/4845 is now merged. Does that fix your problem ? \n", "Give me a while, I'm going to test it, I'm complete sure (looking at the eigen's commit) that this merge into master will solve the problem.\n", "I have tested it and it works. Issue fixed. Please close.\n"]}, {"number": 4727, "title": "Minor issues with C++ custom ops documentation", "body": "I was following the instructions below to define a custom op called `zero_out`.\n\nhttps://github.com/tensorflow/tensorflow/blob/aaeb50c55427350841f7a0c226af9db174397d55/tensorflow/g3doc/how_tos/adding_an_op/index.md\n\nI found a few issues:\n1. You cannot load a `so` file without giving an absolute path (or prefixing with `./`). \n2. The zero_out_module has no zero_out component. \n\n---\n\n```\nIn [1]: import tensorflow as tf\n\nIn [2]: zero_out_module = tf.load_op_library('zero_out.so')\n---------------------------------------------------------------------------\nNotFoundError                             Traceback (most recent call last)\n<ipython-input-2-e4878fcfc44f> in <module>()\n----> 1 zero_out_module = tf.load_op_library('zero_out.so')\n\n/home/hholst/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py in load_op_library(library_filename)\n     73           return _OP_LIBRARY_MAP[library_filename]\n     74       # pylint: disable=protected-access\n---> 75       raise errors._make_specific_exception(None, None, error_msg, error_code)\n     76       # pylint: enable=protected-access\n     77   finally:\n\nNotFoundError: zero_out.so: cannot open shared object file: No such file or directory\n\nIn [3]: zero_out_module = tf.load_op_library('./zero_out.so')\n\nIn [4]: \n\n```\n\n![image](https://cloud.githubusercontent.com/assets/6200749/19033213/9620c9ec-895d-11e6-8f4e-db477b4b5397.png)\n", "comments": ["OK. I fixed the latter issue due to a mistake on my side (I did not call `REGISTER_OP` only `REGISTER_KERNEL_BUILDER`) but the load still requires `./` prefix.\n\nThere are some other minor things with formatting (escaped symbols in code) which I could make a PR for if it is welcome?\n", "@hholst80 Yes, we'd very much appreciate a PR for this!\n", "I can work on this.\n", "I met the same problem and still not understand your solution to the latter issue. What do you mean by `I did not call REGISTER_OP`?, I have added `REGISTER_OP(\"ZeroOut\").Input.....` in zero_out.cc but the issue still exists. Or do you mean adding something like `REGISTER_OP(zero_out)......`? But what is the input and output? Or do I need some additional command to call  `REGISTER_OP`? Thanks in advance! "]}, {"number": 4726, "title": "Inception v3 failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED", "body": "I was following the instructions in https://github.com/tensorflow/models/tree/master/inception to train inception v3. I created the TFRecord data successfully. But when I tried to train the network using the following command:\nbazel-bin/inception/imagenet_train --num_gpus=4 --batch_size=128 --train_dir=... --data_dir=...\nIt has the following warning and error:\n\nWARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:1979] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED\n### Environment info\n\nOperating System: Redhat 7.2\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root   560184 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 59715990 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn_static.a\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`):\n   f98c5ded31d7da0c2d127c28b2c16f0307a368f0\n2. The output of `bazel version`\n   .\n   Build label: 0.3.1- (@non-git)\n   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Thu Sep 29 22:19:27 2016 (1475187567)\n   Build timestamp: 1475187567\n   Build timestamp as int: 1475187567\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\nExcept the above warning and error, there are the following warnings:\nWARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\nWARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n", "comments": ["Could you try a smaller batch size, to see if it works?\n", "Thanks. I tried a smaller batch size 64 and it works now.\n"]}, {"number": 4725, "title": "tutorials_example_trainer failed to build", "body": "when I build the tensorflow with the command:\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nerror happens :\n./tensorflow/core/framework/allocator.h(154): warning: missing return statement at end of non-void function \"tensorflow::Allocator::RequestedSize\"\n\n./tensorflow/core/framework/allocator.h(154): warning: missing return statement at end of non-void function \"tensorflow::Allocator::RequestedSize\"\n\ngcc: error trying to exec 'as': execvp: No such file or directory\nERROR: /home/lonny/tensorflow/tensorflow/core/kernels/BUILD:1673:1: output 'tensorflow/core/kernels/_objs/batch_space_ops_gpu/tensorflow/core/kernels/spacetobatch_functor_gpu.cu.o' was not created.\nERROR: /home/lonny/tensorflow/tensorflow/core/kernels/BUILD:1673:1: not all outputs were created.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 1092.740s, Critical Path: 902.88s\n\nOperating System:Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: \nCUDA toolkit 8.0rc\n\nhow can I solve the error?\n", "comments": ["@lonny1992: we don't yet officially support CUDA 8.0, I think.  Can you try to compile with 7.5?\n", "It looks like you don't have a working assembler. Can you compile a simple C program successfully? i.e.\n\n``` c\ncat > test.c <<EOF \nint main(){return 0;}\nEOF\ngcc test.c\n./a.out\n```\n", "Closing automatically due to lack of recent activity. Please reopen when further information becomes available. Thank you.\n"]}, {"number": 4724, "title": "Fix python bath in python_config script", "body": "When runing `./configure`, two places in the script `python_config` try to invoke `python` but don't use the python-related environment variable `PYTHON_BIN_PATH`. So it will cause the error that can't find python during configuration.\n", "comments": ["Can one of the admins verify this patch?\n", "@viirya, thanks for your PR! By analyzing the history of the files in this pull request, we identified @itsmeolivia, @vrv and @meteorcloudy to be potential reviewers.\n", "@itsmeolivia could you take a look? Thanks.\n", "Jenkins, test this please\n"]}, {"number": 4723, "title": "How to convert a csv file to TFrecord tensorFlow format?", "body": "Hello everybody i need to convert a csv file to TFrecord for TensorFlow. I really appreciate your help.\nAn example of csv file that  i need to convert is:\nCol1 Col2 Col3 Col4 Target\n2.56 0.98 0.45  7.8    0.189\n3.10 5.78  4.78 9.0    0.78\n....\nThank you very much!!!\n", "comments": ["This is suitable for StackOverflow.\n"]}, {"number": 4721, "title": "Make Image Summary in tensorboard more dynamic", "body": "The image summary in Tensorboard is very nice, it's the most static tab on Tensorboard.\nFor example, when I am doing a visual test of my auto-encoder, for example MNIST, I see this:\n![image](https://cloud.githubusercontent.com/assets/7721540/19021497/5d761600-88c3-11e6-90c8-85f8049f0a14.png)\n\nThen I wait a few minutes, and I refresh and it shows:\n![image](https://cloud.githubusercontent.com/assets/7721540/19021501/7187f2d0-88c3-11e6-88f1-8374dbd48356.png)\n\nSo it works. But I would like to be able to somehow review the progress, other than my current F5-stategy.\nI recon all image summaries are saved inside, but I cannot obtain them through Tensorboard.\n", "comments": ["TensorBoard does store many images per tag, but as you point out the UI only surfaces the most recent one.\n\nThis could be improved by adding some UI for loading older images in the sequence - maybe a slider on the left that changes which image gets loaded by each image-loader, e.g. if the slider is set to the far left it loads the earliest image, if it is set to far right it loads the latest image.\n\nI don't think the TensorBoard team will prioritize this right now, but we would gladly advise on a pull request if you (or anyone else) wants to add this feature.\n\nAlso,  you can use the reload icon in the top-left rather than F5.\n", "F5 and reload does not seem to be very smooth though - probably a caching\nissue where flushed data appears (with reloading) only after some time.\n\nCan you give some pointers where to start? i might look into this one over\nthe weekend.\n\nOn Monday, 3 October 2016, Daniel W Mane notifications@github.com wrote:\n\n> TensorBoard does store many images per tag, but as you point out the UI\n> only surfaces the most recent one.\n> \n> This could be improved by adding some UI for loading older images in the\n> sequence - maybe a slider on the left that changes which image gets loaded\n> by each image-loader, e.g. if the slider is set to the far left it loads\n> the earliest image, if it is set to far right it loads the latest image.\n> \n> I don't think the TensorBoard team will prioritize this right now, but we\n> would gladly advise on a pull request if you (or anyone else) wants to add\n> this feature.\n> \n> Also, you can use the reload icon in the top-left rather than F5.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4721#issuecomment-251214553,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHXSRKzdXuNQHrUP0xaLKsav1EdrKRYyks5qwWOSgaJpZM4KMCDj\n> .\n", "We just added a new feature to the image dashboard that lets you load older images by dragging a slider around. It should be in the next major release of TensorFlow after 1.0. Closing the issue since work is complete (although it will be a little while until the feature is available)", "@dandelionmane \r\n> We just added a new feature to the image dashboard that lets you load older images by dragging a slider around. It should be in the next major release of TensorFlow after 1.0. Closing the issue since work is complete (although it will be a little while until the feature is available)\r\n\r\n\r\nHas this feature been released yet?\r\n", "Update: This feature has been released in TensorFlow 1.1.0-rc0 (released 5 days ago).\r\n\r\nThanks to all contributors.\r\n", "I installed 1.1.0-rc1, yet don't see any slider in the image view.  Is there something specific needed to enable it?"]}, {"number": 4720, "title": "check_version: fails if native.bazel_version is undefined", "body": "This feature is in Bazel since 0.2.1, TensorFlow requires 0.3.0\nat least and should error out if the feature is not present.\n\nAlso give a warning when the version number is not present\n(when Bazel is built from HEAD) for better debugging.\n\nSee http://stackoverflow.com/questions/39811568/error-function-repository-rule-does-not-exist-while-trying-to-compile-tensor.\n", "comments": ["Can one of the admins verify this patch?\n", "@damienmg, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ilblackdragon, @vrv and @keveman to be potential reviewers.\n", "I would also suggest @gunan :)\n\nAlso if I ask, do I get ci to test it:\nJenkins test this please\n"]}, {"number": 4719, "title": "Correct height/width ordering to mnist docs", "body": "Not that it matters much, but I believe these two are swapped. Correct me if I'm wrong, but when convolving in only one dimension, I had to reverse the two in order to get good classification for a dataset.\n", "comments": ["@mazinbokhari, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @vrv to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 4718, "title": "i can't import tf.python.ops", "body": "Python 3.5.2 (default, Oct  2 2016, 13:49:15)\n[GCC 5.4.0 20160609] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n/>>> import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n/>>> tf.**version**\n'0.11.0rc0'\n/>>> tf.python.ops\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: module 'tensorflow.python' has no attribute 'ops'\n/>>> tf.python.platform\n<module 'tensorflow.python.platform' from '/home/zhusf/lib/python3.5/site-packages/tensorflow/python/platform/__init__.py'>\n/>>> tf.python.framework\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: module 'tensorflow.python' has no attribute 'framework'\n/>>> tf.python.summary\n<module 'tensorflow.python.summary.summary' from '/home/zhusf/lib/python3.5/site-packages/tensorflow/python/summary/summary.py'>\n/>>> tf.python.training\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: module 'tensorflow.python' has no attribute 'training'\n", "comments": ["This is not typical usage pattern from a python shell.  To use an op, directly use `tf.<op name>(...)`.  To, say, use something from the `training` package, you can do `tf.train.Saver`.\n\nIf you are adding/modifying TensorFlow source code -- instead of operating from within a shell -- there's plenty of examples on how to properly import.\n"]}, {"number": 4717, "title": "Adding missing module", "body": "In the library testing section the PIL module was missing\n", "comments": ["@ultraklon, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vincentvanhoucke, @Jakobovski and @lukas-krecan to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "What's the problem exactly? There is already:\n`from IPython.display import display, Image`\n", "I tried unistalling pillow and running the steps again and they work\nI could not reproduce the problem, don't mind me\n"]}, {"number": 4716, "title": "small typo fix", "body": "\"matlotlib\" should be \"matplotlib\"\n", "comments": ["Can one of the admins verify this patch?\n", "@ultraklon, thanks for your PR! By analyzing the history of the files in this pull request, we identified @lekaha, @vincentvanhoucke and @Jakobovski to be potential reviewers.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4715, "title": "shapes incompatible after upgrade", "body": "After upgrade from 0.10.0 to 0.11.0rc0 my code no longer runs:\n\n```\n(802, 277)\n(802, 1)\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\nTraceback (most recent call last):\n  File \"brain.py\", line 47, in <module>\n    classifier.fit(x=trainX, y=trainY, steps=2000)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 435, in fit\n    max_steps=max_steps)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 333, in fit\n    max_steps=max_steps)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 662, in _train_model\n    train_op, loss_op = self._get_train_ops(features, targets)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 963, in _get_train_ops\n    _, loss, train_op = self._call_model_fn(features, targets, ModeKeys.TRAIN)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 944, in _call_model_fn\n    return self._model_fn(features, targets, mode=mode, params=self.params)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 258, in _dnn_classifier_model_fn\n    weight=_get_weight_tensor(features, weight_column_name))\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py\", line 329, in sigmoid_cross_entropy\n    logits.get_shape().assert_is_compatible_with(multi_class_labels.get_shape())\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 750, in assert_is_compatible_with\n    raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\nValueError: Shapes (?, 1) and (?,) are incompatible\n```\n\nMy code:\n\n```\n...\nprint shape(trainX) # (802, 277)\nprint shape(trainY) # (802, 1)\nclassifier.fit(x=trainX, y=trainY, steps=2000)\n...\n```\n\nThis worked fine on 0.10.0. Notice the shapes in the beginning of output (looks like a valid combination to me).\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nnone\n### Environment info\n\nOperating System: Fedora 23, TF 0.11.0rc0\n", "comments": ["I would say no (I like hard typing). Easily fixed as you can always reshape an (?,1) rank 2 tensor into (?,) with a reshape op. \n", "Can you please elaborate on what the problem is? In the numpy world at least the matrices of shapes (802, 277) and (802, 1) or alternatively (802, 277) and (802, ) vector seem reasonable to work with without reshaping. None of these works for me here. Also please provide the reshape fix so that I can verify. Thanks.\n", "Minimalistic reproducer:\n\n```\nimport tensorflow as tf\nfrom numpy import *\n\ntrainX = mat([[1,1], [0,0]])\ntrainY = array([1,0])\n\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=2)]\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10])\n\nclassifier.fit(x=trainX, y=trainY, steps=2000)\n\n```\n", "I don't know what changes in tf.learn between 0.10 and 0.11rc0 that could cause this.  @martinwicke: can you take a look?\n", "I've run into this as well.  It appears to be related to changes made specifically for n_classes=2.  Setting n_classes to 3 in jharting's example will enable it to run.\n", "This should have been fixed by a recent commit. Sorry for the trouble. Please let me know if the problem persists at head.\n", "Found this issue via this [StackOverflow issue](https://stackoverflow.com/questions/40257607/tf-contrib-learn-quickstart-changing-n-classes-to-2-does-not-work).\n\nThe issue still persists in https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc1-cp35-cp35m-linux_x86_64.whl \n\nAlso, the nightly builds return 404 (https://github.com/tensorflow/tensorflow#installation). The last successful nightly build seems to work though (https://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastStableBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-cp35-cp35m-linux_x86_64.whl), but it is rc0?\n\nConfused of what to use.\n", "We're updating the version, so nightlies `__version__` may be confusing now. `__git_commit__` should be more informative. The fix is very recent, so it won't be in any of the 0.11 versions. It will be 0.12.\n"]}, {"number": 4714, "title": "TensorBoard Feature Request: [DASHBOARD] Static Variable Display Tab", "body": "I do a lot of different runs which contain descriptions as well as different meta parameters.  It would be nice to have a tab on TensorBoard so I can tell which experiment I am viewing.\n\nMy thought would be to simply add static variables from the Graph which might include Strings, Ints, Floats, (maybe matrix in table format if possible, like PCA parameters, maybe confusion matrix, etc).  These may include a text description, current learning rate, network meta parameters, etc.  It would be good to simple present the \"current\" value of these variables so if there is a search on Discount Rates for RL or variation such as learning rate, those can be viewed.  This would be different from simply graphing the Learning rate as that shows the history rather than \"current\".  I think of this as a TensorFlow \"Dashboard\" that I can setup.\n\nSomething like:\ntf.dashboard_summary(tags, tensor/value, collections=None, name=None)\n\neg:\ntf.dashboard_summary(\"Description:\",description_string_tensor_or_python_string)\ntf.dashboard_summary(\"Learning Rate:\",lr_tensor)\ntf.dashboard_summary(\"DiscountRate:\",discountRate_python_variable)\n\nIn this case, a \"DASHBOARD\" tab on tensorflow would contain the 3 labels above and the tensor/value.  If the value is a python variable, then it should be considered constant and will not change over the graph lifecycle.  If it is a tensor/variable, then it should be pulled from the graph at each iteration.\n\nI can help but not sure where to jump in to get this started.\n", "comments": ["Another addition that might be nice to add would be a COLLECTIONS tab.  Simply display the names of the Operation & Tensors within all the collections.  This should include Tensor Size, etc, whatever can be extracted from the graph.  This would be a quick reverence to the topology of the graph and helpful if you go back to a different structure and looking to see what the Network looks like or what the input variable names are for Serving, etc.\n", "Is there anything I can do to help with this request?\n", "Sorry for the slow response, and thanks for the suggestion. \nI have some questions:\n1. The \"dashboard\" tab - would it show one run at a time (like the graph page) or would it be showing multiple runs simultaneously?\n2. It sounds like some of the information it shows would be constant for the run (e.g. description) and some would be variable (e.g. learning rate). So we could just re-use the existing data input for the variable ones, like the learning rate, right? But we would need to add some metadata to the summary so we know that we want it to display in the dashboard tab. \n3. Can you maybe submit an image of a mock up, so I have a clearer idea of what you have in mind? A hand-drawn sketch would be fine.\n\nThis is pretty well-aligned with features and data sources we've wanted to add to TensorBoard. If you're interested in working on this we can discuss in a lot more detail the work that will go into this (maybe with a video chat). Do you have any frontend development experience / are you interested in getting your hands dirty with frontend development? \n", "1) I am not that familiar with multiple runs as I haven't managed to get that working.  But my feeling would be to \"select\" the visible run and if un-selected use the \"latest\" run to show.  For me, a single run is fine as I am trying to find the meta-parameters and a description of the experiment as I have a few running in parallel and it takes time to figure out which is which.\n\n2) That is correct, it might be good to \"offer\" collections of data such as weights, gradients, etc.  These would be the \"latest\" values, and it would be good to view them in the browser if small in size/clipped to a max to maintain performance, and offer a \"csv\" export of the variables for excel or some other means to view.  It would be good if possible to render a 2D image of the variable for 2D data to help visualize what is going on within the graph.\n\n3) I am hoping to get TensorBoard running again soon as it broke a while ago to get a screen to do the mock-up.  My understanding is there is a roll-back in progress which should fix this for now.  I will work on that once I get the fix.\n\nIt has been 5 years or so since I have worked on HTML so a little rusty on getting the data to look nice in a web browser, but I can help get the data that far (and make a stab at it).  My feeling is there is a lot of data stored in the logs, maybe access \"check points\" as well to get real-time training data. \n", "I made a first shot at the DASHBOARD page.  The text in the summary should be aligned but Paint didn't help me there so I drew a gray line to show they are all aligned (line optional).\n\nHopefully this give you a good idea of what is in my mind with this request.  Tensors might be exportable or can be viewed in a grid on a separate page if they are larger than a certain size (for performance).  Rank 1 and 2 tensors should be able to be viewed & exported this way.  The same feature should exist for the COLLECTION browser.  I didn't see a gradient collection so that might need to be added manually to the collection.\n\n![dashboard concept](https://cloud.githubusercontent.com/assets/18412448/19873583/3614a74a-9f96-11e6-82b8-9495883f33e7.png)\n", "HI @danmane,\n\nJust checking back in to see if the image above helped you with my thoughts on the \"Dashboard\" approach.\n\nRegards,\nGreg\n", "Hey greg, thanks for the mock up.\r\n\r\nThis does look like a good fit for the TensorBoard plugin system, and for some additional run metadata that we are planning to add that could populate the Description, Revision, Topology, fields, etc. I could imagine something like this being the default starting page for TensorBoard.\r\n\r\nHowever, on our end we won't be ready to really start integrating plugins like this until the end of January at the earliest. Do you mind if we put this issue on hold for a few months while we get things ready on the TensorBoard side?", "Hi @danmane ,\r\n\r\nThat is fine, I just wanted to make sure the topic didn't get dropped due to lack of interest.  I will have more time in January as well if you want some help.\r\n\r\n", "Ping @dandelionmane.", "+1", "Migrating this to the TensorBoard repo."]}, {"number": 4713, "title": "How to close parameter server with python?", "body": "In the TensorFlow documentation, the tf.train.Server.join() will block forever, but I would like to close it after training. Are there any possible solutions for it?\n\nThanks.\n", "comments": ["Maybe you can create a Supervisor which you then monitor from a separate thread? If you get a should_stop you exit the entire process. I have not tried it myself but could be worth trying.\n", "This should be an easy question for @mrry to answer.\n", "There's no built-in API for shutting down a TensorFlow server remotely; the assumption is that whatever mechanism (e.g. a cluster manager) you used to start the processes would also kill them after the job completes.\n\nIf you're feeling adventurous, you could try using TensorFlow coordination mechanisms, like a `tf.FIFOQueue` to signal when training is complete. The basic idea would be to (1) create a _shared_ queue on one of the PS tasks; (2) on each of the PS tasks, run a dequeue op on that queue instead of calling `server.join(); (3) at the end of training enqueue_many N elements into the queue (where N is the number of parameter servers). If you did that, the PS tasks would all unblock when training was done, and exit.\n", "This looks like a satisfactory answer to me, so I'm going to close this out. Let me know if you want me to reopen this @LiamHe.\n", "Would you reopen this issue, please?  In our cluster setup (LSF+MPI shared by many different groups), it requires extra efforts to monitor a TF job in order to stop it after training.  A simple wait-then-close method for parameter server would be much appreciated because it increases ease-of-use a lot, especially in a shared cluster where parameter servers should be closed after every single job.  Thank you.\r\n\r\nFor your reference, this issue is brought up again here: https://github.com/tensorflow/ecosystem/issues/19.", "There's a work-around -- instead of `server.join()` have your parameter servers do `sess.run(queue.dequeue)` `k` times if there are `k` workers. Then each worker does an `enqueue` at the end of it's work. Here's a full example, search for `create_done_queues` [here]( https://gist.github.com/yaroslavvb/ea1b1bae0a75c4aae593df7eca72d9ca). An alternative solution was started in this pr -- https://github.com/tensorflow/tensorflow/pull/6185", "Hmm, that's a nice option. @martinwicke FYI, possibly worthwhile doing in tf.learn by default?", "@jhseu this is not a reasonable default since workers can in some settings restart unpredictably during training", "@alextp @jhseu How about adding it as an option, disabled by default?  The user may turn it on if desired.", "Stop ps server gracefully is a requirement when run distributed training with [kubernetes batch job](https://kubernetes.io/docs/user-guide/jobs/). Anyone can write a detail demo base on [mnist_replica](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py)? @mrry @jart ", "@hustcat http://stackoverflow.com/questions/39810356/shut-down-server-in-tensorflow/40186129#40186129", "I write a demo for MNIST_data, and it seems run OK. See [dist_fifo](https://github.com/hustcat/tensorflow_examples/blob/master/mnist_distributed/dist_fifo.py)."]}, {"number": 4712, "title": "Add 3.7 to list of default compute capabilities", "body": "The Tesla K80 has a compute capabilities of 3.7. These are the cards in [AWS' new `p2` GPU offering](https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/) and in [Azure's GPU offering](https://azure.microsoft.com/en-us/blog/azure-n-series-preview-availability/) which means that usage of these cards will be quite widespread.\n\nGiven that there are some difference between CC 3.5 and 3.7, it would be ideal to have builds that are able to use the newer features / settings in 3.7.\n", "comments": ["@zheng-xq: WDYT?\n", "Adding a default GPU config has significant impact on build time. Since 3.5 and 3.7 are largely compatible, it is better to keep the default config number low. Unless we see a non-trivial change in performance, we'd prefer to leave the current setting unchanged. Power users can still use configure to choose their favorite compute capabilities. \n", "How hard would it be to generate wheels for different CCs as part of the CI/CD process?\n", "/cc @gunan on the release question above\n", "Without going into specifics, I can say that each new configuration to maintain is an added burden across the board, in terms of build, release and maintenance.\nTherefore, in terms of released packages we would rather create packages that work for most.\nIf a user needs a specific build that would perform best on their setup, they likely also need to add other compiler flags based on which CPU they are using, such as \"-mavx\". \nRather than supporting a cross product of different CPU and GPU configurations, we prefer asking users to build their images, and provide support for build issues.\n"]}, {"number": 4711, "title": "change syntax of curl & tar", "body": "Was throwing syntax error on osx 10.10\n\n```\nError: tensorflow/contrib/makefile/download_dependencies.sh: line 51: syntax error near unexpected token `<'\n```\n#4710\n", "comments": ["@unsalted, thanks for your PR! By analyzing the annotation information on this pull request, we identified @jart, @martinwicke and @tensorflower-gardener to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thank you for your contribution.\n\nMr. Jenkins: test this please\n"]}, {"number": 4710, "title": "download_dependencies.sh throws error on osx 10.10 - syntax issue", "body": "I receive this error on osx 10.10\n\n```\n$ sh tensorflow/contrib/makefile/download_dependencies.sh\ntensorflow/contrib/makefile/download_dependencies.sh: line 51: syntax error near unexpected token `<'\n```\n\nIt was fixed by changing line 51 to: \n\n```\ncurl -Ls \"${url}\" | tar -C \"${dir}\" --strip-components=1 -xz\n```\n", "comments": ["PR #4711 \n", "Fixed with PR #4711\n"]}, {"number": 4709, "title": "Is tensorflow consuming much more memory than torch?", "body": "I am trying to replicate the stacked hourglass architecture which is implemented in torch, https://github.com/anewell/pose-hg-train/. However, I am unable to train it using settings in the opts.lua file which set the train batch size as 6, number of train iteration as 8000, test batch size as 1 and number of test iteration as 1000. I understand that I can reduce the batch size or iteration number to fit the network. However, I just want to know if tensorflow is consuming more memory than torch, if the same architecture and settings can be implemented in tensorflow.\n", "comments": ["They have similar memory usage in general, although there are some special cases where it may differ substantially. You can see track down memory usage by looking at memory allocator statistics you get when running with FULL_TRACING options (see https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659\n)\n", "do you what are the special cases?\n", "calling `tf.gradients` on graph which has `tf.while` loop can take unexpectedly large amounts of memory\n", "@yxchng : Were you able to obtain memory allocator statistics mentioned above? \n", "@yaroslavvb : can you elaborate on using FULL_TRACING to get memory allocator stats?\nI managed to get the timing information via:\n\n``` python\nfrom tensorflow.python.client import timeline\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\nrun_metadata = tf.RunMetadata()\nsess.run(some_stuff, options=run_options, run_metadata=run_metadata)\n\n# for visualization\ntl = timeline.Timeline(run_metadata.step_stats)\nctf = tl.generate_chrome_trace_format()\nwith open('timeline.json', 'w') as f:\n    f.write(ctf)\n```\n\nBut how would I assess the memory allocation from `run_metadata`?\n", "If you print run_metadata, you'll see messages like this:\n\n```\n  output {\n    tensor_description {\n      dtype: DT_FLOAT\n      shape {\n      }\n      allocation_description {\n        requested_bytes: 4\n        allocator_name: \"cpu\"\n        ptr: 4493467328\n      }\n    }\n  }\n```\n\nThat means 4 bytes got allocated\n\nOn Thu, Oct 20, 2016 at 5:17 AM, redst4r notifications@github.com wrote:\n\n> @yaroslavvb https://github.com/yaroslavvb : can you elaborate on using\n> FULL_TRACING to get memory allocator stats?\n> I managed to get the timing information via:\n> \n> from tensorflow.python.client import timeline\n> run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n> run_metadata = tf.RunMetadata()\n> sess.run(some_stuff, options=run_options, run_metadata=run_metadata)\n> \n> # for visualization\n> \n> tl = timeline.Timeline(run_metadata.step_stats)\n> ctf = tl.generate_chrome_trace_format()with open('timeline.json', 'w') as f:\n>     f.write(ctf)\n> \n> But how would I assess the memory allocation from run_metadata?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4709#issuecomment-255089531,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHAUCSGUxmaSCC8P-Wonypmt8AT3nks5q11vVgaJpZM4KLzWI\n> .\n", "You can also get raw memory allocator messages in your log like this:\nhttp://stackoverflow.com/questions/36331419/tensorflow-how-to-measure-how-much-gpu-memory-each-tensor-takes/36505898#36505898\n\nOn Thu, Oct 20, 2016 at 10:17 AM, Yaroslav Bulatov yaroslavvb@gmail.com\nwrote:\n\n> If you print run_metadata, you'll see messages like this:\n> \n> ```\n>   output {\n>     tensor_description {\n>       dtype: DT_FLOAT\n>       shape {\n>       }\n>       allocation_description {\n>         requested_bytes: 4\n>         allocator_name: \"cpu\"\n>         ptr: 4493467328\n>       }\n>     }\n>   }\n> ```\n> \n> That means 4 bytes got allocated\n> \n> On Thu, Oct 20, 2016 at 5:17 AM, redst4r notifications@github.com wrote:\n> \n> > @yaroslavvb https://github.com/yaroslavvb : can you elaborate on using\n> > FULL_TRACING to get memory allocator stats?\n> > I managed to get the timing information via:\n> > \n> > from tensorflow.python.client import timeline\n> > run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n> > run_metadata = tf.RunMetadata()\n> > sess.run(some_stuff, options=run_options, run_metadata=run_metadata)\n> > \n> > # for visualization\n> > \n> > tl = timeline.Timeline(run_metadata.step_stats)\n> > ctf = tl.generate_chrome_trace_format()with open('timeline.json', 'w') as f:\n> >     f.write(ctf)\n> > \n> > But how would I assess the memory allocation from run_metadata?\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > https://github.com/tensorflow/tensorflow/issues/4709#issuecomment-255089531,\n> > or mute the thread\n> > https://github.com/notifications/unsubscribe-auth/AABaHAUCSGUxmaSCC8P-Wonypmt8AT3nks5q11vVgaJpZM4KLzWI\n> > .\n", "thanks for the hint, I'll play around with that\n", "found this very convenient `show_memory` flag, which allows you to display mem-allocation over time in chrome-trace:\n\n``` python\nctf = tl.generate_chrome_trace_format(show_memory=True)\n```\n", "@yxchng have you come to any actionable bugs or issues with memory usage after looking at it more?\n", "@aselle I am busy with other stuff at the moment. I will try to look into it as soon as possible. Really sorry.\n", "Automatically closing due to lack of recent activity, we will reopen if further information becomes available. Thanks!\n"]}, {"number": 4708, "title": "Documentation for creating language bindings and shape inference", "body": "Request to merge documentation updates from master into the 0.11 release branch.\n\n@vrv - I took the liberty to include your change.\n", "comments": ["@asimshankar, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @wang-yang to be potential reviewers\n", "@gunan \n"]}, {"number": 4707, "title": "Create API for creating slot variables", "body": "To create or modify Adadelta, or Adam requires editing the Eigen tensor implementations of each.  However if it were possible to create a slot variable from tensorflow then one could perform tests without needing to recompile the whole tensorflow library.\n", "comments": ["Have you looked into @miyosuda s implementation of Shared RMSProp?\n", "@hholst80 can you provide a link to it?  I don't know from which repo his implementation is in.\n", "Automatically closing due to lack of recent activity, we will reopen when further information becomes available.\n"]}, {"number": 4706, "title": "0.11.rc0 version, using all gpu's memory while running on only one", "body": "With 0.11.rc0 tf, cudnn 7.5\n+------------------------------------------------------+  \n| NVIDIA-SMI 352.39     Driver Version: 352.39         |  \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K40m          On   | 0000:03:00.0     Off |                    0 |\n| N/A   47C    P0   127W / 235W |  10995MiB / 11519MiB |     87%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K40m          On   | 0000:04:00.0     Off |                    0 |\n| N/A   39C    P0    63W / 235W |  10953MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K40m          On   | 0000:83:00.0     Off |                    0 |\n| N/A   40C    P0    62W / 235W |  10954MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K40m          On   | 0000:84:00.0     Off |                    0 |\n| N/A   39C    P0    62W / 235W |  10953MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      8828    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2 10964MiB |\n|    1      8828    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2 10923MiB |\n|    2      8828    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2 10923MiB |\n|    3      8828    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2 10923MiB\n\nWith 0.10.0 tf, same code running with gpu status below\n| NVIDIA-SMI 352.39     Driver Version: 352.39         |  \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K40m          On   | 0000:03:00.0     Off |                    0 |\n| N/A   46C    P0   129W / 235W |  10993MiB / 11519MiB |     89%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K40m          On   | 0000:04:00.0     Off |                    0 |\n| N/A   38C    P0    63W / 235W |    108MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K40m          On   | 0000:83:00.0     Off |                    0 |\n| N/A   39C    P0    62W / 235W |    107MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K40m          On   | 0000:84:00.0     Off |                    0 |\n| N/A   38C    P0    62W / 235W |    107MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     20051    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2 10959MiB |\n|    1     20051    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2    74MiB |\n|    2     20051    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2    74MiB |\n|    3     20051    C   ...mpiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2    74MiB \n", "comments": ["That's by design, TensorFlow grabs all GPUs it sees -- http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n", "@yaroslavvb I see thanks, but I don't think it need to grab all gpu memories if only running on one.\nJust grab one gpu memory as version 0.10.0 did is fine.\nAnyway can force to grab one gpu memory by using CUDA_VISIBLE_DEVICES.\n", "@chenghuige: what do you mean by \"only running on one\"?  To me, this means setting `CUDA_VISIBLE_DEVICES`.  If you instead mean that you use `with tf.device(\"/gpu:0\")` exclusively in your code, then when initializing the TensorFlow runtime, it would have no way to _know_ just one GPU is going to be used.\n", "@concretevitamin  well I think as in version 0.10, it will launch only one GPU, unless your code is like models/image/cifa10/\nto explicitly set using more than 1\n  for i in xrange(FLAGS.num_gpus):\n      with tf.device('/gpu:%d' % i):\n", "You will get best performance by partitioning your variables manually across GPUs in a way that makes sense.\n", "I think the problem is that there's an Eigen scratch allocator in the GPUDevice constructor that allocates memory on the GPU.  Before r0.11, the call was directly calling cuMalloc instead of going through our allocator, and so although some memory was used on the GPU, it wasn't going through our allocator so the whole thing wasn't being taken.  But to properly track memory usage, we make all of our calls now go through the allocator, so GPUDevice construction now unfortunately allocates all the memory.\n\nTwo solutions: \n\n1) We lazily allocate the scratch allocator space instead of eagerly doing it in the constructor (not sure where / when we would do this though).\n\n2) You can set the 'allow_growth' option in ConfigProto.GPUOptions so that the initial allocation doesn't cause all of the GPU memory to be allocated.\n", "No response for 10 days.\nAlso looks like this is working as intended.\nTherefore closing the issue.\n"]}, {"number": 4705, "title": "Error building with cuda after bazel server dies", "body": "This is effectively reopening #4105 as this is another way to reproduce the issue that I am seeing repeatedly. The fix for #4105 was to make sure after running configure clean+fetch is run.\n\nI see this issue almost every day so I'm not sure if bazel flushes some caches or the daemon is dying, but the issue reliably reproduces by killing the bazel daemon. Having to re-run configure and restart the build from scratch is unreasonable, since there is a workaround that avoids a rebuild so this looks very much like a bug.\n\n```\n$ bazel version \n.\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n```\n\nSteps to reproduce and \"fix\":\n\n_NB_: $bazel_build_dir is something like `~/.cache/bazel/_bazel_foo/6e3dd6b174494dc75d93de11da03e7e7`\n\n``````\n# Run ./configure, enabling GPU support.\n\n# The crosstool BUILD file looks good now and building with --config=cuda works\ntensorflow$ cat $bazel_build_dir/external/local_config_cuda/crosstool/BUILD\nlicenses([\"restricted\"])```\n\npackage(default_visibility = [\"//visibility:public\"])\n\ncc_toolchain_suite(\n    name = \"toolchain\",\n    toolchains = {\n        \"local|compiler\": \":cc-compiler-local\",\n        \"darwin|compiler\": \":cc-compiler-darwin\",\n    },\n)\n\ncc_toolchain(\n    name = \"cc-compiler-local\",\n    all_files = \":empty\",\n    compiler_files = \":empty\",\n    cpu = \"local\",\n    dwp_files = \":empty\",\n    dynamic_runtime_libs = [\":empty\"],\n    linker_files = \":empty\",\n    objcopy_files = \":empty\",\n    static_runtime_libs = [\":empty\"],\n    strip_files = \":empty\",\n    supports_param_files = 0,\n)\n\ncc_toolchain(\n    name = \"cc-compiler-darwin\",\n    all_files = \":empty\",\n    compiler_files = \":empty\",\n    cpu = \"darwin\",\n    dwp_files = \":empty\",\n    dynamic_runtime_libs = [\":empty\"],\n    linker_files = \":empty\",\n    objcopy_files = \":empty\",\n    static_runtime_libs = [\":empty\"],\n    strip_files = \":empty\",\n    supports_param_files = 0,\n)\n\nfilegroup(\n    name = \"empty\",\n    srcs = [],\n)\n\n# Backup the crosstool directory so it can be restored later\nmkdir -p /tmp/crosstool\ncp -aR $bazel_build_dir/external/local_config_cuda/crosstool/* /tmp/crosstool/\n\n# Kill bazel server.\nkill $(ps aux | awk '/baze[l]/ {print $2; exit}')\ntensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/...\n.\nERROR: $bazel_build_dir/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\n    File \"$bazel_build_dir/external/local_config_cuda/crosstool/BUILD\", line 4\n        error_gpu_disabled()\n    File \"$bazel_build_dir/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\n        fail(\"ERROR: Building with --config=c...\")\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\nUnhandled exception thrown during build; message: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@808aaea9' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@9a9c82e5', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@66ba3b8e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@97095481')\nINFO: Elapsed time: 0.543s\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@808aaea9' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@9a9c82e5', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@66ba3b8e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@97095481')\n    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)\n    at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalStateException: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by $bazel_build_dir/external/local_config_cuda/crosstool/BUILD\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:179)\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.findCrosstoolConfiguration(CrosstoolConfigurationLoader.java:239)\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.readCrosstool(CrosstoolConfigurationLoader.java:281)\n    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.createParameters(CppConfigurationLoader.java:128)\n    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:73)\n    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:48)\n    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction.compute(ConfigurationFragmentFunction.java:78)\n    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1016)\n    ... 4 more\nCaused by: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by $bazel_build_dir/external/local_config_cuda/crosstool/BUILD\n    at com.google.devtools.build.lib.packages.Package.makeNoSuchTargetException(Package.java:559)\n    at com.google.devtools.build.lib.packages.Package.getTarget(Package.java:543)\n    at com.google.devtools.build.lib.skyframe.SkyframePackageLoaderWithValueEnvironment.getTarget(SkyframePackageLoaderWithValueEnvironment.java:71)\n    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction$ConfigurationBuilderEnvironment.getTarget(ConfigurationFragmentFunction.java:193)\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:177)\n    ... 11 more\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@808aaea9' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@9a9c82e5', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@66ba3b8e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@97095481')\n    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)\n    at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalStateException: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by $bazel_build_dir/external/local_config_cuda/crosstool/BUILD\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:179)\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.findCrosstoolConfiguration(CrosstoolConfigurationLoader.java:239)\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.readCrosstool(CrosstoolConfigurationLoader.java:281)\n    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.createParameters(CppConfigurationLoader.java:128)\n    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:73)\n    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:48)\n    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction.compute(ConfigurationFragmentFunction.java:78)\n    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1016)\n    ... 4 more\nCaused by: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by $bazel_build_dir/external/local_config_cuda/crosstool/BUILD\n    at com.google.devtools.build.lib.packages.Package.makeNoSuchTargetException(Package.java:559)\n    at com.google.devtools.build.lib.packages.Package.getTarget(Package.java:543)\n    at com.google.devtools.build.lib.skyframe.SkyframePackageLoaderWithValueEnvironment.getTarget(SkyframePackageLoaderWithValueEnvironment.java:71)\n    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction$ConfigurationBuilderEnvironment.getTarget(ConfigurationFragmentFunction.java:193)\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:177)\n    ... 11 more\n\n# So now the crosstool dir has been trashed\ntensorflow$ cat $bazel_build_dir/external/local_config_cuda/crosstool/BUILD\n\nload(\"//crosstool:error_gpu_disabled.bzl\", \"error_gpu_disabled\")\n\nerror_gpu_disabled()\n\n# Eh? I didn't disable it! Bazel just died...\n# Let's restore from backup\ncp -aR /tmp/crosstool/* $bazel_build_dir/external/local_config_cuda/crosstool/\n# And restart the bazel server... Almost... Just doing \"bazel version\" isn't enough; the server restarts but at the next build, the same issue occurs and we need to restore the crosstool directory again.\n# Running this seems to do the trick:\ntensorflow$ bazel fetch //tensorflow/...\n# But the crosstool directory has been trashed again\ntensorflow$ cat $bazel_build_dir/external/local_config_cuda/crosstool/BUILD\n\nload(\"//crosstool:error_gpu_disabled.bzl\", \"error_gpu_disabled\")\n\nerror_gpu_disabled()\n\n# Restore once more again\ncp -aR /tmp/crosstool/* $bazel_build_dir/external/local_config_cuda/crosstool/\n# Now we can build fine!\n``````\n\nIn short, the workaround whenever the bazel server dies is, assuming you've backed up your crosstool directory in /tmp/crosstool/:\n\n```\nbazel fetch //tensorflow/...\ncp -aR /tmp/crosstool/* $bazel_build_dir/external/local_config_cuda/crosstool/\n```\n\nThis looks like a bug to me and the resolution of #4105 just doesn't seem related to this, it just coincidentally fixes it.\n", "comments": ["Unfortunately the simple workaround isn't enough as eventually the build fails with the error:\n\n```\nERROR: /data/software/machine-learning/tensorflow/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 115 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ntensorflow/stream_executor/cuda/cuda_blas.cc:22:36: fatal error: cuda/include/cublas_v2.h: No such file or directory\ncompilation terminated.\n```\n\nStill, just killing the bazel server shouldn't mean that the build should have to be done from scratch, surely?\n", "@davidzchen Do you have any thoughts on this?\n", "It seems that if I copy not just the crosstool directory but the whole local_config_cuda directory (ie. the parent), then the rebuild works.\n\nSo something is trashing local_config_cuda.\n\nI'm happy to try out various things if it'll help. I'm not sure exactly when this started but it was around the time I updated to cuda v8 - it was the RC at the time, installed on Ubuntu 15.04 via the deb installer but I'm now on Ubuntu 16.04 and the official cuda 8 deb taken from the nvidia site. Whether any of that is relevant is debatable - I tend to keep tensorflow up to date so that could have been the trigger.\n", "I have the same problem here. It hinders me from developing TensorFlow as it is really expensive to rebuild from scratch (**1000s for a 24 core machine!**). \n\n**Bazel Version**\n\n<pre>\nBuild label: 0.3.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\nBuild timestamp: 1475861110\nBuild timestamp as int: 1475861110\n</pre> \n\n**Error message I encounter when I try to build again several hours after the successful build.**\n\n<pre>\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: /home/ybw/.cache/bazel/_bazel_ybw/36e6bc7e62e4ec6066c451267a1c5751/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\n        File \"/home/ybw/.cache/bazel/_bazel_ybw/36e6bc7e62e4ec6066c451267a1c5751/external/local_config_cuda/crosstool/BUILD\", line 4\n                error_gpu_disabled()\n        File \"/home/ybw/.cache/bazel/_bazel_ybw/36e6bc7e62e4ec6066c451267a1c5751/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\n                fail(\"ERROR: Building with --config=c...\")\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\nERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/ybw/.cache/bazel/_bazel_ybw/36e6bc7e62e4ec6066c451267a1c5751/external/local_config_cuda/crosstool/BUILD.\n</pre>\n", "I just came across this issue after doing a reboot. Suddenly, I got the same error as darrengarvey. Looking into tensorflow/third_party/gpus/cuda_configure.bzl points out that a dummy repository is created 8_create_dummy_repository) when TF_NEED_CUDA=1 is not set. Setting and adding it to my .bashrc solves this for me. \n", "Confirming that @firolino 's solution alone was sufficient to resolve the `ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.` problem without rebuilding Tensorflow.\n_Edit:_\nHowever after I tried to load the custom_op.so with `tf.load_op_library`, I got the `tensorflow.python.framework.errors.NotFoundError: cuda_op_kernel.so: undefined symbol` error. I tried to hack through it but nothing worked.Therefore, I ended up reconfigure with ./configure at Tensorflow's root directory expecting to wait for another 20 minutes of rebuilding process, but this time the building process of the custom_op.so only took about 90 s, weird.\n", "@firolino - You're right that it seems like `TF_NEED_CUDA` needs to be exported in the environment for building with cuda to work. I don't quite see where bazel is picking this environment variable up. \n\nSeems it's just in at `third_party/gpus/cuda_configure.bzl:118:` which is loaded via `tf_workspace()` in `WORKSPACE`. I guess while the bazel server is alive it's doing some caching of the workspace which is why this doesn't show up all of the time.\n\nSo it looks a bit like a user (like me) actually needs to export this environment variable in order to build reliably, which isn't mentioned in any docs that I can see. I see `TF_NEED_CUDA=1` is manually put into the environment in `tensorflow/tools/ci_build/Dockerfile.gpu`\n\nAn alternative could be to store these variables in a local config file and reference them when loading `tf_workspace()`.\n\nThis doesn't look like it's needed for the other variables like `TF_NEED_GCP` or `TF_NEED_HDFS` because those values are written to `tensorflow/core/platform/default/build_config.bzl`. Following the pattern there is a one easy way to fix this, but the disadvantage of doing that is you end up with changes to files that are part of the source tree (a minor inconvenience).\n", "Well it's not quite that simple unfortunately. I fixed the first issue (see the [branch](https://github.com/darrengarvey/tensorflow/tree/fix-cuda-failure-when-bazel-dies)), but the next issue is down to all of the other environment variables that are expected to exist, such as `TF_CUDA_VERSION` and `CUDA_TOOLKIT_PATH`.\n\nAll fixable, but really it looks like either I'm missing something or the assumptions being made in `configure` regarding these environment variables aren't reasonable (namely, that bazel never dies). I haven't tried yet but the same issues possibly exist with other variables like `HOST_CXX_COMPILER`, `TF_NEED_SYCL`, etc. based on a brief look at how they are being used.\n", "+cc @damienmg \n\nSorry for the late reply.\n\n@darrengarvey Currently, the way the settings get propagated from `configure` to `cuda_configure()` is that the `configure` script exports a bunch of environment variables and then runs `bazel fetch //...` before it exits. When this `bazel fetch` invocation is run, it picks up the environment variables and sets up the `@local_config_cuda` workspace according to those settings. Then, when you run `bazel build`, because all of the external repositories have already been fetched, then TF will be built with `@local_config_cuda` set up based on the settings given to the `configure` script.\n\nWhen the Bazel server is killed in this case, this is equivalent to running the `configure` script without it running `bazel fetch` and then running `bazel fetch` or `bazel build` without any of the settings from the `configure` script, which results in `@local_config_cuda` set up without GPU support.\n\nThe eventual fix for this is to move the remaining CUDA configuration logic currently in the `configure` script into the `cuda_configure()` rule so that it will attempt to detect GPU support and the CUDA toolchain and runtime by default. @damienmg is working on [improvements to Skylark remote repositories](https://bazel-review.googlesource.com/c/6697/3/site/designs/_posts/2016-10-18-repository-invalidation.md) to both improve its caching invalidation strategy and enable passing environment variables to workspace rules via command line options. This way, we will be able to make `cuda_configure()` work more similar to the way the Autotools configure invocation works, where it will run its detection logic by default, but users will be able to override whether to build with GPU support or provide a custom CUDA installation directory via command line flags.\n", "I compiled Tensorflow on my Jetson TX1 for around 4 hours. Then the compilation was finished without compile errors, but it shows a strange error, that some artifacts are missing.\r\n\r\nI want to start compile process again with higher verbosity and suddenly I get the error:\r\n\r\n```\r\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support..\r\n``` \r\n\r\nWhich is complete nonsense, since I really switched on CUDA support during configure. And of course I will not run configure again, since I don't want to wait another 4 hours until the error with missing artifacts happens again.\r\nIs there any chance to restart the compile process where it stopped last time... with CUDA support?", "@ZahlGraf I believe it is the same issue with #4848.", "Hi, thanks for your answer.\r\n\r\nI did not find a method to recover from this situation, thus I had to wait again 4h.\r\nHowever I found a workaround, which helps me to recover from this issue in past situations:\r\n\r\n*Before* calling `./configure` I just set several environment variables in my shell:\r\n```\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_VERSION=8.0\r\nexport CUDA_TOOLKIT_PATH=/usr/local/cuda\r\nexport TF_CUDNN_VERSION=5.1.5\r\nexport CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu/\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=5.3\r\n```\r\n\r\nWith this setup, I was able to rebuild as many times as I want without running configure again.\r\n\r\nHere you can find some more informations about compiling TF-0.11 on Jetson TX1, the settings from above are also from this page:\r\nhttps://github.com/jetsonhacks/installTensorFlowTX1\r\n\r\n", "https://www.tensorflow.org/get_started/os_setup should be updated to tell people that bazel must not be run with --batch (and the bazel server not be killed or restarted in any other way) between configure and build. Also, this means that one must not switch to another build machine between these 2 steps.\r\n\r\nRelated: issues #4105 and #4841 ", "@jowagner While these are valid points, we hope these will go away with future bazel releases.\r\nI think one of the problems should be already fixed by bazel 0.4.5\r\n@damienmg can confirm", "With 0.4.5 and the --action_env change, there should be no issue about --batch or restarting the server (unless the change got reverted again). --action_env is basically fixing those environment variable in the rc file instead of relying on the environment to stick."]}, {"number": 4704, "title": "fix spelling errors in comments", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@kborer, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @zheng-xq and @keveman to be potential reviewers\n", "Thanks @kborer !\n"]}, {"number": 4703, "title": "Conditional print doesn't work appropriately as the control flow describe", "body": "I stucked with this problem. I want to check if gradients or variables are NaN before applying them. Print conditionally the name of the variable and a value for both gradient and variable. \n### Environment info\n\nOperating System: Linux\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root 189170 Jul 11 23:19 /opt/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jul 11 23:19 /opt/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jul 11 23:19 /opt/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jul 11 23:19 /opt/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jul 11 23:19 /opt/cuda/lib/libcudart_static.a\n### Tensorflow version:\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nBackend Qt5Agg is interactive backend. Turning interactive mode on.\n0.9.0\n### Bazel\n\nIf installed from source, provide \n\nExtracting Bazel installation...\nBuild label: 0.3.1- (@non-git)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed Aug 31 19:34:37 2016 (1472672077)\nBuild timestamp: 1472672077\nBuild timestamp as int: 1472672077\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nI reproduced with MNIST. I tried many different way in my code. Checked documentation, and try to explicitly restrict control flow, and still no success. The tf.print runs anyway, I didn't find anything that this is on purpose, or how to manage it. \n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\n# Import data\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport tensorflow as tf\n\nFLAGS = None\n\n\ndef main(_):\n    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n    # Create the model\n    x = tf.placeholder(tf.float32, [None, 784])\n    W = tf.Variable(tf.zeros([784, 10]))\n    b = tf.Variable(tf.zeros([10]))\n    y = tf.matmul(x, W) + b\n\n    # Define loss and optimizer\n    y_ = tf.placeholder(tf.float32, [None, 10])\n\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n    train_step = tf.train.GradientDescentOptimizer(0.5)\n    gvs = train_step.compute_gradients(cross_entropy)\n\n    gvs = [(tf.select(tf.is_nan(grad), grad, tf.Print(grad, [grad, var, tf.is_nan(grad), tf.is_nan(var)])), var) for grad, var in gvs]\n\n    minimizer = train_step.apply_gradients(gvs)\n\n    sess = tf.InteractiveSession()\n    # Train\n    tf.initialize_all_variables().run()\n    for _ in range(1000):\n        batch_xs, batch_ys = mnist.train.next_batch(100)\n        sess.run(minimizer, feed_dict={x: batch_xs, y_: batch_ys})\n\n    # Test trained model\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n                                        y_: mnist.test.labels}))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str, default='/tmp/data',\n                        help='Directory for storing data')\n    FLAGS = parser.parse_args()\n    tf.app.run()\n```\n### What other attempted solutions have you tried?\n## 1 with control flow restriction + replacing the condition with tf.greater or tf.is_finite, etc.\n\n```\n    new_gvs = list()\n    for grad, var in gvs :\n        # c = tf.greater(grad, 1)\n        c = tf.is_finite(var)\n        with tf.control_dependencies([c]):\n            newGrad = tf.select(c, tf.Print(grad, [grad, var, tf.is_nan(grad), c]), grad)\n            with tf.control_dependencies([newGrad]):\n                new_gvs.append((newGrad, var))\n    gvs = new_gvs\n```\n## 2 with is_nan 'var'\n\n```\ngvs = [(tf.select(tf.is_nan(var), tf.Print(grad, [grad, var, tf.is_nan(grad), tf.is_nan(var)]), grad), var) for grad, var in gvs]\n```\n### Logs or other output that would be helpful\n\nIt just logs everything, as if there were no condition.\n", "comments": ["Note that `tf.select()` does not guarantee conditional execution at all.  For that please use `tf.cond(pred, fn1, fn2, name=None)`, which is designed for this purpose.  Please feel free to re-open if you can't make `cond` work for this purpose (which I think should work). \n"]}, {"number": 4702, "title": "Fix typo in comment.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for spotting this @LaurentMazare \n"]}]