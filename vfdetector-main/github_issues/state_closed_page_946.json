[{"number": 25053, "title": "eager execution numpy issues", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12.0\r\n- Doc Link: https://www.tensorflow.org/tutorials/eager/eager_basics#numpy_compatibility\r\n\r\n\r\n**Describe the documentation issue**\r\nRunning the following code:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.enable_eager_execution\r\n#tf.enable_eager_execution()\r\n# Gives following erro:    ValueError: tf.enable_eager_execution must be called at program startup.\r\n\r\nndarray = np.ones([3, 3])\r\n\r\nprint(\"TensorFlow operations convert numpy arrays to Tensors automatically\")\r\ntensor = tf.multiply(ndarray, 42)\r\nprint(tensor)\r\n\r\n\r\nprint(\"And NumPy operations convert Tensors to numpy arrays automatically\")\r\nprint(np.add(tensor, 1))\r\n\r\nprint(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\r\nprint(tensor.numpy())\r\n```\r\n\r\nTraceback error:\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-33-4feddfe31f00>\", line 1, in <module>\r\n    runfile('****/t.py', wdir='****')\r\n\r\n  File \"C:\\Users\\****\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 705, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Users\\****\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"****/t.py\", line 21, in <module>\r\n    print(tensor.numpy())\r\n\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n\r\n1st issue: \r\nIn docs it says I should use \r\ntf.enable_eager_execution()\r\nBut if I use it I get following error:\r\nValueError: tf.enable_eager_execution must be called at program startup.\r\n\r\n2nd issue:\r\nDoc says I should get real tensorvalues for arrays, but I get:\r\n```\r\nTensorFlow operations convert numpy arrays to Tensors automatically\r\nTensor(\"Mul_10:0\", shape=(3, 3), dtype=float64)\r\nAnd NumPy operations convert Tensors to numpy arrays automatically\r\nTensor(\"add_10:0\", shape=(3, 3), dtype=float64)\r\n```\r\n\r\n3rd issue:\r\nBased on above Traceback issue:\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n\r\nI had to install tensorflow-eigen          1.12.0, since I could not run without it\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nNo\r\n", "comments": ["I tried on tf2.0preview-gpu, linux, no problem. and I tried on tf1.10-gpu have the same problem. Maybe tf1.x has not .numpy().", "Apparently I still had to use \r\n`with tf.Session() as session:`\r\nClosing."]}, {"number": 25052, "title": "[XLA] erf, lgamma, and others do not compute correct values in f16 mode", "body": "In https://github.com/google-research/bert/pull/255 @thorjohnsen noticed something was wrong with XLA's `erf` implementation in f16 mode.\r\n\r\nAt first I thought this was just a precision issue, but upon investigation (trying all f16 inputs) I think the problem is worse than this.  Instead, I notice that for ~4476 inputs to erf with large absolute value (i.e. close to +/- inf) over/underflow to nan instead of +/-1.  And with the exception of these nans, all other inputs to erf have small relative error (less than 0.1%).\r\n\r\nI notice a similar problem with lgamma, where negative numbers with large absolute value overflow to nan instead of inf.\r\n\r\nacosh has the opposite problem, many inputs which cmath returns nan for, we return -inf.  That's probably not as bad but still may be something we should fix.\r\n\r\nThe lgamma issue at least can be \"fixed\" by doing the computation in f32 (i.e. `static_cast<f16>(lgamma(static_cast<f32>(x))`) but I don't immediately think this is the right thing to do.  As a starting point, I need to understand the algorithms better.\r\n\r\nMany functions also have one or two wrong answers, e.g. when presented with +/-inf.  I suspect that the f32 versions have these problems too.  The f32 versions may also have the same under/overflow problems as the f16 versions, just at more extreme values.  I haven't tested yet.\r\n\r\nThe issue I'm observing broadly occurs on both CPU and GPU, although CPU in some cases gives slightly different errors (e.g. its acosh seems to be fine, but its asinh has a similar problem [?], returning finite values instead of nan).", "comments": ["cc @benbarsdell @majnemer @hawkinsp ", "(Off-topic, but I've been interested in fuzzing XLA, and this is a data point: All of these functions are implemented not as XLA primitives, and therefore they can't be checked by a fuzzer that simply generates random HLO and checks that it gives the right answer.)", "acosh is returning `inf` instead of `nan` because of sqrt term overflowing in fp16 https://github.com/tensorflow/tensorflow/blob/937ff1b4cf1d954806e075d66198429a6d2312be/tensorflow/compiler/xla/client/lib/math.cc#L306-L308. Then, the result of `log` argument is also `inf` (instead of negative number), and `inf` is returned. Obviously, the same thing happens with fp32, but it would require arguments on the order of 10^19, whereas for half 260 more or less is enough for x^2 to overflow.\r\nasinh is defined across the whole real domain, so it should never return `nan`. \r\nFor erf/erfc, if 2 intermediate values overflow in fp16 (say, `pp` and `pq`), result of an operation on them will be `nan`. Overflows are likely, because many of the coefficient values are large.  \r\nGenerally, due to fp16 limited dynamic range we try to not round intermediate results to fp16. But this becomes murkier for special functions where it's hard to come to conclusion at which granularity rounding should or should not occur. I think @benbarsdell's suggestion of following what Eigen does is a good one, and will ensure consistent results between core tensorflow and XLA. ", "Thanks for your analysis, @ngimel.\r\n\r\n> I think @benbarsdell's suggestion of following what Eigen does is a good one, and will ensure consistent results between core tensorflow and XLA.\r\n\r\nI'm pretty strongly in the \"two wrongs don't make a right\" camp.  I think the right approach is to prove that XLA's result is correct (within some stated contract) for all fp16 and fp32 values.  Then we can file bugs against Eigen if necessary.\r\n\r\nIOW XLA's goal is not to match Eigen bit-for-bit or bug-for-bug.  Indeed, bit-for-bit or bug-for-bug matching isn't feasible insofar as Eigen implements some of these functions by calling into the C++ standard library (e.g. for lgamma).", "(BTW I believe our lgamma is broken at least in part because we compute `sin(pi * x)`, which is nan for large values of x.)", "> I think the right approach is to prove that XLA's result is correct (within some stated contract) for all fp16 and fp32 values.\r\n\r\nTo be clear, perhaps the contract we end up with is the same as what's currently implemented by Eigen!  I suspect we can do better in at least some cases, though.  For example, we're not handling +/- inf properly in some of these functions, and I think we should if it's not too expensive (which I think it shouldn't be).  That would make us strictly better than a putative Eigen implementation that doesn't handle +/- inf, but that's a good thing I think.", "FYI I'm actively working on this.  I've improved f16 lgamma some and hope to look at erf soon.\r\n\r\nI'm not sure yet if we're going to end up simply applying the hammer of \"do all arithmetic in f32\", but I think it's useful to push f16 as far as we can without applying that hammer, because I suspect that most of the problems I'm finding when doing arithmetic in f16 will apply just as well to f32, and we don't want to apply the same hammer to f32 and do all the computation at f64 precision.  (At some point this gets out of hand... :smile:)", "`erf` for larger arguments goes through `erfc`, and `erfc` can be short-circuited at `Exp(-x *x)`, which, for halfs, will evaluate to 0 for aguments larger than ~5. This cutoff can also most likely be used for floats as well, erf(4.f)  is strictly equal to 1 in fp32, so e.g. x>8 branch https://github.com/tensorflow/tensorflow/blob/937ff1b4cf1d954806e075d66198429a6d2312be/tensorflow/compiler/xla/client/lib/math.cc#L91 should never be evaluated. ", "I've landed my fix for `erf(f16)`; it should show up in OSS within an hour or so and it should show up in bug like the commits above when it does.\r\n\r\nAccording to the exhaustive tests I've set up, we're getting what I'd describe as pretty tight precision bounds (for f16, anyway): All results are within an absolute error of 0.001 or a relative error of 0.0001.\r\n\r\nUpcasting to f32 should no longer be necessary.  At least, that's the theory.  @thorjohnsen you may wish to verify this.  :)", "I'm going to optimistically close this, although there's more work to be done to get all of the functions up to par.  @thorjohnsen &co, please do not hesitate to re-open."]}, {"number": 25051, "title": "added Quadratic Weighted Kappa to the metrics list", "body": "Quadratic Weighed Kappa is a metric that is heavily used to compare the agreement between two raters. For example, it is currently the evaluation metric for one of the [competitions](https://www.kaggle.com/c/petfinder-adoption-prediction#evaluation) on `Kaggle`. There was no native implementation in Keras so I  implemented one for the same. ", "comments": ["Nagging Reviewer @pavithrasv: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 74 days with no activity and the `awaiting review` label has been applied.", "I think we should add this to `add-ons` for now and and merge into core based on how widely it is used. https://github.com/tensorflow/addons/tree/master/tensorflow_addons", "Sure. When I submitted this PR, `addons` wasn't there but yeah it makes sense to add it to `addons` first. I would open a PR there then. Thanks. Closing this one for now"]}, {"number": 25050, "title": "Error: No gradient defined for operation (op type: CropAndResizeGradImage)", "body": "I'm running FasterRCNN model with second ordered differentiation. \r\nbut an error occur: \r\n`No gradient defined for operation 'tower0/gradients/tower0/roi_align/crop_and_resize/CropAndResize_grad/CropAndResizeGradImage' (op type: CropAndResizeGradImage)` \r\n\r\nI have been looking on the internet and haven't found a solution \r\nplease help\r\nthank you", "comments": ["@john81923 ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n[New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)", "Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. When you post this issue in Stackoverflow, please provide more details on the issue so that community will support you faster. Thanks!", "This is a feature request for the second order gradients of `tf.image.crop_and_resize`", "@ppwwyyxx Could you provide more details on the context and issue. Please fill the [template](https://github.com/tensorflow/tensorflow/issues/new?template=30-feature-request.md). More details help us to faster solution. Thanks!", "I was only pointing out the nature of this issue and do not actually intend to request this feature myself as it is not useful to me. In fact I think this feature may be only useful to a few people in the world trying out very new ideas (who may just implement it by themselves with a new op).", "@john81923,\r\nCan you please confirm if the issue is still relevant? You can refer the [Tensorflow Documentation for Second Order Differentiation ](https://www.tensorflow.org/guide/advanced_autodiff). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25049, "title": "provide a better error when keras.layers.Dot is called with a tuple instead of a list", "body": "- TensorFlow version: 1.12\r\n\r\n`keras.layers.Dot` (functional API) is intended to be called with a list of layers: `Dot(axes=0)([a, b])`.\r\n\r\nIf you call it with multiple args (`Dot(axes=0)(a, b)`), it provides a useful error message: ```ValueError: A `Dot` layer should be called on a list of 2 inputs.```.\r\n\r\nIf you call it with a tuple of layers (`Dot(axes=0)((a, b))`, it fails with a cryptic error message: `TypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'`.\r\n\r\nThere is code at the beginning of `Dot.build` that is intended to help with tuples as well:\r\n\r\n```python\r\n  @tf_utils.shape_type_conversion\r\n  def build(self, input_shape):\r\n    # Used purely for shape validation.\r\n    if not isinstance(input_shape, list) or len(input_shape) != 2:\r\n      raise ValueError('A `Dot` layer should be called '\r\n                       'on a list of 2 inputs.')\r\n```\r\n\r\nHowever, when `input_shape` is a tuple, `tf_utils.shape_type_conversion` fails, producing the cryptic error message above, before the helpful error message can kick in.\r\n\r\nI just lost a couple of hours to this.\r\n\r\nI see two possible fixes:\r\n\r\n* Teach `tf_utils.shape_type_conversion` to understand tuples.\r\n* Add another decorator before `tf_utils.shape_type_conversion` to diagnose tuple inputs.\r\n\r\nThis applies to other keras merge layers as well.\r\n", "comments": ["Added a PR #25061 to return a better error message.", "Are you still seeing this issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25049\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25049\">No</a>\n"]}, {"number": 25048, "title": "Fix sparse_to_dense warning in 25043", "body": "This fix fixes sparse_to_dense warning raised in #25043.\r\n\r\nThis fix fixes #25043.\r\n", "comments": []}, {"number": 25047, "title": "Installation error with flags", "body": "N/A", "comments": []}, {"number": 25046, "title": "failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED", "body": "<em>failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED. </em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): on keras\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10.0/7.4.2\r\n- GPU model and memory: 1050Ti\r\n\r\n\r\n\r\n**Other info / logs**\r\nEpoch 1/10\r\n2019-01-19 23:32:25.086846: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.70GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-01-19 23:32:30.159630: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 595.05MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-01-19 23:32:32.432667: E tensorflow/stream_executor/cuda/cuda_driver.cc:981] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-01-19 23:32:32.437902: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0000019AF3892E90: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-01-19 23:32:32.444503: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0000019AF3892E90: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-01-19 23:32:32.452222: F tensorflow/stream_executor/cuda/cuda_dnn.cc:231] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n\r\n\r\nAnyone have any suggestions, I tried updating to the latest CUDA/cuDNN but still facing this issue.", "comments": ["update: after decreasing the batch size from 32 to 4, I have stopped getting this error", "@humayunr7 Tensorflow 1.12 does not support cuda 10. The rc0 version 1.13 which does support cuda 10 is being released this week though acording to #24839", "I have removed cuda 10 and installed cuda 9.2 and I still occasionally get the same error. The problem is that tensorflow runs its own examples just fine(both on cuda 10 & 9.2), but it crashes on my code sometimes.", "@humayunr7 9.2 isnt supported either. The latest version supported is 9.0.", "@humayunr7 , @MatthiasRathbun is correct, r1.12.0 will run reliably with CUDA 9 & cuDNN 7 ; TensorFlow tends to allocate memory greedily to each session created and assigned to GPU cores. So you have to experiment. Please let us know if you could scale stably to batch size of 32. Thanks.", "@msymp shouldn\u2019t be an issue now, @humayunr7 should make sure his cudnn is compatible with cuda 10, but pip install tensorflow-gpu==1.13.0rc0 works with cuda 10.", "Thanks @MatthiasRathbun  @msymp , I am currently running on CUDA 9.0, Tensorflow 1.12 and training stably on batch size of 32. Appreciate your help!"]}, {"number": 25045, "title": "No code example which allows to use tensorflow lite immediately in iOS", "body": "**System information**\r\n- TensorFlow version: 1.12.0\r\n- Doc Link:\r\nhttps://www.tensorflow.org/lite/tfmobile/ios_build\r\n\r\nMost iOS developers use Swift and Objective C. But tensorflow is written in C++ and \"examples\" require strong C++ knowledge too. In the same time that code contains a lot of hardcoded data and it is very hard to understand how to use it in general!\r\n\r\nA lot of time passed. Why not to write a class-wrapper over tensorflow to hide C++ code from developers? For example, code should convert `UIImage` to `NSString` or array of strings", "comments": ["@Gargo , can you please clearly describe your issue that you are facing? Also\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n[New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)\r\n", "https://webappcodes.com/category/ios\r\n\r\nHere, above link you find all ios example codes", "We recently improved the [iOS demo document](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/demo_ios.md) and added more explanation. \r\nWe're also working on adding more language binding support. Stay tuned. ", "Thanks @miaout17. I am closing this as I don't see a reason to keep this open. Thanks!"]}, {"number": 25044, "title": "[TFLite] Feature request: Add support for DepthToSpace op ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (or github SHA if from source):1.13\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n...  Converting unsupported operation: DepthToSpace\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nDepthToSpace op has been well used for implementing subpixel convolution (aka pixel shuffle) like [this super-resolution task](https://davidreiman.github.io/posts/galaxy-image-super-resolution.html). \r\nI know SELECT_TF_OPS option. But its binary size is too huge for mobile application.\r\nWould you like to add DepthToSpace to built-in ops? ", "comments": ["I don't have any objection to a DepthToSpace op being included, but we have not yet prioritized it with the reset of the work. We would welcome contributions if you have time to contribute, but we will likely add it at some point.", "Hey! When will DepthToSpace be added?", "If depth_to_space isn\u2019t supported even now, Can you tell me an alternative to depth_to_space which I can convert to tflite.", "@stakemura,\r\nSorry for the delayed response. Can you please let us know if [tfl.depth_to_space (TFL::DepthToSpaceOp)](https://www.tensorflow.org/mlir/tfl_ops#tfldepth_to_space_tfldepthtospaceop) is the functionality that you are looking for? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25044\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25044\">No</a>\n"]}, {"number": 25043, "title": "tf.nn.sampled_softmax_loss use deprecated sparse_to_dense", "body": "tf.nn.sampled_softmax_loss is using the deprecated ops sparse_to_dense (from tensorflow.python.ops.sparse_ops). Could it be replaced by the `tf.sparse.SparseTensor` and `tf.sparse.to_dense` as the warning suggested?", "comments": ["Added a PR #25048 to fix the warning.", "@ymodak , can you please assist with this. Thanks."]}, {"number": 25042, "title": "fight club probably with upgrade not see", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Not an issue. Closed."]}, {"number": 25041, "title": "The scale parameter in the slim.batch_norm", "body": "**System information**\r\n- TensorFlow version: tensorflow-gpu 1.4.0\r\n- Doc Link: [link](https://github.com/tensorflow/tensorflow/blob/467c1c2e215c5481372ba4e685869ae504b9e74f/tensorflow/contrib/layers/python/layers/layers.py#L226)\r\n\r\nI try to know the details of slim.batch_norm's parameters. We know the formula of the BN is like y = gamma * x + beta. And when the BN layer before the non-linear layer (like the relu), why the gamma need to set 1.0? Does it will harm the performance? In addition, in the doc, the relu is linear? why is the scaling done by the relu?\r\n```\r\n scale: If True, multiply by `gamma`. If False, `gamma` is\r\n            not used. When the next layer is linear (also e.g. `nn.relu`), this can be\r\n            disabled since the scaling can be done by the next layer.\r\n```", "comments": ["@UpCoder ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n[New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)\r\n", "Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. Github is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 25040, "title": "tf.hessians vs tf.hessian_vector_product won't give the same result", "body": "I want to test whether the hessian of an objective loss is equal to the hessian vector product (HVP) of the same loss when we pass a vector of ones to the HVP function.\r\nHere is my simple code to test this idea and surprisingly the result of the two are not the same.\r\nI've put enough comments in the code to follow the problem easier and the hessian_vector_product function is taken from the tensorflow implementation in [here](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/ops/gradients_impl.py):\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow import gradients\r\n\r\ndef hessian_vector_product(ys, xs, v):\r\n  \"\"\"Multiply the Hessian of `ys` wrt `xs` by `v`.\r\n  This is an efficient construction that uses a backprop-like approach\r\n  to compute the product between the Hessian and another vector. The\r\n  Hessian is usually too large to be explicitly computed or even\r\n  represented, but this method allows us to at least multiply by it\r\n  for the same big-O cost as backprop.\r\n  Implicit Hessian-vector products are the main practical, scalable way\r\n  of using second derivatives with neural networks. They allow us to\r\n  do things like construct Krylov subspaces and approximate conjugate\r\n  gradient descent.\r\n  Example: if `y` = 1/2 `x`^T A `x`, then `hessian_vector_product(y,\r\n  x, v)` will return an expression that evaluates to the same values\r\n  as (A + A.T) `v`.\r\n  Args:\r\n    ys: A scalar value, or a tensor or list of tensors to be summed to\r\n        yield a scalar.\r\n    xs: A list of tensors that we should construct the Hessian over.\r\n    v: A list of tensors, with the same shapes as xs, that we want to\r\n       multiply by the Hessian.\r\n  Returns:\r\n    A list of tensors (or if the list would be length 1, a single tensor)\r\n    containing the product between the Hessian and `v`.\r\n  Raises:\r\n    ValueError: `xs` and `v` have different length.\r\n  \"\"\" \r\n\r\n  # Validate the input\r\n  length = len(xs)\r\n  if len(v) != length:\r\n    raise ValueError(\"xs and v must have the same length.\")\r\n\r\n  # First backprop\r\n  grads = gradients(ys, xs)\r\n\r\n  # grads = xs\r\n\r\n  assert len(grads) == length\r\n\r\n  elemwise_products = [\r\n      math_ops.multiply(grad_elem, array_ops.stop_gradient(v_elem))\r\n      for grad_elem, v_elem in zip(grads, v) if grad_elem is not None\r\n  ]\r\n\r\n  # Second backprop  \r\n  grads_with_none = gradients(elemwise_products, xs)\r\n  return_grads = [\r\n      grad_elem if grad_elem is not None \\\r\n      else tf.zeros_like(x) \\\r\n      for x, grad_elem in zip(xs, grads_with_none)]\r\n  \r\n  return return_grads\r\n\r\n\r\nk = 1\r\ntf.reset_default_graph()\r\na = tf.get_variable('a', [k])\r\n#b = tf.get_variable('b', [k])\r\nb = tf.multiply(a,a) # b = a^2\r\nc = tf.multiply(tf.multiply(a,a),a) + tf.multiply(b,b) # c = a^3 + b^2\r\n\r\n# Calculate the hessian of c w.r.t. a and b\r\n# This is equivalent to the following matrix:\r\n#    -               -\r\n#   | 6a+12a^2    4a  |\r\n# H=|                 |\r\n#   | 4a          2.0 | \r\n#    -               -\r\n\r\n# However the following line returns this matrix or basically just the diagonal values:\r\n#       -               -\r\n#      | 6a+12a^2     0  |\r\n# hess=|                 |\r\n#      |    0        2.0 | \r\n#       -               -\r\nhess = tf.hessians(c, [a,b])\r\n\r\n# while the following line which calculates Hv or H.1 = H, returns this vector:\r\n#       -                     -\r\n# hvp= | 6a+12a^2+4a    2.0+4a |\r\n#       -                     -\r\n\r\nv = [np.ones((k),dtype=np.float32),np.ones((k),dtype=np.float32)]\r\nhvp = hessian_vector_product(c, [a,b], v)\r\n\r\n# which is basically the summation of derivatives in hessian matrix along the rows.\r\n\r\n# According to this observation (and similar tests), \r\n# the result of tf.hessians always return zero for off-diagonal elements,\r\n# which shouldn't be the case\r\n# On the other hand hessian vector product (HVP) function, only returns the summation of hessians over rows\r\n# and we can't access to off-diagonal elements of the hessian matrix using this function, too.\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\n_a, _hess, _hvp = sess.run([a, hess, hvp])\r\n```\r\n\r\nThis problem will be avoided if the variable `b` is not depended on `a` and thus the hessian matrix only should have values on the diagonals and all the other values will be zero. Therefore, HVP and hessian matrix would have the same values. However, as we all know this is a rare scenario and in most of the real-world applications trainable variables are depended to each other.", "comments": ["@yaserkl is investigating the differences between the _tf.hessians_ API in: \r\nhttps://www.tensorflow.org/api_docs/python/tf/hessians\r\nand the unreleased __hessian_vector_product_ in:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/ops/gradients_impl.py : Line 1151\r\n@rmlarsen & @vrv , Can you please advise? Thanks.  ", "I've fixed the issue with tf.hessians as follows:\r\n\r\nI'm sure there would be a better (shorter) way of implementing this but the current implementation of tf.hessians is definitely wrong since it only returns the diagonal elements of the hessian matrix.\r\n\r\nNow, once the hessian matrix is calculated, if we sum over rows or columns (since it's a symmetric matrix, it won't matter), it will give us the same result as the HVP, thus Hv == H.v\r\n\r\n```\r\ndef exact_hessians(ys,\r\n             xs,\r\n             name=\"hessians\",\r\n             colocate_gradients_with_ops=False,\r\n             gate_gradients=False,\r\n             aggregation_method=None):\r\n  \"\"\"Constructs the Hessian of sum of `ys` with respect to `x` in `xs`.\r\n  `hessians()` adds ops to the graph to output the Hessian matrix of `ys`\r\n  with respect to `xs`.  It returns a list of `Tensor` of length `len(xs)`\r\n  where each tensor is the Hessian of `sum(ys)`.\r\n  The Hessian is a matrix of second-order partial derivatives of a scalar\r\n  tensor (see https://en.wikipedia.org/wiki/Hessian_matrix for more details).\r\n  Args:\r\n    ys: A `Tensor` or list of tensors to be differentiated.\r\n    xs: A `Tensor` or list of tensors to be used for differentiation.\r\n    name: Optional name to use for grouping all the gradient ops together.\r\n      defaults to 'hessians'.\r\n    colocate_gradients_with_ops: See `gradients()` documentation for details.\r\n    gate_gradients: See `gradients()` documentation for details.\r\n    aggregation_method: See `gradients()` documentation for details.\r\n  Returns:\r\n    A list of Hessian matrices of `sum(ys)` for each `x` in `xs`.\r\n  Raises:\r\n    LookupError: if one of the operations between `xs` and `ys` does not\r\n      have a registered gradient function.\r\n  \"\"\"\r\n  xs = _AsList(xs)\r\n  kwargs = {\r\n      \"colocate_gradients_with_ops\": colocate_gradients_with_ops,\r\n      \"gate_gradients\": gate_gradients,\r\n      \"aggregation_method\": aggregation_method\r\n  }\r\n  # Compute first-order derivatives and iterate for each x in xs.\r\n  #hessians = []\r\n  _gradients = gradients(ys, xs, **kwargs)\r\n\r\n  h_is = []\r\n  for i, gradient in enumerate(_gradients):\r\n      gradient = array_ops.reshape(gradient, [-1])\r\n      h_i = [tf.gradients(gradient[j], xs) for j in range(gradient.shape[0].value)]  # row ith of hessian matrix\r\n      h_i_reshape = []\r\n      for _hi in h_i:\r\n          h_i_reshape.append(tf.concat([tf.reshape(_, [-1]) for _ in _hi], axis=0))\r\n      h_i_reshape = tf.stack(h_i_reshape)\r\n      h_is.append(h_i_reshape)\r\n\r\n  hessian_matrix = tf.concat(h_is, axis=0)\r\n  return hessian_matrix\r\n```", "@yaserkl,\r\nSorry for the delayed response. Can you please cite the reference which states:\r\n\r\n> hessian of an objective loss is equal to the hessian vector product (HVP) of the same loss \r\n\r\nbecause, the source code of [Hessian_Vector_Product](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/ops/gradients_impl.py#L1152-L1202) looks different from that of [Hessians](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/ops/gradients_impl.py#L1205-L1269).\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25040\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25040\">No</a>\n"]}, {"number": 25039, "title": "dense to sparse", "body": "Since we have sparse to dense op, I think it would be helpful if there is a dense_to_sparse op in Tensorflow. Just like we have a tensor A and it can be converted to a sparse tensor by using a function such as tf. sparse_to_dense(A).", "comments": ["@gbc8181 , can you please provide an example why would need this and what exactly do you mean by converting a dense to sparse? Thanks.", "@msymp Hi, I want to multiply two tensors and one of them is sparse; the other is not. If it can be converted to sparse expression, we can save a lot of memory. Thus, we try to convert it to sparse.  ", "@gbc8181 , Please use this operation available in TensorFlow:\r\nhttps://www.tensorflow.org/api_docs/python/tf/sparse/matmul\r\nThanks.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25038, "title": "[Intel MKL] Adding support for perchannel requantization", "body": "## (Re)Quantization_Perchannel_op\r\n\r\n### 1. Adding two new hidden operators to support requantization to be performed perchannel and provided the mkl implementations for the same.\r\n\r\n        1. requantize_per_channel_op\r\n        2. requantization_range_per_channel_op\r\n\r\n### 2. Provided Unit test to test the implementation logic of both these above mentioned ops.", "comments": ["Can you review and approve this PR please", "@rthadur @hgadig  @ymodak  . I see that this PR has not been assigned to anybody  yet. Can someone please assign this to someone or pick it up for INTEL MKL. ", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "there seems to be an issue in merging.  see if you can create a clean fork of tensorflow HEAD, apply your small diff to it, and then submit that.", "thank you @ebrevdo I opened a new one without the git rebasing problems here in a new PR https://github.com/tensorflow/tensorflow/pull/25203\r\n@hgadig can you pick it up from the new PR please! (25203)"]}, {"number": 25037, "title": "[INTEL MKL]: ADD Tensorflow MKL-DNN Singularity definition files", "body": "This PR adds Singularity definition files for intel-optimized tensorflow", "comments": ["@penpornk @martinwicke here is a new PR for adding singularity definition files.", "I believe we have closed a similar PR before -- I don't think these should be added to the main TensorFlow repo. Is there some context I am not aware of?", "I closed that old PR to re-write singularity definition files from scratch instead of converting docker binaries to singularity. \r\nThis is a new PR with all re-written files. @martinwicke ", "@martinwicke please let me know what do you think.", "I still see no reason to have these files in the TensorFlow repo. We cannot maintain them, and putting core TensorFlow developers into a modification loop seems like a bad idea. I would maintain these in a separate repo, I can see no downside to that.", "Okay thanks @martinwicke . Closing this PR as per your suggestions."]}, {"number": 25036, "title": "Keras subclassed model layers' output shape detection (e.g. for summary)", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently output shapes inference of all layers is essentially disabled if model is subclassed, which results in a rather vague \"multiple\" in model summary. Example:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.dense = tf.keras.layers.Dense(1)\r\n\r\n    def call(self, inputs, **kwargs):\r\n        return self.dense(inputs)\r\n\r\nmodel = MyModel()\r\nmodel.build(input_shape=(None, 1))\r\nmodel.summary()\r\n```\r\n\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense (Dense)                multiple                  2         \r\n=================================================================\r\nTotal params: 2\r\nTrainable params: 2\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\nAfter investigating source code I understand why this happens from a technical point of view, but it is quite frustrating for an end user, especially given that it is quite trivial to infer the shapes manually with given information. I think there should be a way to force `layer.output_shape` recalculation if `model.build()` is manually called. There is of course a risk of mismatching actual input shapes, but that would be responsibility of the `.build()` caller to enforce. \r\n\r\nAlternatively, allow runtime input(s) shape argument to be passed to `model.summary()`, that could be used for one-time shape detection and validation.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo, not directly. Could possibly affect internal checks if `layer.output_shape` is manually overriden. However, currently that parameter is completely unused for subclassed models, so risk should be low.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEnd users of subclassed model providers who could be interested in inspecting model topology.\r\n\r\n**Any Other info.**\r\n\r\nI am willing to contribute either option with a little bit of guidance on the direction.", "comments": ["Hi. \r\nCan I help with this issue?", "@jashrathod0 Please feel free to support any issue here. This is a community helping others in the community. Thanks for coming forward.", "hi \r\ngreat post ,\r\ni m trying to run baloonn detection API Mask_RCNN , and i get stuck with this error \r\n\r\ninput that isn't a symbolic tensor\r\n\r\nany suggession to work this out ?", "Are you working on this? Need help?\r\n\r\nsame happens for tensorflow 2.0", "Same issue here on TF 2 nightly.", "Same issue here, is there anyway to make the sub-class model able to do things like \"model.summary()\"?", "I confront the same issue. In `tf.keras API`, when create a model by define subclass and implement forward pass in method `call`, actually have not build a TF graph. \r\nThe layers in `model.layers` can't get the attributes `layer.input_shape` and `layer.output_shape`. This is because the `layer._inbound_nodes` is an empty list. And in the definition of `layer.input_shape`(also `layer.output_shape`), there is a judgement statement:\r\n```python\r\nif not self._inbound_nodes:\r\n  raise AttributeError('The layer has never been called '\r\n                       'and thus has no defined input shape.')\r\n```\r\nWhen pass a symbolic tensor to a layer, a `Node` class (define in `keras.engine.base_layer.py`) will append to `layer._inbound_nodes` list. But when create model by subclass and implement forward pass in `call` method, it seems that haven't pass symbolic tensors to each layer. So the `layer._inbound_nodes` is empty list, and then can't get the attribute `layer.input_shape` . This is an example(In `model.summary()` method also need to get the `layer.input_shape`):\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\r\n        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\r\n    \r\n    def call(self, inputs):\r\n        x = self.dense1(inputs)\r\n        return self.dense2(x)\r\n\r\nmodel = MyModel()\r\nlayers = model.layers\r\n\r\nfor layer in layers:\r\n    name = layer.name\r\n    input_shape = layer.input_shape\r\n    output_shape = layer.output_shape\r\n    print('%s   input shape: %s, output_shape: %s.\\n' % (name, input_shape, output_shape))\r\n```\r\nAnd this is the traceback:\r\n```prompt\r\nTraceback (most recent call last):\r\n  File \"draft.py\", line 195, in <module>\r\n    test8()\r\n  File \"draft.py\", line 159, in test8\r\n    input_shape = layer.input_shape\r\n  File \"C:\\Users\\Jairo\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\k\r\neras\\engine\\base_layer.py\", line 1118, in input_shape\r\n    raise AttributeError('The layer has never been called '\r\nAttributeError: The layer has never been called and thus has no defined input sh\r\nape.\r\n```\r\nIf create a model by `functional API` or `model.Sequential`, things go well:\r\n```python\r\nimport tensorflow as tf\r\n\r\ninputs = tf.keras.Input(shape=(3,))\r\nx = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\r\noutputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n```\r\nor\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(32, input_shape=(500,)))\r\nmodel.add(tf.keras.layers.Dense(32))\r\n```\r\nBut when the model structure is complicated, I choose to use subclass model and define forward pass in `call`. If can get `layer.input_shape` and `layer.output_shape` when create model by subclass, it will be very convenient.", "This is a serious issue, rendering subclassing the Model class unusable. Anyone working on this?", "You can do all these things (printing input / output shapes) in a Functional or Sequential model because these models are static graphs of layers.\r\n\r\nIn contrast, a subclassed model is a piece of Python code (a `call` method). There is no graph of layers here. We cannot know how layers are connected to each other (because that's defined in the body of `call`, not as an explicit data structure), so we cannot infer input / output shapes.\r\n\r\nPlease see this detailed explanation of the differences between Functional/Sequential models and imperative models: https://medium.com/tensorflow/what-are-symbolic-and-imperative-apis-in-tensorflow-2-0-dfccecb01021", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25036\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25036\">No</a>\n", "@fchollet Thank you for the explanation. It seems that there is a misunderstanding in the purpose of subclassing Model. Every tutorial I've seen uses that as encapsulation of static graphs e.g. as ResNet blocks. However, now I understand that they are meant for dynamic graphs, where each `call` could evaluate different graph based on input parameters.", "@fchollet Thanks.", "@inoryy and anyone interested, I found a dirty workaround that basically mixes functional API and model subclassing by using `__new__`\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Dense, Flatten\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(self._inputs, self._outputs)\r\n        self.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\r\n        \r\n    def __new__(cls, input_shape, num_classes, neurons=100, layers=2):\r\n        cls._inputs = Input( input_shape, dtype=tf.float32, name='inputs' )\r\n        x = Flatten()(cls._inputs)\r\n        for k in range(layers):\r\n            x =  Dense(neurons)(x)\r\n        cls._outputs = Dense(num_classes)(x)\r\n        \r\n        return super().__new__(cls)\r\n    \r\nmodel = MyModel( (28,28), 10 )\r\nmodel.summary()\r\n```\r\nThis can be useful for example if one wants to write a grid-search wrapper that works with either tensorflow/pytorch/scikit-learn type models.\r\n\r\n```python\r\nfor HPC in HPCs:\r\n   model = MyModel(**HPC)  # initialize with specific HyperParameterCombination\r\n   model.fit(train_data)\r\n   score = model.evaluate(test_data)\r\n```\r\n\r\n\r\n", "I borrow the solution from [here](https://stackoverflow.com/questions/55235212/model-summary-cant-print-output-shape-while-using-subclass-model) by adding a `model` method to infer the model explicitly.\r\n\r\nThe output shape is especially uncertain when with `Dense` layer as it depends on the input shape, so it needs to be inferred with a certain input shape.\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.dense = tf.keras.layers.Dense(1)\r\n\r\n    def call(self, inputs, **kwargs):\r\n        return self.dense(inputs)\r\n\r\n    def model(self):\r\n        x = Input(shape=(1))\r\n        return Model(inputs=[x], outputs=self.call(x))\r\n\r\nMyModel().model().summary()\r\n```", "I created a mixin class that solves this problem for static models defined with the subclassing API. It automatically defines input and output shapes and dtypes the first time the model or layer is called. See the [gist](https://gist.github.com/Danmou/bafa5c80356fdb2c843eaf38c8597f84) for a complete example, but here's a demonstration:\r\n```python\r\nclass ExampleNetwork(Model):\r\n    def __init__(self) -> None:\r\n        super().__init__()\r\n        self.encoder = Sequential([\r\n            Conv2D(filters=32, kernel_size=3, strides=2, activation='relu'),\r\n            Flatten(),\r\n        ])\r\n        self.concat = Concatenate(axis=-1)\r\n        self.dense = Dense(units=100)\r\n\r\n    def call(self, inputs: List[tf.Tensor]) -> tf.Tensor:\r\n        encoded = self.encoder(inputs[0])\r\n        joined = self.concat([encoded] + inputs[1:])\r\n        return self.dense(joined)\r\n\r\nmodel = ExampleNetwork()\r\nfirst_batch = [tf.zeros((1, 64, 64, 3)), tf.zeros((1, 10))]\r\nmodel(first_batch)\r\nmodel.summary()\r\n# Model: \"example_network_1\"\r\n# _________________________________________________________________\r\n# Layer (type)                 Output Shape              Param #\r\n# =================================================================\r\n# sequential_1 (Sequential)    (None, 30752)             896\r\n# _________________________________________________________________\r\n# concatenate (Concatenate)    (None, 30762)             0\r\n# _________________________________________________________________\r\n# dense (Dense)                (None, 100)               3076300\r\n# =================================================================\r\n# Total params: 3,077,196\r\n# Trainable params: 3,077,196\r\n# Non-trainable params: 0\r\n# _________________________________________________________________\r\nprint(model.input_shape)\r\n# ListWrapper([TensorShape([None, 64, 64, 3]), TensorShape([None, 10])])\r\n```\r\n\r\n@fchollet and other devs, any chance this (or something similar) could become part of `tf.keras.layers.Layer` when it's not explicitly initialized as dynamic? I find the subclassing API better suited for complex projects (I assume that's also why it's used by all the built-in Keras layers) and this would make it just as easy to use as the functional API.", "It took a few attempts to get this right.\r\n\r\nThe first hack method is simply to run dummy data through the model\r\n```python3\r\nmodel = ClassCNN()\r\nmodel( tf.random.uniform(shape=(128,28,28,1)) )  # required to call .summary() before .fit()\r\nmodel.summary()\r\n```\r\n\r\nHowever a more robust solution involves explictly `.build(input_size)` on the key layers, and then on the model itself inside `__init__()`. \r\n\r\nNot all the layers need to be defined, as some information can be implied. Define the first \"real\" layer (`Flatten()` is not a layer), plus any `Conv2D()` and `Dropout()` layers, which might require a little bit of math.\r\n\r\nHere are some working examples of keras syntax, which fixes the `model.summary()` before `model.fit()` bug:\r\n- https://github.com/JamesMcGuigan/kaggle-digit-recognizer/tree/master/src/keras/examples\r\n\r\n```python3\r\nclass ClassCNN(tf.keras.Model):\r\n\r\n    def __init__(self, input_shape, output_shape, **kwargs):\r\n        super(ClassCNN, self).__init__()\r\n        self._input_shape  = input_shape   # = (28, 28, 1)\r\n        self._output_shape = output_shape  # = 10\r\n\r\n        self.conv1      = Conv2D(32, kernel_size=(3, 3), activation=tf.nn.relu)\r\n        self.conv2      = Conv2D(64, kernel_size=(3, 3), activation=tf.nn.relu)\r\n        self.maxpool    = MaxPooling2D(pool_size=(2, 2))\r\n        self.dropout1   = Dropout(0.25, name='dropout1')\r\n        self.flatten    = Flatten()\r\n        self.dense1     = Dense(128, activation=tf.nn.relu)\r\n        self.dropout2   = Dropout(0.5, name='dropout2')\r\n        self.activation = Dense(self._output_shape, activation=tf.nn.softmax)\r\n\r\n        self.conv1.build(     (None,) + input_shape )\r\n        self.conv2.build(     (None,) + tuple(np.subtract(input_shape[:-1],2)) + (32,) )\r\n        self.maxpool.build(   (None,) + tuple(np.subtract(input_shape[:-1],4)) + (64,) )\r\n        self.dropout1.build( tuple(np.floor_divide(np.subtract(input_shape[:-1],4),2)) + (64,) )\r\n        self.dropout2.build( 128 )\r\n        self.build(           (None,) + input_shape)\r\n\r\n\r\n    def call(self, x, training=False, **kwargs):\r\n        x = self.conv1(x)\r\n        x = self.conv2(x)\r\n        x = self.maxpool(x)\r\n        if training:  x = self.dropout1(x)\r\n        x = self.flatten(x)\r\n        x = self.dense1(x)\r\n        if training:  x = self.dropout2(x)\r\n        x = self.activation(x)\r\n        return x\r\n```\r\n\r\n```python3\r\nclass ClassNN(tf.keras.Model):\r\n\r\n    def __init__(self, input_shape, output_shape, **kwargs):\r\n        super(ClassNN, self).__init__()\r\n        self._input_shape  = np.prod(input_shape)  # = (28, 28, 1) = 784\r\n        self._output_shape = output_shape          # = 10\r\n\r\n        self.flatten    = tf.keras.layers.Flatten()\r\n        self.dense1     = tf.keras.layers.Dense(128, activation=tf.nn.relu, )\r\n        self.dropout    = tf.keras.layers.Dropout(0.2)\r\n        self.dense2     = tf.keras.layers.Dense(128, activation=tf.nn.softmax)\r\n        self.activation = tf.keras.layers.Dense(self._output_shape, activation=tf.nn.softmax)\r\n\r\n        self.dense1.build( self._input_shape)\r\n        self.dropout.build(128)\r\n        self.build( (None, self._input_shape) )\r\n\r\n\r\n    def call(self, inputs, training=False, **kwargs):\r\n        x = self.flatten(inputs)\r\n        x = self.dense1(x)\r\n        if training: x = self.dropout(x)\r\n        x = self.dense2(x)\r\n        x = self.activation(x)\r\n        return x\r\n```\r\n\r\n```python3\r\ndef FunctionalCNN(input_shape, output_shape):\r\n    inputs = Input(shape=input_shape)\r\n    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\r\n    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\r\n    x = MaxPooling2D(pool_size=(2, 2))(x)\r\n    x = Dropout(0.25)(x)\r\n    x = Flatten()(x)\r\n    x = Dense(128, activation='relu')(x)\r\n    x = Dropout(0.5)(x)\r\n    x = Dense(output_shape, activation='softmax')(x)\r\n\r\n    model = Model(inputs, x, name=\"FunctionalCNN\")\r\n    plot_model(model, to_file=os.path.join(os.path.dirname(__file__), \"FunctionalCNN.png\"))\r\n    return model\r\n```\r\n\r\n```python3\r\ndef SequentialCNN(input_shape, output_shape):\r\n    model = Sequential()\r\n    model.add( Conv2D(32, kernel_size=(3, 3),\r\n                     activation='relu',\r\n                     input_shape=input_shape) )\r\n    model.add( Conv2D(64, (3, 3), activation='relu') )\r\n    model.add( MaxPooling2D(pool_size=(2, 2)) )\r\n    model.add( Dropout(0.25) )\r\n    model.add( Flatten() )\r\n    model.add( Dense(128, activation='relu') )\r\n    model.add( Dropout(0.5) )\r\n    model.add( Dense(output_shape, activation='softmax') )\r\n\r\n    plot_model(model, to_file=os.path.join(os.path.dirname(__file__), \"SequentialCNN.png\"))\r\n    return model\r\n```\r\n\r\nmain loop\r\n```python3\r\n#!/usr/bin/env python3\r\nimport multiprocessing\r\nimport os\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # 0, 1, 2, 3  # Disable Tensortflow Logging\r\nos.chdir( os.path.dirname( os.path.abspath(__file__) ) )\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport time\r\n\r\nfrom src.dataset import DataSet\r\nfrom src.keras.examples.ClassCNN import ClassCNN\r\nfrom src.keras.examples.ClassNN import ClassNN\r\nfrom src.keras.examples.FunctionalCNN import FunctionalCNN\r\nfrom src.keras.examples.SequentialCNN import SequentialCNN\r\nfrom src.utils.csv import predict_to_csv\r\n\r\ntf.random.set_seed(42)\r\n\r\ntimer_start = time.time()\r\n\r\ndataset = DataSet()\r\nconfig = {\r\n    \"verbose\":      False,\r\n    \"epochs\":       12,\r\n    \"batch_size\":   128,\r\n    \"input_shape\":  dataset.input_shape(),\r\n    \"output_shape\": dataset.output_shape(),\r\n}\r\nprint(\"config\", config)\r\n\r\n# BUG: ClassCNN accuracy is only 36% compared to 75% for SequentialCNN / FunctionalCNN\r\n# SequentialCNN   validation: | loss: 1.3756675141198293 | accuracy: 0.7430952\r\n# FunctionalCNN   validation: | loss: 1.4285654685610816 | accuracy: 0.7835714\r\n# ClassCNN        validation: | loss: 1.9851970995040167 | accuracy: 0.36214286\r\n# ClassNN         validation: | loss: 2.302224604288737  | accuracy: 0.09059524\r\nmodels = {\r\n    \"SequentialCNN\": SequentialCNN(\r\n        input_shape=dataset.input_shape(),\r\n        output_shape=dataset.output_shape()\r\n    ),\r\n    \"FunctionalCNN\": FunctionalCNN(\r\n        input_shape=dataset.input_shape(),\r\n        output_shape=dataset.output_shape()\r\n    ),\r\n    \"ClassCNN\": ClassCNN(\r\n        input_shape=dataset.input_shape(),\r\n        output_shape=dataset.output_shape()\r\n    ),\r\n    \"ClassNN\":  ClassNN(\r\n        input_shape=dataset.input_shape(),\r\n        output_shape=dataset.output_shape()\r\n    )\r\n}\r\n\r\n\r\nfor model_name, model in models.items():\r\n    print(model_name)\r\n\r\n    model.compile(loss=keras.losses.categorical_crossentropy,\r\n                  optimizer=keras.optimizers.Adadelta(),\r\n                  metrics=['accuracy'])\r\n\r\n    model.summary()\r\n\r\n    model.fit(\r\n        dataset.data['train_X'], dataset.data['train_Y'],\r\n        batch_size = config[\"batch_size\"],\r\n        epochs     = config[\"epochs\"],\r\n        verbose    = config[\"verbose\"],\r\n        validation_data = (dataset.data[\"valid_X\"], dataset.data[\"valid_Y\"]),\r\n        use_multiprocessing = True, workers = multiprocessing.cpu_count()\r\n    )\r\n\r\nfor model_name, model in models.items():\r\n    score = model.evaluate(dataset.data['valid_X'], dataset.data['valid_Y'], verbose=config[\"verbose\"])\r\n    print(model_name.ljust(15), \"validation:\", '| loss:', score[0], '| accuracy:', score[1])\r\n\r\nprint(\"time:\", int(time.time() - timer_start), \"s\")\r\n```\r\n\r\nOutput\r\n```python3\r\n./src/keras/examples/main.py \r\nconfig {'verbose': False, 'epochs': 12, 'batch_size': 128, 'input_shape': (28, 28, 1), 'output_shape': 10}\r\nSequentialCNN\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d (Conv2D)              (None, 26, 26, 32)        320       \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 12, 12, 64)        0         \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 9216)              0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 128)               1179776   \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 128)               0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                1290      \r\n=================================================================\r\nTotal params: 1,199,882\r\nTrainable params: 1,199,882\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nFunctionalCNN\r\nModel: \"FunctionalCNN\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 28, 28, 1)]       0         \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \r\n_________________________________________________________________\r\nconv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \r\n_________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, 12, 12, 64)        0         \r\n_________________________________________________________________\r\nflatten_1 (Flatten)          (None, 9216)              0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 128)               1179776   \r\n_________________________________________________________________\r\ndropout_3 (Dropout)          (None, 128)               0         \r\n_________________________________________________________________\r\ndense_3 (Dense)              (None, 10)                1290      \r\n=================================================================\r\nTotal params: 1,199,882\r\nTrainable params: 1,199,882\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nClassCNN\r\nModel: \"class_cnn\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d_4 (Conv2D)            multiple                  320       \r\n_________________________________________________________________\r\nconv2d_5 (Conv2D)            multiple                  18496     \r\n_________________________________________________________________\r\nmax_pooling2d_2 (MaxPooling2 multiple                  0         \r\n_________________________________________________________________\r\ndropout1 (Dropout)           multiple                  0         \r\n_________________________________________________________________\r\nflatten_2 (Flatten)          multiple                  0         \r\n_________________________________________________________________\r\ndense_4 (Dense)              multiple                  1179776   \r\n_________________________________________________________________\r\ndropout2 (Dropout)           multiple                  0         \r\n_________________________________________________________________\r\ndense_5 (Dense)              multiple                  1290      \r\n=================================================================\r\nTotal params: 1,199,882\r\nTrainable params: 1,199,882\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nClassNN\r\nModel: \"class_nn\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nflatten_3 (Flatten)          multiple                  0         \r\n_________________________________________________________________\r\ndense_6 (Dense)              multiple                  100480    \r\n_________________________________________________________________\r\ndropout_4 (Dropout)          multiple                  0         \r\n_________________________________________________________________\r\ndense_7 (Dense)              multiple                  16512     \r\n_________________________________________________________________\r\ndense_8 (Dense)              multiple                  1290      \r\n=================================================================\r\nTotal params: 118,282\r\nTrainable params: 118,282\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nSequentialCNN   validation: | loss: 1.3733067512512207 | accuracy: 0.739881\r\nFunctionalCNN   validation: | loss: 1.438635626293364 | accuracy: 0.77916664\r\nClassCNN        validation: | loss: 1.3686876785187494 | accuracy: 0.75238097\r\n```", "I believe it can be handled this way, which sounds net and clean, isn't it?\r\nhttps://stackoverflow.com/a/55236388/871418"]}, {"number": 25035, "title": "Allow tf.contrib.tpu._prepare_validation_data to modify x,y", "body": "When the user passes a validation_split parameter, the samples allocated to the val_x, val_y populations should be removed from the training data set (x, y).  In the _prepare_validation_split function, the validation split parameter branch already creates the correctly truncated x, y but does not return those modified x, y values back.  As a result, the validation samples are in both the training data set and the validation data sets. The validation loss/metrics are giving artificially good results since the model is training also the samples in the validation set.\r\n\r\nFixes #25034 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!\r\n\r\n\r\nFrom: googlebot <notifications@github.com>\r\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\r\nDate: Friday, January 18, 2019 at 2:18 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: \"Holcomb, Michael Joseph\" <Michael.Holcomb@utdallas.edu>, Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Allow tf.contrib.tpu._prepare_validation_data to modify x,y (#25035)\r\n\r\nI signed it!\r\n", "CLAs look good, thanks!\n\n<!-- ok -->", "I'm not familiar with the code in tensorflow/contrib/cpu, so we'll need a reviewer from the TPU team to take a look at this. /cc @jhseu "]}, {"number": 25034, "title": "Keras_to_tpu_model: validation_split parameter does not remove validation samples from training set", "body": "**System information**\r\n- Using unmodified Google Colab TPU runtime\r\n\r\n== cat /etc/issue ===============================================\r\nLinux 07e8e3e8ed41 4.14.79+ #1 SMP Wed Dec 19 21:19:13 PST 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"18.04.1 LTS (Bionic Beaver)\"\r\nVERSION_ID=\"18.04\"\r\nVERSION_CODENAME=bionic\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 07e8e3e8ed41 4.14.79+ #1 SMP Wed Dec 19 21:19:13 PST 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nmesh-tensorflow          0.0.5                \r\nmsgpack-numpy            0.4.3.2              \r\nnumpy                    1.14.6               \r\nprotobuf                 3.6.1                \r\ntensorflow               1.12.0               \r\ntensorflow-hub           0.2.0                \r\ntensorflow-metadata      0.9.0                \r\ntensorflow-probability   0.5.0                \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.12.0\r\ntf.GIT_VERSION = v1.12.0-0-ga6d8ffae09\r\ntf.COMPILER_VERSION = v1.12.0-0-ga6d8ffae09\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\r\n\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.2/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7\r\n\r\n**Describe the current behavior**\r\nWhen using the `tf.contrib.tpu.keras_to_tpu_model` API for training models on the TPU, providing a `validation_split` parameter does not remove the validation samples from the training set.  This causes the reported validation loss/metrics to be artificially high since we are training on samples that should have been withheld.\r\n\r\n**Describe the expected behavior**\r\nSpecifically, when calling the fit method with a `validation_split` parameter, a proportion of the samples equal to (count of samples) * (validation_split) should be removed from the training data.\r\n\r\n**Code to reproduce the issue**\r\nA Colab notebook that reproduces the issue is here:\r\nhttps://colab.research.google.com/drive/13g_eEYBUGw9GR6XVjop4TOQEKEDCJQVG\r\n\r\nSpecifically, a dummy dataset with 256 samples with validation_split = 1/8 should result in 224 training samples and 32 validation samples, but currently shows 256 training samples and 32 validation samples.\r\n\r\n**Other info / logs**\r\nI am very grateful for to everyone that is working on this interface.  This makes the on-ramp to TPU training much more manageable.", "comments": ["@mike-holcomb Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from questions. Github is mainly for addressing bugs in installation and performance. There are several questions that are similar to your's ans have a [solution](https://stackoverflow.com/questions/52730645/keras-callbacks-requiring-validation-split\r\n) like this. Thanks!\r\n\r\n", "@jvishnuvardhan Thanks for your quick response.  This is a bug.  Based on your link to a question about the normal `keras.models.Model`, I believe I see the source of the confusion: my issue relates specifically to the `KerasTPUModel` class in `tf.contrib.tpu.keras_support.py.`  \r\n\r\nCurrently, the `KerasTPUModel` implementation of the `validation_split` parameter in the `fit()` method violates the key assumption behind out-of-sample testing (i.e., that your training set and your evaluation set are mutually exclusive).  \r\n\r\nThe `KerasTPUModel._prepare_validation_data()` (called by `fit()` to implement the `validation_split` parameter) performs the proper, mutually exclusive split but only modifies a local training set variable. The `_prepare_validation_data()` returns only the new validation subset to the caller and leaves the caller's training set unmodified.  As such, `fit()` uses the original, full data set for training while incorrectly calculating a the validation metrics on the last _validation_split_ % of the training set.\r\n\r\nMy Colab example and pull request have more details about the issue within the context of `KerasTPUModel`.\r\n\r\nAs a workaround, I am creating my own training/validation splits and then supplying the validation set via the `validation_data` parameter.", "I have the same problem. I have got different training sets with/without TPU."]}, {"number": 25033, "title": "BatchNormalization layer not being folded when converting quantization-aware trained pb to tflite using tflite_convert", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 26\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.0-dev20190118\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0 / 7.4.1\r\n- GPU model and memory: Nvidia Titan RTX\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen converting a pb model file (that includes BatchNorm layers) to tflite, the resulting tflite graph continues to contain batchnorm layers in the form of mul and add ops.\r\n\r\n**Describe the expected behavior**\r\nBatch normalization ops should have been folded into weights and biases of previous layers in the tflite graph in order to optimize inference latency.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Folded batchnorm has its limitations. Only certain ops are supported now:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/fold_batch_norms.py#L40"]}, {"number": 25032, "title": "Disable jni tests on mac (x86_64 toolchain not compatible with our linker script)", "body": "This does not preclude building Android Java JNI since that uses a different toolchain that does support version-script\r\n\r\nPiperOrigin-RevId: 229946385", "comments": []}, {"number": 25031, "title": "del wav_to_features.py", "body": "\r\ndelete wav_to_features.py", "comments": ["> Could you please explain why the file should be deleted?\r\n\r\n@fix27  Could you please give an explanation.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Hello you wrote the Ecologist - Engineer the information processing system Surgu Pozdnyakov Konstantin Nikolayevich. Help the solution of ecological problems with the help of tensorflow. \r\nFor example, teach tensorflow to find and report negative impacts of human activities on the environment. Thanks.", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 25030, "title": "Delete input_data.py", "body": "", "comments": ["Nagging Reviewer @petewarden: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied.", "I'm not sure I understand the context of this change. It appears to just delete the test for input_data.py? Could you add some more information about the motivation for this and I'll look at it again."]}, {"number": 25029, "title": "ImportError: cannot import name 'pywrap_tensorflow'", "body": "**NOTE:**  this is not related to the common case of being inside the tensorflow folder.\r\n\r\n**System information**\r\n- Mac OSX 10.11.6\r\n- installed 1.5 with pip (I use old version because old Mac OSX version)\r\n- TensorFlow version:\r\n- Python version: 3.6.6\r\n- Inside virtualenv and with pip\r\n\r\n**Describe the problem**\r\nI have flask app which I install as python package and TF is installed as part of the required packages. When I run the flask app, I get: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"studio/app.py\", line 9, in <module>\r\n    from simple_neural_net import simple_neural_net\r\n  File \"/Users/mikko/dev/studio/simple_neural_net.py\", line 3, in <module>\r\n    from keras.models import Sequential\r\n  File \"/Users/mikko/dev/studio_test/lib/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"/Users/mikko/dev/studio_test/lib/python3.6/site-packages/keras/utils/__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"/Users/mikko/dev/studio_test/lib/python3.6/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"/Users/mikko/dev/studio_test/lib/python3.6/site-packages/keras/backend/__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/Users/mikko/dev/studio_test/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"/Users/mikko/dev/studio_test/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/mikko/dev/studio_test/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\nImportError: cannot import name 'pywrap_tensorflow'\r\n```\r\nThe problem manifest in the environment regardless of how I try to import tensorflow (i.e. not just inside the flask app). On the same machine I have several envs (including one I installed yesterday) which work. \r\n", "comments": ["This is resolved by installing 1.6 which still apparently works in El Capitan."]}, {"number": 25028, "title": "[r1.13] Update kinesis dataset to match v2 API", "body": "**NOTE: This PR is to r1.13 branch and cherry-picked from #24903.**\r\n\r\nNote also that #24899 may need to be cherry-picked as well to conform to the tf.data API changes in 1.13.\r\n\r\nSince DatasetSource now requires varient_tensor to be passed into constructor,\r\nthe kinesis data will need to be updated to fix the incompatibility issue.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 25027, "title": "you mindmodeling must stop generate sound wave plz in future", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): last\r\n- Are you willing to contribute it (Yes/No): maybe i can, if you math teory about security\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?** yes archvie current. close do\r\n\r\n**Who will benefit with this feature?** deads people\r\n\r\n**Any Other info.** you, theserflow kill 1 peple\r\n", "comments": ["Please wait for next step for fix", "Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Are you getting the error during running of some code. It would be great if you can provide a small code to reproduce the error. Thanks!", "fight club probably with upgrade not see", "you must add adjust m.a.x. count \"stop\" button for people", "step up and continue", "I am closing this as it is not an issue relevant to this repository. Thanks!"]}, {"number": 25026, "title": "Can not build debug wheel due to zip overflow(linux)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.4\r\n- TensorFlow installed from: Source\r\n- TensorFlow version: 3ed46c325f70e2f1521b850b43d2b174101d9472\r\n- Python version: 3.4\r\n- Bazel version (if compiling from source):  0.19.2 (not from source)\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the problem**\r\n\r\nCant create wheel file for a build with debug symbols.\r\n\r\nCommand:\r\n`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\nOutput:\r\n`\r\nFri Jan 18 17:01:09 EET 2019 : === Preparing sources in dir: /tmp/tmp.Qpy8sDXUC9\r\n/tmp/tensorflow /tmp/tensorflow\r\n/tmp/tensorflow\r\nFri Jan 18 17:01:22 EET 2019 : === Building wheel\r\nwarning: no files found matching '*.pyd' under directory '*'\r\nwarning: no files found matching '*.pd' under directory '*'\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/Eigen'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/google'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/unsupported'\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 295, in <module>\r\n    keywords='tensorflow tensor machine learning',\r\n  File \"/home1/private/mavridis/.local/lib/python3.4/site-packages/setuptools/__init__.py\", line 143, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/lib64/python3.4/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib64/python3.4/distutils/dist.py\", line 955, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib64/python3.4/distutils/dist.py\", line 974, in run_command\r\n    cmd_obj.run()\r\n  File \"/home1/private/mavridis/.local/lib/python3.4/site-packages/wheel/bdist_wheel.py\", line 250, in run\r\n    wf.write_files(archive_root)\r\n  File \"/home1/private/mavridis/.local/lib/python3.4/site-packages/wheel/wheelfile.py\", line 122, in write_files\r\n    self.write(path, arcname)\r\n  File \"/home1/private/mavridis/.local/lib/python3.4/site-packages/wheel/wheelfile.py\", line 136, in write\r\n    self.writestr(zinfo, data, compress_type)\r\n  File \"/home1/private/mavridis/.local/lib/python3.4/site-packages/wheel/wheelfile.py\", line 139, in writestr\r\n    ZipFile.writestr(self, zinfo_or_arcname, bytes, compress_type)\r\n  File \"/usr/lib64/python3.4/zipfile.py\", line 1447, in writestr\r\n    data = co.compress(data) + co.flush()\r\nOverflowError: Size does not fit in an unsigned int\r\n`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nFirst build TF with debug symbols, no GPU support (runs succesfully):\r\n\r\n`bazel build --incompatible_remove_native_http_archive=false --incompatible_package_name_is_a_function=false --config=opt -c dbg --strip=never //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nBuild pip wheel file (FAILS):\r\n`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\n\r\n\r\n**Any other info / logs**\r\n\r\nThanks in advance", "comments": ["I think this is simply the package is too big, when you include debug symbols.\r\nUnless pip has support for somehow using 64 bit zip, I do not see how this can be done.\r\n\r\nI do not see how we can fix this on TF side, either.", "> I think this is simply the package is too big, when you include debug symbols.\r\nThis was my suspicion as well.\r\n> Unless pip has support for somehow using 64 bit zip, I do not see how this can be done.\r\n> \r\n> I do not see how we can fix this on TF side, either.\r\nI understand, however what is the 'recomened' aproach to building and debuging of the c++ core?\r\nBuilding twice would be tediously slow.\r\n\r\nThanks\r\n\r\n", "For C++ core debugging, depends on how you use it.\r\nIf you are simply using TF as a C++ library, you can use libtensorflow.\r\nFor debugging C++ through python, unfortunately we may be blocked on python/pip", "> For debugging C++ through python, unfortunately we may be blocked on python/pip\r\nYes this was my conclusion as well.\r\n\r\n> For C++ core debugging, depends on how you use it.\r\n> If you are simply using TF as a C++ library, you can use libtensorflow.\r\n\r\nThanks for noting that, i think i can write unit tests in c++ to stress test the new code and completely bypass the python/pin issues. \r\n\r\nThanks again.\r\n", "me too.\r\nI want to pack tensorflow1.12 dbg wheel, and then install using xxx.whl\uff0cbut i get the same error here.\r\n\r\n`root@wuhp000100031:/home/hjj/code/tensorflow# bazel-bin/tensorflow/tools/pip_package/build_pip_package /home/hjj/HuaweiGit/tensorflow_pkg_tf12_dbg\r\nMon Apr 15 20:10:07 CST 2019 : === Preparing sources in dir: /tmp/tmp.KTUCQY7nMR\r\n/home/hjj/code/tensorflow /home/hjj/code/tensorflow\r\n/home/hjj/code/tensorflow\r\nMon Apr 15 20:10:24 CST 2019 : === Building wheel\r\nwarning: no files found matching '*.pd' under directory '*'\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/Eigen'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/google'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/unsupported'\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 290, in <module>\r\n    keywords='tensorflow tensor machine learning',\r\n  File \"/usr/local/lib/python3.5/dist-packages/setuptools-40.4.3-py3.5.egg/setuptools/__init__.py\", line 140, in setup\r\n  File \"/usr/lib/python3.5/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib/python3.5/distutils/dist.py\", line 955, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib/python3.5/distutils/dist.py\", line 974, in run_command\r\n    cmd_obj.run()\r\n  File \"/root/.local/lib/python3.5/site-packages/wheel/bdist_wheel.py\", line 254, in run\r\n    wf.write_files(archive_root)\r\n  File \"/root/.local/lib/python3.5/site-packages/wheel/wheelfile.py\", line 123, in write_files\r\n    self.write(path, arcname)\r\n  File \"/root/.local/lib/python3.5/site-packages/wheel/wheelfile.py\", line 137, in write\r\n    self.writestr(zinfo, data, compress_type)\r\n  File \"/root/.local/lib/python3.5/site-packages/wheel/wheelfile.py\", line 140, in writestr\r\n    ZipFile.writestr(self, zinfo_or_arcname, bytes, compress_type)\r\n  File \"/usr/lib/python3.5/zipfile.py\", line 1573, in writestr\r\n    data = co.compress(data) + co.flush()\r\nOverflowError: Size does not fit in an unsigned int\r\nroot@wuhp000100031:/home/hjj/code/tensorflow# `\r\n\r\nso, what can i do."]}, {"number": 25025, "title": "Dilated/Atrous Conv implementation with cudnn", "body": "As #24559 described, cudnn provided the same API for dilated/atrous Conv and non-dilated/atrous Conv.  It is more like LSTMBlockCell in tf.contrib.  Maybe I should apply for a new API  of that of Conv using cudnn.  I am not familiar with PR of tensorflow. I commented the old code other than delete it. Look forward to your suggestion.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "Currently the backwards kernel is not supported on CPU which is why the code looks like it does unfortunately. Note the code for the convolution method has been updated since this commit but it may be possible to do a device check running the original on CPU and an updated version on GPU.", "@MlWoo request to re base your branch", "@MlWoo The code underneath has changed a lot so a rebase would be good so can see the changes on top of the current file. Also I am unable to see exactly what the gain is from doing this. Currently convolution supports dilation as a parameter but this code would not have a backwards pass which works on a CPU", "It has been 28 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 25024, "title": "Tensorflow GPU build for Java and Cuda 9.1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: r1.11\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): MinGW\r\n- CUDA/cuDNN version: 9.1/7.1\r\n- GPU model and memory: 1050 Ti 3GB\r\n\r\nI want to build Tensorflow 1.11 with GPU support and the appropriate Java JNI libraries. The build tool that I'm using is CMake. To build the jni-libraries, is there any options I have to supply to CMake, or are these made as default in the build process?  Bazel for windows did not work out for me. \r\n\r\nI have been at this for some (a frustrating amount of time) time now. Any help is appreciated. Solving this issue seems problematic at best. Is what I'm trying to do possible at all? Has anyone else managed to do this? All other issues I have found on this haven't helped. \r\n\r\nThank you. \r\n", "comments": ["The TensorFlow build team doesn't officially support CMake (nor 1.11, I believe, but I'm not certain), so I'm going to close this issue. Please try asking for help on Stack Overflow, where your question is more likely to help other developers."]}]