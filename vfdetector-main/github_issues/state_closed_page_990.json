[{"number": 23696, "title": "GPU crashes when running Keras/tensorflow-gpu, specifically when clock speed goes to idle at 0 MHz", "body": "Below issue was posted by @r8drascal in TF Keras repo.\r\n\r\nI'm using Jupyter Notebook to run Keras with a Tensorflow GPU backend. I've done some testing with various dummy models while simultaneously monitoring my GPU usage using MSI Afterburner, GPU-Z, nvidia-smi and Task Manager. My GPU is a GeForce GTX 960M, which has no issues running games. The temperatures are also low when running Keras.\r\n\r\nWhat I've noticed is that the Keras runs fine (e.g. loading or training a model) in the beginning but whenever Keras is not running anything, the GPU naturally wants to idle from 1097 MHz to 0 MHz and as soon as it does that the GPU crashes. I can see that the \"GPU is lost\" on nvidia-nvsmi. I have to then disable and re-enable my GPU in the Device Manager to get it to work.\r\n\r\nHere's a sample script, which I believe was from the official documentation:\r\n\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation\r\nfrom keras.optimizers import SGD\r\n\r\nimport numpy as np\r\nx_train = np.random.random((1000, 20))\r\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=50)\r\nx_test = np.random.random((100, 20))\r\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=50)\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(200, activation='relu', input_dim=20))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(200, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(50, activation='softmax'))\r\n\r\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=sgd,\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train,\r\n          epochs=20,\r\n          batch_size=128)\r\nscore = model.evaluate(x_test, y_test, batch_size=128)\r\n\r\n\r\nThe model runs fine and completes the training but after a few seconds my GPU dies. This even happens if I just load a model. I.e. I import keras modules, then use \"load_model\", and in less than a minute everything crashes as soon as the clock speed drops to 0 MHz.\r\n\r\nDoes anyone have any idea why this might be happening?", "comments": []}, {"number": 23695, "title": "Failure to parse PYTHONPATH to find keras_preprocessing?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux variant\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: (master branch, latest commit e84d75dd7429cdcbfa20518cce4a0e886271cfa0)\r\n- Python version: 3.6.5.1\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 0.18.1\r\n- GCC/Compiler version (if compiling from source): gcc 7.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am trying to build tensorflow from source using mkl-dnn.  The command I use for compilation is \r\n```\r\n../bazel-0.18.1/output/bazel build --config=mkl -c opt --copt=-g --strip=never --copt='-Wl,rpath=/opt/gcc/7.3.0/snos' --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512pf --copt=-mavx512cd --copt=-mavx512er --copt='-mtune=knl' --copt=\"-DEIGEN_USE_VML\" //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nI get the error eventually:\r\n```\r\nfrom tensorflow.python.keras.preprocessing.sequence import _remove_long_seq\r\n  File \"/gpfs/mira-home/cadams/.cache/bazel/_bazel_cadams/33ea5823a114d12c2a1643d7402e760f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/keras/preprocessing/__init__.py\", line 21, in <module>\r\n    import keras_preprocessing\r\nModuleNotFoundError: No module named 'keras_preprocessing'\r\n```\r\n\r\nFollowing several leads on the internet (such as https://stackoverflow.com/questions/51771039/error-compiling-tensorflow-from-source-no-module-named-keras-applications/51774943#51774943), I installed keras_preprocessing, keras_applications, h5py and put them on PYTHONPATH. This did not work.   \r\n\r\nUltimately, the solution that *did* work was to manually edit the file tensorflow/python/keras/preprocessing/__init__.py to import sys and for the folder containing keras_preprocessing to be added using sys.path.append.\r\n\r\n", "comments": ["When you run ./configure it will ask something like:\r\n\r\n`Please input the desired Python library path to use.  Default is [/usr/lib/python2.7/dist-packages]`\r\n\r\nthat value is saved in as `PYTHON_LIB_PATH` in `.tf_configure.bazelrc` in the root of the TensorFlow clone\r\n\r\nIt is in that directory that the build is expecting to find keras_preprocessing and other packages installed. ", "Ok, I started from scratch to carefully test this, and my build still failed.  Below I've got a lot of the output so you can see if I configured tensorflow correctly, which I think I did.\r\n\r\n1. I downloaded tensorflow fresh, bazel fresh, installed the necessary python packages fresh:\r\n```bash\r\n$ mkdir tf_second_install\r\n$ cd tf_second_install/\r\n$ git clone https://github.com/tensorflow/tensorflow.git\r\n$ wget https://github.com/bazelbuild/bazel/releases/download/0.18.1/bazel-0.18.1-dist.zip\r\n$ export TF_INSTALL=$PWD/tf_install\r\n$ export PYTHONPATH=$TF_INSTALL\r\n$ pip install --target=$TF_INSTALL --upgrade --no-deps keras-applications keras-preprocessing h5py\r\n```\r\n\r\n2. I built bazel with the following commands, which succeeded:\r\n```bash\r\n$ cd bazel-0.18.1\r\n$ mv ../bazel-0.18.1-dist.zip ./\r\n$ unzip bazel-0.18.1-dist.zip\r\n$ bash ./compile.sh\r\n$ cd ../\r\n# Much output removed\r\nBuild successful! Binary is here: /projects/datascience/cadams/software/tf_second_install/bazel-0.18.1/output/bazel\r\n$ export PATH=/projects/datascience/cadams/software/tf_second_install/bazel-0.18.1/output/:$PATH\r\n```\r\n\r\n3. Configuration of tensorflow:\r\n```bash\r\nConfiguration of tensorflow:\r\n$ ./configure\u00a0\r\nWARNING: Processed legacy workspace file /lus/theta-fs0/projects/datascience/cadams/software/tf_second_install/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\r\nExtracting Bazel installation...\r\nWARNING: ignoring LD_PRELOAD in environment.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.18.1- (@non-git) installed.\r\nPlease specify the location of python. [Default is /opt/python/3.6.5.3/bin/python]:\u00a0\r\n\r\n\r\nFound possible Python library paths:\r\n\u00a0 /projects/datascience/cadams/software/tf_second_install/tf_install\r\n\u00a0 /opt/python/3.6.5.3/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.\u00a0 Default is [/projects/datascience/cadams/software/tf_second_install/tf_install]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]:\u00a0\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\u00a0\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --config=mkl\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Build with MKL support.\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --config=monolithic\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Config for mostly static monolithic build.\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --config=gdr\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Build with GDR support.\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --config=verbs\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Build with libverbs support.\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --config=ngraph\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Build with Intel nGraph support.\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --config=dynamic_kernels\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --config=noaws\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Disable AWS S3 filesystem support.\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --config=nogcp\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Disable GCP support.\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --config=nohdfs\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Disable HDFS support.\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --config=noignite\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Disable Apacha Ignite support.\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --config=nokafka\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Disable Apache Kafka support.\r\nConfiguration finished\r\n```\r\n\r\nI used the defaults for both the location of python and for python library paths, which I think is recorded correctly in the `.tf_configure.bazelrc` file:\r\n```bash\r\n$ cat .tf_configure.bazelrc\r\nimport %workspace%/tools/bazel.rc\r\nbuild --action_env PYTHON_BIN_PATH=\"/opt/python/3.6.5.3/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/projects/datascience/cadams/software/tf_second_install/tf_install\"\r\nbuild --python_path=\"/opt/python/3.6.5.3/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=xla\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"0\"\r\nbuild --action_env TF_DOWNLOAD_CLANG=\"0\"\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\n```\r\n\r\nLooking at this, the PYTHON_LIB_PATH appears to be set correctly, and the packages are installed there:\r\n```bash \r\n$ ls /projects/datascience/cadams/software/tf_second_install/tf_install\r\nh5py                  Keras_Applications-1.0.6.dist-info\r\nh5py-2.8.0.dist-info  keras_preprocessing\r\nkeras_applications    Keras_Preprocessing-1.0.5.dist-info\r\n```\r\n\r\n4. Still, the build fails at the same spot when it can not import keras_preprocessing:\r\n```\r\nERROR: /lus/theta-fs0/projects/datascience/cadams/software/tf_second_install/tensorflow/tensorflow/BUILD:562:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/gpfs/mira-home/cadams/.cache/bazel/_bazel_cadams/5d2ab57ebd6673dd9517877558fa4b10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/gpfs/mira-home/cadams/.cache/bazel/_bazel_cadams/5d2ab57ebd6673dd9517877558fa4b10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 81, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/gpfs/mira-home/cadams/.cache/bazel/_bazel_cadams/5d2ab57ebd6673dd9517877558fa4b10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py\", line 29, in <module>\r\n    from tensorflow.python.keras import datasets\r\n  File \"/gpfs/mira-home/cadams/.cache/bazel/_bazel_cadams/5d2ab57ebd6673dd9517877558fa4b10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/keras/datasets/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras.datasets import imdb\r\n  File \"/gpfs/mira-home/cadams/.cache/bazel/_bazel_cadams/5d2ab57ebd6673dd9517877558fa4b10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/keras/datasets/imdb.py\", line 25, in <module>\r\n    from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq\r\n  File \"/gpfs/mira-home/cadams/.cache/bazel/_bazel_cadams/5d2ab57ebd6673dd9517877558fa4b10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/keras/preprocessing/__init__.py\", line 21, in <module>\r\n    import keras_preprocessing\r\nModuleNotFoundError: No module named 'keras_preprocessing'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1770.461s, Critical Path: 236.77s\r\nINFO: 11483 processes: 11483 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nDo you spot something I am not doing right?  I can only get this section of the build to succeed by manually editing the `__init__.py` file to add to the sys.path variable, as mentioned.", "Thank you @coreyjadams , I can confirm this a problem. I see now the same issue is being discussed in: https://github.com/tensorflow/tensorflow/issues/22395\r\n\r\nI'm trying a few things on my end and might end up submitting a PR if I can fix it. [It's odd that configure parses PYTHONPATH, but then fails to pass PYTHONPATH into the bazel environment.]", "Great, I'm glad it's reproducible!  When/If you have a PR candidate, I can attempt a build on our system to see if it works for me too, if you like.\r\n", "Hi, @coreyjadams can you please try cherry-picking this fix: https://github.com/wdirons/tensorflow/commit/01c28a94e63edc8aa301ee377ca026107762858b\r\n\r\nor using the branch from my fork. ", "Hi @wdirons , this fix does let the build complete without problems, and none of the interventions I mentioned above were required.\r\n\r\nAre you planning to submit this as a pull request?", "yes, thanks for verifying. I was going to ask the owner of the other defect (#22395), if he agreed with how I fixed it then submit a PR."]}, {"number": 23694, "title": "Fix paths used in script", "body": "Fix paths used in script, broke with migration from contrib", "comments": ["Changes are merged internally , closing this PR"]}, {"number": 23693, "title": "Dilated convolution support with nn-api", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.11\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTry to run a graph that has dilated convolutions in it with nn-api enabled, and it just fails:\r\nNNAPI does not support dilated Conv2D.\r\nThis library justifies its existence on the lack of this: https://github.com/daquexian/DNNLibrary\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAndroid developers who want to run segmentation models. These models commonly use dilated conv.\r\n\r\n**Any Other info.**\r\n", "comments": ["Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@lauriebyrum, dialted conv is already supported in TFLite. NN API is adding it in future Android version. So it is strange that DNNLibrary claims it can support this through NN API. Dilated convolution can also accomplished in an unfused way by using SpaceToBatch and BatchToSpace (this is how TensorFlow implements dilated conv i.e. atrous_conv_2d). Please clarify.", "I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!", "Is dilated conv supported for quantization case using create_training_graph() ? "]}, {"number": 23692, "title": "Tensor RT 5 Windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.18.1\r\n- GCC/Compiler version (if compiling from source): Visual Stuido 2015\r\n- CUDA/cuDNN version: CUDA 10/ cudnn 7\r\n- GPU model and memory: Titan V\r\n\r\n\r\n\r\n**Describe the problem**\r\nUnable to build with Tensor RT in windows. Newest version (Tensor RT5) claims windows support but build script (configure.py) assumes linux only.\r\n\r\nHas anybody successfully built with TensorRT5 on windows? Will TF 1.13 include support?\r\n", "comments": ["Can you please post the error message you are getting when you try to build Tensor RT in windows?", "There's no error. The config scripts check if windows and don't give the option to build with tensor rt support. Tensor RT path search all assume linux as well", "@trevor-m @pooyadavoodi  Can you PTAL", "I've also been trying to build tensorflow with TensorRT, which is unsupported in Windows. I've been modifying some of the build files and bazel files but ultimately I cause bazel itself to crash... I'd be grateful if you could give me any pointers on what are the main issues with supporting Windows here.\r\n[update: It's building now, I'll see the results tomorrow]", "In the end I got it to compile by doing all the following. I think this is enough as a basis for TF/Nvidia to produce something cleaner that works ok so you can finally support Windows.\r\n@trevor-m @pooyadavoodi @samikama  PTAL\r\n\r\n 1 -Patch configure.py set tensorrt path & version\r\n   if is_windows(): _DEFAULT_TENSORRT_PATH_LINUX = _DEFAULT_CUDA_PATH_WIN\r\n   environ_cp['TF_NEED_TENSORRT'] = '1'\r\n   I think /lib was added later in the build scripts:\r\n     TENSORRT_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib\r\n  And some more, this is my configure.py in the end: https://pastebin.com/mwyDHbjJ\r\n \r\n 2 - D:\\TensorFlow\\v1.13.1\\third_party\\tensorrt\\tensorrt_configure.bzl\r\n  L117-8 set lib to %s.lib (remove .so, version)\r\n  L162: remove config fail if not linux\r\n \r\n 3 - Apply the patches to BUILD and contrib/tensorrt/BUILD:\r\n  These, specifically: https://github.com/tensorflow/tensorflow/issues/22005#issuecomment-423799141\r\n \r\n 4 - D:\\TensorFlow\\v1.13.1\\tensorflow\\contrib\\tensorrt\\segment\\segment.cc: patch excessive use of lambdas that chokes VS2015 (L245). (Just delete get_node and call the in/out_nodes funcs conditionally later where get node was used once.)\r\n \r\n 5- Now build with bazel. Link will fail; after it fails:\r\n -C:\\Users\\....\\_bazel_....\\[vrkp5ufx...]\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\libtensorflow_cc.so-2.params:\r\n  remove /WHOLEARCHIVE: in front of nvinfer.lib (otherwise you'll see a corrupt lib link error with vs2017 link or internal error and crash with vs2015)\r\n  remove bazel-out/x64_windows-opt/genfiles/tensorflow/python/_pywrap_tensorflow_internal.lib\r\n  (causes duplicate symbol definitions)\r\n And execute the link command manually, so your modified libtensorflow_cc.so-2.params doesn't get overwritten by bazel before linking (you'll have the exact command and env vars dumped before, when it crashed).", "@pooyadavoodi @trevor-m Is this done. If so please let me know so that I can close.", "I am trying to build tensorflow 2.  There is also no option to support TensorRT in configure.py.  Please update tensorflow to support Windoww having Tensor RT support.", "> I am trying to build tensorflow 2. There is also no option to support TensorRT in configure.py. Please update tensorflow to support Windoww having Tensor RT support.\r\n\r\n+1", "@alvaroslm Hi, did you see a speed up using this dll with tensorrt under windows? ", "@samikama I'm a new bee in windows tensorflow\uff0cI'm confused by the 5th step of the compiling process, could please give an example on how to execute the link command manually ?\r\n\r\nThank you very much in advance.", "Just compiled the dll successfully. However, another question raised. Currently I am using TensorRT5.0.4.3 on windows to compile tensorflow. But the optimized tftrt model file is generated on ubuntu with tensorrt 5.0.2.6, which is incompatible  with the version under windows. But there is no tensorrt5.0.4.3 for linux... ", "I think you would have  to optimize the model on Windows.\r\nEven if the TensorRT versions are the same, the optimized TRT engines are platform dependent.\r\n\r\nI guess if you use `us_dynamic_op=True` in order to postpone the engine build to inference time on Windows, then having the same TRT version on both platform would be useful.", "> I think you would have to optimize the model on Windows.\r\n> Even if the TensorRT versions are the same, the optimized TRT engines are platform dependent.\r\n> \r\n> I guess if you use `us_dynamic_op=True` in order to postpone the engine build to inference time on Windows, then having the same TRT version on both platform would be useful.\r\n\r\nHi @pooyadavoodi , do you know how to optimize the model on Windows? I tried to use this ConvertGraphDefToTensorRT function to convert the tf model to a trt model, however, I did not see any speed up here.", "I've never run TF-TRT on windows.\r\nWhy not using the same Python API `TrtGraphConverter`?", "> I've never run TF-TRT on windows.\r\n> Why not using the same Python API `TrtGraphConverter`?\r\n\r\n@pooyadavoodi Because there is no python API 'TrtGraphConverter' under windows platform... ", "You can set the session config to do conversion on the first sess.run() call. You can check the Python bindings of the trt converter to see how to setup config.proto to trigger TFTRT pass.", "> You can set the session config to do conversion on the first sess.run() call. You can check the Python bindings of the trt converter to see how to setup config.proto to trigger TFTRT pass.\r\n\r\nHi, could you please explain a little bit more? I could not quite understand. Do you mean using python API to convert a model to trt like the example?", "Trt conversion is a grappler pass like any other graph optimizers in TF. Python bindings setup a session with proper session config that triggers TFTRT optimizer. If you setup session config, you can trigger the conversion yourself. It is applicable both from Python and C++. ", "See https://github.com/tensorflow/tensorflow/blob/55fdf9f5f3485170d083128b6ea6325d0cbc2424/tensorflow/python/compiler/tensorrt/trt_convert.py#L220 for example", "@samikama Hi, thank you for your reply. Would you please have a look at my code?\r\n\r\n```\r\n        Session* session;\r\n\tauto options = tensorflow::SessionOptions();\r\n\toptions.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.5);\r\n\toptions.config.mutable_gpu_options()->set_allow_growth(true);\r\n\tStatus status = NewSession(options, &session);\r\n\r\n\tif (!status.ok()) {\r\n\t\tstd::cerr << status.ToString() << std::endl;\r\n\t\treturn 1;\r\n\t}\r\n\telse {\r\n\t\tstd::cout << \"Session created successfully\" << std::endl;\r\n\t}\r\n\r\n\t// Load the protobuf graph\r\n\tGraphDef graph_def;\r\n\tstd::string graph_path = argv[1];\r\n\tstatus = ReadBinaryProto(Env::Default(), graph_path, &graph_def);\r\n\tif (!status.ok()) {\r\n\t\tstd::cerr << status.ToString() << std::endl;\r\n\t\treturn 1;\r\n\t}\r\n\telse {\r\n\t\tstd::cout << \"Load graph protobuf successfully\" << std::endl;\r\n\t}\r\n\r\n\tint batch_size = 8;\r\n\tGraphDef outGraph;\r\n\tStatus conversion_status =\r\n\t\ttensorrt::convert::ConvertGraphDefToTensorRT(\r\n\t\t\tgraph_def, { \"edsr/lambda_3/mul:0\" }, batch_size, 2097152,\r\n\t\t\t&outGraph, 1, 3, true);\r\n\r\n\tstatus = session->Create(outGraph);\r\n\tsession->Run(inputs, { OutputName }, {}, &finalOutput);\r\n...\r\n```\r\nIs this correct? I am still confused about how to set session config in C++.", "You almost got it. You need to setup `graph_options.rewrite_config` options of `options.config` as done in lines\r\nhttps://github.com/tensorflow/tensorflow/blob/ff76c866295fba11d992f5562f628d3ab95c5860/tensorflow/python/compiler/tensorrt/trt_convert.py#L442-L447\r\nbefore you create NewSession.\r\nHow you construct a rewrite_config from trt conversion arguments are shown in the method I mentioned earlier above. These are protobuf objects so if you are not familiar with it, you may want to do some reading on how to handle protobuf on C++ \r\n\r\nAfter you create a session with properly formatted sessionoptions, any graph that you run with that session will automatically converted to TFTRT graph before execution so you don't need to convert it in advance. Just load regular frozen graphdef and it should just work but will take some time to process first sess.run(). Unless you change feeds and fetches, subsequent calls to sess.run() will not trigger any optimizations and will be quite fast.\r\n ", "> You almost got it. You need to setup `graph_options.rewrite_config` options of `options.config` as done in lines\r\n> https://github.com/tensorflow/tensorflow/blob/ff76c866295fba11d992f5562f628d3ab95c5860/tensorflow/python/compiler/tensorrt/trt_convert.py#L442-L447\r\n> \r\n> \r\n> before you create NewSession.\r\n> How you construct a rewrite_config from trt conversion arguments are shown in the method I mentioned earlier above. These are protobuf objects so if you are not familiar with it, you may want to do some reading on how to handle protobuf on C++\r\n> After you create a session with properly formatted sessionoptions, any graph that you run with that session will automatically converted to TFTRT graph before execution so you don't need to convert it in advance. Just load regular frozen graphdef and it should just work but will take some time to process first sess.run(). Unless you change feeds and fetches, subsequent calls to sess.run() will not trigger any optimizations and will be quite fast.\r\n\r\nThank you very much! Let me have a look at that.", "@samikama Hi Sam, I just found that `ConvertGraphDefToTensorRT` this function already constructed the config proto inside the function. \r\nhttps://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/contrib/tensorrt/convert/convert_graph.cc#L289\r\nSo do I still have to construct the config myself?\r\n", "Guys if you are doing openpose then just comment this line\r\n`import tensorflow.contrib.tensorrt as trt`\r\nin `tf_pose/estimator.py`", "File \"C:\\Users\\Labpc\\AppData\\Local\\Temp\\pip-install-4a4ut353\\nvidia-pyindex\\setup.py\", line 216, in <module>\r\n        \"supported.\".format(sys.platform))\r\n    OSError: Your operating system is not supported by `nvidia-pyindex`: win32.\r\n    Only Linux systems are supported.\r\nAn i downloaded the windows version ,\r\n\r\nEven though i cant installl the Tensor Rt in windows i got this error \r\n\r\ncan any one clear it up\r\n\r\nThank you @sanjoy @dbonner @pooyadavoodi @jtavrisov @ZhuoranLyu @ZhuoranLyu ", "@punitha-valli  are you try to build tensorrt in windows for python?", "Hi,\n\nThank you for your reply\n\nCan you please share me the tensorrt file for Windows ,\n\n\nIt will be great help for me\n\nThank you so much\n\nOn Sun, Feb 28, 2021, 6:15 AM sabrabano0 <notifications@github.com> wrote:\n\n> @punitha-valli <https://github.com/punitha-valli> are you try to build\n> tensorrt in windows for python?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23692#issuecomment-787195743>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AMM7B24TTKBFJXDPK4T4O4TTBFVIJANCNFSM4GDJLC3A>\n> .\n>\n", "@jtavrisov Could you please let us know if it is still an issue with TF v2.6.0 ? Please refer to this similar [issue](https://stackoverflow.com/questions/65283620/how-to-install-wheel-package-of-tensorrt-for-tensorflow-on-windows-10) ,and let us know if it helps ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23692\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23692\">No</a>\n"]}, {"number": 23691, "title": "[Regresssion] TypeError: In op 'ScatterUpdate', input types are not compatible with expected types", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source): 0.18.0\r\n- GCC/Compiler version (if compiling from source):  8.2.1\r\n\r\n**Describe the current behavior**\r\n`tf.scatter_update` keeps failing with TypeError\r\n\r\n**Describe the expected behavior**\r\nShould work like with TF 1.11.0\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.Variable([1,2,3,4,5])\r\nidx = tf.placeholder(dtype=tf.int32, shape=(None,))\r\nupdate = tf.placeholder(dtype=tf.int32, shape=(None,))\r\ntf.scatter_update(x, idx, update)\r\n```\r\n\r\nThis example works with Tensorflow 1.11 but fails with 1.12.\r\nIt does not matter whether the shapes of `idx` and `update` are set explicitly or `None`.\r\n", "comments": ["After some debugging I found out that this fails only after running some code I did not identify yet.\r\n\r\n\r\nStack trace: \r\n```python\r\n  File \"[...]/tf/rsa/estimator.py\", line 86, in map_fn_EM\r\n    updates=tf.transpose(model.estimated_mixture_log_prob)\r\n  File \"/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 296, in scatter_update\r\n    use_locking=use_locking, name=name)\r\n  File \"/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 1210, in scatter_update\r\n    use_locking=use_locking, name=name)\r\n  File \"/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 738, in create_op\r\n    **kwargs)\r\n  File \"/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1748, in __init__\r\n    input_types))\r\nTypeError: In op 'ScatterUpdate', input types ([tf.float64, tf.int32, tf.float64]) are not compatible with expected types ([tf.float64_ref, tf.int32, tf.float64])\r\n```\r\n\r\nThe exact cause of the raised error in my code is the following:\r\n```python\r\n# inputs\r\n[<tf.Tensor 'ScatterUpdate/Placeholder:0' shape=(2221, 3) dtype=float64>,\r\n <tf.Tensor 'arg1:0' shape=(?,) dtype=int32>,\r\n <tf.Tensor 'transpose:0' shape=(?, 3) dtype=float64>]\r\n# input_types\r\n[tf.float64_ref, tf.int32, tf.float64]\r\n```\r\n=> `tf.Tensor 'ScatterUpdate/Placeholder:0' shape=(2221, 3) dtype=float64` is not compatible with `tf.float64_ref`.\r\n\r\n`inputs` gets created somewhere in `_apply_op_helper, op_def_library.py`.", "This is a bug on your code. In some place you're passing a tensor, not a variable, as the first argument to tf.scatter_update.", "@alextp The problem is that I definitely don't do that.\r\nIt is 100% sure a tf.Variable.\r\nI used Dataset.reduce() to scan over a dataset and scatter_update in each iteration a feature vector of the length of that Dataset.", "Can you print the object before each call to scatter_update? Remember that tf ops which consume variables often return tensors, not variables.", "@alextp Thanks for your help, I will try to.\r\n\r\nFor the moment, I can show you the source code, but it's maybe little bit hard to understand.\r\nHere, I set up a tf.Variable:\r\nhttps://github.com/theislab/batchglm/blob/a305c6cb1438c9fa728a931fe67b2a1b2f31867f/batchglm/train/tf/rsa/base.py#L379\r\n```python3\r\nmixture_logits = tf.Variable(init_mixture_logits, name=\"mixture_logits\")\r\n[...]\r\nself.mixture_logits_var = mixture_logits\r\n```\r\nThis tf.Variable will be passed to `scatter_update` here:\r\nhttps://github.com/theislab/batchglm/blob/a305c6cb1438c9fa728a931fe67b2a1b2f31867f/batchglm/train/tf/rsa/estimator.py#L83-L87\r\n```python3\r\nmixture_EM_update = tf.scatter_update(\r\n    ref=model_vars.mixture_logits_var,\r\n    indices=idx,\r\n    updates=tf.transpose(model.estimated_mixture_log_prob)\r\n)\r\n```\r\n\r\nSo, I really don't get how `model_vars.mixture_logits_var` could change its type; \r\nespecially since the same code works in my own `map_reduce` implementation:\r\nhttps://github.com/theislab/batchglm/blob/4daf7d7f27955764e3871fc8296d1024808ac785/batchglm/train/tf/rsa/estimator.py#L76-L80"]}, {"number": 23690, "title": "Fix broken path", "body": "", "comments": ["Thanks."]}, {"number": 23689, "title": "Object detection demo crashing using custom model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): just changed configuration values to match my model \r\n\r\n`  private static final int TF_OD_API_INPUT_SIZE = 640;`\r\n` private static final boolean TF_OD_API_IS_QUANTIZED = false;`\r\n`private static final String TF_OD_API_MODEL_FILE = \"ssd.tflite\";`\r\n`private static final String TF_OD_API_LABELS_FILE = \"file:///android_asset/ssd_labels.txt\";`\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2 XL\r\n- TensorFlow installed from (source or binary): demo commit 61c6c84\r\n- TensorFlow version (use command below):\r\n- Python version: na\r\n- Bazel version (if compiling from source):  na\r\n- GCC/Compiler version (if compiling from source): na\r\n- CUDA/cuDNN version: na\r\n- GPU model and memory: na\r\n\r\n**Describe the current behavior**\r\nCamera view opens for a few seconds, then crashes with output log below\r\n\r\n>  --------- beginning of crash\r\n2018-11-12 10:51:23.193 22685-22703/org.tensorflow.lite.demo A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 22703 (inference), pid 22685 (rflow.lite.demo)\r\n2018-11-12 10:51:23.240 22860-22860/? I/crash_dump64: obtaining output fd from tombstoned, type: kDebuggerdTombstone\r\n2018-11-12 10:51:23.241 872-872/? I//system/bin/tombstoned: received crash request for pid 22685\r\n2018-11-12 10:51:23.241 22860-22860/? I/crash_dump64: performing dump of process 22685 (target tid = 22703)\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG: Build fingerprint: 'Android/aosp_taimen/taimen:8.1.0/OPM2.171026.006.H1/mcabah10301330:userdebug/test-keys'\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG: Revision: 'rev_10'\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG: ABI: 'arm64'\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG: pid: 22685, tid: 22703, name: inference  >>> org.tensorflow.lite.demo <<<\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x0   0000000000000000  x1   00000000000058af  x2   0000000000000006  x3   0000000000000008\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x4   0000007674fb8000  x5   0000007674fb8000  x6   0000007674fb8000  x7   0000001400000002\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x8   0000000000000083  x9   0000000010000000  x10  0000007688956a90  x11  0000000000000001\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x12  0000001400000002  x13  0000010000000002  x14  00000000000000ff  x15  00000076811417c4\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x16  000000594a538fa8  x17  000000771f8c352c  x18  0000000000000400  x19  000000000000589d\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x20  00000000000058af  x21  0000000000000083  x22  0000007688956d58  x23  0000007688956d70\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x24  000000000000004c  x25  000000769d28aac0  x26  000000767e6328c0  x27  000000769d28aaf8\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x28  000000769d2439a0  x29  0000007688956ad0  x30  000000771f878760\r\n2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     sp   0000007688956a90  pc   000000771f878788  pstate 0000000060000000\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG: backtrace:\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #00 pc 000000000001d788  /system/lib64/libc.so (abort+120)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #01 pc 00000000000cab70  /data/app/org.tensorflow.lite.demo-_VB0X9UVAsVLe_Ef7nDX7g==/lib/arm64/libtensorflowlite_jni.so\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #02 pc 00000000000cc5d8  /data/app/org.tensorflow.lite.demo-_VB0X9UVAsVLe_Ef7nDX7g==/lib/arm64/libtensorflowlite_jni.so\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #03 pc 00000000000c9084  /data/app/org.tensorflow.lite.demo-_VB0X9UVAsVLe_Ef7nDX7g==/lib/arm64/libtensorflowlite_jni.so\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #04 pc 000000000011cfb8  /data/app/org.tensorflow.lite.demo-_VB0X9UVAsVLe_Ef7nDX7g==/lib/arm64/libtensorflowlite_jni.so\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #05 pc 0000000000011278  /data/app/org.tensorflow.lite.demo-_VB0X9UVAsVLe_Ef7nDX7g==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+32)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #06 pc 0000000000553bf0  /system/lib64/libart.so (art_quick_generic_jni_trampoline+144)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #07 pc 000000000054ae4c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #08 pc 00000000000dc5d0  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+264)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #09 pc 000000000029b49c  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #10 pc 0000000000295a90  /system/lib64/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+700)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #11 pc 0000000000533f50  /system/lib64/libart.so (MterpInvokeStatic+264)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #12 pc 000000000053ca94  /system/lib64/libart.so (ExecuteMterpImpl+14612)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #13 pc 0000000000275c00  /system/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame&, art::JValue, bool)+444)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #14 pc 000000000027b7cc  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame*, art::JValue*)+216)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #15 pc 0000000000295a70  /system/lib64/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+668)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #16 pc 0000000000532ad8  /system/lib64/libart.so (MterpInvokeVirtual+652)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #17 pc 000000000053c914  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #18 pc 0000000000275c00  /system/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame&, art::JValue, bool)+444)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #19 pc 0000000000525450  /system/lib64/libart.so (artQuickToInterpreterBridge+1052)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #20 pc 0000000000553d0c  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)\r\n2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #21 pc 0000000000009644  /dev/ashmem/dalvik-jit-code-cache (deleted)\r\n\r\n\r\n", "comments": ["@achowdhery,  is it possible the non-quantized path doesn't work?", "Thanks for the feedback. We are checking on our end. Was this working in the Tensorflow version 1.11 and stopped working recently?", "Seems to be working when we pin the build to 1.10, but crashes using the nightly build", "A recent change to `tensorflow-lite:0.0.0-nightly` has re-introduced this bug. I am getting the exact same behavior mentioned above, despite the same code working fine 2-3 weeks ago. ", "Marking issue as resolved due to inactivity. Feel free to re-open this if it's unresolved or file a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose)"]}, {"number": 23688, "title": "remove reduce_ from established functions, make it closer to numpy", "body": "**System information**\r\n- TensorFlow version (you are using): >>> tf.__version__\r\n'1.11.0\r\n- Are you willing to contribute it (Yes/No): I would like to but have no idea how.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nHi everyone. Thanks for your efforts in making tensorflow.\r\nI've been using tensorflow for some time now and no matter my usecase I always find myself in the position of having forgotten how to call the max, sum, ..., etc. functions. I find that the current naming is utterly confusing and doesn't help ppl to easily learn and integrate with the api. It would have been much better if all possible functions were as close as possible to numpy naming conventions. Trying to translate code from some other framework that follows a numpy naming conventions I find myself typing `tf.max` only then to realize that it doesn't exist. Then I have to spend time re-learning that is not `tf.max` but `tf.reduce_max`. Now it could be only me but I suspect that this kind of notation doesn't stick in to ppls long term memory.\r\n\r\n**Will this change the current api? How?**\r\nYes. Most common functions will have the same naming convention to numpy.\r\n\r\n**Who will benefit with this feature?**\r\nAll existing and new users.\r\n\r\n**Any Other info.**\r\nPlease keep unnecessary complexity out of tensorflow.  \r\n", "comments": ["This has come up before, but the answer has generally been that we will not alias these given that the expectation of matching Numpy would be met nominally, but not in full. It would be valuable to implement a more full numpy correspondence, but that is beyond the scope of the issue here. @asimshankar to correct the above if I have misstated it, but I will close this issue for now."]}, {"number": 23687, "title": "scatter_max doesn't work with MirroredStrategy since v1.11.0", "body": "\r\n**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: tensorflow-gpu v1.12.0-0-ga6d8ffae09, v1.11.0-0-gc19e29306c\r\n- Python version: python 2.7.12\r\n- Bazel version: N/A\r\n- GCC/Compiler version: N/A \r\n- CUDA/cuDNN version: CUDA 9.0 / cuDNN 7.1.4.18-1+cuda9.0\r\n- GPU model and memory: (GeForce GTX 1080Ti / 11172MiB) x 2\r\n\r\n**Describe the current behavior**\r\nscatter_max does not work with MirroredStrategy since v1.11.0.\r\n\r\n**Describe the expected behavior**\r\nscatter_max can work with MirroredStrategy in tensorflow-gpu 1.10.0\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\nimport os, sys\r\n\r\ndef model_func(features, labels, mode, params):\r\n    tmp_var = tf.Variable(tf.zeros(shape=[2]), trainable=False)\r\n    a=tf.get_variable('a', initializer=tf.ones(shape=[]), trainable=True, aggregation=tf.VariableAggregation.MEAN)\r\n    with tf.control_dependencies([tmp_var.initializer]):\r\n        y = tf.stop_gradient(tf.scatter_max(tmp_var, [0, 1], tf.random_uniform([2])))\r\n        loss = tf.reduce_sum(y * a)\r\n    opt = tf.train.AdamOptimizer().minimize(loss, tf.train.get_global_step())\r\n    return tf.estimator.EstimatorSpec(mode=tf.estimator.ModeKeys.TRAIN, loss=loss, train_op=opt)\r\n\r\ndef input_fn():\r\n    features =  tf.data.Dataset.from_tensors([[0.]]).repeat()\r\n    labels = tf.data.Dataset.from_tensors(0.).repeat()\r\n    return tf.data.Dataset.zip((features, labels))\r\n\r\ndef main(argv):\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\r\n    run_config = tf.estimator.RunConfig(\r\n        train_distribute=tf.contrib.distribute.MirroredStrategy(num_gpus=2),\r\n        session_config=tf.ConfigProto(\r\n            allow_soft_placement=True,\r\n            gpu_options=tf.GPUOptions(force_gpu_compatible=True))\r\n    )\r\n    model = tf.estimator.Estimator(model_fn=model_func, config=run_config)\r\n\r\n    model.train(input_fn=input_fn, steps=100)\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\ntf.app.run(argv=[sys.argv[0]])\r\n```\r\n\r\n**Other info / logs**\r\n\r\nerror logs from up code running in v1.12.0\r\n```\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:Not using Distribute Coordinator.\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpiGysT0\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': gpu_options {\r\n  force_gpu_compatible: true\r\n}\r\nallow_soft_placement: true\r\n, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f7bc01ee910>, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7bc01ee890>, '_model_dir': '/tmp/tmpiGysT0', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': '', '_distribute_coordinator_mode': None}\r\nWARNING:tensorflow:Estimator's model_fn (<function model_func at 0x7f7bc5e48578>) includes params argument, but params are not passed to Estimator.\r\n2018-11-12 06:18:54.074313: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-11-12 06:18:55.364237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.75GiB\r\n2018-11-12 06:18:55.511370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.75GiB\r\n2018-11-12 06:18:55.512283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2018-11-12 06:18:55.949305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-12 06:18:55.949341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 \r\n2018-11-12 06:18:55.949347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y \r\n2018-11-12 06:18:55.949351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N \r\n2018-11-12 06:18:55.949765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 10400 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2018-11-12 06:18:55.950082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 10400 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nINFO:tensorflow:Configured nccl all-reduce.\r\n2018-11-12 06:18:55.968070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2018-11-12 06:18:55.968135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-12 06:18:55.968145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 \r\n2018-11-12 06:18:55.968150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y \r\n2018-11-12 06:18:55.968155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N \r\n2018-11-12 06:18:55.968526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10400 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2018-11-12 06:18:55.968672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10400 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Error reported to Coordinator: \r\nTraceback (most recent call last):\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/tmp/model_test.py\", line 9, in model_func\r\n    y = tf.stop_gradient(tf.scatter_max(tmp_var, [0, 1], tf.random_uniform([2])))\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 719, in scatter_max\r\n    use_locking=use_locking, name=name)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 510, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 439, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\nTraceback (most recent call last):\r\n  File \"/tmp/model_test.py\", line 33, in <module>\r\n    tf.app.run(argv=[sys.argv[0]])\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/tmp/model_test.py\", line 30, in main\r\n    model.train(input_fn=input_fn, steps=100)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1205, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1316, in _train_model_distributed\r\n    self.config)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/training/distribute.py\", line 721, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 556, in _call_for_each_tower\r\n    return _call_for_each_tower(self, fn, *args, **kwargs)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 183, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/tmp/model_test.py\", line 9, in model_func\r\n    y = tf.stop_gradient(tf.scatter_max(tmp_var, [0, 1], tf.random_uniform([2])))\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 719, in scatter_max\r\n    use_locking=use_locking, name=name)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 510, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 439, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\n```", "comments": ["I've encountered the same problem, it only works in 1.10. Does anyone has better solution? ", "@ymodak \r\nscatter_add also cannot work with MirroredStrategy. ", "Can you please check if this works in 1.13 or latest nightly and let us know if it is still broken?", "@guptapriya, Both 1.13 and nightly still give same errors.", "Thanks for the update. We will look into it. ", "This still fails (`scatter_add`) in the latest v2.0. It's been more than half a year... Has anyone tried to fix it?", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23687\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23687\">No</a>\n"]}, {"number": 23686, "title": "ImportError: DLL load failed: The specified procedure could not be found.", "body": "i got this message error\r\n   \r\n       ImportError: DLL load failed: The specified procedure could not be found.\r\n\r\ni use this packages with python 3.6.0\r\n\r\n$ pip list\r\nPackage              Version\r\n-------------------- ------------------\r\nabsl-py              0.6.1\r\nAPScheduler          3.5.3\r\nastor                0.7.1\r\nattrs                18.2.0\r\nAutomat              0.7.0\r\nboto3                1.9.42\r\nbotocore             1.12.42\r\ncertifi              2018.10.15\r\ncffi                 1.11.5\r\nchardet              3.0.4\r\nClick                7.0\r\ncloudpickle          0.6.1\r\ncolorama             0.4.0\r\ncolorclass           2.2.0\r\ncoloredlogs          10.0\r\ncolorhash            1.0.2\r\nConfigArgParse       0.13.0\r\nconstantly           15.1.0\r\ncycler               0.10.0\r\ncymem                2.0.2\r\ncytoolz              0.9.0.1\r\ndecorator            4.3.0\r\ndill                 0.2.8.2\r\ndocopt               0.6.2\r\ndocutils             0.14\r\nen-core-web-md       2.0.0\r\nfakeredis            0.10.3\r\nfbmessenger          5.3.2\r\nFlask                1.0.2\r\nFlask-Cors           3.0.7\r\nFlask-JWT-Simple     0.0.3\r\nfuture               0.17.1\r\ngast                 0.2.0\r\ngevent               1.3.7\r\ngreenlet             0.4.15\r\ngrpcio               1.16.0\r\nh5py                 2.8.0\r\nhumanfriendly        4.17\r\nhyperlink            18.0.0\r\nidna                 2.7\r\nincremental          17.5.0\r\nitsdangerous         1.1.0\r\nJinja2               2.10\r\njmespath             0.9.3\r\njsonpickle           0.9.6\r\njsonschema           2.6.0\r\nKeras                2.2.4\r\nKeras-Applications   1.0.6\r\nKeras-Preprocessing  1.0.5\r\nkiwisolver           1.0.1\r\nklein                17.10.0\r\nMarkdown             3.0.1\r\nMarkupSafe           1.1.0\r\nmatplotlib           2.2.3\r\nmattermostwrapper    2.1\r\nmock                 2.0.0\r\nmsgpack              0.5.6\r\nmsgpack-numpy        0.4.3.2\r\nmurmurhash           1.0.1\r\nnetworkx             2.2\r\nnumpy                1.15.4\r\npackaging            17.1\r\npathlib              1.0.1\r\npbr                  5.1.1\r\npika                 0.11.2\r\npip                  18.1\r\nplac                 0.9.6\r\npreshed              2.0.1\r\nprompt-toolkit       1.0.14\r\nprotobuf             3.6.1\r\npycparser            2.19\r\npydot                1.2.4\r\nPygments             2.2.0\r\nPyHamcrest           1.9.0\r\nPyInquirer           1.0.2\r\nPyJWT                1.6.4\r\npykwalify            1.6.0\r\npymongo              3.7.2\r\npyparsing            2.3.0\r\npyreadline           2.1\r\nPySocks              1.6.8\r\npython-crfsuite      0.9.6\r\npython-dateutil      2.7.5\r\npython-engineio      2.3.2\r\npython-socketio      2.0.0\r\npython-telegram-bot  10.1.0\r\npytz                 2018.7\r\nPyYAML               3.13\r\nrasa-core            0.12.0\r\nrasa-core-sdk        0.12.1\r\nrasa-nlu             0.13.7\r\nredis                2.10.6\r\nregex                2018.1.10\r\nrequests             2.20.1\r\nrequests-toolbelt    0.8.0\r\nrocketchat-API       0.6.22\r\nruamel.yaml          0.15.77\r\ns3transfer           0.1.13\r\nscikit-learn         0.19.2\r\nscipy                1.1.0\r\nsetuptools           39.1.0\r\nsimplejson           3.16.0\r\nsix                  1.11.0\r\nsklearn-crfsuite     0.3.6\r\nslackclient          1.3.0\r\nspacy                2.0.16\r\ntabulate             0.8.2\r\ntb-nightly           1.13.0a20181112\r\ntensorboard          1.10.0\r\ntensorflow           1.10.0\r\ntensorflow-estimator 1.10.12\r\ntermcolor            1.1.0\r\nterminaltables       3.1.0\r\ntf-nightly           1.13.0.dev20181111\r\nthinc                6.12.0\r\ntoolz                0.9.0\r\ntqdm                 4.28.1\r\ntwilio               6.19.2\r\nTwisted              18.9.0\r\ntyping               3.6.6\r\ntzlocal              1.5.1\r\nujson                1.35\r\nurllib3              1.24.1\r\nwcwidth              0.1.7\r\nwebexteamssdk        1.0.3\r\nwebsocket-client     0.54.0\r\nWerkzeug             0.14.1\r\nwheel                0.32.2\r\nwrapt                1.10.11\r\nzope.interface       4.6.0\r\n\r\ncomplet error stack\r\n\r\n      python -m rasa_core.train -d domain.yml -s data/stories.md -o models/current/dialogue --epochs \r\n      200\r\n     Traceback (most recent call last):\r\n     File \"C:\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n        \"__main__\", mod_spec)\r\n       File \"C:\\python36\\lib\\runpy.py\", line 85, in _run_code\r\n        exec(code, run_globals)\r\n       File \"C:\\python36\\lib\\site-packages\\rasa_core\\train.py\", line 14, in <module>\r\n         from rasa_core import config\r\n       File \"C:\\python36\\lib\\site-packages\\rasa_core\\config.py\", line 9, in <module>\r\n         from rasa_core.policies import PolicyEnsemble\r\n       File \"C:\\python36\\lib\\site-packages\\rasa_core\\policies\\__init__.py\", line 9, in <module>\r\n        from rasa_core.policies.keras_policy import KerasPolicy\r\n       File \"C:\\python36\\lib\\site-packages\\rasa_core\\policies\\keras_policy.py\", line 11, in <module>\r\n        import tensorflow as tf\r\n        File \"C:\\python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n       from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n        File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n       from tensorflow.core.framework.graph_pb2 import *\r\n       File \"C:\\python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n       from google.protobuf import descriptor as _descriptor\r\n        File \"C:\\python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n       from google.protobuf.pyext import _message\r\n       ImportError: DLL load failed: The specified procedure could not be found.\r\n", "comments": ["the problem is comming from protobuf 3.6.1\r\nWhen i change tensorflow to 1.8.0 and protobuf to 3.5.2, i got an other error essage \r\n\r\n    \t\tusage: train.py default [-h] [-o OUT] -d DOMAIN [--augmentation AUGMENTATION]\r\n\t\t\t\t\t\t\t\t-c [CONFIG [CONFIG ...]] [--dump_stories]\r\n\t\t\t\t\t\t\t\t[--debug_plots] (-s STORIES | --url URL | --core CORE)\r\n\t\t\t\t\t\t\t\t[-v] [-vv] [--quiet]\r\n\t\ttrain.py default: error: the following arguments are required: -c/--config\r\n\r\nso changing protobuf to 3.5.2 resolve the problem"]}, {"number": 23685, "title": "Can anyone help me with this import error:", "body": "\r\n\r\n### System information\r\n- **custom code from [here](https://www.datacamp.com/community/tutorials/cnn-tensorflow-python)\r\n- **OS Platform and Distribution : winows 10\r\n- **TensorFlow installed from (source or binary)**: coda install tensorflow\r\n- **Python version**: v3.6\r\n- **CUDA/cuDNN version**: v9. 0\r\n- **GPU model and memory**: Quadro M1200 \r\n TensorFlow version : 'v1.9.0-0-g25c197e023' 1.9.0\r\n......................................................\r\n### Below is my code:\r\n\r\n\r\n>```\r\n import numpy as np\r\n> import matplotlib.pyplot as plt\r\n> import tensorflow as tf\r\n> from tensorflow.examples.tutorials.mnist import input_data\r\n> import os\r\n> os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for training on gpu\r\n> \r\n> #import data\r\n> data = input_data.read_data_sets('C:\\...\\deep learning by python\\fashmnist',one_hot=True)\r\n> \r\n\r\n```\r\n............................................................\r\n### Error of above code is: \r\n( the error is related to this line of source code: \r\nfrom tensorflow.examples.tutorials.mnist import input_data)\r\n.........................................................\r\nrunfile('C:/.../deep learning by python/fashmnist/fashmnist.py', wdir='C:/.../deep learning by python/fashmnist')\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-19-7a288b4aec63>\", line 1, in <module>\r\n    runfile('C:/.../deep learning by python/fashmnist/fashmnist.py', wdir='C:/.../deep learning by python/fashmnist')\r\n\r\n  File \"C:\\Users\\...\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 668, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Users\\...\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 108, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/.../deep learning by python/fashmnist/fashmnist.py\", line 12, in <module>\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n\r\n  File \"C:\\Users\\...\\Anaconda3\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\__init__.py\", line 21, in <module>\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n\r\n  File \"C:\\Users\\...\\Anaconda3\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py\", line 30, in <module>\r\n    from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\r\n\r\n  File \"C:\\Users\\...\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\", line 36, in <module>\r\n    from tensorflow.contrib import data\r\n\r\n  File \"C:\\Users\\...\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\data\\__init__.py\", line 71, in <module>\r\n    from tensorflow.contrib.data.python.ops.error_ops import ignore_errors\r\n\r\n  File \"C:\\Users\\...\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\error_ops.py\", line 20, in <module>\r\n    from tensorflow.contrib.data.python.ops import contrib_op_loader  # pylint: disable=unused-import\r\n\r\n  File \"C:\\Users\\...\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\contrib_op_loader.py\", line 24, in <module>\r\n    resource_loader.get_path_to_datafile(\"../../_dataset_ops.so\"))\r\n\r\n  File \"C:\\Users\\...\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\util\\loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n\r\n  File \"C:\\Users\\...\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\n\r\nNotFoundError: C:\\Users\\...\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\..\\..\\_dataset_ops.so not found\r\n\r\n", "comments": ["Can you please try using this [thread](https://stackoverflow.com/questions/50609845/tensorflow-1-8-0-with-python-3-6-4-in-anaconda-show-error-dataset-ops-so-not-fo/50942347) and check if it solves the issue for you.? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23684, "title": "How to freeze layers that located at the last of the whole graph  when fine-tune", "body": "The grpah is A->B->C->loss, and I want to freeze A and C when fine-tune. A is a pretrained network. It's easy to freeze A. B contains LSTM and fc ops. C is variables or tuple tensors which are the results of fc. C stores one tensor  for one time because the tensor is hidden state of LSTM. Actually C will stores 6 tensors for a iteration.\r\nIt seems there are several solutions:\r\n1. C is variables and \"trainable=False\"           (this results in \"no gradient\" error)\r\n2. tf.stop_gradient()          (not try yet)\r\n3.  add var_list to function optimize_loss()         (not try yet)\r\n\r\nWhich one should I try? Or are there new solutions? \r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23683, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `import tensorflow as tf`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): _pip_ package manager\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: Python 3.6.7 64-bit\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: No GPU\r\n- GPU model and memory: No GPU \r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nJust to test after installation, I tried importing Tensorflow in Python, but got an unexpected error (see Code section) .\r\n\r\n**Describe the expected behavior**\r\nI think it will just keep going like this unless I fix the problem......\r\n\r\n**Code to reproduce the issue**\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#1>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n\r\n```\r\n**Other info / logs**\r\nYeah...... Maybe no logs here. I've got a little `Not included in PATH` log when installing, though.\r\n", "comments": ["https://github.com/tensorflow/tensorflow/issues/23686\r\n\r\npip install protobuf==3.5.1\r\n", "same issue here protobuf downgrade didn't fix it", "@jimakr  try with pip install protobuf==3.5.0", "with protobuf 3.5.0 more errors poped up because it is unsupported by tensorflow 1.12", "try with tensorflow 1.8.0", "tried actually i doesn;t change the log at all even with the usuported version of protobuf. tried to replicate the issue on another machine and it works without problems ", "The second machine have the same config with the first ?", "I've tried downgrading tensorflow with 1.8.0, protobuf with 3.5.0. Then the output slightly changed:\r\n```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#1>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\UNO\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n\r\n```\r\nSo, how to do now?", "the machine configurations was the same. mine stayed the same with tensorflow 1.8 and protobuf 3.5.0 i am sure it has to do something with current windows installation, i also tried with a vm and during import the system slowed down heavily", "have you installed correctly tensorflow ?\r\nhttps://www.tensorflow.org/install/", "try to upgrade setuptools\r\n    \r\n     pip install setuptools\r\n\r\n     python -m pip install -U pip setuptools\r\n\r\n\r\n1 | pip install setuptools\r\n-- | --\r\n\r\n\r\n\r\n\r\n1 | pip install setuptools\r\n-- | --\r\n\r\n\r\n1 | python -m pip install -U pip setuptools\r\n-- | --\r\n\r\n\r\n\r\n", "Nagging Assignee @ymodak: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@sjkim04 Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I think the issue still persists. I have exactly the same output as sjkim04. Is there any update regarding fix on this issue?\r\n\r\n\r\n[issue_tensorflow.txt](https://github.com/tensorflow/tensorflow/files/2923598/issue_tensorflow.txt)\r\n", "Same here, Please rectify", "Same issue. I tried installing tensorflow 1.9.0 so that it is compatible with python 3.6 and Keras. But  still get the same error . OS Windows 10 pro\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\sumanc\\AppData\\Local\\Continuum\\anaconda3\\envs\\usageproject\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\sumanc\\AppData\\Local\\Continuum\\anaconda3\\envs\\usageproject\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 35, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\sumanc\\AppData\\Local\\Continuum\\anaconda3\\envs\\usageproject\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 30, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\sumanc\\AppData\\Local\\Continuum\\anaconda3\\envs\\usageproject\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\sumanc\\AppData\\Local\\Continuum\\anaconda3\\envs\\usageproject\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n@ymodak - Is there any fix ?", "I have exactly the same issue on my old desktop, which runs intel core2 E7500.\r\n\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\nFailed to load the native TensorFlow runtime.\r\n\r\nHowever, on my laptop which runs intel core i5, there is no issue.\r\n\r\nDoes this issue have anything to do with CPU feature?\r\n\r\nalso see:\r\nhttps://github.com/tensorflow/tensorflow/issues/24548", "i am also getting same issue\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow\r\n.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow\r\n_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow\r\n_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialisation routin\r\ne failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, i\r\nn <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", lin\r\ne 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow\r\n.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow\r\n.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow\r\n_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow\r\n_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialisation routin\r\ne failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> ^Z\r\n", "i am also getting same issue please anybody help me ?????\r\n\r\n\r\nImportError                               Traceback (most recent call last)\r\nc:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nc:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nc:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nc:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\nc:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-8cbe2c7f25b8> in <module>\r\n      4 import sys\r\n      5 import tarfile\r\n----> 6 import tensorflow as tf\r\n      7 import zipfile\r\n      8 \r\n\r\nc:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow._api.v1 import app\r\n\r\nc:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nc:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\users\\ajays\\documents\\temp\\envs\\tensorflow1\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "I have the same issue on Win7. Did someone was able to fix?", "Traceback (most recent call last):\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/GabhaDi/PycharmProjects/Django&python/twitter_Bighead.py\", line 56, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\GabhaDi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "i have the same issue any help please?\r\n", "same here ", "People, I have solved my problem taking another path. I uninstalled the TensorFlow package that I installed using pip and reinstall using \"conda install TensorFlow\" and it worked! Now I can import TensorFlow without problems. Hope this can help you guys too! ", "@TamirisCrepalde can you plz help me out? its not working for me. In fact, I don't know how to use conda.", "@sdmars how did you install pip? I installed pip through Anaconda package and for this path it is possible to install the python packages through Anaconda too using \"conda install <package name>\". I usually use pip to install the packages but for this one, I used conda but is the same idea: get access to the terminal and type \"conda install TensorFlow\". If you don't know Anaconda, just get a look on the website https://www.anaconda.com/", "> People, I have solved my problem taking another path. I uninstalled the TensorFlow package that I installed using pip and reinstall using \"conda install TensorFlow\" and it worked! Now I can import TensorFlow without problems. Hope this can help you guys too!\r\n\r\n@TamirisCrepalde you just saved the day! Thank you very much!!", "> \r\n> \r\n> People, I have solved my problem taking another path. I uninstalled the TensorFlow package that I installed using pip and reinstall using \"conda install TensorFlow\" and it worked! Now I can import TensorFlow without problems. Hope this can help you guys too!\r\n\r\nIt worked for me. Thank you very much", "> People, I have solved my problem taking another path. I uninstalled the TensorFlow package that I installed using pip and reinstall using \"conda install TensorFlow\" and it worked! Now I can import TensorFlow without problems. Hope this can help you guys too!\r\n\r\n@TamirisCrepalde thanks a ton. I wasted a full day before i found your solution and It worked for me.", "This is definitely still a big issue. Fortunately installing using conda also worked for me but now I have to use conda and install all the packages I need for my projects again that conda doesn't come with", "Still an issue.  I was having the problem with windows7 so just upgraded to windows10 and getting this error.  I just installed anaconda and went through the tensor flow install:\r\n\r\npip install --upgrade pip\r\npip install tensorflow\r\npip install tensorflow==2.0.0-rc1\r\n\r\nNote that I didn't install GPU support.  Below is my error message:\r\n\r\nrunfile('C:/Users/1000008408/Python/MachLearn/mlearn.py', wdir='C:/Users/1000008408/Python/MachLearn')\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-2-7c5d758c2eab>\", line 1, in <module>\r\n    runfile('C:/Users/1000008408/Python/MachLearn/mlearn.py', wdir='C:/Users/1000008408/Python/MachLearn')\r\n\r\n  File \"C:\\Users\\1000008408\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Users\\1000008408\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/1000008408/Python/MachLearn/mlearn.py\", line 22, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\1000008408\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"C:\\Users\\1000008408\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\1000008408\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\1000008408\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\1000008408\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\1000008408\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\1000008408\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\1000008408\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nI have an Intel Xeon CPU W3520.  Let me know if you want other system information.", "@duncanhalstead\r\nDo the following.\r\n1. Run the anaconda prompt as administrator.(right click-> run as administrator).\r\n2. pip uninstall tensorflow.\r\n3. Close anaconda prompt.\r\n4. Again run anaconda prompt as administrator.\r\n5. type:\r\n     conda install tensorflow\r\nIt will ask for some y / n\r\nType y.\r\n\r\n6. Now after all done, change your python interpreter's environment to anaconda environment where you installed tensorflow.\r\n7. Now try importing tensorflow.\r\n# hope this helps :)", "@sdmars that worked, thanks!\r\n\r\nI found [this](https://towardsdatascience.com/stop-installing-tensorflow-using-pip-for-performance-sake-5854f9d9eb0c) article which is interesting reading", "@duncanhalstead\r\nCan you reach me on linked in?\r\nI want to be connected with developers.\r\nhttps://www.linkedin.com/in/sayan-dutta-chowdhury-153032166", "> People, I have solved my problem taking another path. I uninstalled the TensorFlow package that I installed using pip and reinstall using \"conda install TensorFlow\" and it worked! Now I can import TensorFlow without problems. Hope this can help you guys too!\r\n\r\nthanks\r\nyou save me !  \r\n:-)", "thanks, @TamirisCrepalde  for  your help", "I was able to resolve this by:\r\n\r\n1. Downloading Miniconda from https://docs.conda.io/en/latest/miniconda.html\r\n\r\n2. Installing Miniconda - Make sure you tick 'Add to Path'\r\n\r\n3. Open Anaconda command prompt or power shell and type 'conda' to ensure it is properly installed\r\n\r\n4. Type 'conda activate base' to activate the base environment - you should see the (base) in front of your directory\r\n\r\n5. Type 'conda install tensorflow' and wait for some minutes - this will install tensorflow 2.0 and all its dependencies\r\n\r\n6. import tensorflow\r\n\r\nIt works fine for me!!!\r\n\r\n", "https://github.com/tensorflow/tensorflow/issues/23683#issuecomment-532522740  \r\n\r\nhow to do this Now after all done, change your python interpreter's environment to anaconda environment where you installed tensorflow.\r\n\r\n", "its a kindof brute force approach but works for me everytime \r\ntry:\r\n    import tensorflow_core\r\nexcept:\r\n    ImportError\r\n\r\nimport tensorflow as tf\r\n", ">>> import numpy\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "How to remove the error\r\n", "Thanks @TamirisCrepalde! ^_^ \r\n\r\n> People, I have solved my problem taking another path. I uninstalled the TensorFlow package that I installed using pip and reinstall using \"conda install TensorFlow\" and it worked! Now I can import TensorFlow without problems. Hope this can help you guys too!\r\n\r\n", "@UGLYclown999 Thanks for your brute-force technique......i was wondering weather this will create an issue when the model is deployed online?? can you please share your thoughts on that??\r\nAgain Thanks A Ton for the help", "@UGLYclown999  do you have a similar technique for importing keras too ?? Sir if you have please share with us ....it will be a great help..........\r\nRegards,\r\nAdarshPan", "Please open a new issue if the indications at https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156 don't help. If you open a new issue, please fill in issue template.\r\n\r\nLocking conversation as we already have some solution"]}, {"number": 23682, "title": "Can't install on Raspberry pi 3B", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**Raspberry 9** on Pi 3B;Linux version 4.14.79-v7+ (dc4@dc4-XPS13-9333) (gcc version 4.9.3 (crosstool-NG crosstool-ng-1.22.0-88-g8460611)) #1159 SMP Sun Nov 4 17:50:20 GMT 2018\r\n\r\n\r\n\r\n- TensorFlow version:1.12.0\r\n- Python version:**3.5.3**\r\n- Installed using virtualenv? pip? conda?:**pip**\r\n\r\n**Describe the problem**\r\n\r\nCould not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip3 install tensorflow\r\n\r\n**Any other info / logs**\r\nlibatlas-base-dev is already the newest version (3.10.3-1+rpi1).\r\n\r\n", "comments": ["Make sure to update all pacakges using: pip3 install wheel\r\n\r\nYou can follow the installation guide: [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/), use bazel 0.5.1 and gcc 4.8 to build TensorFlow on raspberry pi model 3B.\r\n\r\n\r\n", "@ron159  Is this issue resolved ? ", "> @ron159 Is this issue resolved ?\r\n\r\nnope,i try to build from source on my pc, but still can not work", "@ron159 Last success RPi build was for python 3.4 (you can download wheel by following link [tensorflow-1.10.0-cp34-none-linux_armv7l.whl](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)\r\n\r\nCurrently is not clear what happening with supporting of this build for latest versions of TF", "I managed to build Tensorflow v1.12 (release branch) from source on Raspberry Pi 3B+ , using Bazel 0.19.2 ,\r\n but my build was for c++ shared library. \r\nMaybe that might work for building .whl ?\r\n", "> @ron159 Last success RPi build was for python 3.4 (you can download wheel by following link [tensorflow-1.10.0-cp34-none-linux_armv7l.whl](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)\r\n> \r\n> Currently is not clear what happening with supporting of this build for latest versions of TF\r\n\r\nDid you get a chance to try this? ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23681, "title": "Generating of wrapper functions for TensorFlow ops for Go fails with not existing framework_go_proto directory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS Mojave 10.14.1**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **from source**\r\n- TensorFlow version: **1.12.0**\r\n- Python version: **N/A**\r\n- Installed using virtualenv? pip? conda?: **N/A**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen I try to install Tensorflow from tag v1.12.0 for Go based on instructions: https://www.tensorflow.org/install/lang_go and https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/go/README.md.\r\nOn step **4. Generate wrapper functions for TensorFlow ops:** I get an error:\r\n```bash\r\n$ go generate github.com/tensorflow/tensorflow/tensorflow/go/op\r\n../genop/internal/api_def_map.go:34:2: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/tensorflow/core/framework_go_proto\" in any of:\r\n\t/usr/local/Cellar/go/1.11.2/libexec/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/tensorflow/core/framework_go_proto (from $GOROOT)\r\n\t/Users/bayandin/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/tensorflow/core/framework_go_proto (from $GOPATH)\r\n../../../go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/generate.go:18: running \"go\": exit status 1\r\n```\r\n\r\nBut `go test -v github.com/tensorflow/tensorflow/tensorflow/go`  passes\r\n\r\n**Any other info / logs**\r\n\r\n* Go version: go1.11.2 darwin/amd64 (from brew)\r\n* libtensorflow version 1.12.0 (from brew)\r\n* protobuf version 3.6.1 (from brew)\r\n* GOPATH=/Users/bayandin/go\r\n\r\n", "comments": ["You can use the command below to install TensorFlow for Go\r\n```\r\ngo get github.com/tensorflow/tensorflow/tensorflow/go\r\n```", "@wt-huang when I use `go get github.com/tensorflow/tensorflow/tensorflow/go` I don't get an access to types defined in protobuf, for example GPUOptions from from https://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/core/protobuf/config.proto#L16", "@bayandin After step 4 build and test, Go functions corresponding to TensorFlow operations are already generated in op/wrappers.go. This is why \r\n\r\n`go test github.com/tensorflow/tensorflow/tensorflow/go`\r\n\r\npasses. You are good to go.\r\n\r\nIf you want to regenerate Go functions, then use:\r\n\r\n`go generate github.com/tensorflow/tensorflow/tensorflow/go/op`\r\n\r\nMake sure TensorFlow repository is under GOPATH and you have Protocol buffer compiler 3.x\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23681\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23681\">No</a>\n", "wqq@Atlantis:~$`go generate github.com/tensorflow/tensorflow/tensorflow/go/op`\r\n../genop/internal/api_def_map.go:34:2: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/tensorflow/core/framework_go_proto\" in any of:\r\n        /usr/local/Cellar/go/1.11.2/libexec/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/tensorflow/core/framework_go_proto (from $GOROOT)\r\n        /Users/wqq/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/tensorflow/core/framework_go_proto (from $GOPATH)\r\ngo/src/github.com/tensorflow/tensorflow/tensorflow/go/op/generate.go:18: running \"go\": exit status 1\r\n"]}, {"number": 23680, "title": "lite compile error under tensorflow/lite/tools/make", "body": "when tflite move from tensorflow/contrib/lite to tensorflow/lite, all shell under lite/tools/make has and error, and should be below.\r\n\r\n```bash\r\ncd \"$SCRIPT_DIR/../../../..\"\r\n```", "comments": ["#23672 ", "It looks like the suggested modification has been already made recently: https://github.com/tensorflow/tensorflow/commit/b4a7f51584d60e26024bb7401e3167704f94d298", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23679, "title": "Installation of tensorflow r1.10 on windows with visual studio c++ (with gpu) help.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.10\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 9.2, cuDNN 7.4.1.5\r\n- GPU model and memory: 940mx 2GB Vram\r\n\r\n\r\n\r\n**Describe the problem**\r\nCUDA and cuDNN were succesfully installed. CMake to configure and generate build files for tensorflow was also done. MSBuild to build the ALL_BUILD.vcxproj project was invoked and an error was encountered during this. The error message shown in the command prompt is given below.\r\n\r\n\r\n**Any other info / logs**\r\n\"C:\\Users\\Allosimanious\\Documents\\cud\\tensorflow-r1.10\\tensorflow\\contrib\\cmake\\build\\ALL_BUILD.vcxproj\" (default targe\r\nt) (1) ->\r\n\"C:\\Users\\Allosimanious\\Documents\\cud\\tensorflow-r1.10\\tensorflow\\contrib\\cmake\\build\\tensorflow.vcxproj\" (default targ\r\net) (135) ->\r\n\"C:\\Users\\Allosimanious\\Documents\\cud\\tensorflow-r1.10\\tensorflow\\contrib\\cmake\\build\\tensorflow_static.vcxproj\" (defau\r\nlt target) (136) ->\r\n\"C:\\Users\\Allosimanious\\Documents\\cud\\tensorflow-r1.10\\tensorflow\\contrib\\cmake\\build\\tf_c.vcxproj\" (default target) (1\r\n37) ->\r\n(ClCompile target) ->\r\n  C:\\Users\\Allosimanious\\Documents\\cud\\tensorflow-r1.10\\tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h(21):\r\nfatal error C1083: Cannot open include file: 'grpcpp/grpcpp.h': No such file or directory [C:\\Users\\Allosimanious\\Docum\r\nents\\cud\\tensorflow-r1.10\\tensorflow\\contrib\\cmake\\build\\tf_c.vcxproj]\r\n", "comments": ["We no longer officially support the cmake build, and suggest that you use bazel instead: https://www.tensorflow.org/install/source_windows\r\n\r\nI'm marking this community support in case anyone else has run into this. cc/ @gunan ", "@skye  Just to make sure, do all the older versions of TF should be used with bazel ?  Because I see only TF v > 1.10 are supported by Bazel as per the tested build configuration. Please clarify.", "I'm not sure, but given that we're no longer developing the 1.10 branch, I suggest trying the most recent version of TF. @gunan probably knows more.", "We encourage users to stay with the two latest versions of TF.\r\nEspecially if they are building from source.\r\n\r\nAs for branches, just like @skye mentioned, as soon as we cut a branch we stop developing on that.\r\nany release before the last release (1.12) will only release critical security patches, nothing else will be patched in old release branches.", "Okay thanks for your help. I will try building TF 1.12 using bazel. But can you tell me how to get .so/ dll file instead of the whl file generated?", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "Hi @Afroman1996 !\r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Have you tried [latest version ](https://www.tensorflow.org/install/pip)yet ? You can use[ tested configurations](https://www.tensorflow.org/install/source_windows#tested_build_configurations) to build older version using [Bazel](https://www.tensorflow.org/install/source_windows).Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23679\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23679\">No</a>\n"]}, {"number": 23678, "title": "Windows does not build tflite in anaconda environment : gives ModuleNotFoundError: No module named \"tensorflow.contrib.lite.python.tflite_convert\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):WINDOWS 10\r\n- TensorFlow installed from (source or binary):anaconda\r\n- TensorFlow version (use command below):1.9\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):NO\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:no\r\n- GPU model and memory:no\r\n-Exact Command:toco help\r\n\r\nDescribe the problem\r\nI am trying to run the codelab tutorial of tensorflow lite. After installing Tensorflow for cpu in anaconda3 environment , when I try to run the command \"toco --help\", I get the error ModuleNotFoundError: No module named 'tensorflow.contrib.lite.toco.python.tflite_convert'.\r\n\r\n\r\nsource code/logs\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\hp\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\hp\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\hp\\Anaconda3\\envs\\tensorflow\\Scripts\\toco.exe\\__main__.py\", line 5, in <module>\r\nModuleNotFoundError: No module named 'tensorflow.contrib.lite.python.tflite_convert'\r\n\r\n", "comments": [" It should work with TensorFlow latest build. Can you please use tf-nightly and build again?", "I have build it using tf-nighty build but  i am getting same problem . I am trying to follow steps provided by tensorflow tutorials \r\n\r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2\r\n\r\ndoes we have to build toco first to use it . As i have read this in many comments but didn't able to build it  ", "Could you try removing contrib from the path i.e we moved out of contrib use:\r\n```python\r\ntensorflow.contrib.lite.tflite_convert\r\n```", "Same problem for my experience with mentioned tutorial... Does anyone have any ideas on it? \r\n\r\n- Windows 10\r\n- Anaconda env\r\n- Tensorflow version 1.12\r\n- Python 3.6.7\r\n- Command: toco help\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Work\\ML\\Anaconda\\envs\\my_env\\Scripts\\toco-script.py\", line 6, in <module>\r\n    from tensorflow.contrib.lite.python.tflite_convert import main\r\nModuleNotFoundError: No module named 'tensorflow.contrib.lite.python.tflite_convert'", "> > It should work with TensorFlow latest build. Can you please use tf-nightly and build again?\r\n> \r\n> it just works fine with tf-nightly build of tensorflow....i converted my model successfully!!!!\r\n> please use this command to upgrade the existing tensorflow in your environment(conda)..\r\n> pip install --ignore-installed --upgrade tf-nightly (p.s. i was using cpu build of tf-nightly)\r\n> \r\n> #23678\r\n> #25162\r\n> @jdduke\r\n> @eXisting\r\n> @aselle\r\n> @tahercoolguy\r\n> @ymodak\r\n\r\n![screenshots of newly obtained tflite model](https://user-images.githubusercontent.com/16951929/52522946-661b8a00-2cbe-11e9-92a4-df0856bee2ae.png)\r\n![tflite conversion successfully completed](https://user-images.githubusercontent.com/16951929/52522947-661b8a00-2cbe-11e9-9eab-86eee30fb15c.png)\r\n", "@Jakejaga5\r\nI install TF==1.13.1 and I command \"pip install --ignore-installed --upgrade tf-nightly\".\r\nBut it seems that Tensorflow will conflict by itself.\r\n![2019-04-18 20-31-01 \u7684\u87a2\u5e55\u64f7\u5716](https://user-images.githubusercontent.com/17959032/56361075-03dc8980-6219-11e9-8046-3b5fa2759587.png)\r\n", "(tf-lite convert) toco compilation/execution was fixed for Windows in TF 1.11 and continues to be available in later TF versions as well.\r\nSee https://github.com/tensorflow/tensorflow/releases/tag/v1.11.0"]}, {"number": 23677, "title": "Sparc compatibility", "body": "<em> Tensorflow Lite Sparc Support tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: r1.10\r\n- Are you willing to contribute it: if able to \r\n\r\n\r\nTensorflow currently supports intel x86 (Windows,IOS) and ARM (raspberry pi), but not sparc architecture. Would be nice to also have support for sparc architecture.\r\n", "comments": ["We'd be happy to accept patches, but we currently have no way to continuously test these features. Can you elaborate on what scope of contribution you imagine?", "Closing due to stale history, feel free to re-open if this would still be of value."]}, {"number": 23676, "title": "Fix issue in expanding of mask in keras.backend.rnn", "body": "Follow up on https://github.com/tensorflow/tensorflow/issues/23632. The axis argument is missing in `array_ops.expand_dims` which leads to that outputs with more than 2 dimensions cannot be masked.", "comments": ["@qlzh727 It was a really minor fix. I include the tests here, the two later are failing without this fix."]}, {"number": 23675, "title": "FastGFile deprecated warning fixed", "body": "fix `FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.` problem.", "comments": ["@ymodak please specify reviewers have it reviewed, thx.", "Ping @petewarden can you please take a look? Thanks!", "Nagging Reviewer @petewarden: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied.", "code is already merged in master , thank you "]}, {"number": 23674, "title": "bazel build on windows10 failed", "body": "**System information**\r\n- OS Platform and Distribution : windows10\r\n- TensorFlow installed from : source\r\n- TensorFlow version: 1.11.0\r\n- Python version: Python 3.6.3 :: Anaconda custom (64-bit)\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.19.0 not from source\r\n\r\nonly cpu version.\r\n\r\n\r\n\r\nI followed the instruction from [tensorflow official site.](https://tensorflow.google.cn/install/source_windows#bazel_build)\r\n\r\nWhen I entered `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\nAn error occurred\r\n```\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nd:\\tensorflow-1.11.0/.bazelrc\r\nd:\\tensorflow-1.11.0/tools/bazel.rc\r\nERROR: SymlinkDirectories(C:\\Users\\Administrator/_bazel_Administrator/install/46e308c2aba33f6ef6ad7b7f2c018098, c:\\users\\administrator\\_bazel_administrator\\buk4ctdf/install): CreateJunction:\r\nFATAL: failed to create installation symlink 'c:\\users\\administrator\\_bazel_administrator\\buk4ctdf/install': success\r\n```\r\n", "comments": ["Try as per the warning message, you can add following lines to **.bazelrc file**:\r\nimport /your_path/tools/bazel.rc\r\n\r\n\r\n", "@ymodak Thanks for replying. I added the line, but it doesn't work.\r\n```\r\n WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nd:\\tensorflow-1.11.0/.bazelrc\r\nd:\\tensorflow-1.11.0/tools/bazel.rc\r\nERROR: SymlinkDirectories(C:\\Users\\Administrator/_bazel_Administrator/install/46e308c2aba33f6ef6ad7b7f2c018098, c:\\users\\administrator\\_bazel_administrator\\buk4ctdf/install): CreateJunction:\r\nFATAL: failed to create installation symlink 'c:\\users\\administrator\\_bazel_administrator\\buk4ctdf/install': success\r\n```\r\nBy the way, the bazel threw an win32 excepiton when I run the configure.py. I just ignored it, because I don't know how to handle it.", "Looks like a permission issue. You can try starting a terminal with Administrator privileges.\r\n@meteorcloudy may know more.", "@gunan I tried the Windows PowerShell(Administrator) , but it didn't work. The same win32 exception , the same error.", "@LJXLJXLJX Can you please try lowering your bazel version to 0.18.1 and build again?", "@ymodak I download the bazel 0.18.1 and delete the previous _bazel_Administrator directory, repeated those process again but met the same win32 exception and error.\r\n", " I entered python ./configure.py.\r\nThen the Windows PowerShell print \"Extracting Bazel installation...\"\r\nThen the exception occurred.\r\nI followed the instruction and opened my visual studio to debug.\r\nIt said there was an exception unhandled  at 0x00007FF8C0EA4D3B(ntdll.dll)(in bazel.exe): 0xC0000374: the heap was broken.(Parameter:0x0000FF8C0F097B0)\r\nFollowed is the screenshot.\r\nhttps://suall-my.sharepoint.com/:i:/g/personal/46093_officeent_top/EYkLTu1cSV9Lr0aIweN7diYBEHlat0rJDX9pfLK0i6e8iw?e=AeVcDK\r\n\r\nIf I ignore the exception, the configuration process can still be completed. But If I enter bazel build then, the error mentioned above occur.\r\n", "@LJXLJXLJX It might be you don't have write permission to `C:\\Users\\Administrator/_bazel_Administrator`\r\nCan you add `startup --output_user_root=C:/tmp` into your bazelrc file? This will change the output base.", "@meteorcloudy \r\n```\r\nWARNING: Duplicate rc file: d:\\tensorflow-1.11.0\\tools\\bazel.rc is read multiple times, most recently imported from d:\\tensorflow-1.11.0\\.bazelrc\r\nWARNING: Processed legacy workspace file d:\\tensorflow-1.11.0/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\r\nERROR: SymlinkDirectories(d:\\tmp/install/358af9e789d95fd87cbc5c9bb842e6bc, d:\\tmp\\buk4ctdf/install): CreateJunction:\r\nFATAL: failed to create installation symlink 'd:\\tmp\\buk4ctdf/install': success\r\n```\r\nit didn't work .", "That's strange. \r\nDoes `mklink /J d:\\tmp\\buk4ctdf\\install d:\\tmp\\install\\358af9e789d95fd87cbc5c9bb842e6bc` work?", "I am also facing the same issue. Please help", "@meteorcloudy No I didn't work even with the win PowerShell(Adminitrator)", "@LJXLJXLJX Can you show me the error message?", "@LJXLJXLJX BTW, you should run it inside cmd.exe instead of powershell", "> \r\n> \r\n> @LJXLJXLJX BTW, you should run it inside cmd.exe instead of powershell\r\n\r\nI am running it on cmd.exe and still facing this problem.", "@gouti26 Can you show me the error message?", "> \r\n> \r\n> @gouti26 Can you show me the error message?\r\nHere it is\r\n![error](https://user-images.githubusercontent.com/30646435/48617087-7374f100-e9bb-11e8-8f0b-666b8cad6651.JPG)\r\n", "@gouti26 This looks like a completely different error, can you file another issue?", "same questions for me, win10, bazel 1.19.0 build from choco, failed and give out the same error.", "@gunan hello, i try to build libtensorflow.so by bazel that can succeed,but now i want to build a libtensorflow_cc.so, that's failed,detail is follow:\r\n\r\nSystem information\r\n\r\n    OS Platform and Distribution : windows10\r\n    TensorFlow installed from : source\r\n    TensorFlow version: r1.12\r\n    Python version: Python 3.5.4 \r\n    Bazel version (if compiling from source): 0.19.0 \r\n    cuda 10.0, cudnn 7.3, gtx1080ti\r\n\r\nand the command is ,\r\nbazel build --config=opt --config=cuda //tensorflow/libtensorflow_cc.so\r\n\r\n\u00e6\u00b3\u00a8\u00e6\u0084\u008f: \u00e5\u008c\u0085\u00e5\u0090\u00ab\u00e6\u0096\u0087\u00e4\u00bb\u00b6:    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ammintrin.h\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/v3_alt.o' was not created\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/v3_alt.o' was not created\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/v3_alt.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/evp_asINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/v3_alt.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/evp_asn1.o' was not created\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/v3_alt.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/evp_asn1.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/curve2INFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/v3_alt.o' was not created\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/v3_alt.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/evp_asn1.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/curve2INFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/v3_alt.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/evp_asn1.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/curve25519.o' was not created\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/v3_alt.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/evp_asn1.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/curve25519.o' was not created\r\nINFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/v3_alt.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/evp_asn1.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/curve2INFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/v3_alt.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/evp_asn1.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/curve2INFO: From Compiling external/boringssl/src/crypto/evp/evp_asn1.c:\r\nINFO: From Compiling external/boringssl/src/crypto/hkdf/hkdf.c:\r\nINFO: From Compiling external/boringssl/src/crypto/asn1/f_enum.c:\r\nINFO: From Compiling external/boringssl/src/crypto/x509v3/v3_alt.c:\r\nINFO: From Compiling external/boringssl/src/crypto/thread.c:\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/hkdf.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/f_enum.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/v3_alt.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/evp_asn1.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/curve25519.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/tasn_new.o' was not created\r\nERROR: C:/users/swls/_bazel_swls/5dz6uozl/external/boringssl/BUILD:130:1: output 'external/boringssl/_objs/crypto/thread.o' was not created\r\nINFO: Elapsed time: 52.047s, Critical Path: 5.81s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 213 processes: 213 local.\r\n\r\nit maybe related with thread.c; so how can i solve it?\r\n", "The original reported problem seems to be solved.\r\n@jesen8, the error you are seeing looks different. And I cannot see the actual cause of the issue because you do not have the full log. Please file a new issue, this time including all the information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23674\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23674\">No</a>\n"]}, {"number": 23673, "title": "Tensorflow build failed to complete", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.10\r\n- Python version: 3\r\n- Installed using virtualenv? pip? conda?: virtualenv/pip\r\n- Bazel version (if compiling from source): 0.19.0\r\n- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) \r\n- CUDA/cuDNN version:\r\n- GPU model and memory:N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am building a mobile object detection app and trying to convert the model to tensorflow lite model. Accoding to the steps in [this](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) blog post, I am supposed to build tensorflow from source using bazel and I followed steps [here](https://www.tensorflow.org/install/source). But after executing for sometime the build process fails. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`git clone https://github.com/tensorflow/tensorflow.git\r\n cd tensorflow`\r\n`bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThe error message is : \r\n```\r\nERROR: /home/stash/.cache/bazel/_bazel_stash/f5e22f69335a98eb99581e9c850e62a4/external/grpc/BUILD:1332:1: C++ compilation of rule '@grpc//:grpc_resolver_dns_ares' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 53 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nexternal/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_ev_driver_posix.cc:23:18: fatal error: ares.h: No such file or directory\r\ncompilation terminated.\r\nINFO: Elapsed time: 26.272s, Critical Path: 18.50s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 12 processes: 12 linux-sandbox.\r\nFAILED: Build did NOT complete successfully\r\n//tensorflow/c:c_api_experimental_test                                NO STATUS\r\n//tensorflow/c:c_api_function_test                                    NO STATUS\r\n//tensorflow/c:c_api_test_gpu                                         NO STATUS\r\n//tensorflow/c:while_loop_test                                        NO STATUS\r\n//tensorflow/c/eager:c_api_test_gpu                                   NO STATUS\r\n//tensorflow/cc:cc_op_gen_test                                        NO STATUS\r\n//tensorflow/cc:client_client_session_test                            NO STATUS\r\n...\r\n...\r\n//tensorflow/tools/proto_text:gen_proto_text_functions_lib_test       NO STATUS\r\n\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nElapsed time Critical Path are low in the error message as I had tried the command again after it failed in the first try.", "comments": ["After this command executes, I am supposed to run this command to get the optimized mode using TOCO according to the blog [post](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193):\r\n`bazel run -c opt tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=$OUTPUT_DIR/tflite_graph.pb \\\r\n--output_file=$OUTPUT_DIR/detect.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops`", "It looks to me like you have an issue with your gcc configuration (maybe gcc isn't in /usr/bin/gcc??).   The command `bazel test` is running the unit tests with some default options.  I've had trouble running this command verbatim and noticed that they don't ask you to run `./configure` before running this.  I'm not sure if that's an oversight or if it's not required but when you run `./configure` you create a file named `.tf_configure.bazelrc` where things like the location of gcc is configured.\r\n\r\n`bazel test` is optional.  You may have better luck skipping these unit tests and go directly to the `bazel build` step.  For me, `test` also has a lot of errors which look like they're related the test setup rather than the actual code itself.  My Tensorflow runs fine when simply doing the configure/build steps.  \r\n", "same problem, how to solve it?", "I encountered the same issue with Tensorflow releases 1.11 and 1.12\r\n\r\nUpdate: I switched to bazel 0.18, python 3.5, tensorflow 1.12,  uninstalled all libc-ares packages, ran `./configure` and then built with `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`. Now when running your `bazel test` I'm seeing test failures but the build completes successfully.", "Try to use bazel 0.15, gcc 4.8, TensorFlow 1.12. Post any errors you observe.", "centos 6, gcc 5.5.0, bazel 0.19.0, tensorflow 1.12.0\r\n\r\nIn the `.tf_configure.bazelrc` under tensorflow 1.10.0 I find:\r\n```\r\nbuild --define=grpc_no_ares=true\r\n```\r\nafter running configure which comes from:\r\n```\r\nconfigure.py:  write_to_bazelrc('build --define grpc_no_ares=true')\r\n```\r\n\r\nThe configure in 1.12.0 does not do this and tools/bazel.rc is not used with bazel 0.19.0\r\n\r\nWhen starting the build I see:\r\n```\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/build/tmp/tensorflow/tools/bazel.rc\r\n```\r\n\r\nSo I added `import /build/tmp/tensorflow/tools/bazel.rc` to `.bazelrc`.\r\n\r\nI am not sure if this is the \"right\" solution, but may shed some light on where the problem lies.\r\n\r\nPerhaps `bazel test` does not load/read `tools/bazel.rc`?\r\nIn newer bazel versions `tools/bazel.rc` is not read at all.", "On my Debian GNU/Linux 9.6 (stretch) I installed prerequisities:\r\n- nvidia-driver 390.87 (official repo)\r\n- Nvidia CUDA toolkit 9.1.85 + libcublas9.1 (official repo)\r\n- libcudnn7 7.1.3.16-1 for cuda9.1 (via deb file)\r\n- libnccl 2.1.15 for cuda 9.1 (via deb file)\r\n- bazel 0.19.2 (official google repo), tested with 0.15.0 too\r\n- and usual python libs via pip\r\n\r\nTo let Bazel find CUDA, I did:\r\n\r\n`sudo mkdir -p /usr/local/cuda/nvvm && cd /usr/local/cuda`\r\n`sudo ln -s /usr/lib/nvidia-cuda-toolkit/bin bin`\r\n`sudo ln -s /usr/lib/x86_64-linux-gnu lib64`\r\n`sudo ln -s /usr/lib/nvidia-cuda-toolkit/libdevice nvvm/libdevice`\r\n`export PATH=$PATH:/usr/local/cuda/bin`\r\n`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64`\r\n`cd /tmp`\r\n\r\nThen cloned Tensorflow sources to **/tmp/tensorflow**, switched to its **r1.12** branch,\r\nrun **./configure** and added line:\r\n\r\n`import /tmp/tensorflow/tools/bazel.rc`\r\n\r\nto **.bazelrc**\r\nCompilation went well with the usual command\r\n\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nAfter building python wheel package with\r\n\r\n`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp`\r\n\r\nand installing produced whl with pip, it correctly detects and runs Tensorflow code on my GTX1060 GPU.\r\nHope it helps!", "I am encountering the same problem of missing `ares.h` header file, compiling in a different environment.\r\n\r\nSystem information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.5.6\r\n- Installed using virtualenv? pip? conda?: virtualenv/pip\r\n- Bazel version (if compiling from source): 0.20.0 \r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n", "I got the same  error, build from source in the following environment  \r\n\r\n#### system information\r\n- OS Platform and Distribution : Raspbian  9.0 (Stretch)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12\r\n- Python version: N/A, I was building C++ library (.so)\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): gcc version 6.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory:N/A\r\n\r\n#### the exact sequence of commands / steps that you executed before running into the problem\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout r1.12\r\n./configure\r\nbazel build -c opt --copt=\"-mfpu=neon-vfpv4\" --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\" --local_resources 1024,1.0,1.0 --verbose_failures //tensorflow:libtensorflow_cc.so\r\n```\r\n\r\n**UPDATE**\r\nsorry for bothering, the error has gone after installing libc-ares-dev .", "Thanks @erley and @metalalive for providing solution. I think it was resolved.  Please open a new ticket if you see similar issue. Thanks!\r\n"]}, {"number": 23672, "title": "tensorflow/lite/tools/make: fix download_dependencies path", "body": "Because of the move out of contrib, the path now needs one less '..'.", "comments": ["Looks like this change has been independently made: https://github.com/tensorflow/tensorflow/commit/b4a7f51584d60e26024bb7401e3167704f94d298", "Nagging Reviewer @petewarden: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied.", "Sorry for the slow response, this was a good fix but we got another change that fixed the issue in the mean time. Thanks for this!"]}, {"number": 23671, "title": "pybullet.error: createCollisionShape failed.", "body": "It is on Ubuntu 16.04LTS, and it shows this error:\r\n![image](https://user-images.githubusercontent.com/15700681/48325102-ff5ee400-e66e-11e8-8569-211441bbe004.png)\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "Ok, new issue is here:\r\nhttps://github.com/tensorflow/tensorflow/issues/23729"]}, {"number": 23670, "title": "Run example graph_mobilenet with QASYMM8 data type and CL target", "body": "Hi, \r\nI try to run example graph_mobilenet with QASYMM8 data type, but it would report error as follows:\r\n$ graph_mobilenet --type=QASYMM8 --target=CL\r\n\r\nERROR in validate_all_nodes src/graph/detail/ExecutionHelpers.cpp:50: in validate_arguments src/core/CL/kernels/CLDirectConvolutionOutputStageKernel.cpp:51: ITensor data type QASYMM8 not supported by this kernel No such process\r\n\r\nAnd running graph_mobilenet with dafault data type goes well:\r\n$graph_mobilenet --target=CL\r\nCould you help to support this issue? Thanks.", "comments": []}, {"number": 23669, "title": "Disable warnning logging by default", "body": "One of the thing that I prefer pytorch more is that tensorflow are so annoying. Those infor logging such as tensorflow build doesn't with AVX or which gpu I got information does not needed to print out, cause we just doesn't care... those information we can get from nvidia or something else, but in tensorflow itself, we just need the model and logic....\r\n\r\nIt would make tensorflow more beautiful and wonderful if there are no such annoying information. simpler is better than complex.", "comments": ["This is really a tradeoff between two sides of TF users.\r\nOn one hand, TF is highly performance sensitive. Anything that can make TF faster, users may want to be aware of it. When we did not have some of these messasges, we received a lot of complaints that TF was silently slow, and users had no idea TF was slow.\r\n\r\nOn the other end of the spectrum is your point of view, where you would like it to \"just work\" without any cryptic messages.\r\n\r\nAssigning to @martinwicke", "Once we switch to MKL, these will disappear. Otherwise, what Gunhan said. "]}, {"number": 23668, "title": "A method to improve accuracy of LazyAdam", "body": "\r\n\r\n\r\n\r\n", "comments": ["Thanks. Can you open a PR putting it in `tf.contrib.opt`, and assign it to @alextp?", "I met the same problem of slow adam in sparse, waiting your feature", "Appreciate your patience. LazyAdam now falls under [tensorflow/addons](https://github.com/tensorflow/addons) starting TF 2.X. The folks of [Addons - Optimizers](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/README.md#addons---optimizers) can be a good point of contact to add this feature. Thanks!"]}, {"number": 23667, "title": "Incorrect cuda9.0 package installation required. Should be cuda-9-0", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.11 +\r\n- Doc Link: https://www.tensorflow.org/install/gpu\r\n\r\n\r\n**Describe the documentation issue**\r\nThe documentation states that you will need to install the cuda9.0 package. This does not exist in the repo - http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/. It should be the cuda-9-0 package instead.\r\n\r\nIf you don't install the package, tensorflow works, but the GPUs are not engaged. This can be verified using the `gpustat -cp` command provided by the python pip gpustat library.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["The documentation should also include installing the `cuda-command-line-tools-9-0 cuda-toolkit-9-0` and `cuda-cupti-9-1` packages found at http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/ versus mentioning the Nvidia website links:\r\n- https://developer.nvidia.com/cuda-zone\r\n- http://docs.nvidia.com/cuda/cupti/\r\n\r\nThey could still be included as a background reference. But the Ubuntu packages are available on the repo.\r\n\r\nThere should also be instructions on how to install `tensorflow-gpu` using RPMs. It's not much of a stretch, but it would really help newcomers to RedHat Linux variants.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23667\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23667\">No</a>\n", "Closing out this issue because it hasn't been updated in the last year.  Please reopen if this is still relevant."]}]