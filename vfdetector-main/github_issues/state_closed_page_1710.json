[{"number": 1600, "title": "Fix for bool strictness in CIFAR multi-GPU trainer.", "body": "Fixes #1595.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "@tensorflow-jenkins: test this please\n", "Jenkins, test this please.\n"]}, {"number": 1599, "title": "Add %matplotlib inline on the first cell", "body": "Config the matlotlib backend as plotting inline in IPython\n", "comments": ["Can one of the admins verify this patch?\n", "This is a huge diff: +782 \u2212754. Can you make it such that the only change is that one line?\n", "@vincentvanhoucke \nUpdated. \n", "Merged. Thank you!\n"]}, {"number": 1598, "title": "Android Demo build Error", "body": "Get an error when building the Android demo on Ubuntu 14.04.\n\nMy Workspace setting for Android SDK and NDK are given below:\n\nandroid_sdk_repository(\nname = \"androidsdk\",\napi_level = 23,\nbuild_tools_version = \"23.0.1\",\npath = \"/home/kuntal/Android/Sdk\"\n)\n\nandroid_ndk_repository (\nname = \"androidndk\",\npath = \"/home/kuntal/knowledge/IDE/android/android-ndk-r10e/\",\napi_level = 21\n)\n\nAlso i have the proper version in the SDK/NDk repo. NDK version (r10e).\n\nNow im trying to build with bazel using the following command:\n\nsudo bazel build tensorflow_demo -c opt --copt=-mfpu=neon\n\nAnd i even tried with sudo bazel build tensorflow_demo\n\n_but i'm getting the following error:_\n\nINFO: Found 1 target...\n*_ERROR: *_/home/kuntal/knowledge/codebase/PRACTICE/BIG-DATA/TensorFlow/tensorflow/tensorflow/examples/android/BUILD:65:1: Processing resources failed: resources_processor failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor --buildToolsVersion 23.0.1 --aapt bazel-out/host/bin/external/androidsdk/aapt_binary --annotationJar ... (remaining 13 argument(s) skipped).\nsrc/main/tools/namespace-sandbox.c:558: mount(opt->mount_sources[i], full_sandbox_path, NULL, MS_REC | MS_BIND | MS_RDONLY, NULL): Permission denied\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 1.950s, Critical Path: 0.14s\n\nPlease suggest how to fix this issue??\n", "comments": ["Andrew, does this look familiar?\n", "@Kuntal-G Which version of Bazel are you using?\n", "I'm using Bazel version 0.2.0 and i installed bazel with this  bazel-0.2.0-installer-linux-x86_64.sh\n\nI even tried with bazel version 0.1.3 ,but its has some problem in fetching from external resource and giving this error:\n\nSending SIGTERM to previous Bazel server (pid=2890)... done.\n.........\nERROR: no such package 'external': Error encountered while dealing with the WORKSPACE file: WORKSPACE file could not be parsed.\nINFO: Elapsed time: 0.634s\n", "WORKSPACE parsing should be just about the first thing that happens when building, so make sure there are no other changes introduced there that could be causing an issue.\n\nAlso, other users have had problems with a trailing slash on paths, so I'd try removing the one on your ndk path and see what happens.\n", "@andrewharp  I tried with new ndk path as mentioned.but getting same error.\n\nBut i was able to build my android .apk file by importing the project into Android studio and building with gradle.\n\nMay be i will give deeper look into this bazel build failure issue later on.\n", "Glad you were able to find a workaround. Closing this now as I can't reproduce myself. Please reopen if you try again and still experience issues.\n"]}, {"number": 1597, "title": "Is it possible to make IPython able to plot with matplotlib as default?", "body": "I am doing the assignment 1 of Deep learning course on Udacity. There is a problem to ask us to plot for checking the dataset.\nI can not plot any figure with matplotlib initially (nothing happened). Then, looked for the solution on the internet. I found it should be with a line that will make IPython able to attach the figure:\n`%matplotlib notebook` or `%matplotlib inline`\n\nif we don't want to key the line above each time we can change the config of IPython. \nIs it possible to add the line as default in this repository? I guess for novice they would be stuck a while on this point.\n", "comments": ["My own setup doesn't require it, so I can't easily test it. Would you mind sending me a pull request with the change? I'll be happy to merge it.\n", "@vincentvanhoucke \nPlease check it out\nhttps://github.com/tensorflow/tensorflow/pull/1599\n", "@vincentvanhoucke \nalso this one #1621 \nThank you\n"]}, {"number": 1596, "title": "Fix a missing-space issue in with_the_same_user", "body": "", "comments": ["@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 1595, "title": "using tensor as bool in cifar10 example", "body": "Hello,\n\nI have just recompiled TF to commit f952246 and got an error on cifar10 example. Line `if grad:` should be updated to `if grad is not None:` due to recent changes. Just minor stuff.\n", "comments": []}, {"number": 1594, "title": "Install from sources: Error: can't copy 'tensorflow/python/pywrap_tensorflow.py': doesn't exist or not a regular file", "body": "I recently tried to install Tensorflow on Ubuntu 14.10. I had cuda 7.0 and cudnn 3.0, so I had to install from sources following the instructions. I got \"error: can't copy 'tensorflow/python/pywrap_tensorflow.py': doesn't exist or not a regular file\" when building the pip package (with GPU support):\n\n`$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n`\nSo far, I have set the configuration for cuda and cudnn correctly. What should I do? Thanks. \n", "comments": ["Just to check: have you installed `swig` on your machine? It has to be installed manually (via `apt-get`) and is reponsible for producing that file. (I would have expected a different error message though.)\n", "@mrry: It's what I forgot. problem fixed. Thanks.\n", "I do have `swig` installed, but I'm still getting the same error. My process is outlined in [this](http://stackoverflow.com/questions/38773402/tensorflow-bazel-build-failing-not-generating-bazel-bin-directory/38773658#38773658) SO post (it was about a different question, but still explains everything). \n", "I see the same error on latest 1.0 version. on MacOS ElCapitan\r\n\r\nerror: can't copy 'tensorflow/python/pywrap_tensorflow.py': doesn't exist or not a regular file", "here is full error message:\r\n\r\nmymac:tensorflow myuser$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nFri 24 Feb 2017 22:41:56 GMT : === Using tmpdir: /var/folders/r5/njbm4bmx7x540l67m3zfp4lm0000gn/T/tmp.XXXXXXXXXX.hKT7gN4Z\r\n~/dev/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/dev/tensorflow\r\n~/dev/tensorflow\r\n/var/folders/r5/njbm4bmx7x540l67m3zfp4lm0000gn/T/tmp.XXXXXXXXXX.hKT7gN4Z ~/dev/tensorflow\r\nFri 24 Feb 2017 22:41:58 GMT : === Building wheel\r\nerror: can't copy 'tensorflow/python/pywrap_tensorflow.py': doesn't exist or not a regular file\r\nmymac:tensorflow myuser$ \r\nmymac:tensorflow myuser$ sudo bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nPassword:\r\nFri 24 Feb 2017 22:42:16 GMT : === Using tmpdir: /var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/tmp.XXXXXXXXXX.KZWr2rvk\r\n~/dev/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/dev/tensorflow\r\n~/dev/tensorflow\r\n/var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/tmp.XXXXXXXXXX.KZWr2rvk ~/dev/tensorflow\r\nFri 24 Feb 2017 22:42:18 GMT : === Building wheel\r\nerror: can't copy 'tensorflow/python/pywrap_tensorflow.py': doesn't exist or not a regular file\r\n\r\n\r\n", "@KendallWeihe @asimonov i got the same error. Do you have solved the problem?\r\n@tamhd  @mrry do you have any else solution?", "I did, but had to turn off SIP and downgrade to bazel 0.4.3\n\n_____________________________\nKind Regards,\nAlexey Simonov\n\n\n> On 4 Mar 2017, at 10:56, IvyGongoogle <notifications@github.com> wrote:\n> \n> @KendallWeihe @asimonov i got the same error. Do you have solved the problem?\n> @tamhd @mrry do you have any else solution?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "@asimonov can you tell me how turn off SIP? I can not understand your mean. Thank you very much", "Google 'how to turn off SIP in El Capitain'\n\n_____________________________\nKind Regards,\nAlexey Simonov\n\n\n> On 4 Mar 2017, at 14:08, IvyGongoogle <notifications@github.com> wrote:\n> \n> @asimonov can you tell me how turn off SIP? I can not understand your mean. Thank you very much\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n"]}, {"number": 1593, "title": "Android Camera Demo : Failed to build on Mac OS due to missing file ../darwin-x86/bin/arm-linux-androideabi-gcc", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nOS X EI Capitan version 10.11\n\nIf installed from sources, provide the commit hash:\n[b3d862b](https://github.com/tensorflow/tensorflow/commit/b3d862b376656c0f58b729e1848ea6e0c5bebf2a)\n### Steps to reproduce\n1. Follow guidelines here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md (Download Android NDK for MAC)\n2. build android demo using `$bazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures`\n3. Error : `process-wrapper: execvp(\"external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86/bin/arm-linux-androideabi-gcc\", ...): No such file or directory\n   Target //tensorflow/examples/android:tensorflow_demo failed to build`\n### What have you tried?\n1. Tried another way to build using `$bazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures --config=android_arm`\n2. Also tried Android NDK version `r10e`\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n`ERROR: /Users/hassanabid/Documents/hassan/open_source/tensorflow/tensorflow_source/tensorflow/examples/android/BUILD:43:1: Linking of rule '//tensorflow/examples/android:libpthread.so' failed: arm-linux-androideabi-gcc failed: error executing command \n  (cd /private/var/tmp/_bazel_hassanabid/96ab15ac510a01a39eca351cb09446d8/tensorflow_source && \\\n  exec env - \\\n  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86/bin/arm-linux-androideabi-gcc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-fastbuild/bin/tensorflow/examples/android/libpthread.so -Wl,-whole-archive -Wl,-no-whole-archive external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libsupc++.a -shared -static-libgcc -no-canonical-prefixes '-march=armv7-a' -Wl,--fix-cortex-a8 -Wl,-S '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: arm-linux-androideabi-gcc failed: error executing command \n  (cd /private/var/tmp/_bazel_hassanabid/96ab15ac510a01a39eca351cb09446d8/tensorflow_source && \\\n  exec env - \\\n  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86/bin/arm-linux-androideabi-gcc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-fastbuild/bin/tensorflow/examples/android/libpthread.so -Wl,-whole-archive -Wl,-no-whole-archive external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libsupc++.a -shared -static-libgcc -no-canonical-prefixes '-march=armv7-a' -Wl,--fix-cortex-a8 -Wl,-S '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nprocess-wrapper: execvp(\"external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86/bin/arm-linux-androideabi-gcc\", ...): No such file or directory\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nINFO: Elapsed time: 0.438s, Critical Path: 0.04s`\n", "comments": ["@hassanabidpk What version of Bazel are you using?\n", "After typing `bazel version`. I saw this : `0.2.0-homebrew`\n", "Where did you get Bazel homebrew from? I'd suggest sticking with a vanilla default Bazel install if you're experiencing problems.\n\nThis is the complete output I see when I type bazel version:\n`$ bazel version\nBuild label: 0.2.0\nBuild target: bazel-out/local_darwin-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue Feb 23 13:11:56 2016 (1456233116)\nBuild timestamp: 1456233116\nBuild timestamp as int: 1456233116\n`\n\nAlso, what did you set your ndk path to in the WORKSPACE file?\nWhen you run `ls -la $NDK_PATH/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi-gcc`, what do you see?\n", "1. `Build label: 0.2.0-homebrew\n   Build target: bazel-out/local_darwin-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Tue Feb 23 14:12:29 2016 (1456236749)\n   Build timestamp: 1456236749\n   Build timestamp as int: 1456236749`\n2. `-rwxr-xr-x@ 1 hassanabid  staff  747340 Mar 15 13:18 toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi-gcc`\n\nSorry for late reply - Got busy in GDE activities\n", "I fixed this by simply making a link (or just copy) the folder `($NDK_PATH)/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64` to  `($NDK_PATH)/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86` and this fixed the problem\n", "@hassanabidpk have you tried @kmader's solution?\n\n@ahumesky Do you have any guess why it's trying to use the non-existent darwin-x86 toolchain here? Any system running El Capitan should be 64bit from what I understand. I've not been able to reproduce on my macbook running OSX 10.11.4 -- it uses the darwin-x86_64 toolchain automatically.\n", "Hi @andrewharp  I have tried the same solution posted by @kmader and it worked. However I am also wondering why it is not using `darwin-x86_64` toolchain automatically. Thanks\n", "What version of the Android NDK are you using? We're working to update bazel to work with Android NDK r11, so for now you'll want to use r10e:\nhttp://dl.google.com/android/repository/android-ndk-r10e-darwin-x86_64.zip\n\nWhat is the content of `$NDK_PATH/RELEASE.TXT`?\nBazel uses that file to figure out if you have a 32-bit or 64-bit NDK.\n", "@ahumesky Build worked fine with `ndk-r10e`\n`RELEASE.TXT` contains `r10e (64-bit)`\n", "Just noticed that you built with --config=android_arm. This should not be necessary (or do anything, but it's worth checking to see if it's what's causing the issue). It was included in some previous directions by mistake.\n", "@ahumesky Did we ever figure out the root cause to this? If not, is this still relevant now that Bazel supports NDK 11?\n", "Not sure of the root cause. If ndk r11 was being used before bazel supported it, it could have been detecting the wrong bit-ness of the ndk.\n", "Seems likely, we can reinvestigate if it ever happens again.\n", "I build it tensorflow on ubunu 64  and referred this link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#building-in-android-studio-using-the-tensorflow-aar-from-jcenter. This problem can also produced, can't find x86 clang. the log is as bellows:\r\nsrc/main/tools/process-wrapper-legacy.cc:56: \"execvp(external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86/bin/clang, ...)\": No such file or directory\r\nAfter I copy the linux-x86_64 to linux-x86, Another problem exist:\r\nclang: error: unable to execute command: Executable \"as\" doesn't exist!\r\nCan you help fix this?"]}, {"number": 1592, "title": "Binary ops", "body": "Is there already a plan to add binary ops like bitcount for [XNOR-NET](http://arxiv.org/abs/1603.05279)?\n", "comments": ["I think this would be awesome to have, and contributions are definitely welcome here :).  \n", "@vrv Probably we need a XnorGemm first in Eigen. /cc @benoitsteiner What do you think?\n", "Or we could also implement them as individual OpKernels if it is too difficult to get it into Eigen.\n", "This is a reference [XnorGemm (with a custom kernel)](https://github.com/MatthieuCourbariaux/BinaryNet/blob/master/Run-time/binary_ops.py) under BSD related to a previous paper.\n\nEdit:\nThe kernel is [here](https://github.com/MatthieuCourbariaux/BinaryNet/blob/master/Run-time/binary_kernels.cu)\n", "@scott-gray is working on an [improved version with the upstream author](https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-200518733). Scott will you release the code under BSD or Apache? Eigen library is currently on BSD and TF on Apache but needs cla signature.\n", "/cc @mrastegari if interested\n", "@benoitsteiner Do you think that this operations could be added in Eigen first?\n", "@bhack we have a set of Eigen extensions to better support quantized operations on tensors in https://github.com/tensorflow/tensorflow/tree/master/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint. It's definitely possible to use the same approach to package a XnorGemm operation.\nI can also talk to the other maintainers of Eigen to check if it makes sense to add the code into the core Eigen and make it more widely available.\n", "@benoitsteiner Yes could be useful if you can collect some upstream opinions.\n", "8 bit quantization is available now. See merged https://github.com/tensorflow/tensorflow/pull/2230\n", "Has there been any progress on this? It would be useful for some embedded applications where an nvidia gpu isn't an option\n", "@kofd You can also start to read https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/quantization/index.md\n", "/cc @petewarden\n", "I have been looking at 'popcount' for binary networks, as bitcount is often known, since that seems to be the trickiest part to map to processor instructions. There is some BSD-licensed work here:\nhttps://github.com/WojciechMula/sse-popcount\nInterestingly the x86 CPU instruction seems to be competitive with SSE implementations. It looks like ARM requires a multi-instruction macro though:\nhttp://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0081b/CHDJJGAJ.html\n", "@petewarden There are also GCC built-in and llvm intrinsic. How many compilers TF wants support?\n", "@bhack: I was talking about the bit count convolutions used in xnor net.\n", "@petewarden A simple test of built-in with GCC and msvc at https://github.com/hanji/popcnt/blob/master/populationcount.cpp I think it is easy to add also llvm intrinsic for popcount.\n", "I have implemented a primitive op on cpu for the XOR + bitcount. but it's too slow right now. Does any one know how to speed this up? if this ever gets to the same speed as tf.matmul than I should provide a patch. Note this is not for convolution., this is simply replacing matmul() with a xnor + bit count.\n\n``` c++\n  void concatenate_col(\n          typename MatMulTypes<T>::in_type array,\n          MaskMatrix &out)\n  {\n      int rowSize = int((array.dimension(0)+31)/ 32);\n      out.resize(array.dimension(1),rowSize );\n\n      for ( int c=0; c< array.dimension(1); ++ c)\n      {\n          for ( int r = 0; r < rowSize; ++ r )\n          {\n              uint32_t rvalue=0;\n              uint32_t sign;\n              for ( int i=0; i< 32; ++i ) {\n                  int rowIdx = r*32 + i;\n                  if ( rowIdx > array.dimension(0)-1 ) {\n                      break;\n                  }\n                  sign = (array(rowIdx, c )>=0);\n                  rvalue = rvalue | (sign <<i);\n              }\n              out(c, r) = rvalue;\n          }\n      }\n  }\n  void concatenate_row(\n          typename MatMulTypes<T>::in_type array,\n          MaskMatrix &out)\n  {\n      int colSize = int((array.dimension(1)+31 )/ 32);\n      out.resize(array.dimension(0), colSize);\n      for ( int r = 0; r < array.dimension(0); ++ r )\n      {\n          for ( int c=0; c< colSize; ++ c)\n          {\n              uint32_t rvalue=0;\n              uint32_t sign;\n              for ( int i=0; i< 32; ++i ) {\n                  int colIdx = c*32 + i;\n                  if ( colIdx > array.dimension(1)-1 ) {\n                      break;\n                  }\n                  sign = (array(r, colIdx)>=0);\n                  rvalue = rvalue | (sign <<i);\n              }\n              out(r,c) = rvalue;\n          }\n      }\n  }\n\n  void concatenate_and_compute(\n          const CPUDevice &d,\n          typename MatMulTypes<T>::in_type a,\n          typename MatMulTypes<T>::in_type b,\n          typename MatMulTypes<T>::out_type out)\n  {\n      MaskMatrix a_;\n      MaskMatrix b_;\n\n      concatenate_row(a, a_);\n      concatenate_col(b, b_);\n\n      for (int ar=0; ar < a_.rows(); ar++)\n      {\n          for (int br=0; br< b_.rows(); br++) {\n              unsigned int Cvalue = 0;\n              for (int c=0; c< a_.cols(); c++)\n              {\n                  unsigned int value =popcnt(a_(ar, c) ^ b_(br,c));\n                  Cvalue += value;\n              }\n              out(ar, br) = - ( 2*(float)Cvalue - a.dimension(1) );\n          }\n      }\n\n  }\n\n```\n", "From my experience the best approach for popcnt on avx2 would be this one: https://github.com/WojciechMula/sse-popcount/blob/master/popcnt-avx2-lookup.cpp. But that code needs a little fix for a counter overflow.  xor also needs to be done in avx2.\nFor TF I guess there needs to be a generic non-avx2 code. There are some references in that repo as well.\n", "@ppwwyyxx Have you benchmarked  against recent GCC, MSVC, LLVM/CLANG intrinsics?\n", "@bhack I don't think there will be a big difference with different intrinsics. They all end up being avx2 instructions anyway.\n", "Are you sure that all internal compiler code use avx2? I think that built-in are supported also on arm/neon.\n", "Oh right. if you are talking about compatibility then compiler builtins may be a good choice. But i never tried them.\n", "Has there been any movement in this issue? I'm very interested in seeing how binary networks can be trained using tensorFlow. I have studied the work of Courbariaux and played a bit with his implementations (specifically BinaryConnect) but my final goal would be to have XNOR-Net running with tensorFlow. \n", "would also be interested in this!\n", "binarization can be done with tf.sign(). though the tricky part is to get the gradient backprop to work after binarizing input. for now this requires a separate op I implemented in tensorflow. https://github.com/zhengwy888/binary_ops. with this code you can implement your own XNOR net on GPU. comments/suggestions welcome.\n", "For who is interested there is also https://arxiv.org/abs/1606.06160\n", "Thanks @bhack for mentioning. We have a DoReFa-Net training implementation available at [dorefa.net](http://dorefa.net), which doesn't make use of any custom C++ Op. Since DoReFa-Net is a generalization of XNOR-Net, XNOR-Net can be built in TF in a similar manner (without binary op acceleration). I'm also releasing a trainable DoReLa(1,2,6)-AlexNet later today.\n", "@ppwwyyxx Nice! /cc @wangyida\n", "Thanks for sharing everyone. But as I understood the DoReFa implementation so far, it still uses the standard 32 bit tensors. I've been thinking about ways to use the official TensorFlow quantization methods to reduce the memory at least to a quarter. The 8-bit datatype is available. Of course the question would be how to use it with the least hassle. \n", "In my perspective of view. released pipeline of DeRoFa is still float, and quantization module in TF could have a 8 bit representation with little performance drop, but this won't be the aim of DeRoFa which is already a quantized model just in a float representation of codes.\n", "@Wangyida Yes, I agree that it seems that the intention of the authors of DoReFa was mostly to reduce training and inference time by using low bitwidth. I don't expect them to release an 8bit version. However they also mention in their paper the idea to use this kind of network on embedded devices. If you want to use AlexNet on a very small device, I don't see the reason to use 32 bit floats if there is no information held in them anyways. To me, the quantization to a smaller datatype is just the next logical step. \n", "@isabel-schwende Yes the released implementation uses float32 to **hold** low-bitwidth numbers, because there is simply no low-bitwidth operations available in TF. And we never planned to build such operations into TF because we already have our own low bitwidth run-time implementations working smoothly on ARM.  \nThe released model is similar: it uses tf.float32 to hold all the binary weights as well as run all the computation. But anyone who would like to implement those binary operations can make use of our pretrained model directly and gain a speedup.\n", "@ppwwyyxx Thank you for your clarification but I think I have to disagree at this point. Yes, there is no Datatype in TensorFlow available for 1,2 or 6 bit but there is the tf.quint8 datatype for tensors. @petewarden and his team have introduced it in their tutorial here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/quantization/index.md  Sure, the method of quantization is different to what DoReFa is doing as they keep minimums and maximums as floats. I've played around with the tutorial and also used a customised AlexNet saved in a protobuf-file to quantize down to 8 bit. I was able to observe that the protobuf-file of the quantized network was indeed much smaller compared to the 32 float original. For now, the way how low-bitwidth weights/activations are used is not compatible but so I was wondering if there is a way to, let's say, create a customized version of the existing quantization tool to also decrease the memory of the DoReFa AlexNet model for inference tasks on small devices. But I guess that it would be still too much work at the moment. \n", "I totally agree and the code we released was never intended for running on mobiles, but for showing how to train such networks in tensorflow, as a supplementary material and proof of the paper. After all I've never seen a public release of binary-weight ImageNet models before.\n\nI can certainly compress our models to about 30x smaller because they are essentially binary, and maybe use tf.quint8 for computation. But that doesn't make sense to me, because I think anyone who really want 1 or 2 bit level of performance & compression rate, should use a much more compact & tiny run-time toolchain as we did, instead of using tensorflow. If the speed & storage of 8 bit models is good enough for the use case, then I would certainly suggest trying the TF quantization tutorial instead of DoReFa-Net, because 8 bit models would have better accuracy. \n", "@ppwwyyxx Thanks a lot for your answer. I definitely agree. For that reason I started with the quantization available by TensorFlow. It seems like using it for inference is currently very slow. As several others have described here https://github.com/tensorflow/tensorflow/issues/2807 the memory might be much lower for saving the model but inference time is approximately doubled, which is undesirable in our project. I've been searching for something that is fast and has low memory requirements but it seems like the available code is still in research phase. I guess we will do some hacking to get something customised for our project. \n", "The current slow speed of quantization is due to reference implementations for some of the ops, which we're actively working on optimizing. The goal is that quantization should be faster than float.\n", "Hi All,\nIs it implemented through code about XNOR-Net? If it can be implemented by Caffe?\n", "See also https://github.com/NervanaSystems/neon/commit/caf0aaaaa1438b09c905e0780ba1120c6fd25f1c\n", "@bhack It is binary net not XNOR-Net! Anyway thank you for the info.\n", "@AaronYKing I guess certain companies might have their own working versions of XNOR-Net but, as far as I know, there is no complete public implementation so far. It's too bad that the original authors didn't release their code for XNOR-Net for the sake for reproducibility. If I remember correctly, the authors of DoReFa-Net tried to reproduce results of the original paper but failed to obtain comparable results. As the description in the paper is a bit thin at times, it might take a while for a really comparable, complete implementation of XNOR to be released (no matter if TensorFlow or Caffe). \n", "We have released our Torch code and trained models for XNOR-Net. This is not the fast implementation. \n", "Here is the link: https://github.com/mrastegari/XNOR-Net\n", "@isabel-schwende Thank you very much for your patient replay. Now the author have released the torch code. \n", "@mrastegari Thank you for your contribution. I wonder if it can be implemented by Caffe and will anybody implement it in the near future\uff1f\n", "@mrastegari thanks a lot for sharing this information with us. I think this is going to be really helpful. Do you have any numerical information of how slow this Torch version is compared to the original version using Darknet? @AaronYKing I'm always glad if there are cases where I'm positively surprised about code being shared publicly.  \n", "Is this xnor gemm kernel useful https://github.com/NervanaSystems/neon/blob/master/neon/backends/kernels/cuda/binary.py?\n", "@bhack This kernel works, but only about 3.4x faster than best fp32 kernels (cublas). It's mentioned in the BNN paper and https://github.com/MatthieuCourbariaux/BinaryNet/pull/1.\n", "Just curious, is anyone seriously pursuing this? I've been working on a fast C++ implementation of XNORNets here at AI2 with @mrastegari and others for Intel and ARM CPUs. We've achieved a modicum of success, with much headroom still remaining untapped. We're kicking around the idea of producing a fast, reference CPU implementation, so it'd be good to know if someone else is already close to releasing it.\n", "@dmitryb-ai2 @mrastegari It will be a great progress if the XNORNets can be implemented by a fast C++ for Intel  and ARM CPUs, especially getting 58\u00d7 faster convolutional operations and 32\u00d7 memory savings as mentioned in the paper. But, sadly, I can't code it. So looking forward to your good news!\n", "I have an \"implementation\", but it achieves only ~15% accuracy on CIFAR-10\n(compared to 86% using real-valued convolutions for the same architecture),\nso something is definitely wrong. I will post my code if and when I get it\nto work.\n\nOn Fri, Jul 29, 2016 at 7:43 PM, dmitryb-ai2 notifications@github.com\nwrote:\n\n> Just curious, is anyone seriously pursuing this? I've been working on a\n> fast C++ implementation of XNORNets here at AI2 with @mrastegari\n> https://github.com/mrastegari and others for Intel and ARM CPUs. We've\n> achieved a modicum of success, with much headroom still remaining untapped.\n> We're kicking around the idea of producing a fast, reference CPU\n> implementation, so it'd be good to know if someone else is already close to\n> releasing it.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1592#issuecomment-236320276,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ACMfUUf79HDexQwcSXpxV9N5E-K85Wxiks5qapAigaJpZM4H23wu\n> .\n", "@lorenlugosch Hi, I am interested in your problem. What framework do you use? And will you release it to the git?\n", "Hi @csyking, my binary convolution operation is a custom TensorFlow op written in C++. Everything else is normal TensorFlow operations. I will publish the code once I get the accuracy up.\n", "Hi @lorenlugosch, It's very kind of you. Looking forward to your good new!\n", "@mrastegari  May you share the model for the Resnet-18? I am training it starting from the model you proposed in your implementation (with some modifications) but the progresses seem to be very slow.\n", "We got a \"version\" working in Eigen::Tensor (~7x performance over float on Xeon AVX-256), but we're still hitting relatively low accuracies (20 to 40 % more error than a float based net). The accuracy drops quickly as you increase the output channels in a layer.  \n\nFrom a performance POV the bit packing code slows things down a bit and the calculation of beta values still takes time. I'm not sure if the original paper took this into account because it defined a \"convolution\" as seperate from the binarize step. In practice, however, you need both for every conv2d layer. Still, I might be missing something. \n\nBTW I'm having issues getting the code to compile in tensorflow. Works fine in Eigen though.\n", "@rapatel0 7x performance means 7x faster? could you release some part of your code? I tried to use gcc popcnt for bitcounting but it's really slow.\n", "Depending on your compiler flags gcc popcount won't emit a _popcnt64 instruction which limits the performance. You should try that first. The code is still buggy and requires some more testing but I'll release it once we get a chance to clean it up. \n", "What do you think of [this Adobe patent](http://www.freepatentsonline.com/y2016/0148078.html)?\n", "For who is still interested in the topic see QNN https://arxiv.org/abs/1609.07061\n", "Any progress on this? Besides more sophisticated kernels like XnorGEMM, there's value in supporting bitwise and, or, xor on types like int32 and int64.", "Definitely very low precision networks become more and more popular: see [Trained Ternary Quantization](https://arxiv.org/abs/1612.01064) and [Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights](https://arxiv.org/abs/1702.03044). We are looking forward for arbitrary quantization and efficient operations (e.g. XNOR-popcount) in TensorFlow ", "Or in gemmlowp so other frameworks could use it.", "@lorenlugosch Did you managed to finish your binary convolution? I'm interested in knowing how to implement low-precision operations for the forward pass in CNNs... @bhack Do you know if there is any support for this in TF? Besides the 8bit quantisation process, I want a \"hard\" quantisation, not the one that uses the max and min with float values", "Does anyone have an update on progress towards binary TF ops? I'm weighing the pros and cons of working on this problem myself (the pro being that it'll be useful, and the con primarily being the technical investment of fully grokking the Eigen/cuBLAS side of TF).\r\n\r\nThanks!", "https://github.com/Microsoft/CNTK/tree/master/Examples/Extensibility/BinaryConvolution\r\nhttps://github.com/hpi-xnor/BMXNet\r\n\r\nBinary nets in other frameworks ", "It's still a bit rough, but here's a custom op with an mnist training example and benchmarks against tf.matmul. Feedback and suggestions welcome!\r\n\r\nhttps://github.com/AngusG/tensorflow-xnor-bnn", "ebrevdo@, I found that you added PopulationCount op some time ago:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/bitwise_ops.cc\r\nwhich seems like it supports 'bitcount':\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_PopulationCount.pbtxt\r\n\r\nIs this op available to use or is there more work needed?", "The op is supported for use; but not part of the public API; you can access it directly though.  See an example [here](https://github.com/tensorflow/tensorflow/blob/40d1b257636a0b510dba59aff0af54e42d602313/tensorflow/python/ops/bitwise_ops_test.py#L69)", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Marking as closed since it's available.", "gemm_op.so not find", "> Is there already a plan to add binary ops like bitcount for [XNOR-NET](http://arxiv.org/abs/1603.05279)?\r\n\r\nIf someone is still interested, we have developed open source libraries on top of @tensorflow and TensorFlow Lite to train and deploy binarized neural networks (BNNs) similar to XNOR-NET.\r\n- [Larq](https://github.com/larq/larq) to train BNNs.\r\n- [Larq Compute Engine](https://github.com/larq/compute-engine ) to deploy BNNs on Arm devices. "]}, {"number": 1591, "title": "fix test sizes", "body": "", "comments": ["@girving we have set the timeouts way too strict... see http://ci.tensorflow.org/job/tensorflow-master-cpu/403/console\n\n@girving @martinwicke @vrv why have we shrink down the timeouts? It will imho bring a lot of flakiness to the tests. Plus even if it runs well on our ci servers it may be always failing on contributor's notebook which has probably slower cpu. Why don't we have default \"medium\" or even \"large\"? 99% of tests will be fine and the 1% currently flaky will fail anyway. We should, imho, fix tests like session_test instead of chasing timeouts in BUILD files.\n", "Jenkins, test this please.\n\n@martinwicke: Preferences?  I'm not the right person to judge the usefulness, since I'm not familiar with the details of timeouts. \n", "I just want to avoid the endless build when we have an error that causes a\ndeadlock in session.run or some such. It happened before.\nOn Wed, Mar 23, 2016 at 07:41 Geoffrey Irving notifications@github.com\nwrote:\n\n> Jenkins, test this please.\n> \n> @martinwicke https://github.com/martinwicke: Preferences? I'm not the\n> right person to judge the usefulness, since I'm not familiar with the\n> details of timeouts.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1591#issuecomment-200374373\n"]}, {"number": 1590, "title": "Merge internal changes from late March 22nd", "body": "Some small fixes to skflow in merge commit to rectify the original import.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 1589, "title": "Delete superfluous file added by a commit", "body": "", "comments": []}, {"number": 1588, "title": "Slicing sparse tensor", "body": "There does not appear to be a way to extract a (sparse or dense) slice from a sparse tensor in the same manner as there is from a dense tensor. Will this be a feature in the future, or is it already present and I'm just missing something? Perhaps it should be added to the planned improvements for indexing.\n", "comments": ["To expand on this, I'd like to be able to construct a (ultimately dense, but this is irrelevant) subtensor from a  much larger, sparse tensor that's far too large to fit into memory. In numpy, this is something like having:\n\n```\nx = np.array(range(16)).reshape(4, 4)\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15]])\n```\n\nAnd wanting to extract the subtensor formed by the 2nd (i.e., index 1) and 4th row with the 2nd and 3rd column:\n\n```\nx[[[1],[3]],[1,2]]\narray([[ 5,  6],\n       [13, 14]])\n```\n\nUltimately, the batches of the network I'm training will rely on such fixed-size subtensors randomly constructed from the larger tensor. However, at present the only way of doing this currently appears to be manually feeding the subtensors in from Scipy.\n", "We don't currently have any plans to implement a slice op on SparseTensor.  My suggestion would be to first implement a sparse->sparse slice, and then use sparse_to_dense to densify it.\n\nThe closest thing we have is a sparse split; the meat of which is implemented here:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/sparse/sparse_tensor.h#L437\n\nIf you're interested in implementing a SparseSlice, contributions are welcome.  The signature should probably be:\n\n```\nSparseTensor SparseTensor::Slice(\n  const SparseTensor& input_tensor,\n  const gtl::ArraySlice<int64>& start,\n  const gtl::ArraySlice<int64>& size)\n```\n", "@concretevitamin: Is slice one of the ops for which you're implementing SparseTensor support?\n", "mdan is implementing SparseTensor slice this week.  I think there's some\ncode already done.\n\nOn Mon, May 16, 2016 at 11:25 AM, Derek Murray notifications@github.com\nwrote:\n\n> @concretevitamin https://github.com/concretevitamin: Is slice one of\n> the ops for which you're implementing SparseTensor support?\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1588#issuecomment-219504264\n", "I am not actively working on this op.  Happy to review any PRs ;)\n\nOn Monday, May 16, 2016, ebrevdo notifications@github.com wrote:\n\n> mdan is implementing SparseTensor slice this week. I think there's some\n> code already done.\n> \n> On Mon, May 16, 2016 at 11:25 AM, Derek Murray <notifications@github.com\n> <javascript:_e(%7B%7D,'cvml','notifications@github.com');>>\n> wrote:\n> \n> > @concretevitamin https://github.com/concretevitamin: Is slice one of\n> > the ops for which you're implementing SparseTensor support?\n> > \n> > \u2014\n> > You are receiving this because you were assigned.\n> > Reply to this email directly or view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/issues/1588#issuecomment-219504264\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1588#issuecomment-219516188\n", "Is someone working on this op? ", "No, you are welcome to give it a shot.\n\nOn Tue, Jan 10, 2017 at 11:43 PM, J\u00f6rg Franke <notifications@github.com>\nwrote:\n\n> Is someone working on this op?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/1588#issuecomment-271800019>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9T06FacWj2LTZiiCtoUEUCp68Joks5rRIg_gaJpZM4H2rG1>\n> .\n>\n", "@ebrevdo \r\nI've read a related [comment](https://github.com/tensorflow/tensorflow/issues/342#issuecomment-160354041) of yours and it helped me a lot.\r\n\r\nIs it possible to extract sparse ids and train sparse weights on the fly?\r\n\r\nMy application is finetuning sparse embedding: Given a sequence of word ids, firstly use a cookbook to look up sparse indices and weights of each word, then use `tf.nn.embedding_lookup_sparse` to get their embeddings. Since I want to finetune the sparse weights (indices, i.e.: sparse structure of data, are fixed, of course),  directly feed them to placeholders are not adequate.\r\n\r\nTo be more specific:\r\n1. How can I extract certain rows (maybe discrete and even have duplicate rows) out of a `SparseTensor`?\r\n2. If the first one is possible, can I perform training on sparse weights? (Words may duplicate in a sentence, so sparse weights may duplicate in a sentence, so the variables need to be shared to make sure gradient descent algorithms work properly. Simply pass pre-calculated weights to a placeholder does not work for this purpose)\r\n\r\nCurrently is it possible to do this in TensorFlow? Or I need some work around (e.g. use sparse dense matrix multiplication or something else) to do this?\r\n\r\n---------------------------------\r\n\r\nUpdate\r\n============\r\n\r\nI've worked this out. Just check [stackoverflow](http://stackoverflow.com/questions/43557785/how-can-i-select-a-row-from-a-sparsetensor-in-tensorflow/43615108).", "Added a PR #9540 to support slice of sparse tensors. Please take a look.", "What would it take to implement a `SparseTensor.__getitem__` method?\r\nThere's no `tf.sparse_strided_slice` function which `Tensor.__getitem__` seems to use."]}, {"number": 1587, "title": "TensorBoard doesn't handle ~ in paths properly", "body": "Right now, TensorBoard does not properly evaluate a path beginning with `~` (at least on mac). Example:\n\n```\n(tensorflow) ~/space\u276f pwd\n/Users/danmane/space\n(tensorflow) ~/space\u276f tensorboard --logdir=~/foo/zoid --debug --host=localhost\nINFO:tensorflow:TensorBoard is in debug mode.\nINFO:tensorflow:Starting TensorBoard in directory /Users/danmane/space\nINFO:tensorflow:TensorBoard path_to_run is: {'/Users/danmane/space/~/foo/zoid': None}\n```\n", "comments": []}, {"number": 1586, "title": "TensorBoard prints warning: TAG not not found", "body": "On start, TensorBoard prints the following message:\n\n```\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/Users/danmane/tensorflow/lib/python2.7/site-packages/tensorflow/tensorboard/TAG' on path /Users/danmane/tensorflow/lib/python2.7/site-packages/tensorflow/tensorboard/TAG\nWARNING:tensorflow:Unable to read TensorBoard tag\n```\n\nThis issue is harmless but distracting. \n", "comments": ["Fixed as of r0.8 :)\n"]}, {"number": 1585, "title": "Upstream changes from internal for March 22", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 1584, "title": "Note about compiling user op with gcc 5", "body": "Added a note about compiling user op library with gcc 5\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n"]}, {"number": 1583, "title": "Add __pycache__ to .gitignore", "body": "", "comments": ["@girving: is this ready to merge?  I assumed since it was assigned to @itsmeolivia, I shouldn't yet merge.\n", "Merge away, thanks!\n"]}, {"number": 1582, "title": "Crash on retraining flowers", "body": "I am following the tutorial [How to Retrain Inception's Final Layer for New Categories](https://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html) but am getting this error:   \n\n```\n...\n3600 bottleneck files created.\nTraceback (most recent call last):\n  File \"/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py\", line 829, in <module>\n    tf.app.run()\n  File \"/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py\", line 785, in main\n    ground_truth_tensor: train_ground_truth})\n  File \"/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/client/session.py\", line 332, in run\n    result = self._run(None, fetches, feed_dict, options_ptr, run_outputs_ptr)\n  File \"/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/client/session.py\", line 530, in _run\n    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\nValueError: Cannot feed value of shape (100, 2048) for Tensor u'pool_3/_reshape:0', which has shape '(1, 2048)'\n```\n### Environment info\n\nOperating System: OSX  10.11.1 El Capitan\n\nI've installed from source from this commit `ad1d98011ebc`.  I ran `./configure` with the default answers and followed [these build instructions](https://www.tensorflow.org/versions/master/get_started/os_setup.html#installation-for-mac-os-x)\n### Steps to reproduce\n\n```\ncd ~\ncurl -O http://download.tensorflow.org/example_images/flower_photos.tgz\ntar xzf flower_photos.tgz\nbazel build -c opt --copt=-mavx tensorflow/examples/image_retraining:retrain\n# Crashes with error above\nbazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos\n```\n### What have you tried?\n1.  I've tried deleting `/tmp/bottleneck` and re-running, but I got the same error.\n", "comments": ["Could you try passing in the following argument:\n\nbazel-bin/tensorflow/examples/image_retraining/retrain --train_batch_size=1 --image_dir ~/flower_photos\n\nSherry\n", "I ran it and it still crashed, although the output and stack trace is a little different.  Here is the relevant output:\n\n```\n...\n3600 bottleneck files created.\n2016-03-23 10:03:18.932934: Step 0: Train accuracy = 100.0%\n2016-03-23 10:03:18.933883: Step 0: Cross entropy = 0.116586\nTraceback (most recent call last):\n  File \"/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py\", line 829, in <module>\n    tf.app.run()\n  File \"/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py\", line 804, in main\n    ground_truth_tensor: validation_ground_truth})\n  File \"/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/client/session.py\", line 332, in run\n    result = self._run(None, fetches, feed_dict, options_ptr, run_outputs_ptr)\n  File \"/Users/rick/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/client/session.py\", line 530, in _run\n    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\nValueError: Cannot feed value of shape (100, 2048) for Tensor u'pool_3/_reshape:0', which has shape '(1, 2048)'\n```\n", "If you set test_batch_size, validation_batch_size and train_batch_size all to 1 it works. But that's probably not what you want.\n", "I looked into this, and it appears to have broken when the new shape strictness checking went into effect:\nhttps://github.com/tensorflow/tensorflow/commit/9a8c5ad18c61cb0695d31e2ce969008c82999c7c\n\nUnfortunately, the input we're feeding in is not a placeholder, but is an override of a regular node that's loaded from the graph def, so it's not clear from the examples how to fix the error in this case. Passing over to @mrry for advice on the best way to fix this.\n", "Right, you need to replace the node `\"pool_3/_reshape:0\"` with a `tf.placeholder_with_default(tf.reshape(...), shape=[None, 2048])`.\n\nWould one of you be a dear and show me where this graph is created? Then I can send a PR or make the appropriate changes. If as I fear it's using a binary graph downloaded the website, we might need to mint a new graph.\n", "> If as I fear it's using a binary graph downloaded the website, we might need to mint a new graph.\n\nYour fear is correct, and it's actually even worse, because we also need to save the result out to a binary file, so we'll have to do any swapouts twice. Here's the place where the loading is done:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py#L291\n\nHere's where we save it out again at the end:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py#L820\n", "Oh, maybe it's not so bad then: in `create_inception_graph()` you can specify `{BOTTLENECK_TENSOR_NAME: some_placeholder_tensor}` as the `input_map` argument. That way, you can define the placeholder to be whatever shape you want it to be in `retrain.py`, and you won't have to modify the original binary graph!\n", "Thanks Derek! I'll see if I can get your help on this tomorrow, since I'm still a bit lost. I expected that changing input_map would mean that the original Reshape node would be inaccessible when I ran the full graph?\n", "Absolutely, let's talk tomorrow! (Using `input_map` would modify all consumers of the reshape node so that they took the placeholder instead. In the present implementation, the reshape node would actually still be in the graph, but nothing would be connected to it, and it probably wouldn't ever be executed.)\n", "I'm getting the exact same issue as @ricochet2200  with the shape of `Tensor u'pool_3/_reshape:0'` except I'm running on Ubuntu 14.04 and commit `46bfa0feef`\n\n> Right, you need to replace the node \"pool_3/_reshape:0\" with a tf.placeholder_with_default(tf.reshape(...), shape=[None, 2048]).\n\nWhere can I find the node?\n", "We have just got a fix completed and approved, I should be able to check it in this evening. Apologies for the breakage!\n", "Sorry for the delay, I have this working on our internal repo, but there are some hiccups on exporting it to Github. I should be able to get this released on Monday morning.\n", "Thanks!  I followed your tutorial on getting it set up on 6d46c0b37083 Docker and that worked, but getting it running on bare metal would be nice to utilize my GPU.\n", "The fixed version should now be in the top-of-tree code. Please let me know if you're still having issues:\n\nhttps://github.com/tensorflow/tensorflow/commit/3ca08f75896d733d12d7106f76abf51e782f50da\n", "I verified this and it is indeed working as expected.  Thanks everyone!\n", "The error _ValueError: Cannot feed value of shape (100, 2048) for Tensor 'pool_3/_reshape:0\r\n', which has shape '(1, 2048)'_ still occurs for the current build _#202 (08.06.2017 02:25:00)_ (tensorflow 1.2.0 rc1) on Windows 7.\r\n\r\nEdit: For the flower image data from the official tutorial as well as for normalized custom images (same size, same color depth)."]}, {"number": 1581, "title": "Odd error message for empty session run with queues", "body": "I have encountered a counter intuative error when working with queues and accidental `sess.run([])`.\nThis is a stretch, but possibly related to #1277 where batch norm updates are being run but there is no batch norm in the graph. \n### Environment info\n\nOperating System: Ubuntu 14.04, tf version (63409bd23facad471973b110df998782c0e19c06) with cuda.\n### Steps to reproduce\n\n``` python\nimport tensorflow as tf\n\nBREAK_ME = False\n\ndef get_iter():\n    img_list = [(tf.zeros((12,)), ), (tf.zeros((12, )), )]\n    xx = tf.train.shuffle_batch_join(\n        img_list, batch_size=5,\n        capacity=1000 + 3 * 5,\n        min_after_dequeue=1000)\n    return xx\n\nif BREAK_ME:\n    w = get_iter()\n\nsess = tf.Session()\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\nsess.run([])\n```\n\nAs expected, when `BREAK_ME` is false I get:\n`RuntimeError: The Session graph is empty.  Add operations to the graph before calling run().`\n\nWhen `BREAK_ME` is true, enabling the creation of a queue I get the following:\n\n```\nTraceback (most recent call last):\n  File \"test.py\", line 20, in <module>\n    sess.run([])\n  File \"/home/indico/Apps/tensorflow/_python_build/tensorflow/python/client/session.py\", line 332, in run\n    result = self._run(None, fetches, feed_dict, options_ptr, run_outputs_ptr)\n  File \"/home/indico/Apps/tensorflow/_python_build/tensorflow/python/client/session.py\", line 537, in _run\n    feed_dict_string, options, run_outputs)\n  File \"/home/indico/Apps/tensorflow/_python_build/tensorflow/python/client/session.py\", line 599, in _do_run\n    target_list, options, run_outputs)\n  File \"/home/indico/Apps/tensorflow/_python_build/tensorflow/python/client/session.py\", line 621, in _do_call\n    e.code)\ntensorflow.python.framework.errors.OutOfRangeError: RandomShuffleQueue '_0_shuffle_batch_join/random_shuffle_queue' is closed and has insufficient elements (requested 5, current size 0)\n     [[Node: shuffle_batch_join = QueueDequeueMany[component_types=[DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch_join/random_shuffle_queue, shuffle_batch_join/n/_8)]]\nCaused by op u'shuffle_batch_join', defined at:\n  File \"test.py\", line 14, in <module>\n    w = get_iter()\n  File \"test.py\", line 10, in get_iter\n    min_after_dequeue=1000)\n  File \"/home/indico/Apps/tensorflow/_python_build/tensorflow/python/training/input.py\", line 770, in shuffle_batch_join\n    dequeued = queue.dequeue_many(batch_size, name=name)\n  File \"/home/indico/Apps/tensorflow/_python_build/tensorflow/python/ops/data_flow_ops.py\", line 301, in dequeue_many\n    self._queue_ref, n, self._dtypes, name=name)\n  File \"/home/indico/Apps/tensorflow/_python_build/tensorflow/python/ops/gen_data_flow_ops.py\", line 359, in _queue_dequeue_many\n    timeout_ms=timeout_ms, name=name)\n  File \"/home/indico/Apps/tensorflow/_python_build/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/home/indico/Apps/tensorflow/_python_build/tensorflow/python/framework/ops.py\", line 2104, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/indico/Apps/tensorflow/_python_build/tensorflow/python/framework/ops.py\", line 1129, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": ["I think this is \"intended behavior\" for calling `sess.run([])`: although its use is not recommended, it runs all of the ops in the graph. This would include the enqueue, dequeue, and close ops for the queue created in `tf.train.shuffle_batch_join()`, leading (non-deterministically) to the error you saw).\n\nI don't think any well-formed programs rely on the behavior of `sess.run([])`, and it can't possibly be used in a program with a complex input pipeline, so we should probably make it an error.\n", "Makes sense. Didn't realize that this was supposed to do anything. The call occurred by accident when manually triggering non existent batch norm updates and was rather annoying to track down. I will close.\n"]}, {"number": 1580, "title": "Fix docs for cross-entropy loss functions", "body": "Should close https://github.com/tensorflow/tensorflow/issues/1234\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n"]}, {"number": 1579, "title": "Fix a Tensor-as-boolean issue in cifar10 tutorial", "body": "The bug in cifar10.py was caught be the nightly tutorial tests today. For example, see:\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/39/console\n\nThis PR should fix it.\n", "comments": []}, {"number": 1578, "title": "Release GPU memory after computation", "body": "Is it possible to release all resources after computation?\n\nFor example,\n\n``` python\nimport time\nimport tensorflow as tf\n\nfor i in range(0,10000000):\n  t0 = time.clock()\n\n  with tf.Graph().as_default():\n    sess = tf.Session()\n\n    a = tf.placeholder(tf.int16, name='a')\n    y = tf.identity(a, name='y')\n\n    sess.run(y, feed_dict={a: 3})\n    sess.close()\n\n  time.sleep(20)\n\nprint time.clock() - t0\n```\n\nWhen the program is sleeping, I type `nvidia-smi`, and the memory is always occupied.\n\n``` bash\n| NVIDIA-SMI 352.79     Driver Version: 352.79         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:03:00.0      On |                  N/A |\n| 29%   48C    P2    81W / 250W |   5843MiB /  6143MiB |      1%      Default |\n```\n\nThe behavior I have observed is that only after the program exit, the memory is released. It makes using multiprocessing hard. Suppose one process is waited on a lock for another progress to finish, and all two processes need to join the main process. Then when process one release the lock, process two cannot get GPU memory, so it would fail.\n\nIs there any way to release memory, so when the above program(not the two process example) is sleeping, it will release memory? \n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\n``` bash\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n```\n1. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n``` bash\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.7.1\n```\n", "comments": ["Try passing \"https://github.com/tensorflow/tensorflow/blob/30b52579f6d66071ac7cdc7179e2c4aae3c9cb88/tensorflow/core/protobuf/config.proto#L35\" set to true as an argument to your Session's config arguments, it might help a little, though it won't release memory, it just allows growth at the cost of some memory efficiency.\n\nAlternatively, you could delete your session objects (which should release the memory associated with them) when you don't need them.\n", "How could I delete a session object in python?\n\nI tried\n\n``` python\nimport time\nimport tensorflow as tf\n\nfor i in range(0,10000000):\n  t0 = time.clock()\n\n  with tf.Graph().as_default():\n    sess = tf.Session()\n\n    a = tf.placeholder(tf.int16, name='a')\n    y = tf.identity(a, name='y')\n\n    sess.run(y, feed_dict={a: 3})\n    sess.close()\n\n  del sess\n\n  time.sleep(20)\n\nprint time.clock() - t0\n```\n\nBut when the program is sleeping, it still occupies memory.\n\nAlso, could you provide the line that configure session to use `allow_growth`?\n\nI tried\n\n``` python\nimport time\nimport tensorflow as tf\n\nfor i in range(0,10000000):\n  t0 = time.clock()\n\n  with tf.Graph().as_default():\n    sess = tf.Session(config=tf.GPUOptions(allow_growth=True))\n\n    a = tf.placeholder(tf.int16, name='a')\n    y = tf.identity(a, name='y')\n\n    sess.run(y, feed_dict={a: 3})\n    sess.close()\n\n  del sess\n\n  time.sleep(20)\n\nprint time.clock() - t0\n```\n\nand got this error report\n\n``` bash\nTypeError: Expected config_pb2.ConfigProto, but got <class 'tensorflow.core.protobuf.config_pb2.GPUOptions'>\n```\n\nNote that this time I used a tensorflow compiled from source, since the 0.7.1 release does not have `allow_growth` option yet.\n", "```\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth=True\nsess = tf.Session(config=config)\n```\n", "Oh, I thought GPUOptions is parallel to ConfigProto according to the protobuf message. Thanks!\n\nWhat about the first problem? Normally deleting an object in python does not guarantee releasing memories, also the case here relates to GPU. It is up to tensorflow to decide what to do. Is that right? Or am I missing something?\n", "Not sure, you could try invoking python's garbage collector?\n", "Already tried before.\n\ngc.collect()\n\ndid not work.\n\u200b\n", "Well, I'm out of ideas :(.  Hopefully allow_growth is good enough for you. \n", "Thanks anyway. I switched to use sub shell call instead of multi-processing.\n\nUnfortunately, I cannot use allow_growth now. Version 0.7.1 and current master HEAD seem to have a serious performance problem in some ops, split, reshape or concat(about 20 times slower, I am sure I am using GPU). I fall back to 0.6.0 now. Do not have time to check where goes wrong yet.\n", "TensorFlow preallocates all the memory in self-managed pools. nvidia-smi does not show the pool use percentage, because only TensorFlow know that. You could try tensorboard, not sure if it shows the memory status.\n\nSee this:\nhttp://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n", "@vrv Hi, where to put these lines?\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\nsess = tf.Session(config=config)\r\n```", "hi @AliceDinh, \r\nif you are using tensorflow itself then while creating session you need to pass the configurations in `tf.Session()`.  \r\n And if you are using `keras` on top of `tensorflow` then you can use in following way..\r\n```import tensorflow as tf  \r\nfrom keras.backend.tensorflow_backend import set_session  \r\nconfig = tf.ConfigProto()  \r\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU  \r\nsess = tf.Session(config=config)  \r\nset_session(sess)  # set this TensorFlow session as the default session for Keras.\r\n``` \r\n", "In my case, i can not get keras.backend modul. so,\r\n\r\n`from tensorflow.compat.v1 import ConfigProto`\r\n`from tensorflow.compat.v1 import InteractiveSession`\r\n`config = ConfigProto()`\r\n`config.gpu_options.allow_growth = True`\r\n`session = InteractiveSession(config=config)`\r\n\r\nI don't have much experience with this, so let me know if I did it wrong."]}, {"number": 1577, "title": "TensorBoard: histogram charts sometimes duplicated", "body": "I unclicked run1 and run2, but when I click on some variables, the run1 and run2 still show up. If I click on run1 and run2 again, five plots will be shown, with 2 of them repeated! By unclicking and clicking a run1, my board just messed up with more and more plots. This is a Google product? :-O\n\nScreenshots:\n\n![image](https://cloud.githubusercontent.com/assets/5016962/13942946/3a633a78-efb5-11e5-9dec-795b306778b3.png)\n\n![image](https://cloud.githubusercontent.com/assets/5016962/13942974/8c51f4dc-efb5-11e5-9b14-3813f4110586.png)\n", "comments": ["@danmane: Do you have enough information to look at this? \n", "I'm planning to replace the histograms with entirely separate code soon(tm), so considering this obsolete/won't fix.\n"]}, {"number": 1576, "title": "Fix Skflow imports (esp in examples)", "body": "Prior to this, the imports in the examples were broken. \n\nIt does not appear to me that this could be resolved with changes only to `/contrib/skflow/__init__.py`\n\nI mentioned this in gitter to @ilblackdragon though he hasn't seen it before now.\n", "comments": ["Can one of the admins verify this patch?\n", "This issue is handled in [334702e19a920ac21fbbbf5b14f7619cb860c427](https://github.com/tensorflow/tensorflow/commit/334702e19a920ac21fbbbf5b14f7619cb860c427)\n"]}, {"number": 1575, "title": "da", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["mistake?\n"]}, {"number": 1574, "title": "build offline documentation, how to?", "body": "I've noticed the API doc in https://www.tensorflow.org/versions/master/api_docs/index.html is not complete. Is there a way to build a nice HTML based API _locally_ that contains a complete documentation?\n\nBasically the question is: how is this https://www.tensorflow.org/versions/master/api_docs/index.html being automatically generated? What's the magical command?\n", "comments": ["The gen_docs_combined script should build markdown, but I'm not sure about HTML.  @martinwicke?\n", "You cannot currently build html, but you can build the complete set of markdown, using [gen_docs.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docs/gen_docs.sh).\n", "Closing, please ping if you have trouble with gen_docs.sh.\n", "Seems like there's no more gen_docs.sh file. What's the proper way to build documentation?", "Cc @wolffg.", "See here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md", "Trying the \"Generating docs and previewing links\" section, I end up with this error:\r\n\r\n```\r\nINFO: Running command line: bazel-bin/tensorflow/tools/docs/generate '--src_dir=/Users/daze/PersonalGDrive/Hacks/ML/tensorflow/tensorflow/tensorflow/docs_src/' '--output_dir=/tmp/tfdocs/'\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\nWriting docs for tf.abs (<function abs at 0x10b19fd08>).\r\nWriting docs for tf.contrib.learn.monitors (<module 'tensorflow.contrib.learn.python.learn.monitors' from '/private/var/tmp/_bazel_daze/dbc852bfd1d7b33406bd81c3c9000a9d/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/monitors.py'>).\r\nWriting docs for tf.estimator.Estimator (<class 'tensorflow.python.estimator.estimator.Estimator'>).\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_daze/dbc852bfd1d7b33406bd81c3c9000a9d/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/generate.py\", line 49, in <module>\r\n    sys.exit(doc_generator.build(flags))\r\n  File \"/private/var/tmp/_bazel_daze/dbc852bfd1d7b33406bd81c3c9000a9d/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/generate_lib.py\", line 473, in build\r\n    write_docs(output_dir, parser_config, yaml_toc=self.yaml_toc)\r\n  File \"/private/var/tmp/_bazel_daze/dbc852bfd1d7b33406bd81c3c9000a9d/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/generate_lib.py\", line 127, in write_docs\r\n    f.write(pretty_docs.build_md_page(page_info))\r\n  File \"/private/var/tmp/_bazel_daze/dbc852bfd1d7b33406bd81c3c9000a9d/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/pretty_docs.py\", line 39, in build_md_page\r\n    return _build_class_page(page_info)\r\n  File \"/private/var/tmp/_bazel_daze/dbc852bfd1d7b33406bd81c3c9000a9d/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate.runfiles/org_tensorflow/tensorflow/tools/docs/pretty_docs.py\", line 126, in _build_class_page\r\n    parts.append(h3.format(**method_info.__dict__))\r\nAttributeError: '_MethodInfo' object has no attribute '__dict__'\r\nERROR: Non-zero return code '1' from command: Process exited with status 1.\r\n```\r\n\r\nIs this still the official answer? Thanks in advance for the help.", "It looks like you are running this in python 3. I have not converted the doc generator to be compatible with python3. Some of these issues would be easy to fix, but that would only lead to you finding more issues. Since inspect is seriously different and codegen is very old, it may be a bit of an endeavor. I'd welcome anyone who wants to take this on, but it's not a priority for us at the moment, we use py2 to generate the docs.", "Thanks, using Python 2 makes more progress.\r\n\r\nIssue 1:\r\nBut the output still isn't error-free. [gist with errors](https://gist.github.com/dmonopoly/b67b5ba35748a25bdc02130a54ca18d6).\r\n\r\nIssue 2:\r\nIf the above is fine output, and I can indeed look at the generated results, then...\r\nAre we sure that this generator generates docs the same way as they appear on say https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Evaluable ?\r\n\r\nWhile looking into #9313, I generated the docs as described at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md via\r\n\r\n```\r\nbazel run tools/docs:generate -- \\\r\n          --src_dir=`pwd`/tensorflow/docs_src/ \\\r\n          --output_dir=/tmp/tfdocs/\r\n```\r\n\r\nbut the output markdown for this file does not have the same problem as featured on that site.\r\n\r\nThe [code in the master branch](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/evaluable.py#L88) and the [code in r1.1](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/learn/python/learn/evaluable.py#L88) are also the same there, so it's not a versioning difference.\r\n\r\nThanks in advance - hope I just made a trivial oversight", "@MarkDaoust for expertise on the doc generation scripts. \r\n\r\nIt is surprising that so many doc references are not found. You appear to be giving the correct doc_src directory with guides. \r\n\r\nMark, is there a problem with the documentation documentation?", "It looks like you've got one too many `tensorflow` in your `src_dir` path.\r\n\r\nIt turns out that `os.walk` doesn't throw an error for a non-existant directory.\r\n\r\nFixes are in the pipeline for that example line of code, and adding clear error-messages to the doc-generator."]}, {"number": 1573, "title": "Add Tan, Asin, Acos, Atan trigonometric functions. See #1108", "body": "This branch was previously in https://github.com/tensorflow/tensorflow/pull/1124 but for some reason (most likely a bad force push on my repo) was permanently closed. \n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n", "Looks like one small test failure for tan -- can you fix and verify?  Otherwise looking good!\n", "@Mistobaan -- I see other people requesting it, this would be a useful addition\n", "Can one of the admins verify this patch?\n", "Can you fix cwise_op_test failures?\n\n```\nINFO: From Testing //tensorflow/python:cwise_ops_test (shard 2 of 2):\n==================== Test output for //tensorflow/python:cwise_ops_test (shard 2 of 2):\n.............................................................F...\n======================================================================\nFAIL: testComplex64Basic (__main__.UnaryOpTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-fastbuild/bin/tensorflow/python/cwise_ops_test.runfiles/tensorflow/python/kernel_tests/cwise_ops_test.py\", line 250, in testComplex64Basic\n    self._compareCpu(x, np.tan, tf.tan)\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-fastbuild/bin/tensorflow/python/cwise_ops_test.runfiles/tensorflow/python/kernel_tests/cwise_ops_test.py\", line 75, in _compareCpu\n    self.assertAllClose(jacob_t, jacob_n, rtol=1e-3, atol=1e-3)\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-fastbuild/bin/tensorflow/python/cwise_ops_test.runfiles/tensorflow/python/framework/test_util.py\", line 431, in assertAllClose\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\n  File \"/usr/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 1183, in assert_allclose\n    verbose=verbose, header=header)\n  File \"/usr/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 644, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nNot equal to tolerance rtol=0.001, atol=0.001\n\n(mismatch 100.0%)\n x: array([[ 0.00947874,  0.00274417,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ],...\n y: array([[ 0.00947807, -0.00274181,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ],...\n\n----------------------------------------------------------------------\nRan 65 tests in 197.312s\n```\n", "It's a strange failure, let's retry the test.\n\n@tensorflow-jenkins can you test this please?\n", "@vrv @ebrevdo \n\nI spent quite a bit of time but I couldn't figure out why the complex64 atan function was failing. As you see from the output  seems that the problem is a sign in the second element. \nTo move this patch forward and prevent from falling behind master I removed the complex64 support from atan for now. What do you think ? \n", "ping\n", "the branch currently has conflicts, and yes, removing complex64 support for now is fine -- just leave a comment saying something was broken when you tried it, so others can pick up from where you left off.\n", "@vrv rebased \n", "@ebrevdo @yaroslavvb can one of you run the tests on this patch and see if we can merge it ? \n", "@tensorflow-jenkins, test this please.\n", "@mrry  did the test run ? \n", "@Mistobaan: ...apparently not. @caisq, @ebrevdo, @martinwicke: can one of you please kick off the tests?\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "Still seeing errors for complex64, and also for float16.\n", "Search the [console log](http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/890/console) for the string `ERROR:`\n", "If you can, please fix the tests and make sure you verify they are fixed via bazel test tensorflow/python:cwise_ops_test\n", "Yes I am trying to test on my local machine, but something has changed that is failing with different errors like: `undefined symbol: _ZN10tensorflow7functor12UnaryFunctorIN5Eigen9GpuDeviceENS0_4asinIdEEEclERKS3_NS2_9TensorMapINS2_6TensorIdLi1ELi1ElEELi16EEENS9_INSA_IKdLi1ELi1ElEELi16EEE`\n", "Probably becaues you aren't compiling any GPU code.\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_gpu_abs.cu.cc -- no equivalent file exists\n", "The updated patch is compiling and passing all the tests with cuda on my local machine.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 1572, "title": "Fixing a python3 issue in bias_op_test.py", "body": "Issue 1: range in python3 does not return a list as in python2\n\nNote that this PR doesn't fix all the currently failing Python3+GPU tests. It only gets the basic issues out of the way. Deeper issues related to dimension mismatches in bias_op_test, conv_op_test and pooling_op_test need to be addressed. \n", "comments": ["Looks like a flaky test?  Should we merge?\n", "@tensorflow-jenkins test this please\n", "Yes, that was a flaky test: //tensorflow/python:coordinator_test\n\nReady to merge.\n"]}, {"number": 1571, "title": "Push changes from internal", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}]