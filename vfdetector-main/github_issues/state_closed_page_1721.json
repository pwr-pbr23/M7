[{"number": 1270, "title": "User ops documentation not sufficient", "body": "I followed the documentation for building a user ops (https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html), \ntrying the zero_out op, there are a couple of places where the doc is not clear and also after hacking my way around it the op failed to work.\n\nI am following the instructions for adding an op with Tensorflow source installation. \nHere are the questions:\n1- Where should the bazel build rule be placed? \n2- How to exactly use bazel to build the op?\n\nI appreciate if you can clarify the documentation, it will help a lot.\n", "comments": ["@keveman, @josh11b \n", "The documentation is actually not correct. You have to add the line\n#include \"tensorflow/core/framework/op_kernel.h\"\nto your zero_op.cc in order for it to build (this line was in the original 0.6 release user operator tutorial). \nHowever including this line will not work as stated if you try to use the binary installation, as this file includes \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\" which is not in the /usr/local/lib/python2.7/dist-packages/tensorflow/include directory. \n\ng++ -std=c++11 -shared zero_out.cc -o zero_out.so \\\n -I $TF_INC -l tensorflow_framework -L $TF_LIB \\\n -fPIC -Wl,-rpath $TF_LIB\nIn file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/type_traits.h:22:0,\n                 from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:25,\n                 from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:22,\n                 from zero_out.cc:10:\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:23:61: fatal error: third_party/eigen3/unsupported/Eigen/CXX11/Tensor: No such file or directory\n #include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n\nIt is not enough to just install eigen externally because the tensorflow include files hard-code the \"third_party\" path. Those files then include the \"real\" eigen files. I had to first download eigen-eigen-ed4c9730b545 and then download the full tensorflow source and copy the third_party directory and then put both eigen-eigen-ed4c9730b545 path and the path to my third_party directory in my build command as -I flags. This kind of defeats the goal of being able to create user operators with only the binary installation. \n\nWith all the include paths set up, I was able to run the zero_out op from python, but it generates a segfault after the op is run. Exit code is 139, which is an invalid memory access. Note, I am running the gpu version of the pip package, but the zero_out op is running on my cpu as it has no gpu operator registered.\n\n$ cat zero_out_test.py \nimport tensorflow as tf\nzero_out_module = tf.load_op_library('/home/kbrems/zeroout/libzero_out.so')\nwith tf.Session(''):\n  result = zero_out_module.zero_out([[1, 2], [3, 4]]).eval()\n  print result\n\n$ python zero_out_test.py \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.51GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\n...\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB\n[[1 0]\n [0 0]]\nSegmentation fault (core dumped)\n\nBeing able to create new operators without having to fork tensorflow and bury them in the user_ops directory and build all of tensorflow is a very useful feature. It seems though that it is not yet completely implemented. If I put this same zero_out.cc file into the tensorflow/core/user_ops directory and build all of tensorflow and run the test using the bazel test framework, putting the test in tensorflow/python/kernel_tests/zero_out_op_test.py, it does work. \n", "mark.\nAlso have the same question!\n", "There is only one 'tensorflow' folder in the 'include' folder. When I add a user op in it, it fails. Hope this bug will soon be fixed. At least the docs should be edited in order to not mislead more people.\n", "Sorry everyone about the breakage. Some changes got inadvertently pulled in that removed Eigen and third_party header files from the pip package. I added them back, and also clarified the documentation a bit. The 0.7.1 pip package still has the problem, but if you create the pip package from HEAD and install it, that should work.\n", "I built the pip package from master HEAD today and installed it. It now seems that the tensorflow_framework library is not being installed (or is not being installed or named the way the instructions say it is). Following the instructions on the tensorflow \"Adding a New Op\" -> \"Building the Op library\" -> \"With TensorFlow binary installation\"\n\n$ set | grep TF_\nTF_INC=/usr/local/lib/python2.7/dist-packages/tensorflow/include\nTF_LIB=/usr/local/lib/python2.7/dist-packages/tensorflow/core\n\n$ g++ -std=c++11 -shared zero_out.cc -o zero_out.so \\\n\n> -I $TF_INC -l tensorflow_framework -L $TF_LIB \\\n> -fPIC -Wl,-rpath $TF_LIB\n> /usr/bin/ld: cannot find -ltensorflow_framework\n> collect2: error: ld returned 1 exit status\n\n$ ls -l /usr/local/lib/python2.7/dist-packages/tensorflow/core\ntotal 24\ndrwxr-sr-x 2 root staff 4096 Mar 17 14:41 example\ndrwxr-sr-x 2 root staff 4096 Mar 17 14:41 framework\n-rw-r--r-- 1 root staff    0 Mar 17 14:41 **init**.py\n-rw-r--r-- 1 root staff  147 Mar 17 14:41 **init**.pyc\ndrwxr-sr-x 3 root staff 4096 Mar 17 14:41 lib\ndrwxr-sr-x 2 root staff 4096 Mar 17 14:41 protobuf\ndrwxr-sr-x 2 root staff 4096 Mar 17 14:41 util\n\n$ ls -l /usr/local/lib/python2.7/dist-packages/tensorflow/core/lib\ntotal 8\ndrwxr-sr-x 2 root staff 4096 Mar 17 14:41 core\n-rw-r--r-- 1 root staff    0 Mar 17 14:41 **init**.py\n-rw-r--r-- 1 root staff  151 Mar 17 14:41 **init**.pyc\n\n$ ls -l /usr/local/lib/python2.7/dist-packages/tensorflow/core/lib/core\ntotal 16\n-rw-r--r-- 1 root staff 4848 Mar 17 14:41 error_codes_pb2.py\n-rw-r--r-- 1 root staff 3960 Mar 17 14:41 error_codes_pb2.pyc\n-rw-r--r-- 1 root staff    0 Mar 17 14:41 **init**.py\n-rw-r--r-- 1 root staff  156 Mar 17 14:41 **init**.pyc\n", "The website is lagging behind the documentation from HEAD. Can you please follow the instructions here : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/adding_an_op/index.md\n"]}, {"number": 1269, "title": "Loading the newer inception model in Android demo example and No OpKernel was registered to support Op error", "body": "### Steps to reproduce\n1. Load the model in the asset folder from here: http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n   rather than here: https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\n   This step loads the model which is used in image_retrain, label_image and classify examples. The link for the model in the android demo is old and has some differences that don't work with the other demos as mentioned in #1253 \n2. Change the input_width and input_mean in TensorflowImageListener.java\n3. Change the input tensor name and output tensor name as follows in the tensorflow_jni.cc\n### What have you tried?\n1. I changed the input_width from 224 to 299 and input_mean from 117 to 128.\n2. When the input tensor name was **input:0** and output tensor name was **output:0** I got the following node not found error\n\n`02-23 23:56:11.066 15084-15105/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:271 Width: 299\n02-23 23:56:11.066 15084-15105/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:272 Stride: 1196\n02-23 23:56:11.066 15084-15105/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:176 Tensorflow: Copying Data.\n02-23 23:56:11.070 15084-15105/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:194 Start computing.\n02-23 23:56:11.157 15084-15105/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:205 End computing. Ran in 86ms (91ms avg over 7 runs)\n02-23 23:56:11.157 15084-15105/org.tensorflow.demo E/native: tensorflow/examples/android/jni/tensorflow_jni.cc:210 Error during inference: Not found: FetchOutputs node output:0: not found`\n1. When the input tensor name was **Mul:0** and output tensor name was **softmax:0** I got the following error\n\n`02-24 00:07:46.149 23093-23123/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:270 Height: 299\n02-24 00:07:46.149 23093-23123/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:271 Width: 299\n02-24 00:07:46.149 23093-23123/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:272 Stride: 1196\n02-24 00:07:46.149 23093-23123/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:176 Tensorflow: Copying Data.\n02-24 00:07:46.152 23093-23123/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:194 Start computing.\n02-24 00:07:46.222 23093-23123/org.tensorflow.demo I/native: tensorflow/examples/android/jni/tensorflow_jni.cc:205 End computing. Ran in 70ms (77ms avg over 35 runs)\n02-24 00:07:46.222 23093-23123/org.tensorflow.demo E/native: tensorflow/examples/android/jni/tensorflow_jni.cc:210 Error during inference: Invalid argument: No OpKernel was registered to support Op 'BatchNormWithGlobalNormalization' with these attrs\n                                                                 [[Node: conv/batchnorm = BatchNormWithGlobalNormalization[T=DT_FLOAT, scale_after_normalization=false, variance_epsilon=0.001](conv/Conv2D, conv/batchnorm/moving_mean, conv/batchnorm/moving_variance, conv/batchnorm/beta, conv/batchnorm/gamma)]]\n`\n\nThat's good news that the android demo code is finding the nodes in the new model after the changes. However, I have no idea what this error means. I would really appreciate if someone can explain. (Although this issue is being taken care of in #1253, I am still doing it myself in an attempt to learn more about tensorflow)\n`No OpKernel was registered to support Op 'BatchNormWithGlobalNormalization' with these attrs\n                                                                 [[Node: conv/batchnorm = BatchNormWithGlobalNormalization[T=DT_FLOAT, scale_after_normalization=false, variance_epsilon=0.001](conv/Conv2D, conv/batchnorm/moving_mean, conv/batchnorm/moving_variance, conv/batchnorm/beta, conv/batchnorm/gamma)]]`\n\nLooking at the main.cc in the label_image example, I see something about \"normalized\" in the ReadTensorFromImage function. Don't know if that's helpful information. \n", "comments": ["batch_norm_op.cc is not included in the default Android operator sets. You can add it in to one of the filegroups in the kernels BUILD file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/BUILD#L987\n", "@andrewharp: shouldn't we do this?\n", "Sure, I don't see any reason it shouldn't be included, just assumed it was left out on purpose.\n", "@andrewharp @vrv Hi Guys, thank you for the commits. I have been keeping up to date with it. I am now able to load the newer inception model and even load a retrained inception model on my custom classes into the android demo! \n\nHere are some things I encountered when making the android demo work with the newer inception model/retrained model. The android kernel thing requires the check_numerics_op.cc. I found this error after rebuilding @andrewharp 's commit on the batch_norm_op. So I added the check_numerics_op.cc like @andrewharp did with batch_norm_op.\n\nThe second thing I had to do was what is mentioned in #582 . I guess this is the protobuf big size handling thing you guys are working on.\n\nLastly I modified the input_width and input_mean and the app started recognizing. \n", "Thanks for the update @syed-ahmed -- will add check_numeric_op.cc to the extended kernel filegroup as well.\n", "hi @andrewharp , I did a pull request for the check_numeric_op.cc and @vrv merged it. so all's good. :)\n", "@syed-ahmed @andrewharp  please can you advise what changes have you done to make it work\n1 - I changed the input_witdh and input_mean\n2 - applied the workaround mentioned in #582 \n\nwhat else should I change for the names of the input and output layer\n\nThank You\n", "@islamoc Hi, I did the following (you'll find some steps you already did, so ignore them :) )\n- Get the latest tensorflow from git hub and build it using bazel.\n- Find the coded_stream.h in the google/protobuf section of your tensorflow build and modify the 64 to 256 in the following line:\n\n`static const int kDefaultTotalBytesLimit = 64 << 20;  // Change the 64 to 256 MB`\n- Rebuild the tensorflow\n- Download the newer inception model from https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015.zip\n\nNote: The .pb file in the zip is the same as the classify_image_graph_def used in some other example. Only the name is different. It is the same name as in the original android demo but you will notice the filesize is bigger, reflecting that it is the newer model. Also you'll find in the zip file that the label file is different than the imagenet_synset_to_human_label_map.txt. It is because of the way the android demo is reading the labels vs how the the label_image example is reading them. So don't use the imagenet_synset_to_human_label_map in the android demo.\n- Modify only the input_size to 299 and the image_mean to 128 in the TensorflowImageListener.java\n- Go to tensorflow_jni.cc in the android demo and modify as follows:\n\n```\n\n      input_tensor_mapped(0, i, j, 0) =\n          (static_cast<float>(src->red) - g_image_mean)/g_image_mean;\n      input_tensor_mapped(0, i, j, 1) =\n          (static_cast<float>(src->green) - g_image_mean)/g_image_mean;\n      input_tensor_mapped(0, i, j, 2) =\n          (static_cast<float>(src->blue) - g_image_mean)/g_image_mean;\n      ++src;\n\nstd::vector<std::pair<std::string, tensorflow::Tensor> > input_tensors(\n      {{\"Mul\", input_tensor}});\n\nstd::vector<std::string> output_names({\"softmax\"});\n```\n- Build the android demo\n\nThis should let you use the android demo with the newest inception model. If you want to use a retrained model. Just change the model name and label in the android demo and change the output_names({\"softmax\"}) to output_names({\"final_result\"})\n", "@syed-ahmed Hi, what have you done to solve the \" Invalid argument: No OpKernel was registered to support Op 'BatchNormWithGlobalNormalization' with these attrs ...\" problem?\n\nI also encounter a similar problem when using a self-trained face-recognition model to make inference on android platform. The error says something like this:\n\n```\n\n06-05 16:25:11.322 28605-28605/jp.narr.tensorflowmnist I/native: tensorflow_jni.cc:196 End computing.\n06-05 16:25:11.322 28605-28605/jp.narr.tensorflowmnist E/native: tensorflow_jni.cc:199 Error during inference: Invalid argument: No OpKernel was registered to support Op 'Inv' with these attrs\n                                                                      [[Node: incept5b/in4_conv1x1_55/batch_norm/moments/moments_1/divisor = Inv[T=DT_FLOAT](incept5b/in4_conv1x1_55/batch_norm/moments/moments/Const)]]\n06-05 16:25:11.322 28605-28605/jp.narr.tensorflowmnist A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x10 in tid 28605 (tensorflowmnist)\n06-05 16:25:11.423 186-186/? I/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** \n```\n\nCould you give me some advice?\nThanks a lot!\n\nThe structure of the model is like this:\n\n```\n\ndef inference_nn4_max_pool_96(images, pool_type, use_lrn, keep_probability, phase_train=True):\n  conv1 = _conv(images, 3, 64, 7, 7, 2, 2, 'SAME', 'conv1_7x7', phase_train=phase_train, use_batch_norm=True)\n  pool1 = _mpool(conv1,  3, 3, 2, 2, 'SAME')\n  if use_lrn:\n    lrn1 = tf.nn.local_response_normalization(pool1, depth_radius=5, bias=1.0, alpha=1e-4, beta=0.75)\n  else:\n    lrn1 = pool1\n  conv2 = _conv(lrn1,  64, 64, 1, 1, 1, 1, 'SAME', 'conv2_1x1', phase_train=phase_train, use_batch_norm=True)\n  conv3 = _conv(conv2,  64, 192, 3, 3, 1, 1, 'SAME', 'conv3_3x3', phase_train=phase_train, use_batch_norm=True)\n  if use_lrn:\n    lrn2 = tf.nn.local_response_normalization(conv3, depth_radius=5, bias=1.0, alpha=1e-4, beta=0.75)\n  else:\n    lrn2 = conv3\n  pool3 = _mpool(lrn2,  3, 3, 2, 2, 'SAME')\n\n  incept3a = _inception(pool3,    192, 1, 64, 96, 128, 16, 32, 3, 32, 1, 'MAX', 'incept3a', phase_train=phase_train, use_batch_norm=True)\n  incept3b = _inception(incept3a, 256, 1, 64, 96, 128, 32, 64, 3, 64, 1, pool_type, 'incept3b', phase_train=phase_train, use_batch_norm=True)\n  incept3c = _inception(incept3b, 320, 2, 0, 128, 256, 32, 64, 3, 0, 2, 'MAX', 'incept3c', phase_train=phase_train, use_batch_norm=True)\n\n  incept4a = _inception(incept3c, 640, 1, 256, 96, 192, 32, 64, 3, 128, 1, pool_type, 'incept4a', phase_train=phase_train, use_batch_norm=True)\n  incept4b = _inception(incept4a, 640, 1, 224, 112, 224, 32, 64, 3, 128, 1, pool_type, 'incept4b', phase_train=phase_train, use_batch_norm=True)\n  incept4c = _inception(incept4b, 640, 1, 192, 128, 256, 32, 64, 3, 128, 1, pool_type, 'incept4c', phase_train=phase_train, use_batch_norm=True)\n  incept4d = _inception(incept4c, 640, 1, 160, 144, 288, 32, 64, 3, 128, 1, pool_type, 'incept4d', phase_train=phase_train, use_batch_norm=True)\n  incept4e = _inception(incept4d, 640, 2, 0, 160, 256, 64, 128, 3, 0, 2, 'MAX', 'incept4e', phase_train=phase_train, use_batch_norm=True)\n\n  incept5a = _inception(incept4e,    1024, 1, 384, 192, 384, 0, 0, 3, 128, 1, pool_type, 'incept5a', phase_train=phase_train, use_batch_norm=True)\n  incept5b = _inception(incept5a, 896, 1, 384, 192, 384, 0, 0, 3, 128, 1, 'MAX', 'incept5b', phase_train=phase_train, use_batch_norm=True)\n  pool6 = _apool(incept5b,  3, 3, 1, 1, 'VALID')\n\n  resh1 = tf.reshape(pool6, [-1, 896])\n  affn1 = _affine(resh1, 896, 128)\n  if keep_probability<1.0:\n    affn1 = control_flow_ops.cond(phase_train,\n                                  lambda: tf.nn.dropout(affn1, keep_probability), lambda: affn1)\n  norm = tf.nn.l2_normalize(affn1, 1, 1e-10, name='embeddings')\n\n  return norm\n```\n", "@TianweiXing Hi, the \" Invalid argument: No OpKernel was registered to support Op 'BatchNormWithGlobalNormalization' with these attrs ...\" was caused by not having batch_norm_op.cc in the android build process - tensorflow/tensorflow/core/kernels/BUILD . There was another similar error which was caused by not having check_numerics_op.cc. So these errors were fixed by adding a line (\"batch_norm_op.cc, check_numerics_op.cc\") in the \"android_extended_ops_group1\" section of the tensorflow/tensorflow/core/kernels/BUILD file. \n\nNow looking at your error, I found out that the \"Inv\" op is registered in the math_op.cc file located here: tensorflow/tensorflow/core/ops/ and it is absent in the android ops section of the BUILD file. So you could try and add the line math_op.cc in the \"android_extended_ops_group1\" section of the build. However, I am not sure if that would solve the problem seeing that the BUILD file has some sort of math imports (if you just search the word \"math\") already and I would have expected the \"Inv\" operation to work. Also the math_op.cc lies in a different folder (/core/ops) than (/core/kernels/). Hence, I'm doubtful that what I did is gonna work for you as well, but you could give it a try :) \n", "Hello,\nI followed the steps listed above, but I'm facing this issue:\n\n06-06 12:41:59.923 24534 24552 E native  : tensorflow_jni.cc:303 Error during inference: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs\n06-06 12:41:59.923 24534 24552 E native  :   [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]\n\nDoes anyone faced this before?\n", "Hi Mateo, if you check the BUILD at tensorflow/tensorflow/core/kernels/BUILD, you'll see the decode_jpeg_op.cc is excluded from the android filegroup for the following reason documented in the file: \n\n\"# Ops which we are currently excluding because they are likely\"\n\n\"# not used on Android. Those ops also do not compile if included,\"\n\n\"# unless we add the additional deps they need.\"\n\nSo as suggested from the comment, they don't compile when included. However you could try and remove the decode_jpeg_op from the exclude section and find the dependencies.\n", "Hi, I tried including the op, unfortunately it doesn't build due duplicate libraries being added to the build.\nThe other thing I tried was to strip_unused script in python/tools, but that script when run shows me protobuf errors, any ideas how to get around the no op support for DecodeJpeg\n\n  name = tokenizer.ConsumeIdentifier()\n  File \"/Users/Jz/tensorflow/lib/python2.7/site-packages/google/protobuf/text_format.py\", line 764, in ConsumeIdentifier\n    raise self._ParseError('Expected identifier.')\ngoogle.protobuf.text_format.ParseError: 2:1 : Expected identifier.\n", "@mattpoggi i have the same error - did you find a solution?\n", "Nope, I tried to remove the op from the exclude list, but I'm now I'm facing more build errors. I'll try again in the weekend.\n", "@mattpoggi please let me know if you find a solution - i will also do my best!\n", "@mattpoggi @entttom Try running [strip_unused](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/strip_unused.py) on the graph to remove the unsupported jpeg decoding layers. For inception v3 I believe the input you'll actually want to use is \"Mul:0\".\n\nYou may need to also pass in --input_binary=true depending on the format of your graph.\n", "Hi, i have the same issue with a retrained v3 model\n\n06-20 15:00:28.597 3137-3162/org.tensorflow.demo A/native: tensorflow_jni.cc:316 Error during inference: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs\n                                                             [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]\n- i changed the things \"@syed-ahmed commented on 21 Mar\" mentioned,\n- i tried stripping away this DecodeJpeg Layers using the \"strip_unused.py\", but nothing changed\n\n> python strip_unused.py --input_graph=\"graphfile from inception retrain\" --output_graph=\"new graph name\" --input_node_names=\"Mul:0\" --output_node_names=\"final_result\" --input_binary=true\n\nthen i copy and rename the new graph.pb + labels.txt to the examples/android/assets folder\n\nthe model works fine on desktop by testing it with \"label_image\", but i cannot load this model in the android demo app without getting the 'DecodeJpeg' Error\n\ni hope someone has some hints how i can solve this problem.\n", "Hi,\n\nI have the same problem as @Shaikjalal \n\nAnyone know what to do in this case?\n\nThanks\n", "@dmirk I'm not sure that label_image will work if the decodejpeg op has been stripped out of the graph. Similarly it won't work on android unless you have stripped it out, so the symptoms point to it not having been removed. Can you double check that you're using the right output graph in both places please?\n\n@natanielruiz You should be able to use strip_unused like so to remove the DecodeJpeg node:\n\n```\nbazel build tensorflow/python/tools:strip_unused\nbazel-bin/tensorflow/python/tools/strip_unused --input_graph=inception.pb --output_graph=/tmp/stripped_inception.pb --input_node_names=\"Mul\" --output_node_names=\"final_result\" --input_binary=true\n```\n", "My stripped retrained model works by following @andrewharp 's suggestion and modifying some source code.\n\nIf you got the error messages with your retrained model below:\n\n```\nF/native  (10551): tensorflow_jni.cc:304 Error during inference: Not found: FeedInputs: unable to find feed output input:0\nF/libc    (10551): Fatal signal 6 (SIGABRT), code -6 in tid 10570 (InferenceThread)\n```\n\nYou also need to modify the class `TensorflowImageListener` in `TensorflowImageListener.java` by following the suggestion in its comment (change the values of `IMAGE_SIZE`, `IMAGE_MEAN`, `IMAGE_STD`, `INPUT_NAME`, and `OUTPUT_NAME`).\n", "@bafu \nWhat steps did you exactly did to get it working? I changed the code in \"TensorflowImageListener\" and \"coded_stream.h\", do i miss anything?\n\n@andrewharp \ni double checked that the right output graphs are used in the right place. Here is a link to my full shell script performing all the tasks\n[http://pastebin.com/NVbUJ3Ue](http://pastebin.com/NVbUJ3Ue)\n\nIt seems the strip_unused doesn't strip anything out of my graph. If i do the following steps:\n\n`OUT=$IMAGEDIR/imagenet-out/output_graph.pb`\n`STRIPPED=$IMAGEDIR/imagenet-out/output_graph_stripped.pb`\n`LABELS=$IMAGEDIR/imagenet-out/output_labels.txt`\n`INNAMES=\"Mul:0\"`\n`OUTNAMES=\"final_result\"``\n\n`bazel-bin/tensorflow/python/tools/strip_unused \\\n--input_graph=$OUT \\\n--output_graph=$STRIPPED \\\n--input_node_names=$INNAMES \\\n--output_node_names=$OUTNAMES \\\n--input_binary=true`\n\n`echo \"- \"$OUT\" - size: \"$( stat -c %s $OUT)`\n`echo \"- \"$STRIPPED\" - size: \"$( stat -c %s $STRIPPED)`\n\n`bazel-bin/tensorflow/examples/label_image/label_image \\\n--graph=$OUT \\\n--labels=$LABELS \\\n--output_layer=$OUTNAMES \\\n--image=$TEST`\n\n`bazel-bin/tensorflow/examples/label_image/label_image \\\n--graph=$STRIPPED \\\n--labels=$LABELS \\\n--output_layer=$OUTNAMES \\\n--image=$TEST`\n` `\nproduces following output for the newly trained classes (reduced train_steps to 300 for testing):\n\n> -- Strip graph\n> 1007 ops in the final graph.\n> - /home/robimo/images/imagenet-out/output_graph.pb - size: 87497129\n> - /home/robimo/images/imagenet-out/output_graph_stripped.pb - size: 87497129\n> \n> -- Test /home/robimo/images/imagenet/train/fordButtonOnOff/img4.jpg on full graph\n> W tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\n> I tensorflow/examples/label_image/main.cc:207] fordbuttononoff (4): 0.580274\n> I tensorflow/examples/label_image/main.cc:207] fordbutton01234 (0): 0.10882\n> I tensorflow/examples/label_image/main.cc:207] fordbutton1 (9): 0.100351\n> I tensorflow/examples/label_image/main.cc:207] fordbuttonvolseek (3): 0.0679065\n> I tensorflow/examples/label_image/main.cc:207] fordbutton3 (7): 0.0511736\n> \n> -- Test /home/robimo/images/imagenet/train/fordButtonOnOff/img4.jpg on stripped graph\n> W tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\n> I tensorflow/examples/label_image/main.cc:207] fordbuttononoff (4): 0.580274\n> I tensorflow/examples/label_image/main.cc:207] fordbutton01234 (0): 0.10882\n> I tensorflow/examples/label_image/main.cc:207] fordbutton1 (9): 0.100351\n> I tensorflow/examples/label_image/main.cc:207] fordbuttonvolseek (3): 0.0679065\n> I tensorflow/examples/label_image/main.cc:207] fordbutton3 (7): 0.0511736\n\nBoth the stripped and unstripped graph have identical file sizes, and both label_image runs are succesfull.\nWhat i am doing wrong?\n", "Hello, I was able to use the strip_graph script finally but I have exactly the same problem as @dmirk  It just seems that the script doesn't strip the JPEGDecoder.\n", "I think i got it working by modifying the for loop at line 93 in [strip_unused.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/strip_unused.py). \nIts just quick and dirty but it seems to work.\nAs the DecodeJpeg Node assumes to be passed as unit8 we cannot simply add the DecodeJpeg as additional input_node_names\n\n```\nfor node in input_graph_def.node:\n    if node.name in input_node_names_list or \"jpeg\" in node.op.lower():\n      placeholder_node = tf.NodeDef()\n      placeholder_node.op = \"Placeholder\"\n      placeholder_node.name = node.name\n      if \"jpeg\" in node.op.lower():\n        placeholder_node.attr[\"dtype\"].CopyFrom(tf.AttrValue(\n            type=tf.uint8.as_datatype_enum))\n      else:\n        placeholder_node.attr[\"dtype\"].CopyFrom(tf.AttrValue(\n            type=placeholder_type_enum))\n      inputs_replaced_graph_def.node.extend([placeholder_node])\n    else:\n      inputs_replaced_graph_def.node.extend([copy.deepcopy(node)])\n```\n\nI will test it in the next days\n", "@dmirk I tested your solution and the DecodeJpeg Node is succesfully removed. The final graph has 1006 ops and when you use the graph in the tensorflow demo app it build and runs without crashing. Pretty cool !\n", "@dmirk Here are the steps how I build the Android demo project with a retrained model:\n1. Retrained an Inception v3 model by `tensorflow/examples/image_retraining/retrain.py`.\n2. Go to the root directory of the Android example (`tensorflow/examples/android`).\n3. Put the retrained model and its label list into the `assets` directory \n4. Strip the retrained model by the commend:\n   \n   ```\n   python strip_unused.py\n   --input_graph=<retrained-pb-file> \\\n   --output_graph=<your-stripped-pb-file> \\\n   --input_node_names=\"Mul\" \\\n   --output_node_names=\"final_result\" \\\n   --input_binary=true\n   ```\n5. Modify `src/org/tensorflow/demo/TensorflowImageListener.java` by modifying these variables:\n   \n   ```\n   # in class TensorflowImageListener\n   private static final int INPUT_SIZE = 299;\n   private static final int IMAGE_MEAN = 128;\n   private static final float IMAGE_STD = 128;\n   private static final String INPUT_NAME = \"Mul:0\";\n   private static final String OUTPUT_NAME = \"final_result:0\";\n   ```\n6. Build the project.\n", "@syed-ahmed hello, I'm following your steps to modified the android demo in order to use the retrained model. I modified parameters  (img size, mean, file name etc) except the protobuf thing. I changed the 64 to 256 in the coded_stream.h files which are in /usr/local/lib/python folder (is it correct? and should I compile it after modified ?). That would be helpful if you can give me some suggestions. thanks.\n", "@sek550c\nhttps://github.com/tensorflow/tensorflow/commit/23fe378520e77bbeb8f47d00eac6737a8ec36d0f should have made the protobuf modification unnecessary by adding a separate codepath for loading protos > 64mb.\n", "@andrewharp thanks for your reply. I check the file, it seems not the problem. I download the newest tensorflow and I think it is not the protobuf problem. I can build and run the android demo but cannot modified it to use v3 model. I modified parameters as the comments state in TensorflowImageLinsener.java, and I can build the apk but the app crash after running. \nI use SDK 24.0.1 and NDK r12b. I am stuck here for a few days, any suggestions would be helpful. Thanks.\n", "@sek550c Can you paste the `adb logcat` of the crash?\n", "@andrewharp thanks for your reply. I found it is the Op 'DecodeJpeg' problem as mentioned above. I follow @dmirk method to modify the strip_unused.py and finally solve the Op 'DecodeJpeg' problem and run the app successfully. But it seems the label is not correct (displaying 'prediction 0, 1, 2 ... ' rather than 'tulips', 'roses', ...) with the retrained flower model when app runs (update: the reason is the wrong label file name in the LABEL_FILE string). I will work on that the next day. Finally, thank @andrewharp @dmirk and other participants in this post.\n", "I just noticed that the `:0`s in my example strip_unused command above were unnecessary and may contribute to other issues noted here (I did not need any of the other patches in this thread after correcting it). I've updated the command above in-place.\n", "im getting this error in the log \nE/AndroidRuntime: FATAL EXCEPTION: main\n                  Process: org.tensorflow.demo, PID: 6330\n                  java.lang.UnsatisfiedLinkError: com.android.tools.fd.runtime.IncrementalClassLoader$DelegateClassLoader[DexPathList[[dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_9-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_8-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_7-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_6-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_5-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_4-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_3-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_2-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_1-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_0-classes.dex\"],nativeLibraryDirectories=[/data/app/org.tensorflow.demo-1/lib/arm64, /system/lib64, /vendor/lib64]]] couldn't find \"libtensorflow_demo.so\"\n                      at java.lang.Runtime.loadLibrary0(Runtime.java:972)\n                      at java.lang.System.loadLibrary(System.java:1530)\n                      at org.tensorflow.demo.TensorFlowImageClassifier.<clinit>(TensorFlowImageClassifier.java:35)\n                      at org.tensorflow.demo.TensorFlowImageListener.<init>(TensorFlowImageListener.java:63)\n                      at org.tensorflow.demo.CameraConnectionFragment.<init>(CameraConnectionFragment.java:452)\n                      at org.tensorflow.demo.CameraConnectionFragment.newInstance(CameraConnectionFragment.java:271)\n                      at org.tensorflow.demo.CameraActivity.setFragment(CameraActivity.java:85)\n                      at org.tensorflow.demo.CameraActivity.onRequestPermissionsResult(CameraActivity.java:57)\n                      at android.app.Activity.dispatchRequestPermissionsResult(Activity.java:7069)\n                      at android.app.Activity.dispatchActivityResult(Activity.java:6921)\n                      at android.app.ActivityThread.deliverResults(ActivityThread.java:4049)\n                      at android.app.ActivityThread.handleSendResult(ActivityThread.java:4096)\n                      at android.app.ActivityThread.-wrap20(ActivityThread.java)\n                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1516)\n                      at android.os.Handler.dispatchMessage(Handler.java:102)\n                      at android.os.Looper.loop(Looper.java:154)\n                      at android.app.ActivityThread.main(ActivityThread.java:6077)\n                      at java.lang.reflect.Method.invoke(Native Method)\n                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:865)\n                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:755)\n", "Can someone help me with this error\n", "@bafu Thank you, your suggestion solved the problem. By the way, when testing original demo app (using inception v1), the detection ran smoothly - there was almost no latency, whereas the retrained model I just tested is supposed to detect just two categories and the delay is really large (1 - 2 seconds).\nFor each category, I used a little less than 200 photos. With time I plan to extend the dataset and include more categories. Anyway, I would appreciate if anyone shared his experience using retrained model in terms of latency and also, any suggestions for possible speed-up would be really helpful.\n", "@bafu  and if we wanted to load the new inception v3 model what are the changes that we can do\nI tried to replace final_result with softmax but did not work for me\n\nThank you\n", "@andrewharp please advise on how to load the new inception v3 in the new demo code", "@warmhug what error are you seeing?", "@warmhug I wonder if the graph is already stripped in that case?\r\n\r\nPlease use adb logcat to capture the actual crash, otherwise it's impossible to tell what's going on.\r\ne.g. run:\r\n`adb logcat *:E native:V tensorflow:V`\r\n\r\nthen start the app, and watch the log for anything relevant that you can paste here.", "@warmhug Here's the relevant log line: `jni_utils.cc:137 'asset' Must be non NULL`\r\n\r\nLooks like the pb didn't actually make it into your APK. If you run:\r\n`unzip -v bazel-bin/tensorflow/examples/android/tensorflow_demo.apk`\r\n\r\ndo you see any pb files under assets/?", "Thank @andrewharp  and I'm very sorry, I put the file name misspelled. Thank you for your debugging tip `adb logcat *:E native:V tensorflow:V`, I am not skilled on Android. I will delete my comments to avoid misleading others.", "I have successfully run the inception5h model on my android 5 device. I have trained the inception-v3 network and I have successfully loaded the model to the device, but the predictions I am getting are wrong. Basically, the output classes are \"Food\" and \"Non-food\" but the model seems to be very slow. In some cases it predicts well but in others it's wrong. I have also run the strip_unused.py script but nothing has changed. The model can predict with high accuracy because I have tested it, but it fails as an application. The code is the following:\r\n\r\n```\r\nwith tf.Graph().as_default(): \r\n      \r\n        images = tf.placeholder(dtype=tf.float32, shape=[None, 299, 299, 3], name='inputs')\r\n        labels = tf.placeholder(dtype=tf.int32, shape=[None])\r\n\r\n        # Number of classes in the Dataset label set plus 1.\r\n        # Label 0 is reserved for an (unused) background class.\r\n        num_classes = dataset.num_classes() + 1\r\n        \r\n        logits, softmax_weights, convoluted = inception.inference(images, num_classes)    \r\n\r\n        final_tensor = tf.nn.softmax(logits, name='final_result')\r\n        prediction = tf.argmax(final_tensor, 1)\r\n        top_2_op = tf.nn.in_top_k(logits, labels, 2)\r\n        activation_map = heat_map(convoluted, softmax_weights, labels)\r\n        \r\n        saver = tf.train.Saver()\r\n        # Build an initialization operation to run below.\r\n        init = tf.initialize_all_variables()\r\n        sess = tf.Session()\r\n        sess.run(init)\r\n        \r\n        assert tf.gfile.Exists(FLAGS.pretrained_model_checkpoint_path)\r\n        saver.restore(sess, FLAGS.pretrained_model_checkpoint_path)\r\n        print('%s: Pre-trained model restored from %s' %\r\n                    (datetime.now(), FLAGS.pretrained_model_checkpoint_path))\r\n        \r\n         \r\n        output_graph_def = graph_util.convert_variables_to_constants(\r\n        sess, sess.graph.as_graph_def(), ['final_result'])\r\n        with gfile.FastGFile(FLAGS.output_graph, 'wb') as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n``` ", "hi everybody,\r\n\r\nI'm a old developper from Paris. I'm interesting by tensorflow, so I can to build and test.\r\nBut : I have a some problem. I have read all the forum, but I have not idea of my mistake.\r\n\r\nI have trained image and unstripped (It's work fine), I have created the file \"TensorFlowImageListener.java\" and put it in my Android-studio in the right place (I think) but when I launch a build.....I have this \r\n\r\nError:(66, 57) error: TensorFlowImageClassifier() has private access in TensorFlowImageClassifier\r\nError:(86, 15) error: cannot find symbol method initializeTensorflow(AssetManager,String,String,int,int,int)\r\n\r\nI'm not used before Java...so I learn.....But If you have a idea....I'm very interesting.\r\nThanks for everythings.\r\nP", "@SloaneLAD Thanks for the interest in TensorFlow. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android and https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android for examples and build instructions.\r\n\r\nHowever, for general usage and Android help please post questions to StackOverflow, as we try to limit GitHub issues to actual issues with TF.", "@syed-ahmed @andrewharp \r\nHi Guys,\r\nThanks for your comments,now I am able to use the new inception model .\r\nI trained im2txt model and I need to use it in the Android demo example instead of inception model,I only have experience in python and TensorFlow and I didn`t do java or c++ before .\r\nMy question is : the changes will it be major changes or simple like this one ?\r\n\r\nThanks in advance "]}, {"number": 1268, "title": "Running language model example on multiple GPUs?", "body": "Hi all,\n\nFirst of all thanks for this great library and state of the art examples provided.\n\nI am trying to train a language model on multiple GPUs following your language model example(https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html). This is because (i) Dataset is huge (ii) Vocabulary is large. \n\nI tried to change `with tf.device(\"/cpu:0\"):` in the code to something like: \n\n``` python\nfor d in ['/gpu:0', '/gpu:1']:\n    with tf.device(d):\n        embedding = tf.get_variable(\"embedding\", [vocab_size, size])\n        inputs = tf.nn.embedding_lookup(embedding, self._input_data) \n```\n\nBut got an error from `MatMul`. It happened even when I set it to one GPU. To resolve the problem with one GPU I tried to use:\n\n``` python\ndef device_for_node(n):\n  if n.type == \"MatMul\":\n      return \"/gpu:1\"\n  else:\n      return \"/cpu:0\"\n<some code>\nwith tf.Graph().as_default(), tf.Session() as session:\n    with session.graph.device( device_for_node ):\n```\n\nwhich worked but speed dropped significantly, e.g., from 32,000 wps to 1200 wps which apparently means that it is not a good idea probably because TF optimizes both CPU and GPU usage. \nI still can not figure out how to use multiple GPUs without losing the speed for this example. Anyone has had the same problem?\n\nThanks!\nHamid\n", "comments": ["@zer0n\n@xiaodonghe\n", "We'll need more information to help: what is the error you're getting \"from MatMul\"?\n", "Thanks for reply. Let me ask them separately, \n\n(1) Is it possible to use embedding layer with GPU? I think it is not because when I change `with tf.device(\"/cpu:0\")` to `with tf.device(\"/gpu:0\")` I get the following error which is caused by embedding look up. Is this because it is faster on CPU or it also supports GPU with a different syntax?\n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'model/embedding_lookup': Could not satisfy explicit device specification '/device:GPU:0'\n     [[Node: model/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/device:GPU:0\"](model/embedding/read, model/Placeholder)]]\nCaused by op u'model/embedding_lookup'\n```\n\n(2) For the rest of the code it uses just one GPU even if I made two GPUs visible for it with:\n\n```\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n```\n\nHow I can set the language model example to use more than one GPU?\n\nThanks!\nHamid\n", "Oh no!  I appear to have missed a notification here.  Fortunately since this bug was filed we've added `tf.gather` for GPUs, so part (1) is fixed.\n\n@zheng-xq: Do you want to comment on the multiple GPU bit?  \n", "@Palang2014, a typical set up is to have all the variables on CPU, and all the computation on GPU. If you have multiple GPUs, it is a good idea to merge the gradients on them before sending the delta back to CPU. \n\nIt is not a good idea to place some ops on one device, and others on another, with fine granularity. The problem with that is the data transfer. Each time your data flow across device boundary, the device needs to synchronize and copy the data over. This can easily defeats whatever computation gain you are getting. \n", "Thank you girving and zheng-xq for explanations.\n", "Given the lack of recent activity, I assume the question has been answered sufficiently. Closing now due to lack of recent activity. Please re-open or comment if further information becomes available.\n"]}, {"number": 1267, "title": "learning rate reset to 0 in seq2seq example", "body": "In the `translate.py` example, I used four layers without attention, the learning rate suddenly becomes zero...\n\n```\n itr 1/311830 epoch 1/10 lr 0.7000 cost 9.77 ppl 17455.77\n itr 2/311830 epoch 1/10 lr 0.0000 cost 9.86 ppl 19082.18\n itr 3/311830 epoch 1/10 lr 0.0000 cost 9.80 ppl 18084.05\n itr 4/311830 epoch 1/10 lr 0.0000 cost 9.78 ppl 17700.32\n```\n\nThis is parameter specific, when I set the batch size to be 128, learning rate to be 0.7 or 1.\nWhen I change the batch size to be 64, or set the learning rate to be 0.5, the problem DISAPPEARS.\n\nIn debugging, when I try to set it back to 0.7 after it becomes 0., it will automatically go back to 0 in the next time step...\n\nI confirm the learning rate update operation is not called.\n\nI am using the `0.7.1` version with `cuda 7.5`\n", "comments": ["This seems to be a problem with my gpu...\n"]}, {"number": 1266, "title": "Does tensorflow support Tesla K80?", "body": "Tesla K80 is not listed in the device support list.\n", "comments": ["We don't currently test on K80s, so we can't guarantee it will work, but it should work.  We just won't currently be able to pinpoint a problem if there is a problem and it's specific to K80.\n\nMaybe in the future we'll have a larger testbed of devices to validate against though.\n"]}, {"number": 1265, "title": "DiagPart Operator", "body": "Implemented the get diagonal GetDiag operator #824\n\nI believe numpy is also trying to have two different operators, but they are using diagonal and diagflat for their names.  GetDiag sounds reasonable to me, but discussion welcome.\n", "comments": ["Can one of the admins verify this patch?\n", "Thank you for the contribution!  I added a bunch of comments.  Let me know when I should take another look.\n", "wow, thanks a lot for the detailed comments.  I'll work on the fixes and resubmit.  Since it hasn't merged yet, is it ok if I do a force push to squash the commits or leave them separately?\n", "A force push is good.\n", "Seems like a forced push erases your line comments, unfortunately.  I have addressed your concerns earlier.  Including indentation issues, removed unnecessary template parameter, documentation, and test matrices. \n\nOne thing I realized from looking at numpy code, shouldn't diagonals be where all the indices are the same instead of how it is now?  I mostly reversed the diag operator for this commit though.\n\nThanks.\n", "Thanks, getting much closer!  I guess add commits for now if forcing removes old comments, and once everything is good we can squash into one.\n", "I was thinking if `get_diag` a good name for the operation ?  reading the doc and having `diag` and `get_diag` feels a bit confusing\n", "@Mistobaan: I don't much like it either, but I haven't thought of something good yet.  Do you have suggestions?\n", "Ah, got it!  `diag_part`.\n", "Diagflat in numpy is actually the \"diag\" operation here, (which diagflat doesn't make sense to me either).  Having diag_part might be confusing to numpy users.  I thought of using ExtractDiag for this commit and CreateDiag for the existing \"diag\" operation, but not sure if you wanted names that long.\n", "diag_part is mathematically unambiguous and matches common usage (\"the diagonal part of A\").  It's unfortunately that numpy is different, but `get_diag` is just as bad there.\n", "Ok, give me a few minutes to finish working on the rintop pull request and\nI'll change the name in this one.\n\nOn Wed, Mar 2, 2016, 1:28 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> diag_part is mathematically unambiguous and matches common usage (\"the\n> diagonal part of A\"). It's unfortunately that numpy is different, but\n> get_diag is just as bad there.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1265#issuecomment-191439791\n> .\n", "Thanks for the time spent reviewing the code!  Hopefully this is good for now.\n\nShould I add offset in the future in both diag and diag_part?\n", "Looks good, thank you!  @vrv: How does one run tests and get this merged?\n", "Yep, an offset would be a reasonable addition for a future CL, but it has the unfortunate issue that it would ideally be an `input` rather than an `attr`, and adding new inputs is awkward to do in a backwards compatible way.\n\nAlso, before we merge (but perhaps after the tests?) we should squash this into one commit.  Thanks for keeping it separated for now to make it easier to incrementally review.\n", "@girving I think you just say \"jenkins, test this please\" and it will trigger the tests.\n\nSo, should I work on offset for this first before we merge?  \n\nAlso, for both of these operations should there be a value to indicate the diagonal that is created and extracted?\nFor example:\ndim=0 => A[i, i, i, i, i, i]\ndim=1 => A[i, j, i, j, i, j]\ndim=2 => A[i, j, k, i, j, k]\netc.\n", "only admins can trigger it :)\n\n@tensorflow-jenkins: test this please\n", "Let's leave generalizations to a separate CL.  The symmetry of your existing change is good, since it doesn't change the existing op and adds it's one sided inverse.\n", "I just squashed the commits.\n", "@tensorflow-jenkins: test this please\n", "Merged\n"]}, {"number": 1264, "title": "It seems there is protobuf problem after building tensorflow 0.7 from source code", "body": "For bugs/issues, please fill in the following.  The more information you\nprovide, the more likely we can help you.\n### Environment info\n\nOperating System: ubuntu 1404\n\nHi all\n\nI am caffe user.\nI've executed caffe application since the end of the last year.\nUntil i tried to install tensorflow, my system is working well.\nAfter install tensorflow 0.7 from source code.\nit shows me some error like below.\n\nEven though i installed protobuf 3.0 via pip and source code, i couldn't solve this problem.\nI think there is some static variable for requiring specific version of protobuf(in this case 3.0).\nDo you have any idea for this problem?\n\nI0224 10:41:24.543294 19528 layer_factory.hpp:74] Creating layer data\n[libprotobuf FATAL google/protobuf/stubs/common.cc:61] This program requires version 3.0.0 of the Protocol Buffer runtime library, but the installed version is 2.5.0.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"google/protobuf/src/google/protobuf/any.pb.cc\".)\nterminate called after throwing an instance of 'google::protobuf::FatalException'\n  what():  This program requires version 3.0.0 of the Protocol Buffer runtime library, but the installed version is 2.5.0.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"google/protobuf/src/google/protobuf/any.pb.cc\".)\n\nThank you\n", "comments": ["I would suggest uninstalling protobuf and installing the protobuf version you want to make Caffe work.  I would then suggest installing TensorFlow in a virtualenv, so that you don't have to worry about having incompatible copies of protobuf in your system libraries.  \n", "Thank you vrv.\nIt is working now.\n", "@kepricon I faced the same error\uff0cso what can I do ? \r\n\"[libprotobuf FATAL google/protobuf/src/google/protobuf/stubs/common.cc:67] This program requires version 3.2.0 of the Protocol Buffer runtime library, but the installed version is 3.0.0.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"google/protobuf/descriptor.pb.cc\".)\r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  This program requires version 3.2.0 of the Protocol Buffer runtime library, but the installed version is 3.0.0.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"google/protobuf/descriptor.pb.cc\".)\r\n\"\r\nLooking forward to your reply. Thanks\uff01"]}, {"number": 1263, "title": "How to extract final embedding matrix from word2vec.py?", "body": "I am using word2vec.py. I want to extract the final embedding matrix and save it to a txt file. How can I extract the final embedding?\n", "comments": ["this would be a better question for StackOverflow, GitHub issues are for bugs / feature requests.\n", "@michelleowen Have you solved the problem? I have the same question. Thanks", "@davidoak yes, I did. Please refer to the stackoverflow: https://stackoverflow.com/questions/35582858/how-can-i-get-final-embeddings-from-word2vec-py/35611003?noredirect=1#comment76826763_35611003", "@michelleowen thank you! I was also wondering after generating the .txt embedding file, can I use this to implement KNN retrieval task? For example, displaying top 10 nearest words. If so, do you know how I can do it?"]}, {"number": 1262, "title": "Import error when installing from the latest binaries on Ubuntu14, Python3.4", "body": "I wanted to updat tensorflow to the newest release and now cannot import it (I am not in a source tree).\n### Environment info\n\nOperating System:\nUbuntu 14\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   Latest 3.4 cpu Ubuntu\n   `sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl`\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### What have you tried?\n\n`python3 -c \"import tensorflow; print(tensorflow.__version__)\".`\n### Logs or other output that would be helpful\n\n```\ndima@dima-TP-T410s:~/data/repos$ python3 -c \"import tensorflow; print(tensorflow.__version__)\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: cannot import name 'descriptor'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py\", line 41, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: cannot import name 'descriptor'\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n```\n", "comments": ["Can you try pip uninstalling tensorflow _and_ protobuf, then reinstalling tensorflow?\n", "this solved the issue\n", "envy@ub1404:~/os_pri/github$ \n\n## envy@ub1404:~/os_pri/github$ pip show protobuf\n\nName: protobuf\nVersion: 2.6.1\nLocation: /home/envy/.local/lib/python2.7/site-packages\nRequires: setuptools\n\n## envy@ub1404:~/os_pri/github$ pip show tensorflow\n\nName: tensorflow\nVersion: 0.7.1\nLocation: /home/envy/.local/lib/python2.7/site-packages\nRequires: six, protobuf, wheel, numpy\nenvy@ub1404:~/os_pri/github$ \n\ni got this error yet!\n", "Thanks guys that worked. When i installed tensorflow it uninstalled protobuf 2.6.1 and installed the latest 3.0 version required for tensorflow..works now\n"]}, {"number": 1261, "title": "Setting momentum", "body": "Hi,\n\nI am trying to change the value of momentum for SGD. Can you please tell me where it is set?\n\nThanks!\n", "comments": ["Instead of using `GradientDescentOptimizer`, take a look at `MomentumOptimizer`\n", "This kind of question should be asked in https://stackoverflow.com/questions/tagged/tensorflow. Please read the [community guide lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/index.md#community) for more information.\n", "Thanks @rdipietro ! @cesarsalgado : sure, next time :)\n"]}, {"number": 1260, "title": "Added sqrt to LRN doc equation.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Actually there should be no sqrt in the formula, but the beta should be moved outside the parentheses. I will close this one and make another one.\n"]}, {"number": 1259, "title": "Adding switch for tutorial tests to ci_parameterized_build.sh", "body": "", "comments": ["Can one of the admins verify this patch?\n", "The main script for tutorial tests has arrived from internal: \nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/builds/test_tutorials.sh\n", "Jenkins, test this please\n", "merged.\n"]}, {"number": 1258, "title": "Create a way to control log message output.", "body": "Create a way to control log message output., most options should be passed via ConfigProto or another proto. Maybe add a LogOptions structure to hold options related to logging.  #1229\n### Environment info\n\nOperating System: Mac 10.9.5\n### Feature request\n1. Have a way to control logs that you record.As in gflags/re2 flags.Refer  #1229 \n### What have you tried?\n1. #1229 \n", "comments": ["Okay, see https://github.com/tensorflow/tensorflow/commit/0ed66eb560957987901eef40dcf8e74247841528 for how you can set a min log level for logging in C++ using an environment variable.\n\nDefaults to 0, so all logs are shown.  Set TF_CPP_MIN_LOG_LEVEL to 1 to filter out INFO logs, 2 to additionall filter out WARNING, 3 to additionally filter out ERROR.\n", "What about setting a min logging level in Python?", "You can do it using `os.environ` \r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='3'\r\nimport tensorflow as tf\r\n\r\n```", "just got 12.1 of tensorflow-gpu for python on windows 10 (64 bit), installed via pip, and setting TF_CPP_MIN_LOG_LEVEL has no effect on the log messages. I have tried via environment variable in the shell and via setting it in the python script as above. Is this a known issue for windows?", "It's not in 12.1, you need a later version (nightly)\n\nOn Jan 19, 2017 4:14 AM, \"FeralWhippet\" <notifications@github.com> wrote:\n\n> just got 12.1 of tensorflow-gpu for python on windows 10 (64 bit),\n> installed via pip, and setting TF_CPP_MIN_LOG_LEVEL has no effect on the\n> log messages. I have tried via environment variable in the shell and via\n> setting it in the python script as above. Is this a known issue for windows?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/1258#issuecomment-273761886>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHEjv1lFnUSyHg_Gkx-pnWQ48sCkRks5rT1O2gaJpZM4HhA0o>\n> .\n>\n", "@yaroslavvb : and there was MUCH rejoicing! \ud83d\udc6f \ud83d\udc4f ", "Hi, I am using tf 12.1. I compiled it from the master branch. I have set the following in my python code: \r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='0'`\r\n```\r\nbut I don't get any VLOG outputs from the C code.\r\n\r\nSpecifically I am after this particular log (tensorflow/core/platform/default/conv_op.cc)\r\n\r\n```\r\nVLOG(2) << \"Conv2D: in_depth = \" << in_depth\r\n            << \", input_cols = \" << input_cols\r\n            << \", filter_cols = \" << filter_cols\r\n            << \", input_rows = \" << input_rows\r\n            << \", filter_rows = \" << filter_rows\r\n            << \", stride_rows = \" << stride_rows\r\n            << \", stride_cols = \" << stride_cols\r\n            << \", out_depth = \" << out_depth;\r\n```\r\nI do get INFO logs from python part like this:\r\n\r\n`INFO:tensorflow:Summary name loss/cross_entropy (raw) is illegal; using loss/cross_entropy__raw_ instead.\r\n`\r\n\r\nBut nothing from the compiled C library.", "It's TF_CPP_MIN_VLOG_LEVEL, not TF_CPP_MIN_LOG_LEVEL\nAlso, note that if TF_CPP_MIN_LOG_LEVEL is set, then TF_CPP_MIN_VLOG_LEVEL\nvalues are ignored\n\nOn Sun, Feb 5, 2017 at 7:32 PM, russellanam <notifications@github.com>\nwrote:\n\n> Hi, I am using tf 12.1. I compiled it from the master branch. I have set\n> the following in my python code:\n>\n> import os os.environ['TF_CPP_MIN_LOG_LEVEL']='0'\n>\n> but I don't get any VLOG outputs from the C code.\n>\n> Specifically I am after this particular log (tensorflow/core/platform/\n> default/conv_op.cc)\n>\n> VLOG(2) << \"Conv2D: in_depth = \" << in_depth\n> << \", input_cols = \" << input_cols\n> << \", filter_cols = \" << filter_cols\n> << \", input_rows = \" << input_rows\n> << \", filter_rows = \" << filter_rows\n> << \", stride_rows = \" << stride_rows\n> << \", stride_cols = \" << stride_cols\n> << \", out_depth = \" << out_depth;\n>\n> I do get INFO logs from python part like this:\n>\n> INFO:tensorflow:Summary name loss/cross_entropy (raw) is illegal; using\n> loss/cross_entropy__raw_ instead.\n>\n> But nothing from the compiled C library.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/1258#issuecomment-277579308>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHERXu_UduW4d9ME8LxYA3aqclv1fks5rZpREgaJpZM4HhA0o>\n> .\n>\n", "Also, not sure if 12.1 is late enough, you may need 1.0rc1 or later\n\nOn Sun, Feb 5, 2017 at 9:27 PM, Yaroslav Bulatov <yaroslavvb@gmail.com>\nwrote:\n\n> It's TF_CPP_MIN_VLOG_LEVEL, not TF_CPP_MIN_LOG_LEVEL\n> Also, note that if TF_CPP_MIN_LOG_LEVEL is set, then TF_CPP_MIN_VLOG_LEVEL\n> values are ignored\n>\n> On Sun, Feb 5, 2017 at 7:32 PM, russellanam <notifications@github.com>\n> wrote:\n>\n>> Hi, I am using tf 12.1. I compiled it from the master branch. I have set\n>> the following in my python code:\n>>\n>> import os os.environ['TF_CPP_MIN_LOG_LEVEL']='0'\n>>\n>> but I don't get any VLOG outputs from the C code.\n>>\n>> Specifically I am after this particular log (tensorflow/core/platform/defa\n>> ult/conv_op.cc)\n>>\n>> VLOG(2) << \"Conv2D: in_depth = \" << in_depth\n>> << \", input_cols = \" << input_cols\n>> << \", filter_cols = \" << filter_cols\n>> << \", input_rows = \" << input_rows\n>> << \", filter_rows = \" << filter_rows\n>> << \", stride_rows = \" << stride_rows\n>> << \", stride_cols = \" << stride_cols\n>> << \", out_depth = \" << out_depth;\n>>\n>> I do get INFO logs from python part like this:\n>>\n>> INFO:tensorflow:Summary name loss/cross_entropy (raw) is illegal; using\n>> loss/cross_entropy__raw_ instead.\n>>\n>> But nothing from the compiled C library.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/1258#issuecomment-277579308>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AABaHERXu_UduW4d9ME8LxYA3aqclv1fks5rZpREgaJpZM4HhA0o>\n>> .\n>>\n>\n>\n", "Thanks for the quick reply. I can confirm that TF_CPP_MIN_VLOG_LEVEL does not work with v0.12. I will try with 1.0rc1.", "It works with TF v0.12.1 \r\n![image](https://cloud.githubusercontent.com/assets/3029862/23041760/52a7a074-f496-11e6-907c-d36a0f73e812.png)\r\n", "@russellanam  I can confirm the following works with TF v1.0.0\r\n```python\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='3'\r\nimport tensorflow as tf\r\n```", "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:374: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01\r\n\r\n#tf.logging.set_verbosity(tf.logging.FATAL)#INFO\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'#0\r\n\r\ncan't suppress from tflearn? they seem more like printed strings", "For one interested, in C++ you may manage TF logging with this line of code:\r\n_putenv_s(\"TF_CPP_MIN_LOG_LEVEL\", \"1\");\r\nLog levels are defined in tensorflow\\core\\platform\\default\\logging.h", "Hi @AndreyPlotkinOr . I've tried your approach and I cannot hide the:\r\n `2020-07-03 13:31:19.012804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll` type of messages with any log level (1, 2 or 3)\r\n\r\nDo you mind sharing where did you call the function? Or it should not matter? In my case I call it very early in the `main` function.\r\nMind you it may be that these types of messages cannot be hidden by means of the logging flags.\r\n\r\nCheers!\r\n\r\nPS: I'm using 1.15.3 build from source on Windows if it helps! :)\r\n\r\nFerran."]}, {"number": 1257, "title": "Attribute Error from Tensorboard.py following upgrade to v0.7.1 upgrade", "body": "For bugs/issues, please fill in the following.  The more information you\nprovide, the more likely we can help you.\n### Environment info\n\nOperating System:Ubuntu 14.0.4\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.7.1\n   If installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.In the command prompt try to launch a tensorboard graph\n2.\n3.\n### What have you tried?\n1. I tried using the command      **tensorboard --logdir=/home/tattoo/Tabor_Stuff**\n   Previously for v0.6.0 the graph launched. However after installing v0.7.1 with the command above I get the following attribute error. Am I missing an install step or is this really an error?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nTraceback (most recent call last):\n  File \"/usr/local/bin/tensorboard\", line 11, in <module>\n    sys.exit(main())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/tensorboard.py\", line 117, in main\n    multiplexer = event_multiplexer.AutoloadingMultiplexer(\nAttributeError: 'module' object has no attribute 'AutoloadingMultiplexer'\n", "comments": ["Can you try uninstalling and re-installing TensorFlow?  That looks like a stale file (we no longer have AutoloadingMultiplexer).\n", "Hi Vijay,\n\nHow do I uninstall it?\nI'm new to ubuntu.\n\nDon\n\nOn Tue, Feb 23, 2016 at 11:02 PM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> Can you try uninstalling and re-installing TensorFlow? That looks like a\n> stale file (we no longer have AutoloadingMultiplexer).\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1257#issuecomment-188053121\n> .\n", "Yeah, re-installing fixed the problem. Just re-run the upgrade command.\n", "pip uninstall tensorflow\npip uninstall protobuf\npip install url/to/the/tensorflow-...whl\n", "Hi Vijay,\n\nThanks for the tips.\n\nI'm still getting the same error. Do I need to run these commands from a\nparticular directory?\n\nDon\n\nOn Thu, Feb 25, 2016 at 2:05 AM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> pip uninstall tensorflow\n> pip uninstall protobuf\n> pip install url/to/the/tensorflow-...whl\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1257#issuecomment-188643353\n> .\n", "Weird, I tried it again and I have it launch now. Thank you.\n\nOn Thu, Feb 25, 2016 at 1:09 PM, Donald McMenemy donald.mcmenemy@uconn.edu\nwrote:\n\n> Hi Vijay,\n> \n> Thanks for the tips.\n> \n> I'm still getting the same error. Do I need to run these commands from a\n> particular directory?\n> \n> Don\n> \n> On Thu, Feb 25, 2016 at 2:05 AM, Vijay Vasudevan <notifications@github.com\n> \n> > wrote:\n> > \n> > pip uninstall tensorflow\n> > pip uninstall protobuf\n> > pip install url/to/the/tensorflow-...whl\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/tensorflow/tensorflow/issues/1257#issuecomment-188643353\n> > .\n", "We're good now. It's curious. Now I have to use the full path to the\ntensorboard.py instead of simply the tensorboard --logdir=/stuff\n\nOn Thu, Feb 25, 2016 at 1:56 PM, Donald McMenemy donald.mcmenemy@uconn.edu\nwrote:\n\n> Weird, I tried it again and I have it launch now. Thank you.\n> \n> On Thu, Feb 25, 2016 at 1:09 PM, Donald McMenemy <\n> donald.mcmenemy@uconn.edu> wrote:\n> \n> > Hi Vijay,\n> > \n> > Thanks for the tips.\n> > \n> > I'm still getting the same error. Do I need to run these commands from a\n> > particular directory?\n> > \n> > Don\n> > \n> > On Thu, Feb 25, 2016 at 2:05 AM, Vijay Vasudevan <\n> > notifications@github.com> wrote:\n> > \n> > > pip uninstall tensorflow\n> > > pip uninstall protobuf\n> > > pip install url/to/the/tensorflow-...whl\n> > > \n> > > \u2014\n> > > Reply to this email directly or view it on GitHub\n> > > https://github.com/tensorflow/tensorflow/issues/1257#issuecomment-188643353\n> > > .\n", "Most likely you have a stale tensorboard.py in your /usr/local/bin dir (or some other bin path)\n", "Thank you Vijay.  I've been dealing with the same error after upgrade.  Per recommendations above, tried \npip uninstall tensorflow\npip uninstall protobuf\npip install url/to/the/tensorflow-...whl\nbut the problem persisted.  \n\nThen tried \npip uninstall tensorflow\npip uninstall protobuf\nmanually deleted tensorboard.py from /usr/local/bin\npip install url/to/the/tensorflow-...whl\ntensorboard.py was recreated with the new tensorflow installation and the problem resolved.\n"]}, {"number": 1256, "title": "install issue!", "body": "envy@ub1404:~/os_pri/github/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nINFO: Found 1 target...\nERROR: /home/envy/os_pri/github/tensorflow/third_party/gpus/cuda/BUILD:126:1: declared output 'third_party/gpus/cuda/include/cudnn.h' is a dangling symbolic link.\nERROR: /home/envy/os_pri/github/tensorflow/third_party/gpus/cuda/BUILD:126:1: not all outputs were created.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 38.334s, Critical Path: 37.13s\nenvy@ub1404:~/os_pri/github/tensorflow$\n\nbut I can see:\nenvy@ub1404:~/os_pri/github/tensorflow$ ll third_party/gpus/cuda/include/cudnn.h \nlrwxrwxrwx 1 envy envy 35  2\u6708 24 01:13 third_party/gpus/cuda/include/cudnn.h -> /usr/local/cuda-7.0/include/cudnn.h\n\nenvy@ub1404:~/os_pri/github/tensorflow$ ll /usr/local/cuda/include/cudnn.h\n-rw------- 1 root root 38830  2\u6708 23 13:07 /usr/local/cuda/include/cudnn.h\n\nenvy@ub1404:~/os_pri/github/tensorflow$ ll /usr/local/cuda-7.0/include/cudnn.h\n-rw------- 1 root root 38830  2\u6708 23 13:07 /usr/local/cuda-7.0/include/cudnn.h\nenvy@ub1404:~/os_pri/github/tensorflow$ \n", "comments": ["sudo chmod a+r /usr/local/cuda/include/*\n\ncan fix that!\n"]}, {"number": 1255, "title": "update_version.sh: automatic version update in source", "body": "Usage example:\n  update_version.sh 0.8.0\n", "comments": ["Yes!\n", "Addressed @martinwicke's comment. Added safety check for lingering old version strings and warning if those are found. \n", "merged.\n"]}, {"number": 1254, "title": "import tensorflow failed with 0.7.1 pip package", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   I flollowed: sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n2. The output from python -c \"import tensorflow \".\n   Traceback (most recent call last):\n   File \"<stdin>\", line 1, in <module>\n   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/**init**.py\", line 23, in <module>\n     from tensorflow.python import *\n   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 41, in <module>\n     raise ImportError(msg)\n   ImportError: Traceback (most recent call last):\n   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 35, in <module>\n     from tensorflow.core.framework.graph_pb2 import *\n   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n     from google.protobuf import descriptor as _descriptor\n   ImportError: No module named protobuf\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\n### What have you tried?\n1. I reinstall the old 0.6.0 pip package, everything works\n2. I installed 0.7.1 cpu only version, it still doesn't work\n3. I upgrade cuda & cudnn to 7.5 &v4, it still doesn't work\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Sorry I just found a dup with #1180. Will close this for now\n", "\uff0c\uff0c\uff0c  --upgrade\uff1f\uff1f\uff1f\n"]}, {"number": 1253, "title": "Update the Android demo to use retrained models", "body": "The Android example currently uses an older Inception model. It requires some changes to load the one used by the label_image, classify_image, and image_retraining examples. These include altering the names of the input and output layers, ensuring the larger model can be loaded, and updating the mean, standard deviation, and image size parameters.\n\nOnce this is done, people should be able to transfer retrained models to a device easily. This issue also covers documenting that process.\n", "comments": ["This is now fixed, closing.\n"]}, {"number": 1252, "title": "tensorflow-0.7.1-cp27-none-any.whl is not a supported wheel on this platform.", "body": "For bugs/issues, please fill in the following.  The more information you\nprovide, the more likely we can help you.\n### Environment info\n\nOperating System:\nyosemite os x\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.source ~/tensorflow/bin/activate\n2.pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\ntensorflow-0.7.1-cp27-none-any.whl is not a supported wheel on this platform.\n", "comments": ["What do you get when you run\n\n```\npip --version\n```\n\n?\n", "pip 7.1.2 from /usr/local/lib/python3.5/site-packages (python 3.5)\n", "Try installing the python3 wheel, not the python2 one.\n", "(https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp35-none-any.whl)\n", "Issue is not closed I am getting below exception when I use cp35.\n\n**Exception**:\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/shutil.py\", line 538, in move\n    os.rename(src, real_dst)\nPermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.5/site-packages/external/**init**.py' -> '/var/folders/55/crqk_b7n0pn13rh7c0kwp5300000gn/T/pip-o8eda0ef-uninstall/usr/local/lib/python3.5/site-packages/external/**init**.py'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/site-packages/pip/basecommand.py\", line 211, in main\n    status = self.run(options, args)\n  File \"/usr/local/lib/python3.5/site-packages/pip/commands/install.py\", line 311, in run\n    root=options.root_path,\n  File \"/usr/local/lib/python3.5/site-packages/pip/req/req_set.py\", line 640, in install\n    requirement.uninstall(auto_confirm=True)\n  File \"/usr/local/lib/python3.5/site-packages/pip/req/req_install.py\", line 716, in uninstall\n    paths_to_remove.remove(auto_confirm)\n  File \"/usr/local/lib/python3.5/site-packages/pip/req/req_uninstall.py\", line 125, in remove\n    renames(path, new_path)\n  File \"/usr/local/lib/python3.5/site-packages/pip/utils/**init**.py\", line 315, in renames\n    shutil.move(old, new)\n  File \"/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/shutil.py\", line 553, in move\n    os.unlink(src)\nPermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.5/site-packages/external/**init**.py'\n", "Sounds like an issue with permissions -- consider installing in a virtualenv, as described on the install guide on tensorflow.org\n", "Getting the same thing:\nPermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.4/dist-packages/tensorflow-0.7.1.dist-info'\n\nWhen attempting to install in a conda env. The python 2 version works, but not python 3. It seems to want to install globally instead of in the env.\n", "Thanks  I resolved it, i installed globally instead in envirnoment\n"]}, {"number": 1251, "title": "install from source failed!", "body": "envy@ub1404:~/os_pri/github/tensorflow$ sudo rm -rf ~/.cache/bazel   # just try at second time\n\nenvy@ub1404:~/os_pri/github/tensorflow$ sudo bazel build -c opt --config=cuda --verbose_failures //tensorflow/cc:tutorials_example_trainer \nExtracting Bazel installation...\n.....\nINFO: Found 1 target...\nINFO: From Executing genrule @png_archive//:configure [for host]:\nsrc/main/tools/namespace-sandbox.c:460: mount(opt->sandbox_root, opt->sandbox_root, NULL, MS_BIND | MS_NOSUID, NULL): Permission denied\nERROR: /home/envy/.cache/bazel/_bazel_root/d161cc2b736082b8df6a4c27a7f7e3a8/external/png_archive/BUILD:23:1: Executing genrule @png_archive//:configure failed: bash failed: error executing command \n  (cd /home/envy/.cache/bazel/_bazel_root/d161cc2b736082b8df6a4c27a7f7e3a8/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; pushd external/png_archive/libpng-1.2.53; workdir=$(mktemp -d -t tmp.XXXXXXXXXX); cp -a \\* $workdir; pushd $workdir; ./configure --enable-shared=no --with-pic=no; popd; popd; cp $workdir/config.h bazel-out/host/genfiles/external/png_archive/libpng-1.2.53; rm -rf $workdir;'): bash failed: error executing command \n  (cd /home/envy/.cache/bazel/_bazel_root/d161cc2b736082b8df6a4c27a7f7e3a8/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; pushd external/png_archive/libpng-1.2.53; workdir=$(mktemp -d -t tmp.XXXXXXXXXX); cp -a \\* $workdir; pushd $workdir; ./configure --enable-shared=no --with-pic=no; popd; popd; cp $workdir/config.h bazel-out/host/genfiles/external/png_archive/libpng-1.2.53; rm -rf $workdir;').\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 27.500s, Critical Path: 2.21s\nenvy@ub1404:~/os_pri/github/tensorflow$ \n", "comments": ["You probably don't want to run bazel with 'sudo'. \n", "yes, that is the root cause.\nsince the first time I am have bazel installed without '--user' , I have to run with 'sudo'\n\nnow I reinstall bazel with --user, it works without sudo\n"]}, {"number": 1250, "title": "Let sparse_softmax_cross_entropy_with_logits accept int32 labels?", "body": "Right now `sparse_softmax_cross_entropy_with_logits` only accepts `labels` as an int64 `Tensor`. Is this intentional? Why would we want to avoid int32 `labels`?\n", "comments": ["This is due to a deep technical issue with being able to access the values on a GPU.  For now, use tf.to_int64 to convert your values.\n", "Closed because the deeper issue has an open internal bug case.\n", "int32 should work fine on GPUs now.  I'll fix this.\n"]}, {"number": 1249, "title": "Bidirectional rnn now returns forward and backward output states, upd\u2026", "body": "\u2026ated tests, tests pass. Updated my email with the right email this time!\n", "comments": ["Can one of the admins verify this patch?\n", "@lukaszkaiser, @ludimagister can you take a look (once this is updated to master)?\n", "This change looks good, just needs a merge.\n", "great! One thing I forgot to do was update the docstring. How do we want to do that? \n", "@lucaswiser just change the text in the output docstring https://github.com/lucaswiser/tensorflow/blob/bidirectional-rnn-state/tensorflow/python/ops/rnn.py#L295\n", "OK the docstring has been updated\n", "Let's get it in, what's missing?\n", "I think it's all set to go.\nOn Mar 2, 2016 9:23 PM, \"Lukasz Kaiser\" notifications@github.com wrote:\n\n> Let's get it in, what's missing?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1249#issuecomment-191589896\n> .\n", "I still see \"This branch is out-of-date with the base branch. Merge the latest changes from master into this branch.\" just above this comment. Are you sure it's all merged?\n", "Thanks for doing this -- just a few comments from my side, and then if everything looks good, please squash the commits and we'll test and merge.\n", "Don't worry about that, it just means we pushed a commit since the last\nupdate.\n\nOn Wed, Mar 2, 2016, 11:07 PM Lukasz Kaiser notifications@github.com\nwrote:\n\n> I still see \"This branch is out-of-date with the base branch. Merge the\n> latest changes from master into this branch.\" just above this comment. Are\n> you sure it's all merged?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1249#issuecomment-191619663\n> .\n", "@tensorflow-jenkins: test this please\n", "Ok commits are squashed. Should I merge master into the branch to update it or should someone else do that?\n", "No need, as long as there are no conflicts we rebase for you.\n\n@tensorflow-jenkins: test this please.  PLEASE.\n", "Merged\n"]}, {"number": 1248, "title": "libtensorflow_framework.so: cannot open shared object file: No such file or directory", "body": "centos 7\npython -c \"import tensorflow; print(tensorflow.**version**)\"\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.7.1\n\n$ python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())'\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n/usr/lib/python2.7/site-packages/tensorflow/core\n\n$ ls /usr/lib/python2.7/site-packages/tensorflow/core/\nexample/                    **init**.py                 kernels/                    libtensorflow_framework.so  util/  \nframework/                  **init**.pyc                lib/                        protobuf/                   \n\nipython\nPython 2.7.5 (default, Nov 20 2015, 02:00:19) \nType \"copyright\", \"credits\" or \"license\" for more information.\n\nIPython 3.2.1 -- An enhanced Interactive Python.\n?         -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp      -> Python's own help system.\nobject?   -> Details about 'object', use 'object??' for extra details.\n\nIn [1]: import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n## In [2]: tf.load_op_library('./auc.so')\n\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-2-32f9df443d84> in <module>()\n----> 1 tf.load_op_library('./auc.so')\n\n/usr/lib/python2.7/site-packages/tensorflow/python/framework/load_library.pyc in load_op_library(library_filename)\n     55   try:\n     56     if py_tf.TF_GetCode(status) != 0:\n---> 57       raise RuntimeError(compat.as_text(py_tf.TF_Message(status)))\n     58   finally:\n     59     py_tf.TF_DeleteStatus(status)\n\nRuntimeError: libtensorflow_framework.so: cannot open shared object file: No such file or directory\n\nIn [3]: \n\nI need to copy libtensorflow_framework.so to the working path to run script.\n", "comments": ["How do you build auc.so?\nLike it says in the documentation [here](https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html#building-the-op-library), make sure you have `-rpath $TF_LIB` in the command line arguments to `g++`. If you indeed pass that flag and it still doesn't work, try adding `$TF_LIB` to your `$LD_LIBRARY_PATH`. I verified that I can indeed load an op with `load_op_library` and didn't see the errors you are seeing. Closing the issue. Please reopen with more information, in case it still doesn't work.\n", "@keveman  Thanks, the so is built on another ubuntu platform, and I miss this problem when using the so on centos, after rebuilding auc.so now every thing is Ok.\nAnother thing, might worth a look, in order to gen auc.so, I need to first install google protobuffer, then, I need to add below to my ~/.bashrc for a workaround for missing eigen related headers.\nexport CPLUS_INCLUDE_PATH=~/.cache/bazel/_bazel_gezi/90eddd4c58c33600d0766ab4c9609dbb/external/eigen_archive/:~/.cache/bazel/_bazel_gezi/90eddd4c58c33600d0766ab4c9609dbb/external/eigen_archive/eigen-eigen-70505a059011:~/.cache/bazel/_bazel_gezi/90eddd4c58c33600d0766ab4c9609dbb/external/tf/third_party/:~/other/tensorflow/\n"]}, {"number": 1247, "title": "Updated bidirectional rnn to return forward and backward final states\u2026", "body": "\u2026, updated tests, tests pass. Changed commit to use this email for Google permissions.\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 1246, "title": "Feature Request: spatial local response normalization.", "body": "Currently tf.nn.local_response_normalization only normalizes along the the depth. I would like to suggest adding a spatial version of LRN as in caffe: http://caffe.berkeleyvision.org/tutorial/layers.html#local-response-normalization-lrn\n", "comments": ["If someone is interested I made this implementation using python:\n\n``` python\ndef spatial_lrn(tensor, depth_radius=5, bias=1.0, alpha=1.0, beta=0.5):\n  squared = tf.square(tensor)\n  in_channels = tensor.get_shape().as_list()[3]\n  kernel = tf.constant(1.0, shape=[depth_radius, depth_radius, in_channels, 1])\n  squared_sum = tf.nn.depthwise_conv2d(squared,\n                                       kernel,\n                                       [1, 1, 1, 1],\n                                       padding='SAME')\n  bias = tf.constant(bias, dtype=tf.float32)\n  alpha = tf.constant(alpha, dtype=tf.float32)\n  beta = tf.constant(beta, dtype=tf.float32)\n  return tensor / ((bias + alpha * squared_sum) ** beta)\n```\n\nI won't be able to implement it in C++ for now. Should I submit PR with this python only implementation?\n", "I would like to take this up @cesarsalgado \n", "@yash14123 ok, thanks!\n", "Is anyone working on this? I have not implemented an Op yet, so I think this could be a good learning experience for me. I can take this up, unless it needs to be done quickly. Can one of the admins comment?\n", "@vrv Could you comment on this? (since you are already a participant on the issue)\n", "@theaverageguy said he was working on it, but no movement in a while, so go ahead.  As always, good documentation and tests are going to be needed!\n", "@vrv I tried doing this, I feel that I'm still not ready for writing an Op (I'm not familiar with Eigen functionalities, and don't understand what is efficient v/s what is not). I will look at some C++ issues before coming back to this. Are there some issues that will help me with this, or do you recommend another way?\n", "This is a reasonable feature, but I'll close for now since it's not getting any activity.  Happy to reopen if someone wants to take this on."]}, {"number": 1245, "title": "tf.train.match_filenames_once incompatible with checkpointer?", "body": "On Mac 0.7.1 pip binary I'm getting the following error when trying to load a model that had called `tf.train.match_filenames_once` (named 'file' below) to obtain a list of file names:\n\n`InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [] rhs shape= [60]\n     [[Node: geomnet/save/Assign_6 = Assign[T=DT_STRING, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](geomnet/files, geomnet/save/restore_slice_6)]]\nCaused by op u'geomnet/save/Assign_6', defined at:\n  File \"/Applications/Canopy.app/appdata/canopy-1.4.1.1975.macosx-x86_64/Canopy.app/Contents/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)`\n\nIs this expected behavior?\n", "comments": ["It seems that setting the `reshape` option of `Saver` to `True` should resolve the above problem, but it doesn't, per this [question](https://stackoverflow.com/questions/35585053/restoring-from-checkpoint-when-variable-shape-changes) on Stack Overflow.\n", "I believe the reshape option only reshapes if the shapes have matching sizes.  Here the lhs would have 1 element and the rhs 60.  I'll post an answer on Stackoverflow to that effect.\n"]}, {"number": 1244, "title": "Can't import tensorflow 0.7.1 It thinks I'm importing from source dir.", "body": "For bugs/issues, please fill in the following.  The more information you\nprovide, the more likely we can help you.\n### Environment info\n\nOperating System: Ubuntu 64bit \nCUDA: 7.5\n\nIf installed from binary pip package, provide:\nI tried installing using both pip CPU and GPU (making sure I uninstalled the previous first):\n\n```\n$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp34-none-linux_x86_64.whl\n\n$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp34-none-linux_x86_64.wh\n```\n### Logs or other output that would be helpful\n\nI get the following error using the new 0.7.1 wheel, although it is fine with the previous one 0.7.0\n\n```\n(If logs are large, please upload as attachment).\nIn [1]: import tensorflow as tf\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n<ipython-input-1-41389fad42b5> in <module>()\n----> 1 import tensorflow as tf\n\n/home/eders/anaconda/lib/python2.7/site-packages/tensorflow/__init__.py in <module>()\n     21 from __future__ import print_function\n     22\n---> 23 from tensorflow.python import *\n\n/home/eders/anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py in <module>()\n     39 please exit the tensorflow source tree, and relaunch your python interpreter\n     40 from there.\"\"\" % traceback.format_exc()\n---> 41   raise ImportError(msg)\n     42\n     43 from tensorflow.core.framework.summary_pb2 import *\n\nImportError: Traceback (most recent call last):\n  File \"/home/eders/anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/home/eders/anaconda/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: No module named protobuf\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n```\n\nNote that I don't even have the source files on my computer. Any idea of what it might be?\nThanks for the help.\n", "comments": ["sorry, I forgot to pip install protobuf. I missed that line of the error. It seems to run with GPU under anaconda env btw.\n", "I have this same Error. When I pip install protobuf it says my requirements are satisfied but I still get the error. Any ideas?\n", "@murraymack I resolved this issue by downgrading the protobuf library for now. The latest one is missing `__init__.py`, i.e. a broken package.\n", "I had this error on mac but then upgraded easy_install\n\n`sudo easy_install --upgrade six`\n", "@xiao-cheng Yes. I did the same unintentionally. I ended up pip uninstalling tensorflow AND pip uninstalling protobuf, and then just pip installed tensorflow and it worked.\n\nFor anyone reading, I did this through a Python 2.7 virtual env created in Anaconda.\n\nCode: \n\n`$ pip uninstall tensorflow`\n`$ pip uninstall protobuf`\n`$ pip install tensorflow`\n", "@murraymack  thanks for your methods, it works fine even with only pip installation.\n", "Same issue, same fix as mentioned by @murraymack , thanks!!\n", "@murraymack solution worked for me\n", "Same error occurred after upgrading Tensorflow to version 0.8 in Mac OS.\n@murraymack solution works for me\n", "Upgrading to protobuf 3.0.0 from 3.0.0b seemed to solve this for me. Just did: \n`pip install protobuf --upgrade`\n", "@thatindiandude above solution for upgrading protobuf did it for me on MacOS. Thx.\n", " I did this through a Python 3.5 virtual env created in Anaconda. \r\n@murraymack solution is not  worked for me", "I ended up using virtual env viz: https://www.tensorflow.org/get_started/os_setup#virtualenv_installation", "pip install protobuf --upgrade and restart interpret works for me.", "@murraymack I've got the same issue, although, how do you get pip to install and uninstall from the virtual environment instead of the default one? (Python 3 in my case is the default)", "as of today,(17th January 2017) this issue is still not resolved.\r\n\r\nI had it three days ago on my Windows 10 laptop and now I'm running into it again on a AWS EC2 instance.\r\n\r\nSeveral times I've tried starting over with\r\n```bash\r\nconda create -n venv35 numpy python=3.5\r\n```\r\nfollowed by\r\n```bash\r\nsource activate venv35\r\npip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl\r\n```\r\nI've tried uninstalling/installing protobuf, but since I still get the same behaviour even if protobuf is not even installed, I don't believe that is has something to do with it.\r\n\r\nThis is very frustrating. Any help is appreciated.", "according to https://github.com/tensorflow/tensorflow/issues/5343#issuecomment-257765523\r\nthe recent tensorflow releases only support CUDA 8 !\r\n\r\nI think in my case that's the reason for the incompability.", "thanks @murraymack, that worked for me.\r\n\r\nthe method where we edit /usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py didn't work for me", "@murraymack's method worked for me as well. Thank you! ", "What does it mean to be in the \"source directory\". I am also having the same issue however, I have no idea what the source directory is.\r\n", "  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\decoder.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\descriptor_database_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\descriptor_pool_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\descriptor_pool_test1_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\descriptor_pool_test2_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\descriptor_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\encoder.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\enum_type_wrapper.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\factory_test1_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\factory_test2_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\file_options_test_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\generator_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\import_test_package\\__init__.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\import_test_package\\__pycache__\\__init__.cpython-35.pyc\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\import_test_package\\__pycache__\\inner_pb2.cpython-35.pyc\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\import_test_package\\__pycache__\\outer_pb2.cpython-35.pyc\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\import_test_package\\inner_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\import_test_package\\outer_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\json_format_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\message_factory_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\message_listener.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\message_set_extensions_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\message_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\missing_enum_values_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\more_extensions_dynamic_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\more_extensions_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\more_messages_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\packed_field_test_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\proto_builder_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\python_message.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\reflection_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\service_reflection_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\symbol_database_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\test_bad_identifiers_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\test_util.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\testing_refleaks.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\text_encoding_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\text_format_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\type_checkers.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\unknown_fields_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\well_known_types.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\well_known_types_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\wire_format.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\internal\\wire_format_test.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\json_format.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\map_unittest_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\message.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\message_factory.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\proto_builder.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\pyext\\__init__.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\pyext\\__pycache__\\__init__.cpython-35.pyc\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\pyext\\__pycache__\\cpp_message.cpython-35.pyc\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\pyext\\__pycache__\\python_pb2.cpython-35.pyc\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\pyext\\cpp_message.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\pyext\\python_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\reflection.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\service.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\service_reflection.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\source_context_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\struct_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\symbol_database.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\test_messages_proto3_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\text_encoding.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\text_format.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\timestamp_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\type_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\unittest_arena_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\unittest_custom_options_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\unittest_import_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\unittest_import_public_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\unittest_mset_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\unittest_mset_wire_format_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\unittest_no_arena_import_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\unittest_no_arena_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\unittest_no_generic_services_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\unittest_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\unittest_proto3_arena_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\util\\__init__.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\util\\__pycache__\\__init__.cpython-35.pyc\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\util\\__pycache__\\json_format_proto3_pb2.cpython-35.pyc\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\util\\json_format_proto3_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\google\\\r\nprotobuf\\wrappers_pb2.py\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\protobu\r\nf-3.2.0-py2.7-nspkg.pth\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\protobu\r\nf-3.2.0.dist-info\\description.rst\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\protobu\r\nf-3.2.0.dist-info\\installer\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\protobu\r\nf-3.2.0.dist-info\\metadata\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\protobu\r\nf-3.2.0.dist-info\\metadata.json\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\protobu\r\nf-3.2.0.dist-info\\namespace_packages.txt\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\protobu\r\nf-3.2.0.dist-info\\record\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\protobu\r\nf-3.2.0.dist-info\\top_level.txt\r\n  c:\\users\\jass\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\protobu\r\nf-3.2.0.dist-info\\wheel\r\nProceed (y/n)? y\r\n  Successfully uninstalled protobuf-3.2.0\r\n\r\nC:\\>pip install tensorflow\r\nCollecting tensorflow\r\n  Using cached tensorflow-1.0.0-cp35-cp35m-win_amd64.whl\r\nCollecting protobuf>=3.1.0 (from tensorflow)\r\n  Using cached protobuf-3.2.0-py2.py3-none-any.whl\r\nRequirement already satisfied: six>=1.10.0 in c:\\users\\jass\\appdata\\local\\progra\r\nms\\python\\python35\\lib\\site-packages (from tensorflow)\r\nRequirement already satisfied: numpy>=1.11.0 in c:\\users\\jass\\appdata\\local\\prog\r\nrams\\python\\python35\\lib\\site-packages (from tensorflow)\r\nRequirement already satisfied: wheel>=0.26 in c:\\users\\jass\\appdata\\local\\progra\r\nms\\python\\python35\\lib\\site-packages (from tensorflow)\r\nRequirement already satisfied: setuptools in c:\\users\\jass\\appdata\\local\\program\r\ns\\python\\python35\\lib\\site-packages (from protobuf>=3.1.0->tensorflow)\r\nRequirement already satisfied: appdirs>=1.4.0 in c:\\users\\jass\\appdata\\local\\pro\r\ngrams\\python\\python35\\lib\\site-packages (from setuptools->protobuf>=3.1.0->tenso\r\nrflow)\r\nRequirement already satisfied: packaging>=16.8 in c:\\users\\jass\\appdata\\local\\pr\r\nograms\\python\\python35\\lib\\site-packages (from setuptools->protobuf>=3.1.0->tens\r\norflow)\r\nRequirement already satisfied: pyparsing in c:\\users\\jass\\appdata\\local\\programs\r\n\\python\\python35\\lib\\site-packages (from packaging>=16.8->setuptools->protobuf>=\r\n3.1.0->tensorflow)\r\nInstalling collected packages: protobuf, tensorflow\r\nSuccessfully installed protobuf-3.2.0 tensorflow-1.0.0\r\n\r\nC:\\>python\r\nPython 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AM\r\nD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__ini\r\nt__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__ini\r\nt__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__ini\r\nt__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\t\r\nensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Users\\JASS\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__ini\r\nt__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_st\r\narted/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> i", "plz help me in this problem ..", "I'm doing it on a windows machine. None of the solutions given above worked for me; except I haven't yet tried through conda.\r\n", "Any concrete solution we have for this problem. I am struggling with it for quite some time now\r\n"]}, {"number": 1243, "title": "Tensorflow on ARMv7", "body": "I am running Ubuntu 14.04 on a ARMv7 equipped SoC. I successfully compiled _bazel_ and installed the necessary dependencies.\n\nSome details on:\n**gcc**\n\n```\n$ g++ --version\ng++ (Ubuntu/Linaro 4.8.4-2ubuntu1~14.04.1) 4.8.4\n```\n\n**python**\n\n```\n$ python --version\nPython 2.7.6\n```\n\n**pip**\n\n```\n$ pip --version\npip 8.0.2 from /usr/local/lib/python2.7/dist-packages (python 2.7) \n```\n\nand **bazel**\n\n```\n$ bazel version\nBuild label: head (@d774022)\n```\n\nI just cloned the TensorFlow repository (commit: 225de5e336d4d241ce02652eb961cdb6e0eec847) and I run most of the TensorFlow compilation. But at a certain point I get this error:\n\n```\nroot@soc:~/tensorflow$ bazel build --verbose_failures -c opt //tensorflow/tools/pip_package:build_pip_package\nINFO: Waiting for response from Bazel server (pid 2045)...\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nINFO: From Compiling tensorflow/python/lib/core/py_func.cc:\nIn file included from third_party/py/numpy/numpy_include/numpy/ndarraytypes.h:1761:0,\n                 from third_party/py/numpy/numpy_include/numpy/ndarrayobject.h:17,\n                 from third_party/py/numpy/numpy_include/numpy/arrayobject.h:4,\n                 from tensorflow/python/lib/core/py_func.cc:19:\nthird_party/py/numpy/numpy_include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n #warning \"Using deprecated NumPy API, disable it by \" \\\n  ^\nERROR: /home/turing/tensorflow/tensorflow/core/BUILD:359:1: C++ compilation of rule '//tensorflow/core:kernel_lib' failed: gcc failed: error executing command \n  (cd /home/turing/.cache/bazel/_bazel_turing/508b67e7022bdc82335b30f64f498c49/tensorflow && \\\n  exec env - \\\n    PATH=/opt/bazel/output:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-fe78cbc4f8f9 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-fe78cbc4f8f9 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/relu_op.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/relu_op.pic.d -fPIC -c tensorflow/core/kernels/relu_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/relu_op.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4: gcc failed: error executing command \n  (cd /home/turing/.cache/bazel/_bazel_turing/508b67e7022bdc82335b30f64f498c49/tensorflow && \\\n  exec env - \\\n    PATH=/opt/bazel/output:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-fe78cbc4f8f9 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-fe78cbc4f8f9 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/relu_op.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/relu_op.pic.d -fPIC -c tensorflow/core/kernels/relu_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/relu_op.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\ngcc: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 599.857s, Critical Path: 587.04s\n```\n\nHow can I fix or workaround it?\n\nThank you.\n", "comments": ["Let me add some more details. I found out the issue:\n\n```\n$ /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-c5e90d9e764e -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-c5e90d9e764e -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/transpose_op.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/transpose_op.pic.d -fPIC -c tensorflow/core/kernels/transpose_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernel_lib/tensorflow/core/kernels/transpose_op.pic.o\nIn file included from ./tensorflow/core/kernels/transpose_op.h:19:0,\n                 from tensorflow/core/kernels/transpose_op.cc:20:\n./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:67: fatal error: eigen-eigen-c5e90d9e764e/unsupported/Eigen/CXX11/Tensor: No such file or directory\n #include \"eigen-eigen-c5e90d9e764e/unsupported/Eigen/CXX11/Tensor\"\n```\n\nThe problem is **No such file or directory\n #include \"eigen-eigen-c5e90d9e764e/unsupported/Eigen/CXX11/Tensor**. How can I solve this?\n", "I found a preliminary workaround. When the compilation fails, to generate some of the missing objects I had to add the following include directives:\n\n`-I/home/root/.cache/bazel/_bazel_turing/508b67e7022bdc82335b30f64f498c49/tensorflow/external/eigen_archive/eigen-eigen-ed4c9730b545/ -I/home/root/.cache/bazel/_bazel_turing/508b67e7022bdc82335b30f64f498c49/tensorflow/external/eigen_archive/`\n\nApparently those are missing. Can you comment about it?\n", "Hey! I had this problem while compiling on a Raspberry Pi. The sum of the answer is: your machine is running out of memory. In my case, adding a usb drive and partitioning it as swap worked fine. [Here](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md#4-install-a-memory-drive-as-swap-for-compiling) are the steps I took to add swap, which I got from the [ask Ubuntu forums](http://askubuntu.com/questions/173676/how-to-make-a-usb-stick-swap-disk).\n\nAfterward, you'll probably want to safely remove the swap disk, so you could just follow [these instructions](http://askubuntu.com/questions/616437/is-it-safe-to-delete-a-swap-partition-on-a-usb-install)\n\n---\n\nJust for further discussion- I'm not certain, but I believe the \"No such file or directory\" bug has to do with the Bazel build process creating some form of temporary file. That bug only occurs when you run the `gcc` command on it's own. I encountered the same issue [here](https://github.com/tensorflow/tensorflow/issues/445#issuecomment-195548409). `eigen-eigen-c5e90d9e764e` only exists when running through the entire Bazel process.\n", "Feel free to reopen if @samjabrahams's suggestions did not work.\n", "https://github.com/tensorflow/tensorflow/issues/1243 similar problem re-appears on Centos 7.5 w/ kernel 3.10.0-862.14.4.el7.x86_64 w/ 32 GB of memory when\r\n\r\n`git clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit pull\r\ngit checkout v1.11.0\r\nbazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...\r\n`\r\nleads to\r\n\r\n`ERROR: /home/user/tensorflow/tensorflow/python/BUILD:5553:1: C++ compilation of rule '//tensorflow/python:framework/fast_tensor_util.so' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 49 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nIn file included from bazel-out/host/genfiles/external/local_config_python/numpy_include/numpy/ndarrayobject.h:18:0,\r\n                 from bazel-out/host/genfiles/external/local_config_python/numpy_include/numpy/arrayobject.h:4,\r\n                 from bazel-out/host/genfiles/tensorflow/python/framework/fast_tensor_util.cpp:581:\r\nbazel-out/host/genfiles/external/local_config_python/numpy_include/numpy/ndarraytypes.h:1821:36: fatal error: npy_1_7_deprecated_api.h: No such file or directory\r\n #include \"npy_1_7_deprecated_api.h\"\r\n                                    ^\r\ncompilation terminated.\r\nINFO: Elapsed time: 71.255s, Critical Path: 14.15s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 545 processes: 545 processwrapper-sandbox.\r\nFAILED: Build did NOT complete successfully\r\n`"]}, {"number": 1242, "title": "Mac PIP 0.7.1 breaks protobuf installation", "body": "I just upgraded to the latest 0.7.1 on Mac OSX using the PIP package, and I can no longer run TF. When importing in Python 2.7 it reports that protobuf cannot be found, even though during the pip install it supposedly upgraded protobuf to 3.0.0b2.\n", "comments": ["Some finagling and reinstalling (of both protobuf and tensorflow) seems to have done the trick.\n", "Yes, earlier wheels were doing bad things to the protobuf installation, and the only way to recover is to uninstall to 'start from scratch.'\n", "Maybe worth noting this in the installation instructions so that people uninstall protobuf.\n"]}, {"number": 1241, "title": "Sparse matrix multiplication?", "body": "Hi There,\n\nIs there currently support for sparse matrix multiplication in tensorflow?\n\nI found this post:\nhttp://stackoverflow.com/questions/34030140/is-sparse-tensor-multiplication-implemented-in-tensorflow\n\nBut it seems to indicate that one must first convert the sparse matrix into a dense matrix before running the multiplication. \n", "comments": ["This was recently added.  See sparse_ops.sparse_tensor_dense_matmul.  The first input should be a 2-D SparseTensor and the second a regular dense tensor (matrix) of the same dtype.\n\nBackprop coming soon (at least into the dense tensor, no backprop into the SparseTensor is currently planned).\n", "I'm closing this issue unless there's missing functionality in the op I mentioned.\n", "Use sparse_ops.sparse_tensor_dense_matmul.  Does this do what you need?\n", "Woah awesome! Thanks @ebrevdo . Had no idea this recently made it in. I think this will fit what I need, and will try this out tomorrow.\n", "I don't think this is sufficient to close this issue. The stackoverflow post is talking about the general case where you want something like `sparse_matmul(sp_a, sp_b)`.\n\nI currently have that problem: I want to calculate a graph Laplacian given a sparse oriented edge matrix, D. Every row is an edge between two nodes, with -1 for node_i and 1 for node_j; the graph Laplacian is then (D'.D). My graph in this case has something like a million nodes but only about 3 million edges, so the overall sparse D representation has about 6 million entries. This is a piece of cake to calculate using something like scipy's sparse matrix multiplication, but converting it to tensorflow requires creating a dense 1M x 1M matrix.\n", "Feel free to open a new issue asking specifically for (SparseTensor, SparseTensor) matmul.\n", "I also have similar problem to @tansey.\r\nI know that there is a matrix solver using LU decomposition in Tensorflow, but it's supported to dense matrices only. I can calculate very sparse L, U, Linv, Uinv matrices (as a SparseTensor type) but there's no way to use it..\r\nUsing dense version for any of those blows up my memory (I'm working currently working on graphs with several million nodes).", "I also need sparse@sparse matrix multiplication. Even with a_is_sparse and b_is_sparse set to true TF still load n^2 dense matrices into memory.\r\nhttps://github.com/tensorflow/tensorflow/issues/3246#issuecomment-231807152\r\n\r\nit seems possible theoretically but I am unsure, are there any updates pertaining to this? Thanks."]}]