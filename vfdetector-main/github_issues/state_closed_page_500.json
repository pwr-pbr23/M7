[{"number": 38773, "title": "Allow name argument of tf.name_scope to be a tf.string", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes (if it's not too difficult/time-taking)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, something like the following doesn't work since `name` in `tf.name_scope` must be a Python `str`:\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nfor i in tf.range(4):\r\n    tag = tf.strings.format('tag{}', i + 1)\r\n    with tf.name_scope(tag):\r\n        tf.summary.scalar('value', tf.constant(i**4))\r\n```\r\n\r\n**Will this change the current API? How?**\r\nNot significantly.\r\n\r\n**Who will benefit with this feature?**\r\nUsers of autograph who want to use a Tensor's value in `tf.name_scope`.\r\n\r\n**Any Other info.**\r\n", "comments": ["This is not possible. Names in name_scope have to be statically known at graph build time, but string tf tensors are dynamic and their value is only known at graph execution time.", "Thanks for your response, @alextp! How would you suggest I deal with use-cases like the one mentioned in this issue (names based on variables from a for-loop)? Is `tf.py_function` the best way to approach this?", "Use \"for i in range\", not \"for i in tf.range\" to use a python for loop and statically unroll the graph.", "Can you explain what you mean by statically unrolling the graph?", "Creating a graph that looks like loop(0), loop(1), ..., loop(n) as opposed\nto one that looks like for i in range(n): loop(i)\n\nOn Mon, Apr 27, 2020 at 7:48 PM Sumanth Ratna <notifications@github.com>\nwrote:\n\n> Can you explain what you mean by statically unrolling the graph?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38773#issuecomment-620345338>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNPXAZJ4RDCAFIIBUTROY7WDANCNFSM4MNV3WLQ>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 38771, "title": "Update `recognize_commands.cc` in `micro_speech` example", "body": "Improve the algorithm that finds the highest scoring category.", "comments": ["While this does slightly improve the micro_speech post processing algorithm, the benefits will be extremely minor. Unfortunately, merging a pull request is quite an involved process. I do not feel this confers enough performance improvement to warrant merging. Please re-open if I am mistaken or if this affects correctness."]}, {"number": 38769, "title": "Update CI build scripts for TF Lite micro", "body": "Put `${ROOT_DIR}` in quotes in order to avoid the error: `cd : too many arguments`.\r\n\r\nThis happens if `${ROOT_DIR}` contains one or more spaces (for example when\r\ntensorflow is downloaded into a directory whose name contains one or more spaces)\r\nand so enclosing it between quotes solves the issue.\r\nThe same fix has been applied to some others bash variables in the CI build scripts.", "comments": ["You are welcome ! Glad to improve TF :thumbsup:"]}, {"number": 38768, "title": "TF 1.15.0  savedModel running in  TF 2.1.0", "body": "OS Platform and Distribution : macOS Catalina 10.15.3\r\n\r\nTensorFlow installed from : binary\r\n\r\nTensorFlow version : 1.15.0\r\n\r\nPython version: 3.7.3\r\n\r\nHi All,\r\n\r\nI have general question (so no code is provided) . Are below 2 questions possible :\r\n\r\n1. Can we run TF 1.15.0 savedModel (which has feature transformation ported as part of savedModel) in TF 2.1.0 directly ? \r\n\r\n2. Is there any way to convert TF 1.15.0 savedModel (which has feature transformation ported as part of savedModel) directly to TF 2.1.0 ? \r\n\r\n\r\n", "comments": ["@17patelumang \r\n\r\nPlease, refer this [link](https://www.tensorflow.org/guide/migrate#saved_models_compatibility) and see if it helps you.\r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "we can close this thank you ", "@17patelumang \r\n\r\nThanks for the confirmation. I am closing this issue"]}, {"number": 38767, "title": "ConvRNN2D.build() - TypeError: can only concatenate list (not \"tuple\") to list", "body": "**System information**\r\n- Have I written custom code: Yes, see code snippet below\r\n- OS Platform and Distribution: `Ubuntu 19.10`\r\n- TensorFlow installed from: `pip install tensorflow==2.1`\r\n- TensorFlow version: `v2.1.0-rc2-17-ge5bf8de 2.1.0`\r\n- Python version: `3.7.5`\r\n- CUDA/cuDNN version: `CUDA 10.1`\r\n- GPU model and memory: `GeForce GTX 1080 Ti 11GB`\r\n\r\n**Describe the current behavior**\r\nConvRNN2D fails to build when restoring from config.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nfrom tensorflow.python.keras.layers.convolutional_recurrent import (\r\n    ConvRNN2D,\r\n    ConvLSTM2DCell,\r\n)\r\n\r\ncell = ConvLSTM2DCell(filters=32, kernel_size=(3, 3))\r\nrnn = ConvRNN2D(cell)\r\nrnn = ConvRNN2D.from_config(rnn.get_config())\r\nrnn.build(input_shape=(1, 10, 15, 15, 16))\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/kevin/dev/tf_bug.py\", line 9, in <module>\r\n    rnn.build(input_shape=(1, 10, 15, 15, 16))\r\n  File \"/home/kevin/dev/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\", line 306, in wrapper\r\n    output_shape = fn(instance, input_shape)\r\n  File \"/home/kevin/dev/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional_recurrent.py\", line 244, in build\r\n    self.cell.build([step_input_shape] + constants_shape)\r\nTypeError: can only concatenate list (not \"tuple\") to list\r\n```\r\n", "comments": ["There seem to be multiple issues here. \r\nApart from the obvious list-tuple concatenation, the corresponding [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional_recurrent.py#L243) shouldn't be reached at all.\r\nI presume during `build()` `_num_constants` is expected to be `None` or a value > 0, but is actually `0`. This might be due to the following:\r\n`RNN.__init__()` sets `self._num_constants=0` and `ConvRNN2D.__init__()` then overwrites it with `self._num_constants=None`. However, when instantiating with `from_config()` `self._num_constants` [defaults](https://github.com/tensorflow/tensorflow/blob/b9edec000c94761fc52e2ce38efa6385fff45f42/tensorflow/python/keras/layers/recurrent.py#L983) to `0`.", "Was able to reproduce the issue with TF v2.1, [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/94f0640515455b1026a6208608f5dc57/38767.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/f7405682d8e6f1a1ff94caad1bd1944e/38767-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Was able to reproduce the issue with TF nightly version(`2.4.0-dev20200805`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/ca6d33d2cd83437d12c4ca4e7034988e/untitled222.ipynb).Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210526, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/a124252444a1118ac18b8e2996da2cef/38767.ipynb). Thanks!", "Hi @kevinkepp !\r\nWe are checking to see whether you still need help in this issue . Have you tried building your model using [RNN documentation ](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN)from latest version (2.7) yet? Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38767\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38767\">No</a>\n"]}, {"number": 38765, "title": "tf.keras.callbacks.ProgbarLogger(count_mode='samples') does not work", "body": "**System information**\r\n(I'm sorry, this is my first time writing an issue.)\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below): 2.2.0-rc3\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nI noticed from [this commit](https://github.com/tensorflow/tensorflow/commit/10666c59dd4858645d1b03ce01f4450da80710ec) that the default behavior of `ProgbarLogger` has been changed to always show the number of `'steps'` instead of `'samples'`. I was curious and tried to manually use a `ProgbarLogger` callback argument to `Model.fit()` with `count_mode='samples'` instead, but then an error showed up.\r\n\r\n**Describe the expected behavior**\r\n I expected it to work normally as with the older version of TensorFlow?\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\n# Assuming we use mnist data set\r\nmodel = Sequential([\r\n    Flatten(),\r\n    Dense(128, activation='relu'),\r\n    Dense(10)\r\n])\r\n\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nmodel.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, callbacks=[tf.keras.callbacks.ProgbarLogger('steps')])\r\n```\r\n\r\n**Other info / logs**\r\nOn my local machine (TF 2.1), this is the default behavior:\r\n```\r\nEpoch 1/5\r\n16500/16500 [==============================] - 3s 207us/sample - loss: 0.4841 - accuracy: 0.8584\r\nEpoch 2/5\r\n16500/16500 [==============================] - 2s 95us/sample - loss: 0.2430 - accuracy: 0.9276\r\nEpoch 3/5\r\n...\r\n```\r\n\r\nOn Google Colab (TF 2.2), I got this when I tried my code:\r\n```\r\n0/Unknown - 1s 0s/sample - loss: 0.3902 - accuracy: 0.8912\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py:595: RuntimeWarning: divide by zero encountered in log10\r\n  numdigits = int(np.log10(self.target)) + 1\r\n\r\n---------------------------------------------------------------------------\r\nOverflowError                             Traceback (most recent call last)\r\n<ipython-input-51-834d420b09ab> in <module>()\r\n      8 model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\r\n      9 \r\n---> 10 model.fit(x_train, y_train, callbacks=[tf.keras.callbacks.ProgbarLogger('samples')])\r\n\r\n5 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    877           epoch_logs.update(val_logs)\r\n    878 \r\n--> 879         callbacks.on_epoch_end(epoch, epoch_logs)\r\n    880         if self.stop_training:\r\n    881           break\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n    363     logs = self._process_logs(logs)\r\n    364     for callback in self.callbacks:\r\n--> 365       callback.on_epoch_end(epoch, logs)\r\n    366 \r\n    367   def on_train_batch_begin(self, batch, logs=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n    892 \r\n    893   def on_epoch_end(self, epoch, logs=None):\r\n--> 894     self._finalize_progbar(logs)\r\n    895 \r\n    896   def on_test_end(self, logs=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _finalize_progbar(self, logs)\r\n    933       self.progbar.target = self.seen\r\n    934     logs = logs or {}\r\n--> 935     self.progbar.update(self.seen, list(logs.items()), finalize=True)\r\n    936 \r\n    937 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in update(self, current, values, finalize)\r\n    593 \r\n    594       if self.target is not None:\r\n--> 595         numdigits = int(np.log10(self.target)) + 1\r\n    596         bar = ('%' + str(numdigits) + 'd/%d [') % (current, self.target)\r\n    597         prog = float(current) / self.target\r\n\r\nOverflowError: cannot convert float infinity to integer\r\n```\r\n", "comments": ["@dfangs \r\n\r\nLooks like code is incomplete. Request you to provide colab link or simple standalone code to reproduce the issue reported here.It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\n\r\nHere is the link to a simple [colab](https://colab.research.google.com/drive/19gsTsz94MpW5r03NwgWD3Kl1iPgWFFdT) that I made, thank you.", "@dfangs as noted [here](https://github.com/tensorflow/tensorflow/issues/38618#issuecomment-617907735) can you add verbose to model.fit() call. this should work. ", "@goldiegadde But that would default to `count_mode='steps'`. The thing is I would like the progress bar to display the number of samples (instead of steps), which is not the default behavior per the [commit](https://github.com/tensorflow/tensorflow/commit/10666c59dd4858645d1b03ce01f4450da80710ec) I mentioned above. Since apparently there is no way to configure that, I tried to manually insert `tf.keras.callbacks.ProgbarLogger(count_mode='samples')` into the `callbacks` parameter. I wish there was an option to configure that.\r\n\r\nI looked at the link you gave me but it seems that the problem wasn't resolved. Hopefully this makes it clear.", "@dfangs \r\n\r\nI tried in colab with TF nightly version(`2.4.0-dev20200916`) and i am not seeing any issue.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/4acd5f63414cf559763431c971d3e6f2/untitled82.ipynb).Please, verify once and close the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38765\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38765\">No</a>\n"]}, {"number": 38764, "title": "tpu lstm", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@samoalfred \r\nplease provide a code snippet to reproduce the issue reported here,Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38763, "title": "Large overhead when calling custom pure python code in training loop ", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 \r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI am training a deep RL agent using tensorflow on a custom environment and have noticed that calling the environment.step method in my training loop is much slower than just calling it outside. Specifically, if I time how long it takes to run the environment.step inside my agent.train method, it takes ~50 times longer than when I just run environment.step by itself. \r\n\r\n**Describe the expected behavior**\r\nThere shouldn't be any overheads running my custom environment.step method inside agent.train\r\n\r\n**Standalone code to reproduce the issue**\r\ndownload \r\nhttps://github.com/ronan-keane/havsim/tree/DL3 \r\nnavigate to folder, pip install havsim . \r\nrun traintest.py in scripts/meng assignments/control 1/ folder \r\n\r\n**Other info / logs** \r\nI tested gym's cartpole environment and it doesn't have the issue. Since cartpole and my custom environment are both implemented in pure python it makes me think the issue is that the custom environment has attributes which are dictionaries. \r\n", "comments": ["@ronan-keane \r\n\r\nWill it be possible to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "The issue was that the output from the neural net which was inputted to the custom python code was being converted from a tensor to a tf.float32. This would then cause the python code to convert everything to tensor, and all the math operations in the pure python code would then be acting on the newly converted tensors. This added the huge overheads. Issue was fixed by just converting the output of the NN to a regular python float. "]}, {"number": 38761, "title": "Matching Keras Backend functions", "body": "`tensorflow.keras.backend` is missing a function `logsumexp`\r\n\r\nI was orginally just going to add the function from `tf.math` but it turns out that `tensorflow.keras.backend` is machine-generated. When I go to `create_python_api.py` to see if I could add it I was hit with nontrivial code on how this file finds what should be included as imports for the keras backend.\r\n\r\nIs there anything I could do to get a change like this added to the project or was this already considered?\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.*\r\n- Are you willing to contribute it (Yes/No): yes\r\n", "comments": ["@arose13 Sorry for the late response. `logsumexp` method was deprecated from `keras.backend`. Please use it the same function under TF. Please check [here](https://www.tensorflow.org/api_docs/python/tf/math/reduce_logsumexp) for more information. Thanks\r\n\r\n ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38760, "title": "Problem with read and get batch from 2d array tfrecords dataset", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/load_data/tfrecord#tfrecord_files_using_tfdata\r\n\r\n## Description of issue (what needs changing):\r\nProblem with read and get batch from 2d array tfrecords dataset\r\n\r\n### Clear description\r\n\r\nHello. I use Tensorflow 2.0 version. I have some problems with reading Tfrecords file when get batch.\r\n\r\nFirst, this is my read_tfrecords.py file.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nfrom glob import glob\r\nimport numpy as np\r\n\r\n\r\ndef serialize_example(batch, list1, list2):\r\n    filename = \"./train_set.tfrecords\"\r\n    writer = tf.io.TFRecordWriter(filename)\r\n\r\n    for i in range(batch):\r\n        feature = {}\r\n        feature1 = np.load(list1[i])\r\n        feature2 = np.load(list2[i])\r\n        print('feature1 shape {} feature2 shape {}'.format(feature1.shape, feature2.shape)) \r\n        feature['input'] = tf.train.Feature(float_list=tf.train.FloatList(value=feature1.flatten()))\r\n        feature['target'] = tf.train.Feature(float_list=tf.train.FloatList(value=feature2.flatten()))\r\n\r\n        features = tf.train.Features(feature=feature)\r\n        example = tf.train.Example(features=features)\r\n        serialized = example.SerializeToString()\r\n        writer.write(serialized)\r\n        print(\"{}th input {} target {} finished\".format(i, list1[i], list2[i]))\r\n\r\n\r\n\r\nlist_inp = sorted(glob('./input/2d_magnitude/*'))\r\nlist_tar = sorted(glob('./target/2d_magnitude/*'))\r\n\r\n\r\nprint(len(list_inp))\r\nserialize_example(len(list_inp), list_inp, list_tar)\r\n```\r\n\r\nMy input and target shapes are 2d array (Material of dataset is spectrogram). Therefore, my **Tfrecords** file includes two features likes ```[number_of_dataset, x, y]```. About 100,000 dataset was successfully saved as **Tfrecords** file.\r\n\r\nAnd I have problem when I read **Tfrecords** file to get batch. This is my code ```read_tfrecords.py```:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\n\r\nshuffle_buffer_size = 50000\r\nbatch_size = 10\r\nrecord_file = '/data2/dataset/tfrecords/train_set.tfrecords'\r\n\r\nraw_dataset = tf.data.TFRecordDataset(record_file)\r\nprint('raw_dataset', raw_dataset) # ==> raw_dataset <TFRecordDatasetV2 shapes: (), types: tf.string>\r\n\r\nraw_dataset = raw_dataset.repeat()\r\nprint('repeat', raw_dataset) # ==> repeat <RepeatDataset shapes: (), types: tf.string>\r\n\r\nraw_dataset = raw_dataset.shuffle(shuffle_buffer_size)\r\nprint('shuffle', raw_dataset) # ==> shuffle <ShuffleDataset shapes: (), types: tf.string>\r\n\r\nraw_dataset = raw_dataset.batch(batch_size, drop_remainder=True)\r\nprint('batch', raw_dataset) # ==> batch <BatchDataset shapes: (10,), types: tf.string>\r\n\r\nraw_example = next(iter(raw_dataset)) \r\n\r\nparsed = tf.train.Example.FromString(raw_example.numpy()) # ==> read_tfrecords.py:25: RuntimeWarning: Unexpected end-group tag: Not all data was converted\r\n\r\nprint('parsed', parsed) # ==> ''\r\n\r\ninput = parsed.features.feature['input'].float_list.value\r\nprint('input', input) # ==> []\r\ntarget = parsed.features.feature['target'].float_list.value\r\nprint('target', target) # ==> []\r\n```\r\n\r\nHere are results from code:\r\n```\r\nraw_dataset <TFRecordDatasetV2 shapes: (), types: tf.string>\r\nrepeat <RepeatDataset shapes: (), types: tf.string>\r\nshuffle <ShuffleDataset shapes: (), types: tf.string>\r\nbatch <BatchDataset shapes: (10,), types: tf.string>\r\nread_tfrecords.py:25: RuntimeWarning: Unexpected end-group tag: Not all data was converted\r\n  parsed = tf.train.Example.FromString(raw_example.numpy())\r\nparsed\r\ninput []\r\ntarget []\r\n```\r\nAs a result, I wonder how I get the batch from Tfrecords file to train. \r\n**read_tfrecords.py:25: RuntimeWarning: Unexpected end-group tag: Not all data was converted**\r\nCould you give advice? Thank you very much.\r\n\r\n\r\n### Usage example\r\nMaybe...\r\n\r\n```\r\nraw_dataset = tf.data.TFRecordDataset(record_file)\r\n\r\nraw_dataset = raw_dataset.repeat()\r\n\r\nraw_dataset = raw_dataset.shuffle(shuffle_buffer_size)\r\n\r\nraw_dataset = raw_dataset.batch(batch_size, drop_remainder=True)\r\n\r\nraw_example = next(iter(raw_dataset)) \r\n\r\nparsed = tf.train.Example.FromString(raw_example.numpy())\r\n\r\ninput = parsed.features.feature['input'].float_list.value\r\ntarget = parsed.features.feature['target'].float_list.value\r\n```\r\n", "comments": ["@kaen2891 \r\ni ran the code shared by you and face [this error](https://colab.sandbox.google.com/gist/Saduf2019/d425510d88e6d4f7670e3d4e2f1e0969/untitled152.ipynb), if possible please share a colab gist of the error faced for us to analyse", "@Saduf2019 \r\n\r\nI uploaded the code [here](https://colab.research.google.com/gist/kaen2891/54b4a21ddfd2da4f69e3b6a22b2b6678/untitled152.ipynb).\r\n\r\nCould you give me an advice? Thanks.", "@kaen2891 \r\ncan you please share the file uploaded in the cell \"from google colab import files\r\nexample_file = files.upload()\" for me to replicate the issue in the latest version.", "@Saduf2019 \r\n\r\nI don't know how to share the file with you. Instead of, I will share the link via [GoogleDrive](https://drive.google.com/file/d/1zNLXaY1arqLyMVxVV8xdKbQQe_1gQRY_/view?usp=sharing).\r\n\r\nThanks a lot.", "@kaen2891 \r\nI am unable to access the data shared, it says no data/preview available.", "@Saduf2019 \r\n\r\nOkay. I changed code and now you can load the Tfrecords file in [here](https://colab.research.google.com/gist/kaen2891/54b4a21ddfd2da4f69e3b6a22b2b6678/untitled152.ipynb)\r\n", "@Saduf2019 \r\n\r\nThe **Tfrecords file** that I made as an example is a conversion of datasets. The shape of ```inp``` is **(100, 201, 22)** and the shape of ```tar``` is **(100,201,23)**. Actually, it's same shape and format of the speech dataset which I actually have. For convenience, it is created with ```np.random.uniform(0,1,(100,201, 22))```. Would you check it please?", "I am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/af6025c35b94c42c0c08160619ac0feb/38760.ipynb). Thanks!", "@Saduf2019 \r\nI don't know what you mean. Because the shared colab code is not changing. So should I just wait for this problem? ", "@aaudiber can you please take a look? thanks", "@kaen2891, `tf.train.Example.FromString` parses a single example, not a batch of examples. See https://www.tensorflow.org/tutorials/load_data/tfrecord#reading_a_tfrecord_file for how to read `TFRecord` files with tf.data", "@aaudiber , Okay. Then how can I read ```TFrecord``` to get batch? It's pretty hard to find information when I read ```TFrecord``` to get batch. Could you change the colab [code](https://colab.research.google.com/gist/Saduf2019/af6025c35b94c42c0c08160619ac0feb/38760.ipynb)?", "After parsing, you can call `Dataset.batch(batch_size)` to put the examples in batches. Did the link I shared above help?", "@aaudiber, No. Not yet.\r\nCould you explain in more detail and kindness? In the example, it is not possible to put a batch. And I can't understand your words. When I add your comment to my code, I get the following error:\r\n```tensorflow.python.framework.errors_impl.InvalidArgumentError: Key: input. Can't parse serialized Example.```\r\nI wrote code with this [reference](https://www.tensorflow.org/tutorials/load_data/tfrecord#read_the_tfrecord_file).\r\n\r\nI'll share my code via [Colab.](https://colab.research.google.com/gist/kaen2891/54b4a21ddfd2da4f69e3b6a22b2b6678/untitled152.ipynb) Could you solve this? In fact, if the data is small, I can just use directly, but in my case it is over 700GB. So I must use ```TFrecord``` to run the model. Please help me.", "Please read https://www.tensorflow.org/tutorials/load_data/tfrecord#reading_a_tfrecord_file to understand how to read TFRecords with tf.data. Is there something specific in there that doesn't make sense to you?", "@aaudiber , when I followed your comment, it doesn't work. Same error message. Could you check error [message?](https://colab.research.google.com/gist/kaen2891/54b4a21ddfd2da4f69e3b6a22b2b6678/untitled152.ipynb)", "It looks like you are using `FixedLenFeature` to parse your features, but the features are sequences not scalars, so you need to use `FixedLenSequenceFeature` instead.", "@aaudiber,\r\nThanks to you, I solve my problem. It worked as you said using ```FixedLenSequenceFeature```. Thanks for your kindness.\r\n\r\nOn the other hand, is it possible to change ```complex ndarray``` to ```tfrecords``` file? If possible, what kinds of format do I use for change? tf.float32? ", "If your data is complex numbers, you can use the [tf.complex64](https://www.tensorflow.org/api_docs/python/tf#complex64) Tensorflow type", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38760\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38760\">No</a>\n"]}, {"number": 38759, "title": "[ROCm] Fix for a build failure on the ROCm platform - 200420 ", "body": "Please see individual commit message for details.\r\n\r\n/cc @chsigg @cheshire @nvining-work ", "comments": ["@chsigg gentle ping"]}, {"number": 38758, "title": "installation issue", "body": "\r\n*Traceback (most recent call last):\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\girid\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nthis is showin up when i was tryi'n to import the tensorflow", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "Previously I got this error when I was trying to install TensorFlow 1.15.0 but somehow Tensorboard 2.1.0 was installed on my conda environment.  Then I removed all previously installed TensorFlow related packages and reinstalled TensorFlow 1.15.0 ...and that worked for me.", "What is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38758\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38758\">No</a>\n"]}, {"number": 38757, "title": "[ROCm] Updating ROCm CI scripts to skip //tensorflow/lite/...", "body": "tensorflow CI scritps already skip tests under `//tensorflow/lite`. \r\n\r\nThis PR is to update the ROCM CI scripts to do the same.\r\n\r\n\r\n/cc @cheshire @chsigg @nvining-work", "comments": ["Will manually import", "Oh, cannot manually import. Needs to be rebased at head as master touched one the files too and import code cannot work in presence of such conflicts.", "do you need me to rebase the PR?", "@deven-amd yes please rebase.", "@mihaimaruseac @rthadur \r\n\r\nrebase done, please re-approve and merge. thanks"]}, {"number": 38755, "title": "Add gitpod config", "body": "this commit adds support for Gitpod.io, a free automated\ndev environment that makes contributing and generally working on GitHub\nprojects much easier. It allows anyone to start a ready-to-code dev\nenvironment for any branch, issue and pull request with a single click.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38755) for more info**.\n\n<!-- need_sender_cla -->", "@DZERK17 Can you please check build failures. Thanks!", "@DZERK17 Can you please sign CLA? Thanks!", "I don't know it will be so much useful cause the `bazel` cache it is not available in the container [since 2016](https://github.com/tensorflow/tensorflow/commit/8a724211a9d4871338afe874276abf1d358f799b)\r\nAll the bazel cache for the Docker image builds currently are \"internal\" and remote. \r\nSo in the current status will be hard to have a quite ready dev container for quickly online editing and run change. \r\nThe build require many hours without an available cache.\r\n\r\nEDIT: It is not about GitPod we will have the same problem also with similar solutions like [Github Codespaces](https://github.com/features/codespaces/) or local sparse development with docker images.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 38753, "title": "Update documentation of micro_speech example", "body": "Hello Tensorflowers,\r\nas this is my first contribution, first I would like to say a big thank to all the TensorFlow contributors that every day take care about improving it and fixing issues.\r\n\r\nThis pull request updates the documentation related to `micro_speech` example. Further info is also provided in the commit messages.\r\n\r\nBest regards,\r\nBiagio.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38753) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38753) for more info**.\n\n<!-- ok -->", "@biagiom Can you please resolve conflicts? Thanks!", "Done. I also updated the alignment of the table in the **Trained Models** section ([micro_speech/train/README.md](https://github.com/tensorflow/tensorflow/pull/38753/commits/d0c4a0489c8669e4a9c652e839694b074b5a900a#diff-371c67872e41288cb2aefe6faf6f4688))", "@biagiom Still, conflicts are appearing. Can you please resolve those? Thanks!", "Hi @gbaned, conflicts solved. Thanks and sorry for the delay !"]}, {"number": 38752, "title": "Add return statement to the micro-test framework.", "body": "Return kTfLiteOk/kTfLiteError depending on if the test succeeded or not.", "comments": ["@petewarden Ping for review!", "@jenselofsson can you please check below error \r\n\r\n`tensorflow/lite/micro/examples/magic_wand/gesture_predictor_test.cc:66:1: error: use of undeclared identifier 'kTfLiteOk'\r\nTF_LITE_MICRO_TESTS_END\r\n^\r\n.//tensorflow/lite/micro/testing/micro_test.h:88:12: note: expanded from macro 'TF_LITE_MICRO_TESTS_END'\r\n    return kTfLiteOk;                                          \\\r\n           ^/tensorflow/lite/micro/examples/magic_wand/gesture_predictor_test.cc:66:1: error: use of undeclared identifier 'kTfLiteError'\r\n.//tensorflow/lite/micro/testing/micro_test.h:91:12: note: expanded from macro 'TF_LITE_MICRO_TESTS_END'\r\n    return kTfLiteError;                                       \\\r\n           ^\r\n2 errors generated.`", "@njeffrie If I recall correctly, it's more that it's nice to be able to use the return value to see if the test failed or not instead of the logs, although what you mention is a bonus."]}, {"number": 38751, "title": "[Keras Application]May a bug under gradientTape?", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindow 10 1804\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNone\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nevery 2.1.0 cpu/gpu\r\n- Python version:\r\n3.7.7\r\n- Bazel version (if compiling from source):None\r\n- GCC/Compiler version (if compiling from source):None\r\n- CUDA/cuDNN version:10\r\n- GPU model and memory: MSI GTX 1070 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nunknown 2.1.0\r\ninstall from conda\r\n\r\n**Describe the current behavior**\r\n tf.keras.application work under GradientTape may crash by:\r\nLookupError: No gradient defined for operation 'IteratorGetNext' (op type: IteratorGetNext)\r\n\r\n**Describe the expected behavior**\r\nMay GradientTape need pass Keras Application's Model?\r\nOr Keras Application's Model could be set distrainable?\r\n\r\n**Standalone code to reproduce the issue**\r\nsimple\r\n```python\r\ndef vgg16_encode(vgg_input_img_tensor):\r\n    vgg16_model = VGG16(weights='imagenet', include_top=False)\r\n    vgg_input_img = preprocess_input(vgg_input_img_tensor)\r\n    vgg16_output = vgg16_model.predict(vgg_input_img)\r\n    return vgg16_output\r\n\r\nwith tf.GradientTape() as tape:\r\n    //use vgg16_encode here\r\n```\r\n**Other info / logs**\r\ntree:\r\n  File \"C:/233/test.py\", line 29, in vgg16_encode\r\n    vgg16_output = vgg16_model.predict(vgg_input_img)\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1013, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 498, in predict\r\n    workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 475, in _model_iteration\r\n    total_epochs=1)\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 638, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1697, in _call_flat\r\n    forward_function, args_with_tangents = forward_backward.forward()\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1423, in forward\r\n    self._inference_args, self._input_tangents)\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1185, in forward\r\n    self._forward_and_backward_functions(inference_args, input_tangents))\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1379, in _forward_and_backward_functions\r\n    outputs, inference_args, input_tangents)\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 890, in _build_functions_for_outputs\r\n    src_graph=self._func_graph)\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\detaining-master\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\", line 623, in _GradientsHelper\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'IteratorGetNext' (op type: IteratorGetNext)\r\n\r\nProcess finished with exit code 1\r\n\r\n", "comments": ["@fly3366 \r\ncould you please refer to these issues with similar error:\r\n#33135 #[link1](https://www.gitmemory.com/issue/tensorflow/tensorflow/33135/539915524)\r\n\r\nthe code shared is incomplete, please provide with simple standalone code for us to replicate the issue.", "like this...\r\n```python\r\nimport tensorflow as tf\r\n    from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\r\n\r\n\r\n    def vgg16_encode(vgg_input_img_tensor):\r\n        vgg16_model = VGG16(weights='imagenet', include_top=False)\r\n        vgg_input_img = preprocess_input(vgg_input_img_tensor)\r\n        vgg16_output = vgg16_model.predict(vgg_input_img)\r\n        return vgg16_output\r\n\r\n\r\n    with tf.GradientTape() as tape:\r\n        vgg16_encode(tf.Variable(tf.ones([1, 256, 256, 3])))\r\n\r\n```", "@Saduf2019 ", "@fly3366 \r\ni ran the code shared by you and face [this error](https://colab.sandbox.google.com/gist/Saduf2019/f89499ad4a9baf36d2c13ac090ef2a70/untitled143.ipynb)", "@Saduf2019 \r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\r\n\r\n\r\ndef vgg16_encode(vgg_input_img_tensor):\r\n    vgg16_model = VGG16(weights='imagenet', include_top=False)\r\n    vgg_input_img = preprocess_input(vgg_input_img_tensor)\r\n    vgg16_output = vgg16_model.predict(vgg_input_img)\r\n    return vgg16_output\r\n\r\n\r\nwith tf.GradientTape() as tape:\r\n    vgg16_encode(tf.Variable(tf.ones([1, 256, 256, 3])))\r\n```\r\nsorry, some spaces may be add when markdown's covert", "i am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/542e9fd6330a656c7c372befa001c57a/untitled145.ipynb)", "@fly3366 Please take a look at this issue [here](https://stackoverflow.com/questions/50784337/gradients-of-operations-done-in-the-tensorflow-data-dataset-map-function) and I don't think you are using gradient tape the right way and hence the error.", "@gowthamkpr \r\nThanks. Use `watch_accessed_variables=False` solved it.\r\n```pyhton\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\r\n\r\n\r\ndef vgg16_encode(vgg_input_img_tensor):\r\n    vgg16_model = VGG16(weights='imagenet', include_top=False)\r\n    vgg_input_img = preprocess_input(vgg_input_img_tensor)\r\n    vgg16_output = vgg16_model.predict(vgg_input_img)\r\n    return vgg16_output\r\n\r\n\r\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\r\n    vgg16_encode(tf.Variable(tf.ones([1, 256, 256, 3])))\r\n    // Another model\r\n    tape.watch(model.trainable_variables)\r\n```\r\nDo you have plan to add some like `unwatch(variables)`?\r\nSome keras's Application may need disable watching default.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38751\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38751\">No</a>\n"]}, {"number": 38750, "title": "Issue installing tensor flow python", "body": "Please I have issues installing tensorflow in python using pip. I tried\r\n```shell\r\npip install tensorflow\r\n```\r\nAnd \r\n \r\n```shell\r\npip3 install tensorflow \r\n```\r\n\r\n**System information**\r\n- OS Platform Linux Ubuntu 18):\r\n- Mobile device (Android (using termux) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version: 3\r\n- Installed using virtualenv? No \r\npip? Yes \r\nconda?: No\r\nI am using android \r\n\r\nLog : \r\n\r\n> Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n", "comments": ["@JosiasAurel \r\n\r\nRequest you to fill[ issue template ](https://github.com/tensorflow/tensorflow/issues/new/choose)\r\n\r\nYou are using 64-bit version of Python or 32 bit version?.Please, refer [link1](https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip).Also, let us know which version of Tensorflow you are using?.Please, follow instruction in[ Tensorfow website](https://www.tensorflow.org/install/pip).Thanks!\r\n\r\n               ", "I ran into the same issue, I am using Python 3.7.4 (64 bit), and the latest pip 20.0.2", "@JosiasAurel \r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "In termux I used \n```shell\npip install tensorflow\n```\nThe error stated above occurred\n\nSame when I used in Ubuntu\n```shell\npip3 install tensorflow\n```\nWindows I used the same command \n```shell\npip install tensorflow\n```", "@JosiasAurel  What is your pip version? and python version?\r\nTry\r\n```\r\npip show pip\r\n```\r\nTF requires pip version >=19.0", "My version of pip is 20.0.2", "when I type \n```shell\npip show pip\n```\nMy output is \n```shell\npip show pip\nName: pip\nVersion: 20.0.2\nSummary: The PyPA recommended tool for installing Python packages.\nHome-page: https://pip.pypa.io/\nAuthor: The pip developers\nAuthor-email: pypa-dev@groups.google.com\nLicense: MIT\nLocation: /data/data/com.termux/files/usr/lib/python3.8/site-packages\nRequires:\nRequired-by:\n```", "I see you are using python 3.8.  We added python 3.8 support with Tensorflow 2.2.0-rc1.\r\nSee release notes https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0-rc1\r\nCan you try installing the latest tensorflow release candidate?\r\n`pip install tensorflow==2.2.0-rc4`\r\n", "I still get an error \r\n```shell\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0-rc4 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==2.2.0-rc4\r\n```", "Is your Python 64 bits?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I faced same issue. Just chek Is your Python 64 bits with \r\n```\r\nimport platform\r\nplatform.architecture()\r\n```\r\nAnd if it 32bit just reinstall python to 64.", "64 bit", "What is the output of `pip debug --verbose`?", "There it is\r\n``` shell\r\npip version: pip 20.1 from /data/data/com.termux/files/usr/lib/python3.8/site-packages/pip (python 3.8)\r\nsys.version: 3.8.2 (default, Mar 20 2020, 08:09:52)\r\n[Clang 8.0.7 (https://android.googlesource.com/toolchain/clang b55f2d4ebfd35bf6\r\nsys.executable: /data/data/com.termux/files/usr/bin/python3\r\nsys.getdefaultencoding: utf-8\r\nsys.getfilesystemencoding: utf-8\r\nlocale.getpreferredencoding: UTF-8\r\nsys.platform: linux\r\nsys.implementation:\r\n  name: cpython\r\n'cert' config value: Not specified\r\nREQUESTS_CA_BUNDLE: None\r\nCURL_CA_BUNDLE: None\r\npip._vendor.certifi.where(): /data/data/com.termux/files/usr/lib/python3.8/site-packages/pip/_vendor/certifi/cacert.pem\r\npip._vendor.DEBUNDLED: False\r\nvendored library versions:\r\n  appdirs==1.4.3\r\n  CacheControl==0.12.6\r\n  colorama==0.4.3\r\n  contextlib2==0.6.0.post1 (Unable to locate actual module version, using vendor.txt specified version)\r\n  distlib==0.3.0\r\n  distro==1.5.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  html5lib==1.0.1\r\n  ipaddress==1.0.23\r\n  msgpack==1.0.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  packaging==20.3\r\n  pep517==0.8.2\r\n  progress==1.5\r\n  pyparsing==2.4.7\r\n  requests==2.23.0\r\n  certifi==2020.04.05.1\r\n  chardet==3.0.4\r\n  idna==2.9\r\n  urllib3==1.25.8\r\n  resolvelib==0.3.0\r\n  retrying==1.3.3 (Unable to locate actual module version, using vendor.txt specified version)\r\n  setuptools==44.0.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  six==1.14.0\r\n  toml==0.10.0\r\n  webencodings==0.5.1 (Unable to locate actual module version, using vendor.txt specified version)\r\nCompatible tags: 30\r\n  cp38-cp38-linux_aarch64\r\n  cp38-abi3-linux_aarch64\r\n  cp38-none-linux_aarch64\r\n  cp37-abi3-linux_aarch64\r\n  cp36-abi3-linux_aarch64\r\n  cp35-abi3-linux_aarch64\r\n  cp34-abi3-linux_aarch64\r\n  cp33-abi3-linux_aarch64\r\n  cp32-abi3-linux_aarch64\r\n  py38-none-linux_aarch64\r\n  py3-none-linux_aarch64\r\n  py37-none-linux_aarch64\r\n  py36-none-linux_aarch64\r\n  py35-none-linux_aarch64\r\n  py34-none-linux_aarch64\r\n  py33-none-linux_aarch64\r\n  py32-none-linux_aarch64\r\n  py31-none-linux_aarch64\r\n  py30-none-linux_aarch64\r\n  cp38-none-any\r\n  py38-none-any\r\n  py3-none-any\r\n  py37-none-any\r\n  py36-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any\r\n\r\n```", "```\r\nCompatible tags: 30\r\n  cp38-cp38-linux_aarch64\r\n  cp38-abi3-linux_aarch64\r\n  cp38-none-linux_aarch64\r\n  cp37-abi3-linux_aarch64\r\n  cp36-abi3-linux_aarch64\r\n  cp35-abi3-linux_aarch64\r\n  cp34-abi3-linux_aarch64\r\n  cp33-abi3-linux_aarch64\r\n  cp32-abi3-linux_aarch64\r\n  py38-none-linux_aarch64\r\n  py3-none-linux_aarch64\r\n  py37-none-linux_aarch64\r\n  py36-none-linux_aarch64\r\n  py35-none-linux_aarch64\r\n  py34-none-linux_aarch64\r\n  py33-none-linux_aarch64\r\n  py32-none-linux_aarch64\r\n  py31-none-linux_aarch64\r\n  py30-none-linux_aarch64\r\n  cp38-none-any\r\n  py38-none-any\r\n  py3-none-any\r\n  py37-none-any\r\n  py36-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any\r\n```\r\n\r\nNone of those tags match tags for [the released pips](https://pypi.org/project/tensorflow/#files)\r\n\r\nI see you are using Android? Probably you want to use tflite instead of tensorflow, since we don't directly build for mobile platforms.", "Okay thanks. I don't have a PC that's why I use termux. But Pydroid 3 supports it", "An alternative is to use Google Colab: Notebooks similar to jupyter notebooks but treated similar to other Google Drive files so you can access them from anywhere while they run on Google infra.", "Closing this issue now. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38750\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38750\">No</a>\n"]}, {"number": 38749, "title": "Excute `coco_object_detection:preprocess_coco_minival` error(ImportError: cannot import name preprocessing_steps_pb2)", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): master\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): \r\n\r\n**Describe the problem**\r\n```\r\nINFO: Analyzed target //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival up-to-date:\r\n  bazel-bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival\r\nINFO: Elapsed time: 0.242s, Critical Path: 0.01s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Running command line: bazel-bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival '--images_folder=/paddle/data/coco/INFO: Build completed successfully, 1 total action\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/71ecaedffe808614528ea8e9600b0716/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.runfiles/org_tensorflow/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.py\", line 39, in <module>\r\n    from tensorflow.lite.tools.evaluation.proto import evaluation_stages_pb2\r\n  File \"/root/.cache/bazel/_bazel_root/71ecaedffe808614528ea8e9600b0716/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.runfiles/org_tensorflow/tensorflow/lite/tools/evaluation/proto/evaluation_stages_pb2.py\", line 16, in <module>\r\n    from tensorflow.lite.tools.evaluation.proto import preprocessing_steps_pb2 as tensorflow_dot_lite_dot_tools_dot_evaluation_dot_proto_dot_preprocessing__steps__pb2\r\nImportError: cannot import name preprocessing_steps_pb2\r\n```\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n```\r\nbazel run //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival --  \\\r\n--images_folder=/paddle/data/coco/val2017  \\\r\n--instances_file=/paddle/data/coco/annotations/instances_val2017.json  \\\r\n--output_folder=../models/research/coco_data/\r\n```\r\nI try commend:\r\n```\r\nwget -O protobuf.zip https://github.com/google/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip\r\n./bin/protoc tensorflow/lite/tools/evaluation/proto/*.proto --python_out=.\r\n```\r\nBut not useful.\r\n", "comments": ["I am handling this issue.\r\nWas able to reproduce it.", "@still-wait it should be fixed in the master branch. please take a look. I'll close this issue.\r\nPlease reopen if it still happen. "]}, {"number": 38748, "title": "Using tf.Dataset in non-eager mode impossible", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\n\r\nI'd like to compare eager and non-eager mode using a custom training loop and a TF Dataset as the input.\r\n\r\nThis seems to be impossible as shape information seems to be unavailable and even basic training loops throw \"Attempting to capture an EagerTensor without building a function.\"\r\n\r\nI also couldn't find any documentation on how to do that.\r\n\r\n**Describe the expected behavior**\r\n\r\nCustom training loops should be possible in graph (non-eager) mode with tf.Dataset and documented\r\n\r\n**Standalone code to reproduce the issue**\r\nHeavily reduced example:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n\r\ndef preprocess(x, l):\r\n    return tf.image.convert_image_dtype(x, tf.float32), l\r\n\r\ntrain_data, test_data = tf.keras.datasets.mnist.load_data()\r\ntrain_data = tf.data.Dataset.from_tensor_slices(train_data).map(preprocess)\r\n\r\n@tf.function\r\ndef run_loop(model, data):\r\n    res = True\r\n    for x, y in data:\r\n        batch_size = int(x.shape[0])\r\n        res = model(x)[0] == 1.\r\n    return res\r\n\r\nmodel = tf.keras.Sequential([tf.keras.layers.Dense(10, activation='softmax')])\r\nresult = run_loop(model, train_data.batch(32, drop_remainder=False)) # drop_remainder=True for \"EagerTensor\" exception\r\n```\r\n\r\n**Other info / logs** \r\n\r\nThe first failure in the above is the line with `batch_size` as it tries to convert a `None` value, but at that point the value should be known already.\r\nChanging the default `drop_remainder` to `True` avoids this.\r\n\r\nThe tensor causing the \"capture...\" failure has a dtype of `resource`. Not sure what that is.\r\n\r\nObviously the above doesn't do much, but reproduces the failure. My real training loop has all the GradientTape, loss, callbacks etc.", "comments": ["@Flamefire \r\ni ran the code shared by you and face this error,please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/dcf9229cae9d4ee245ee7907ed73c4d6/untitled145.ipynb)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "not solved", "@Flamefire\r\nPlease update as per above comment", "@Saduf2019 Sorry? You said you ran the code provided and facing the same error reported here. To quote:\r\n> TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'\r\n\r\nWhich is mentioned in the description:\r\n\r\n> The first failure in the above is the line with batch_size as it tries to convert a None value, but at that point the value should be known already.\r\nChanging the default drop_remainder to True avoids this.\r\n\r\nSo what is missing?", "@tomerk could you please take a look (or triage to someone on the Keras team)? Thank you.", "Hi @Flamefire,\r\n\r\nA few things here:\r\n1. Why are you trying to use a custom training loop *and* disabling eager execution?  Generally speaking if you're using a custom training loop (as opposed to using tf1 session-based code) then disabling eager execution probably isn't the right way to achieve whatever you're trying to achieve. If you're trying to get graph performance using tf.function will do that. But, I see you're using a tf.function *and* calling compat.v1 to disable eager execution. So what's the goal here?\r\n\r\nMany TF2-only apis won't work properly if you've explicitly disabled eager. This can pop up with pretty unexpected error messages\r\n\r\n2. The tensor's `.shape` attribute actually  *isn't*  known at that point in time.\r\n`tensor.shape` is a static property on the tensor that is set at tensor construction time and can always be accessed as a direct python object. When you don't drop the remainder the batch size is unknown and will be `None` in the static property.\r\n\r\nIf you want to dynamically grab the batch size and use it in your computation you need to use `tf.shape(x[0])`. `tf.shape` returns a *tensor* representing whatever the shape of the tensor is at the point when it's used.\r\n\r\nFrom the documentation of `tensor.shape`:\r\n```\r\n    \"\"\"Returns a `tf.TensorShape` that represents the shape of this tensor.\r\n\r\n    >>> t = tf.constant([1,2,3,4,5])\r\n    >>> t.shape\r\n    TensorShape([5])\r\n\r\n    `tf.Tensor.shape` is equivalent to `tf.Tensor.get_shape()`.\r\n\r\n    In a `tf.function` or when building a model using\r\n    `tf.keras.Input`, they return the build-time shape of the\r\n    tensor, which may be partially unknown.\r\n\r\n    A `tf.TensorShape` is not a tensor. Use `tf.shape(t)` to get a tensor\r\n    containing the shape, calculated at runtime.\r\n\r\n    See `tf.Tensor.get_shape()`, and `tf.TensorShape` for details and examples.\r\n    \"\"\"\r\n```\r\n\r\n3. I was originally seeing the \"capture EagerTensor\" exception you mentioned, but it no longer seems to be appearing for me... do you see it in the tf-nightly & with the batch size access fixed?\r\n\r\nKeep in mind it's very low priority for us to fix if you're using `compat.v1.disable_eager_execution` unless it's an active regression from TF1 running the same code.", "@tomerk Thanks for your response\r\n\r\n1. As written in the OP I wanted to compare eager and non-eager execution of the same code. From your response I conclude that using the v2 APIs this is simply not intended and hence impossible and only worked by chance. Maybe this could be made more clear in the documentation? From the current description I understood that \"eager\" is the default which trades off performance for ease of use. I hence wanted to see what happens when it is turned off and expected a performance improvement. The problem with tf.function is, that it does not fully recover graph performance. Likely due to synchronization happening after each batch to get the metric values from the GPUs and maybe due to different data handling (preprocessing, uploading, ...). Also some callbacks can't be run inside a tf.function so the whole training loop can't be wrapped in a tf.function but only the individual steps\r\n\r\n2. Thanks for the clarification. It seems I used that previously outside of a tf.function where it worked. The quoted part of the documentation is pretty clear. Haven't seen that previously or it was updated since.\r\n\r\n3. Can confirm this is fixed in the nightly.", "The way to think about TF2 is it's eager (by default), and you can enter a `tf.function` to run part of your code in a graph for performance reasons.\r\n\r\n`tf.compat.v1.disable_eager_execution` isn't designed for entering a graph for performance reasons, it's meant as a totally-legacy-compatibility toggle if you find yourself in a situation where you need to run graph-only TF1 code with TF2 binaries. This might happen e.g. if you're actively migrating code from TF1 to TF2, or running legacy code in an environment that only has TF2 installed.\r\n\r\nFrom the TF api docs for compat.v1.disable_eager_execution:\r\n```\r\nThis function can only be called before any Graphs, Ops, or Tensors have been created. It can be used at the beginning of the program for complex migration projects from TensorFlow 1.x to 2.x.\r\n```\r\n\r\nPlease let us know how we could clarify this, or if it was a different piece of documentation that led you to believe that to build graphs in TF2 you are supposed to call `disable_eager_execution()`\r\n\r\n--------------\r\n\r\nGenerally speaking if code doesn't run in a tf.function it would not run in a legacy TF 1.x graph at all, so that's unlikely to be the cause of tf.function not matching TF 1.x graph performance. E.g. datasets can be constructed (or passed into) tf.functions, so it's possible to iterate over your entire dataset in a tf.function call.\r\n\r\nCallbacks don't usually have to be called every training step, so you can roll several training steps into one tf.function and only run callbacks in between them. If you're writing a custom loop it's possible to do yourself in your loop. As of recently, Keras models have a `_make_train_function` you can override to do just when you're fitting the model with `model.fit()`. (Guides for this are in progress I believe, or @omalleyt12 should be able to point you to some). These callbacks wouldn't run inside a TF 1.x graph regardless.\r\n\r\nThat's not to say `tf.function` never has overheads. Depending on how you write your code, you might accidentally cause the function to retrace/recompile more than you expected. If that's the case you should look at the \"better performance\" w/ tf.functions guide:\r\nhttps://www.tensorflow.org/guide/function\r\nAny part of your code that is inside a single tf.function trace should run just as fast (or faster) w/ TF2 as it would in a TF1 graph though. (and for things that don't feel free to file a bug!)\r\n\r\n\r\n\r\n", "Closing this issue for now because the reported error is fixed in nightly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38748\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38748\">No</a>\n"]}, {"number": 38747, "title": "tf.experimental.tensorrt.Converter sample code not working in tf 2.1.0", "body": "Ubuntu 14\r\nPython 3.5.2\r\nTensorflow-gpu 'v2.1.0-rc2-17-ge5bf8de'\r\n\r\nThe sample codes provided on page https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter do not work, raise exception:\r\n> NameError: name 'DEFAULT_TRT_CONVERSION_PARAMS' is not defined\r\n\r\nThis fix worked for me:\r\n> from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n> params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\r\n>     precision_mode='FP16')\r\n", "comments": ["@akkiss \r\ncould you please provide us with a simple standalone code so we could replicate your issue, if possible share a colab gist for us to analyse it.\r\n\r\nplease check to this [link](https://github.com/tensorflow/tensorflow/issues/33277) for reference\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38745, "title": "tf.keras.Model#load_weights() cannot be used on a directory generated by tf.keras.callback.ModelCheckpoint", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04, Windows 10\r\n- TensorFlow installed from (source or binary): pipy, anaconda\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using the `tf.keras.callbacks.ModelCheckpoint('path/to/checkpoint.tf', save_weights_only=False)` callback when training with `tf.keras.Model#fit()`,  tensorflow stores a checkpoint as a directory with the following content:\r\n\r\n```\r\n/path/to/checkpoint.tf/\r\n|  saved_model.pb\r\n|  assets/\r\n     | ....\r\n| variables\r\n     | variables.data-00000-of-x\r\n     | ....\r\n     | variables.index\r\n```\r\n\r\nAccording to the documentation of [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint), `save_weights_only=False` uses [`tf.keras.Model#save()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save), which will save:\r\n\r\n> * The model architecture, allowing to re-instantiate the model.\r\n> * The model weights.\r\n> * The state of the optimizer, allowing to resume training exactly where you left off.\r\n\r\n\r\nWhen `tf.keras.callbacks.ModelCheckpoint('/path/to/weights.tf', save_weights_only=True)`  is used, a checkpoint is stored as:\r\n```\r\n/path/to/\r\n| checkpoint\r\n| weights.tf.data-00000-of-00002\r\n| weights.tf.data-00001-of-00002\r\n| weights.tf.index\r\n```\r\nSide note: `save_weights_only=False` expects a directory path, `save_weights_only=True`expects a file path. \r\n\r\nUsing `save_weights_only=True` uses [`tf.keras.Model#save_weights()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save_weights).\r\n\r\nConsequently, the model can be loaded by using\r\n\r\n```python\r\n# this works\r\ndef build_model():\r\n    ...\r\n    return tf.keras.Model()\r\n\r\nmodel1 = tf.keras.models.load_model(\"path/to/checkpoint.tf\")\r\n\r\nmodel2 = build_model()\r\nmodel2.load_weights(\"path/to/weights.tf\")\r\n```\r\n\r\nHowever, using `path/to/checkpoint.tf` with `tf.keras.Model#load_weights()` fails.\r\n\r\n```python\r\nmodel3 = build_model()\r\nmodel3.load_weights(\"path/to/checkpoint.tf\")\r\n```\r\noutputs:\r\n```\r\n\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-16-af9095619790> in <module>()\r\n----> 1 model = build_model().load_weights(str(save_full))\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)\r\n    171         if swmr and swmr_support:\r\n    172             flags |= h5f.ACC_SWMR_READ\r\n--> 173         fid = h5f.open(name, flags, fapl=fapl)\r\n    174     elif mode == 'r+':\r\n    175         fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/h5f.pyx in h5py.h5f.open()\r\n\r\nOSError: Unable to open file (file read failed: time = Tue Apr 21 10:08:58 2020\r\n, filename = 'checkpoint.tf', file descriptor = 60, errno = 21, error message = 'Is a directory', buf = 0x7ffced128070, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)\r\n```\r\nInstead, I need to do:\r\n\r\n```python\r\n# this works\r\nnew_model = build_model()\r\nnew_model.load_weights(\"path/to/checkpoint.tf/variables/variables\")\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nAs `tf.keras.callbacks.ModelCheckpoint` hides the differences between `tf.keras.Model#save()` and `tf.keras.Model#save_weights()` from users, I expect the following code to work:\r\n\r\n```python\r\ndef build_model():\r\n    ...\r\n\r\npath = \"path/to/checkpoint.tf\"\r\ncallbacks = [tf.keras.callbacks.ModelCheckpoint(path, save_weights_only=False)]\r\nmodel = build_model()\r\nmodel.fit(..., callbacks=callbacks)\r\n\r\n# now I can either load the whole model\r\nnew_model = tf.keras.models.load_model(path)\r\n\r\n# or I can only load the weights\r\nanother_new_model = build_model()\r\nanother_new_model = another_new_model.load_weights(path)\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/104UFbStlddH8tH_xOD58dTbjEAj9SsKE\r\n", "comments": ["Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/fecda942b8d8dcc16d2a0db53547c9f6/38745-2-1.ipynb), [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/2c080a1673c1dd1df03f96ea37774bf7/38745.ipynb#scrollTo=hE1uriFRRyW1), and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/db6e53e41b9d4bf21c3fe4abc5857be9/38745-tf-nightly.ipynb). Please find the attached gist. Thanks!", "any news regarding this issue? i'm experiencing the same thing", "@nikvaessen I had answered similar question sometime back. This is not a bug. It is implementation related issue.\r\n\r\nAs you correctly mentioned that  `ModelCheckpoint`, with  `save_weights_only=False` uses  `tf.keras.Model#save()` which saves entire model architecture, optimizer status and weights. When you want to load the model, you need to use `load_model` as in your `model1`.\r\n\r\n`ModelCheckpoint`, with  `save_weights_only=True` uses `tf.keras.Model#save_weights()` which saves weights. When you want to load the saved weights, you need to use `load_weights` as in your `model2`.\r\n\r\nIn your `model3`, you built a model and need to use `load_weights` with the saved weights file as you had used in `model2` example. However, you are using entire saved_model while the code is expecting weights only. This is the reason the code is throwing error.\r\n\r\n```\r\nnew_model = build_model()\r\nnew_model.compile(loss='mse')\r\nmodel.load_weights(str(save_full))\r\n\r\n```\r\n\r\nPlease verify once and close the issue if the issue was resolved for you. Thanks!\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38745\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38745\">No</a>\n", "@jvishnuvardhan \r\nI think @nikvaessen was asking for correcting the implementation based on the tutorial [here](https://www.tensorflow.org/tutorials/keras/save_and_load#checkpoint_callback_usage).\r\n\r\n        checkpoint_path = \"training_1/cp.ckpt\"\r\n        checkpoint_dir = os.path.dirname(checkpoint_path)\r\n        \r\n        # Create a callback that saves the model's weights\r\n        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\r\n                                                         save_weights_only=True,\r\n                                                         verbose=1)\r\n        \r\n        # Train the model with the new callback\r\n        model.fit(train_images, \r\n                  train_labels,  \r\n                  epochs=10,\r\n                  validation_data=(test_images, test_labels),\r\n                  callbacks=[cp_callback])  # Pass callback to training\r\n        \r\n        # This may generate warnings related to saving the state of the optimizer.\r\n        # These warnings (and similar warnings throughout this notebook)\r\n        # are in place to discourage outdated usage, and can be ignored.\r\n\r\n\r\n        # Loads the weights\r\n        model.load_weights(checkpoint_path)\r\n        \r\n        # Re-evaluate the model\r\n        loss, acc = model.evaluate(test_images, test_labels, verbose=2)\r\n        print(\"Restored model, accuracy: {:5.2f}%\".format(100 * ACC))\r\n\r\n\r\n\r\n"]}, {"number": 38744, "title": "TFLu compile option in msys under Win64", "body": "Users that need a build option of TFLu under msys on Win64 need to install the Win executables of the gcc tool chain. The PR adds this option to the TFLu installation to automatically download and unpack.", "comments": []}, {"number": 38743, "title": "solve all python related issues", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 38742, "title": "[TF 2.2.0rc] Regression - the `{predict,train,test}_on_batch` trace functions with fixed batch size #34907", "body": "See #34907 for description.\r\n\r\nEven if the referenced issue was fixed for 2.1, it is again present in TF 2.2.0rc3.", "comments": ["i am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/58b082e917b16769fef0319f98b8e399/38742.ipynb)", "Thanks @foxik for the report. We are working on a long-term fix right now, which we are hoping to get into the next 2.2rc, but, if it proves difficult, may have to wait for 2.3.", "@karmel I perfectly understand that it is late in the release cycle. If unfixed, it would make the `*_on_batch` methods unusable for dynamic size input, but affected people can always wait for 2.3.\r\n\r\nBTW, how come that `.fit` is not affected, i.e.\r\n```python\r\ninputs = tf.keras.layers.Input([None])\r\nsummed = tf.math.reduce_sum(inputs, axis=1, keepdims=True)\r\noutput = tf.keras.layers.Dense(1)(summed)\r\nmodel = tf.keras.Model(inputs, output)\r\nmodel.compile(optimizer=tf.optimizers.Adam(), loss=tf.losses.MeanSquaredError())\r\nfor i in range(1, 300):\r\n    model.train_on_batch(np.ones([1, i]), np.ones([1]) * i)\r\n```\r\ncauses retraces while\r\n```python\r\ndef dataset():\r\n    for i in range(1, 300):\r\n        yield np.ones([1, i]), np.ones([1]) * i\r\n\r\nmodel.fit(dataset())\r\n```\r\ndoes not? Does the dataset iteration sees that the shapes are changing and retraces the training method with dynamic shapes?", "@karmel BTW, Google Colab is using 2.2.0rc3 and I already have students hitting this issue (we have started working with RNNs).", "@foxik Thanks for the issue!\r\n\r\nThere is a potential fix in the works, but it's unclear if it will make the release cut.\r\n\r\nI'd actually recommend moving away from the `train_on_batch` methods when possible, and instead doing custom training logic via overriding `Model.train_step`. This feature was added in this release and gives a really nice way to write custom training logic while still getting all the benefits (distribution, callbacks, data formats, etc) of `Model.fit`. It should also be noticeably faster and won't have the retracing issue\r\n\r\nA thread explaining how this works: https://twitter.com/fchollet/status/1250622989541838848", "@omalleyt12 Thanks! I already went through the sources of `train_step` and I like how it improved applicability -- for example we use CRF/CTC losses quite a lot, and will now be able to use .fit, which is a great simplification, even if only for the distribution strategy.\r\n\r\nBut we have sources which we want to work with pre-2.2 TF for the time being, and `train_on_batch` not working in Collab is also an issue, so it would make our life easier if the regression did not appear. But I definitely understand that it is a trade-off.\r\n\r\nThank you very much for all your hard work!", "For people looking for a way to generate predictions with variable size in 2.2.0 and avoid 1) the concatenation error raised by `model.predict` 2) tf.function retracing; simply call the `model.predict_step` function in a batch loop.", "This is fixed with tf-nightly version '2.2.0-dev20200508'.\r\nRefer [colab gist](https://colab.research.google.com/gist/ymodak/205ae3ad0d709a84e07c90be10b46190/38742.ipynb).\r\nThanks!", "@foxik closing this issue as this has been resolved and the next TF 2.3.0 release will have it. Please feel free to reopen if you run into any other issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38742\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38742\">No</a>\n"]}, {"number": 38741, "title": "'CollectBatchStats' object has no attribute 'batch_losses'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nwhen I learn \"https://tensorflow.google.cn/tutorials/images/transfer_learning_with_hub\", \r\nthe following code shows an error \"AttributeError: 'CollectBatchStats' object has no attribute 'batch_losses'\"\r\nthe code is following:\r\n`plt.figure()\r\nplt.ylabel(\"Loss\")\r\nplt.xlabel(\"Training Steps\")\r\nplt.ylim([0,2])\r\nplt.plot(batch_stats_callback.batch_losses)`\r\n**Describe the expected behavior**\r\nI can't find any solution for this , please give me some advice about this, thanks\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@dynamor2019 \r\n\r\nI have tried in colab with TF version 2.2.0-dev20200421  and i am not seeing any issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/ff5d36b8aaae5a082a6d6f694e61dcdc/untitled798.ipynb).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38741\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38741\">No</a>\n"]}, {"number": 38739, "title": "Can't import tflearn in Tensorflow 2.1.0.  ModuleNotFoundError: No module named 'tensorflow.contrib'", "body": "Tried importing tflearn and it's component for building a CNN model.\r\n```import tflearn\r\nfrom tflearn.layers.conv import conv_2d, max_pool_2d\r\nfrom tflearn.layers.core import input_data, dropout, fully_connected\r\nfrom tflearn.layers.estimator import regression\r\nimport tflearn.datasets.mnist as mnist \r\n```\r\nBut it is thowing an error :\r\n```\r\nModuleNotFoundError Traceback (most recent call last)\r\nin\r\n----> 1 import tflearn\r\n2 from tflearn.layers.conv import conv_2d, max_pool_2d\r\n3 from tflearn.layers.core import input_data, dropout, fully_connected\r\n4 from tflearn.layers.estimator import regression\r\n5\r\n\r\n~\\Anaconda3\\Lib\\site-packages\\tflearn_init_.py in\r\n2\r\n3 # Config\r\n----> 4 from . import config\r\n5 from .config import is_training, get_training_mode, init_graph\r\n6\r\n\r\n~\\Anaconda3\\Lib\\site-packages\\tflearn\\config.py in\r\n3 import tensorflow as tf\r\n4\r\n----> 5 from .variables import variable\r\n6\r\n7 # -------------------\r\n\r\n~\\Anaconda3\\Lib\\site-packages\\tflearn\\variables.py in\r\n5 import tflearn\r\n6\r\n----> 7 from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope\r\n8 from tensorflow.python.framework import ops\r\n9 from tensorflow.python.ops import variable_scope\r\n\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n```\r\nFound this issue on many other forums and it says 'tensorflow.contrib' was removed in version 2.0 and can't find a solution except downgrading the tensorflow version from 2.1.0 to 1.14.0. ", "comments": ["tensorflow.contrib is being removed in version TF 2.x and it works in only TF 1.x. Thanks!", "For more info see this https://www.tensorflow.org/guide/migrate#a_note_on_slim_contriblayers\r\nThanks!", "Thanks for responding. @ravikyram and @ymodak "]}, {"number": 38738, "title": "add \"variables\" argument to `argspec.kwonlyaards`", "body": "**no unit tests written**\r\n**not tested locally**\r\n\r\nThis is related to these two pull issues:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/31945\r\nhttps://github.com/tensorflow/tensorflow/issues/38622\r\n\r\nWhat this should do is force the input of `grad()` function of class `Loss` to forget an extra argument required when instantiating a `Variable()` with the custom loss function.\r\n\r\nIdeally I'd implement the patch and build from source, but I'm encountering build from source issues via bazel: `https://github.com/tensorflow/tensorflow/issues/38736`, likely due to Fedora.\r\n\r\nLet me know what need be changed.\r\n\r\nIt'd be awesome to be able to build from source so that I can test the patch, but bazel is still messing up, if one of the devs knows a quick fix I can implement it.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38738) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38738) for more info**.\n\n<!-- ok -->", "@drezap  Can you please address Ubuntu Sanity errors? Thanks!", "yes easy fix not now too busy. soon", "@drezap Any update on this PR? Please. Thanks!", "This bug remains unsolved then?"]}, {"number": 38737, "title": "Build Tensorflow with triSYCL?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4\r\n- TensorFlow installed from (source or binary): build from source \r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.0.0 \r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: [triSYCL](https://github.com/triSYCL/triSYCL)\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Bazel 3.0.0** has been installed onto my **aarch64** successfully:\r\n\r\n```console\r\n$ bazel version\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nBuild label: 3.0.0- (@non-git)\r\nBuild target: bazel-out/aarch64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Apr 20 22:21:14 2020 (1587421274)\r\nBuild timestamp: 1587421274\r\nBuild timestamp as int: 1587421274\r\n```\r\n\r\nHowever, when I tried to build Tensorflow with SYCL enabled, I got the following error messages:\r\n\r\n```console\r\nERROR: ~/.cache/bazel/_bazel_khadas/0de19da26a472240f23447517e34d888/external/local_config_sycl/crosstool/BUILD:12:1: Target '@local_config_sycl//crosstool:empty' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:cc-compiler-local'\r\nERROR: ~/.cache/bazel/_bazel_khadas/0de19da26a472240f23447517e34d888/external/local_config_sycl/crosstool/BUILD:5:1: Target '@local_config_sycl//crosstool:cc-compiler-local' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:toolchain'\r\nERROR: ~/.cache/bazel/_bazel_khadas/0de19da26a472240f23447517e34d888/external/bazel_tools/src/tools/launcher/BUILD:9:1: every rule of type cc_binary implicitly depends upon the target '@local_config_sycl//crosstool:toolchain', but this target could not be found because of: Target '@local_config_sycl//crosstool:toolchain' contains an error and its package is in error\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 0.724s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 3 targets configured)\r\n```\r\n\r\nIt seems it's telling that **bazel DIDN'T successfully specify crosstool:toolchain** ?\r\n\r\nWell, yes, I didn't specify the toolchain, but on my **aarch64 board**, the toolchain are **ALREADY** **symbolic linked** to things including: **gcc**, **g++**, etc. I believe **it does relate to triSYCL** ....\r\n\r\nCheers\r\nPei\r\n", "comments": ["I am unsure what is the current support for SYCL in the latest TensorFlow.\r\nWe have tried a few years ago but it was not an efficient way for FPGA, so we gave up this route...", "@jiapei100,\r\n\r\nAs we have the latest stable version `2.6.0`, Can you try building `TF 2.6.0` from source using this [guide](https://www.tensorflow.org/install/source), and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38737\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38737\">No</a>\n"]}, {"number": 38736, "title": "'bazel' --batch mode deprecated issue for Fedora", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 29\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.21.0 and the latest.\r\n- GCC/Compiler version (if compiling from source): N/A, command?\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nCan't compile from source due to `bazel` issue. Tried switching to recommended version of bazel, still unsuccessful.\r\n\r\nThis happened when trying to modify the source and build for the patch suggested here:\r\n\r\n`https://github.com/tensorflow/tensorflow/issues/31945`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nfrom tensorflow directory:\r\n\r\n`python configure.py`\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\n`INFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=190\r\nINFO: Reading rc options for 'version' from /home/andre/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nERROR: Unrecognized option: --experimental_repo_remote_exec\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 0e4f21d3-187f-4d35-8f91-27a3ea88e768\r\nTraceback (most recent call last):\r\n  File \"configure.py\", line 1551, in <module>\r\n    main()\r\n  File \"configure.py\", line 1368, in main\r\n    _TF_MAX_BAZEL_VERSION)\r\n  File \"configure.py\", line 483, in check_bazel_version\r\n    ['bazel', '--batch', '--bazelrc=/dev/null', 'version'])\r\n  File \"configure.py\", line 159, in run_shell\r\n    output = subprocess.check_output(cmd, stderr=stderr)\r\n  File \"/usr/lib64/python3.7/subprocess.py\", line 411, in check_output\r\n    **kwargs).stdout\r\n  File \"/usr/lib64/python3.7/subprocess.py\", line 512, in run\r\n    output=stdout, stderr=stderr)\r\nsubprocess.CalledProcessError: Command '['bazel', '--batch', '--bazelrc=/dev/null', 'version']' returned non-zero exit status 2.\r\n`", "comments": ["Can you try with newer bazel version like 2.0.0 or higher?\r\nSee,\r\nhttps://github.com/tensorflow/tensorflow/blob/ced65fa9b07b8362fc716a7086bc7a9a25416262/configure.py#L52", "I had to use bazel 3.0. configure file wasn't happy with 3.1. Nor earlier versions. Thanks!\r\n\r\nhere: https://github.com/tensorflow/tensorflow/blob/master/.bazelversion\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38736\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38736\">No</a>\n", "Just to be clear I did check out the config file after the build issues, and I did have a bazel version that fell within constants MAX and MIX bazel version, and it would still throw that issue. If I remembered, I would tell you."]}]