[{"number": 28659, "title": "model.fit", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "This \"extra\" issue was created by mistake, when creating https://github.com/tensorflow/tensorflow/issues/28661"]}, {"number": 28658, "title": "tf.cond\uff1f\uff1f\uff1f\uff1f", "body": "\r\nHow to achieve in tensorflow: meet the condition, execute 1 (do not execute 2); do not satisfy the condition, execute 2 (do not execute 1)", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "In the process of using the resnet50 for imagenet training, we used LARS to update the learning rate and calculate LR at each step of the training. The throughput of the training is about 5500. For this we intend to optimize and calculate the LR operation every few steps to improve throughput. In the original code, we perform compute_lr calculation every step.\r\nI modified the code as shown below\uff1a\r\n    gg is a tensor used to observe which step of training;\r\n    2 is a constant, indicating that lr is calculated every 2 steps.\r\n\r\nThe code:\r\n\r\ndef compute_lr()\r\n    coumpte_lr \r\n       ...\r\n    stored_lr\r\n       ...\r\n    return lr\r\ndef get_larsvalue()\r\n    get_stored_lr\r\n       ...\r\n    return lr\r\n\r\ntf.cond(tf.cast(tf.equal(tf.mod(gg,2),0),tf.bool),lambda:self.compute_lr(),lambda: self.get_larsvalue())\r\n\r\n\r\nBut after I modified the code, the throughput dropped. After analysis, I think it is because tf.cond is not a lazy operation, it will execute both branches, which is obviously not what I want. I don't know how to code to complete my thoughts now, ask you to help. Thanks\r\n\r\n\r\n"]}, {"number": 28657, "title": "custom tflite model run failed with Gpudelegate  (android P)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below):1.12.2\r\n- Python version:3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nbuild GPUdelegate failed on android P   java demo\r\n**Describe the expected behavior**\r\nbuild pass\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nI convert my model like this:\r\nfreeze_graph --input_graph=eval.pb --input_checkpoint=./model_quant_self/model.ckpt-19-19 --output_graph=frozen_eval_graph.pb --output_node_names=Softmax\r\ntflite_convert --output_file=poolnet_gzq.tflite --graph_def_file=./model_gzq.pb --inference_type=FLOAT --input_arrays=Placeholder --input_shapes=1,224,224,3 --output_arrays=oup\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nhere is the log:\r\n\r\n2019-05-13 16:25:04.870 20456-22983/? W/System.err: java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: GpuDelegate Prepare: fuse_auto_input failedNode number 133 (GpuDelegate) failed to prepare.\r\n\r\n2019-05-13 16:25:04.870 20456-22983/? W/System.err: Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: GpuDelegate Prepare: fuse_auto_input failedNode number 133 (GpuDelegate) failed to prepare.\r\nfailedNode133 is my outputNode Sigmiod/Softmax ,I had tried ,both are failed\r\n", "comments": ["@sunzhe09 \r\n\r\nIs it possible for you to share the .tflite file with me?  If weights are confidential, you can share an uninitialized graph with random weights; all I care about is the network structure.", "@impjdi  My model is from here :https://github.com/zhangludl/A-bi-directional-message-passing-model-for-salient-object-detection.  You can download the model from the line Download the pretrained model from here, and put it under the ./model directory. If you transform it to tflite,you can reproduce my issue. I am sorry I don't have right to upload my file ...", "In case it helps; I had a similar problem and was able to reproduce the error with the following simple keras model:\r\n\r\n```\r\ninput = keras.Input(shape=(68,128,34))\r\n\r\nleft = keras.layers.Conv2D(22, (3,3), padding=\"same\", activation=\"relu\")(input)\r\nright = keras.layers.Conv2D(22, (1,1), padding=\"same\", activation=\"relu\")(input)\r\n\r\ncenter = keras.layers.add([left, right])\r\n\r\nleft = keras.layers.add([right, center])\r\nright = keras.layers.Conv2D(22, (3,1), padding=\"same\", activation=\"relu\")(center)\r\n\r\nend = keras.layers.add([left,right])\r\n\r\n\r\nmodel = keras.Model(input, end)\r\n\r\nsess = keras.backend.get_session()\r\n\r\nconverter = lite.TFLiteConverter.from_session(sess, [model.input], [model.output])\r\nmodel = converter.convert()\r\nfile = open( 'reproduce.tflite' , 'wb' ) \r\nfile.write( model )\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28657\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28657\">No</a>\n"]}, {"number": 28656, "title": "Added 8-bit Quantization Support for ELU.", "body": "", "comments": ["@talumbau , can you please review the PR and provide your feedback, this is kind of important PR for me and need your help to get it though.\r\n\r\nRegards\r\nAmit", "@talumbau , i have covered all the scenario and tested even with model, kindly check as this is very important PR for me.\r\n\r\nRegards\r\nAmit", "@talumbau , based on your comments on my previous PR (Quantization support for exp operator).\r\nI have reworked the applicable comments here as well.\r\nKindly check and approve.\r\n\r\nRegards\r\nAmit", "@talumbau , i have added TC and kernel version in operator.cc file as well.\r\n\r\nRegards\r\nAmit", "@talumbau , i have resolved the merge conflicts, can you please review the code.\r\n\r\nRegards\r\nAmit", "@talumbau , can you have a look at the PR and provide your comments, this PR is important for me.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 Could you please resolve the conflicts? Thanks!", "Can one of the admins verify this patch?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28655, "title": "train.py", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): git clone\r\n- TensorFlow version: newest\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n./train.py\r\n./train.py: line 68: --wanted_words: command not found\r\n./train.py: line 68: rSimple speech recognition to spot a limited number of keywords.\r\nwhy is that?how to solve it?", "comments": ["@yangjinghit Please provide the link for the mentioned train.py, to further investigate on this.Also please mention the tensorflow version( 1.13.1 or 2.0.0alpha) currently being used .", "@yangjinghit It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 28654, "title": "NotFoundError: Resource worker/embedding_layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- GPU model and memory: TPU\r\n\r\n\r\n**Describe the current behavior**\r\nWhen i try to call fit on a character level LSTM based model I get following error `NotFoundError: Resource worker/embedding_layer/embeddings/N10tensorflow3VarE does not exist.\r\n\t [[{{node embedding_layer/embedding_lookup}}]]`\r\n\r\n**Describe the expected behavior**\r\nIf I run the exact same model on a GPU, everything works as expected (eg. the model trains).\r\n\r\n**Code to reproduce the issue**\r\nI am following the official google guides as to how train a keras model on tpu using the tensorflow contrib keras to tpu model function\r\n\r\n**Other info / logs**\r\n```Train on 501147 samples, validate on 55684 samples\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-32-fff2e852201f> in <module>()\r\n----> 1 model.fit(X,y,epochs=1,validation_split=0.1,batch_size=batch_size)#,callbacks=callbacks)\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    526             None, None,\r\n    527             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 528             c_api.TF_GetCode(self.status.status))\r\n    529     # Delete the underlying status object from memory otherwise it stays alive\r\n    530     # as there is a reference to status from this from the traceback due to\r\n\r\nNotFoundError: Resource worker/embedding_layer/embeddings/N10tensorflow3VarE does not exist.\r\n\t [[{{node embedding_layer/embedding_lookup}}]]```\r\n", "comments": ["@sarim-zafar Could you provide the minimum code snippet to reproduce the issue reported here. Thanks!", "I had a bug in my code... I am sorry for the inconvenience."]}, {"number": 28653, "title": "Multi-GPU performance degradation in custom built TF", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): tensorflow/benchmarks\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy): N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 33141833284a81a5ab3300047d2845c7124f4a66\r\n- Python version: Python 3.5.3\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 6.3\r\n- CUDA/cuDNN version: 10.0/7.4\r\n- GPU model and memory: V100-PCIE-32GB\r\n\r\n**Describe the current behavior**\r\n\r\nUse the following command to compile (without running of `./configure`):\r\n\r\n```\r\nbazel build \\\r\n    -c opt \\\r\n    --jobs=32 \\\r\n    --action_env=PYTHON_BIN_PATH=\"/usr/bin/python3\" \\\r\n    --action_env=PYTHON_LIB_PATH=\"/usr/local/lib/python3.5/dist-packages\" \\\r\n    --python_path=\"/usr/bin/python3\" \\\r\n    --define with_xla_support=true \\\r\n    --copt=-Wno-sign-compare \\\r\n    --copt=-march=broadwell \\\r\n    --host_copt=-march=broadwell \\\r\n    --define=with_default_optimizations=true \\\r\n    --config=cuda \\\r\n    --config=gdr \\\r\n    --config=mkl \\\r\n    //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nUse the following command for benchmarking:\r\n\r\n```\r\npython benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n    --model=resnet50 \\\r\n    --batch_size=256 \\\r\n    --use_fp16 \\\r\n    --variable_update=parameter_server \\\r\n    --local_parameter_device=cpu \\\r\n    --num_gpus=$NUM_GPUS\r\n```\r\n\r\n| TF Version | 1-GPU | 2-GPU | 4-GPU | 8-GPU |\r\n|:-:|:-:|:-:|:-:|:-:|\r\n|1.13.1|786.63|1557.89|2982.52|5741.76|\r\n|1.14.1.dev20190512|779.81|1547.23|2872.75|5671.7|\r\n| 33141833284a81a5ab3300047d2845c7124f4a66|776.75|387.99|713.77|1249.80|\r\n\r\n**Describe the expected behavior**\r\n\r\nRelated to #28628. I am currently not sure how to reproduce a custom build with the same GIT_VERSION as of nightly. I will try to reproduce this issue with the next nightly package.", "comments": ["Removing ``--config=mkl`` resolves this issue.", "Gentle ping @claynerobison @nhasabni @mahmoud-abuzaina what is your recommended configuration if we try to run our wide and deep models on a mixed CPU/GPU cluster? Do we need to have a MKL build and a separate CUDA build?", "As requested on the RFC, here is the build script for the nightly configuration.\r\n\r\nUbuntu 14.04\r\nPython 3.5 \r\nBazel 0.24.1\r\nGCC 4.8\r\n\r\n\r\n```\r\n# Run configure.\r\nexport TF_NEED_GCP=1\r\nexport TF_NEED_HDFS=1\r\nexport TF_NEED_S3=1\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_VERSION=10\r\nexport TF_CUDNN_VERSION=7\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=3.5,3.7,5.2,6.0,6.1,7.0\r\nexport TF_NEED_TENSORRT=1\r\nexport TENSORRT_INSTALL_PATH=/usr/local/tensorrt\r\nexport CC_OPT_FLAGS='-mavx'\r\nexport PYTHON_BIN_PATH=$(which python3.5)\r\nexport PROJECT_NAME=\"tf_nightly_gpu\"\r\nyes \"\" | \"$PYTHON_BIN_PATH\" configure.py\r\n\r\n# Build the pip package\r\nbazel build --config=opt tensorflow/tools/pip_package:build_pip_package\r\nmkdir pip_pkg\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package pip_pkg --gpu --nightly_flag\r\n```"]}, {"number": 28652, "title": "8-bit Quantization Support for Exp.", "body": "This is part of issue #28438 ", "comments": ["@talumbau , can you please review the PR and provide your feedback, this is kind of important PR for me and need your help to get it though.\r\n\r\nRegards\r\nAmit", "@talumbau have updated all the possible scenarios, kindly review and provide the feedback.\r\n\r\nRegards\r\nAmit", "@talumbau , first of all i would like to thanks you for you deep review and spending time on the PR, I have accepted all your comments and reworked as per your suggestion. Kindly check and approve.\r\n\r\nRegards\r\nAmit", "@talumbau , i have added TC and kernel version in operator.cc file as well.\r\n\r\nRegards\r\nAmit", "@jianlijianli , can you also take a look, for this PR.\r\n\r\nRegards\r\nAmit", "@jianlijianli , can you please spend some time on the PR, this is important to me.\r\n\r\n\r\nRegards\r\nAmit", "Can one of the admins verify this patch?", "@amitsrivastava78 Could you please resolve the conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28651, "title": "mode.fit() Error: ValueError: TypeError: len() of unsized object", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): No\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-alpha0\r\n- TensorFlow version (use command below):2.0.0-alpha0\r\n- Python version: 3.6.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n2.0.0-alpha0\r\n**Describe the current behavior**\r\nI run the code on my local machine : the TF 2.0 Alpha Train your first neural network: basic classification.\r\nThere is all right with the code before i call : model.fit(train_images, train_labels, epochs=5).\r\nThe errors are:\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-2e1aa44bfe09> in <module>\r\n      1 # \u8bad\u7ec3\r\n----> 2 model.fit(x_train, y_train, batch_size=50, epochs=50, validation_split=0.1, verbose=1)\r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    871           validation_steps=validation_steps,\r\n    872           validation_freq=validation_freq,\r\n--> 873           steps_name='steps_per_epoch')\r\n    874 \r\n    875   def evaluate(self,\r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    237     # Setup work for each epoch\r\n    238     epoch_logs = {}\r\n--> 239     model.reset_metrics()\r\n    240     if mode == ModeKeys.TRAIN:\r\n    241       callbacks.on_epoch_begin(epoch, epoch_logs)\r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in reset_metrics(self)\r\n   1171     if hasattr(self, 'metrics'):\r\n   1172       for m in self.metrics:\r\n-> 1173         m.reset_states()\r\n   1174 \r\n   1175     # Reset the state of loss metric wrappers.\r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py in reset_states(self)\r\n    197     when a metric is evaluated during training.\r\n    198     \"\"\"\r\n--> 199     K.batch_set_value([(v, 0) for v in self.variables])\r\n    200 \r\n    201   @abc.abstractmethod\r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py in batch_set_value(tuples)\r\n   2878   if ops.executing_eagerly_outside_functions():\r\n   2879     for x, value in tuples:\r\n-> 2880       x.assign(np.asarray(value, dtype=dtype(x)))\r\n   2881   else:\r\n   2882     with get_graph().as_default():\r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py in assign(self, value, use_locking, name, read_value)\r\n   1051     # initialize the variable.\r\n   1052     with _handle_graph(self.handle):\r\n-> 1053       value_tensor = ops.convert_to_tensor(value, dtype=self.dtype)\r\n   1054       self._shape.assert_is_compatible_with(value_tensor.shape)\r\n   1055       assign_op = gen_resource_variable_ops.assign_variable_op(\r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)\r\n   1048   preferred_dtype = deprecation.deprecated_argument_lookup(\r\n   1049       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\r\n-> 1050   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n   1051 \r\n   1052 \r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)\r\n   1106       name=name,\r\n   1107       preferred_dtype=dtype_hint,\r\n-> 1108       as_ref=False)\r\n   1109 \r\n   1110 \r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\r\n   1184 \r\n   1185     if ret is None:\r\n-> 1186       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1187 \r\n   1188     if ret is NotImplemented:\r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    302                                          as_ref=False):\r\n    303   _ = as_ref\r\n--> 304   return constant(v, dtype=dtype, name=name)\r\n    305 \r\n    306 \r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name)\r\n    243   \"\"\"\r\n    244   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 245                         allow_broadcast=True)\r\n    246 \r\n    247 \r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    251   ctx = context.context()\r\n    252   if ctx.executing_eagerly():\r\n--> 253     t = convert_to_eager_tensor(value, ctx, dtype)\r\n    254     if shape is None:\r\n    255       return t\r\n\r\nD:\\Softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n    112     return t\r\n    113   else:\r\n--> 114     return ops.EagerTensor(value, handle, device, dtype)\r\n    115 \r\n    116 \r\n\r\nValueError: TypeError: len() of unsized object\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@IcarusZqt In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "````\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\r\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\r\ninput_shape = (28, 28, 1)\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\n\r\n# normalize rgb codes by dividing by the max rgb value of 255...\r\nx_train /= 255\r\nx_test /= 255\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Flatten()) # Flattening the 2D arrays for fully connected layers\r\nmodel.add(Dense(128, activation=tf.nn.relu))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(10,activation=tf.nn.softmax))\r\n\r\nmodel.compile(optimizer = 'adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n\r\nmodel.fit(x=x_train, y=y_train, epochs=10)\r\n````\r\n\r\nHere's a repro for an example that is giving me the same issue... just attempting to use mnist to go through some 2.0 stuff... hope this is the right place to respond... I'm getting the same or similar error as follows....\r\n\r\n> Traceback (most recent call last):\r\n> \r\n>   File \"<ipython-input-14-c21a5ac899c2>\", line 1, in <module>\r\n>     model.fit(x=x_train, y=y_train, epochs=10)\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 873, in fit\r\n>     steps_name='steps_per_epoch')\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 239, in model_iteration\r\n>     model.reset_metrics()\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1173, in reset_metrics\r\n>     m.reset_states()\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\", line 199, in reset_states\r\n>     K.batch_set_value([(v, 0) for v in self.variables])\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 2880, in batch_set_value\r\n>     x.assign(np.asarray(value, dtype=dtype(x)))\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1053, in assign\r\n>     value_tensor = ops.convert_to_tensor(value, dtype=self.dtype)\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1050, in convert_to_tensor\r\n>     return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1108, in convert_to_tensor_v2\r\n>     as_ref=False)\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1186, in internal_convert_to_tensor\r\n>     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 304, in _constant_tensor_conversion_function\r\n>     return constant(v, dtype=dtype, name=name)\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 245, in constant\r\n>     allow_broadcast=True)\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 253, in _constant_impl\r\n>     t = convert_to_eager_tensor(value, ctx, dtype)\r\n> \r\n>   File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 114, in convert_to_eager_tensor\r\n>     return ops.EagerTensor(value, handle, device, dtype)\r\n> \r\n> ValueError: TypeError: len() of unsized object", "> ```\r\n> import tensorflow as tf\r\n> import matplotlib.pyplot as plt\r\n> from tensorflow.keras.models import Sequential\r\n> from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\r\n> \r\n> (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n> \r\n> x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\r\n> x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\r\n> input_shape = (28, 28, 1)\r\n> \r\n> x_train = x_train.astype('float32')\r\n> x_test = x_test.astype('float32')\r\n> \r\n> # normalize rgb codes by dividing by the max rgb value of 255...\r\n> x_train /= 255\r\n> x_test /= 255\r\n> \r\n> model = Sequential()\r\n> model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\r\n> model.add(MaxPooling2D(pool_size=(2, 2)))\r\n> model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\r\n> model.add(Dense(128, activation=tf.nn.relu))\r\n> model.add(Dropout(0.2))\r\n> model.add(Dense(10,activation=tf.nn.softmax))\r\n> \r\n> model.compile(optimizer = 'adam',\r\n>               loss='sparse_categorical_crossentropy',\r\n>               metrics=['accuracy'])\r\n> \r\n> \r\n> model.fit(x=x_train, y=y_train, epochs=10)\r\n> ```\r\n> \r\n> Here's a repro for an example that is giving me the same issue... just attempting to use mnist to go through some 2.0 stuff... hope this is the right place to respond... I'm getting the same or similar error as follows....\r\n> \r\n> > Traceback (most recent call last):\r\n> > File \"\", line 1, in \r\n> > model.fit(x=x_train, y=y_train, epochs=10)\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 873, in fit\r\n> > steps_name='steps_per_epoch')\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 239, in model_iteration\r\n> > model.reset_metrics()\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1173, in reset_metrics\r\n> > m.reset_states()\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\", line 199, in reset_states\r\n> > K.batch_set_value([(v, 0) for v in self.variables])\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 2880, in batch_set_value\r\n> > x.assign(np.asarray(value, dtype=dtype(x)))\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1053, in assign\r\n> > value_tensor = ops.convert_to_tensor(value, dtype=self.dtype)\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1050, in convert_to_tensor\r\n> > return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1108, in convert_to_tensor_v2\r\n> > as_ref=False)\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1186, in internal_convert_to_tensor\r\n> > ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 304, in _constant_tensor_conversion_function\r\n> > return constant(v, dtype=dtype, name=name)\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 245, in constant\r\n> > allow_broadcast=True)\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 253, in _constant_impl\r\n> > t = convert_to_eager_tensor(value, ctx, dtype)\r\n> > File \"C:\\Users\\c\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 114, in convert_to_eager_tensor\r\n> > return ops.EagerTensor(value, handle, device, dtype)\r\n> > ValueError: TypeError: len() of unsized object\r\n\r\nHi, i uninstall my Anaconda(python 3.6.3) and install the latest Anaconda and the error is gone."]}, {"number": 28650, "title": "Getting error message while converting data to TFrecords. TypeError: 'Self-emp-not-inc' has type str, but expected one of: bytes", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 64 bit \r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```Code. import pandas\r\nimport tensorflow as tf\r\n\r\n\r\nDATA_FILE = \"C:/Users/Harika Reddy/AppData/Local/Programs/Python/Python37-32/data-linter/adult.data\"\r\nOUTPUT_FILE = \"C:/Users/Harika Reddy/AppData/Local/Programs/Python/Python37-32/data-linter/adult.tfrecords\"\r\n\r\ncolumn_names = [\r\n    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\r\n    'marital-status', 'occupation', 'relationship', 'race', 'sex',\r\n    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\r\n    'label'\r\n]\r\n\r\nnumerical_features = ['age', 'fnlwgt', 'education-num', 'capital-gain',\r\n                      'capital-loss', 'hours-per-week']\r\n\r\ndf = pandas.read_csv(DATA_FILE).values\r\nwith tf.python_io.TFRecordWriter(OUTPUT_FILE) as writer:\r\n  for row in df:\r\n    example = tf.train.Example()\r\n    for col_name, val in zip(column_names, row):\r\n      if col_name in numerical_features:\r\n        example.features.feature[col_name].float_list.value.append(val)\r\n      else:\r\n        example.features.feature[col_name].bytes_list.value.append(val.strip())\r\n    writer.write(example.SerializeToString())\r\n#\r\n\r\n Getting Error message: \r\n\r\n(C:\\Anaconda) C:\\Users\\Harika Reddy\\AppData\\Local\\Programs\\Python\\Python37-32\\d\r\nta-linter\\demo>python convert_to_tfrecord.py\r\nTraceback (most recent call last):\r\n  File \"convert_to_tfrecord.py\", line 51, in <module>\r\n    example.features.feature[col_name].bytes_list.value.append(val.strip())\r\nTypeError: 'Self-emp-not-inc' has type str, but expected one of: bytes", "comments": ["Let us know which TensorFlow version you are using. Also will it be possible to provide a minimal code snippet to replicate the issue. Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28649, "title": "tf.lite.TFLiteConverter.from_saved_model error", "body": "**System information**\r\n- OS Platform and Distribution (MacOS 10.14.4):\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version (tensorflow==1.13.1):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, EQUAL, EXPAND_DIMS, FILL, FULLY_CONNECTED, GATHER, LESS, LOGICAL_AND, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, PACK, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, SHAPE, SPARSE_TO_DENSE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: LSTMBlockCell, Merge, Switch.\r\nTraceback (most recent call last):\r\n  File \"/Users/jim/src/ai/xiayu/deepiano/venv/bin/toco_from_protos\", line 11, in <module>\r\n    load_entry_point('tensorflow', 'console_scripts', 'toco_from_protos')()\r\n  File \"/Users/jim/src/ai/xiayu/deepiano/venv/lib/python3.7/site-packages/tensorflow-1.13.1-py3.7-macosx-10.14-x86_64.egg/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/jim/src/ai/xiayu/deepiano/venv/lib/python3.7/site-packages/tensorflow-1.13.1-py3.7-macosx-10.14-x86_64.egg/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/Users/jim/src/ai/xiayu/deepiano/venv/lib/python3.7/site-packages/tensorflow-1.13.1-py3.7-macosx-10.14-x86_64.egg/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, EQUAL, EXPAND_DIMS, FILL, FULLY_CONNECTED, GATHER, LESS, LOGICAL_AND, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, PACK, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, SHAPE, SPARSE_TO_DENSE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: LSTMBlockCell, Merge, Switch.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@toandrew Request you to please refer [link1](https://www.tensorflow.org/lite/guide/ops_select), [link2](https://www.tensorflow.org/lite/guide/ops_custom). Please let us know how it progresses. Thanks!", "@toandrew Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28648, "title": "Keras fit_generator using validation does not respect verbose argument", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1809\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.13 gpu\r\n- Python version: 3.6\r\n\r\n**Bug**\r\nWhen using `fit_generator` with validation the progress bar is printed regardless of the `verbose` setting.\r\n\r\n**Cause**\r\nLine 216 of tensorflow/python/keras/engine/training_generator.py calls `model_iterator` to do validation, but does not pass the `verbose` parameter, meaning that the default value of `verbose=1` is used.\r\n\r\n**Solution**\r\nAdd in the missing parameter, e.g.\r\n\r\n```\r\n    # Run the test loop every epoch during training.\r\n    if do_validation and not callbacks.model.stop_training:\r\n      val_results = model_iteration(\r\n          model,\r\n          validation_data,\r\n          steps_per_epoch=validation_steps,\r\n          batch_size=batch_size,\r\n          class_weight=class_weight,\r\n          workers=workers,\r\n          use_multiprocessing=use_multiprocessing,\r\n          max_queue_size=max_queue_size,\r\n          verbose=verbose,\r\n          mode='test')\r\n```\r\n", "comments": ["@geometrikal Could you provide a standalone code to reproduce the issue? Thanks!", "From #28738 \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Dense(1, input_shape=(1,))\r\n])\r\nmodel.compile(loss=\"mae\", optimizer=\"adam\")\r\n\r\ndef generator():\r\n    i=0\r\n    while 1:\r\n        yield (np.array([i]),[i])\r\n        i+=1\r\nvalData = (np.arange(10), np.arange(10))\r\n\r\nhistory = model.fit_generator(generator(), steps_per_epoch=5, verbose=0, validation_data=valData)\r\n```", "Hi @geometrikal, thanks for the issue! As you have a solution, this would be a good opportunity to submit a PR if you'd like. Would you like to?\r\n\r\nOtherwise I'll look into the fix", "Closing because this appears fixed in 1.14", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28648\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28648\">No</a>\n"]}, {"number": 28647, "title": "switch statement", "body": "I am a beginner of tensorflow\u3002When I use tensorflow, I want to do this:\r\nIf (global_step%2==0):\r\n\u00a0\u00a0\u00a0\u00a0\u00a0 Compute and store\r\nelif(global_step%2!=0):\r\n\u00a0\u00a0\u00a0\u00a0\u00a0 Get the stored date\r\nAfter analysis, I found that tensorflow builds a graph using tf.cond instead of if\u3002\u3002\u3002 else\u3002\u3002\u3002.So I modified the code\uff1atf.cond(tf.cast(global_step%2==0),tf.bool,Compute and store,\u00a0 Get the stored date).However, there are still problems with such modifications after testing\uff1aBecause the use of the tf.cond function will run both branches at the time of the feed, so add extra store and get overhead.\r\nThe purpose I want to accomplish is If (global_step%2==0)\uff0cCompute and store\uff1belse Get the stored date\u3002This can reduce the computational overhead\u3002\r\nSo I want to ask everyone how to achieve this in tensorflow?\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and can provide better support for such issues. Thanks!"]}, {"number": 28646, "title": "[TF 2.0] keras model.train_on_batch allocates extremely large CPU memory", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1809\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0-dev20190504\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: Geforce GTX 1070 8GB\r\n\r\n**Describe the current behavior**\r\nWhen I try to train_on_batch, TF 2.0 allcaotes memory endless and finally crashes. Using `tf.keras.backend.set_learning_phase(1)` can be temporary solution, but this trick doesn work with lateset build (dev20190511).\r\n\r\n**Describe the expected behavior**\r\nThe model can be trained regardless of `tf.keras.backend.set_learning_phase(1)` because I used model.train_on_batch(). It should handle learning phase internally.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef conv(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal'):\r\n    c = tf.keras.layers.Conv2D(\r\n        filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=initializer, use_bias=False)(x)\r\n\r\n    return c\r\n\r\n\r\ndef conv_bn(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal', bn_gamma_initializer='ones'):\r\n    c = conv(x, filters=filters, kernel_size=kernel_size,\r\n             strides=strides, padding=padding, initializer=initializer)\r\n\r\n    c_bn = tf.keras.layers.BatchNormalization(\r\n        gamma_initializer=bn_gamma_initializer)(c)\r\n\r\n    return c_bn\r\n\r\n\r\ndef conv_bn_relu(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal', bn_gamma_initializer='ones'):\r\n    c_bn = conv_bn(x, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding,\r\n                   initializer=initializer, bn_gamma_initializer=bn_gamma_initializer)\r\n\r\n    return tf.keras.layers.Activation('relu')(c_bn)\r\n\r\n\r\ndef conv_gap(x, output_filters, kernel_size=(1, 1)):\r\n    x = conv(x, filters=output_filters, kernel_size=kernel_size)\r\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\r\n\r\n    return x\r\n\r\n\r\ndef my_block(x, output_filters, inter_filters):\r\n    c1 = conv_bn_relu(x, inter_filters, (1, 1))\r\n    c2 = conv_bn_relu(c1, inter_filters, (3, 3))\r\n    c3 = conv_bn(\r\n        c2, output_filters, (1, 1), bn_gamma_initializer='zeros')\r\n\r\n    p = tf.keras.layers.add([c3, x])\r\n\r\n    return tf.keras.layers.Activation('relu')(p)\r\n\r\n\r\ndef my_block_inc(x, output_filters, inter_filters, strides1x1=(1, 1), strides2x2=(2, 2)):\r\n    c1 = conv_bn_relu(\r\n        x, inter_filters, (1, 1), strides=strides1x1)\r\n    c2 = conv_bn_relu(\r\n        c1, inter_filters, (3, 3), strides=strides2x2)\r\n    c3 = conv_bn(\r\n        c2, output_filters, (1, 1), bn_gamma_initializer='zeros')\r\n\r\n    strides = np.multiply(strides1x1, strides2x2)\r\n    s = conv_bn(\r\n        x, output_filters, (1, 1), strides=strides)  # shortcut\r\n\r\n    p = tf.keras.layers.add([c3, s])\r\n\r\n    return tf.keras.layers.Activation('relu')(p)\r\n\r\n\r\ndef repeat_blocks(x, block_delegate, count, **kwargs):\r\n    assert count >= 0\r\n\r\n    for _ in range(count):\r\n        x = block_delegate(x, **kwargs)\r\n    return x\r\n\r\n\r\n# This line makes trick!\r\n# tf.keras.backend.set_learning_phase(1)\r\nshape = (299, 299, 3)\r\n\r\ninputs = tf.keras.Input(shape, dtype='float32')\r\noutputs = conv_bn_relu(inputs, 256 // 4, (7, 7), strides=(2, 2))\r\noutputs = tf.keras.layers.MaxPool2D(\r\n    (3, 3), strides=(2, 2), padding='same')(outputs)\r\noutputs = my_block_inc(outputs, 256, 256 // 4, strides2x2=(1, 1))\r\noutputs = repeat_blocks(outputs, block_delegate=my_block,\r\n                        count=2,\r\n                        output_filters=256,\r\n                        inter_filters=256 // 4)\r\n\r\noutputs = my_block_inc(outputs, 512, 512 // 4, strides2x2=(2, 2))\r\noutputs = repeat_blocks(outputs, block_delegate=my_block,\r\n                        count=7,\r\n                        output_filters=512,\r\n                        inter_filters=512 // 4)\r\n\r\noutputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))\r\noutputs = repeat_blocks(outputs, block_delegate=my_block,\r\n                        count=40,\r\n                        output_filters=1024,\r\n                        inter_filters=1024 // 4)\r\n\r\noutputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))\r\noutputs = repeat_blocks(outputs, block_delegate=my_block,\r\n                        count=16,\r\n                        output_filters=1024,\r\n                        inter_filters=1024 // 4)\r\n\r\noutputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))\r\noutputs = repeat_blocks(outputs, block_delegate=my_block,\r\n                        count=16,\r\n                        output_filters=1024,\r\n                        inter_filters=1024 // 4)\r\n\r\noutputs = my_block_inc(outputs, 2048, 2048 // 4, strides2x2=(2, 2))\r\noutputs = repeat_blocks(outputs, block_delegate=my_block,\r\n                        count=6,\r\n                        output_filters=2048,\r\n                        inter_filters=2048 // 4)\r\n\r\noutputs = conv_gap(outputs, 1024)\r\noutputs = tf.keras.layers.Activation('sigmoid')(outputs)\r\nmodel = tf.keras.models.Model(\r\n    inputs=inputs, outputs=outputs, name='resnet_custom_v2')\r\n\r\noptimizer = tf.optimizers.Adam(0.001)\r\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy',\r\n              metrics=['mean_absolute_error'])\r\n\r\n\r\ndef make_batch(_):\r\n    x = np.ones((shape[0], shape[1], shape[2]), dtype='float32')\r\n    y = np.ones((1024), dtype='float32')\r\n\r\n    return (x, y)\r\n\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(\r\n    ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\r\ndataset = dataset.map(lambda x: tf.py_function(\r\n    make_batch, (x,), (tf.float32, tf.float32)))\r\ndataset = dataset.batch(10)\r\n\r\nfor x, y in dataset:\r\n    step_results = model.train_on_batch(x=x, y=y)\r\n    print(f'loss={step_results[0]}')\r\n\r\n```\r\n\r\nIf you uncomment `tf.keras.backend.set_learning_phase(1)`, the model may be successully trained. But with TF 2.0 dev20190511, the traininig is always failed regardless of `tf.keras.backend.set_learning_phase(1)`.\r\n", "comments": ["Related comment:\r\nhttps://github.com/tensorflow/tensorflow/issues/28110#issuecomment-490082449", "@KichangKim Able to reproduce the issue using build (dev20190511) allocates endless memory and finally crashes. ", "I found that TensorFlow 2.0.0-beta1 still have this issue.", "Same problem here. Exact same code was running fine on tf.keras 1.14 causes OOM with tf.keras 2.0. Code snippet:\r\n\r\n```\r\nimport numpy as np\r\nfrom tensorflow.keras.applications.vgg16 import VGG16\r\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\n\r\nimagenet_test = [e.g. your image directory]\r\n\r\nmodel = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\r\nmodel.compile('adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\r\n\r\ndatagen = ImageDataGenerator(preprocessing_function=preprocess_input)\r\ntest_generator = datagen.flow_from_directory(directory=imagenet_test,\r\n                                            batch_size=256,\r\n                                            class_mode='sparse',\r\n                                            target_size=(224, 224))\r\nsteps = np.ceil(len(test_generator.classes) / 256)\r\n\r\nmodel.evaluate_generator(test_generator, steps, verbose=1)\r\n```\r\nThe above code won't throw OOM error when use a very small batch size (e.g. 8) or with eager execution turned off.", "> Same problem here. Exact same code was running fine on tf.keras 1.14 causes OOM with tf.keras 2.0. Will update code snippet.\r\n\r\nI think OOM issue is related with #31871 too. Also it is not fixed yet.", "I found that tf-nightly-gpu==2.1.0.dev20191112 does not have this issue anymore. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28646\">No</a>\n"]}, {"number": 28645, "title": "AttributeError: module 'tensorflow.python.saved_model.signature_constants' has no attribute 'DEFAULT_TRAIN_SIGNATURE_DEF_KEY'", "body": "```\r\nSuccessfully installed tensorflow-1.13.1 tensorflow-probability-0.6.0\r\n\r\n```\r\n```\r\nimport tensorflow as tf\r\n```\r\n```\r\n/home/maksim/dev_projects/rl/venv/bin/python /home/maksim/dev_projects/rl/racetrack.py\r\nTraceback (most recent call last):\r\n  File \"/home/maksim/dev_projects/rl/racetrack.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow/__init__.py\", line 29, in <module>\r\n    from tensorflow._api.v1 import compat\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow/_api/v1/compat/__init__.py\", line 21, in <module>\r\n    from tensorflow._api.v1.compat import v1\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow/_api/v1/compat/v1/__init__.py\", line 626, in <module>\r\n    child_package_str=('tensorflow_estimator.python.estimator.api.estimator'))\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow/python/tools/component_api_helper.py\", line 56, in package_hook\r\n    child_pkg = importlib.import_module(child_package_str)\r\n  File \"/home/maksim/anaconda3/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator._api.v1 import estimator\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator._api.v1.estimator import experimental\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\r\n    import tensorflow_estimator.python.estimator.estimator_lib\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 22, in <module>\r\n    from tensorflow_estimator.python.estimator.canned.baseline import BaselineClassifier\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/baseline.py\", line 64, in <module>\r\n    from tensorflow_estimator.python.estimator import estimator\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 66, in <module>\r\n    from tensorflow_estimator.python.estimator import model_fn as model_fn_lib\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/model_fn.py\", line 36, in <module>\r\n    from tensorflow_estimator.python.estimator.export import export_lib\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/export/export_lib.py\", line 25, in <module>\r\n    from tensorflow.python.saved_model.model_utils import build_all_signature_defs\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow/python/saved_model/model_utils/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.saved_model.model_utils.export_utils import build_all_signature_defs\r\n  File \"/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow/python/saved_model/model_utils/export_utils.py\", line 51, in <module>\r\n    ModeKeys.TRAIN: signature_constants.DEFAULT_TRAIN_SIGNATURE_DEF_KEY,\r\nAttributeError: module 'tensorflow.python.saved_model.signature_constants' has no attribute 'DEFAULT_TRAIN_SIGNATURE_DEF_KEY'\r\n\r\nProcess finished with exit code 1\r\n```", "comments": ["@makslevental Please provide details about what platform you are using (operating system, architecture). Also, did you compile from source or install a binary? Thanks!\r\n", "@gadagashwini ubuntu 19.04, x86_64. pip installed\r\n\r\nmust be something having to do with the pip package because https://github.com/tensorflow/tensorflow/blob/73640d97f67d6bd5ec84375449c6b9bba179606b/tensorflow/python/saved_model/signature_constants.py#L139", "Hi,\r\nIs there any news about this issue? I believe I have the same issue in `Ubuntu 16.04.3` with \r\n```\r\ntensorflow==1.13.1\r\ntensorflow-estimator==1.13.0\r\ntensorflow-gpu==2.0.0a0\r\n```", "@makslevental and @sslavian812 Could you check the following things and share the details. I see more people are facing this issue. Thanks!\r\n1. pip list\r\n2. python3 --version\r\n3. pip3 --version\r\n4. virtualenv --version\r\n\r\nThanks!", "Thank you for the response, @jvishnuvardhan \r\n\r\nThe problem was about tensorflow versions, especially `tensorflow-gpu==2.0.0a0`.\r\n\r\nWith these versions, the error is gone:\r\n```\r\ntensorflow==1.12.2\r\ntensorflow-estimator==1.13.0\r\ntensorflow-gpu==1.13.1\r\n```\r\nregarding your quesion: \r\n\r\npython3 --version : **3.5.2**\r\npip3 --version: **pip 19.0.3**\r\nvirtualenv --version: **15.0.1**", "@sslavian812 Sorry for the delay in my response? Can you check with latest versions TF1.14, TF1.15 (tf-nightly), and 2.0.0b1? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28644, "title": "GPU not available after installation of TF 2.0 alpha", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Python version: Python 3.6.8 Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: CUDA 10, cuDNN release 9.2, V9.2.148\r\n- GPU model and memory: GTX1050M, 4GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI installed the tensorflow following the instructions:\r\n```pip install tensorflow-gpu==2.0.0-alpha0 tensorflow==2.0.0-alpha0```\r\nand then instruction on installing CUDA on Ubuntu 18.04\r\n```\r\n# Add NVIDIA package repositories\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\nsudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\nsudo apt-get update\r\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt-get update\r\n\r\n# Install NVIDIA driver\r\nsudo apt-get install --no-install-recommends nvidia-driver-418\r\n# Reboot. Check that GPUs are visible using the command: nvidia-smi\r\n\r\n# Install development and runtime libraries (~4GB)\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-10-0 \\\r\n    libcudnn7=7.4.1.5-1+cuda10.0  \\\r\n    libcudnn7-dev=7.4.1.5-1+cuda10.0\r\n```\r\nAfter the above steps, I checked the installation with\r\n```\r\n$ nvidia-smi\r\nSun May 12 21:43:20 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1050    On   | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   50C    P0    N/A /  N/A |    356MiB /  4042MiB |      3%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1313      G   /usr/lib/xorg/Xorg                           207MiB |\r\n|    0      1819      G   kwin_x11                                      19MiB |\r\n|    0      1823      G   /usr/bin/krunner                              11MiB |\r\n|    0      1825      G   /usr/bin/plasmashell                          50MiB |\r\n|    0      2287      G   ...Charm-P/ch-0/191.6605.12/jre64/bin/java     2MiB |\r\n|    0      2355      G   ...uest-channel-token=13980915512808857744    63MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n$ nvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Tue_Jun_12_23:07:04_CDT_2018\r\nCuda compilation tools, release 9.2, V9.2.148\r\n```\r\n\r\nWhen I test, however, whether TF runs on GPU, I get:\r\n\r\n```\r\nimport tensorflow as tf\r\nLimited tf.compat.v2.summary API due to missing TensorBoard installation\r\nLimited tf.summary API due to missing TensorBoard installation\r\ntf.debugging.set_log_device_placement(True)\r\n# Creates some tensors\r\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\nprint(c)\r\n2019-05-12 21:45:27.263715: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-05-12 21:45:27.277136: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2019-05-12 21:45:27.277588: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x561bde54b680 executing computations on platform Host. Devices:\r\n2019-05-12 21:45:27.277607: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\nExecuting op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2019-05-12 21:45:27.278422: I tensorflow/core/common_runtime/eager/execute.cc:394] Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2019-05-12 21:45:27.278804: I tensorflow/core/common_runtime/eager/execute.cc:394] Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\n```\r\nwhile previously (TF 1.13 with CUDA 9) it worked fine.", "comments": ["@colonder How did you fix this? getting same issue\r\n"]}, {"number": 28643, "title": "Keras fit_generator fails in graph mode when input is dict", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): simple keras model combined from examples\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS X 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: no, CPU-version\r\n- GPU model and memory: no, CPU-version\r\n\r\n**Describe the current behavior**\r\nWhen running provided code it fails only when fit_generator executed in graph mode.\r\nIn other cases fit() and fit_generator() work well.\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-11180391ba3c> in <module>()\r\n     61 # Fails\r\n     62 model.run_eagerly = False\r\n---> 63 model.fit_generator(input_fn())\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1513         shuffle=shuffle,\r\n   1514         initial_epoch=initial_epoch,\r\n-> 1515         steps_name='steps_per_epoch')\r\n   1516 \r\n   1517   def evaluate_generator(self,\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training_generator.pyc in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n    255 \r\n    256       is_deferred = not model._is_compiled\r\n--> 257       batch_outs = batch_function(*batch_data)\r\n    258       if not isinstance(batch_outs, list):\r\n    259         batch_outs = [batch_outs]\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.pyc in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1248     else:\r\n   1249       if not isinstance(K.symbolic_learning_phase(), int):\r\n-> 1250         ins = x + y + sample_weights + [True]\r\n   1251       else:\r\n   1252         ins = x + y + sample_weights\r\n\r\nTypeError: unsupported operand type(s) for +: 'dict' and 'list'\r\n```\r\n\r\n**Describe the expected behavior**\r\ntf.keras.Model().fit_generator() should work properly with input of type dict.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef input_fn():\r\n    x = np.random.random((1024, 10))\r\n    y = np.random.randint(2, size=(1024, 1))\r\n    x = tf.cast(x, tf.float32)\r\n    \r\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\r\n    dataset = dataset.shuffle(100)\r\n    dataset = dataset.batch(32)\r\n    dataset = dataset.repeat(10)\r\n    \r\n    def _extract_features(_x, _y):\r\n        features = {\r\n            'x': _x,\r\n            'z': tf.zeros_like(_x)\r\n        }\r\n        \r\n        return features, _y\r\n\r\n    dataset = dataset.map(_extract_features)\r\n\r\n    return dataset\r\n\r\n\r\nclass MyModel0(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        \r\n        self.features = tf.keras.layers.DenseFeatures([\r\n            tf.feature_column.numeric_column('x', shape=(10,))\r\n        ])\r\n        self.dense1 = tf.keras.layers.Dense(16, activation='relu')\r\n        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\r\n        \r\n    def call(self, inputs, training=None, mask=None):\r\n        outputs = self.features(inputs)\r\n        outputs = self.dense1(outputs)\r\n        outputs = self.dense2(outputs)\r\n\r\n        return outputs\r\n\r\n\r\nmodel = MyModel()\r\nmodel.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.05))\r\n\r\n# Works\r\nmodel.run_eagerly = True\r\nmodel.fit(input_fn())\r\n\r\n# Works\r\nmodel.run_eagerly = False\r\nmodel.fit(input_fn())\r\n\r\n# Works\r\nmodel.run_eagerly = True\r\nmodel.fit_generator(input_fn())\r\n\r\n# Fails\r\nmodel.run_eagerly = False\r\nmodel.fit_generator(input_fn())\r\n```", "comments": ["@shkarupa-alex Able to reproduce the issue with the provided code.\r\n\r\n     62 model.run_eagerly = False\r\n---> 63 model.fit_generator(input_fn())\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1248     else:\r\n   1249       if not isinstance(K.symbolic_learning_phase(), int):\r\n-> 1250         ins = x + y + sample_weights + [True]\r\n   1251       else:\r\n   1252         ins = x + y + sample_weights\r\n\r\nTypeError: unsupported operand type(s) for +: 'dict' and 'list'", "tf-nightly is free from this issue (but that is tf 1.x branch as i know)", "I think this issue has been fixed by https://github.com/tensorflow/tensorflow/commit/254e39f810fd88e14c3743a5aaf903e5dd6bb397.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28643\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28643\">No</a>\n"]}, {"number": 28642, "title": "Is it possible to train custom models for Tensorflow Android?", "body": "Hello,\r\n\r\nCurrently, I'm working on one projects and I need to train my models. I want to train models using YoloV2 and my question is that will my custom trained models work on Android Tensorflow if I add .pb model into the assets folder?\r\n\r\n**System information**\r\n- TensorFlow version 1.5.0\r\n- Are you willing to contribute it (YES)\r\n\r\n\r\n**It will not change the API .**\r\n\r\n**I would like to benefit from this feature.**\r\n\r\n", "comments": ["You can work with pb models. \r\nCheck out [https://github.com/dailystudio/ml/tree/master/object_detection](https://github.com/dailystudio/ml/tree/master/object_detection)\r\n\r\nFor better performance in terms of computation and time, you can convert your pb files into tflite and run on mobile phones", "I would like to train my own models with YoloV2 and add it to Android/assets folder with .txt file(label names). I have tried using Faster RCNN and my android didn't accept that .pb model and application stopped working. \r\n", "@nijatmursali It looks This question is better asked on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and provides support faster. Let us know. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28641, "title": "verbs: fixes GPU build broken by 4c3a1fb", "body": "Use the default argument as in DeviceContext::CopyCPUTensorToDevice.\r\n\r\n@poxvoculi is probably the best person to review this, but he's currently on leave.\r\n\r\nPing @dubey if you have time reviewing this. As r1.14 is to be recut, in case we missed it, ping @bananabowl to see if we can cherry-pick this PR.\r\n\r\nThanks!", "comments": []}, {"number": 28640, "title": "verbs: fixes GPU build broken by 55a7da3", "body": "Use the default argument sync_dst_compute=true as in DeviceContext::CopyCPUTensorToDevice.\r\n\r\nSigned-off-by: Bairen Yi <byi@connect.ust.hk>", "comments": ["Wrong commit message. Will reopen another PR."]}, {"number": 28639, "title": "tf 1.8.0 with horovod hang at the middle of training", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n`yes`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n`Linux Ubuntu 16.04`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n`No`\r\n- TensorFlow installed from (source or binary):\r\n`pip install tensorflow_gpu==1.8.0`\r\n- TensorFlow version (use command below):\r\n```\r\nwork@job1b-pub-v100-5wh5z:~$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.8.0-0-g93bc2e2072 1.8.0\r\n```\r\n- Python version:\r\n`Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)` \r\n- Bazel version (if compiling from source):\r\nNone\r\n- GCC/Compiler version (if compiling from source):\r\nNone\r\n- CUDA/cuDNN version:\r\n`cuda 9.0.176-1`\r\n- GPU model and memory:\r\n`V100` and `32G` mem\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\ni run the webvision train code in tf 1.8.0 with horovod (nccl), in 4 (nodes) * 8 Nvidia V100 GPU cluster, but the trainning job rang at the middle of training. and one of the nodes processes info are\r\n\r\n```\r\nwork       1955      0  0 May09 ?        00:00:00 /bin/sh -c     PATH=/usr/local/openmpi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/usr/local/openmpi/lib:$L\r\nwork       1961   1955  0 May09 ?        00:00:08 /usr/local/openmpi/bin/orted -mca ess env -mca ess_base_jobid 2660237312 -mca ess_base_vpid 1 -mca ess_bas\r\nwork       1965   1961 99 May09 ?        3-20:29:04 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3\r\nwork       1966   1961 99 May09 ?        3-18:25:41 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3\r\nwork       1967   1961 99 May09 ?        5-18:09:20 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3\r\nwork       1968   1961 99 May09 ?        5-12:52:59 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3\r\nwork       1969   1961 99 May09 ?        3-18:07:41 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3\r\nwork       1970   1961 99 May09 ?        3-18:04:06 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3\r\nwork       1971   1961 99 May09 ?        3-18:19:37 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3\r\nwork       1972   1961 99 May09 ?        3-17:27:18 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3\r\nwork       2665   1965  0 May09 ?        00:00:12 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/\r\nwork       2759   1972  0 May09 ?        00:00:14 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/\r\nwork       2813   1971  0 May09 ?        00:00:12 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/\r\nwork       2839   1966  0 May09 ?        00:00:14 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/\r\nwork       2853   1969  0 May09 ?        00:00:12 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/\r\nwork       2861   1967  0 May09 ?        00:00:12 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/\r\nwork       2876   1968  0 May09 ?        00:00:14 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/\r\nwork       2882   1970  0 May09 ?        00:00:14 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/\r\nwork     165333   1965  0 May10 ?        00:00:10 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/\r\n```\r\n\r\n[ps-ef-info.txt](https://github.com/tensorflow/tensorflow/files/3170040/ps-ef-info.txt)\r\n\r\nand i print the `1965` process python call stack it shows\r\n\r\n```\r\nThread 0x7f50cdffb700\r\n  File \"/home/work/anaconda3/lib/python3.6/threading.py\", line 884, in _bootstrap\r\n    self._bootstrap_inner()\r\n  File \"/home/work/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/work/anaconda3/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py\", line 391, in _run\r\n    enqueue_callable()\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1244, in _single_operation_run\r\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\n\r\nThread 0x7f5a3f48c700\r\n  File \"webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py\", line 1065, in <module>\r\n    tf.app.run(main=main)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py\", line 1036, in main\r\n    save_model_secs=flags.save_model_secs)\r\n  File \"/home/work/user-job-dir/webvision-train-code/moxing/tensorflow/executor/learning_builder.py\", line 473, in run\r\n    save_model_secs, export_model, use_trt, fetch_strategy_fn, save_model_steps)\r\n  File \"/home/work/user-job-dir/webvision-train-code/moxing/tensorflow/executor/learning_wrapper.py\", line 270, in run\r\n    self._run()\r\n  File \"/home/work/user-job-dir/webvision-train-code/moxing/tensorflow/executor/learning_wrapper.py\", line 540, in _run\r\n    self._save_model_steps)\r\n  File \"/home/work/user-job-dir/webvision-train-code/moxing/tensorflow/executor/learning.py\", line 878, in run\r\n    self.training()\r\n  File \"/home/work/user-job-dir/webvision-train-code/moxing/tensorflow/executor/learning.py\", line 1530, in training\r\n    self.train_step(sess)\r\n  File \"/home/work/user-job-dir/webvision-train-code/moxing/tensorflow/executor/learning.py\", line 1274, in train_step\r\n    feed_dict=feed_dict)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 567, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1043, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1119, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1191, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 971, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\n```\r\n\r\n[python-call-stack.txt](https://github.com/tensorflow/tensorflow/files/3170036/python-call-stack.txt)\r\n\r\nand gdb bt of this process are\r\n\r\n```\r\n(gdb) bt full\r\n#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38\r\nNo locals.\r\n#1  0x00007f5971f7da54 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nNo symbol table info available.\r\n#2  0x00007f5971f7d221 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nNo symbol table info available.\r\n#3  0x00007f5971f7a764 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nNo symbol table info available.\r\n#4  0x00007f5971f7ac85 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) ()\r\n   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nNo symbol table info available.\r\n#5  0x00007f5971f80f2b in tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, tensorflow::CancellationManager*, long long)\r\n    () from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nNo symbol table info available.\r\n#6  0x00007f5971f85010 in tensorflow::DirectSession::RunInternal(long long, tensorflow::RunOptions const&, tensorflow::CallFrameInterface*, tensorflow::DirectSession::ExecutorsAndKeys*, tensorflow::RunMetadata*) ()\r\n   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nNo symbol table info available.\r\n#7  0x00007f5971f8e3d5 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()\r\n   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nNo symbol table info available.\r\n#8  0x00007f596f424c8a in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) ()\r\n   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nNo symbol table info available.\r\n#9  0x00007f596f425886 in TF_SessionRun () from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nNo symbol table info available.\r\n#10 0x00007f596f0d4186 in tensorflow::TF_SessionRun_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Outp---Type <return> to continue, or q <return> to quit---\r\nut> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()\r\n   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nNo symbol table info available.\r\n#11 0x00007f596f0d42ca in tensorflow::TF_SessionRun_wrapper(TF_Session*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()\r\n   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nNo symbol table info available.\r\n#12 0x00007f596f090a6e in _wrap_TF_SessionRun_wrapper ()\r\n   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```\r\n\r\n[gdb-backtrace.txt](https://github.com/tensorflow/tensorflow/files/3170038/gdb-backtrace.txt)\r\n\r\nand it seems it wait at the `direct_session.cc`, L552\r\n\r\n```\r\n  WaitForNotification(&run_state, &step_cancellation_manager,\r\n                      run_options.timeout_in_ms() > 0\r\n                          ? run_options.timeout_in_ms()\r\n                          : operation_timeout_in_ms_);\r\n```\r\n\r\nbut i never set the `run_options.timeout_in_ms` and `config.operation_timeout_in_ms`, and it seems it wait for the deadline in gdb bt, so the question is how can this forever hang happen when the `nsync::nsync_cv_wait_with_deadline` be called ? (at least for now, it runs almost three days ...)\r\n\r\noh, i know why ... the gdb lack of some output (as it lack of symbols ...) ref [https://github.com/tensorflow/tensorflow/issues/26559](https://github.com/tensorflow/tensorflow/issues/26559)\r\n\r\n`nsync::nsync_cv_wait` -> `nsync::nsync_cv_wait_with_deadline`\r\n\r\n**Describe the expected behavior**\r\n\r\nNot hang at the middle of training, the final train log are\r\n\r\n```\r\nINFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 368.464     ent_loss: 4.997 top-1: 0.375    top-5: 0.547    reg_loss: 0.309 total_loss: 5.306\r\nINFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 368.328     ent_loss: 4.942 top-1: 0.344    top-5: 0.531    reg_loss: 0.309 total_loss: 5.251\r\nINFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 368.118     ent_loss: 4.036 top-1: 0.578    top-5: 0.672    reg_loss: 0.309 total_loss: 4.344\r\nINFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 367.937     ent_loss: 4.518 top-1: 0.453    top-5: 0.578    reg_loss: 0.309 total_loss: 4.827\r\nINFO:tensorflow:global_step/sec: 1.46428\r\nINFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 367.743     ent_loss: 4.083 top-1: 0.500    top-5: 0.703    reg_loss: 0.309 total_loss: 4.392\r\nINFO:tensorflow:global_step/sec: 1.46428\r\nINFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 286.646     ent_loss: 3.888 top-1: 0.484    top-5: 0.641    reg_loss: 0.309 total_loss: 4.197\r\nINFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 286.646     ent_loss: 3.888 top-1: 0.484    top-5: 0.641    reg_loss: 0.309 total_loss: 4.197\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nnot sure ...\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nall nodes GPU utils is 100%\r\n\r\n```\r\n[root@job1b-pub-v100-5wh5z ~]# /var/paas/nvidia/bin/nvidia-smi \r\nSun May 12 20:19:41 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.93       Driver Version: 410.93       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-SXM2...  Off  | 00000000:2D:00.0 Off |                    0 |\r\n| N/A   41C    P0    65W / 300W |  29952MiB / 32480MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla V100-SXM2...  Off  | 00000000:32:00.0 Off |                    0 |\r\n| N/A   40C    P0    71W / 300W |  29954MiB / 32480MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla V100-SXM2...  Off  | 00000000:5B:00.0 Off |                    0 |\r\n| N/A   42C    P0    69W / 300W |  29942MiB / 32480MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla V100-SXM2...  Off  | 00000000:5F:00.0 Off |                    0 |\r\n| N/A   37C    P0    64W / 300W |  29942MiB / 32480MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  Tesla V100-SXM2...  Off  | 00000000:B5:00.0 Off |                    0 |\r\n| N/A   40C    P0    67W / 300W |  29954MiB / 32480MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Tesla V100-SXM2...  Off  | 00000000:BE:00.0 Off |                    0 |\r\n| N/A   39C    P0    66W / 300W |  29954MiB / 32480MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Tesla V100-SXM2...  Off  | 00000000:E1:00.0 Off |                    0 |\r\n| N/A   41C    P0    67W / 300W |  29954MiB / 32480MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Tesla V100-SXM2...  Off  | 00000000:E9:00.0 Off |                    0 |\r\n| N/A   41C    P0    67W / 300W |  29954MiB / 32480MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     71807      C   /home/work/anaconda3/bin/python            29939MiB |\r\n|    1     71808      C   /home/work/anaconda3/bin/python            29939MiB |\r\n|    2     71809      C   /home/work/anaconda3/bin/python            29927MiB |\r\n|    3     71810      C   /home/work/anaconda3/bin/python            29927MiB |\r\n|    4     71811      C   /home/work/anaconda3/bin/python            29939MiB |\r\n|    5     71812      C   /home/work/anaconda3/bin/python            29939MiB |\r\n|    6     71813      C   /home/work/anaconda3/bin/python            29939MiB |\r\n|    7     71814      C   /home/work/anaconda3/bin/python            29939MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nand dmesg output the segment fault info\r\n\r\n```\r\n[Sun May 12 19:53:37 2019] sh[429661]: segfault at 7fff7f09f428 ip 00007fff7f09f428 sp 00007fff7f09e078 error 15\r\n[Sun May 12 19:54:37 2019] sh[433888]: segfault at 7ffe30bcccd8 ip 00007ffe30bcccd8 sp 00007ffe30bcb928 error 15\r\n[Sun May 12 19:55:37 2019] sh[437936]: segfault at 7ffc5e4a0958 ip 00007ffc5e4a0958 sp 00007ffc5e49f5a8 error 15\r\n[Sun May 12 19:56:37 2019] sh[442090]: segfault at 7ffc4ad375b8 ip 00007ffc4ad375b8 sp 00007ffc4ad36208 error 15\r\n[Sun May 12 19:57:37 2019] sh[446313]: segfault at 7ffcba547148 ip 00007ffcba547148 sp 00007ffcba545d98 error 15\r\n[Sun May 12 19:57:37 2019] sh[446409]: segfault at 7ffca4e2dc18 ip 00007ffca4e2dc18 sp 00007ffca4e2c868 error 15\r\n[Sun May 12 19:58:37 2019] sh[450425]: segfault at 7ffd09ab86d8 ip 00007ffd09ab86d8 sp 00007ffd09ab7328 error 15\r\n[Sun May 12 19:59:37 2019] sh[454603]: segfault at 7ffca20adf08 ip 00007ffca20adf08 sp 00007ffca20acb58 error 15\r\n[Sun May 12 20:00:37 2019] sh[499]: segfault at 7ffc588dd7e8 ip 00007ffc588dd7e8 sp 00007ffc588dc438 error 15\r\n[Sun May 12 20:01:37 2019] sh[4810]: segfault at 7ffe67cdfbd8 ip 00007ffe67cdfbd8 sp 00007ffe67cde828 error 15\r\n[Sun May 12 20:02:37 2019] sh[8913]: segfault at 7ffc6fd6a3b8 ip 00007ffc6fd6a3b8 sp 00007ffc6fd69008 error 15\r\n[Sun May 12 20:02:37 2019] sh[8973]: segfault at 7ffe613b4388 ip 00007ffe613b4388 sp 00007ffe613b2fd8 error 15\r\n[Sun May 12 20:03:37 2019] sh[13161]: segfault at 7fff17ac1bd8 ip 00007fff17ac1bd8 sp 00007fff17ac0828 error 15\r\n[Sun May 12 20:04:37 2019] sh[17308]: segfault at 7fff5fe44448 ip 00007fff5fe44448 sp 00007fff5fe43098 error 15\r\n[Sun May 12 20:05:37 2019] sh[21475]: segfault at 7ffeca86ff58 ip 00007ffeca86ff58 sp 00007ffeca86eba8 error 15\r\n[Sun May 12 20:06:37 2019] sh[25887]: segfault at 7ffea93cd2a8 ip 00007ffea93cd2a8 sp 00007ffea93cbef8 error 15\r\n[Sun May 12 20:07:37 2019] sh[30090]: segfault at 7ffe9a32ba78 ip 00007ffe9a32ba78 sp 00007ffe9a32a6c8 error 15\r\n[Sun May 12 20:07:37 2019] sh[30151]: segfault at 7ffe0e8b00e8 ip 00007ffe0e8b00e8 sp 00007ffe0e8aed38 error 15\r\n[Sun May 12 20:08:37 2019] sh[34257]: segfault at 7fff46231e38 ip 00007fff46231e38 sp 00007fff46230a88 error 15\r\n[Sun May 12 20:09:37 2019] sh[38356]: segfault at 7ffc8b9d2ff8 ip 00007ffc8b9d2ff8 sp 00007ffc8b9d1c48 error 15\r\n[Sun May 12 20:10:37 2019] sh[42685]: segfault at 7ffd19c3c068 ip 00007ffd19c3c068 sp 00007ffd19c3acb8 error 15\r\n[Sun May 12 20:11:37 2019] sh[46730]: segfault at 7ffc11dcc218 ip 00007ffc11dcc218 sp 00007ffc11dcae68 error 15\r\n[Sun May 12 20:12:37 2019] sh[50885]: segfault at 7ffde26e73c8 ip 00007ffde26e73c8 sp 00007ffde26e6018 error 15\r\n[Sun May 12 20:12:37 2019] sh[51091]: segfault at 7ffcaaf42788 ip 00007ffcaaf42788 sp 00007ffcaaf413d8 error 15\r\n[Sun May 12 20:13:37 2019] sh[55116]: segfault at 7ffc9faf70a8 ip 00007ffc9faf70a8 sp 00007ffc9faf5cf8 error 15\r\n[Sun May 12 20:14:37 2019] sh[59159]: segfault at 7fff7ed38518 ip 00007fff7ed38518 sp 00007fff7ed37168 error 15\r\n[Sun May 12 20:15:37 2019] sh[63389]: segfault at 7ffcf8b8d068 ip 00007ffcf8b8d068 sp 00007ffcf8b8bcb8 error 15\r\n[Sun May 12 20:16:37 2019] sh[67564]: segfault at 7fffc0e890d8 ip 00007fffc0e890d8 sp 00007fffc0e87d28 error 15\r\n[Sun May 12 20:17:37 2019] sh[71615]: segfault at 7ffd22222168 ip 00007ffd22222168 sp 00007ffd22220db8 error 15\r\n[Sun May 12 20:17:37 2019] sh[71666]: segfault at 7ffc62aff5e8 ip 00007ffc62aff5e8 sp 00007ffc62afe238 error 15\r\n[Sun May 12 20:18:37 2019] sh[75987]: segfault at 7ffdf614bd88 ip 00007ffdf614bd88 sp 00007ffdf614a9d8 error 15\r\n[Sun May 12 20:19:37 2019] sh[80035]: segfault at 7ffffcbe7c38 ip 00007ffffcbe7c38 sp 00007ffffcbe6888 error 15\r\n```\r\n", "comments": ["Ping @alsrgv; mind to take a look here?", "@byronyi  .. thx byr (BUPT ?) ... i just print the current thread bt ... i should print all the threads bt (i.e. `thread apply all bt`), my fault\r\n\r\nbut in my memory, i see some horovod `common` call and it end with `cudaEvent*` ...\r\n\r\nfortunately, it raise at a high rate ... i can post more backtrace info in processes", "I\u2019ll suggest you to raise this issue to horovod directly. You could try upgrading your TF as well.", "@zrss, I'd suggest TF upgrade as well.  Can you run with NCCL_DEBUG=INFO, and share NCCL debug output & version info?", "> @zrss, I'd suggest TF upgrade as well. Can you run with NCCL_DEBUG=INFO, and share NCCL debug output & version info?\r\n\r\nthx @alsrgv , i run a new job and it hang again, but this time i get more info ... (yes, included NCCL_DEBUG log)\r\n\r\n[nccl-debug-info.txt](https://github.com/tensorflow/tensorflow/files/3172031/nccl-debug-info.txt)\r\n\r\nps -ef info in one (A) node ( 4 nodes * 8 V100 GPU)\r\n\r\n[ps-ef-info.txt](https://github.com/tensorflow/tensorflow/files/3172035/ps-ef-info.txt)\r\n\r\nnvidia-smi info in A node\r\n\r\n[nvidia-smi-info.txt](https://github.com/tensorflow/tensorflow/files/3172037/nvidia-smi-info.txt)\r\n\r\nthe final train output\r\n\r\n[train-final-output.txt](https://github.com/tensorflow/tensorflow/files/3172038/train-final-output.txt)\r\n\r\nthe python thread stack of process 144 (144 - 151 act like the same, and the remain processes are s3 connections)\r\n\r\n[python-thread-stack.txt](https://github.com/tensorflow/tensorflow/files/3172043/python-thread-stack.txt)\r\n\r\ngdb `info thread` of process 144\r\n\r\n[gdb-info-threads.txt](https://github.com/tensorflow/tensorflow/files/3172044/gdb-info-threads.txt)\r\n\r\ngdb `thread apply all bt` of process 144\r\n\r\n[gdb-all-threads-bt-part1.txt](https://github.com/tensorflow/tensorflow/files/3172078/gdb-all-threads-bt-part1.txt)\r\n\r\n[gdb-all-threads-bt-part2.txt](https://github.com/tensorflow/tensorflow/files/3172079/gdb-all-threads-bt-part2.txt)\r\n\r\n... TF upgrade needs some time ...\r\n\r\n\r\n\r\n\r\n\r\n", "@zrss, thanks for all the details.  Can you try to downgrade your NCCL to 2.3.7 and reinstall Horovod (with `--no-cache-dir`)?  I'm wondering if https://github.com/NVIDIA/nccl/pull/185 could be causing this issue.\r\n\r\nIf that doesn't help, could you dump `thread apply all bt` from all 8 processes?", "@alsrgv thx for reply :smiley:, i will try, `horovod` is quite a interesting framework, i would like to dive into it ~ \r\n\r\nand, to clarify ... the segment faults in my previous post are unrelated ...\r\n\r\n```\r\n[Sun May 12 20:17:37 2019] sh[71666]: segfault at 7ffc62aff5e8 ip 00007ffc62aff5e8 sp 00007ffc62afe238 error 15\r\n[Sun May 12 20:18:37 2019] sh[75987]: segfault at 7ffdf614bd88 ip 00007ffdf614bd88 sp 00007ffdf614a9d8 error 15\r\n[Sun May 12 20:19:37 2019] sh[80035]: segfault at 7ffffcbe7c38 ip 00007ffffcbe7c38 sp 00007ffffcbe6888 error 15\r\n``` \r\n\r\nwe found that one of our agents periodly call the monitor shell script (sh), but this script failed to execute as it lack of some sys libs ...", "@zrss Is this issue resolved? Thanks!", "thx for the reply ... but the hang issue is still be somewhere (and we observed the same case in tf 1.13 cuda 10 nccl 2.4.2), especially, it always hang at a [bert](https://github.com/google-research/bert) model train job", "@zrss, NCCL 2.4.2 is [known to hang](https://github.com/horovod/horovod/issues/1078#issuecomment-493210708), can you downgrade to NCCL 2.3.7 or upgrade to NCCL 2.4.7?", "thx @alsrgv  ~ i will tell my colleagues for this action, they have been stuck at this problem for too long since last time i posted this issue .... -_-# i didn't have much time to follow it", "@zrss Is this issue resolved? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28639\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28639\">No</a>\n", "@jvishnuvardhan , sorry for the late reply, we suppose it is resolved now after upgrading the nccl to 2.4.7"]}, {"number": 28637, "title": "make condition of DumpGraphToFile in jit passes be same", "body": " When i debugged the xla module, i found the 3 passes in jit used different conditions to dump their graphs. So here i modified the condition of DumpGraphToFile in MarkForCompilationPass in order to \r\n align with EncapsulateSubgraphPass and BuildXlaOpsPass. ", "comments": ["@rthadur I thought I already approved this CL internally.  Did that not go through?", "@sanjoy there were new changes pushed by @LogX2  so we need your approval again , thank you ", "sorry to disturb you, it is caused by another code change in a wrong branch and have been  corrected after i realized that. ", "@LogX2 there are still conflicts , can you please resolve them."]}, {"number": 28636, "title": "[Java] Add eager tensor support", "body": "PR #5/5 for eager execution environment support in Java\r\n\r\nAmong other things, it adds a `Output.tensor()` method to retrieve the result of an operation that has been executed eagerly.\r\n\r\nSome more optimizations are about to come in separate PRs. Still, after merging this one, it will be possible to use eager execution of TensorFlow operations in Java.\r\n\r\nCC: @sjamesr ", "comments": []}, {"number": 28635, "title": "`ConcreteFunction` does not raise errors when inputs and `tf.TensorSpec` are not compatible with each other", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): osx\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.1-dev20190509\r\n- Python version: 3.7.1\r\n- Bazel version (if compiling from source):None\r\n- GCC/Compiler version (if compiling from source):None\r\n- CUDA/cuDNN version:None\r\n- GPU model and memory:None\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n```\r\nimport tensorflow as tf\r\ntf.enable_v2_behavior()\r\n\r\n\r\n@tf.function\r\ndef test_rank(x):\r\n    return x\r\n\r\n\r\ntest_rank_cf = test_rank.get_concrete_function(tf.TensorSpec([None, None], tf.float32))\r\n# run smoothly, should raise errors here?\r\ntest_rank_cf(tf.random.normal((2, 3, 4)))\r\n```\r\n**Describe the expected behavior**\r\nErrors should be rasied if inputs and `tf.TensorSpec` are not compatible with each other\r\n**Code to reproduce the issue**\r\nSee the above\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I was able to replicate the behavior with TensorFlow 1.14.1-dev20190509 on Colab.", "@zakizhou I think it is working as expected. Please check the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/c7fde452c04e77184a796f4a700cb80d/untitled196.ipynb).\r\n\r\nWhen I provide input of `int32`, it throws an error as the op is looking for `float32`. I added this line in the end\r\n`test_rank_cf(tf.fill([2,3,4],3))   # this throws an error as input is int32 whereas it was expecting float32`\r\n\r\nThanks!", "@jvishnuvardhan \r\n`tf.TensorSpec([None, None], tf.float32)` means that the tensor provided for this concrete function should be a Tensor of rank two and tf.float32 as dtype, neither a mismatch with dtype nor rank should raise errors, in your example, there is a mismatch between dtype, so this concrete function exactly raises errors on the mismatch of dtype as it is expected to do, but `tf.random.normal((2, 3, 4))` is a tensor of rank three, not rank two, so errors should also be raised.\r\n```\r\n# this line should raise errors, but it doesn't\r\ntest_rank_cf(tf.cast(tf.fill((2, 3, 4), 3), tf.float32))\r\n```", "This is somewhat working as intended. By design concretefunction performs very little validation; you trade this against getting very low python overhead at execution time.\r\n\r\nIf you want validation pass a signature argument to tf.function and then call it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28635\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28635\">No</a>\n"]}, {"number": 28634, "title": "Add TFTRT Resize Op Converter", "body": "1. convert_nodes.cc\r\n    - Add convertResize i.e. resize op converter. Map it to nvinfer1::IResizeLayer.\r\n    - Support conversion for ResizeBilinear and ResizeNearestNeighbor op.\r\n\r\n2. convert_nodes_test.cc\r\n    - Unit test for convertResize converter.", "comments": []}, {"number": 28633, "title": "Fix TensorRT-6 Version Parsing", "body": "1. find_cuda_config.py - Fix version parsing for TRT 6.  If required version >= 6, parse version information from NvInferVersion.h, else parse version information from NvInfer.h.", "comments": ["@annarev Do you have any comments or suggestions on the review?", "@aaroey Addressed review comments."]}, {"number": 28632, "title": "AttributeError: module 'tensorflow' has no attribute 'gfile'", "body": "Hi,\r\n\r\nI am trying to load a file using the following code below. However, whenever I run my code I get an error stating that `AttributeError: module 'tensorflow' has no attribute 'gfile'` . I have successfully installed gin using `pip install gin-config`.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef load_directory_data(directory):\r\n  data = {}\r\n  data[\"sentence\"] = []\r\n  data[\"sentiment\"] = []\r\n  for file_path in os.listdir(directory):\r\n    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\r\n      data[\"sentence\"].append(f.read())\r\n      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\r\n  return pd.DataFrame.from_dict(data)\r\n```\r\n\r\nI've even done `import gin.tf` , but when I run my code I then get...\r\n\r\n`\r\n  File \"C:\\Users\\WTC\\Anaconda3\\lib\\site-packages\\bert\\tokenization.py\", line 125, in load_vocab\r\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'gfile'`\r\n\r\n\r\nAny thoughts as to why?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version and gfile version you are using. Meanwhile you can also have a look on this [issue](https://github.com/google/gin-config/issues/9) and let us know if that solves the problem. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Tensor-flow Version - **2.0.0-alpha0**\r\n\r\nSame error occurred while running a notebook from the Tensorflow site  - [**Build a linear model with Estimators**](https://www.tensorflow.org/tutorials/estimators/linear )\r\n<br>\r\nDownload the dataset:\r\n```\r\nfrom official.wide_deep import census_dataset\r\nfrom official.wide_deep import census_main\r\n\r\ncensus_dataset.download(\"/tmp/census_data/\")\r\n```\r\n\r\n> ---------------------------------------------------------------------------\r\n> AttributeError                            Traceback (most recent call last)\r\n> <ipython-input-9-fa1d43ace000> in <module>()\r\n>       2 from official.wide_deep import census_main\r\n>       3 \r\n> ----> 4 census_dataset.download(\"/tmp/census_data/\")\r\n> \r\n> /content/models/official/wide_deep/census_dataset.py in download(data_dir)\r\n>      76 def download(data_dir):\r\n>      77   \"\"\"Download census data if it is not already present.\"\"\"\r\n> ---> 78   **tf.gfile.MakeDirs(data_dir)**\r\n>      79 \r\n>      80   training_file_path = os.path.join(data_dir, TRAINING_FILE)\r\n> \r\n> **AttributeError: module 'tensorflow' has no attribute 'gfile'**\r\n<hr>\r\n\r\n## The document suggests the following changes in the file.\r\n### _Should I go about making the following changes manually in the file census_dataset.py ?_\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0625 16:04:36.412110 139807458662144 deprecation_wrapper.py:119] From /tmpfs/src/temp/site/en/tutorials/estimators/models/official/wide_deep/census_dataset.py:78: The name tf.gfile.MakeDirs is deprecated. Please use **tf.io.gfile.makedirs** instead.\r\n\r\nW0625 16:04:36.413802 139807458662144 deprecation_wrapper.py:119] From /tmpfs/src/temp/site/en/tutorials/estimators/models/official/wide_deep/census_dataset.py:81: The name tf.gfile.Exists is deprecated. Please use **tf.io.gfile.exists** instead.\r\n\r\nW0625 16:04:38.253764 139807458662144 deprecation_wrapper.py:119] From /tmpfs/src/temp/site/en/tutorials/estimators/models/official/wide_deep/census_dataset.py:62: The name tf.gfile.Open is deprecated. Please use **tf.io.gfile.GFile** instead.\r\n\r\nW0625 16:04:38.488776 139807458662144 deprecation_wrapper.py:119] From /tmpfs/src/temp/site/en/tutorials/estimators/models/official/wide_deep/census_dataset.py:73: The name tf.gfile.Remove is deprecated. Please use **tf.io.gfile.remove** instead. \r\n\r\n#https://github.com/google/gin-config/issues/9\r\n#https://github.com/tobegit3hub/simple_tensorflow_serving/issues/45"]}, {"number": 28631, "title": "Add Tensor support for LearningRateScheduler", "body": "Optimizer can use Tensor as lr,allow Tensor as a return value of LearningRateScheduler", "comments": ["> Not sure if I understand it. If it's a tensor, then lr should be passed as tensor in optimizer constructor, instead of changing it in callbacks?\r\n\r\n\r\nIf I use tf.train.cosine_decay which will return a tensor in LearningRateScheduler,it will throw a TypeError\r\n``` python\r\n       cos_lr = tf.keras.callbacks.LearningRateScheduler(\r\n            lambda epoch, _: tf.train.cosine_decay(lr[1], epoch - freeze_step,\r\n                                                   train_step), 1)\r\n```", "@tanzhenyu Could you have a look with above example", "> A unit test would help us to understand better what you intend to achieve.\r\n\r\nTest case added", "@tanzhenyu PTAL", "@fsx950223 Could you please check failed build errors? Thanks!", "> @fsx950223 Could you please check failed build errors? Thanks!\r\n\r\nFixed", "I fix problems and pass test on local evironment.@tanzhenyu @gbaned ", "Could some one review it", "I forgot to check the pylint last time. And I pass the pylint test and the bazel test just now. Please review it again @tanzhenyu.Thanks", "Can one of the admins verify this patch?", "I have no idea about this. My changes will not cause such problems. And I have passed the local test. It seems this [commit](https://source.cloud.google.com/results/invocations/660d34d4-e677-4506-963d-7ddeb32b9ec3/targets) cause the problem"]}, {"number": 28630, "title": "TensorFlow install", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI tried to install Tensorflow, but it wont import (in either python 3.6 or 3.7). I think the problem is that I used pip, and that I don't have \"pywrap_tensorflow_internal\"\r\n\r\nHere is the Error Message:\r\nImportError                               Traceback (most recent call last)\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n/anaconda3/lib/python3.6/imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n/anaconda3/lib/python3.6/imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: dlopen(/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime\r\n  Referenced from: /anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.13)\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n in /anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-8-c542f92e050d> in <module>\r\n      6 import numpy as np\r\n      7 import pandas as pd\r\n----> 8 import tensorflow as tf\r\n      9 from sklearn import metrics\r\n     10 from tensorflow.python.data import Dataset\r\n\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>\r\n     25 import sys as _sys\r\n     26 \r\n---> 27 from tensorflow._api.v2 import audio\r\n     28 from tensorflow._api.v2 import autograph\r\n     29 from tensorflow._api.v2 import bitwise\r\n\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/_api/v2/audio/__init__.py in <module>\r\n      6 from __future__ import print_function as _print_function\r\n      7 \r\n----> 8 from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n      9 from tensorflow.python.ops.gen_audio_ops import encode_wav\r\n     10 \r\n\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime\r\n  Referenced from: /anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.13)\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n in /anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@tomguyman Can you please follow [these](https://www.tensorflow.org/install/pip) steps to install TF. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28629, "title": "Fixed contrib warning, removing duplicate 'WARNING'", "body": "Remove duplicate \"WARNING\" from the contrib warning message.", "comments": ["This should fix #27045.", "I couldn't see the details of the build failures and only got \"Oops, page not found.\"\r\n\r\nI doubt I broke the build though. Is there anything for me to do now to get the PR merged?\r\n\r\nWill there be another release of v1.13? I hope I can use v1.13 without the contrib warning bug.", "The only new releases to 1.13 are security fixes and patches. Would be better to rebase this to work on master (that's why builds failed).", "I'm uncertain about the release implications of the master branch. Is the\ncode in the master branch destined for v1, v2 or both? I was under the\nimpression that there won't be a v1.14.\n\nOn Thu, May 16, 2019 at 10:42 PM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> The only new releases to 1.13 are security fixes and patches. Would be\n> better to rebase this to work on master (that's why builds failed).\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/28629?email_source=notifications&email_token=AADNBVET4U5MP6WZWOOPD73PVVXEZA5CNFSM4HMJFLQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSAG3I#issuecomment-493093741>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AADNBVDTQK7QAHFLQQGOBP3PVVXEZANCNFSM4HMJFLQA>\n> .\n>\n", "There will be a 1.14.\r\n\r\nI'll review again once conflicts are resolved.", "It appears that in the master branch, 64e362bb already mostly fixes the problem, but 'WARNING' is repeated.", "Can you make this against master? We are not taking PRs directly against the release branches that are not on master first. I don't think we'd want to make a patch for the aestetics, but thank you for the fix!", "oh. you just did. Never mind me.", "I think the PR is already destined for matter, although the branch name in\nmy GitHub fork is still r1.13. Sorry for the confusion of not initially\nusing master. I didn't know the branch policy.\n\nOn Thu, May 16, 2019, 23:49 Martin Wicke <notifications@github.com> wrote:\n\n> Can you make this against master? We are not taking PRs directly against\n> the release branches that are not on master first. I don't think we'd want\n> to make a patch for the aestetics, but thank you for the fix!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/28629?email_source=notifications&email_token=AADNBVDOGARWIXV42CRXA53PVV67ZA5CNFSM4HMJFLQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSG3TY#issuecomment-493120975>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AADNBVCJCXC2XZ56LMRJEB3PVV67ZANCNFSM4HMJFLQA>\n> .\n>\n", "It got merged, I think this PR can be closed"]}]