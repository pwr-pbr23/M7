[{"number": 46568, "title": "typos", "body": "There are multiple typos in the tensorflow/python/keras/layers/merge.py\r\nUsed in a **_functiona_** model\r\nshould be **_functional_**", "comments": ["Closing this issue since the associated PR has been merged . Thanks!"]}, {"number": 46567, "title": "LSTM layer not Using cuDNN even with default arguments", "body": "**Problem description**\r\nI am using a TF model in RLlib in my project. The model contains a ConvLSTM2D layer and a LSTM layer. However, even with default arguments, there is a warning:\r\n`WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU`\r\nNote that there is no complaint about my ConvLSTM2D layer.\r\nI also tried to manually filled in all relevant arguments with values required in TF document, but the warning still persists. Can anyone help me figure out where the problem is?\r\nFull code for constructing the model and the system information are provided below.\r\n\r\n**Code for model construction**\r\nNote that obs_space.shape=46800, activation=tanh, and normc_initializer is a built-in kernel initializer in RLlib.\r\n```\r\n        inputs = tf.keras.layers.Input(obs_space.shape, name=\"observations\")\r\n\r\n        input_depth = inputs[..., :45000]\r\n        input_sensor = inputs[..., 45000:]\r\n\r\n        input_depth = tf.keras.layers.Reshape(target_shape=(50, 30, 30, 1))(input_depth)\r\n        input_sensor = tf.keras.layers.Reshape(target_shape=(50, 36))(input_sensor)\r\n\r\n        conv_lstm = tf.keras.layers.ConvLSTM2D(filters=10, kernel_size=(3, 3))(input_depth)\r\n        pool = tf.keras.layers.MaxPool2D()(conv_lstm)\r\n        flat = tf.keras.layers.Flatten()(pool)\r\n\r\n        fc1 = tf.keras.layers.Dense(512, activation=activation)(flat)\r\n        fc2 = tf.keras.layers.Dense(128, activation=activation)(fc1)\r\n        conv_out = tf.keras.layers.Dense(16, activation=activation)(fc2)\r\n\r\n        sensor_lstm = tf.keras.layers.LSTM(units=64)(input_sensor)\r\n\r\n        action_out = tf.keras.layers.Concatenate()([sensor_lstm, conv_out])\r\n        action_out = tf.keras.layers.Dense(128, activation=activation,\r\n                                           kernel_initializer=normc_initializer(1.0))(action_out)\r\n        action_out = tf.keras.layers.Dense(64, activation=activation,\r\n                                           kernel_initializer=normc_initializer(1.0))(action_out)\r\n        final_action_out = tf.keras.layers.Dense(num_outputs,\r\n                                                 activation=activation,\r\n                                                 kernel_initializer=normc_initializer(1.0))(action_out)\r\n\r\n        value_out = tf.keras.layers.Dense(\r\n            1,\r\n            name=\"value_out\",\r\n            activation=None,\r\n            kernel_initializer=normc_initializer(0.01))(action_out)\r\n\r\n        self.base_model = tf.keras.Model(\r\n            inputs, [final_action_out, value_out])\r\n        self.register_variables(self.base_model.variables)\r\n```\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04.5 LTS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA version: 11.2\r\n- cuDNN version: 8.0.5\r\n- GPU model and memory: GeForce RTX 3080, 10009MiB\r\n", "comments": ["@kevin1zc,\r\nOn running the given code snippet, I am facing an error stating `NameError: name 'num_outputs' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b82c1d230238dbf7311209faabfbd9bb/46567.ipynb). \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using.\r\n\r\nAlso, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/40944#issuecomment-655665293) from issue #40944 with a similar error log and check if it helps. Thanks!", "@amahendrakar \r\nThank you for your response. I double checked my code and LSTM documentation, and this seems to be a problem of RLlib. It disabled eager execution by default, which violates the LSTM requirement for using cuDNN. Enabling eager mode manually should fix the problem."]}, {"number": 46566, "title": "macOS Target //tensorflow/tools/pip_package:build_pip_package failed to build in debug mode", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: macOS Big Sur 11.1\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: commit 582c8d236cb079023657287c318ff26adb239002 (HEAD, tag: v2.4.0)\r\n- Python version: /usr/local/anaconda3/envs/python3.8tf2.4/bin/python 3.8.5\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): Apple clang version 11.0.3 (clang-1103.0.32.62)\r\n- CUDA/cuDNN version: no GPU, CPU-only\r\n- GPU model and memory: no GPU\r\n- apple clang version\r\nApple clang version 11.0.3 (clang-1103.0.32.62)\r\nTarget: x86_64-apple-darwin20.2.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n**exec command**\r\n\r\nbazel --output_base=../tensorflow-mac-cache/build_output_base --output_user_root=../tensorflow-mac-cache/build_output_user_root build --config=v2 --config=dbg --config=noaws //tensorflow/tools/pip_package:build_pip_package --repository_cache ../tensorflow-mac-cache\r\n\r\n\r\n**Describe the problem**\r\n\r\n**/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/install_name_tool: object: bazel-out/darwin-dbg/bin/tensorflow/python/_pywrap_tensorflow_internal.so truncated or malformed object (LC_SEGMENT_64 command 0 fileoff field plus filesize field extends past the end of the file)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build**\r\n\r\n**Any other info / logs**\r\n\r\nWARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=254\r\nINFO: Reading rc options for 'build' from /Users/zhouleqin/oss/tensorflow/tensorflow-mac/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/zhouleqin/oss/tensorflow/tensorflow-mac/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /Users/zhouleqin/oss/tensorflow/tensorflow-mac/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/anaconda3/envs/python3.8tf2.4/bin/python --action_env PYTHON_LIB_PATH=/usr/local/anaconda3/envs/python3.8tf2.4/lib/python3.8/site-packages --python_path=/usr/local/anaconda3/envs/python3.8tf2.4/bin/python --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /Users/zhouleqin/oss/tensorflow/tensorflow-mac/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/zhouleqin/oss/tensorflow/tensorflow-mac/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /Users/zhouleqin/oss/tensorflow/tensorflow-mac/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:v2 in file /Users/zhouleqin/oss/tensorflow/tensorflow-mac/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:dbg in file /Users/zhouleqin/oss/tensorflow/tensorflow-mac/.bazelrc: --config=opt -c dbg --cxxopt -DTF_LITE_DISABLE_X86_NEON --copt -DDEBUG_BUILD\r\nINFO: Found applicable config definition build:opt in file /Users/zhouleqin/oss/tensorflow/tensorflow-mac/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:noaws in file /Users/zhouleqin/oss/tensorflow/tensorflow-mac/.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:macos in file /Users/zhouleqin/oss/tensorflow/tensorflow-mac/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /Users/zhouleqin/oss/tensorflow/tensorflow-mac-cache/build_output_base/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /Users/zhouleqin/oss/tensorflow/tensorflow-mac-cache/build_output_base/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (399 packages loaded, 29982 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /Users/zhouleqin/oss/tensorflow/tensorflow-mac/tensorflow/python/BUILD:6068:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::grappler::graph_analyzer::SigNode::NodeOrderLess&, tensorflow::grappler::graph_analyzer::SigNode**>(tensorflow::grappler::graph_analyzer::SigNode**, tensorflow::grappler::graph_analyzer::SigNode**, tensorflow::grappler::graph_analyzer::SigNode**, tensorflow::grappler::graph_analyzer::SigNode**, tensorflow::grappler::graph_analyzer::SigNode**, tensorflow::grappler::graph_analyzer::SigNode::NodeOrderLess&) from bazel-out/darwin-dbg/bin/tensorflow/core/grappler/graph_analyzer/libgraph_analyzer_lib.a(sig_node_d733b32da0773161a7330c56df6f758b.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::grappler::graph_analyzer::SigNode::HashedPeer::LessByRank&, tensorflow::grappler::graph_analyzer::SigNode::HashedPeer*>(tensorflow::grappler::graph_analyzer::SigNode::HashedPeer*, tensorflow::grappler::graph_analyzer::SigNode::HashedPeer*, tensorflow::grappler::graph_analyzer::SigNode::HashedPeer*, tensorflow::grappler::graph_analyzer::SigNode::HashedPeer*, tensorflow::grappler::graph_analyzer::SigNode::HashedPeer*, tensorflow::grappler::graph_analyzer::SigNode::HashedPeer::LessByRank&) from bazel-out/darwin-dbg/bin/tensorflow/core/grappler/graph_analyzer/libgraph_analyzer_lib.a(sig_node_d733b32da0773161a7330c56df6f758b.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::vector<tensorflow::tfprof::CodeNode*, std::__1::allocator<tensorflow::tfprof::CodeNode*> > tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*, std::__1::allocator<tensorflow::tfprof::CodeNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode const*)&, tensorflow::tfprof::CodeNode**>(std::__1::vector<tensorflow::tfprof::CodeNode*, std::__1::allocator<tensorflow::tfprof::CodeNode*> > tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*, std::__1::allocator<tensorflow::tfprof::CodeNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode const*)&, std::__1::vector<tensorflow::tfprof::CodeNode*, std::__1::allocator<tensorflow::tfprof::CodeNode*> > tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*, std::__1::allocator<tensorflow::tfprof::CodeNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode const*)&, std::__1::vector<tensorflow::tfprof::CodeNode*, std::__1::allocator<tensorflow::tfprof::CodeNode*> > tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*, std::__1::allocator<tensorflow::tfprof::CodeNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode const*)&, std::__1::vector<tensorflow::tfprof::CodeNode*, std::__1::allocator<tensorflow::tfprof::CodeNode*> > tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*, std::__1::allocator<tensorflow::tfprof::CodeNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode const*)&, std::__1::vector<tensorflow::tfprof::CodeNode*, std::__1::allocator<tensorflow::tfprof::CodeNode*> > tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*, std::__1::allocator<tensorflow::tfprof::CodeNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode const*)&, tensorflow::tfprof::CodeNode) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/internal/libtfprof_code.a(tfprof_code_a26c563a46a40084f9c18fa4e14b5dfc.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::vector<tensorflow::tfprof::GraphNode*, std::__1::allocator<tensorflow::tfprof::GraphNode*> > tensorflow::tfprof::TFShow::SortNodes<tensorflow::tfprof::GraphNode>(std::__1::vector<tensorflow::tfprof::GraphNode*, std::__1::allocator<tensorflow::tfprof::GraphNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::GraphNode const*, tensorflow::tfprof::GraphNode const*)&, tensorflow::tfprof::GraphNode**>(std::__1::vector<tensorflow::tfprof::GraphNode*, std::__1::allocator<tensorflow::tfprof::GraphNode*> > tensorflow::tfprof::TFShow::SortNodes<tensorflow::tfprof::GraphNode>(std::__1::vector<tensorflow::tfprof::GraphNode*, std::__1::allocator<tensorflow::tfprof::GraphNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::GraphNode const*, tensorflow::tfprof::GraphNode const*)&, std::__1::vector<tensorflow::tfprof::GraphNode*, std::__1::allocator<tensorflow::tfprof::GraphNode*> > tensorflow::tfprof::TFShow::SortNodes<tensorflow::tfprof::GraphNode>(std::__1::vector<tensorflow::tfprof::GraphNode*, std::__1::allocator<tensorflow::tfprof::GraphNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::GraphNode const*, tensorflow::tfprof::GraphNode const*)&, std::__1::vector<tensorflow::tfprof::GraphNode*, std::__1::allocator<tensorflow::tfprof::GraphNode*> > tensorflow::tfprof::TFShow::SortNodes<tensorflow::tfprof::GraphNode>(std::__1::vector<tensorflow::tfprof::GraphNode*, std::__1::allocator<tensorflow::tfprof::GraphNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::GraphNode const*, tensorflow::tfprof::GraphNode const*)&, std::__1::vector<tensorflow::tfprof::GraphNode*, std::__1::allocator<tensorflow::tfprof::GraphNode*> > tensorflow::tfprof::TFShow::SortNodes<tensorflow::tfprof::GraphNode>(std::__1::vector<tensorflow::tfprof::GraphNode*, std::__1::allocator<tensorflow::tfprof::GraphNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::GraphNode const*, tensorflow::tfprof::GraphNode const*)&, std::__1::vector<tensorflow::tfprof::GraphNode*, std::__1::allocator<tensorflow::tfprof::GraphNode*> > tensorflow::tfprof::TFShow::SortNodes<tensorflow::tfprof::GraphNode>(std::__1::vector<tensorflow::tfprof::GraphNode*, std::__1::allocator<tensorflow::tfprof::GraphNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::GraphNode const*, tensorflow::tfprof::GraphNode const*)&, tensorflow::tfprof::GraphNode) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/internal/libtfprof_graph.a(tfprof_graph_2f61d25b8e7928c7de3fad0300552800.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::vector<tensorflow::tfprof::OpNode*, std::__1::allocator<tensorflow::tfprof::OpNode*> > tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::OpNode>(std::__1::vector<tensorflow::tfprof::OpNode*, std::__1::allocator<tensorflow::tfprof::OpNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::OpNode const*, tensorflow::tfprof::OpNode const*)&, tensorflow::tfprof::OpNode**>(std::__1::vector<tensorflow::tfprof::OpNode*, std::__1::allocator<tensorflow::tfprof::OpNode*> > tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::OpNode>(std::__1::vector<tensorflow::tfprof::OpNode*, std::__1::allocator<tensorflow::tfprof::OpNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::OpNode const*, tensorflow::tfprof::OpNode const*)&, std::__1::vector<tensorflow::tfprof::OpNode*, std::__1::allocator<tensorflow::tfprof::OpNode*> > tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::OpNode>(std::__1::vector<tensorflow::tfprof::OpNode*, std::__1::allocator<tensorflow::tfprof::OpNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::OpNode const*, tensorflow::tfprof::OpNode const*)&, std::__1::vector<tensorflow::tfprof::OpNode*, std::__1::allocator<tensorflow::tfprof::OpNode*> > tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::OpNode>(std::__1::vector<tensorflow::tfprof::OpNode*, std::__1::allocator<tensorflow::tfprof::OpNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::OpNode const*, tensorflow::tfprof::OpNode const*)&, std::__1::vector<tensorflow::tfprof::OpNode*, std::__1::allocator<tensorflow::tfprof::OpNode*> > tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::OpNode>(std::__1::vector<tensorflow::tfprof::OpNode*, std::__1::allocator<tensorflow::tfprof::OpNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::OpNode const*, tensorflow::tfprof::OpNode const*)&, std::__1::vector<tensorflow::tfprof::OpNode*, std::__1::allocator<tensorflow::tfprof::OpNode*> > tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::OpNode>(std::__1::vector<tensorflow::tfprof::OpNode*, std::__1::allocator<tensorflow::tfprof::OpNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::OpNode const*, tensorflow::tfprof::OpNode const*)&, tensorflow::tfprof::OpNode) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/internal/libtfprof_op.a(tfprof_op_a70008a9da797874493284813e09cc5f.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::vector<tensorflow::tfprof::ScopeNode*, std::__1::allocator<tensorflow::tfprof::ScopeNode*> > tensorflow::tfprof::TFShow::SortNodes<tensorflow::tfprof::ScopeNode>(std::__1::vector<tensorflow::tfprof::ScopeNode*, std::__1::allocator<tensorflow::tfprof::ScopeNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::ScopeNode const*, tensorflow::tfprof::ScopeNode const*)&, tensorflow::tfprof::ScopeNode**>(std::__1::vector<tensorflow::tfprof::ScopeNode*, std::__1::allocator<tensorflow::tfprof::ScopeNode*> > tensorflow::tfprof::TFShow::SortNodes<tensorflow::tfprof::ScopeNode>(std::__1::vector<tensorflow::tfprof::ScopeNode*, std::__1::allocator<tensorflow::tfprof::ScopeNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::ScopeNode const*, tensorflow::tfprof::ScopeNode const*)&, std::__1::vector<tensorflow::tfprof::ScopeNode*, std::__1::allocator<tensorflow::tfprof::ScopeNode*> > tensorflow::tfprof::TFShow::SortNodes<tensorflow::tfprof::ScopeNode>(std::__1::vector<tensorflow::tfprof::ScopeNode*, std::__1::allocator<tensorflow::tfprof::ScopeNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::ScopeNode const*, tensorflow::tfprof::ScopeNode const*)&, std::__1::vector<tensorflow::tfprof::ScopeNode*, std::__1::allocator<tensorflow::tfprof::ScopeNode*> > tensorflow::tfprof::TFShow::SortNodes<tensorflow::tfprof::ScopeNode>(std::__1::vector<tensorflow::tfprof::ScopeNode*, std::__1::allocator<tensorflow::tfprof::ScopeNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::ScopeNode const*, tensorflow::tfprof::ScopeNode const*)&, std::__1::vector<tensorflow::tfprof::ScopeNode*, std::__1::allocator<tensorflow::tfprof::ScopeNode*> > tensorflow::tfprof::TFShow::SortNodes<tensorflow::tfprof::ScopeNode>(std::__1::vector<tensorflow::tfprof::ScopeNode*, std::__1::allocator<tensorflow::tfprof::ScopeNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::ScopeNode const*, tensorflow::tfprof::ScopeNode const*)&, std::__1::vector<tensorflow::tfprof::ScopeNode*, std::__1::allocator<tensorflow::tfprof::ScopeNode*> > tensorflow::tfprof::TFShow::SortNodes<tensorflow::tfprof::ScopeNode>(std::__1::vector<tensorflow::tfprof::ScopeNode*, std::__1::allocator<tensorflow::tfprof::ScopeNode*> > const&, tensorflow::tfprof::Options const&)::'lambda'(tensorflow::tfprof::ScopeNode const*, tensorflow::tfprof::ScopeNode const*)&, tensorflow::tfprof::ScopeNode) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/internal/libtfprof_scope.a(tfprof_scope_11c51e84c798e4ddcba54fe391baf72c.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::GraphTransferer::TransferParamsComparator&, google::protobuf::internal::RepeatedPtrIterator<tensorflow::GraphTransferNodeInfo> >(google::protobuf::internal::RepeatedPtrIterator<tensorflow::GraphTransferNodeInfo>, google::protobuf::internal::RepeatedPtrIterator<tensorflow::GraphTransferNodeInfo>, google::protobuf::internal::RepeatedPtrIterator<tensorflow::GraphTransferNodeInfo>, google::protobuf::internal::RepeatedPtrIterator<tensorflow::GraphTransferNodeInfo>, google::protobuf::internal::RepeatedPtrIterator<tensorflow::GraphTransferNodeInfo>, tensorflow::GraphTransferer::TransferParamsComparator&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/hexagon/libgraph_transferer.lo(graph_transferer_22753e0a5a216a0b5219abadd4751aab.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::__less<tensorflow::boosted_trees::quantiles::WeightedQuantilesBuffer<float, float, std::__1::less<float> >::BufferEntry, tensorflow::boosted_trees::quantiles::WeightedQuantilesBuffer<float, float, std::__1::less<float> >::BufferEntry>&, tensorflow::boosted_trees::quantiles::WeightedQuantilesBuffer<float, float, std::__1::less<float> >::BufferEntry*>(tensorflow::boosted_trees::quantiles::WeightedQuantilesBuffer<float, float, std::__1::less<float> >::BufferEntry*, tensorflow::boosted_trees::quantiles::WeightedQuantilesBuffer<float, float, std::__1::less<float> >::BufferEntry*, tensorflow::boosted_trees::quantiles::WeightedQuantilesBuffer<float, float, std::__1::less<float> >::BufferEntry*, tensorflow::boosted_trees::quantiles::WeightedQuantilesBuffer<float, float, std::__1::less<float> >::BufferEntry*, tensorflow::boosted_trees::quantiles::WeightedQuantilesBuffer<float, float, std::__1::less<float> >::BufferEntry*, std::__1::__less<tensorflow::boosted_trees::quantiles::WeightedQuantilesBuffer<float, float, std::__1::less<float> >::BufferEntry, tensorflow::boosted_trees::quantiles::WeightedQuantilesBuffer<float, float, std::__1::less<float> >::BufferEntry>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/boosted_trees/libquantile_ops.lo(quantile_ops_de7a0d0c958ae29bbcf86ecb1b8f3cf0.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::ctc::ctc_beam_search::BeamComparer<float, tensorflow::ctc::ctc_beam_search::EmptyBeamState>&, tensorflow::ctc::ctc_beam_search::BeamEntry<float, tensorflow::ctc::ctc_beam_search::EmptyBeamState>**>(tensorflow::ctc::ctc_beam_search::BeamEntry<float, tensorflow::ctc::ctc_beam_search::EmptyBeamState>**, tensorflow::ctc::ctc_beam_search::BeamEntry<float, tensorflow::ctc::ctc_beam_search::EmptyBeamState>**, tensorflow::ctc::ctc_beam_search::BeamEntry<float, tensorflow::ctc::ctc_beam_search::EmptyBeamState>**, tensorflow::ctc::ctc_beam_search::BeamEntry<float, tensorflow::ctc::ctc_beam_search::EmptyBeamState>**, tensorflow::ctc::ctc_beam_search::BeamEntry<float, tensorflow::ctc::ctc_beam_search::EmptyBeamState>**, tensorflow::ctc::ctc_beam_search::BeamComparer<float, tensorflow::ctc::ctc_beam_search::EmptyBeamState>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libctc_ops.lo(ctc_decoder_ops_4698ee8777e83dca130b29b59c4317bc.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::ctc::ctc_beam_search::BeamComparer<double, tensorflow::ctc::ctc_beam_search::EmptyBeamState>&, tensorflow::ctc::ctc_beam_search::BeamEntry<double, tensorflow::ctc::ctc_beam_search::EmptyBeamState>**>(tensorflow::ctc::ctc_beam_search::BeamEntry<double, tensorflow::ctc::ctc_beam_search::EmptyBeamState>**, tensorflow::ctc::ctc_beam_search::BeamEntry<double, tensorflow::ctc::ctc_beam_search::EmptyBeamState>**, tensorflow::ctc::ctc_beam_search::BeamEntry<double, tensorflow::ctc::ctc_beam_search::EmptyBeamState>**, tensorflow::ctc::ctc_beam_search::BeamEntry<double, tensorflow::ctc::ctc_beam_search::EmptyBeamState>**, tensorflow::ctc::ctc_beam_search::BeamEntry<double, tensorflow::ctc::ctc_beam_search::EmptyBeamState>**, tensorflow::ctc::ctc_beam_search::BeamComparer<double, tensorflow::ctc::ctc_beam_search::EmptyBeamState>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libctc_ops.lo(ctc_decoder_ops_4698ee8777e83dca130b29b59c4317bc.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::__less<tensorflow::tstring, tensorflow::tstring>&, tensorflow::tstring*>(tensorflow::tstring*, tensorflow::tstring*, tensorflow::tstring*, tensorflow::tstring*, tensorflow::tstring*, std::__1::__less<tensorflow::tstring, tensorflow::tstring>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libmatching_files_op.lo(matching_files_op_a2c9e45966d3d26b4c3530dd5b51d53e.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, unsigned long long>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<unsigned long long const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<unsigned long long, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda0'(int, int)&, int*>(unsigned long long, unsigned long long, unsigned long long, unsigned long long, unsigned long long, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, unsigned long long>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<unsigned long long const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<unsigned long long, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda'(int, int)&, int*>(unsigned long long, unsigned long long, unsigned long long, unsigned long long, unsigned long long, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, long long>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<long long const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<long long, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda0'(int, int)&, int*>(long long, long long, long long, long long, long long, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, long long>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<long long const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<long long, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda'(int, int)&, int*>(long long, long long, long long, long long, long long, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, unsigned int>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<unsigned int const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<unsigned int, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda0'(int, int)&, int*>(unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, unsigned int>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<unsigned int const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<unsigned int, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda'(int, int)&, int*>(unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, unsigned short>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<unsigned short const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<unsigned short, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda0'(int, int)&, int*>(unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, unsigned short>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<unsigned short const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<unsigned short, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda'(int, int)&, int*>(unsigned short, unsigned short, unsigned short, unsigned short, unsigned short, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, short>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<short const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<short, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda0'(int, int)&, int*>(short, short, short, short, short, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, short>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<short const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<short, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda'(int, int)&, int*>(short, short, short, short, short, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, unsigned char>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<unsigned char, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda0'(int, int)&, int*>(unsigned char, unsigned char, unsigned char, unsigned char, unsigned char, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, unsigned char>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<unsigned char, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda'(int, int)&, int*>(unsigned char, unsigned char, unsigned char, unsigned char, unsigned char, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, signed char>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<signed char, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda0'(int, int)&, int*>(signed char, signed char, signed char, signed char, signed char, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, signed char>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<signed char, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda'(int, int)&, int*>(signed char, signed char, signed char, signed char, signed char, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda0'(int, int)&, int*>(int, int, int, int, int, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, int>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda'(int, int)&, int*>(int, int, int, int, int, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, Eigen::half>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<Eigen::half, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda0'(int, int)&, int*>(Eigen::half, Eigen::half, Eigen::half, Eigen::half, Eigen::half, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, Eigen::half>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<Eigen::half, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda'(int, int)&, int*>(Eigen::half, Eigen::half, Eigen::half, Eigen::half, Eigen::half, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, Eigen::bfloat16>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<Eigen::bfloat16 const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<Eigen::bfloat16, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda0'(int, int)&, int*>(Eigen::bfloat16, Eigen::bfloat16, Eigen::bfloat16, Eigen::bfloat16, Eigen::bfloat16, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, Eigen::bfloat16>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<Eigen::bfloat16 const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<Eigen::bfloat16, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda'(int, int)&, int*>(Eigen::bfloat16, Eigen::bfloat16, Eigen::bfloat16, Eigen::bfloat16, Eigen::bfloat16, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda0'(int, int)&, int*>(float, float, float, float, float, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda'(int, int)&, int*>(float, float, float, float, float, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, double>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<double, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda0'(int, int)&, int*>(double, double, double, double, double, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, double>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<double, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)::'lambda'(long long, long long)::operator()(long long, long long) const::'lambda'(int, int)&, int*>(double, double, double, double, double, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtopk_op.lo(topk_op_5354c76f51d39c5b8f6b4b8cc0d2b1a3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<0>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<0>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_concat_op.lo(sparse_concat_op_4674e7b63e015e31356d9be955fef609.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<1>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<1>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_concat_op.lo(sparse_concat_op_4674e7b63e015e31356d9be955fef609.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<2>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<2>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_concat_op.lo(sparse_concat_op_4674e7b63e015e31356d9be955fef609.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<3>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<3>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_concat_op.lo(sparse_concat_op_4674e7b63e015e31356d9be955fef609.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<4>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<4>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_concat_op.lo(sparse_concat_op_4674e7b63e015e31356d9be955fef609.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<5>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<5>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_concat_op.lo(sparse_concat_op_4674e7b63e015e31356d9be955fef609.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::DimComparator&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::DimComparator&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_concat_op.lo(sparse_concat_op_4674e7b63e015e31356d9be955fef609.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<0>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<0>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reduce_op.lo(sparse_reduce_op_b05ba9e16702b505a468507c58621ce4.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<1>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<1>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reduce_op.lo(sparse_reduce_op_b05ba9e16702b505a468507c58621ce4.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<2>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<2>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reduce_op.lo(sparse_reduce_op_b05ba9e16702b505a468507c58621ce4.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<3>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<3>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reduce_op.lo(sparse_reduce_op_b05ba9e16702b505a468507c58621ce4.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<4>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<4>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reduce_op.lo(sparse_reduce_op_b05ba9e16702b505a468507c58621ce4.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<5>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<5>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reduce_op.lo(sparse_reduce_op_b05ba9e16702b505a468507c58621ce4.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::DimComparator&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::DimComparator&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reduce_op.lo(sparse_reduce_op_b05ba9e16702b505a468507c58621ce4.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<0>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<0>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reorder_op.lo(sparse_reorder_op_be89eeaa50bd444ea933702e4fdf59af.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<1>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<1>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reorder_op.lo(sparse_reorder_op_be89eeaa50bd444ea933702e4fdf59af.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<2>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<2>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reorder_op.lo(sparse_reorder_op_be89eeaa50bd444ea933702e4fdf59af.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<3>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<3>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reorder_op.lo(sparse_reorder_op_be89eeaa50bd444ea933702e4fdf59af.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<4>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<4>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reorder_op.lo(sparse_reorder_op_be89eeaa50bd444ea933702e4fdf59af.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<5>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<5>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reorder_op.lo(sparse_reorder_op_be89eeaa50bd444ea933702e4fdf59af.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::DimComparator&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::DimComparator&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_reorder_op.lo(sparse_reorder_op_be89eeaa50bd444ea933702e4fdf59af.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<0>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<0>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_softmax.lo(sparse_softmax_op_5efd3ee1fa1ac3f14aa4925eae52e1aa.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<1>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<1>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_softmax.lo(sparse_softmax_op_5efd3ee1fa1ac3f14aa4925eae52e1aa.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<2>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<2>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_softmax.lo(sparse_softmax_op_5efd3ee1fa1ac3f14aa4925eae52e1aa.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<3>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<3>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_softmax.lo(sparse_softmax_op_5efd3ee1fa1ac3f14aa4925eae52e1aa.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<4>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<4>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_softmax.lo(sparse_softmax_op_5efd3ee1fa1ac3f14aa4925eae52e1aa.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::FixedDimComparator<5>&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::FixedDimComparator<5>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_softmax.lo(sparse_softmax_op_5efd3ee1fa1ac3f14aa4925eae52e1aa.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::sparse::DimComparator&, long long*>(long long*, long long*, long long*, long long*, long long*, tensorflow::sparse::DimComparator&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libsparse_softmax.lo(sparse_softmax_op_5efd3ee1fa1ac3f14aa4925eae52e1aa.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::VariableInputLockHolder tensorflow::MaybeLockVariableInputMutexesInOrder<Eigen::ThreadPoolDevice, Eigen::half>(tensorflow::OpKernelContext*, bool, bool, std::__1::vector<int, std::__1::allocator<int> > const&)::'lambda'(int, int)&, int*>(Eigen::half, Eigen::half, Eigen::half, Eigen::half, Eigen::half, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtraining_ops.lo(training_ops_b043fa34f8ffc5e560b358393a8a593e.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::VariableInputLockHolder tensorflow::MaybeLockVariableInputMutexesInOrder<Eigen::ThreadPoolDevice, Eigen::bfloat16>(tensorflow::OpKernelContext*, bool, bool, std::__1::vector<int, std::__1::allocator<int> > const&)::'lambda'(int, int)&, int*>(Eigen::bfloat16, Eigen::bfloat16, Eigen::bfloat16, Eigen::bfloat16, Eigen::bfloat16, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtraining_ops.lo(training_ops_b043fa34f8ffc5e560b358393a8a593e.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::VariableInputLockHolder tensorflow::MaybeLockVariableInputMutexesInOrder<Eigen::ThreadPoolDevice, float>(tensorflow::OpKernelContext*, bool, bool, std::__1::vector<int, std::__1::allocator<int> > const&)::'lambda'(int, int)&, int*>(float, float, float, float, float, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtraining_ops.lo(training_ops_b043fa34f8ffc5e560b358393a8a593e.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::VariableInputLockHolder tensorflow::MaybeLockVariableInputMutexesInOrder<Eigen::ThreadPoolDevice, double>(tensorflow::OpKernelContext*, bool, bool, std::__1::vector<int, std::__1::allocator<int> > const&)::'lambda'(int, int)&, int*>(double, double, double, double, double, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtraining_ops.lo(training_ops_b043fa34f8ffc5e560b358393a8a593e.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::VariableInputLockHolder tensorflow::MaybeLockVariableInputMutexesInOrder<Eigen::ThreadPoolDevice, std::__1::complex<float> >(tensorflow::OpKernelContext*, bool, bool, std::__1::vector<int, std::__1::allocator<int> > const&)::'lambda'(int, int)&, int*>(std::__1::complex<float>, std::__1::complex<float>, std::__1::complex<float>, std::__1::complex<float>, std::__1::complex<float>, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtraining_ops.lo(training_ops_b043fa34f8ffc5e560b358393a8a593e.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::VariableInputLockHolder tensorflow::MaybeLockVariableInputMutexesInOrder<Eigen::ThreadPoolDevice, std::__1::complex<double> >(tensorflow::OpKernelContext*, bool, bool, std::__1::vector<int, std::__1::allocator<int> > const&)::'lambda'(int, int)&, int*>(std::__1::complex<double>, std::__1::complex<double>, std::__1::complex<double>, std::__1::complex<double>, std::__1::complex<double>, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libtraining_ops.lo(training_ops_b043fa34f8ffc5e560b358393a8a593e.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::SkipgramOp::Init(tensorflow::Env*, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)::'lambda'(std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int> const&, std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int> const&)&, std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int>*>(std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int>*, std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int>*, std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int>*, std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int>*, std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int>*, tensorflow::SkipgramOp::Init(tensorflow::Env*, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)::'lambda'(std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int> const&, std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int> const&)&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/libword2vec_kernels.lo(word2vec_kernels_81e6f553c012d7458fbb8cfb8a6d72b8.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::Status tensorflow::EinsumHelper::ReduceOperand<Eigen::ThreadPoolDevice, Eigen::bfloat16>(tensorflow::OpKernelContext*, tensorflow::Tensor const&, std::__1::vector<tensorflow::EinsumHelper::DimensionType, std::__1::allocator<tensorflow::EinsumHelper::DimensionType> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, bool*, tensorflow::Tensor*)::'lambda'(int, int)&, int*>(Eigen::bfloat16, Eigen::bfloat16, Eigen::bfloat16, Eigen::bfloat16, Eigen::bfloat16, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/linalg/libeinsum_op.lo(einsum_op_impl_bfloat16_e96046132695ada08436540f6b862e23.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::Status tensorflow::EinsumHelper::ReduceOperand<Eigen::ThreadPoolDevice, std::__1::complex<double> >(tensorflow::OpKernelContext*, tensorflow::Tensor const&, std::__1::vector<tensorflow::EinsumHelper::DimensionType, std::__1::allocator<tensorflow::EinsumHelper::DimensionType> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, bool*, tensorflow::Tensor*)::'lambda'(int, int)&, int*>(std::__1::complex<double>, std::__1::complex<double>, std::__1::complex<double>, std::__1::complex<double>, std::__1::complex<double>, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/linalg/libeinsum_op.lo(einsum_op_impl_complex128_1ddb1b9c54cc3cb1ae550daea2f10911.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::Status tensorflow::EinsumHelper::ReduceOperand<Eigen::ThreadPoolDevice, std::__1::complex<float> >(tensorflow::OpKernelContext*, tensorflow::Tensor const&, std::__1::vector<tensorflow::EinsumHelper::DimensionType, std::__1::allocator<tensorflow::EinsumHelper::DimensionType> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, bool*, tensorflow::Tensor*)::'lambda'(int, int)&, int*>(std::__1::complex<float>, std::__1::complex<float>, std::__1::complex<float>, std::__1::complex<float>, std::__1::complex<float>, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/linalg/libeinsum_op.lo(einsum_op_impl_complex64_34f20a9f7e93ea248a2771cde501aa66.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::Status tensorflow::EinsumHelper::ReduceOperand<Eigen::ThreadPoolDevice, double>(tensorflow::OpKernelContext*, tensorflow::Tensor const&, std::__1::vector<tensorflow::EinsumHelper::DimensionType, std::__1::allocator<tensorflow::EinsumHelper::DimensionType> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, bool*, tensorflow::Tensor*)::'lambda'(int, int)&, int*>(double, double, double, double, double, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/linalg/libeinsum_op.lo(einsum_op_impl_double_cdb86d35f5123022c97ee09f6889c4eb.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::Status tensorflow::EinsumHelper::ReduceOperand<Eigen::ThreadPoolDevice, float>(tensorflow::OpKernelContext*, tensorflow::Tensor const&, std::__1::vector<tensorflow::EinsumHelper::DimensionType, std::__1::allocator<tensorflow::EinsumHelper::DimensionType> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, bool*, tensorflow::Tensor*)::'lambda'(int, int)&, int*>(float, float, float, float, float, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/linalg/libeinsum_op.lo(einsum_op_impl_float_a65e15b63aa636cc95248c27d203925a.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::Status tensorflow::EinsumHelper::ReduceOperand<Eigen::ThreadPoolDevice, Eigen::half>(tensorflow::OpKernelContext*, tensorflow::Tensor const&, std::__1::vector<tensorflow::EinsumHelper::DimensionType, std::__1::allocator<tensorflow::EinsumHelper::DimensionType> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, bool*, tensorflow::Tensor*)::'lambda'(int, int)&, int*>(Eigen::half, Eigen::half, Eigen::half, Eigen::half, Eigen::half, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/linalg/libeinsum_op.lo(einsum_op_impl_half_1eb10a78ae8567588818b0b09d3982f1.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::Status tensorflow::EinsumHelper::ReduceOperand<Eigen::ThreadPoolDevice, int>(tensorflow::OpKernelContext*, tensorflow::Tensor const&, std::__1::vector<tensorflow::EinsumHelper::DimensionType, std::__1::allocator<tensorflow::EinsumHelper::DimensionType> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, bool*, tensorflow::Tensor*)::'lambda'(int, int)&, int*>(int, int, int, int, int, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/linalg/libeinsum_op.lo(einsum_op_impl_int32_2f56c4c4f95659c74b815f5a7cee0793.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::Status tensorflow::EinsumHelper::ReduceOperand<Eigen::ThreadPoolDevice, long long>(tensorflow::OpKernelContext*, tensorflow::Tensor const&, std::__1::vector<tensorflow::EinsumHelper::DimensionType, std::__1::allocator<tensorflow::EinsumHelper::DimensionType> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> > const&, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, absl::lts_2020_02_25::InlinedVector<int, 8ul, std::__1::allocator<int> >*, bool*, tensorflow::Tensor*)::'lambda'(int, int)&, int*>(long long, long long, long long, long long, long long, Eigen::ThreadPoolDevice) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/linalg/libeinsum_op.lo(einsum_op_impl_int64_435e129393df940ec8d3b4de668a1d5b.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::__less<tensorflow::tstring, tensorflow::tstring>&, tensorflow::tstring*>(tensorflow::tstring*, tensorflow::tstring*, tensorflow::tstring*, tensorflow::tstring*, tensorflow::tstring*, std::__1::__less<tensorflow::tstring, tensorflow::tstring>&) from bazel-out/darwin-dbg/bin/tensorflow/core/kernels/data/experimental/libsnapshot_dataset_op.lo(snapshot_dataset_op_91741f4343bbe6a4fc3927373da58952.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::profiler::XLinesComparatorByName&, google::protobuf::internal::RepeatedPtrOverPtrsIterator<tensorflow::profiler::XLine*, void*> >(google::protobuf::internal::RepeatedPtrOverPtrsIterator<tensorflow::profiler::XLine*, void*>, google::protobuf::internal::RepeatedPtrOverPtrsIterator<tensorflow::profiler::XLine*, void*>, google::protobuf::internal::RepeatedPtrOverPtrsIterator<tensorflow::profiler::XLine*, void*>, google::protobuf::internal::RepeatedPtrOverPtrsIterator<tensorflow::profiler::XLine*, void*>, google::protobuf::internal::RepeatedPtrOverPtrsIterator<tensorflow::profiler::XLine*, void*>, tensorflow::profiler::XLinesComparatorByName&) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/internal/cpu/libhost_tracer_utils.a(host_tracer_utils_90b6373b66ad9b998bbc11a215e407d3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::__less<tensorflow::TensorId, tensorflow::TensorId>&, tensorflow::TensorId*>(tensorflow::TensorId*, tensorflow::TensorId*, tensorflow::TensorId*, tensorflow::TensorId*, tensorflow::TensorId*, std::__1::__less<tensorflow::TensorId, tensorflow::TensorId>&) from bazel-out/darwin-dbg/bin/tensorflow/compiler/jit/libcompilation_passes.a(deadness_analysis_f1267db28f708ac918fcdae7398bb918.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::NodeComparatorID&, tensorflow::Node**>(tensorflow::Node**, tensorflow::Node**, tensorflow::Node**, tensorflow::Node**, tensorflow::Node**, tensorflow::NodeComparatorID&) from bazel-out/darwin-dbg/bin/tensorflow/compiler/jit/libcompilation_passes.a(mark_for_compilation_pass_cac15d051d331b682147f799f0293b20.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::profiler::XEventsComparator&, google::protobuf::internal::RepeatedPtrOverPtrsIterator<tensorflow::profiler::XEvent*, void*> >(google::protobuf::internal::RepeatedPtrOverPtrsIterator<tensorflow::profiler::XEvent*, void*>, google::protobuf::internal::RepeatedPtrOverPtrsIterator<tensorflow::profiler::XEvent*, void*>, google::protobuf::internal::RepeatedPtrOverPtrsIterator<tensorflow::profiler::XEvent*, void*>, google::protobuf::internal::RepeatedPtrOverPtrsIterator<tensorflow::profiler::XEvent*, void*>, google::protobuf::internal::RepeatedPtrOverPtrsIterator<tensorflow::profiler::XEvent*, void*>, tensorflow::profiler::XEventsComparator&) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/utils/libxplane_utils.a(xplane_utils_9a1bc04280f6bc8f819e1acc8e9d3fa3.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<google::protobuf::internal::CompareByDerefFirst<google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::ToolRequestOptions> const*>&, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::ToolRequestOptions> const**>(google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::ToolRequestOptions> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::ToolRequestOptions> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::ToolRequestOptions> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::ToolRequestOptions> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::ToolRequestOptions> const**, google::protobuf::internal::CompareByDerefFirst<google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::ToolRequestOptions> const*>&) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/libprofiler_service_proto_cc_impl.lo(profiler_service.pb_8737cb65d1a2d29f389fd3b6368ea2ea.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<google::protobuf::internal::CompareByFirstField<google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ProfileNode> const*> >&, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ProfileNode> const*>*>(google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ProfileNode> const*>*, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ProfileNode> const*>*, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ProfileNode> const*>*, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ProfileNode> const*>*, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ProfileNode> const*>*, google::protobuf::internal::CompareByFirstField<google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ProfileNode> const*> >&) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/libprotos_all_cc_impl.lo(tfprof_log.pb_6d69bfbededc497228a1433d58c70cc9.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<google::protobuf::internal::CompareByDerefFirst<google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const*>&, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const**>(google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const**, google::protobuf::internal::CompareByDerefFirst<google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const*>&) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/libprotos_all_cc_impl.lo(tfprof_log.pb_6d69bfbededc497228a1433d58c70cc9.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<google::protobuf::internal::CompareByFirstField<google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ExecProfile> const*> >&, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ExecProfile> const*>*>(google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ExecProfile> const*>*, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ExecProfile> const*>*, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ExecProfile> const*>*, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ExecProfile> const*>*, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ExecProfile> const*>*, google::protobuf::internal::CompareByFirstField<google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, tensorflow::tfprof::ExecProfile> const*> >&) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/libprotos_all_cc_impl.lo(tfprof_log.pb_6d69bfbededc497228a1433d58c70cc9.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<google::protobuf::internal::CompareByFirstField<google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Tuple> const*> >&, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Tuple> const*>*>(google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Tuple> const*>*, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Tuple> const*>*, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Tuple> const*>*, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Tuple> const*>*, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Tuple> const*>*, google::protobuf::internal::CompareByFirstField<google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Tuple> const*> >&) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/libprotos_all_cc_impl.lo(tfprof_log.pb_6d69bfbededc497228a1433d58c70cc9.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<google::protobuf::internal::CompareByDerefFirst<google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::ExecTime> const*>&, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::ExecTime> const**>(google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::ExecTime> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::ExecTime> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::ExecTime> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::ExecTime> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::ExecTime> const**, google::protobuf::internal::CompareByDerefFirst<google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::ExecTime> const*>&) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/libprotos_all_cc_impl.lo(tfprof_log.pb_6d69bfbededc497228a1433d58c70cc9.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<google::protobuf::internal::CompareByFirstField<google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Memory> const*> >&, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Memory> const*>*>(google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Memory> const*>*, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Memory> const*>*, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Memory> const*>*, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Memory> const*>*, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Memory> const*>*, google::protobuf::internal::CompareByFirstField<google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::tfprof::Memory> const*> >&) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/libprotos_all_cc_impl.lo(tfprof_log.pb_6d69bfbededc497228a1433d58c70cc9.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<google::protobuf::internal::CompareByDerefFirst<google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdvisorOptionsProto_CheckerOption> const*>&, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdvisorOptionsProto_CheckerOption> const**>(google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdvisorOptionsProto_CheckerOption> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdvisorOptionsProto_CheckerOption> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdvisorOptionsProto_CheckerOption> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdvisorOptionsProto_CheckerOption> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdvisorOptionsProto_CheckerOption> const**, google::protobuf::internal::CompareByDerefFirst<google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdvisorOptionsProto_CheckerOption> const*>&) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/libprotos_all_cc_impl.lo(tfprof_options.pb_c051573c5c19805f236ebf81236c99ac.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<google::protobuf::internal::CompareByFirstField<google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::TensorShapeProto> const*> >&, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::TensorShapeProto> const*>*>(google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::TensorShapeProto> const*>*, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::TensorShapeProto> const*>*, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::TensorShapeProto> const*>*, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::TensorShapeProto> const*>*, google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::TensorShapeProto> const*>*, google::protobuf::internal::CompareByFirstField<google::protobuf::internal::SortItem<int, google::protobuf::MapPair<int, tensorflow::TensorShapeProto> const*> >&) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/libprotos_all_cc_impl.lo(tfprof_output.pb_63ada445ea0a3affff8c00dfbf36f738.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<google::protobuf::internal::CompareByDerefFirst<google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdviceProto_Checker> const*>&, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdviceProto_Checker> const**>(google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdviceProto_Checker> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdviceProto_Checker> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdviceProto_Checker> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdviceProto_Checker> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdviceProto_Checker> const**, google::protobuf::internal::CompareByDerefFirst<google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::tfprof::AdviceProto_Checker> const*>&) from bazel-out/darwin-dbg/bin/tensorflow/core/profiler/libprotos_all_cc_impl.lo(tfprof_output.pb_63ada445ea0a3affff8c00dfbf36f738.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<google::protobuf::internal::CompareByDerefFirst<google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const*>&, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const**>(google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const**, google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const**, google::protobuf::internal::CompareByDerefFirst<google::protobuf::MapPair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::AttrValue> const*>&) from bazel-out/darwin-dbg/bin/tensorflow/core/protobuf/libeager_service_proto_cc_impl.lo(eager_service.pb_be4c6375cd8b11bdf5675a2df2ef5f13.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::__less<tensorflow::gtl::IntType<xla::GlobalDeviceId_tag_, long long>, tensorflow::gtl::IntType<xla::GlobalDeviceId_tag_, long long> >&, tensorflow::gtl::IntType<xla::GlobalDeviceId_tag_, long long>*>(tensorflow::gtl::IntType<xla::GlobalDeviceId_tag_, long long>*, tensorflow::gtl::IntType<xla::GlobalDeviceId_tag_, long long>*, tensorflow::gtl::IntType<xla::GlobalDeviceId_tag_, long long>*, tensorflow::gtl::IntType<xla::GlobalDeviceId_tag_, long long>*, tensorflow::gtl::IntType<xla::GlobalDeviceId_tag_, long long>*, std::__1::__less<tensorflow::gtl::IntType<xla::GlobalDeviceId_tag_, long long>, tensorflow::gtl::IntType<xla::GlobalDeviceId_tag_, long long> >&) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/gpu/libgpu_executable_run_options.a(gpu_executable_run_options_c82bcadad723fe1693d8dea4216d3923.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<xla::HloAliasAnalysis::LiveOutBuffers() const::'lambda'(xla::HloBuffer const*, xla::HloBuffer const*)&, xla::HloBuffer const**>(xla::HloBuffer const**, xla::HloBuffer const**, xla::HloBuffer const**, xla::HloBuffer const**, xla::HloBuffer const**, xla::HloAliasAnalysis::LiveOutBuffers() const::'lambda'(xla::HloBuffer const*, xla::HloBuffer const*)&) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libbuffer_assignment.a(buffer_assignment_7ff3dd807a6707c0d8f1783c4f200ccb.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<bool (*&)(xla::HloValue const*, xla::HloValue const*), xla::HloValue const**>(xla::HloValue const**, xla::HloValue const**, xla::HloValue const**, xla::HloValue const**, xla::HloValue const**, bool (*&)(xla::HloValue const*, xla::HloValue const*)) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libbuffer_assignment.a(buffer_assignment_7ff3dd807a6707c0d8f1783c4f200ccb.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::function<bool (xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval const&, xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval const&)>&, xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval*>(xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval*, xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval*, xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval*, xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval*, xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval*, std::__1::function<bool (xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval const&, xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval const&)>&) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libheap_simulator.a(heap_simulator_bd7b0db06b3e616f9556d8e2bf846d2e.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::FindChunkCandidate(xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval const&, long long) const::'lambda'(xla::HeapSimulator::Chunk const&, xla::HeapSimulator::Chunk const&)&, xla::HeapSimulator::Chunk*>(xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::FindChunkCandidate(xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval const&, long long) const::'lambda'(xla::HeapSimulator::Chunk const&, xla::HeapSimulator::Chunk const&)&, xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::FindChunkCandidate(xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval const&, long long) const::'lambda'(xla::HeapSimulator::Chunk const&, xla::HeapSimulator::Chunk const&)&, xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::FindChunkCandidate(xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval const&, long long) const::'lambda'(xla::HeapSimulator::Chunk const&, xla::HeapSimulator::Chunk const&)&, xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::FindChunkCandidate(xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval const&, long long) const::'lambda'(xla::HeapSimulator::Chunk const&, xla::HeapSimulator::Chunk const&)&, xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::FindChunkCandidate(xla::GlobalDecreasingSizeBestFitHeap<xla::HloValue>::BufferInterval const&, long long) const::'lambda'(xla::HeapSimulator::Chunk const&, xla::HeapSimulator::Chunk const&)&, xla::HloValue) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libheap_simulator.a(heap_simulator_bd7b0db06b3e616f9556d8e2bf846d2e.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::function<bool (xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval const&, xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval const&)>&, xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval*>(xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval*, xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval*, xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval*, xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval*, xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval*, std::__1::function<bool (xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval const&, xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval const&)>&) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libheap_simulator.a(heap_simulator_bd7b0db06b3e616f9556d8e2bf846d2e.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::FindChunkCandidate(xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval const&, long long) const::'lambda'(xla::HeapSimulator::Chunk const&, xla::HeapSimulator::Chunk const&)&, xla::HeapSimulator::Chunk*>(xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::FindChunkCandidate(xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval const&, long long) const::'lambda'(xla::HeapSimulator::Chunk const&, xla::HeapSimulator::Chunk const&)&, xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::FindChunkCandidate(xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval const&, long long) const::'lambda'(xla::HeapSimulator::Chunk const&, xla::HeapSimulator::Chunk const&)&, xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::FindChunkCandidate(xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval const&, long long) const::'lambda'(xla::HeapSimulator::Chunk const&, xla::HeapSimulator::Chunk const&)&, xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::FindChunkCandidate(xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval const&, long long) const::'lambda'(xla::HeapSimulator::Chunk const&, xla::HeapSimulator::Chunk const&)&, xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::FindChunkCandidate(xla::GlobalDecreasingSizeBestFitHeap<xla::MemorySpaceAssignmentRepacker::AllocationBlock>::BufferInterval const&, long long) const::'lambda'(xla::HeapSimulator::Chunk const&, xla::HeapSimulator::Chunk const&)&, xla::MemorySpaceAssignmentRepacker::AllocationBlock) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libheap_simulator.a(heap_simulator_bd7b0db06b3e616f9556d8e2bf846d2e.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<bool (*&)(xla::HloBuffer const*, xla::HloBuffer const*), xla::HloBuffer const**>(xla::HloBuffer const**, xla::HloBuffer const**, xla::HloBuffer const**, xla::HloBuffer const**, xla::HloBuffer const**, bool (*&)(xla::HloBuffer const*, xla::HloBuffer const*)) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libhlo_alias_analysis.a(hlo_alias_analysis_98b09d9b5341f72a61ead38c995a276b.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<bool (*&)(xla::HloValue const*, xla::HloValue const*), xla::HloValue const**>(xla::HloValue const**, xla::HloValue const**, xla::HloValue const**, xla::HloValue const**, xla::HloValue const**, bool (*&)(xla::HloValue const*, xla::HloValue const*)) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libhlo_alias_analysis.a(hlo_alias_analysis_98b09d9b5341f72a61ead38c995a276b.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::__less<xla::HloPosition, xla::HloPosition>&, xla::HloPosition*>(xla::HloPosition*, xla::HloPosition*, xla::HloPosition*, xla::HloPosition*, xla::HloPosition*, std::__1::__less<xla::HloPosition, xla::HloPosition>&) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libhlo_buffer.a(hlo_buffer_5dd5cb2616f0a526c1afa963f9eeb4c0.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<bool (*&)(xla::HloValue const*, xla::HloValue const*), xla::HloValue**>(xla::HloValue**, xla::HloValue**, xla::HloValue**, xla::HloValue**, xla::HloValue**, bool (*&)(xla::HloValue const*, xla::HloValue const*)) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libhlo_dataflow_analysis.a(hlo_dataflow_analysis_43d901a8c1959e1d81ae59552723fb54.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<bool (*&)(xla::HloValue const*, xla::HloValue const*), xla::HloValue const**>(xla::HloValue const**, xla::HloValue const**, xla::HloValue const**, xla::HloValue const**, xla::HloValue const**, bool (*&)(xla::HloValue const*, xla::HloValue const*)) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libhlo_value.a(hlo_value_1fd7a5734f4a8556ea8008f22635103e.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::function<bool (std::__1::pair<int, xla::HloInstruction const*>, std::__1::pair<int, xla::HloInstruction const*>)>&, std::__1::pair<int, xla::HloInstruction*>*>(std::__1::pair<int, xla::HloInstruction*>*, std::__1::pair<int, xla::HloInstruction*>*, std::__1::pair<int, xla::HloInstruction*>*, std::__1::pair<int, xla::HloInstruction*>*, std::__1::pair<int, xla::HloInstruction*>*, std::__1::function<bool (std::__1::pair<int, xla::HloInstruction const*>, std::__1::pair<int, xla::HloInstruction const*>)>&) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libhlo.a(hlo_instruction_0edf4881a3b85d91ea3a69a0a2c5f990.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<bool (*&)(xla::HloComputation*, xla::HloComputation*), xla::HloComputation**>(xla::HloComputation**, xla::HloComputation**, xla::HloComputation**, xla::HloComputation**, xla::HloComputation**, bool (*&)(xla::HloComputation*, xla::HloComputation*)) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libhlo.a(hlo_module_80ef1175230ab4359f90ce77c288b243.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<google::protobuf::internal::CompareByFirstField<google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, xla::HloScheduleProto_InstructionSequence> const*> >&, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, xla::HloScheduleProto_InstructionSequence> const*>*>(google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, xla::HloScheduleProto_InstructionSequence> const*>*, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, xla::HloScheduleProto_InstructionSequence> const*>*, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, xla::HloScheduleProto_InstructionSequence> const*>*, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, xla::HloScheduleProto_InstructionSequence> const*>*, google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, xla::HloScheduleProto_InstructionSequence> const*>*, google::protobuf::internal::CompareByFirstField<google::protobuf::internal::SortItem<long long, google::protobuf::MapPair<long long, xla::HloScheduleProto_InstructionSequence> const*> >&) from bazel-out/darwin-dbg/bin/tensorflow/compiler/xla/service/libhlo_proto_cc_impl.lo(hlo.pb_247e720ee9eb6dbacb0f50c185005e9f.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::grappler::internal::NodeMapInternal<tensorflow::GraphDef, tensorflow::NodeDef>::GetOutputsOrderedByNodeName(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const::'lambda'(tensorflow::NodeDef const*, tensorflow::NodeDef const*)&, tensorflow::NodeDef**>(tensorflow::NodeDef, tensorflow::NodeDef, tensorflow::NodeDef, tensorflow::NodeDef, tensorflow::NodeDef, tensorflow::GraphDef) from bazel-out/darwin-dbg/bin/tensorflow/core/grappler/optimizers/libarithmetic_optimizer.a(arithmetic_optimizer_00afebfd8dbac9401e7f3de830bb479c.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<tensorflow::grappler::internal::NodeMapInternal<tensorflow::GraphDef, tensorflow::NodeDef>::GetOutputsOrderedByNodeName(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const::'lambda'(tensorflow::NodeDef const*, tensorflow::NodeDef const*)&, tensorflow::NodeDef**>(tensorflow::NodeDef, tensorflow::NodeDef, tensorflow::NodeDef, tensorflow::NodeDef, tensorflow::NodeDef, tensorflow::GraphDef) from bazel-out/darwin-dbg/bin/tensorflow/core/grappler/optimizers/libconstant_folding.a(constant_folding_20910a966e3a1a4d77043c3c399dc460.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::function<bool (tensorflow::Node const*, tensorflow::Node const*)>&, tensorflow::Node**>(tensorflow::Node**, tensorflow::Node**, tensorflow::Node**, tensorflow::Node**, tensorflow::Node**, std::__1::function<bool (tensorflow::Node const*, tensorflow::Node const*)>&) from bazel-out/darwin-dbg/bin/tensorflow/core/libgraph.a(algorithm_578ef39f4d71a37f469a1d717629d65f.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::function<bool (tensorflow::Node const*, tensorflow::Node const*)>&, tensorflow::Node const**>(tensorflow::Node const**, tensorflow::Node const**, tensorflow::Node const**, tensorflow::Node const**, tensorflow::Node const**, std::__1::function<bool (tensorflow::Node const*, tensorflow::Node const*)>&) from bazel-out/darwin-dbg/bin/tensorflow/core/libgraph.a(algorithm_578ef39f4d71a37f469a1d717629d65f.o)\r\nld: warning: cannot export hidden symbol unsigned int std::__1::__sort5<std::__1::__less<std::__1::pair<tensorflow::Node const*, int>, std::__1::pair<tensorflow::Node const*, int> >&, std::__1::pair<tensorflow::Node const*, int>*>(std::__1::pair<tensorflow::Node const*, int>*, std::__1::pair<tensorflow::Node const*, int>*, std::__1::pair<tensorflow::Node const*, int>*, std::__1::pair<tensorflow::Node const*, int>*, std::__1::pair<tensorflow::Node const*, int>*, std::__1::__less<std::__1::pair<tensorflow::Node const*, int>, std::__1::pair<tensorflow::Node const*, int> >&) from bazel-out/darwin-dbg/bin/tensorflow/core/libgraph.a(optimizer_cse_d75b72ac47a91b0109458536f7f1df58.o)\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/install_name_tool: object: bazel-out/darwin-dbg/bin/tensorflow/python/_pywrap_tensorflow_internal.so truncated or malformed object (LC_SEGMENT_64 command 0 fileoff field plus filesize field extends past the end of the file)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 9209.870s, Critical Path: 2417.25s\r\nINFO: 17280 processes: 17280 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Can you modify .bazelversion to 3.4.0 and use that version instead?\r\nOr might be just try to build our master branch, which is using bazel 3.7.2.", "# RRROR\r\n\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/install_name_tool: object: bazel-out/darwin-dbg/bin/tensorflow/python/_pywrap_tensorflow_internal.so truncated or malformed object (LC_SEGMENT_64 command 0 fileoff field plus filesize field extends past the end of the file)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n# System information\r\n\r\nOS Platform and Distribution: macOS Big Sur 11.1\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version: commit 56cf6452acc55f24fd1aa29f33c0708d449952f5\r\nPython version: /usr/local/anaconda3/envs/python3.8tf2.4/bin/python 3.8.5\r\nInstalled using virtualenv? pip? conda?: conda\r\nBazel version (if compiling from source): 3.7.2\r\nGCC/Compiler version (if compiling from source): Apple clang version 12.0.0 (clang-1200.0.32.29)\r\nCUDA/cuDNN version: no GPU, CPU-only\r\nGPU model and memory: no GPU\r\napple clang version\r\nApple clang version 12.0.0 (clang-1200.0.32.29)\r\nTarget: x86_64-apple-darwin20.2.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n# command\r\n\r\n```\r\nbazel \\\r\n--output_base=../tensorflow-cache/build_output_base \\\r\n--output_user_root=../tensorflow-cache/build_output_user_root \\\r\nbuild \\\r\n--config=dbg \\\r\n--copt=-O0 \\\r\n--strip=never \\\r\n--config=noaws \\\r\n--config=nogcp \\\r\n--config=nohdfs \\\r\n--config=nonccl \\\r\n--repository_cache ../tensorflow-cache \\\r\n--verbose_failures \\\r\n//tensorflow/tools/pip_package:build_pip_package\r\n```\r\n", "@rockspring \r\nCan you try updating to the latest stable version 2.6.0 and let us know if the issue still persists? Many bugs have been fixed in the latest version", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46566\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46566\">No</a>\n", "Interestingly, I didn't have this error half a year ago when this issue was raised, but now I can reproduce it, with latest versions, i.e. tensorflow 2.6 and bazel 3.7.2. I even tried bazel 4.1 (in case this was fixed there but not backported to v3.x) by increasing the max-allowed-version beyond 3.99.0 in tensorflow/configure.py."]}, {"number": 46565, "title": "How the quantization on BERT", "body": "tf version:2.2.0\r\nenviroment: win10 cpu\r\n\r\n`\r\nimport tensorflow as tf\r\nsaved_model_dir='bert_en_uncased_L-12_H-768_A-12_3'\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\r\ntflite_model = converter.convert()\r\n\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n`\r\n\r\nHere is the code for quantization  from [quantization](https://tensorflow.google.cn/lite/guide/get_started#\u91cf\u5316)\r\nI want to do the quantization on the device without GPU.\r\n\r\nhere is the error message, \r\n\"ValueError: None is only supported in the 1st dimension. Tensor 'input_mask' has invalid shape '[None, None]'.\"\r\n\r\n\r\n`\r\n2021-01-21 12:11:06.978175: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-Q99TL0E\r\n2021-01-21 12:11:06.978491: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2021-01-21 12:11:06.984840: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c5f66f5e80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-01-21 12:11:06.985036: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-01-21 12:11:16.651117: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2021-01-21 12:11:16.651396: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2021-01-21 12:11:16.761993: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2021-01-21 12:11:16.762196: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 1585 nodes (1367), 5592 edges (5360), time = 70.744ms.\r\n2021-01-21 12:11:16.762434: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 2.131ms.\r\n2021-01-21 12:11:22.677739: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2021-01-21 12:11:22.678009: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2021-01-21 12:11:41.718415: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2021-01-21 12:11:41.721239: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 1383 nodes (-203), 5188 edges (-405), time = 15370.7383ms.\r\n2021-01-21 12:11:41.721428: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 1383 nodes (0), 5188 edges (0), time = 766.249ms.\r\nTraceback (most recent call last):\r\n  File \"E:/Desktop/demo_models/convert.py\", line 10, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"D:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 483, in convert\r\n    _get_tensor_name(tensor), shape_list))\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_mask' has invalid shape '[None, None]'.\r\n\r\n\r\n`", "comments": ["Have a same error when run  quantization on 2080TI", "@TLCFYBJJHYYSND \r\n\r\nCan you try with latest stable version 2.4 and Nightly version and see if the issue still persists.Please, share the colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!\r\n", "> @TLCFYBJJHYYSND\r\n> \r\n> Can you try with latest stable version 2.4 and Nightly version and see if the issue still persists.Please, share the colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!\r\n\r\nThanks for reply, i tried it on tf2.4 and met a similar error, it causeed by BERT model i suppose?\r\n`tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_0/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Reshape' op requires 'shape' to have at most one dynamic dimension, but got multiple dynamic dimensions at indices 0 and 1\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from `\r\n\r\nthe model i used is download from [Model](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3), and i didn't do any fine-tuning just for testing the quantization on the raw model.\r\n\r\n\r\n`import tensorflow as tf`\r\n`saved_model_dir='bert_en_uncased_L-12_H-768_A-12_3'`\r\n`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory`\r\n`tflite_model = converter.convert()`\r\n`with open('model.tflite', 'wb') as f:`\r\n    `    f.write(tflite_model)`\r\n\r\nthey are my all test code, all from    https://tensorflow.google.cn/lite/guide/get_started#\u91cf\u5316\r\n\r\nDo we met this error before?", "Could you try the conversion code with the tf-nightly version? I think the above error has been fixed in the tf-nightly version.", "> Could you try the conversion code with the tf-nightly version? I think the above error has been fixed in the tf-nightly version.\r\n\r\nThanks, i try the tf-nightly version and fix the previous problem, but it comes a new problem\r\n\r\n\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_0/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_0/self_attention/query/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_0/self_attention/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_0/self_attention/value/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_0/self_attention/einsum_1/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_0/self_attention/attention_output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_0/intermediate/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_0/output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_1/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_1/self_attention/query/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_1/self_attention/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_1/self_attention/value/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_1/self_attention/einsum_1/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_1/self_attention/attention_output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_1/intermediate/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_1/output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_2/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_2/self_attention/query/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_2/self_attention/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_2/self_attention/value/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_2/self_attention/einsum_1/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_2/self_attention/attention_output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_2/intermediate/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_2/output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_3/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_3/self_attention/query/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_3/self_attention/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_3/self_attention/value/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_3/self_attention/einsum_1/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_3/self_attention/attention_output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_3/intermediate/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_3/output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_4/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_4/self_attention/query/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_4/self_attention/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_4/self_attention/value/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_4/self_attention/einsum_1/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_4/self_attention/attention_output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_4/intermediate/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_4/output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_5/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_5/self_attention/query/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_5/self_attention/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_5/self_attention/value/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_5/self_attention/einsum_1/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_5/self_attention/attention_output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_5/intermediate/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_5/output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_6/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_6/self_attention/query/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_6/self_attention/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_6/self_attention/value/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_6/self_attention/einsum_1/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_6/self_attention/attention_output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_6/intermediate/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_6/output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_7/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_7/self_attention/query/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_7/self_attention/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_7/self_attention/value/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_7/self_attention/einsum_1/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_7/self_attention/attention_output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_7/intermediate/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_7/output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_8/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_8/self_attention/query/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_8/self_attention/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_8/self_attention/value/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_8/self_attention/einsum_1/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_8/self_attention/attention_output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_8/intermediate/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_8/output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_9/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_9/self_attention/query/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_9/self_attention/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_9/self_attention/value/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_9/self_attention/einsum_1/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_9/self_attention/attention_output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_9/intermediate/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_9/output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_10/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_10/self_attention/query/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_10/self_attention/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_10/self_attention/value/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_10/self_attention/einsum_1/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_10/self_attention/attention_output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_10/intermediate/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_10/output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_11/self_attention/key/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_11/self_attention/query/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_11/self_attention/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_11/self_attention/value/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_11/self_attention/einsum_1/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_11/self_attention/attention_output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_11/intermediate/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/bert_encoder/transformer/layer_11/output/einsum/Einsum@__inference__wrapped_model_8622\" at \"StatefulPartitionedCall@__inference_signature_wrapper_22272\") at \"StatefulPartitionedCall\")): 'tf.Einsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: failed while converting: 'main': \r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: Einsum\r\nDetails:\r\n\ttf.Einsum {device = \"\", equation = \"abc,cd->abd\"}\r\n\ttf.Einsum {device = \"\", equation = \"abc,cde->abde\"}\r\n\ttf.Einsum {device = \"\", equation = \"abcd,cde->abe\"}\r\n\ttf.Einsum {device = \"\", equation = \"acbe,aecd->abcd\"}\r\n\ttf.Einsum {device = \"\", equation = \"aecd,abcd->acbe\"}\r\n\r\n\r\n\r\nProcess finished with exit code 1\r\n\r\n", "You can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select", "@TLCFYBJJHYYSND Could you please try as per the above comment and let us know if this issue still persists ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46565\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46565\">No</a>\n"]}, {"number": 46564, "title": "Regarding tensorflow error in ubuntu 18.04", "body": " F tensorflow/core/platform/cpu_feature_guard.cc:38] The TensorFlow library was compiled to use FMA instructions, but these aren't available on your machine.\r\nAborted (core dumped)\r\n\r\n\r\ncan anyone please help me with this error.", "comments": ["@Ashitha97,\r\nIn order to expedite the trouble-shooting process, could you please provide the following details\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nand also the exact sequence of commands / steps that you executed before running into the error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46564\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46564\">No</a>\n"]}, {"number": 46563, "title": "tf.Variable throws TypeError on conversion to typed numpy ndarray", "body": "The `__array__()` method is recognized by Numpy to allow objects to be conveniently converted to a Numpy ndarray.  This doesn't work consistently for`tf.Variable` because its definition of the method uses an incorrect signature that accepts no arguments (other than `self`): https://github.com/tensorflow/tensorflow/blob/569095ba3d5a57a95595d7db685b4bb748ca7337/tensorflow/python/ops/resource_variable_ops.py#L469\r\n\r\nThis breaks Numpy usage, which expects to be able to pass a dtype argument when converting to an explicitly typed array, as in `np.array(tf.Variable(0), dtype=np.int64)`.  (The expected signature is documented a bit sparsely but can been seen [here - scroll to \"PyArray_FromArrayAttr\"](https://numpy.org/devdocs/reference/c-api/array.html) or by example of how `ndarray` itself defines its `__array__` method [here](https://numpy.org/devdocs/reference/generated/numpy.ndarray.__array__.html).)\r\n\r\nTested against the latest nightly and latest numpy:\r\n\r\n```\r\nnumpy==1.19.5\r\ntf-nightly==2.5.0.dev20210119\r\n```\r\n\r\n#### Expected behavior:\r\n\r\nConverting `tf.Variable` with a dtype succeeds the same as `tf.constant` with a dtype, or `tf.Variable` without a dtype: \r\n\r\n```\r\n$ python -c 'import tensorflow as tf; import numpy as np; np.array(tf.constant(0), dtype=np.int32)'\r\n$ python -c 'import tensorflow as tf; import numpy as np; np.array(tf.Variable(0))'\r\n$\r\n```\r\n\r\n#### Actual behavior:\r\n\r\nIt raises a TypeError:\r\n\r\n```\r\n$ python -c 'import tensorflow as tf; import numpy as np; np.array(tf.Variable(0), dtype=np.int32)'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nTypeError: __array__() takes 1 positional argument but 2 were given\r\n```\r\n\r\n---\r\n\r\nNote that there are various other single-argument `__array__(self)` definitions within TF that should probably also be updated:\r\n- https://github.com/tensorflow/tensorflow/blob/5a1d7415893a2e27ee6084809820cc18d2183225/tensorflow/python/keras/engine/keras_tensor.py#L276\r\n- https://github.com/tensorflow/tensorflow/blob/0c3d87c81ed5d8ffb868b006036b6da6a0f73a1a/tensorflow/python/framework/ops.py#L852\r\n- https://github.com/tensorflow/tensorflow/blob/0c3d87c81ed5d8ffb868b006036b6da6a0f73a1a/tensorflow/python/framework/ops.py#L1030\r\n\r\nOn that note, I'm a bit confused why `tf.constant(0)` can be converted above because the `EagerTensorBase.__array__()` definition has the same problem as `tf.Variable`.  Indeed, this can be confirmed by trying to explicitly call it like this:\r\n\r\n```\r\n$ python -c 'import tensorflow as tf; import numpy as np; tf.constant(0).__array__(np.int32)'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nTypeError: __array__() takes 1 positional argument but 2 were given\r\n```\r\n\r\nMy best guess is that there's some other path through the \"convert python object to numpy array\" flowchart that is being taken, e.g. perhaps it's because `EagerTensorBase` implements `__len__` while `ResourceVariable` does not and so the constant is interpreted as a sequence and gets converted via that path.  However, it'd still be best if `__array__()` was consistently defined with the `dtype` argument within TF code, even for types where that path isn't reachable in normal usage today.", "comments": ["Was able to reproduce the issue in TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/365bb581f878da3c4f713223ea3c0fb2/untitled91.ipynb).Thanks!", "After testing various tensorflow versions, it looks like the buggy behavior was introduced between tensorflow version 2.3.2 (which does not have the bug) and version 2.4.0 (which does have the bug).", "Hi @nfelt ! This Issue is getting resolved in [2.8](https://colab.sandbox.google.com/gist/mohantym/ab47d10263f9fe50bf39e44526af041b/untitled91.ipynb#scrollTo=6gUtjKfliYGz) . Can we move this issue to closed status now?", "yes now that the issue is fixed in version 2.8, imho it's fine to close this issue. thank you", "Ok @kaczmarj ! Closing this issue as it is resolved in 2.8 . Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46563\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46563\">No</a>\n"]}, {"number": 46562, "title": "[Intel MKL] Fixing threadpool bug", "body": "This change fixes a potential bug by managing the lifetime of MKL threadpool object in a better way.", "comments": []}, {"number": 46561, "title": "Add sanitizer builds to the TFLM CI.", "body": "Also, updated the docs with the new way of using the sanitizers with bazel.\r\n    \r\nWith https://github.com/tensorflow/tensorflow/commit/38f8ad1795b392f0f6bdf86a5a9c4f227bac7b76 we now have a more concise way of using asan/msan/ubsan with bazel.\r\n    \r\nFixes http://b/177076500\r\n\r\nStill open issue: http://b/178621680\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46560, "title": "Update version numbers for TensorFlow 2.4.1", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 4 -> 4\nPatch: 0 -> 1\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.4.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\nBinary file \ntensorflow/cc/saved_model/testdata/StaticHashTableModule/saved_model.pb matches\nBinary file tensorflow/cc/saved_model/testdata/AssetModule/saved_model.pb \nmatches\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:188:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:211:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:212:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:217:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:267:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:268:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:269:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:270:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:271:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:272:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:296:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:297:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:298:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:301:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:302:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:129:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:178:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:179:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:184:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:215:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:216:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:217:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:218:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:222:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:223:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:224:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:225:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:128:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:176:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:177:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:181:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:217:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:218:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:219:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:220:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:227:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:228:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:229:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:230:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:63:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:68:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:77:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:78:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:86:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:87:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:117:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:148:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:153:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:164:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:180:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:197:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:233:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:313:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:321:2.4.0\ntensorflow/security/advisory/tfsa-2020-028.md:16:2.4.0\ntensorflow/security/advisory/tfsa-2020-027.md:41:2.4.0\ntensorflow/tools/pip_package/setup.py:100:2.4.0\ntensorflow/tools/pip_package/setup.py:112:2.4.0\ntensorflow/tools/pip_package/setup.py:114:2.4.0\ntensorflow/tools/ci_build/release/common_win.bat:49:2.4.0\ntensorflow/tools/ci_build/release/common.sh:143:2.4.0\ntensorflow/tools/ci_build/release/common.sh:199:2.4.0\ntensorflow/python/keras/__init__.py:35:2.4.0\nBinary file \ntensorflow/c/experimental/saved_model/internal/testdata/UninitializedVariable/sa\nved_model.pb matches\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.4.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\nBinary file \ntensorflow/cc/saved_model/testdata/StaticHashTableModule/saved_model.pb matches\nBinary file tensorflow/cc/saved_model/testdata/AssetModule/saved_model.pb \nmatches\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:188:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:211:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:212:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:217:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:267:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:268:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:269:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:270:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:271:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:272:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:296:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:297:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:298:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:301:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:302:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:129:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:178:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:179:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:184:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:215:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:216:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:217:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:218:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:222:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:223:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:224:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:225:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:128:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:176:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:177:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:181:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:217:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:218:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:219:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:220:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:227:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:228:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:229:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:230:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:63:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:68:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:77:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:78:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:86:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:87:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:117:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:148:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:153:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:164:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:180:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:197:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:233:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:313:2.4.0\ntensorflow/lite/tools/versioning/runtime_version.cc:321:2.4.0\ntensorflow/security/advisory/tfsa-2020-028.md:16:2.4.0\ntensorflow/security/advisory/tfsa-2020-027.md:41:2.4.0\ntensorflow/tools/pip_package/setup.py:100:2.4.0\ntensorflow/tools/pip_package/setup.py:112:2.4.0\ntensorflow/tools/pip_package/setup.py:114:2.4.0\ntensorflow/tools/ci_build/release/common_win.bat:49:2.4.0\ntensorflow/tools/ci_build/release/common.sh:143:2.4.0\ntensorflow/tools/ci_build/release/common.sh:199:2.4.0\ntensorflow/python/keras/__init__.py:35:2.4.0\nBinary file \ntensorflow/c/experimental/saved_model/internal/testdata/UninitializedVariable/sa\nved_model.pb matches\n```", "comments": []}, {"number": 46559, "title": "Issue on using tf.vectorized_map on a function with a tf.while_loop", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): ```conda install tensorflow-gpu``` in a conda env\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.8.3\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory:  GeForce RTX 2060 SUPER computeCapability: 7.5, Memory  8 GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThe included code tries to vectorize a function which adds 10.0 to the input and does so through a while loop which adds 1.0 each time (for 10 times). The function runs perfectly when using tf.map_fn and fails when using tf.vectorized_map\r\n\r\n**Describe the expected behavior**\r\nThe function would not run when using vectorized map and the error points towards  `` Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower``\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nif __name__ == \"__main__\":\r\n    import tensorflow as tf\r\n    @tf.function()\r\n    def add(a):       \r\n        i = tf.constant(0, dtype = tf.int32)\r\n        c = tf.constant(1., dtype = tf.float32)\r\n        loop_index = lambda  i, c, a: i < 10\r\n        def body(i, c, a):\r\n            a = c + a\r\n            i = i + 1\r\n            return i,c, a\r\n        i,c, a = tf.while_loop(loop_index, body, [i,c, a],\\\r\n             shape_invariants=[tf.TensorShape(()), tf.TensorShape(()),tf.TensorShape([1])], back_prop= False, parallel_iterations=1)\r\n\r\n        return a\r\n\r\n    counter =  tf.reshape(tf.range(0, 40, delta = 1, dtype = tf.float32), shape = [40,1])\r\n\r\n    all_ = tf.vectorized_map(add, counter) # does not work\r\n    # all_ = tf.map_fn(add, counter)\r\n    print(all_, '<-- should be [40,1] float32 tensor with elements [10., 11., ...49.]')\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n2021-01-20 17:26:53.074085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2021-01-20 17:26:53.101801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-20 17:26:53.102114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5\r\ncoreClock: 1.71GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-01-20 17:26:53.102265: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-01-20 17:26:53.103295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-01-20 17:26:53.104241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-01-20 17:26:53.104386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-01-20 17:26:53.105247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-01-20 17:26:53.105734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-01-20 17:26:53.107658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-01-20 17:26:53.107742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-20 17:26:53.108011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-20 17:26:53.108212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2021-01-20 17:26:53.108397: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2021-01-20 17:26:53.112400: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz\r\n2021-01-20 17:26:53.112694: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55853e36c7b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-01-20 17:26:53.112704: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-01-20 17:26:53.112820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-20 17:26:53.113110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5\r\ncoreClock: 1.71GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-01-20 17:26:53.113143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-01-20 17:26:53.113154: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-01-20 17:26:53.113163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-01-20 17:26:53.113171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-01-20 17:26:53.113180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-01-20 17:26:53.113188: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-01-20 17:26:53.113197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-01-20 17:26:53.113232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-20 17:26:53.113508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-20 17:26:53.113760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2021-01-20 17:26:53.113779: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-01-20 17:26:53.182683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-20 17:26:53.182703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2021-01-20 17:26:53.182707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2021-01-20 17:26:53.182837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-20 17:26:53.183090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-20 17:26:53.183343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-20 17:26:53.183550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6620 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2021-01-20 17:26:53.184629: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55853ec63ff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-01-20 17:26:53.184638: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2060 SUPER, Compute Capability 7.5\r\nWARNING:tensorflow:From distransTest.py:266: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nback_prop=False is deprecated. Consider using tf.stop_gradient instead.\r\nInstead of:\r\nresults = tf.while_loop(c, b, vars, back_prop=False)\r\nUse:\r\nresults = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\r\nERROR:tensorflow:Got error while pfor was converting op name: \"loop_body/PartitionedCall\"\r\nop: \"PartitionedCall\"\r\ninput: \"loop_body/GatherV2\"\r\nattr {\r\n  key: \"Tin\"\r\n  value {\r\n    list {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"Tout\"\r\n  value {\r\n    list {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"_read_only_resource_inputs\"\r\n  value {\r\n    list {\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"config\"\r\n  value {\r\n    s: \"\"\r\n  }\r\n}\r\nattr {\r\n  key: \"config_proto\"\r\n  value {\r\n    s: \"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0012\\005*\\0010J\\0008\\001\"\r\n  }\r\n}\r\nattr {\r\n  key: \"executor_type\"\r\n  value {\r\n    s: \"\"\r\n  }\r\n}\r\nattr {\r\n  key: \"f\"\r\n  value {\r\n    func {\r\n      name: \"__inference_add_57\"\r\n    }\r\n  }\r\n}\r\nwith inputs (<tf.Tensor 'loop_body/GatherV2:0' shape=(1,) dtype=float32>,)\r\n, converted inputs [WrappedTensor(t=<tf.Tensor 'loop_body/GatherV2/params:0' shape=(40, 1) dtype=float32>, is_stacked=True, is_sparse_stacked=False)]\r\nin user code:\r\n\r\n    /home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/pfor.py:3600 f  *\r\n        [converter._convert_helper(x).t for x in func._func_graph_outputs])\r\n    /home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/pfor.py:1460 _convert_helper  **\r\n        raise ValueError(\"No converter defined for %s\\n%s\\ninputs: %s. \"\r\n\r\n    ValueError: No converter defined for StatelessWhile\r\n    name: \"while\"\r\n    op: \"StatelessWhile\"\r\n    input: \"while/loop_counter\"\r\n    input: \"while/maximum_iterations\"\r\n    input: \"Const\"\r\n    input: \"Const_1\"\r\n    input: \"a\"\r\n    attr {\r\n      key: \"T\"\r\n      value {\r\n        list {\r\n          type: DT_INT32\r\n          type: DT_INT32\r\n          type: DT_INT32\r\n          type: DT_FLOAT\r\n          type: DT_FLOAT\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"_lower_using_switch_merge\"\r\n      value {\r\n        b: true\r\n      }\r\n    }\r\n    attr {\r\n      key: \"_num_original_outputs\"\r\n      value {\r\n        i: 5\r\n      }\r\n    }\r\n    attr {\r\n      key: \"_read_only_resource_inputs\"\r\n      value {\r\n        list {\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"body\"\r\n      value {\r\n        func {\r\n          name: \"while_body_20\"\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"cond\"\r\n      value {\r\n        func {\r\n          name: \"while_cond_19\"\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"output_shapes\"\r\n      value {\r\n        list {\r\n          shape {\r\n          }\r\n          shape {\r\n          }\r\n          shape {\r\n          }\r\n          shape {\r\n          }\r\n          shape {\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"parallel_iterations\"\r\n      value {\r\n        i: 1\r\n      }\r\n    }\r\n    \r\n    inputs: [WrappedTensor(t=<tf.Tensor 'while/loop_counter/pfor/Const:0' shape=() dtype=int32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'while/maximum_iterations/pfor/Const:0' shape=() dtype=int32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'Const/pfor/Const:0' shape=() dtype=int32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'Const_1/pfor/Const:0' shape=() dtype=float32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'args_0:0' shape=(40, 1) dtype=float32>, is_stacked=True, is_sparse_stacked=False)]. \r\n    Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower\r\n\r\nHere are the pfor conversion stack traces:\r\nERROR:tensorflow:name: \"loop_body/PartitionedCall\"\r\nop: \"PartitionedCall\"\r\ninput: \"loop_body/GatherV2\"\r\nattr {\r\n  key: \"Tin\"\r\n  value {\r\n    list {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"Tout\"\r\n  value {\r\n    list {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"_read_only_resource_inputs\"\r\n  value {\r\n    list {\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"config\"\r\n  value {\r\n    s: \"\"\r\n  }\r\n}\r\nattr {\r\n  key: \"config_proto\"\r\n  value {\r\n    s: \"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0012\\005*\\0010J\\0008\\001\"\r\n  }\r\n}\r\nattr {\r\n  key: \"executor_type\"\r\n  value {\r\n    s: \"\"\r\n  }\r\n}\r\nattr {\r\n  key: \"f\"\r\n  value {\r\n    func {\r\n      name: \"__inference_add_57\"\r\n    }\r\n  }\r\n}\r\n\r\ncreated at:\r\n    File \"distransTest.py\", line 273, in <module>\r\n    all_ = tf.vectorized_map(add, counter) # does not work\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 407, in vectorized_map\r\n    return pfor(loop_fn, batch_size)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 198, in pfor\r\n    outputs = f()\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 627, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 505, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2657, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 957, in wrapper\r\n    return autograph.converted_call(\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 183, in f\r\n    return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 237, in _pfor_impl\r\n    loop_fn_outputs = loop_fn(loop_var)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 400, in loop_fn\r\n    return fn(gathered_elems)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 650, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\r\n    return self._call_flat(\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1760, in _call_flat\r\n    flat_outputs = forward_function.call(ctx, args_with_tangents)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 621, in call\r\n    outputs = functional_ops.partitioned_call(\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/functional_ops.py\", line 1180, in partitioned_call\r\n    op = graph.create_op(op_name, args, tout, name=op_name, attrs=op_attrs)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3257, in create_op\r\n    return self._create_op_internal(op_type, inputs, dtypes, input_types, name,\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 593, in _create_op_internal\r\n    return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3319, in _create_op_internal\r\n    ret = Operation(\r\n    File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1791, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nTraceback (most recent call last):\r\n  File \"distransTest.py\", line 273, in <module>\r\n    all_ = tf.vectorized_map(add, counter) # does not work\r\n  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 407, in vectorized_map\r\n    return pfor(loop_fn, batch_size)\r\n  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 198, in pfor\r\n    outputs = f()\r\n  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 627, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 505, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2657, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in user code:\r\n\r\n    /home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:183 f  *\r\n        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n    /home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/pfor.py:3600 f  *\r\n        [converter._convert_helper(x).t for x in func._func_graph_outputs])\r\n    /home/arka/anaconda3/envs/Tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/pfor.py:1460 _convert_helper  **\r\n        raise ValueError(\"No converter defined for %s\\n%s\\ninputs: %s. \"\r\n\r\n    ValueError: No converter defined for StatelessWhile\r\n    name: \"while\"\r\n    op: \"StatelessWhile\"\r\n    input: \"while/loop_counter\"\r\n    input: \"while/maximum_iterations\"\r\n    input: \"Const\"\r\n    input: \"Const_1\"\r\n    input: \"a\"\r\n    attr {\r\n      key: \"T\"\r\n      value {\r\n        list {\r\n          type: DT_INT32\r\n          type: DT_INT32\r\n          type: DT_INT32\r\n          type: DT_FLOAT\r\n          type: DT_FLOAT\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"_lower_using_switch_merge\"\r\n      value {\r\n        b: true\r\n      }\r\n    }\r\n    attr {\r\n      key: \"_num_original_outputs\"\r\n      value {\r\n        i: 5\r\n      }\r\n    }\r\n    attr {\r\n      key: \"_read_only_resource_inputs\"\r\n      value {\r\n        list {\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"body\"\r\n      value {\r\n        func {\r\n          name: \"while_body_20\"\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"cond\"\r\n      value {\r\n        func {\r\n          name: \"while_cond_19\"\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"output_shapes\"\r\n      value {\r\n        list {\r\n          shape {\r\n          }\r\n          shape {\r\n          }\r\n          shape {\r\n          }\r\n          shape {\r\n          }\r\n          shape {\r\n            dim {\r\n              size: 1\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"parallel_iterations\"\r\n      value {\r\n        i: 1\r\n      }\r\n    }\r\n    \r\n    inputs: [WrappedTensor(t=<tf.Tensor 'while/loop_counter/pfor/Const:0' shape=() dtype=int32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'while/maximum_iterations/pfor/Const:0' shape=() dtype=int32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'Const/pfor/Const:0' shape=() dtype=int32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'Const_1/pfor/Const:0' shape=() dtype=float32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'args_0:0' shape=(40, 1) dtype=float32>, is_stacked=True, is_sparse_stacked=False)]. \r\n    Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower\r\n\r\n```\r\n", "comments": ["@arkadeepnc \r\n\r\nI have tried in colab with TF-GPU versions 2.3 , 2.4 and I am not seeing any issue.Attached gist [here](https://colab.research.google.com/gist/ravikyram/43c6698c61e793b03cb1f82d0b62f6fd/untitled627.ipynb).\r\nPlease, verify once and close the issue. Thanks!", "Thanks @ravikyram. The issue is specific to my setup and is not present on TF2.4. I tested this on my system ans can confirm this.\r\nClosing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46559\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46559\">No</a>\n"]}, {"number": 46558, "title": "[Intel MKL] Re-enable Conv2D+Squeeze+BiasAdd fusion", "body": "This PR enables Conv2D+Squeeze+BiasAdd fusion in grappler which is already supported by MKL.", "comments": []}, {"number": 46557, "title": "Update release notes for TensorFlow 2.4.1", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.4.1\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 46556, "title": "TypeError: cannot pickle '_thread.lock' object in TensorFlow 2.4", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.4.0-0-g582c8d236cb 2.4.0\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nRunning a simple training process with MultiWorkerMirroredStrategy fails with `TypeError: can't pickle _thread.lock objects`.\r\n\r\n**Describe the expected behavior**\r\nThe training should proceed without errors.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nThe example needs to run in a distributed environment to reproduce the issue, so save the script in a file and run it in 3 different terminals.\r\n\r\n```\r\nTF_CONFIG='{\"cluster\": {\"chief\": [\"localhost:2222\"], \"worker\": [\"localhost:2223\", \"localhost:2224\"]}, \"task\": {\"type\": \"chief\", \"index\": 0}}' python script.py \r\nTF_CONFIG='{\"cluster\": {\"chief\": [\"localhost:2222\"], \"worker\": [\"localhost:2223\", \"localhost:2224\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}}' python script.py\r\nTF_CONFIG='{\"cluster\": {\"chief\": [\"localhost:2222\"], \"worker\": [\"localhost:2223\", \"localhost:2224\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}}' python script.py\r\n```\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\u00a0\r\nbuffer_size = 10000\r\nbatch_size = 64\r\nlearning_rate = 1e-4\r\n\u00a0\r\ndef input_fn(mode, input_context=None):\r\n  tfds.disable_progress_bar()\r\n  datasets, _ = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n  mnist_dataset = ( \r\n      datasets['train']\r\n      if mode == tf.estimator.ModeKeys.TRAIN else datasets['test'])\r\n\u00a0\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255 \r\n    return image, label\r\n\u00a0\r\n  if input_context:\r\n    mnist_dataset = mnist_dataset.shard(input_context.num_input_pipelines,\r\n                                        input_context.input_pipeline_id)\r\n  return mnist_dataset.map(scale).cache().shuffle(buffer_size).batch(batch_size)\r\n\u00a0\r\ndef model_fn(features, labels, mode):\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n  ])\r\n  logits = model(features, training=False)\r\n\u00a0\r\n  if mode == tf.estimator.ModeKeys.PREDICT:\r\n    predictions = {'logits': logits}\r\n    return tf.estimator.EstimatorSpec(labels=labels, predictions=predictions)\r\n\u00a0\r\n  optimizer = tf.compat.v1.train.GradientDescentOptimizer(\r\n      learning_rate=learning_rate)\r\n  loss = tf.keras.losses.SparseCategoricalCrossentropy(\r\n      from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(labels,\r\n                                                                  logits)\r\n  loss = tf.reduce_sum(loss) * (1. / batch_size)\r\n  if mode == tf.estimator.ModeKeys.EVAL:\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n\u00a0\r\n  logging_hook = tf.estimator.LoggingTensorHook({'loss': loss}, every_n_iter=10)\r\n\u00a0\r\n  return tf.estimator.EstimatorSpec(\r\n      mode=mode,\r\n      loss=loss,\r\n      training_hooks=[logging_hook],\r\n      train_op=optimizer.minimize(\r\n          loss, tf.compat.v1.train.get_or_create_global_step()))\r\n\u00a0\r\n\u00a0\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\u00a0\r\nconfig = tf.estimator.RunConfig(train_distribute=strategy)\r\n\u00a0\r\nclassifier = tf.estimator.Estimator(\r\n    model_fn=model_fn, model_dir='/tmp/multiworker', config=config)\r\n\u00a0\r\ntf.estimator.train_and_evaluate(\r\n    classifier,\r\n    train_spec=tf.estimator.TrainSpec(input_fn=input_fn),\r\n    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nFull logs:\r\n```\r\nTF_CONFIG='{\"cluster\": {\"chief\": [\"localhost:2222\"], \"worker\": [\"localhost:2223\", \"localhost:2224\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}}' python script.py\r\nWARNING:tensorflow:From script.py:68: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\r\nInstructions for updating:                                                                \r\nuse distribute.MultiWorkerMirroredStrategy instead                                                       \r\n2021-01-20 18:24:44.477611: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-20 18:24:44.479538: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-01-20 18:24:44.491607: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job chief -> {0 -> localhost:2222}\r\n2021-01-20 18:24:44.491654: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224}\r\n2021-01-20 18:24:44.492211: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:2224\r\nTraceback (most recent call last):                                                                                           \r\n  File \"script.py\", line 73, in <module>                                                  \r\n    model_fn=model_fn, model_dir='/tmp/multiworker', config=config)                                \r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 183, in __init__\r\n    config, model_dir)                                                                          \r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1832, in maybe_overwrite_model_dir_and_session_config\r\n    config = run_config.RunConfig.replace(config, session_config=session_config)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/run_config.py\", line 923, in replace\r\n    copy.deepcopy(self),                                                             \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 180, in deepcopy                    \r\n    y = _reconstruct(x, memo, *rv)                                                                 \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 281, in _reconstruct         \r\n    state = deepcopy(state, memo)                                                              \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 150, in deepcopy            \r\n    y = copier(x, memo)                                                                            \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 241, in _deepcopy_dict       \r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)          \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 161, in deepcopy                      \r\n    y = copier(memo)                                                                                                       \r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1542, in __deepcopy__\r\n    setattr(result, k, copy.deepcopy(v, memo))                                                    \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 180, in deepcopy                                             \r\n    y = _reconstruct(x, memo, *rv)                                                \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 281, in _reconstruct                                             \r\n    state = deepcopy(state, memo)                                                   \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 150, in deepcopy                             \r\n    y = copier(x, memo)                                                                    \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 241, in _deepcopy_dict              \r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)                                                                   \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 180, in deepcopy                      \r\n    y = _reconstruct(x, memo, *rv)                                                             \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 281, in _reconstruct                                             \r\n    state = deepcopy(state, memo)                                                   \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 150, in deepcopy            \r\n    y = copier(x, memo)                                                                      \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 241, in _deepcopy_dict                               \r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)                            \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 180, in deepcopy                       \r\n    y = _reconstruct(x, memo, *rv)                                                                                             \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 281, in _reconstruct                       \r\n    state = deepcopy(state, memo)                                                                          \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 150, in deepcopy                 \r\n    y = copier(x, memo)                                                                   \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 241, in _deepcopy_dict                     \r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)                                                                     \r\n  File \"/opt/conda/lib/python3.7/copy.py\", line 169, in deepcopy                       \r\n    rv = reductor(4)                                                                                                 \r\nTypeError: can't pickle _thread.lock objects  \r\n```", "comments": ["Was able to reproduce the issue with TF v2.4 and TF-nightly. Please check the attached screenshot for reference. \r\n\r\n![Screenshot 2021-01-20 at 2 29 33 PM](https://user-images.githubusercontent.com/57165142/105221025-8dec1b80-5b7e-11eb-92b3-baadf15acfca.png)\r\n\r\nWhereas with TF v2.3 the error is `W tensorflow/core/common_runtime/eager/context.cc:566] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.`\r\n\r\nSimilar to issue [#45918](https://github.com/tensorflow/tensorflow/issues/45918)\r\n\r\nThanks!", "Hi @rpasricha, to narrow down the issue here I just tried running the sample provided in the[ Multi-worker training with Estimator tutorial](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator). I seem to be getting the same `can't pickle _thread.lock objects` error. Can you test with that code and let me know if you see the same?", "I am having the same problem in TF2.4.0 and TF2.4.1 with the same stacktrace. I tried both python3.6/python3.7 with `tf.distribute.experimental.MultiWorkerMirroredStrategy()` and `tf.distribute.MultiWorkerMirroredStrategy()`. Any updates? ", "Also facing the issue. Is there a workaround for this issue?", "> Also facing the issue. Is there a workaround for this issue?\r\n\r\nThe problem is this `threading` introduced in tf2.4 https://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/python/distribute/collective_all_reduce_strategy.py#L22 \r\n\r\nI find a workaround by explicitly overwriting `__deepcopy__` for both `RunConfig` and `MultiWorkerMirroredStrategy`:\r\n\r\n```\r\nclass MyRunConfig(tf.estimator.RunConfig):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n\r\n    def __deepcopy__(self, memo={}):\r\n        cls = self.__class__\r\n        result = cls.__new__(cls)\r\n        memo[id(self)] = result\r\n        for k, v in self.__dict__.items():\r\n            if '_distribute' in k:\r\n                setattr(result, k, v)\r\n            else:\r\n                setattr(result, k, deepcopy(v, memo))\r\n        return result\r\n\r\nclass MyDistributeStrategy(tf.distribute.MultiWorkerMirroredStrategy):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n\r\n    def __deepcopy__(self, memo={}):\r\n        cls = self.__class__\r\n        result = cls.__new__(cls)\r\n        memo[id(self)] = result\r\n        for k, v in self.__dict__.items():\r\n            if '_extend' in k:\r\n                setattr(result, k, v)\r\n            else:\r\n                setattr(result, k, deepcopy(v, memo))\r\n        return result\r\n```\r\n\r\nIn your code, just use the above two class for `RunConfig` and `tf.distribute.MultiWorkerMirroredStrategy` as follows \r\n```\r\nstrategy = MyDistributeStrategy()\r\nconfig = MyRunConfig(train_distribute=strategy)\r\n```\r\n", "Could you try disable eager execution via: tf.compat.v1.disable_eager_execution()\r\n\r\nit needs to be called at the beginning of the program.", "Found another workaround\r\n\r\nAdd the following code to explicitly disable health check\r\n```\r\nfrom tensorflow.python.distribute.collective_all_reduce_strategy import CollectiveAllReduceExtended\r\nCollectiveAllReduceExtended._enable_check_health = False\r\n```\r\n", "I was able to get around the issue by disabling eager execution, thanks. \r\n\r\n@nikitamaia The tutorial only runs the code on a single node, the issue only arises when doing distributed training with estimator + multi worker mirrored strategy.", "@rpasricha ah yes, I meant to say to run the code from the tutorial but in your multi node environment (not in colab). Regardless, seems we have a workaround for now.", "Closing this issue since it is a duplicate of #45918. For further updates please refer to the other thread so we can track this in one place.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46556\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46556\">No</a>\n"]}, {"number": 46555, "title": "[INTEL MKL] Remove unnecessary oneDNN dependency.", "body": "Removes the redundant inclusion of oneDNN in libtensorflow_framework.so.2.", "comments": ["@agramesh1 can you please check for sanity build failures ?", "> @agramesh1 can you please check for sanity build failures ?\r\n\r\n@gbaned pushed an update to fix the issue. Thanks."]}, {"number": 46553, "title": "set_weights() for tf.keras.layers.Layer not working for type string.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Cataline 10.15.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): clang 11.0.3\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen trying to `set_weights()` for a layer where the weights are strings the data type `string` is not understood. See the standalone code below.  \r\n\r\n**Describe the expected behavior**\r\nThe weights should be updated with no error.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef initializer(shape, dtype=None):\r\n    return tf.reshape(tf.constant([str(x) for x in range(shape[0])]), shape)\r\n\r\n\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n\r\n    def __init__(self, length=10, **kwargs):\r\n        super(CustomLayer, self).__init__(**kwargs)\r\n        self.length = length\r\n\r\n    def build(self, input_shape=None):\r\n        self.keys = self.add_weight(shape=(self.length,),\r\n                                    name='keys',\r\n                                    initializer=initializer,\r\n                                    dtype=tf.string,\r\n                                    trainable=False)\r\n\r\n        self.built = True\r\n\r\n    def call(self, x):\r\n        return self.keys\r\n\r\n    def get_config(self):\r\n        config = super(CustomLayer, self).get_config()\r\n        config.update({'length': self.length})\r\n        return config\r\n\r\n\r\ninputs = tf.keras.layers.Input(shape=())\r\noutputs = CustomLayer(name='lookup')(inputs)\r\nmodel = tf.keras.Model(inputs, outputs)\r\nmodel.compile()\r\n\r\nmodel.get_layer('lookup').set_weights(model.get_layer('lookup').get_weights())\r\n```\r\n\r\n**Other info / logs** \r\n```\r\nTraceback (most recent call last):\r\n  File \"example.py\", line 37, in <module>\r\n    model.get_layer('lookup').set_weights(model.get_layer('lookup').get_weights())\r\n  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1877, in set_weights\r\n    backend.batch_set_value(weight_value_tuples)\r\n  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3706, in batch_set_value\r\n    x.assign(np.asarray(value, dtype=dtype(x)))\r\n  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 83, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\nTypeError: data type 'string' not understood\r\n```\r\n", "comments": ["I have tried in colab with TF version 2.3,2.4 and Nightly version(`2.5.0-dev20210119`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/60925393fd76ea4a37ea09c4d3669c21/untitled626.ipynb).Thanks!", "What is the scope of having trainable weights as string? Do you need non trainable variables https://www.tensorflow.org/guide/keras/custom_layers_and_models#layers_can_have_non-trainable_weights?", "@bhack you are right, using `tf.Variable()`works for my use case. Using version 2.2.0 (AI platform on google does not support 2.4.0 yet) and had to create a custom layer for a lookup table. However, using a Variable will work as well. As long as there is no reason to have weights as strings this issue can be closed.\r\n\r\nUPDATE: It is not working..", "@mihaimaruseac Can you evaluate if we could \"fail-fast\" for trainabile weights with string dtype?", "> @mihaimaruseac Can you evaluate if we could \"fail-fast\" for trainabile weights with string dtype?\r\n\r\nAlso non-trainable weights, no?", "I tried using a non-trainable `tf.Variable` instead, however there are still some issues with loading the model. My understanding is the difference between how TensorFlow and Numpy treats `dtype`.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n\r\n    def __init__(self, **kwargs):\r\n        super(CustomLayer, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape=None):\r\n        self.keys = tf.Variable(['foo'],\r\n                                shape=(1,),\r\n                                trainable=False,\r\n                                dtype=tf.string)\r\n        self.built = True\r\n\r\n    def call(self, x):\r\n        return x\r\n\r\ninputs = tf.keras.layers.Input(shape=())\r\noutputs = CustomLayer(name='lookup')(inputs)\r\nmodel = tf.keras.Model(inputs, outputs)\r\nmodel.compile(loss='mse')\r\n\r\ncallbacks = [\r\n    tf.keras.callbacks.EarlyStopping(monitor='loss', restore_best_weights=True)\r\n]\r\n\r\nmodel.fit([1], [1], epochs=10, callbacks=callbacks)\r\n```\r\n\r\n```\r\nEpoch 1/10\r\n1/1 [==============================] - 0s 92ms/step - loss: 0.0000e+00\r\nEpoch 2/10\r\n1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\r\nTraceback (most recent call last):\r\n  File \"temp.py\", line 29, in <module>\r\n    model.fit([1], [1], epochs=10, callbacks=callbacks)\r\n  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1145, in fit\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 432, in on_epoch_end\r\n    callback.on_epoch_end(epoch, numpy_logs)\r\n  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 1778, in on_epoch_end\r\n    self.model.set_weights(self.best_weights)\r\n  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1877, in set_weights\r\n    backend.batch_set_value(weight_value_tuples)\r\n  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3706, in batch_set_value\r\n    x.assign(np.asarray(value, dtype=dtype(x)))\r\n  File \"/Users/mans.bermell/.pyenv/versions/tensorflow2.4/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 83, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\nTypeError: data type 'string' not understood\r\n```\r\n\r\nI managed to do a work around by changing https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L3810 to:\r\n```\r\n      _dtype = str if dtype(x) == 'string' else dtype(x)\r\n      x.assign(np.asarray(value, dtype=_dtype))\r\n```\r\n", "I think it's better to send that as a PR.", "@MansBermellLeo You can try to make a PR with\r\n\r\n```\r\ndtype=dtypes_module.as_dtype(x.dtype).as_numpy_dtype\r\n```", "We can close this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46553\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46553\">No</a>\n", "@mihaimaruseac Is the reverting bot fixed to reopen this on revert?", "I don't think so. We basically need GitHub to know that reverting a commit that closes an issue reopens it.\r\n\r\nIn this case, the issue was closed manually, so for sure we'd have to do manual work in case of revert.", "@mihaimaruseac If we cover the first case we could use a policy for the reviewer to connect the ticket before merge if the PR author didn't mentioned correctly the original ticket for Github PR-issue auto-link."]}, {"number": 46552, "title": "Source code for tf.signal.rfft", "body": "Hi,\r\n\r\nI am trying to convert tf.signal.fft into c code for my project. I have gone through the below link to understand the functionality\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/signal/fft_ops.py#L115-L141\r\n\r\nI could not find the complete source to understand the  tf.signal.fft  API.\r\n\r\nThe source code found is returning  fft_fn(input_tensor, fft_length, Tcomplex=complex_dtype, name=name) but no complete steps.\r\n\r\nCan someone please help to find the definition of the above function  fft_fn(..)\r\n\r\nor\r\n\r\nif there is any c code for this function, please provide the link or attachment.\r\n\r\nRegards,\r\nYugesh.\r\n\r\n\r\n\r\n", "comments": ["I think code is based on [Eigen and Cublas]\r\n(https://github.com/tensorflow/tensorflow/blob/7b4c01794fbc2e6dc46e93a42416dd80929ce1e5/tensorflow/core/kernels/fft_ops.cc#L124)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46551, "title": "Add cudaMallocAsync as an option", "body": "@sanjoy \r\n\r\nThis PR add cudaMallocAsync as an option when CUDA 11.2 is used.", "comments": ["I just added a commit that add information to the error message.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46551) for more info**.\n\n<!-- need_author_cla -->", "The CI have many errors. I checked 2 of them and it was:\r\n```\r\nIn file included from tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:22:0:\r\n./tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.h:21:10: fatal error: third_party/gpus/cuda/include/cuda.h: No such file or directory\r\n #include \"third_party/gpus/cuda/include/cuda.h\"\r\n```\r\n\r\nI just pushed a fix that should fix those Google CI build error. The build was working here.", "I amended the last commit to try to fix build failure in Google CI.\r\nHopefully, I didn't miss anything this time.", "Amended again to include this diff:\r\n```\r\ndiff --git a/tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.h b/tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.h\r\nindex 7e79414d3be..517d112a115 100644\r\n--- a/tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.h\r\n+++ b/tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.h\r\n@@ -76,7 +76,7 @@ class GpuCudaMallocAsyncAllocator : public Allocator {\r\n   // compute stream and already synchronize with the h2d, d2h and d2d\r\n   // stream. So we do not need to ask cudaMallocAsync to add extra\r\n   // synchronization.\r\n-#ifdef GOOGLE_CUDA && CUDA_VERSION >= 11020\r\n+#if defined(GOOGLE_CUDA) && CUDA_VERSION >= 11020\r\n   cudaStream_t cuda_stream_;\r\n\r\n```", "As the PR is still open, I added something that I needed. A way to preallocate the memory.", "This looks good to me, thanks a lot Fr\u00e9d\u00e9ric.\r\nI will merge the change manually to do some extra testing.", "GpuCudaMallocAsyncAllocator checks whether op-determinism is enabled and slightly changes it behavior if it is, but I don't think this is necessary. Op-determinism only affects whether individual ops are deterministic, and does not guarantee deterministic memory consumption or op execution order.\r\n\r\n@nouiz, Can I remove the following determinism check, or is this important for some reason?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/892f28f602427e64df34ba4b5813a1b3cc31ecf8/tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc#L102-L121\r\n\r\n/CC @duncanriach ", "@reedwm Why do you want to remove it?\r\n\r\nYou are right that having it doesn't make the memory usage deterministic.\r\nBut I think user would be surprised that using it make the memory usage less deterministic then it was. Especially when they try to get deterministic behavior.\r\n\r\nPersonally, I do not like having tons of flags.", "The [determinism RFC](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md) specifies the only thing op-determinism does is \"if a user runs an op multiple times with the same inputs, the outputs will be the same each time\". The intent is that a user can run training or inference multiple times and get the same weights/outputs each time.\r\n\r\nI agree having tons of flags is bad, but TF should not use `TF_DETERMINISTIC_OPS` to make memory consumption more deterministic, as the purpose of the environmental variable (and upcoming determinism API) is to only make op outputs deterministic.", "Thanks for the CC; I agree with @reedwm. It seems that this should be a different flag, @nouiz. I am happy to chat with you one-on-one about this, if you like.", "I'm fine with that. What about `TF_DETERMINISTIC_ALLOCATOR`? Other idea?\r\n\r\nRight now it would make the allocator deterministic, but as the node execution order isn't deterministic, then it could still cause non-deterministic allocation as a side effect.", "I don't have any opinion on the environmental variable name. @sanjoy, @chsigg, any thoughts?", "`TF_DETERMINISTIC_ALLOCATOR` SGTM\r\n\r\nCC @atondwal "]}, {"number": 46550, "title": "Allow environment variables to execution of flatc command", "body": "Allow setting of LD_LIBRARY_PATH to reach execution environment of flatc so it can load correct version of libstdc++ and so avoid build failure when built with non-system gcc\r\nFixes #46549 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46550) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Isn't it proper to apply the change to the r2.4 branch as well?", "For those suffering from the flatc issue with other branches, a manual fix of the following change works.\r\n[View change](https://github.com/tensorflow/tensorflow/pull/46550/files/60aea05061a01b18ff02582341ab374abf253437)", "Please open a cherrypick PR and tag me."]}, {"number": 46549, "title": "Build failure when using non-system gcc with later glibc", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: HEAD\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nBuild using gcc 10.2.0 compiled on system with a later version of glibc than installed on system.\r\nThe tool flatc which is compiled during the build links with the glibc from gcc 10.2.0.\r\nWhen flatc is executed later in the build, the environment is cleared out and it is not possible to set LD_LIBRARY_PATH so that it would load the correct version of glibc.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nSystem installed libstdc++.so.6 is 3.4.25\r\ngcc 10.2.0 built with glibc 2.28 and libstdc++.so.6 3.4.28 installed to /usr/local\r\nBuild command line:\r\n$ TF_MKL_ROOT=~builder/1/built_tarballs/onednn/ bazel build --config=noaws --config=nogcp --config=nonccl --config=mkl_aarch64 --action_env=TF_MKL_ROOT=/home/builder/1/built_tarballs/onednn/ --action_env=LD_LIBRARY_PATH=/usr/local/lib64 --host_action_env=LD_LIBRARY_PATH=/usr/local/lib64 //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nBuild fails with the below messages:\r\nERROR: /home/debian/src/tensorflow_1dnn-git/tensorflow/lite/python/BUILD:11:22: Generating flatbuffer files for <source file tensorflow/lite/schema/schema.fbs>: failed (Exit 1): flatc failed: error executing command \r\n  (cd /home/debian/.cache/bazel/_bazel_debian/0988af24c3a1bb0051e8f606aa777b9e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/flatbuffers/flatc --python -o bazel-out/aarch64-opt/bin/tensorflow/lite/python/schema_py_srcs_no_include_all -I ./ -I bazel-out/aarch64-opt/bin -I bazel-out/aarch64-opt/bin --no-includes --no-union-value-namespacing --gen-object-api tensorflow/lite/schema/schema.fbs)\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/host/bin/external/flatbuffers/flatc: /lib/aarch64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by bazel-out/host/bin/external/flatbuffers/flatc)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46549\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46549\">No</a>\n", "From the following page, the issue could be solved in my case.\r\n[https://github.com/tensorflow/tensorflow/pull/46550](https://github.com/tensorflow/tensorflow/pull/46550)"]}, {"number": 46548, "title": "Compared to TF1.15, do TF2 have some speedup  in CPU inference?", "body": "I find this is not a good Stackoverflow question.\r\n\r\nSo I post it here. Thank you for your time.", "comments": ["@guotong1988 \r\n\r\nCan you please share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.\r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 46547, "title": " An error occurs when inferring by loaded pretrained model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Google colab\r\n\r\n**Describe the current behavior**\r\n- Error occurs when inferring from loaded model (before saving, it is successfully inferred from the same input)\r\n- same happens even if the input signature is redefined when saving the model \r\n\r\n**Describe the expected behavior**\r\n- Must be inferable from the loaded model\r\n\r\n**Standalone code to reproduce the issue**\r\n- here is code : https://colab.research.google.com/drive/1CJyLvind12Df-WFst4UTPMk0e5C996Tq?usp=sharing\r\n- Links to the data are also in colab \r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n- here  is error message\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-14-13d7f1beda6b> in <module>()\r\n      1 loaded_bert_model = tf.saved_model.load(DATA_OUT_PATH+'/bert/saved')\r\n      2 loaded_electra_model = tf.saved_model.load(DATA_OUT_PATH+\"/electra/saved\")\r\n----> 3 logits = loaded_bert_model(input_encodings_bert)['logits']\r\n      4 print(\"bert_saved\",logits)\r\n      5 logits = loaded_electra_model(input_encodings_electra)['logits']\r\n\r\n9 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py in restored_function_body(*args, **kwargs)\r\n    271         .format(_pretty_format_positional(args), kwargs,\r\n    272                 len(saved_function.concrete_functions),\r\n--> 273                 \"\\n\\n\".join(signature_descriptions)))\r\n    274 \r\n    275   concrete_function_objects = []\r\n\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (9 total):\r\n    * {'input_ids': <tf.Tensor 'input_ids_1:0' shape=(1, 14) dtype=int32>, 'attention_mask': <tf.Tensor 'input_ids:0' shape=(1, 14) dtype=int32>}\r\n    * None\r\n    * None\r\n    * None\r\n    * None\r\n    * None\r\n    * None\r\n    * None\r\n    * False\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 2 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (9 total):\r\n    * {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}\r\n    * None\r\n    * None\r\n    * None\r\n    * None\r\n    * None\r\n    * None\r\n    * None\r\n    * True\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (9 total):\r\n    * {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}\r\n    * None\r\n    * None\r\n    * None\r\n    * None\r\n    * None\r\n    * None\r\n    * None\r\n    * False\r\n  Keyword arguments: {}\r\n```\r\n", "comments": ["@karrdy89,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/37973#issuecomment-605448707) from issue #37973 with a similar error and let us know if it helps. Thanks!", "> @karrdy89,\r\n> Please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/37973#issuecomment-605448707) from issue #37973 with a similar error and let us know if it helps. Thanks!\r\n\r\n\r\nThank you for your answer. The link has already been checked. I think it's a good alternative, but I want to save the model in pb format for serving.", "In my case redefine the input signature can solve the problem.\r\nIf you just save transformers distilbert model, the form of the _default_save_signature function will look like this.\r\n```\r\n input_signature: ({'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids')})\r\n```\r\nTherefore, if you want to pass a total of three inputs(input_ids, attention_mask, token_type_ids), it is necessary to design the model layer according to the input and redefine the input signature through the __call__ method with @tf.function decorator.\r\n\r\nHowever, it seems there is a problem that an error occurs in the loaded model when the same value as the input before saving.\r\n\r\n", "@karrdy89,\r\n\r\nCan you try updating TF to latest stable version i.e 2.7.0 and let us know if the issue still persists? Also you can take a look at similar issue on this [link1](https://github.com/tensorflow/tensorflow/issues/37973),[link2](https://stackoverflow.com/questions/58575586/could-not-find-matching-function-to-call-loaded-from-the-savedmodel)  Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46547\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46547\">No</a>\n"]}, {"number": 46546, "title": "TensorFlow Lite  support multi scale input?", "body": "Did TensorFlow Lite  support multi scale input?\r\nIn same case,i need dynamic input shapes;\r\n\r\nfor examples:\r\nthe tensorflow lite mode input shape is  1x3x360x640\r\n\r\nbut I can not input 1x3x720x1080 .\r\n(without fixed size, FC,all ops are conv).\r\ncaffe ,ncnn,TNN model, i can change the protext  to support multi scale input.\r\n\r\n", "comments": ["TensorFlowLite supports dynamic dimension features like TensorFlow. However, the orginal model should be trained with dynamic dimensions to fit your needs.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46546\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46546\">No</a>\n"]}, {"number": 46545, "title": "[ROCm] Enabling/Disabling some unit tests (via no_rocm tag) on the ROCm platform", "body": "all commits either add or remove the `no_rocm` tag from unit-tests to disable / enable them...nothing else.\r\n\r\n\r\n-----------------------------------------------\r\n\r\n\r\n/cc @cheshire @chsigg @nvining-work \r\n", "comments": ["The failure in `Linux GPU` CI job does not seem to be related to this PR. \r\n\r\nThere is only 1 failure in that CI job in the unit test `//tensorflow/compiler/tests:image_ops_test_gpu`, and this PR does not touch that test. ", "re-based PR on tip in hopes of getting rid of the error in `Linux GPU` Ci job", "still getting the error in `Linux GPU` CI job, and this time I am unable to access the log file.\r\n\r\n@gbaned, can you help? thanks", "able to access the log file (for the failed CI job) now....it seems that the same unit test is failing again `\r\n//tensorflow/compiler/tests:image_ops_test_gpu`.  Fairly certain that the failure is not related to the changes in this PR.  \r\n\r\nWhat to do next?", "> able to access the log file (for the failed CI job) now....it seems that the same unit test is failing again ` //tensorflow/compiler/tests:image_ops_test_gpu`. Fairly certain that the failure is not related to the changes in this PR.\r\n> \r\n> What to do next?\r\n\r\nIndeed, this failure is unrelated to this change. I will get it merged."]}, {"number": 46544, "title": "Fix an issue for multi CUPTI sessions", "body": "When using an external profiler tools (like NVIDIA Nsight Systems), some collected components might be missing in the profiling results if the TF profiler is used in the python script. This PR fixes this issue by explicitly checking if the CUPTI tracer is successfully enabled or not. If not, we return an error msg instead of a OK status.\r\n\r\nFYI. @nluehr ", "comments": ["It is insufficient to check that CUPTI is already enabled, as this means the TensorFlow Profiler is active and incurring other overheads (e.g., TraceMe annotations). The correct thing to do when using another profiling tool is to fully disable the TF Profiler. Typically TF Profiler is off by default. One exception is when using the Keras TensorBoard callback [1]: the profile_batch argument should be set to 0 but the default is 2. Unfortunately changing the default would break backwards compatibility. One alternative is to ensure that models using the callback offer a flag as in [2]. Note that TF itself is a library that should not define command-line flags. An acceptable alternative would be an environment variable that disables the callback and is set by the script that enables Nsight.\r\n\r\n[1] https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\r\n[2] https://github.com/tensorflow/models/blob/2986bcafb9eaa8fed4d78f17a04c4c5afc8f6691/official/vision/image_classification/resnet/common.py", "Shouldn't the user disable device tracing in TF profiler before running the application under another profiler (Nsight Systems)? Is this not possible for some cases? The expectation is that if the user should not request to start the TensorFlow GPU profiler programmatically when using an external GPU profiler.", "An alternative solution is provided in:\r\nhttps://github.com/tensorflow/tensorflow/commit/6d7e583c890e852319629ee00bf8ca87c233475b\r\n\r\nYou can set the environment variable TF_DISABLE_PROFILING to 1 to disable the profiler. It should prevent CUPTI (and all other profiler sources) from being activated by the internal TF profiler when an external profiler is active.", "Hi @jbaiocchi , thanks for adding the env variable. I just tested it but it seems it works differently than I thought.\r\n\r\nSay, now I have a script using the TF profiler. If I set TF_DISABLE_PROFILING=1 and run it, the execution will error out with `tensorflow.python.framework.errors_impl.UnavailableError: Another profiler session is active.` even when I haven't use other profilers yet.\r\n\r\nBut I thought what we wanted is by setting the TF_DISABLE_PROFILING, the internal TF profiler will be skipped silently (or maybe with some warning). Then, the users can use any external profilers.\r\n\r\nFor example, the script demo.py uses TF profiler. And this is what we wanted to achieve:\r\n1. `python demo.py` # run with internal profiler\r\n2. `TF_DISABLE_PROFILING=1 python demo.py` # still run but without any profiling (Unfortunately it will error out now) \r\n3. `TF_DISABLE_PROFILING=1 nsys profile python demo.py` # run with external nsys profiler\r\n4. `nsys profile python demo.py` # undefined for now.", "Setting TF_DISABLE_PROFILING=1 is still the way to go.\r\nThe problem (and root cause of this bug) is that the Keras TensorBoard callback enables profiling by default.\r\nThe profiler API raises exceptions that the callback was not catching. I have a fix under review.", "Please verify if the following commit is sufficient or there's still some issue:\r\nhttps://github.com/tensorflow/tensorflow/commit/53ff2339b3c05a6efbe5bdbdd6c40b8973fb8e17", "Just tried with the nightly, but it seems the error is still on.\r\nBelow is the script that I used:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.profiler.experimental.start('logdir_path')\r\n\r\ninputs = tf.ones((10, 100, 100, 4))\r\nconv = tf.keras.layers.Conv2D(\r\n    10, (3, 3), strides=(1, 1), padding='valid',)\r\noutputs = conv(inputs)\r\n_ = outputs.numpy()\r\n\r\ntf.profiler.experimental.stop()\r\n\r\n```\r\nThen, I ran it with:\r\n```\r\nroot@96002906402c:/home/kernel_tests/cupti_tracer# TF_DISABLE_PROFILING=1 python -u cupti_tracer.py\r\n2021-02-27 00:46:22.652281: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-27 00:46:23.534106: W tensorflow/core/profiler/lib/profiler_lock.cc:39] TensorFlow Profiler is permanently disabled by env var TF_DISABLE_PROFILING.\r\n2021-02-27 00:46:23.534211: I tensorflow/core/profiler/lib/profiler_session.cc:158] Profiler session tear down.\r\nTraceback (most recent call last):\r\n  File \"cupti_tracer.py\", line 3, in <module>\r\n    tf.profiler.experimental.start('logdir_path')\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/profiler_v2.py\", line 122, in start\r\n    _profiler.start(logdir, opts)\r\ntensorflow.python.framework.errors_impl.UnavailableError: Another profiler session is active.\r\n\r\n```", "The behavior you are seeing is intended. tf.profiler.experimental.start and tf.profiler.experimental.stop are documented to raise exceptions. Setting TF_DISABLE_PROFILING=1 makes then raise. If this was python user code, it would be the responsibility of the user to catch exceptions. The commit addresses the bug in the TensorboardCallback:  it was making calls to the profiler without catching exceptions.", "The above code is not from users but myself to quick test the cupti profiler.\r\n\r\n> The commit addresses the bug in the TensorboardCallback: it was making calls to the profiler without catching exceptions.\r\n\r\nSo, do you mean if the script uses the Tensorboard, the profiler will be silently skipped when we set TF_DISABLE_PROFILING?\r\n\r\nFor example, I just tried the following tensorboard_tf2.py in the nightly build:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntensorboard_dir = 'logdir_path'\r\ntraining_hooks = []\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_dir)\r\ntraining_hooks.append(tensorboard_callback)\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(8))\r\nmodel.add(tf.keras.layers.Dense(1))\r\nmodel.compile(optimizer='sgd', loss='mse')\r\n\r\n\r\nx = tf.ones((100, 4))\r\ny = tf.ones((100,))\r\n\r\n# This builds the model for the first time:\r\nmodel.fit(x, y, batch_size=32, epochs=10, callbacks=training_hooks)\r\n```\r\nHowever, I still got the same error:\r\n```\r\nroot@b6c450ec0f0c:/home/kernel_tests/cupti_tracer# TF_DISABLE_PROFILING=1 python -u tensorboard_tf2.py\r\n2021-03-01 18:44:06.635002: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-01 18:44:07.528315: W tensorflow/core/profiler/lib/profiler_lock.cc:39] TensorFlow Profiler is permanently disabled by env var TF_DISABLE_PROFILING.\r\n2021-03-01 18:44:07.528436: I tensorflow/core/profiler/lib/profiler_session.cc:158] Profiler session tear down.\r\nTraceback (most recent call last):\r\n  File \"tensorboard_tf2.py\", line 5, in <module>\r\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_dir)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 2139, in __init__\r\n    self._init_profile_batch(profile_batch)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 2348, in _init_profile_batch\r\n    self._start_profiler(logdir='')\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 2527, in _start_profiler\r\n    profiler.start(logdir=logdir)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/profiler_v2.py\", line 122, in start\r\n    _profiler.start(logdir, opts)\r\ntensorflow.python.framework.errors_impl.UnavailableError: Another profiler session is active.\r\n```", "We had a mismatch between the python exception and the C++ error code. This is now fixed:\r\nhttps://github.com/tensorflow/tensorflow/commit/f07914d9d7e6eaf97b913fed985303da93544a7d\r\nPlease give it a try. Thanks!", "Thx. Just verified it with the nightly.\r\n\r\nOne more thing to confirm: so it seems we need to cherry-pick these three commits for this `TF_DISABLE_PROFILING` feature, right? @jbaiocchi \r\nhttps://github.com/tensorflow/tensorflow/commit/6d7e583c890e852319629ee00bf8ca87c233475b\r\nhttps://github.com/tensorflow/tensorflow/commit/53ff2339b3c05a6efbe5bdbdd6c40b8973fb8e17\r\nhttps://github.com/tensorflow/tensorflow/commit/f07914d9d7e6eaf97b913fed985303da93544a7d\r\n ", "Yes, those are the 3 required commits for TF_DISABLE_PROFILING.", "Thx. Based on https://github.com/tensorflow/tensorflow/pull/46544#issuecomment-790067558, close this PR."]}, {"number": 46543, "title": "Upgrade to CUDNN RNN v8 APIs", "body": "This PR updates the code to use cudnnRNNForward/cudnnRNNBackwardxxx_v8 when the CUDNN Version >= 8.0 and CudnnRNNV3 operation is used.\r\n\r\n\r\n\r\nfyi. @nluehr ", "comments": ["@sanjoy I have updated the code by placing the host seq_lengths in `CudnnRnnSequenceTensorDescriptor` to make sure it is still valid when the H2D copy happens. I also updated the comments. PTAL.", "After a second thought, I think we might be able to remove the dependence on the `BlockHostUtilDone()` in the current PR. Since the `sequence_lengths` is passed in via `.HostMemory(\"sequence_lengths\")`, it could be guaranteed it will stay long enough for the H2D copy.\r\n\r\nThe current implement (before the PR) is to copy it into input_descriptor (which is a local pageable location). And this PR's strategy is to fetche it from the input_descriptor and copy it to device, which requires the sync op.\r\n\r\nI think we can do something different by passing the sequence_lengths to the compute method so that we can call H2D copy over it directly, which will require no sync anymore.\r\n\r\nI will push a commit later today.", "@sanjoy I have refactored the code. PTAL."]}, {"number": 46542, "title": "[CherryPick:r2.4] Pass cc_opt_flags to host_copt.", "body": "", "comments": []}, {"number": 46541, "title": "Go: Deallocate large TF_TString on Tensor finalization", "body": "Since version 2.4 Tensorflow [moved](https://github.com/tensorflow/tensorflow/commit/b9b042cfd91908456055564ffa2ce86bfa44bda9) to the [new way](https://github.com/tensorflow/community/pull/91) to store string tensors.\r\n\r\nEarlier `std::string` was used an array of strings was encoded as an array of 8-byte offsets followed by string data. With the current design of the `tensorflow::tstring` creating tensor with `NewTensor` function could allocate string in two ways ([description of the tstring types](https://github.com/tensorflow/tensorflow/blob/a2766ce758be7d31e1f4aab93ef2328d4a56b07a/tensorflow/core/platform/ctstring.h#L53)):\r\n- `TF_TSTR_SMALL` -  the contents of strings less than 22-bytes are stored in the TF_TString struct\r\n- `TF_TSTR_LARGE` - Heap allocated string\r\n\r\nThe second case assumes that, in addition to [TF_DeleteTensor](https://github.com/tensorflow/tensorflow/blob/71c1ae43e2504da27ab60d1848b5fc0deec3fb31/tensorflow/c/tf_tensor.h#L111), the string should be deallocated with [TF_TString_Dealloc](https://github.com/tensorflow/tensorflow/blob/3fcba829d82ea34245db62afd45b9523e8db02d4/tensorflow/core/platform/ctstring.h#L28). This situation [wasn't handled](https://github.com/tensorflow/tensorflow/commit/24f835217fd27f21141ff0254a2b93ea3cfd7b6c#diff-025d2fb7b19fc74ebf16c55f0e3bd73d8c0530bddb63f9a9b1e322303515bf98) in the Go bindings.", "comments": []}, {"number": 46540, "title": "tf.nn.convolution crashes Floating-point exception when filters has 0 in shape", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.6.9\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.nn.convolution` crashes (Floating-point exception) when `filters` has 0 in shape\r\n\r\nerror message:\r\n~~~\r\n[1]    10129 floating point exception (core dumped)  python\r\n~~~\r\n**Describe the expected behavior**\r\nexpect no crash\r\n**Standalone code to reproduce the issue**\r\n~~~\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.nn.convolution(input=np.ones((1,1,1,1)), filters=np.ones((1,0,1,1)))\r\n~~~\r\n", "comments": ["I have tried in colab with TF version 2.1, 2.4 and nightly version(`2.5.0-dev20210119`) and noticed that session is being crashed. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/86610191c4b189c94d36a872672d034a/untitled624.ipynb). Thanks!", "I tested with TF 2.3.1, 2.4.0 on macOS, Python 3.7 with PyCharm IDE, I didn't see the crash. The code executed successfully.", "It is not only the case when the filter has a 0 shape value cause this works fine:\r\n\r\n```\r\ntf.nn.convolution(input=tf.ones((1,0,1,1)), filters=tf.ones((1,0,1,1)))\r\n```", "@ymodak \r\nI tested on the nightly version (`2.5.0-dev20210114`) with command line and it still crashes.\r\n", "I wonder if this issue only fails on colab.\r\nOn macOS pycharm it gives:\r\n```python\r\ntf.Tensor(\r\n[[[[0.]\r\n   [0.]]]], shape=(1, 1, 2, 1), dtype=float64)\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46540\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46540\">No</a>\n"]}, {"number": 46539, "title": "tf.keras.applications.densenet.preprocess_input does not work for data_format=\"channels_first\" for symbolic tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- TensorFlow installed from (source or binary): colab\r\n- TensorFlow version (use command below): 2.4.0, v2.4.0-0-g582c8d236cb\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\nRunning `tf.keras.applications.densenet.preprocess_input` on a `data_format=\"channels_first\"` symbolic tensor raises an Exception. This is caused by the input transposition not being applied if the `mode` parameter to [_preprocess_symbolic_input](https://github.com/tensorflow/tensorflow/blob/7a49c87f9f56a7fc169669cfe97728859798967c/tensorflow/python/keras/applications/imagenet_utils.py#L242) is set to \"torch\" (as is the case for densenet preprocessing).\r\n\r\nSee the relevant lines here:\r\nhttps://github.com/tensorflow/tensorflow/blob/7a49c87f9f56a7fc169669cfe97728859798967c/tensorflow/python/keras/applications/imagenet_utils.py#L262-L281\r\n\r\nThis is not caught by the unit tests as only the default `mode=\"caffe\"` is tested.\r\n\r\nThe equivalent numpy function `_preprocess_numpy_input` handles this correctly (treating the input differently depending on `data_format` for all modes\r\n\r\n**Describe the expected behavior**\r\nPreprocessing should work for `data_format=\"channels_first\"`\r\n\r\n**Standalone code to reproduce the issue**\r\nColab notebook with minimum example:\r\nhttps://colab.research.google.com/drive/1THrNYTAAzPxw9135h-sFt-LxMz_7SEeQ?usp=sharing\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/88605d7199898f076e057b8ed35457c9/46539.ipynb#scrollTo=NRQ2tF_zk5qY). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46539\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46539\">No</a>\n"]}, {"number": 46537, "title": "libtensorflow-gpu-windows-x86_64-2.3.1 does not support GPUs with compute capability 5.0", "body": "TensorFlow 2.3.1 crashes on my GPU with compute capability (CC) 5.0 with the error \"no kernel image is available for execution on the device\". I used [cuobjdump](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#cuobjdump) to inspect tensorflow.dll to see what binary and PTX code was included. I found binary code for CC 3.5, 3.7, 5.2, 6.0, 6.1, and 7.0 as well as PTX code for CC 7.0.\r\n\r\nPTX code is forward-compatible, but 7.0 PTX cannot run on a 5.0 GPU. Binary code is forward-compatible but only for minor version updates. This means that neither the 3.7 nor the 5.2 binary can run on a 5.0 GPU. In fact, the current configuration means that TensorFlow 2.3.1 can run on any GPU with CC 3.5 and up *except* for CC 5.0.\r\n\r\nI suggest building for CC 5.0 instead of CC 5.2 so that TensorFlow 2.3.X can run on any GPU with CC 3.5 and up.", "comments": ["What version of TF are you using? TF 2.4 supports cc 5.0\r\nhttps://github.com/tensorflow/tensorflow/issues/45904#issuecomment-750485891", "This is with 2.3.1 and I updated the description above to make that clear. I also confirmed that 2.4.0 does support CC 5.0.\r\n\r\nWill there be any more 2.3.X releases? If so, this is worth fixing. ", "Was this a regression from 2.3.0?\r\n\r\nWe don't have a plan for more 2.3.x releases at the moment but if there are security vulnerabilities in a year since the release we will have to do a patch.\r\n\r\nIf there is a patch, let's open a PR against the `r2.3` branch and assign me to it please. This way we'll know to include it in the 23 release when it comes.", "@mihaimaruseac  I just checked and this is not a regression from 2.3.0. Both 2.3.0 and 2.3.1 support the same set of of GPU compute capabilities. (They both exclude 5.0.)", "In this case, we won't be able to release a patch release changing the compute capabilities. We try to limit the delta between normal release and patches.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46537\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46537\">No</a>\n"]}]