[{"number": 26722, "title": "[ROCm] Adding rocmprim as a third_party library, minor update to eigen patch", "body": "This PR does two things\r\n\r\n1. Add rocmprim as a third_party library, add it to the bazel target(s) which will eventually need it (in PRs to TF operators to be filed soon)\r\n\r\n2. Make a minor update to the eigen patch.\r\nThe updates are to fix a couple of ROCm/HIP specific errors in the Eigen code. We have already begun the process of upstreaming these changes to Eigen. So this patch update is expected to be temporary in nature.\r\n\r\nThis PR is pre-requisite to enabling ROCm support for some of TF Operators. Please exepdite it as much as possible.\r\n\r\nThanks\r\n\r\ndeven\r\n\r\n-------------------------------------\r\n\r\n@tatianashp , @whchung : just FYI", "comments": ["rebased to remove merge conflicts", "Adding @rmlarsen for the changes in eigen patch.\r\nPlease do not merge without his approval.", "* rebased to remove merge conflict\r\n* also added a commit to fix a compile error that got recently introduced in the `--config=rocm` build\r\n\r\n----\r\n\r\n@rmlarsen \r\n\r\nFor now, I am leaving in the commit for the eigen patch. We can drop that commit from this PR, once the corresponding Eigen PR has been merged + integrated into TF. \r\n\r\nThere are a few other PRs that are built on top of this PR, so dropping the eigen commit from this PR (before the TF integration of the Eigen PR) would break the compile for this + all those PRs.\r\n\r\n", "@rmlarsen \r\n\r\nThank you for getting the Eigen PR merged. I appreciate your help in expediting that process.\r\n\r\nPlease update this PR once the TF eigen pointer is updated as well. At that point I will rebase this PR and drop the commit for the eigen patch file update.\r\n\r\nThanks again\r\n\r\ndeven\r\n\r\n", "@rmlarsen \r\n\r\ngentle ping.  would you happen to have any insight on when the TF eigen pointer will move foward to include the eigen PR that was merged?\r\n\r\nthanks", "@rmlarsen , thank you for updating the TF eigen pointer.\r\n\r\nI have re-based this PR to drop the appends to the eigen patch file. \r\nThe only change that remains is the minor change \r\nfrom :  `#if defined(EIGEN_CUDA_ARCH)`\r\nto : `#if defined(EIGEN_CUDA_ARCH) || defined(EIGEN_HIP_DEVICE_COMPILE)`\r\nMy understanding is that you will upstream this change to Eigen, (if needed). Let me know if that is incorrect.\r\n\r\nPlease review and approve this PR. It is a pre-req for quite a few other PRs :)\r\n\r\nthanks\r\n\r\n", "@gunan @rmlarsen \r\n\r\ngentle ping\r\n\r\nplease review and merge the PR. This PR is a gating item for quite a few other ROCm PRs.\r\n\r\nthanks", "@gunan @rmlarsen\r\n\r\ngentle ping", "This change was approved, but is failing internally because third_party dependency changes need special handling for conversion. I'm working on it.", "@deven-amd I'm running into trouble merging this due to differences in internal behavior for `is_rocm/cuda` and `is_rocm/cuda_configured`. Can you explain why you needed to make changes with the `cuda` calls? Favoring `is_rocm` where `is_cuda` is used (and similar for where `is_cuda_configured`) instead of changing it may help resolve these issues.", "@angersson \r\n\r\n@chsigg had a similar query in another PR...please see the discussion here (https://github.com/tensorflow/tensorflow/pull/26753#discussion_r267022934) for details.\r\n\r\nThe summary of it is that, the change from `if_cuda / if_rocm` to `if_cuda_is_configured / if_rocm_is_configured` is needed in scenarios where there is a common target being added by by both (`rocm` and `cuda`) to the dependency list. \r\n\r\nUsing `if_cuda / if_rocm` (which returns a select statement) in such scenarios results in a bazel error (duplicate depedency). \r\n\r\nSwitching to `if_cuda_is_configured / if_rocm_is_configured` (which returns the `true` arg or empty list) resolves the bazel error.\r\n\r\nHope this clears things up. ", "@chsigg Can you look at importing this, or find someone who can, please? Thanks!", "@chsigg   gentle ping\r\n\r\nhow close are we to getting this PR merged?\r\n\r\nthanks", "the CI failures seem unrelated to the changes in this PR....not sure what is going on", "This is a problem with a internal/external build setup mismatch. I'll try to merge it tomorrow, but probably I'll have to find someone who understands our build tooling better than me... might take few days.", "I've merged non-build related changes, could you please sync it with master, I'll try to find someone who can help with build-related changes.", "@ezhulenev , thanks for merging the non-build commits.\r\n\r\nI will rebase this branch onto master, and push it out later today. After the update, there will be only one commit left in this PR (build change for rocprim)", "@ezhulenev rebase done", "I am trying to merge this, but this is proving to be very very difficult.\r\nI had to revert all the changes from \"if_cuda\" to \"if_cuda_is_configured\"\r\nNext time, let's split such changes into two, where one only adds the new external dependency, and another where we start using it.", "@gunan thank you for getting this merge done\r\n\r\nwill keep your suggestions in mind, the next time we need to add a dependency.", "@gunan \r\n\r\n> I had to revert all the changes from \"if_cuda\" to \"if_cuda_is_configured\"\r\n\r\nThe change from `if_cuda` to `if_cuda_is_configured` was intentional and needed for the `--config=rocm` build to work. With that change reverted, the `--config=rocm` build is broken :(\r\n\r\nHow can I help you resolve this such that `--config=rocm` build is working again?\r\n\r\n@chsigg, just FYI to keep you in the loop\r\n\r\nthanks\r\n\r\ndeven\r\n\r\n----------------\r\n\r\nbuild errors I get when trying to do a `--config=rocm` build\r\n```\r\nERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:4029:1: Label '//tensorflow/core/kernels:reduction_ops' is duplicated in the 'deps' attribute of rule 'bias_op_gpu'\r\nERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:4029:1: Label '//tensorflow/core/kernels:reduction_ops' is duplicated in the 'deps' attribute of rule 'bias_op'\r\nERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:4029:1: Label '//tensorflow/core/kernels:reduction_ops' is duplicated in the 'deps' attribute of rule 'libtfkernel_bias_op.so'\r\nERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:4069:1: Label '//tensorflow/core/kernels:reduction_ops' is duplicated in the 'deps' attribute of rule 'softmax_op_gpu'\r\nERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:4069:1: Label '//tensorflow/core/kernels:reduction_ops' is duplicated in the 'deps' attribute of rule 'softmax_op'\r\nERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:4069:1: Label '//tensorflow/core/kernels:reduction_ops' is duplicated in the 'deps' attribute of rule 'libtfkernel_softmax_op.so'\r\nERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:4735:1: Label '//tensorflow/core/kernels:reduction_ops' is duplicated in the 'deps' attribute of rule 'sparse_xent_op_gpu'\r\nERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:4735:1: Label '//tensorflow/core/kernels:reduction_ops' is duplicated in the 'deps' attribute of rule 'sparse_xent_op'\r\nERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:4735:1: Label '//tensorflow/core/kernels:reduction_ops' is duplicated in the 'deps' attribute of rule 'libtfkernel_sparse_xent_op.so'\r\nERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:5227:1: Label '//tensorflow/core/kernels:reduction_ops' is duplicated in the 'deps' attribute of rule 'multinomial_op_gpu'\r\nERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:5227:1: Label '//tensorflow/core/kernels:reduction_ops' is duplicated in the 'deps' attribute of rule 'multinomial_op'\r\nERROR: /root/tensorflow/tensorflow/core/kernels/BUILD:5227:1: Label '//tensorflow/core/kernels:reduction_ops' is duplicated in the 'deps' attribute of rule 'libtfkernel_multinomial_op.so'\r\nERROR: /root/tensorflow/tensorflow/core/BUILD:1558:1: Target '//tensorflow/core/kernels:dataset_ops' contains an error and its package is in error and referenced by '//tensorflow/core:all_kernels'\r\nERROR: /root/tensorflow/tensorflow/core/BUILD:1558:1: Target '//tensorflow/core/kernels:list_kernels' contains an error and its package is in error and referenced by '//tensorflow/core:all_kernels'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Cannot compute config conditions\r\nINFO: Elapsed time: 13.208s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (366 packages loaded, 10066 targets configured)\r\n```"]}, {"number": 26721, "title": "Inconsistent encoding leads to AttributeError (reopen)", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 7.5\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\nTensorFlow installed from (source or binary): compiled from source\r\nTensorFlow version (use command below): r13.1\r\nPython version: 3.6.3\r\nBazel version (if compiling from source): 0.22\r\nGCC/Compiler version (if compiling from source): 4.8.5\r\nCUDA/cuDNN version: 10.0\r\nGPU model and memory: P5000/16G\r\nDeepMind open-sourced the implementation of IMPALA: https://github.com/deepmind/scalable_agent\r\n\r\nFor parallelism, they wrap a mechanism which is class-based in doing so, the file is: https://github.com/deepmind/scalable_agent/blob/master/py_process.py\r\n\r\nThere is a code snippet at the beginning of the file:\r\n```\r\n\r\n  class Zeros(object):\r\n    def __init__(self, dim0):\r\n      self._dim0 = dim0\r\n    def compute(self, dim1):\r\n      return np.zeros([self._dim0, dim1], dtype=np.int32)\r\n    @staticmethod\r\n    def _tensor_specs(method_name, kwargs, constructor_kwargs):\r\n      dim0 = constructor_kwargs['dim0']\r\n      dim1 = kwargs['dim1']\r\n      if method_name == 'compute':\r\n        return tf.contrib.framework.TensorSpec([dim0, dim1], tf.int32)\r\n  with tf.Graph().as_default():\r\n    p = py_process.PyProcess(Zeros, 1)\r\n    result = p.proxy.compute(2)\r\n    with tf.train.SingularMonitoredSession(\r\n        hooks=[py_process.PyProcessHook()]) as session:\r\n      print(session.run(result))  # Prints [[0, 0]].\r\n\r\n```\r\nhowever, when I tried to run it, I got the following error:\r\n\r\n```\r\n2019-03-01 17:37:16.260732: W tensorflow/core/framework/op_kernel.cc:1389] Unknown: AttributeError: 'Zeros' object has no attribute 'b'compute''\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/yuming/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/data/yuming/eeg-dpg/py_process.py\", line 89, in py_call\r\n    raise result\r\n\r\nAttributeError: 'Zeros' object has no attribute 'b'compute''\r\n\r\n```\r\nI suspect maybe there is a mismatch between encoding (python interpreter and source code?), but not for sure, since I have no problem in running the code if I use Python 2.7.\r\n\r\nCould anyone please take some effort on looking into it?\r\n\r\n(The first people look into it just casually and unprofessionally close it, but I would think it is a bug. If you do think it is not a bug, you can close it since I got a workaround (back to python 2.7). I already posted it on stackoverflow)", "comments": ["The link to stack overflow:\r\nhttps://stackoverflow.com/questions/54939480/attribute-error-when-running-a-code-snippet-when-using-tensorflow", "I think this is a problem with pyproxy. It's saying that in the line where you have\r\n\r\n```\r\n    result = p.proxy.compute(2)\r\n```\r\n\r\n`p.proxy` is an object which doesn't have a compute method. So this does not look like a tf issue to me.", "I think the root cause is not \"p.proxy is an object which doesn't have a compute method\", but potentially some changes in Python 3. Actually the attribute of \"compute\" can be either in unicode form, or in so-called byte stream form. That means \"compute\" and b\"compute\" are different things.\r\n\r\nAnyway, let's see whether engineers at DeepMind have the time to address this or not."]}, {"number": 26720, "title": "Fix compilation problems when using MPI & CUDA", "body": "This PR fixes two compilation errors:\r\n- Error in `contrib.mpi` caused by a changed function definition as noted in issue #25638 \r\n- Error that surfaces in the `contrib.mpi_collectives` contribution and likely got introduced with [this ](8a653bd99c69602675fc9926f201985e65c980be) commit \r\n\r\n", "comments": []}, {"number": 26719, "title": "how to use the saved model after trained to detect new pictures", "body": "Hi all,\r\nthis might be repeated question.\r\nI trained classifier on cats, dogs. and saved the model. Now I want to use the saved trained model to detect new pictures. How to do this? Is there a code for this?\r\n\r\nThanx", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26718, "title": "[ROCm] Enable ROCm support for \"AddN\" op", "body": "This PR enables ROCm support for the \"AddN\" op.", "comments": []}, {"number": 26717, "title": "[ROCm] Enable ROCm support for \"AdjustContrast\" op", "body": "This PR enables ROCm support for the \"AdjustContrast\" op.", "comments": []}, {"number": 26716, "title": "cannot use a placeholder dependent tensor as a initializer in get_variable", "body": "How does one go about creating variables with placeholders dependent tensors as initializers? The following graph breaks down with:\r\n\r\n\r\n\r\n    InvalidArgumentError: You must feed a value for placeholder 'Placeholder_1' with dtype float\r\n    \t [[node Placeholder_1 (defined at <ipython-input-10-b8d54264dc85>:3)  = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nMy code:\r\n\r\n    tf.reset_default_graph()\r\n    a = tf.placeholder(dtype=tf.float32,shape=())\r\n    d = tf.placeholder(dtype=tf.float32,shape=())\r\n    b = tf.get_variable(name='b',initializer=d)\r\n    c=a+d\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        print(sess.run(c, feed_dict={a:5.,d:10.}))\r\n\r\n\r\nThe documentation on initializers in tensorflow says:\r\n>Initializer for the variable if one is created. Can either be an initializer object or a Tensor. If it's a Tensor, its shape must be known unless validate_shape is False.\r\n\r\nHowever if i comment out the line where i create b the code seems to run. My fetch is not even dependent upon b.\r\n\r\nHow do i go about creating variables that initialize according to some placeholder?", "comments": ["It doesn't need \"sess.run(tf.global_variables_initializer())\" because you has already initialize b as d, just run \"print(sess.run(c, feed_dict={a:5.,d:10.}))\" is ok.", "```\r\ntf.reset_default_graph()\r\na = tf.placeholder(dtype=tf.float32,shape=())\r\nd = tf.placeholder(dtype=tf.float32,shape=())\r\nb = tf.get_variable(name='b',initializer=d)\r\nc=a+d\r\nwith tf.Session() as sess:\r\n    # sess.run(tf.global_variables_initializer())\r\n    print(sess.run(c, feed_dict={a:5.,d:10.}))", "What if i want to run `tf.global_variables_initializer()`? \r\n\r\n```\r\na = tf.placeholder(dtype=tf.float32,shape=())\r\nb = tf.get_variable(name='b',initializer=a)\r\ne = tf.get_variable(name='e',initializer=tf.constant(2.))\r\nc=a+b+e\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run(c, feed_dict={a:5.}))\r\n```", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 26715, "title": "Docker latest images are using tf 2.0.0a0", "body": "\r\n**System information**\r\n- Have I written custom code: **No**\r\n- OS Platform and Distribution: **Linux Ubuntu 16.04**\r\n- TensorFlow installed from: **binary**\r\n- TensorFlow version (use command below): **2.0.0a0** (script is however broken, see below)\r\n- Python version: 2.7.12 / 3.5.2\r\n- CUDA/cuDNN version: **any**\r\n- GPU model and memory: **any**\r\n\r\n\r\n> You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n> You can also obtain the TensorFlow version with\r\n> python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n> \r\nWhile this is not the issue itself, its worth mentioning that both are broken \r\n```\r\nTraceback (most recent call last):\r\n  File \"/tmp/check_tf.py\", line 2, in <module>\r\n    print(\"tf.VERSION = %s\" % tf.VERSION)\r\nAttributeError: 'module' object has no attribute 'VERSION'\r\n```\r\n\r\n**Describe the current behavior**\r\n**The Docker `latest` family contains tensorflow version 2.0.0a0.** \r\n\r\n**Describe the expected behavior**\r\nThe Docker `latest` family should contain  tensorflow version 1.13.*. \r\n\r\nAccording to the docs (and docker convention), the `latest` docker images should contain the latest  **stable** version, as in 1.13 while the nightly images should contain the unstable images, such us alphas, betas and RCs\r\n**Code to reproduce the issue**\r\n`docker run -it --rm tensorflow/tensorflow  pip list 2>&1 | grep tensorflow`\r\n`docker run -it --rm tensorflow/tensorflow:latest-py3  pip list 2>&1 | grep tensorflow`\r\n(the issue persists in docker versions as well)\r\n\r\n**Other info / logs**\r\n<img width=\"311\" alt=\"Screen Shot 2019-03-14 at 22 56 17\" src=\"https://user-images.githubusercontent.com/1699509/54392157-8f5d7a80-46af-11e9-8b00-9e4e1979db6a.png\">\r\n", "comments": ["Also: \r\ndocker run --rm -it tensorflow/tensorflow:1.12.0-gpu-py3 \r\nis actually jupyter version", "Thanks for reporting this. It looks like it's probably a mistake in the way our CI handles PyPi pre-releases. \"Latest\" usually refers to pre-releases, but since 2.0 is more of an alpha, the CI gets confused with PyPI's limited classification system.\r\n\r\nIn the meantime, if you run into this, please use the numbered tag directly instead of \"latest\", which will be accurately tagged.", "With regards to the 1.12.0 images, only the images newer than 1.13 use the newer Dockerfiles, and we haven't rebuilt them to avoid breaking anyone who depends on the old structure. There's a note about this at the top of the Docker Hub repository.", "I've fixed the underlying issue and re-tagged the `latest` images to be based on the `1.13.1` images. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26715\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26715\">No</a>\n"]}, {"number": 26714, "title": "Error when using the class_weight parameter in the fit function in tensorflow.python.keras", "body": "- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6\r\n\r\nI wanted to test my network on a small set of records with two imbalanced classes (0 and 1). I was going to use the _class_weight_ parameter in the _fit_ function to improve the balance, but unfortunately there were problems probably associated with tensor support. Without the _class_weight_ parameter, the fit function works correctly. I also tried to use tensorflow.python.keras.layers.Input as an input to the Xception model, but with the same effect.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import Dense, Dropout\r\nfrom tensorflow.python.keras.applications.xception import Xception, preprocess_input\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.optimizers import Adam\r\n\r\n# parsing images and labels from TFRecords\r\ndef parse_function(proto):\r\n    example = {'image_raw': tf.FixedLenFeature([], tf.string), 'label': tf.FixedLenFeature([], tf.int64)}\r\n    parsed_example = tf.parse_single_example(proto, example)\r\n    image = tf.decode_raw(parsed_example['image_raw'], tf.uint8)\r\n    image = tf.reshape(image, [HEIGHT, WIDTH, DEPTH])\r\n    image = preprocess_input(tf.cast(image, tf.float32))\r\n    return image, parsed_example['label']\r\n\r\ndef get_data(filepath, schuffle_size=32, batch_size=8, prefetch=1, repeat=None, num_parallel_calls=1):\r\n    dataset = tf.data.TFRecordDataset(filepath)\r\n    if schuffle_size != 0:\r\n        dataset = dataset.shuffle(schuffle_size)\r\n    dataset = dataset.repeat(repeat)\r\n    dataset = dataset.map(parse_function, num_parallel_calls=num_parallel_calls)\r\n    dataset = dataset.batch(batch_size)\r\n    dataset = dataset.prefetch(prefetch)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    return iterator\r\n\r\ndef build_model(number_of_neurons_in_dense_layer, dropout, learning_rate):\r\n    base_model = Xception(weights='imagenet', include_top=False, pooling='avg', input_shape=(HEIGHT, WIDTH, 3))\r\n    for layer in base_model.layers:\r\n        layer.trainable = True\r\n    x = base_model.output\r\n    x = Dropout(dropout)(x)\r\n    x = Dense(number_of_neurons_in_dense_layer, activation='relu')(x)\r\n    x = Dropout(dropout)(x)\r\n    logits = Dense(NUMBER_OF_CLASSES, activation='softmax')(x)\r\n    model = Model(inputs=base_model.input, outputs=logits)\r\n    model.compile(optimizer=Adam(lr=learning_rate), loss='sparse_categorical_crossentropy', metrics=['categorical_accuracy'])\r\n    return model\r\n\r\nglobal NUMBER_OF_CLASSES, HEIGHT, WIDTH, DEPTH\r\nNUMBER_OF_CLASSES = 2\r\n...\r\nCLASS_WEIGHTS = {\r\n        0: 1,\r\n        1: 7\r\n       }\r\nmodel = build_model(64, 0.4, 0.001)\r\ntrain = get_data(..., 8, 2, num_parallel_calls=8)\r\nval = get_data(...., 0, 4, num_parallel_calls=8)\r\nmodel.fit(train, validation_data=val, epochs=3,steps_per_epoch=8//2,\r\n           validation_steps=8//4, shuffle=False, \r\n           class_weight=CLASS_WEIGHTS)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 51, in _wrapfunc\r\n    return getattr(obj, method)(*args, **kwds)\r\nAttributeError: 'Tensor' object has no attribute 'reshape'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./model.py\", line 137, in main\r\n    model.fit(train, validation_data=val, epochs=3, steps_per_epoch=8 // 2, validation_steps=8 // 4, shuffle=False, class_weight=CLASS_WEIGHTS)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 776, in fit\r\n    shuffle=shuffle)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 2432, in _standardize_user_data\r\n    feed_sample_weight_modes)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 2431, in <listcomp>\r\n    for (ref, sw, cw, mode) in zip(y, sample_weights, class_weights,\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\", line 758, in standardize_weights\r\n    y_classes = np.reshape(y, y.shape[0])\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 279, in reshape\r\n    return _wrapfunc(a, 'reshape', newshape, order=order)\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 61, in _wrapfunc\r\n    return _wrapit(obj, method, *args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 41, in _wrapit\r\n    result = getattr(asarray(obj), method)(*args, **kwds)\r\nTypeError: __index__ returned non-int (type NoneType)\r\n```", "comments": ["@jvishnuvardhan any updates?", "@imarkiew This looks like due to the fact that code in training_utils.py of tf.keras is still wrongly assuming input data to be numpy arrays. Try enabling eager execution. It goes away for me if I do that but then it also uncovers other bugs. ", "@dd1923 Honestly, I changed my approach some time ago and completely switched to Kerasa (so I don't use TFRecords and Datset API). But I also think that there is something broken underneath between the Tensors and numpy arrays.", "@imarkiew @dd1923 There were lots of improvements over the last two releases. Can you please check with `TF2.0` or `tf-nightly` and let us know whether the issue persists in the newer TF versions. Please provide a standalone code reproduce the issue. Thanks!", "@imarkiew Is this still an issue? If it not, please close the issue. Thanks!", "@jvishnuvardhan I tested this for `2.0.0b1` version and it still seems that the problem exists (but with another error). Unfortunately, due to some problems I wasn't able to test `tf-nightly`.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2326, in get_attr\r\n    c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'dropout_1/cond' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py\", line 344, in _MaybeCompile\r\n    xla_compile = op.get_attr(\"_XlaCompile\")\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2330, in get_attr\r\n    raise ValueError(str(e))\r\nValueError: Operation 'dropout_1/cond' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 427, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'dropout_shape_dense_identity' expects to be colocated with unknown node 'dropout_1/cond'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 52, in <module>\r\n    model.fit(train, validation_data=val, epochs=3, steps_per_epoch=8//2, validation_steps=8//4, class_weight=CLASS_WEIGHTS, shuffle=False)\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 643, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 694, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 264, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 917, in train_on_batch\r\n    self._make_train_function()\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1974, in _make_train_function\r\n    params=self._collected_trainable_weights, loss=self.total_loss)\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/keras/optimizers.py\", line 476, in get_updates\r\n    grads = self.get_gradients(loss, params)\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/keras/optimizers.py\", line 91, in get_gradients\r\n    grads = K.gradients(loss, params)\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3568, in gradients\r\n    loss, variables, colocate_gradients_with_ops=True)\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 158, in gradients\r\n    unconnected_gradients)\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py\", line 677, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py\", line 349, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py\", line 677, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/ops/cond_v2.py\", line 100, in _IfGrad\r\n    true_graph, false_graph = _get_func_graphs(if_op)\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/ops/cond_v2.py\", line 268, in _get_func_graphs\r\n    return (_get_func_graph_for_branch(op.get_attr(\"then_branch\")),\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/ops/cond_v2.py\", line 258, in _get_func_graph_for_branch\r\n    fdef, input_shapes)\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/framework/function_def_to_graph.py\", line 62, in function_def_to_graph\r\n    importer.import_graph_def(graph_def, name=\"\")\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/PycharmProjects/TfRecordsTestV2/venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 431, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: Node 'dropout_shape_dense_identity' expects to be colocated with unknown node 'dropout_1/cond'\r\n```", "@imarkiew Can you please try `TF2.0` or create a simple standalone code to reproduce the issue. Thanks!", "@jvishnuvardhan It seems that the change form `2.0.0b1` to `2.0.0` has helped. I don't see this issue anymore.", "@imarkiew Thanks for the confirmation. We will close this issue as it was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26714\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26714\">No</a>\n"]}, {"number": 26713, "title": "Large Variation in Compute Times", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: Python 2.7.15rc1\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: 10.0/7.4.2\r\n- GPU model and memory: NVIDIA Corporation GP104GL [Quadro P5000] (2 GPUs)\r\n\r\n\r\n**Describe the current behavior**\r\nI am running a toy matrix multiplication example multiple times on TensorFlow ([code here](https://github.com/xilenteyex/toy_matmul_tf)). When I look into the timeline for multiple iterations, I see an increase in the time to compute the same graph over multiple iterations. Specifically, look at these three timelines:\r\n[Timeline 1](https://github.com/xilenteyex/toy_matmul_tf/blob/master/logs/timeline_matmul_2.15_1_.ctf.json)\r\n[Timeline 3](https://github.com/xilenteyex/toy_matmul_tf/blob/master/logs/timeline_matmul_2.15_3_.ctf.json)\r\n[Timeline 8](https://github.com/xilenteyex/toy_matmul_tf/blob/master/logs/timeline_matmul_2.15_8_.ctf.json)\r\nFor **MatMul_25**, [first iteration](https://github.com/xilenteyex/toy_matmul_tf/blob/master/logs/timeline_matmul_2.15_1_.ctf.json) shows compute time to be ~8000 ms, while [3rd iteration](https://github.com/xilenteyex/toy_matmul_tf/blob/master/logs/timeline_matmul_2.15_3_.ctf.json) shows that compute time is ~15000 ms and [8th iteration](https://github.com/xilenteyex/toy_matmul_tf/blob/master/logs/timeline_matmul_2.15_8_.ctf.json) shows that compute time is ~27000 ms.\r\n\r\n**Describe the expected behavior**\r\nI expect compute times of the same matrix multiplication operation to be stay similar over multiple iterations instead of increasing linearly. Is this the expected behavior or am I missing something ?\r\n\r\n**Code to reproduce the issue**\r\n[code to rerun the toy example can be found here.](https://github.com/xilenteyex/toy_matmul_tf)\r\n\r\n**Other info / logs**\r\n[Timelines generated when I ran the code are in this folder](https://github.com/xilenteyex/toy_matmul_tf/tree/master/logs)\r\n", "comments": ["I am trying to write a custom graph partitioner and am stuck on this issue for a very long time. If someone can help me figure this out, it will be great. Thanks!", "@penpornk, it will be great if can comment on this issue.\r\nThanks!", "@xilenteyex I'm so sorry for the delay! I'll look into this soon!", "Cc'ing our GPU expert, @reedwm, as I mostly work on the CPU side.", "@xilenteyex This is a stale issue. Looks like this was resolved in recent TF1.x version. I have tried your code with `TF1.15.5` on colab and I cannot reproduce the issue. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/d9108463d8a114d71d69c1ec4bd9ae96/untitled1033.ipynb#scrollTo=lm7rbzxeXUYy)\r\n\r\nAlso, please note that there is no support for `TF1.x` issues.\r\n\r\nI am closing this issue as this was resolved. Please feel free to reopen the issue (if it persists) after testing with `TF2.x`. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26713\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26713\">No</a>\n"]}, {"number": 26712, "title": "Fix two golint warnings in graph.go", "body": "The current verion of `graph.go` produces some warnings when run through\r\n`golint`:\r\n```shell\r\n$ ~/go/bin/golint \r\ngraph.go:56:1: comment on exported type GraphImportOptions should be of the form \"GraphImportOptions ...\" (with optional leading article)\r\ngraph.go:173:21: should drop = 0 from declaration of var pos; it is the zero value\r\n```\r\nI'm presuming that these warnings are unintentional, as all of the other non-generated files in the TensorFlow Go API pass `golint` cleanly.\r\n\r\nThis PR fixes the two warnings by changing two lines in `graph.go`.\r\n\r\nAfter the change, running `golint` in `tensorflow/go` produces no warnings. The regression tests for the Go API also pass.\r\n", "comments": []}, {"number": 26711, "title": "Numpy operation on List of Tensor is considerably slow.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Docker\r\n- TensorFlow version (use command below): 2.0.0-alpha\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.\r\n- GPU model and memory: V100\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nNumpy operations on a list of Tensor is considerably slow. Commands to reproduce the results and corresponding time are shown below in the screenshot below.\r\n\r\n![image](https://user-images.githubusercontent.com/18187806/54388179-8f3f8980-4673-11e9-8009-a076402333c4.png)\r\n\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@HanGuo97,\r\nI executed your code with the **`latest version of Tensorflow (2.4.1)`**  and observed that the **`Numpy Operation`** on **`List`** of **`Tensor`** is faster enough. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/587dd5ce7c60fc9fb835eb17253eb106/gh_26711.ipynb) of the working code. Thanks!", "Thanks!"]}, {"number": 26710, "title": "avoid SONAME check using objdump on Windows", "body": "Haskell Platform comes with a objdump in PATH, which fails finding cuda libs because of SONAME checking.\r\nThis is already patched in mainstream, but not merged into 2.0 branch.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26710) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26710) for more info**.\n\n<!-- ok -->", "@goldiegadde what's the process for cherry-picking for 2.0? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 26709, "title": "Mirrored Strategy example code erroring out", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nrhel 7.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: release 9.1, V9.1.85\r\n- GPU model and memory:  C4130 K80, 30404 MB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nerror \r\n\r\n**Describe the expected behavior**\r\nmodel to fit using the gpus \r\n\r\n**Code to reproduce the issue**\r\n```\r\nfeatures = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)\r\nlabels = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)\r\ntrain_dataset = tf.data.Dataset.zip((features, labels))\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy(['/device:CPU:0', '/device:GPU:0', '/device:GPU:1'])\r\n\r\nwith distribution.scope():\r\n  inputs = tf.keras.layers.Input(shape=(1,))\r\n  predictions = tf.keras.layers.Dense(1)(inputs)\r\n  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\r\n\r\n  model.compile(loss='mean_squared_error',\r\n                optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.2))\r\n\r\nmodel.fit(train_dataset, epochs=5, steps_per_epoch=10)\r\n\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-4446125ee1ea> in <module>()\r\n----> 1 model.fit(train_dataset, epochs=5, steps_per_epoch=10)\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n   1637           initial_epoch=initial_epoch,\r\n   1638           steps_per_epoch=steps_per_epoch,\r\n-> 1639           validation_steps=validation_steps)\r\n   1640\r\n   1641   def evaluate(self,\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/keras/engine/training_arrays.pyc in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\r\n     84       ValueError: in case of invalid arguments.\r\n     85   \"\"\"\r\n---> 86   model._make_train_function()\r\n     87   f = model.train_function\r\n     88\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/keras/engine/training.pyc in _make_train_function(self)\r\n    698           # Training updates\r\n    699           updates = self.optimizer.get_updates(\r\n--> 700               params=self._collected_trainable_weights, loss=self.total_loss)\r\n    701         # Unconditional updates\r\n    702         updates += self.get_updates_for(None)\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/keras/optimizers.pyc in get_updates(self, loss, params)\r\n    730       # incremented in optimizer.apply_gradients()\r\n    731       self.updates = []\r\n--> 732       grads = self.optimizer.compute_gradients(loss, params)\r\n    733       opt_update = self.optimizer.apply_gradients(\r\n    734           grads, global_step=self.iterations)\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/training/optimizer.pyc in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\r\n    509     var_list += ops.get_collection(ops.GraphKeys._STREAMING_MODEL_PORTS)\r\n    510     # pylint: enable=protected-access\r\n--> 511     processors = [_get_processor(v) for v in var_list]\r\n    512     if not var_list:\r\n    513       raise ValueError(\"No variables to optimize.\")\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/training/optimizer.pyc in _get_processor(v)\r\n    202     # True if and only if `v` was initialized eagerly.\r\n    203     return _DenseResourceVariableProcessor(v)\r\n--> 204   if v.op.type == \"VarHandleOp\":\r\n    205     return _DenseResourceVariableProcessor(v)\r\n    206   if isinstance(v, variables.Variable):\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/contrib/distribute/python/values.pyc in op(self)\r\n    306                               self._primary_var.op.graph,\r\n    307                               self._primary_var.op.type)\r\n--> 308     return self.get().op\r\n    309\r\n    310   @property\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/contrib/distribute/python/values.pyc in get(self, device)\r\n     74       six.raise_from(\r\n     75           ValueError(\"Device %s not found in %s (current device %s)\" %\r\n---> 76                      (device, self._index.keys(), device_util.current())), e)\r\n     77\r\n     78   def on_device(self, device):\r\n\r\n/jump/software/rhel7/python27_six-1.10.0/lib/python2.7/site-packages/six.py in raise_from(value, from_value)\r\n    716 else:\r\n    717     def raise_from(value, from_value):\r\n--> 718         raise value\r\n    719\r\n    720\r\n\r\nValueError: Device /replica:0/task:0/device:CPU:0 not found in ['/replica:0/task:0/device:GPU:0', '/replica:0/task:0/device:GPU:1', '/replica:0/task:0/device:GPU:2', '/replica:0/task:0/device:GPU:3'] (current device )\r\n```", "comments": ["Tried changing the distribution strategy to be, as suggested in one of the comments on https://github.com/tensorflow/tensorflow/issues/22550\r\n\r\n```\r\ndistribution = tf.contrib.distribute.MirroredStrategy(['/device:CPU:0', '/device:GPU:0', '/device:GPU:1'])\r\n```\r\n\r\nError output now changes to \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-4-11c19e2f24cd> in <module>()\r\n      7                 optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.2))\r\n      8\r\n----> 9 model.fit(train_dataset, epochs=5, steps_per_epoch=10)\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n   1637           initial_epoch=initial_epoch,\r\n   1638           steps_per_epoch=steps_per_epoch,\r\n-> 1639           validation_steps=validation_steps)\r\n   1640\r\n   1641   def evaluate(self,\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/keras/engine/training_arrays.pyc in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\r\n     84       ValueError: in case of invalid arguments.\r\n     85   \"\"\"\r\n---> 86   model._make_train_function()\r\n     87   f = model.train_function\r\n     88\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/keras/engine/training.pyc in _make_train_function(self)\r\n    698           # Training updates\r\n    699           updates = self.optimizer.get_updates(\r\n--> 700               params=self._collected_trainable_weights, loss=self.total_loss)\r\n    701         # Unconditional updates\r\n    702         updates += self.get_updates_for(None)\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/keras/optimizers.pyc in get_updates(self, loss, params)\r\n    732       grads = self.optimizer.compute_gradients(loss, params)\r\n    733       opt_update = self.optimizer.apply_gradients(\r\n--> 734           grads, global_step=self.iterations)\r\n    735\r\n    736     self.updates.append(opt_update)\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/training/optimizer.pyc in apply_gradients(self, grads_and_vars, global_step, name)\r\n    607         else:\r\n    608           scope_name = var.op.name\r\n--> 609         with ops.name_scope(\"update_\" + scope_name), ops.colocate_with(var):\r\n    610           update_ops.append(processor.update_op(self, grad))\r\n    611       if global_step is None:\r\n\r\n/jump/software/rhel7/Python-2.7.7/lib/python2.7/contextlib.pyc in __enter__(self)\r\n     15     def __enter__(self):\r\n     16         try:\r\n---> 17             return self.gen.next()\r\n     18         except StopIteration:\r\n     19             raise RuntimeError(\"generator didn't yield\")\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/framework/ops.pyc in _colocate_with_for_gradient(self, op, gradient_uid, ignore_existing)\r\n   4092   def _colocate_with_for_gradient(self, op, gradient_uid,\r\n   4093                                   ignore_existing=False):\r\n-> 4094     with self.colocate_with(op, ignore_existing):\r\n   4095       if gradient_uid is not None and self._control_flow_context is not None:\r\n   4096         self._control_flow_context.EnterGradientColocation(op, gradient_uid)\r\n\r\n/jump/software/rhel7/Python-2.7.7/lib/python2.7/contextlib.pyc in __enter__(self)\r\n     15     def __enter__(self):\r\n     16         try:\r\n---> 17             return self.gen.next()\r\n     18         except StopIteration:\r\n     19             raise RuntimeError(\"generator didn't yield\")\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/framework/ops.pyc in colocate_with(self, op, ignore_existing)\r\n   4144     if op is not None and not isinstance(op, Operation):\r\n   4145       # We always want to colocate with the reference op.\r\n-> 4146       op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n   4147\r\n   4148     # By default, colocate_with resets the device function stack,\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/framework/ops.pyc in internal_convert_to_tensor_or_indexed_slices(value, dtype, name, as_ref)\r\n   1305   else:\r\n   1306     return internal_convert_to_tensor(\r\n-> 1307         value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1308\r\n   1309\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/python/framework/ops.pyc in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1144\r\n   1145     if ret is None:\r\n-> 1146       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1147\r\n   1148     if ret is NotImplemented:\r\n\r\n/jump/software/rhel7/python27_tensorflow-1.12.0/lib/tensorflow/contrib/distribute/python/values.pyc in _tensor_conversion_mirrored(var, dtype, name, as_ref)\r\n    437   # Try to avoid assignments to and other mutations of MirroredVariable\r\n    438   # state except through a DistributionStrategy.update() call.\r\n--> 439   assert not as_ref\r\n    440   return ops.internal_convert_to_tensor(\r\n    441       var.get(), dtype=dtype, name=name, as_ref=as_ref)\r\n\r\nAssertionError:\r\n\r\n```", "Could you try tf-nightly and see if the issue remains? Thanks!", "@byronyi  don't have access to the new cuda 10.0 drivers in my environment and no easy way to get them. Anything else I can try? ", "Could you try to remove CPU:0 from your devices?", "Not sure if I understood you correctly, but I tried this \r\n\r\n```\r\ndistribution = tf.contrib.distribute.MirroredStrategy(['/device:GPU:0', '/device:GPU:1'])\r\n```\r\n\r\nThe error I got was \r\n\r\n```\r\nValueError: Device /replica:0/task:0/device:CPU:0 not found in ['/replica:0/task:0/device:GPU:0', '/replica:0/task:0/device:GPU:1'] (current device )\r\n```", "As additional information, I also tried this \r\n\r\n```\r\n# Creates a graph.\r\nwith tf.device('/cpu:0'):\r\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\n# Creates a session with log_device_placement set to True.\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n# Runs the op.\r\nprint(sess.run(c))\r\n```\r\n\r\nAnd it worked fine with no error ", "@yuefengz Yuefeng, could you take a look here?", "> As additional information, I also tried this\r\n> \r\n> ```\r\n> # Creates a graph.\r\n> with tf.device('/cpu:0'):\r\n>   a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n>   b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n> c = tf.matmul(a, b)\r\n> # Creates a session with log_device_placement set to True.\r\n> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n> # Runs the op.\r\n> print(sess.run(c))\r\n> ```\r\n> And it worked fine with no error\r\n\r\ni meet this problem,too.this example given by tensorflow.org confused me", "The `with distribution.scope():` API with Keras (as in your example above) only works with TensorFlow nightly or with TensorFlow 2.0 alpha currently. It will be available in TF next release (1.14) as well. ", "@guptapriya  have plans for 1.14 release been cancelled?", "AFAIK no, the cut date of branch r1.14 is April 15."]}, {"number": 26708, "title": "Attempting to convert model from basic_classification dataset to TFLite", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- OS Platform: Windows 10 x64\r\n- Tensorflow installed from: tf-nightly-2.0-preview\r\n- TensorFlow version: 2.0 Preview\r\n- Python version: 3.6\r\n- Installed using: pip\r\n- Doc Link:\r\nhttps://www.tensorflow.org/alpha/tutorials/keras/basic_classification\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r2/convert/concrete_function.md\r\n\r\nI am starting from the basic_classification dataset and attempting to save the model result as tflite but can't figure out how to perform the operation correctly.  When I export the saved model it tells me that the function will only be available through the v1 compatibility library and that 'Export includes no default signature'.  Trying to create the concrete function directly off the generated model returns the error 'Sequential object has no attribute 'signatures''.  \r\n\r\nIt would be really nice if those example datasets included in their instructions how to save the model as a tflite model.\r\n\r\nI'd also like to restore a v1 frozen model and save it as a tflite model but can't figure that out either.  The instructions about concrete functions appear to imply that only Tensorflow 2.0 models (properly created) can be converted to tflite models.\r\n\r\nCan v1 models be saved as v2 tflite models or do v1 models have to be saved using the compat.v1 TFLiteConverter only?", "comments": ["Can you provide the following so that I can reproduce the errors you are getting and suggest the next steps you should take:\r\n\r\n1. Your 2.0 model file that you generated from the basic_classification dataset that you are trying to convert\r\n2. Your conversion code\r\n3. The exact error message that you are getting\r\n\r\nLate yesterday, I added a short section to the [`python_api.md`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r2/convert/python_api.md) document for clarification on what's supported in the 2.0 converter:\r\n\r\n> `TFLiteConverter` in 2.0 supports SavedModels and Keras model files generated in both 1.X and 2.0. However, the conversion process no longer supports frozen `GraphDefs` generated in 1.X. Users who want to convert frozen GraphDefs to TensorFlow Lite should use `tensorflow.compat.v1`.\r\n\r\nThis means some 1.X models can be converted using the 2.0 converter, but not 1.X frozen graphs.\r\n\r\nHope this helps. Thanks for your patience!", "[saved.zip](https://github.com/tensorflow/tensorflow/files/2968142/saved.zip)\r\nFirst I was just trying to save and load the regular Tensorflow 2.0 model in saved model format but ran into problems with that.\r\nTo save this model I followed the directions in https://www.tensorflow.org/alpha/tutorials/keras/basic_classification and then used the Python command:\r\n>>> tf.saved_model.save(model, './saved')\r\nwhich created a model in the format 'TensorFlow Saved Model v1'\r\n\r\nWhen I attempt to load it with the Python command:\r\n>>> loaded = tf.saved_model.load('./saved')\r\nI get the AttributeError: '_UserObject' object has no attribute '_create_or_restore_slot_variable'\r\nSo I couldn't create a concrete_function from a model signature.\r\n\r\nI then tried to create the concrete function using:\r\n>>> run_model = tf.function(lambda x: model(x))\r\n>>> concrete_func = run_model.get_concrete_function(tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))\r\nand got the error: \r\nWARNING: Entity <function <lambda> at 0x000002B660E61BF8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming <function <lambda> at 0x000002B660E61BF8>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output when filing the bug report. Caused by: Unable to locate the source code of <function <lambda> at 0x000002B660E61BF8>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\n\r\nI then tried:\r\n>>> class BasicModel(tf.Module):\r\n>>>     @tf.function(input_signature=[tf.TensorSpec(shape=[None,28,28], dtype=tf.float32)])\r\n>>>     def get_x(self, x):\r\n>>>         return x\r\n>>> img = test_images[0]\r\n>>> img = (np.expand_dims(img,0))\r\n>>> input_data = tf.constant(img, shape=[1,28,28])\r\n>>> concrete_func = root.get_x.get_concrete_function(input_data)\r\nwith the same error as above", "I eventually used:\r\n\r\n>>> concrete_func = root.get_x.get_concrete_function(img)\r\n>>> converter = tf.lite.TFLiteconverter.from_concrete_function(concrete_func)\r\n>>> tflite_model = converter.convert()\r\nand received the error: \r\nValueError: Unsupported tf.dtype <dtype: 'float64'>", "When I try converting the model you provided, I ran into an issue with optimizers. However, I believe that's  been resolved since. I tried reproducing the model and converting it locally and I was successfully able to convert it with the following code:\r\n\r\n```\r\nrun_model = tf.function(lambda x: model(x))\r\nconcrete_func = run_model.get_concrete_function(\r\n      tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))\r\nconverter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)\r\ntflite_model = converter.convert()\r\nopen(\"/tmp/mnist_fashion/converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nFor reference, this was the complete code:\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\n\r\ndef get_data_set(num_images):\r\n  \"\"\"Returns the training and testing data base.\"\"\"\r\n  fashion_mnist = keras.datasets.fashion_mnist\r\n  (train_images, train_labels), (test_images, test_labels) = (\r\n      fashion_mnist.load_data())\r\n  train_images = train_images / 255.0\r\n  test_images = test_images / 255.0\r\n\r\n  num_images = min(num_images, len(train_images))\r\n  train_images = train_images[:num_images]\r\n  train_labels = train_labels[:num_images]\r\n  return train_images, train_labels\r\n\r\n\r\ndef build_model(train_images, train_labels, epochs=5):\r\n  \"\"\"Build the model.\"\"\"\r\n  model = keras.Sequential([\r\n      keras.layers.Flatten(input_shape=(28, 28)),\r\n      keras.layers.Dense(128, activation='relu'),\r\n      keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  model.compile(\r\n      optimizer='adam',\r\n      loss='sparse_categorical_crossentropy',\r\n      metrics=['accuracy'])\r\n  model.fit(train_images, train_labels, epochs)\r\n  return model\r\n\r\n\r\ndef main():\r\n  export_dir = '/tmp/mnist_fashion'\r\n\r\n  # Build the model.\r\n  train_images, train_labels = get_data_set(num_images=100)\r\n  model = build_model(train_images, train_labels, epochs=1)\r\n  tf.saved_model.save(model, export_dir) # not needed for this code\r\n\r\n  # Convert the model directly.\r\n  run_model = tf.function(lambda x: model(x))\r\n  concrete_func = run_model.get_concrete_function(\r\n      tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))\r\n  converter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)\r\n  tflite_model = converter.convert()\r\n  open(\"/tmp/mnist_fashion/converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```\r\n\r\nI was also able to convert the SavedModel with the following code:\r\n```\r\nmodel = tf.saved_model.load(export_dir)\r\nconcrete_func = model.signatures[\r\n    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nconcrete_func.inputs[0].set_shape([1, 28, 28])\r\nconverter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)\r\ntflite_model = converter.convert()\r\nopen(\"/tmp/mnist_fashion/converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nCan you try regenerating the model and converting it one of the ways shown above?", "Are you using the nightly build?\r\n\r\nI get this:\r\n>>> concrete_func = run_model.get_concrete_function(tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))\r\nW0319 12:28:09.571433 40164 tf_logging.py:161] Entity <function <lambda> at 0x00000276D2AF62F0> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming <function <lambda> at 0x00000276D2AF62F0>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output when filing the bug report. Caused by: Unable to locate the source code of <function <lambda> at 0x00000276D2AF62F0>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nWARNING: Entity <function <lambda> at 0x00000276D2AF62F0> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming <function <lambda> at 0x00000276D2AF62F0>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output when filing the bug report. Caused by: Unable to locate the source code of <function <lambda> at 0x00000276D2AF62F0>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code", "I tested on today's nightly `2.0.0.dev20190327` and I was able to successfully run the script in https://github.com/tensorflow/tensorflow/issues/26708#issuecomment-474152784.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\nAdding here for documentation for future users. This functionality has been changed in https://github.com/tensorflow/tensorflow/commit/312222d2a0657d4e0086ec0add4331063d0b2264. `from_concrete_function` is now `from_concrete_functions`. Additionally, `from_keras_model` and `from_saved_model` have been added."]}, {"number": 26707, "title": "[TF2.0] Sequential API serialization bug", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\ndocker image from tensorflow/tensorflow:2.0.0a0-gpu-jupyter\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\ngit version 'v1.12.0-9492-g2c319fb415'\r\ntensorflow version '2.0.0-alpha0'\r\n- Python version:\r\n2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\ncuda version 415.27\r\n- GPU model and memory:\r\nGTX 1080Ti\r\n\r\n**Describe the current behavior**\r\nIf I serialize a model built using the Sequential API and recreate the model from the config it fails.\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nThe model to be initialized from the configuration\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata = np.random.random((1000, 32))\r\nlabels = np.random.random((1000, 10))\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\n# The compile step specifies the training configuration\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(data, labels, batch_size=32, epochs=5)\r\nconfig = model.get_config()\r\nnew_model = tf.keras.Model.from_config(config)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n------------------------------------------------------------\r\nKeyError                   Traceback (most recent call last)\r\n<ipython-input-48-db6b212995c2> in <module>()\r\n     12 model.fit(data, labels, batch_size=32, epochs=5)\r\n     13 config = model.get_config()\r\n---> 14 new_model = keras.Model.from_config(config)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/network.pyc in from_config(cls, config, custom_objects)\r\n   1229     # First, we create all layers and enqueue nodes to be processed\r\n   1230     for layer_data in config['layers']:\r\n-> 1231       process_layer(layer_data)\r\n   1232     # Then we process nodes in order of layer depth.\r\n   1233     # Nodes that cannot yet be processed (if the inbound node\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/network.pyc in process_layer(layer_data)\r\n   1208           ValueError: In case of improperly formatted `layer_data` dict.\r\n   1209       \"\"\"\r\n-> 1210       layer_name = layer_data['name']\r\n   1211 \r\n   1212       # Instantiate layer.\r\n\r\nKeyError: 'name'\r\n```\r\n```\r\nprint(config)\r\n{'build_input_shape': (None, 32),\r\n 'layers': [{'class_name': 'Dense',\r\n   'config': {'activation': 'relu',\r\n    'activity_regularizer': None,\r\n    'bias_constraint': None,\r\n    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\r\n    'bias_regularizer': None,\r\n    'dtype': 'float32',\r\n    'kernel_constraint': None,\r\n    'kernel_initializer': {'class_name': 'GlorotUniform',\r\n     'config': {'seed': None}},\r\n    'kernel_regularizer': None,\r\n    'name': 'dense_6',\r\n    'trainable': True,\r\n    'units': 128,\r\n    'use_bias': True}},\r\n  {'class_name': 'Dropout',\r\n   'config': {'dtype': 'float32',\r\n    'name': 'dropout_3',\r\n    'noise_shape': None,\r\n    'rate': 0.2,\r\n    'seed': None,\r\n    'trainable': True}},\r\n  {'class_name': 'Dense',\r\n   'config': {'activation': 'softmax',\r\n    'activity_regularizer': None,\r\n    'bias_constraint': None,\r\n    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\r\n    'bias_regularizer': None,\r\n    'dtype': 'float32',\r\n    'kernel_constraint': None,\r\n    'kernel_initializer': {'class_name': 'GlorotUniform',\r\n     'config': {'seed': None}},\r\n    'kernel_regularizer': None,\r\n    'name': 'dense_7',\r\n    'trainable': True,\r\n    'units': 10,\r\n    'use_bias': True}}],\r\n 'name': 'sequential_3'}\r\n```", "comments": ["I guess you have forgotten to save the model.You can use model.save() or model.save_weights().\r\nAn example can be seen here :- https://machinelearningmastery.com/save-load-keras-deep-learning-models/", "I am interested in architecture only saving. As can be seen in the tensorflow documentation https://www.tensorflow.org/alpha/guide/keras/saving_and_serializing#architecture-only_saving\r\n\r\nThe documentation says that both Functional and Sequential APIs support this functionality. However only the Functional API works as the documentation says. \r\n\r\nHere is the same example written in the Functional API and it works without a problem.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata = np.random.random((1000, 32))\r\nlabels = np.random.random((1000, 10))\r\n\r\ninputs = tf.keras.Input(shape=(32,))\r\nx = tf.keras.layers.Dense(128, activation='relu')(inputs)\r\nx = tf.keras.layers.Dropout(0.2)(x)\r\noutputs = tf.keras.layers.Dense(10, activation='softmax')(x)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n# The compile step specifies the training configuration\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(data, labels, batch_size=32, epochs=5)\r\nconfig = model.get_config()\r\nnew_model = tf.keras.Model.from_config(config)\r\n```", "@TahaK There are approaches (`to_json`, `clone_model`) other than `get_config` to save only model architecture. Please check the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/65497d8e97423ea7aac128da24ddd3b8/tf_26707.ipynb) that uses `to_json` and `clone_model` approaches to save architecture of the model. Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26707\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26707\">No</a>\n", "While this issue has already been closed, I guess the problem still persists?\r\n\r\nWith TensorFlow 2.0.0, `get_config` and `model_from_config` are still incompatible.\r\nAccording to https://www.tensorflow.org/guide/keras/save_and_serialize, I would expect the config serialization / deserialization to be compatible with each-other (as I think they were in TensorFlow 1.13.1).\r\n\r\nHas the config serialization / deserialization been deprecated (and meant to be superseded by JSON serialization / deserialization)?\r\n\r\nExample, adapted from @TahaK, with the config serialization / deserialization added.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(\"TensorFlow \" + tf.__version__)\r\n\r\ndata = np.random.random((1000, 32))\r\nlabels = np.random.random((1000, 10))\r\nmodel = tf.keras.models.Sequential([\r\n      tf.keras.layers.Flatten(input_shape=(32,)),\r\n        tf.keras.layers.Dense(128, activation='relu',),\r\n          tf.keras.layers.Dropout(0.2),\r\n            tf.keras.layers.Dense(10, activation='softmax')\r\n            ])\r\n\r\n# The compile step specifies the training configuration\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\r\n                      loss='categorical_crossentropy',\r\n                                    metrics=['accuracy'])\r\n\r\nmodel.fit(data, labels, batch_size=32, epochs=5)\r\n# json approach\r\njson_config = model.to_json()\r\nnew_model = tf.keras.models.model_from_json(json_config)\r\n# model cloning\r\ncloned_model=tf.keras.models.clone_model(model)\r\n# config approach\r\ndict_config = model.get_config()\r\nnew_model2 = tf.keras.models.model_from_config(dict_config)\r\n\r\nmodel.summary()\r\nnew_model.summary()\r\ncloned_model.summary()\r\nnew_model2.summary()\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nTensorFlow 2.0.0\r\nTrain on 1000 samples\r\nEpoch 1/5\r\n1000/1000 [==============================] - 1s 503us/sample - loss: 12.6396 - accuracy: 0.0990\r\nEpoch 2/5\r\n1000/1000 [==============================] - 0s 43us/sample - loss: 14.7099 - accuracy: 0.0940\r\nEpoch 3/5\r\n1000/1000 [==============================] - 0s 40us/sample - loss: 16.9181 - accuracy: 0.0840\r\nEpoch 4/5\r\n1000/1000 [==============================] - 0s 39us/sample - loss: 18.8631 - accuracy: 0.0900\r\nEpoch 5/5\r\n1000/1000 [==============================] - 0s 39us/sample - loss: 21.1111 - accuracy: 0.0980\r\nTraceback (most recent call last):\r\n  File \"test2.py\", line 28, in <module>\r\n    new_model2 = tf.keras.models.model_from_config(dict_config)\r\n  File \"/home/?/.conda/envs/nn-test/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/?/.conda/envs/nn-test/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/serialization.py\", line 94, in deserialize\r\n    layer_class_name = config['class_name']\r\nKeyError: 'class_name'\r\n```", "@Sussch You can use `tf.keras.Sequential.from_config(dict_config)`. I have used it for instantiating model object to load successfully as shown below.\r\n\r\n```\r\n# config approach\r\ndict_config = model.get_config()\r\n#new_model2 = tf.keras.models.model_from_config(dict_config)\r\nnew_model2 = tf.keras.Sequential.from_config(dict_config)\r\nnew_model2.summary()\r\n\r\n```\r\n[Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/19989131373b9fb0837cab5968307008/untitled672.ipynb) is the gist for your reference. Thanks!", "Ah, thank you.\r\n\r\nIndeed, it says `keras.Model.from_config`, not `keras.models.model_from_config` in the guide at https://www.tensorflow.org/guide/keras/save_and_serialize.\r\nSorry, I must have missed it.\r\n\r\nI now tried `keras.Model.from_config` and got a slightly different exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test2.py\", line 29, in <module>\r\n    new_model2 = tf.keras.Model.from_config(dict_config)\r\n  File \"/home/?/.conda/envs/nn-test/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 906, in from_config\r\n    config, custom_objects)\r\n  File \"/home/?/.conda/envs/nn-test/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1842, in reconstruct_from_config\r\n    process_layer(layer_data)\r\n  File \"/home/?/.conda/envs/nn-test/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1816, in process_layer\r\n    layer_name = layer_data['name']\r\nKeyError: 'name'\r\n```\r\n\r\n@jvishnuvardhan Your suggested `tf.keras.Sequential.from_config(dict_config)` works perfectly.\r\nCould the guide (https://www.tensorflow.org/guide/keras/save_and_serialize) be updated accordingly?\r\n\r\nEdit: It seems that the guide does actually say that it is different for Sequential models.\r\n> Saving for custom subclasses of Model is covered in the section \"Saving Subclassed Models\". The APIs in this case are slightly different than for Sequential or Functional models.\r\n\r\nThank you!"]}, {"number": 26706, "title": "Fixed file name", "body": "I think there is an error in the file name as no such file exists. Am I right? @rthadur ", "comments": ["Sorry about the logo commits, I committed to master (newbie error). Will it be an issue?", "Looks reasonable to me! (We don't test the CMake build continuously, which allows errors like this creep in, but as far as I can tell, that name doesn't occur in absl.)", "What's the hold up on this? @mrry @rthadur "]}, {"number": 26705, "title": "Add missing include", "body": "This is needed for building on macOS 10.14.3, bazel 0.23.1, clang `Apple LLVM version 10.0.0 (clang-1000.11.45.5)`.", "comments": ["This will be obsoleted by https://github.com/tensorflow/tensorflow/pull/26675 if that gets in (which appears likely), so I'll likely delete this PR soon.  I made it because I found a nasty memory look and wasn't 100% sure the warnings PR hadn't caused it (I'm sure now).\r\n\r\nActually, might as well delete it now."]}, {"number": 26704, "title": "Fixed typos in GradientTape warning message", "body": "I frequently encounter the amended warning message when using tensorflow/privacy differentially private optimizers in Eager Mode.\r\n\r\nThis PR is simply a typo correction of the warning message.", "comments": []}, {"number": 26703, "title": "[2.0] Combining metrics places updates on the wrong graph", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: `2.0.0.dev20190311`\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: 10.0\r\n\r\n**Describe the current behavior**\r\n\r\nI wanted to define a custom metric that is the combination of 2 metrics:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass F1(tf.metrics.Metric):\r\n\r\n  def __init__(self, **kwargs):\r\n    super(F1, self).__init__(**kwargs)\r\n    self.precision = tf.metrics.Precision()\r\n    self.recall = tf.metrics.Recall()\r\n\r\n  @property\r\n  def updates(self):\r\n    return self.precision.updates + self.recall.updates\r\n\r\n  def update_state(self, y_true, y_pred):\r\n    self.precision.update_state(y_true, y_pred)\r\n    self.recall.update_state(y_true, y_pred)\r\n\r\n  def result(self):\r\n    precision = self.precision.result()\r\n    recall = self.recall.result()\r\n    return (2 * precision * recall) / (recall + precision)\r\n```\r\n\r\nBut when run in graph mode, the `updates` operations are placed on the wrong graph which raises an error in the Estimator code:\r\n\r\nhttps://github.com/tensorflow/estimator/blob/v2.0.0-alpha/tensorflow_estimator/python/estimator/model_fn.py#L497\r\n\r\nAm I missing something here? The `F1` implementation looks correct so I suspect this is a bug.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe updates should be placed on the default graph.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nwith tf.Graph().as_default() as graph:\r\n  precision = tf.metrics.Precision()\r\n  precision.update_state([0, 0, 1], [1, 0, 1])\r\n  print(precision.updates[0].graph is graph)  # True\r\n\r\n  f1 = F1()\r\n  f1.update_state([0, 0, 1], [1, 0, 1])\r\n  print(f1.updates[0].graph is graph)  # False\r\n```", "comments": ["@pavithrasv can you look at this?", "Yes, I am able to repro, will look into the fix.", "@guillaumekln is there a reason you have overridden `updates` property? \r\n\r\nWe wrap metrics `update_state` API in `tf.function` internally to allow users to write simple metric updates. Since you have overridden updates, the updates you get are the updates of the inner Precision, Recall metrics which are in the `FuncGraph` scope. Your F1 metric should work as expected with both Estimator and Keras APIs without having to override updates. Thank you!", "I'm not sure that's the issue because if you don't override `updates`, it will still return the updates of the inner Precision and Recall metrics.  For example, when `updates` is not overriden, the graph of some updates is still not the default one:\r\n\r\n```python\r\nf1 = F1()\r\nf1.update_state([0, 0, 1], [1, 0, 1])\r\nprint(f1.updates[0].graph is graph)  # True\r\nprint(f1.updates[1].graph is graph)  # False\r\nprint(f1.updates[2].graph is graph)  # False\r\n```\r\n\r\nI was not sure why there are 3 updates in this case, that's why I overrided `updates`.\r\n\r\nNote that when integrating with Estimator, the error is the same whether `updates` is overriden or not:\r\n\r\n```text\r\nTraceback (most recent call last):\r\n  File \"/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/model_fn.py\", line 180, in __new__\r\n    eval_metric_ops = _validate_eval_metric_ops(eval_metric_ops)\r\n  File \"/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/model_fn.py\", line 573, in _validate_eval_metric_ops\r\n    'eval_metric_ops', '{0}: {1}'.format(key, val.name)))\r\nValueError: eval_metric_ops with \"precision: group_deps\" must be from the default graph. Possible causes of this error include: \r\n\r\n1) eval_metric_ops was created outside the context of the default graph.\r\n\r\n2) The object passed through to EstimatorSpec was not created in the most recent call to \"model_fn\".\r\n```", "For other users facing this, a possible workaround is to override the `__new__` method:\r\n\r\n```python\r\nclass F1(tf.metrics.Metric):\r\n  def __new__(cls, *args, **kwargs):\r\n    return tf.keras.layers.Layer.__new__(cls)\r\n```", "@guillaumekln,\r\nLooks like it is fixed in latest Tf 2.0 nightly version.\r\nPlease have a look at colab gist [here](https://colab.sandbox.google.com/gist/gadagashwini/08cfa8cffc1958a581de88b6927ed7ea/untitled148.ipynb). Thanks!", "It does not seem like it. The second `print` should display True.", "@guillaumekln \r\nCould you please check [this gist](https://colab.sandbox.google.com/gist/Saduf2019/3cd2a6de34d417b28aefcc2db2d6ad07/26703.ipynb) run on latest version of tensorflow, and let us know if it helps. Thanks!", "Your gist shows that the issue is still not fixed (both `print` should output `True`). However, I'm no longer interested in this issue and it does not seem other users were impacted, so feel free to close it.", "Moving the issue to closed status as confirmed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26703\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26703\">No</a>\n"]}, {"number": 26702, "title": "[ROCm] Enable ROCm support for the \"staging\" op", "body": "This PR enables ROCm support for the \"staging\" ops.\r\n\r\nPR #26457 is a pre-req for this PR, and hence this PR includes commits from that PR.\r\nOnly the last commit in this PR (the one which updates the `tensorflow/core/kernels/stage_op.cc` file)  should be reviewed here (as all others will be reviewed as part of PR #26457 )\r\n\r\n------------------\r\n\r\n@tatianashp @whchung : just FYI.", "comments": ["I would not be a good reviewer for this PR, please reassign.", "rebased this PR since PR #26457 has now been merged", "a much smaller PR indeed", "The PR looks good to me, but I don't maintain TF ops. Adding @chsigg ."]}, {"number": 26701, "title": "use enumerate instead of iterating with range and len", "body": "", "comments": ["@rthadur could you please help to merge this pr, its approved."]}, {"number": 26700, "title": "keras: Enabling eager mode breaks weights setting per kwargs in Layer constructors", "body": "Enabling eager mode breaks the ability to specify layer weights in the constructor.\r\n\r\nFor example passing all-zeros weights to a dense layer results in all zero outputs (for `use_bias=False`). \r\nThis is however not the case if eager mode is enabled, as the zero weights are not set; it seems to be known (see TODO by @fchollet), but I can't see why `set_weights` should not be called in non-eager mode: https://github.com/tensorflow/tensorflow/blob/5f071f2eb2321a8e4f46f16f706e9d743ec55554/tensorflow/python/keras/engine/base_layer.py#L660\r\n\r\n\r\nto easily reproduce comment in/out the `tf.enable_eager_execution()` bellow:\r\n\r\n```python\r\nimport unittest\r\nimport numpy as np\r\nfrom tensorflow.python import keras as tfk\r\nimport tensorflow as tf\r\n\r\n#tf.enable_v2_behavior()\r\ntf.enable_eager_execution()\r\n\r\nclass EagerWeightsTest(unittest.TestCase):\r\n\r\n    def test_eager_weights(self):\r\n        weights = np.zeros((10, 11))                           # use zero weights\r\n        dense = tfk.layers.Dense(units=11, input_shape=(10,),\r\n                                 weights=[weights],\r\n                                 use_bias=False)\r\n\r\n        model = tfk.models.Sequential()\r\n        model.add(dense)\r\n        model.compile(\"adam\", tfk.losses.mae)\r\n\r\n        logits = model.predict(np.random.random((3, 10)))\r\n\r\n        self.assertTrue(np.allclose(np.zeros((3, 11)),          # expect zero outputs\r\n                                    logits))\r\n\r\n```", "comments": ["This is fixed with latest tf-nightly build version '1.15.0-dev20190808'. Thanks!"]}, {"number": 26699, "title": "sparse_cataegorical_accuracy buggued in Keras LSTM when return_sequences=True", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): not for the original issue, then I tried some workarounds\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.6\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): 1.12.0, 1.13.1 and 2.0.0-dev20190313\r\n- Python version: 3.6.7\r\n- We are using CPU here.\r\n\r\n**Describe the current behavior**\r\n\r\nWhen training a simple neural network (one embedding layer, one LSTM layer, one dense layer) with `return_sequences=True`, the computation of sparse_categorical_accuracy crashes. The reason of the crash seems different in tensorflow 1.12.0 from one hand, and in tf 1.13.1 and tf 2.0.0 from another hand.\r\n\r\nThe setting is simple and common for each tensorflow version. See the notebooks in [this dedicated repository](https://github.com/durandg12/keras-lstm-issue) for the complete code. I generate 640 sequences of 5 random integers between 0 and 11. And I want to train an LSTM to predict the 5th element of a sequence given the 4 previous ones, with a batch size of 64. Hence the code looks like this:\r\n\r\n```\r\nsequence_length = 4\r\nnumber_of_categories = 12\r\nBATCH_SIZE = 64\r\ntotal_examples = 640\r\nsteps_per_epoch = total_examples // BATCH_SIZE\r\nraw_data = np.random.randint(0, number_of_categories, size=(total_examples, sequence_length+1))\r\nraw_dataset = tf.data.Dataset.from_tensor_slices(raw_data)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe training should not crash because of the accuracy computation. No matter the tensorflow version, note that the training does not crash and the accuracy is properly computed when `return_sequences=False`.\r\n\r\n**Code to reproduce the issue**\r\n\r\nPlease find all the code I used in the notebooks that I have put in [this dedicated repository](https://github.com/durandg12/keras-lstm-issue).\r\n\r\n**Other info / logs**\r\n\r\nFor tensorflow 1.12.0, the error I get is:`Incompatible shapes: [64] vs. [64,4] [Op:Equal]`. `y_true` and `y_pred` should both be of shape `[64,4]` so I quickly figured out where the problem comes from. In the code of `sparse_categorical_accuracy` [found here](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/keras/metrics.py), the first line `y_true = math_ops.reduce_max(y_true, axis=-1)` shrinks one dimension of `y_true` for apparently no reason. Hence, with a custom function that simply removes this line, training succeeds, with both `return_sequences=True` and `return_sequences=False`. See [the attached notebook](https://github.com/durandg12/keras-lstm-issue/blob/master/Fail%20LSTM%201.12.0.ipynb).\r\n\r\nFor tensorflow 1.13.1 and 2.0.0 the situation is more unclear and I couldn't find a workaround this time. This time, the error message is:\r\n\r\n```\r\nCan not squeeze dim[1], expected a dimension of 1, got 4\r\n\t [[{{node metrics/sparse_categorical_accuracy_tf13/Squeeze}}]] [Op:StatefulPartitionedCall]\r\n```\r\n\r\nand this seems to be due to the lines\r\n\r\n```\r\nif (len(K.int_shape(y_true)) == len(K.int_shape(y_pred))):\r\n    y_true = array_ops.squeeze(y_true, [-1])\r\n```\r\nin the code of `sparse_categorical_accuracy` [found here](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/metrics.py) [and here](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/metrics.py). `y_true` seems to have the expected `[64,4]` shape so we cannot squeeze it on its dimension `1`. But I don't get why we would want to squeeze it in the first place. Looking at the comment line `# If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)` in the code, it is clear that something is not working as intended. My attempt to workaround was to comment the previous lines, so as to use the same workaround as I did with tensorflow 1.12.0. But this time it didn't work for an obscure reason that I can't really understand. The error message I get is:\r\n\r\n```\r\nInvalid reduction dimension (2 for input with 2 dimension(s)\r\n\t [[{{node metrics_1/sparse_categorical_accuracy_tf13_v2/Sum}}]] [Op:StatefulPartitionedCall]\r\n```\r\nThere seems to be an error with the dimensions of the input data of `sparse_categorical_accuracy`. But `y_true` should be of shape `[64,4]`, and `y_pred`should be of shape `[64,4,12]` before applying ` y_pred = math_ops.argmax(y_pred, axis=-1)` and of shape `[64,4]` after.\r\n\r\nSo I made an experiment where I replaced the sparse categorical accuracy function by a function simply computing the rank of `y_true` (this is the function `ndim_y_true` in the attached notebooks) and I found even more confusing results. Whereas for 1.12.0, `ndim_y_true` yields `2` as expected for a `[64,4]` tensor, in 1.13.1 and 2.0.0 it yields `3`. This does not make sense to me, especially since when trying to apply `y_true = array_ops.squeeze(y_true, [-1])`, the error message properly aknowledge that the last dimension is dimension `1` (recall the `Can not squeeze dim[1]` above).\r\n\r\nWhen `return_sequences=False`, `ndim_y_true` also yields the strange result of `2` (this time, in the three versions of tensorflow I tried), while I expect `y_true` to be of shape `[64,]` in this case.\r\n\r\nAs a side note, it seems that adding a `TimeDistributed` layer to the `Dense` layer or not when `return_sequences=True` does not change anything.", "comments": ["I think the problem is somewhere else. If you import keras.metrics.sparse_caategorical_accuracy, you get the same error. Oddly enough, if you literally copy the code for keras.metrics.sparse_categorical_accuracy and put it in your own project, it will run fine. So it seems to me the problem happens when the model is compiled.\r\n\r\nEdit: tf 1.11 through 1.13, keras 2.2.4, python 3.5.2", "> I think the problem is somewhere else. If you import keras.metrics.sparse_caategorical_accuracy, you get the same error. Oddly enough, if you literally copy the code for keras.metrics.sparse_categorical_accuracy and put it in your own project, it will run fine. So it seems to me the problem happens when the model is compiled.\r\n\r\nIn tf 1.13.1 there is indeed something odd happening at model compilation, because of the value `3` returned by `ndim_y_true`.\r\n\r\nPlease detail your answer with the version of tf and of keras you used.", "I solved the issue (in all tensorflow versions) by adding a dimension at the end of my target data. In the code of the notebooks I changed the last line of\r\n\r\n```\r\ndef split_input_target(chunk, return_sequences):\r\n    input_data = chunk[:-1]\r\n    if return_sequences:\r\n        #the target is the whole sequence, shifted by 1\r\n        target_data = chunk[1:]\r\n    else:\r\n        #the target is only the last element\r\n        target_data = chunk[-1]\r\n    return input_data, target_data\r\n```\r\nin `return input_data, expand_dims(target_data, -1)`.\r\n\r\nSo in sparse encoding, Keras expects the target data to have 3 dimensions, as with one-hot encoding.\r\n\r\nIt would be nice to tell it somewhere in the documentation."]}, {"number": 26698, "title": "Update ios_image_load.h", "body": "tabs got replaced with spaces", "comments": ["I get it now."]}, {"number": 26696, "title": "tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  ubuntu16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  binary, tf-nightly\r\n- TensorFlow version (use command below):   1.14.1\r\n- Python version:  3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:   no cuda, only cpu\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI wanna to convert .pb model to tflite model.\r\n\r\n**Describe the expected behavior**\r\n\r\n********************\r\nthe following errors occured:\r\n\r\nTraceback (most recent call last):\r\n  File \"onnx2tf.py\", line 33, in <module>\r\n    main(sys.argv)\r\n  File \"onnx2tf.py\", line 27, in main\r\n    tflite_model = converter.convert()\r\n  File \"/home/riddick/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 762, in convert\r\n    **converter_kwargs)\r\n  File \"/home/riddick/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/riddick/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-03-14 03:21:21.151470: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: PyFunc\r\n2019-03-14 03:21:21.316309: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)\r\nFatal Python error: Aborted\r\n\r\n******************\r\n\r\n**Code to reproduce the issue**\r\n\r\n*******************************\r\n\r\n    input_arrays = [\"image\"]\r\n    output_arrays = [\"Add\"]\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(pbPath, input_arrays, output_arrays)\r\n    tflite_model = converter.convert()\r\n    open(tflitePath, \"wb\").write(tflite_model)\r\n\r\n*****************************\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nmy model has been uploaded to google drive:\r\n[https://drive.google.com/open?id=1V4ovRkU6Tv78oxCHjLZRClnubnDz9YzC](url)", "comments": ["Did you try using [TFLITE_BUILTINS, SELECT_TF_OPS](https://www.tensorflow.org/lite/guide/ops_select) to convert your model? Thanks!", "@ymodak  I have tried using  TFLITE_BUILTINS, SELECT_TF_OPS  options. But the same error occurs.\r\n\r\n    input_arrays = [\"image\"]\r\n    output_arrays = [\"Add\"]\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(pbPath, input_arrays, output_arrays)\r\n    converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\r\n    tflite_model = converter.convert()\r\n    open(tflitePath, \"wb\").write(tflite_model)\r\n\r\n\r\nif I only use the option TFLITE_BUILTINS, the Error is same to mentioned above. \r\nif I only use the option  SELECT_TF_OPS, the Error is as following:\r\n\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-03-14 19:22:41.572495: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: Pad\r\n2019-03-14 19:22:41.583638: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: Transpose\r\n2019-03-14 19:22:41.583764: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: Conv2D\r\n2019-03-14 19:22:41.583786: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: Transpose\r\n2019-03-14 19:22:41.583797: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: Mul\r\n2019-03-14 19:22:41.583808: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: Add\r\n2019-03-14 19:22:41.583821: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: Relu\r\n2019-03-14 19:22:41.583834: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: PyFunc\r\n2019-03-14 19:22:41.583849: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)\r\nFatal Python error: Aborted\r\n", "get the same problem when trying to convert from session, code as below:\r\n`\r\n\r\n    converter = tf.lite.TFLiteConverter.from_session(\r\n                        sess, [inputs], [outputs])\r\n    converter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\n    input_arrays = converter.get_input_arrays()\r\n    converter.quantized_input_stats = {input_arrays[0] : (0., 1.)}  # mean, std_dev\r\n    tflite_model = converter.convert()\r\n    open(\"small_unet.tflite\", \"wb\").write(tflite_model)`", "Based on the CHECK failure, my guess is that the type information in that particular NodeDef is missing. Could you check the attributes of that NodeDef? How is this graph generated?", "Any Update?", "Any Update?", "same error, maybe, any update?)", "Same error", "Any update?", "Hi,\r\n\r\nSorry for the update. Could you try use tf-nightly which enables the new MLIR converter by default?\r\n\r\nI believe this is an issue with the old TOCO converter which is deprecated. Thanks!", "Have you solved your problems? I have met the same question!", "I have met the same problem no solved", "> get the same problem when trying to convert from session, code as below:\r\n> `\r\n> \r\n> ```\r\n> converter = tf.lite.TFLiteConverter.from_session(\r\n>                     sess, [inputs], [outputs])\r\n> converter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\n> input_arrays = converter.get_input_arrays()\r\n> converter.quantized_input_stats = {input_arrays[0] : (0., 1.)}  # mean, std_dev\r\n> tflite_model = converter.convert()\r\n> open(\"small_unet.tflite\", \"wb\").write(tflite_model)`\r\n> ```\r\n\r\nany update?", "similarly, i ues python scripts to convert my SavedModel to lite format,but get the same error above,besides i use command line to convert,i get more error message:\r\n```\r\n2020-04-28 10:17:51.741003: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2020-04-28 10:17:51.741526: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"NoOp\" device_type: \"CPU\"') for unknown op: NoOp\r\n2020-04-28 10:17:51.741785: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"NoOp\" device_type: \"GPU\"') for unknown op: NoOp\r\n2020-04-28 10:17:51.742032: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostRecv\" device_type: \"GPU\" host_memory_arg: \"tensor\"') for unknown op: _HostRecv\r\n2020-04-28 10:17:51.742328: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Send\" device_type: \"CPU\"') for unknown op: _Send\r\n2020-04-28 10:17:51.742579: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostRecv\" device_type: \"CPU\"') for unknown op: _HostRecv\r\n2020-04-28 10:17:51.742888: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Send\" device_type: \"GPU\"') for unknown op: _Send\r\n2020-04-28 10:17:51.743179: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Recv\" device_type: \"CPU\"') for unknown op: _Recv\r\n2020-04-28 10:17:51.743399: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostSend\" device_type: \"GPU\" host_memory_arg: \"tensor\"') for unknown op: _HostSend\r\n2020-04-28 10:17:51.743697: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Recv\" device_type: \"GPU\"') for unknown op: _Recv\r\n2020-04-28 10:17:51.743994: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostSend\" device_type: \"CPU\"') for unknown op: _HostSend\r\n2020-04-28 10:17:51.744290: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"CPU\"') for unknown op: WrapDatasetVariant\r\n2020-04-28 10:17:51.744547: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: WrapDatasetVariant\r\n2020-04-28 10:17:51.744915: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"CPU\"') for unknown op: UnwrapDatasetVariant\r\n2020-04-28 10:17:51.745166: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: UnwrapDatasetVariant\r\n2020-04-28 10:17:51.745559: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\r\n2020-04-28 10:17:51.745775: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2020-04-28 10:17:51.745986: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\r\n2020-04-28 10:17:51.749493: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV3\r\n2020-04-28 10:17:51.749813: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: NonMaxSuppressionV3\r\n2020-04-28 10:17:51.750256: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CropAndResize\r\n2020-04-28 10:17:51.750446: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: CropAndResize\r\n2020-04-28 10:17:51.750635: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CropAndResize\r\n2020-04-28 10:17:51.750885: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: CropAndResize\r\n2020-04-28 10:17:51.751075: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CropAndResize\r\n2020-04-28 10:17:51.751259: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: CropAndResize\r\n2020-04-28 10:17:51.751449: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CropAndResize\r\n2020-04-28 10:17:51.751639: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: CropAndResize\r\n2020-04-28 10:17:51.752170: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DenseToDenseSetOperation\r\n2020-04-28 10:17:51.752374: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: DenseToDenseSetOperation\r\n2020-04-28 10:17:51.752614: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2020-04-28 10:17:51.752883: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\r\n2020-04-28 10:17:51.753200: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2020-04-28 10:17:51.753378: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\r\n2020-04-28 10:17:51.753555: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2020-04-28 10:17:51.753809: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\r\n2020-04-28 10:17:51.754059: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3\r\n2020-04-28 10:17:51.754248: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayV3\r\n2020-04-28 10:17:51.754448: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3\r\n2020-04-28 10:17:51.754642: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayV3\r\n2020-04-28 10:17:51.754866: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2020-04-28 10:17:51.755043: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\r\n2020-04-28 10:17:51.755228: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2020-04-28 10:17:51.755405: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\r\n2020-04-28 10:17:51.755581: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2020-04-28 10:17:51.755824: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\r\n2020-04-28 10:17:51.756000: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2020-04-28 10:17:51.756178: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\r\n2020-04-28 10:17:51.756357: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3\r\n2020-04-28 10:17:51.756561: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayScatterV3\r\n2020-04-28 10:17:51.756843: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2020-04-28 10:17:51.757028: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\r\n2020-04-28 10:17:51.757201: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2020-04-28 10:17:51.757381: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\r\n2020-04-28 10:17:51.757568: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: LoopCond\r\n2020-04-28 10:17:51.757818: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: LoopCond\r\n2020-04-28 10:17:51.758011: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit\r\n2020-04-28 10:17:51.758182: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Exit\r\n2020-04-28 10:17:51.758421: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3\r\n2020-04-28 10:17:51.758620: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayReadV3\r\n2020-04-28 10:17:51.758896: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArraySizeV3\r\n2020-04-28 10:17:51.759079: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArraySizeV3\r\n2020-04-28 10:17:51.759284: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayGatherV3\r\n2020-04-28 10:17:51.759473: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayGatherV3\r\n2020-04-28 10:17:51.759756: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV3\r\n2020-04-28 10:17:51.759961: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: NonMaxSuppressionV3\r\n2020-04-28 10:17:51.760172: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DenseToDenseSetOperation\r\n2020-04-28 10:17:51.760355: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: DenseToDenseSetOperation\r\n2020-04-28 10:17:51.760588: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3\r\n2020-04-28 10:17:51.760812: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayWriteV3\r\n2020-04-28 10:17:51.761182: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CropAndResize\r\n2020-04-28 10:17:51.761369: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: CropAndResize\r\n2020-04-28 10:17:51.761563: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CropAndResize\r\n2020-04-28 10:17:51.761805: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: CropAndResize\r\n2020-04-28 10:17:51.762121: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CropAndResize\r\n2020-04-28 10:17:51.762294: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: CropAndResize\r\n2020-04-28 10:17:51.762476: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CropAndResize\r\n2020-04-28 10:17:51.762647: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: CropAndResize\r\n2020-04-28 10:17:51.882698: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1509 operators, 2776 arrays (0 quantized)\r\n2020-04-28 10:17:51.972229: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1509 operators, 2776 arrays (0 quantized)\r\n2020-04-28 10:17:52.639779: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 625 operators, 1351 arrays (0 quantized)\r\n2020-04-28 10:17:52.676911: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 617 operators, 1343 arrays (0 quantized)\r\n2020-04-28 10:17:52.712779: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 615 operators, 1339 arrays (0 quantized)\r\n2020-04-28 10:17:52.748882: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 615 operators, 1339 arrays (0 quantized)\r\n2020-04-28 10:17:52.777368: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 615 operators, 1339 arrays (0 quantized)\r\n2020-04-28 10:17:52.813057: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 1208238720 bytes, theoretical optimal value: 805306560 bytes.\r\n2020-04-28 10:17:52.822540: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, CAST, CONCATENATION, CONV_2D, DIV, EQUAL, EXP, EXPAND_DIMS, FULLY_CONNECTED, GATHER, GATHER_ND, GREATER, GREATER_EQUAL, LESS, LOG, LOGICAL_AND, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, PADV2, RANGE, REDUCE_MAX, RESHAPE, RESIZE_NEAREST_NEIGHBOR, ROUND, SHAPE, SOFTMAX, SPARSE_TO_DENSE, SPLIT, SQRT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TOPK_V2, TRANSPOSE_CONV, UNIQUE, WHERE. Here is a list of operators for which you will need custom implementations: CropAndResize, DenseToDenseSetOperation, Enter, Exit, LoopCond, Merge, NonMaxSuppressionV3, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\nTraceback (most recent call last):\r\n  File \"e:\\anaconda3\\envs\\tensorflow-gpu-1-14\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"e:\\anaconda3\\envs\\tensorflow-gpu-1-14\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"E:\\Anaconda3\\envs\\tensorflow-gpu-1-14\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"e:\\anaconda3\\envs\\tensorflow-gpu-1-14\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"e:\\anaconda3\\envs\\tensorflow-gpu-1-14\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"e:\\anaconda3\\envs\\tensorflow-gpu-1-14\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"e:\\anaconda3\\envs\\tensorflow-gpu-1-14\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"e:\\anaconda3\\envs\\tensorflow-gpu-1-14\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, CAST, CONCATENATION, CONV_2D, DIV, EQUAL, EXP, EXPAND_DIMS, FULLY_CONNECTED, GATHER, GATHER_ND, GREATER, GREATER_EQUAL, LESS, LOG, LOGICAL_AND, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, PADV2, RANGE, REDUCE_MAX, RESHAPE, RESIZE_NEAREST_NEIGHBOR, ROUND, SHAPE, SOFTMAX, SPARSE_TO_DENSE, SPLIT, SQRT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TOPK_V2, TRANSPOSE_CONV, UNIQUE, WHERE. Here is a list of operators for which you will need custom implementations: CropAndResize, DenseToDenseSetOperation, Enter, Exit, LoopCond, Merge, NonMaxSuppressionV3, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\n```\r\nthat maybe the real problem while using python scripts don't show", "Update podungo thambi", "I am trying to convert yolov4.weights file to tflite file in colab but I am getting the same \r\nCan you guide me how to solve this issue\r\n[error.txt](https://github.com/tensorflow/tensorflow/files/6823449/error.txt)\r\n"]}, {"number": 26695, "title": "Show tflite version when interpreter initialize", "body": "", "comments": ["@haozha111 I have used `TFLITE_VERSION_STRING` directly because `tensorflow/lite/experimental` is not linked as of now. Could you please review?", "@haozha111 Could you please kindly review this PR, I have resolved the conflict and uploaded", "Please don't land this yet, we have some internal tests that it will break. I already have a CL to update the version, so I'm going to close this for now."]}, {"number": 26694, "title": "remove unnecessary package in activations.py", "body": "I removed the unnecessary import of package \"six\" in activations.py and modified the parts of code using this package.\r\n\r\nPlease review @rthadur ", "comments": []}, {"number": 26693, "title": "remove Node for tflite", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No):NO\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.** \r\n\r\nThanks.\r\n\r\n\r\n**Will this change the current api? How?** NO\r\n\r\n**Who will benefit with this feature?** Everyone\r\n\r\n**Any Other info.** None\r\n", "comments": ["@ilous12 Could you explain your issue with more details? Thanks!", "hi @jvishnuvardhan \r\n\r\nI uploaded two Visualized image for models. Network nodes are different and I painted a circle on our tflite network. \r\n", "please let me know if you know to exclude operations.\r\n\r\nI tried below.\r\n\r\n```\r\nPB_FILE=datasets/pascal_voc_seg/exp/train_on_trainval_set_mobilenetv2/export/frozen_inference_graph.pb\r\nDIMENSION=257\r\n\r\ntflite_convert\r\n --output_file=deeplab_${DIMENSION}_gpu.tflite\r\n --graph_def_file=$PB_FILE\r\n --output_format=TFLITE\r\n --input_arrays=sub_7\r\n --output_arrays=ResizeBilinear_2\r\n --inference_input_type=FLOAT\r\n --inference_type=FLOAT\r\n```", "I resolved. Thanks", "@ilous12 If possible, provide a solution here so that it might be helpful to the community. Thanks!", "#26474 \r\n\r\nNot able to obtain speed up of Depthwise Conv2d Version 2, as flatten_atrous_conv is applied. Need suggestion on what kind of other conversion mechanism is there to obtain the advantages of Depthwiseconv2d.. Is there any other mechanism other than applying flatten_atrous_conv, to remove batch_to_space and space_to_batch?\r\n", "@ilous12 how did you  exclude operations?? I don't think its possible"]}, {"number": 26692, "title": "Adding support for Linux s390x", "body": "As discussed in https://github.com/tensorflow/tensorflow/issues/26135 creating a PR to fix below test cases on s390x:\r\n\r\n//tensorflow/python:nn_test\r\n//tensorflow/contrib/distributions:independent_test\r\n//tensorflow/contrib/layers:normalization_test\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26692) for more info**.\n\n<!-- need_sender_cla -->", "@kbhute-ibm please sign CLA", "Hmm... changing the dtype clearly does the trick, but it seems broken that we have to. big endian shouldn't be inherently less accurate, right? I'm just wondering whether this simply papers over a bug.", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26692) for more info**.\n\n<!-- ok -->", "Hi @martinwicke,\r\n\r\nThanks for the comment. We already had a discussion with @gunan [here](https://github.com/tensorflow/tensorflow/issues/26135#issuecomment-467963074). Please let me know if you need any more details.\r\n\r\nThanks :)", "Hi @rthadur @martinwicke,\r\nAny inputs please? ", "I looked at #26135, I still don't understand why big endian systems would require a different threshold (I didn't see a discussion of that in the other PR -- just a verification that it's not flaky). \r\n\r\nThis doesn't have to block this -- but I'm a little concerned that we're hiding a problem instead of fixing it. Unless big endian architectures have inherently less numerical precision (which seems hard to believe), this change to the tests is hard to justify.", "Hi @martinwicke, \r\nI will confirm if this is related to [this](https://github.com/tensorflow/tensorflow/pull/12963) Numpy bug and [this](https://github.com/tensorflow/tensorflow/issues/11431) github issue. Please share your inputs on this if any.\r\n\r\nTill I reach to some point, could you please guide me in knowing the impact of these test case failures on TensorFlow functionality?\r\n", "Yes, that numpy bug could very well be the root cause. If your theory is correct, I'd love for this to be fixed in a workaround similar to https://github.com/tensorflow/tensorflow/pull/12963 (instead of fixing the tests). If you cannot find a root cause, we can merge this instead.", "Hi @martinwicke \r\nWe debugged the code further for //tensorflow/python:nn_test. The test case `python ./tensorflow/python/ops/nn_test.py MomentsTest.testOutput2DInput01` fails with following values:\r\n`mu = 1e3`, `sigma = 0.1` and `keep_dims = False`. When it hits `nparray = values.astype(dtype.as_numpy_dtype)` [here](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/framework/tensor_util.py#L416) the array values changes for dtype=tf.float32. However if its dtype=tf.float64, the values remain constant on s390x.\r\n\r\n```\r\n(Pdb) p values\r\narray([[ 1000.01846343,  1000.07748109,  1000.05901822, ...,\r\n         1000.09638957,  1000.09352149,  1000.04648203],\r\n       [ 1000.0527755 ,  1000.06733742,  1000.0641509 , ...,\r\n         1000.04154237,  1000.09075805,  1000.03772178],\r\n       [ 1000.03718904,  1000.03666347,  1000.09894839, ...,\r\n         1000.00636682,  1000.09297756,  1000.00986914],\r\n       ...,\r\n       [ 1000.03859482,  1000.03546114,  1000.05678384, ...,\r\n         1000.00672955,  1000.07011743,  1000.04866641],\r\n       [ 1000.06618747,  1000.05226396,  1000.09398577, ...,\r\n         1000.08058321,  1000.02070808,  1000.03280688],\r\n       [ 1000.00648291,  1000.09405838,  1000.03243373, ...,\r\n         1000.05311184,  1000.08248722,  1000.02524148]])\r\n(Pdb) p dtype.as_numpy_dtype\r\n<type 'numpy.float64'>\r\n(Pdb) values.astype(dtype.as_numpy_dtype)\r\narray([[ 1000.01846343,  1000.07748109,  1000.05901822, ...,\r\n         1000.09638957,  1000.09352149,  1000.04648203],\r\n       [ 1000.0527755 ,  1000.06733742,  1000.0641509 , ...,\r\n         1000.04154237,  1000.09075805,  1000.03772178],\r\n       [ 1000.03718904,  1000.03666347,  1000.09894839, ...,\r\n         1000.00636682,  1000.09297756,  1000.00986914],\r\n       ...,\r\n       [ 1000.03859482,  1000.03546114,  1000.05678384, ...,\r\n         1000.00672955,  1000.07011743,  1000.04866641],\r\n       [ 1000.06618747,  1000.05226396,  1000.09398577, ...,\r\n         1000.08058321,  1000.02070808,  1000.03280688],\r\n       [ 1000.00648291,  1000.09405838,  1000.03243373, ...,\r\n         1000.05311184,  1000.08248722,  1000.02524148]])\r\n\r\n(Pdb) dtype1.as_numpy_dtype\r\n<type 'numpy.float32'>\r\n(Pdb) values.astype(dtype1.as_numpy_dtype)\r\narray([[ 1000.01849365,  1000.07745361,  1000.059021  , ...,\r\n         1000.09637451,  1000.09350586,  1000.04650879],\r\n       [ 1000.05279541,  1000.06732178,  1000.06414795, ...,\r\n         1000.04156494,  1000.09075928,  1000.03771973],\r\n       [ 1000.03717041,  1000.03668213,  1000.09893799, ...,\r\n         1000.00634766,  1000.09295654,  1000.0098877 ],\r\n       ...,\r\n       [ 1000.03857422,  1000.03546143,  1000.0567627 , ...,\r\n         1000.00671387,  1000.07012939,  1000.04864502],\r\n       [ 1000.06616211,  1000.05224609,  1000.09399414, ...,\r\n         1000.08056641,  1000.02069092,  1000.03283691],\r\n       [ 1000.00646973,  1000.09405518,  1000.03240967, ...,\r\n         1000.05310059,  1000.0824585 ,  1000.02526855]], dtype=float32)\r\n```\r\n\r\nDebugging is still going on but please share your inputs on same if any as it would streamline our investigation.\r\n\r\nThanks,\r\nKoumudini", "Hi @martinwicke \r\nFew more findings on the investigation:\r\n\r\nOn Intel:\r\n```\r\n(Pdb) input_shape = (3,3)\r\n(Pdb) sigma = 0.1\r\n(Pdb) mu = 1000.0\r\n(Pdb) m = np.random.rand(*input_shape) * sigma + mu\r\n(Pdb) m.astype(np.float32)\r\narray([[1000.09625, 1000.0288 , 1000.06494],\r\n       [1000.0495 , 1000.02466, 1000.0644 ],\r\n       [1000.0143 , 1000.031  , 1000.09705]], dtype=float32)\r\n(Pdb) m.astype(np.float64)\r\narray([[1000.09628103, 1000.02878154, 1000.06495031],\r\n       [1000.04949626, 1000.02468053, 1000.06442176],\r\n       [1000.01426861, 1000.03101769, 1000.09702152]])\r\n```\r\n\r\nOn Z:\r\n```\r\n(Pdb) input_shape = (3,3)\r\n(Pdb) sigma = 0.1\r\n(Pdb) mu = 1000.0\r\n(Pdb) m = np.random.rand(*input_shape) * sigma + mu\r\n(Pdb) m.astype(np.float32)\r\narray([[ 1000.034729  ,  1000.06030273,  1000.09344482],\r\n       [ 1000.07342529,  1000.01916504,  1000.06616211],\r\n       [ 1000.08764648,  1000.09729004,  1000.07043457]], dtype=float32)\r\n(Pdb) m.astype(np.float64)\r\narray([[ 1000.03471172,  1000.06030737,  1000.09343678],\r\n       [ 1000.07343308,  1000.01916033,  1000.06618081],\r\n       [ 1000.08764095,  1000.09729445,  1000.07041618]])\r\n```\r\n\r\nI wonder in case of `m.astype(np.float32)` where does `dtype=float32` come from in output. Could you please give us some pointers on this? \r\n\r\nIn case of [this](https://github.com/tensorflow/tensorflow/pull/12963) issue, ndarray is created with `np.ndarray(shape=(), buffer=np.array([input_shape.ndims], dtype=np.int32), dtype=np.int32)` command, whereas in this case, it get randomly created with `np.random.rand(*input_shape) * sigma + mu`. The issue looks similar but not sure where to exactly look into. Please suggest.\r\n\r\nThanks.", "Hi @martinwicke ,\r\nAs I mentioned in one of my earlier comments, original NumPy array with basetype float64 is cast into float32. As per comments from @mrry and @zffchen78 on [this issue](https://github.com/tensorflow/tensorflow/issues/14017#issuecomment-352573861) this behavior of casting can not be changed. \r\n\r\nIn this test case, `nparray.tostring()` changes its byte ordering on BE. This is NumPy's native behavior. To fix this issue we have a workaround as below, please let us know your suggestion on same:\r\n\r\n```diff\r\n import numpy as np\r\n import six\r\n+import sys\r\n from tensorflow.core.framework import tensor_pb2\r\n from tensorflow.core.framework import tensor_shape_pb2\r\n from tensorflow.python.framework import ops\r\n@@ -409,11 +409,16 @@ def make_tensor_proto(values, dtype=None, shape=None, verify_shape=False):\r\n           dtypes.qint8, dtypes.quint8, dtypes.qint16, dtypes.quint16,\r\n           dtypes.qint32\r\n       ])\r\n+  isbasetypesame = False\r\n   # We first convert value to a numpy array or scalar.\r\n   if isinstance(values, (np.ndarray, np.generic)):\r\n     if dtype:\r\n-      nparray = values.astype(dtype.as_numpy_dtype)\r\n+      # Do not cast `values` to float32 on big endian\r\n+      if sys.byteorder == 'big':\r\n+        nparray = values\r\n+      else:\r\n+        nparray = values.astype(dtype.as_numpy_dtype)\r\n+        isbasetypesame = dtype.base_dtype != numpy_dtype.base_dtype\r\n@@ -471,8 +476,7 @@ def make_tensor_proto(values, dtype=None, shape=None, verify_shape=False):\r\n   if is_quantized:\r\n     numpy_dtype = dtype\r\n\r\n-  if dtype is not None and (not hasattr(dtype, \"base_dtype\") or\r\n-                            dtype.base_dtype != numpy_dtype.base_dtype):\r\n+  if dtype is not None and (not hasattr(dtype, \"base_dtype\") or isbasetypesame):\r\n     raise TypeError(\"Incompatible types: %s vs. %s. Value is %s\" %\r\n                     (dtype, nparray.dtype, values))\r\n\r\n```\r\n\r\nAdding @gunan to the discussion for more suggestions. Thanks.", "@martinwicke , please post your response.", "Just to make sure I understand correctly, would the diff you showed fix the problems?\r\n\r\nI don't think it's terribly problematic to change the casting behavior on BE only, given that a bug in numpy makes the behavior inconsistent on the platform. \r\n\r\n@mrry in case I'm getting this wrong.", "Is there a small reproduction that shows the problem, without involving the details of these unit tests? I don't understand what's being proposed.", "Hi @martinwicke @mrry \r\nI had shifted to some other test case failures and will get back to this one soon. ", "This PR is being discussed in one of the related open issues. We will close this if not valid.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Can one of the admins verify this patch?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}]