[{"number": 4008, "title": "Change device_setter.py for choose ps device", "body": "Add arguments \"ps_device_name\" for replica_device_setter(), in order to choose ps decive .\nReference the issue[#3762](https://github.com/tensorflow/tensorflow/issues/3762)\n", "comments": ["Can one of the admins verify this patch?\n", "I'm not sure I see the point of this change. Can't you pass (e.g.) `\"/job:ps/cpu:0\"` to the existing `ps_device` argument if (as in #3762) you want to put all the variables on the CPU by default?\n", "@mrry  This is the code block in def replica_device_setter()\n\n```\n ps_job_name = ps_device.lstrip(\"/job:\")\n if ps_job_name not in cluster_spec or cluster_spec[ps_job_name] is None:\n      return None\n```\n\nif the argument `ps_device=\"/job:ps/cpu:0\"` it will return None\n", "OK, that `lstrip()` is completely wrong. Thanks for pointing it out.\n\nMore properly, it should do:\n\n``` python\nps_job_name = pydev.DeviceSpec.from_string(ps_device).job\n```\n\nDo you want to try making that change in this PR?\n", "@mrry  Yes and thanks for your reply\n", "@mrry I have changed it \n", "Great, thanks! Can you also add a test that covers this case to `device_setter_test.py`?\n\nI imagine you could copy `testPS2TasksWithDevice()` and make it `testPS2TasksWithCPUConstraint()`. The code would look something like:\n\n``` python\n  def testPS2TasksWithCPUConstraint(self):\n    cluster_spec = tf.train.ClusterSpec({\n        \"sun\": [\"sun0:2222\", \"sun1:2222\", \"sun2:2222\"],\n        \"moon\": [\"moon0:2222\", \"moon1:2222\"]})\n\n    with tf.device(tf.train.replica_device_setter(\n        ps_device=\"/job:moon/cpu:0\", worker_device=\"/job:sun\",\n        cluster=cluster_spec.as_cluster_def())):\n      v = tf.Variable([1, 2])\n      w = tf.Variable([2, 1])\n      a = v + w\n      self.assertDeviceEqual(\"/job:moon/task:0/cpu:0\", v.device)\n      self.assertDeviceEqual(\"/job:moon/task:0/cpu:0\", v.initializer.device)\n      self.assertDeviceEqual(\"/job:moon/task:1/cpu:0\", w.device)\n      self.assertDeviceEqual(\"/job:moon/task:1/cpu:0\", w.initializer.device)\n      self.assertDeviceEqual(\"/job:sun\", a.device)\n```\n", "@mrry  I have added testPS2TasksWithCPUConstraint() in device_setter_test.py, any other tests should be add?\n", "@tensorflow-jenkins test this please.\n", "Looks good to me! Thanks for making this change.\n", "Thanks for the contribution!\n"]}, {"number": 4007, "title": "Request for TF example  \"DeepMind : Teaching Machines to Read and Comprehend\" ", "body": "Deepmind's paper:Teaching Machines to Read and Comprehend\n[http://arxiv.org/pdf/1506.03340v3.pdf](url)\n will be a good example for RNN with **attention**. \nHowever, an unofficial Tensorflow implementation has been suspended over 7 months.\n[https://github.com/carpedm20/attentive-reader-tensorflow](url)\n\nOther theano implementations do not demonstrate how to visualize the attention weight:\n[https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend](url)\n[https://github.com/adbrebs/rnn_reader](url)\n", "comments": ["This is a general usage question best suited for StackOverflow.  Please re-ask your question there.\n"]}, {"number": 4006, "title": "Possible bug in tensorboard \"EVENT\" display?", "body": "Hi, I wanted to display the training accuracy of my model. However, I found that it could't display when I updated with tensorflow 0.10 rc version. \n\nI checked two different situations:\nFirstly, it do have values when I download the csv file, as show in below figure.\n![tensorboard_v0 10](https://cloud.githubusercontent.com/assets/7037235/17919903/803c2bf4-6a03-11e6-969e-d23358c40387.PNG)\n\nSecondly, it can display the accuracy successfully when I rollback tensorflow version to 0.9, as show in below figure.\n![tensorboard_v0 9](https://cloud.githubusercontent.com/assets/7037235/17920056/991e1f5a-6a04-11e6-8782-23d7bac342b8.PNG)\n\nIs that a bug in tensorflow 0.10? \n", "comments": ["What browser/OS are you on? Can you open the JS console and tell us if there are any visible errors?\n", "Hi danmane,\n\nI used Google Chrome and a modified version of redhat 7. And here is the JS console error info:\n![error_tf0 10](https://cloud.githubusercontent.com/assets/7037235/18031181/106cebcc-6d07-11e6-8505-298623baeffd.PNG)\nBTW, I used remote access tensorboard method in my local machine. The data is located in a remote server.\n\nThanks\nJinlong\n", "@jinhou is that still happening with the latest version?", "Closing due to inactivity. Feel free to re-open if you would like us to look again."]}, {"number": 4005, "title": "Can we have a feature for max out activation function?", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["See https://github.com/tensorflow/tensorflow/pull/5528\n", "Closing in line with abandoned PR."]}, {"number": 4004, "title": "Update tf_op_files.txt to add tile_6.cc", "body": "To fix makefile\n", "comments": ["@tensorflow-jenkins test this please\n"]}, {"number": 4003, "title": "Print Version of cuDNN Being Used", "body": "Right now what you get is device info for example:\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GRID K520\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.797\npciBusID 0000:00:03.0\nTotal memory: 4.00GiB\nFree memory: 3.95GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\n```\n\nTheano prints:\n\n```\nUsing gpu device 0: GRID K520 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 4007)\n```\n\ngiving you device info but also info on the version of cuDNN.\n", "comments": ["There should also be a log message with the cuDDN shared library version from [here] ?(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/dso_loader.cc#L108)\n", "No version though:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n```\n", "When there is a mismatch, might also be cool to print what compute capabilities TF was built with next to the compute capabilities of the device. Here, when I say mismatch I mean not an exact match: e.g. built with 3.5 but the card is 3.7.\n", "OK - I'm pretty sure I see versions on the .so files when I run internally.  \nI agree that having the fill path would be useful to triage library version issues.\n", "@zheng-xq  These sort of diagnostics might be useful - is there an obvious place to add some extra log info?  \n", "Stream-executor has both the build time and compile time information of Cudnn. Adding Jason, the owner of stream-executor here. \n", "Yes, it's just embedded in cuda_dnn.cc.  Might be worth somehow surfacing that when we load the DSO.   People already don't like the fact that we log this to output though, so I'm worried about adding a separate LOG statement :)\n", "Also notice that Description has already a `ToMap()` that could be consumed to output all the key/values.  I agree that is unexpected to see all dump of information and at the beginning looks like the library is not working. \nMaybe add a verbosity env parameter to control when to output the device description ?\n", "The problem is that, most of the time, this information shows up in logs that people use to report to us and is actually useful.  It would make our life harder to help people if we turned that off by default. :(   Maybe one day when GPUs are just easier to use, we could turn it off by default.\n", "I think this is related to #4090 .\n\nWould it be possible to not only dump the CUDA, cuDNN version and Compute capabilities into the log, but also make them available as python variables in the TensorFlow module?\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\ndeduoing against https://github.com/tensorflow/tensorflow/issues/10827"]}, {"number": 4002, "title": "Ignoring gpu device with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.", "body": "I build TF with: \n\n```\nTF_UNOFFICIAL_SETTING=1\nTF_CUDA_COMPUTE_CAPABILITIES=3.0\n```\n\nwhich used to work but within the last few days I now get:\n\n```\nIgnoring gpu device (device: 0, name: GRID K520, pci bus id: 0000:00:03.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n```\n\nI wonder if this got broken with: https://github.com/tensorflow/tensorflow/commit/353235e0b2b35f3df43f42ef84ca00ccda7a3a6d\n", "comments": ["Perhaps I now need `CUDA_COMPUTE_CAPABILITIES=3.0` ?\n", "you can use cudnn 4.0,\n", "The issue is that the env flag changed.\n", "From the discussion in #3269, it appears that the `bazel fetch` command ran by the `configure` script used the correct CUDA compute capabilities but the values are somehow changed when running `bazel build`.\n", "@davidzchen @vrv Great! Can we get it cherry-picked into r0.10?\nThanks!\n", "Yeah, there are a few things I need to cherry-pick into r0.10 today, so I'll try to get to it.\n", "@cancan101 @flx42 - To confirm, did this patch fix this issue for you?\n", "I assumed you already verified before merging :(.\n", "Looks fine for me.\n", "@vrv This patch works on my end, and was a needed cleanup change in any case. I wanted to be sure in case there were any issues involving Bazel re-fetching, which I recall running into at one point. Sorry, I should have been more clear in my comment.\n"]}, {"number": 4001, "title": "iOS ld error", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN:  No\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.  \n   from source\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.10.0rc0\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`) \n   cc3153a7a0a23533d14ead34db37e4ccd7892079\n2. The output of `bazel version`  \n   0.3.1-homebrew\n### Steps to reproduce\n1. ./configure\n2.  sh tensorflow/contrib/makefile/build_all_ios.sh\n   or sh sh tensorflow/contrib/makefile/compile_ios_tensorflow.sh\n### What have you tried?\n1.   only compile the libtensorflow-core-<arm arch>.a  on different platform and lipo them to get libtensorflow-core.a\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nall arm arch have got these logs.\n\n\"tensorflow::functor::ReduceAndReshape<Eigen::ThreadPoolDevice, float, 6, 1>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 6, 1, long>, 16>, Eigen::TensorMap<Eigen::Tensor<float const, 6, 1, long>, 16>, Eigen::DSizes<long, 1> const&, Eigen::DSizes<long, 6> const&) const\", referenced from:\n      void tensorflow::TileGradientOpEigen::ThreadPoolDevice::HandleReduce<float, 6, 1>(tensorflow::OpKernelContext_, std::__1::vector<int, std::__1::allocator<int> > const&, tensorflow::Tensor_) in libtensorflow-core-arm64.a(tile_ops.o)\n  \"tensorflow::functor::TileGrad<Eigen::ThreadPoolDevice, int, 6>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<int, 6, 1, long>, 16>, Eigen::TensorMap<Eigen::Tensor<int const, 6, 1, long>, 16>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 6> const&, bool) const\", referenced from:\n      void tensorflow::TileGradientOpEigen::ThreadPoolDevice::HandleCaseImpl<(tensorflow::DataType)3, 6>(tensorflow::OpKernelContext_, std::__1::vector<int, std::__1::allocator<int> > const&, tensorflow::gtl::ArraySlice<int> const&, tensorflow::Tensor_) in libtensorflow-core-arm64.a(tile_ops.o)\n  \"tensorflow::functor::TileGrad<Eigen::ThreadPoolDevice, float, 6>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 6, 1, long>, 16>, Eigen::TensorMap<Eigen::Tensor<float const, 6, 1, long>, 16>, Eigen::DSizes<long, 6> const&, Eigen::DSizes<long, 6> const&, bool) const\", referenced from:\n      void tensorflow::TileGradientOpEigen::ThreadPoolDevice::HandleCaseImpl<(tensorflow::DataType)1, 6>(tensorflow::OpKernelContext_, std::__1::vector<int, std::__1::allocator<int> > const&, tensorflow::gtl::ArraySlice<int> const&, tensorflow::Tensor_) in libtensorflow-core-arm64.a(tile_ops.o)\n  \"tensorflow::functor::Tile<Eigen::ThreadPoolDevice, int, 6>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<int, 6, 1, long>, 16>, Eigen::TensorMap<Eigen::Tensor<int const, 6, 1, long>, 16>, std::__1::array<int, 6ul> const&) const\", referenced from:\n      void tensorflow::TileOpEigen::ThreadPoolDevice::HandleCaseImpl<(tensorflow::DataType)3, 6>(tensorflow::OpKernelContext_, tensorflow::gtl::ArraySlice<int> const&, tensorflow::Tensor_) in libtensorflow-core-arm64.a(tile_ops.o)\n  \"tensorflow::functor::ReduceAndReshape<Eigen::ThreadPoolDevice, int, 6, 1>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<int, 6, 1, long>, 16>, Eigen::TensorMap<Eigen::Tensor<int const, 6, 1, long>, 16>, Eigen::DSizes<long, 1> const&, Eigen::DSizes<long, 6> const&) const\", referenced from:\n      void tensorflow::TileGradientOpEigen::ThreadPoolDevice::HandleReduce<int, 6, 1>(tensorflow::OpKernelContext_, std::__1::vector<int, std::__1::allocator<int> > const&, tensorflow::Tensor_) in libtensorflow-core-arm64.a(tile_ops.o)\n  \"tensorflow::functor::Tile<Eigen::ThreadPoolDevice, float, 6>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 6, 1, long>, 16>, Eigen::TensorMap<Eigen::Tensor<float const, 6, 1, long>, 16>, std::__1::array<int, 6ul> const&) const\", referenced from:\n      void tensorflow::TileOpEigen::ThreadPoolDevice::HandleCaseImpl<(tensorflow::DataType)1, 6>(tensorflow::OpKernelContext_, tensorflow::gtl::ArraySlice<int> const&, tensorflow::Tensor_) in libtensorflow-core-arm64.a(tile_ops.o)\nld: symbol(s) not found for architecture arm64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nmake: **\\* [/Users/chh/Dev_Ling/tensorflow/tensorflow/contrib/makefile/gen/bin/ios_ARM64/benchmark] Error 1\narm64 compilation failed.\n", "comments": ["https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/tf_op_files.txt#L14\nif you add tile_ops_cpu_impl_6.cc to that list, does it work?\n", "It's long time to compile the ios lib\nIt works! Thank you again.\nAnd I think there is no need to compile x86 and x86-64 on ios platform.\nIn the compile_ios_tensorflow.sh, there are \n\nmake -f tensorflow/contrib/makefile/Makefile \\\nTARGET=IOS IOS_ARCH=I386 LIB_NAME=${LIB_PREFIX}-i386.a OPTFLAGS=\"$1\" $2 $3\nif [ $? -ne 0 ]\nthen\n  echo \"i386 compilation failed.\"\n  exit 1\nfi\n\nmake -f tensorflow/contrib/makefile/Makefile \\\nTARGET=IOS IOS_ARCH=X86_64 LIB_NAME=${LIB_PREFIX}-x86_64.a OPTFLAGS=\"$1\" $2 $3\nif [ $? -ne 0 ]\nthen\n  echo \"x86_64 compilation failed.\"\n  exit 1\nfi\n\nand  'lipo x86 , x64'  increases the static lib size.\n", "The reason for the x86/x64 architectures is so we can run simulator targets. It looks like this is fixed, so I'm closing, but do re-open if you have more information!\n"]}, {"number": 4000, "title": "Disable tf_stream_executor in the CMake build.", "body": "Temporary workaround for issue #3996.\n", "comments": ["Tested this fix locally, so hopefully it should get through the CI tests....\n", "Third time's a charm. @rmlarsen: should be ready to merge now.\n"]}, {"number": 3999, "title": "The problem about data flow graph?", "body": "Hi everyone:\n     I am a beginner for tensorflow, and do not understand something about the demo in tutorial files, like following:\n![image](https://cloud.githubusercontent.com/assets/12611573/17914869/688d864a-69d8-11e6-8692-4c3e41e695bf.png)\n     During testing, how can accuracy call the trained model for mnist.test.images and mnist.test.labels.\n     Does the trained model flow to the accuracy?\n     Can anyone help me? Thanks a lot!\n", "comments": ["this question is more suitable for stackoverflow.\n", "@JohnnyY8 `accuracy` is just a node of the graph, a bfs can traversal the whole graph.\n", "@suiyuan2009 Thank you. \n"]}, {"number": 3998, "title": "Enable building tensorflow as a submodule in cuda_configure", "body": "Add support for path_prefix and tf_repo_name so that projects importing tf as a submodule can build with the new CUDA autoconf.\n", "comments": ["This was fixed by @damienmg in #4055. The corresponding fix for Tensorflow Serving is tensorflow/serving#166.\n"]}, {"number": 3997, "title": "Disable CMake build of tf_stream_executor", "body": "Temporary mitigation for #3996.\n", "comments": ["Thanks @mrry I'll merge once this passes.\n", "Running tests again - needed to fix up a dependency in debug_ops.h.\n", "Urgh, that did not go as planned. Looks like the easiest solution will be to remove debug_ops from the CMake build. I'll send another PR for that.\n"]}, {"number": 3996, "title": "Fix CMake build due to cuda_configure changes", "body": "The new changes from #3269 broke the CMake build due to the header include path changes and the new `cuda_config.h` file generated by `cuda_configure`.\n", "comments": ["This is now fixed, thanks to @guschmue's great work on the Windows GPU build. There are still some wrinkles regarding CMake-on-Linux (see #5465), but I'm going to close this issue for now.\n", "Thanks for fixing this, @guschmue!\n"]}, {"number": 3995, "title": "extend tf.select to broadcast a scalar condition #3945", "body": "This is WIP. The test fails on my machine for the GPU version. Comments are welcome.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "from the CI_BUILD Is hard to understand where is the problem. But debugging locally I find it segfaults on the gpu's 'cond()' statement.  I tried to trace down what actually happens but that is dark Eigen magic.  \n", "I will have to patch this in and try to repro on my machine. Will try to do so later this evening.\n", "@Mistobaan What's the status here? Should I poke some Googlers to give you some more feedback, e.g. on SelectScalarFunction thread?\n", "@danmane this was so long ago that I have to check. I think the GPU kernel fails.\n", "@tensorflow-jenkins Test this please.\n", "Can you rebase on head & fix conflicts?\n", "@Mistobaan the simplest thing to do is the HostMemory annotation, but @ebrevdo's solution should work as well. The copying should be justifiable if the other inputs don't have to be moved off of the GPU as a result.\n\nCan you make those changes and rebase?\n", "As mentioned above, if the cond scalar is forced on host memory, then there's little benefit to using this over tf.cond(), which is exactly the same thing.\n\nI don't think this PR makes sense if the 'cond' input is host memory, it's just adding more complexity for no benefit.\n", "I will go with the custom cuda kernel solution\n", "Thanks Mistobaan, closing since you're going to go another path. Please re-open if I got it wrong.\n", "@drpngx can yo reopen this? I just commit the changes that @benoitsteiner suggested and seem to work on my machine. \n", "github says it can't be re-opened because it was force-pushed or re-created. Did you do that?\n", "@drpngx Yes I did rebase and force push to my branch. I will open a new PR then\n"]}, {"number": 3994, "title": "Cannot use Variables as gradients in apply_gradients", "body": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py#L287 prevents the user from using a variable as a gradient when calling apply_gradients.\n\nI'd like to do this to accumulate gradients over multiple minibatches, and then do a single gradient update.\n\nThe current code I have is\n\n``` python\nopt = tf.train.AdamOptimizer()                                                                                                   \n\ntvs = tf.trainable_variables()\naccum_vars = [tf.Variable(tf.zeros_like(tv.initialized_value()), trainable=False) for tv in tvs]                                        \nzero_ops = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]\n\ngvs = opt.compute_gradients(rmse, tvs)\naccum_ops = [accum_vars[i].assign_add(gv[0]) for i, gv in enumerate(gvs)]\n\ntrain_step = opt.apply_gradients([(accum_vars[i], gv[1]) for i, gv in enumerate(gvs)])  \n```\n\nthat I'd like to run with logic like\n\n```\nwhile True:\n    sess.run(zero_ops)\n\n    for i in xrange(n_minibatches):\n        sess.run(accum_ops, feed_dict=dict(X: Xs[i], y: ys[i]))\n\n    sess.run(train_step)\n```\n\nIs there any reason variables cannot be used as arguments to apply_gradients? It seems to me that they should be able to be used as gradients. If there is a good reason, is there a recommended way to have the pattern I desire? I'm currently using the ugly hack of replacing the `train_step` definition with \n\n```\ntrain_step = opt.apply_gradients([(accum_vars[i].assign(accum_vars[i]), gv[1]) \n                                  for i, gv in enumerate(gvs)])\n```\n\nbecause `Variable.assign` returns a tensor.\n", "comments": ["I found the `Variable.value()` method that seems like a nicer workaround than the `assign` (no-)op.\nFrom reading the docs, it seems like like a good fix would be for `apply_gradients` to call `convert_to_tensor`. \n", "This is a general usage question best suited to StackOverflow.  Could you please re-ask there.\n", "Unless there is a technical reason variables cannot be used, I think this is a bug (or at least unintuitive behavior) in `apply_gradients` that should be fixed.\n", "It's an inconsistency that doesn't bite many of our users, and it should be an easy fix. I'm going to mark this as \"contributions welcome\", since it should be an easy pull request to prepare and review.\n", "Thanks Derek!  Eric, please feel free to send us the fix with a test, if you can! \n", "Will do this evening!\n"]}, {"number": 3993, "title": "time_major parameter for ctc_loss op", "body": "Just as dynamic_rnn and to be more regular across the TensorFlow's code I added a flag capable of transpose the input data to be in time_major, required by gen_ctc_ops._ctc_loss. Note that, instead of set time_major=False (as the most of TensorFlow data), it was set False to be compatible with previous ctc_loss function call (but I don't think that this is a good idea).\n", "comments": ["Can one of the admins verify this patch?\n", "Can you resolve conflicts?\n", "Jenkins, test this please\n", "@ebrevdo What do you mean? I'm new on this.\n", "Can you sync you repo to the latest version of the master and resolve any merge conflicts that might occur?\n", "@rmlarsen I did it, but I don't have any clue if I did it right. :)\n", "@tensorflow-jenkins test this please\n", "@igormq looks like you did it right :) \ud83d\udc4d \n@ebrevdo would you mind reviewing this?\n", "May have you wait till Monday\n\nOn Aug 25, 2016 5:45 PM, \"Rasmus Munk Larsen\" notifications@github.com\nwrote:\n\n> @igormq https://github.com/igormq looks like you did it right :) \ud83d\udc4d\n> @ebrevdo https://github.com/ebrevdo would you mind reviewing this?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3993#issuecomment-242589381,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim85ksue6DGBbRNOqt8Ad4-v9GQAJks5qjjc9gaJpZM4JrVaR\n> .\n", "Err to wait\n\nOn Aug 25, 2016 6:11 PM, \"Eugene Brevdo\" ebrevdo@gmail.com wrote:\n\n> May have you wait till Monday\n> \n> On Aug 25, 2016 5:45 PM, \"Rasmus Munk Larsen\" notifications@github.com\n> wrote:\n> \n> > @igormq https://github.com/igormq looks like you did it right :) \ud83d\udc4d\n> > @ebrevdo https://github.com/ebrevdo would you mind reviewing this?\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > https://github.com/tensorflow/tensorflow/pull/3993#issuecomment-242589381,\n> > or mute the thread\n> > https://github.com/notifications/unsubscribe-auth/ABtim85ksue6DGBbRNOqt8Ad4-v9GQAJks5qjjc9gaJpZM4JrVaR\n> > .\n", "@ebrevdo Please take a look\n", "Jenkins, test this please.\n", "One tiny docstring change; then it's good to merge.\n", "Comment here when you've modified the docstring.\n", "@ebrevdo I've finished. \ud83d\udc4d \n", "@tensorflow-jenkins Test this please\n", "@andrewharp It looks like you can merge this.\n"]}, {"number": 3992, "title": "Add support for pyx_library in the open-source build.", "body": "I have a model that has a lot of big constant Tensors (2048x2048), and building the graph is extremely slow. After profiling my code, I see that the bottlenecks are the functions `SlowAppend<TYPE>ArrayToTensorProto`  defined in `tensorflow/python/framework/tensor_util.py` which take 98% of the time. \n\nReviewing the source code I see the following lines:\n\n```\n# TODO(opensource): Add support for pyx_library in the open-source build.\n# For now, we use the slow versions that fast_tensor_util replaces.\n# pylint: disable=g-import-not-at-top\n```\n\nSo I was wondering if it is planned to add this feature in the short term, and if not, any advice on how to speed up the graph building process? \n\nThanks\n", "comments": ["> So I was wondering if it is planned to add this feature in the short term, \n\nWe are unable to comment on timeframes for specific feature requests.  \n\n>  any advice on how to speed up the graph building process?\n\nThis is a general TensorFlow usage question best suited for StackOverflow.  Please can you re-ask there. \n(In general, you would be better off avoiding adding large constants to your program.  Consider reading them as input, or computing them once at runtime.)\n", "@prb12 Sorry to open this issue again and thanks for the suggestion of using placeholders / computing them on runtime. \n\nHowever, I have a follow up question related to the implementation of the`SlowAppend<TYPE>ArrayToTensorProto` functions. Seems that most of these functions follow the pattern:\n\n```\ndef SlowAppend<TYPE>ArrayToTensorProto(tensor_proto, proto_values):\n    tensor_proto.<TYPE>_val.extend([np.asscalar(x) for x in proto_values])\n```\n\nI was wondering if there is an special reason for not using the `tolist()` method from numpy, as in:\n\n```\n def SlowAppend<TYPE>ArrayToTensorProto(tensor_proto, proto_values):\n    tensor_proto.<TYPE>_val.extend(proto_values.tolist())\n```\n\nThis is much faster for large arrays, and doing some tests seems that the values obtained using both methods are the same (value & type). So I just wanted to check if I am missing something.\nThe only reason for not using `tolist()` I can think of is that it does not return the same type as `np.asscalar()`. Reading the documentation for `tolist()` it says:\n\n> Data items are converted to the nearest compatible Python type.\n\nwhereas for `np.asscalar()` it reads:\n\n> A copy of the specified element of the array as a suitable Python scalar\n\nIsn't the nearest compatible Python type the same as the suitable Python scalar? I just wanted to check because changing this in the code is super-fast and the speed-up for big constant tensors is really huge.\n\nThanks again!\n", "@keveman  I realize the functions are named `SlowAppend` ... but looks like they could be slightly less slow?\n"]}, {"number": 3991, "title": "import tensorflow: Library not loaded: libcudart 7.5 (Mac OSX)", "body": "### Environment info\n\nOperating System: Mac OSX El Capitan\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nWhat is CUDA and cuDNN? I didn't notice any mention of these!?\n\n`Optional: Install CUDA (GPUs on Linux)` but I'm on a Mac!!\n\n```\n$ ls -l /path/to/cuda/lib/libcud*\nls: /path/to/cuda/lib/libcud*: No such file or directory\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n`ImportError ... Library not loaded: @rpath/libcudart.7.5.dylib`\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### Steps to reproduce\n\nFollowing install instructions for Mac OSX: http://learningtensorflow.com/lesson1/\nhttps://www.tensorflow.org/versions/r0.8/get_started/os_setup.html\n\nJust realised there is one for 0.10 as well: https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html\n### What have you tried?\n\nI tried to follow the install instructions.\n\n```\n21:37 $ source activate tensorflow\n(tensorflow) \u2714 \n21:38 $ pip install --ignore-installed --upgrade $TF_BINARY_URL\nCollecting tensorflow==0.10.0rc0 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.10.0rc0-py3-none-any.whl\n  Downloading https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.10.0rc0-py3-none-any.whl (94.1MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 94.1MB 7.1kB/s \nCollecting numpy>=1.10.1 (from tensorflow==0.10.0rc0)\n  Using cached numpy-1.11.1-cp35-cp35m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\nCollecting six>=1.10.0 (from tensorflow==0.10.0rc0)\n  Using cached six-1.10.0-py2.py3-none-any.whl\nCollecting protobuf==3.0.0b2 (from tensorflow==0.10.0rc0)\n  Using cached protobuf-3.0.0b2-py2.py3-none-any.whl\nCollecting wheel>=0.26 (from tensorflow==0.10.0rc0)\n  Using cached wheel-0.29.0-py2.py3-none-any.whl\nCollecting setuptools (from protobuf==3.0.0b2->tensorflow==0.10.0rc0)\n  Using cached setuptools-26.0.0-py2.py3-none-any.whl\nInstalling collected packages: numpy, six, setuptools, protobuf, wheel, tensorflow\nSuccessfully installed numpy-1.11.1 protobuf-3.0.0b2 setuptools-25.1.6 six-1.10.0 tensorflow-0.10.0rc0 wheel-0.29.0\n(tensorflow) \u2714 ~/repos/aurelia-projs/ai/ai-component [master|\u2714] \n\n21:40 $ python\nPython 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:52:12) \n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/kristianmandrup/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Users/kristianmandrup/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 48, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/Users/kristianmandrup/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/Users/kristianmandrup/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n  File \"/Users/kristianmandrup/anaconda/envs/tensorflow/lib/python3.5/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/Users/kristianmandrup/anaconda/envs/tensorflow/lib/python3.5/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: dlopen(/Users/kristianmandrup/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.7.5.dylib\n  Referenced from: /Users/kristianmandrup/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\n  Reason: image not found\n```\n", "comments": ["Seems the trouble is this: http://stackoverflow.com/questions/38710339/library-not-loaded-rpath-libcudart-7-5-dylib-tensorflow-error-on-mac\n", "I had this issue just after upgrading to 10.11.  I had been using the GPU version of Tensorflow just fine in 10.10.  I reinstalled CUDA SDK for 10.11.   I also tried reinstalling tensorflow w/ pip and from source with the same issue.  I could only resolve it by restarting in Recovery mode (CMD+R during start up), and then open Terminal, and run \"csrutil disable\", to disable System Integration Protection or something along those lines.  I think without doing this, 10.11 will not check DYLD/LD_LIBRARY_PATH.  In any case, hope this helps someone, or that they can shed some light on whether this was a bad idea or not (seems slightly bad).\n", "There are multiple ways to get this error, only one of which is having the wrong TF (cpu vs gpu). I've run into this, and all paths, env, CUDAs are straight, but per @pkmital I suspect I'll have to dispable 10.11's SIP \ud83d\udc4e \n", "I've seen similar issue with SIP. Was getting \"@rpath/libcudart.7.5.dylib\" error, disable SIP, it started working. Upgraded El Capitan (this re-enabled SIP), it stopped working. Disabled SIP, started working again.\n", "hi @pkmital  - I came across your solution.\n\nI'm running in the same problems.\n``Library not loaded: @rpath/libcudart.7.5.dylib\n\nIn my case, I'm installing for Python 2.7.x; TensorFlow 0.10.  Using TF's install instructions to download CUDA first (using homebrew, it automatically downloaded CUDA-8.0)\n\nIs your solution for CUDA-8.0 or for CUDA-7.5?\n\nThanks!\n", "CUDA-7.5 because TF official binaries (11rc0 or before) don't suport CUDA 8\n", "I was able to resolve my problem by _uninstalling_ tensorflow (which I had installed using the `https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0rc0-py3-none-any.whl` & my anaconda's pip). \n\nInstead I did `conda install -c conda-forge tensorflow=0.10.0` per [https://anaconda.org/conda-forge/tensorflow](https://anaconda.org/conda-forge/tensorflow)\n\nMy system is El Capitan (10.11.6), Python 3.5.2 :: Anaconda custom (x86_64), conda 4.1.6\n", "Did you have NVIDA GPU on your Mac?\r\nIf not then don't use the CUDA version of the tensor flow.\r\nUninstall tensor flow and reinstall the CPU version of the tensor flow you will be able to bypass the error."]}, {"number": 3990, "title": "TypeError: evaluate() got an unexpected keyword argument 'batch_size'", "body": "ok\uff0cWhen I run this code as follow, I have some error.\n**Code:**\nThis code is from https://github.com/mouradmourafiq/tensorflow-lstm-regression ,\nI found  the error may be result from some code:\n_validation_monitor = learn.monitors.ValidationMonitor(X['val'], y['val'],\n                                                      every_n_steps=PRINT_STEPS,\n                                                      early_stopping_rounds=1000)\nregressor.fit(X['train'], y['train'], monitors=[validation_monitor], logdir=LOG_DIR)_\n\n**Then, here is the error message:**\nINFO:tensorflow:Error reported to Coordinator: <type 'exceptions.TypeError'>, evaluate() got an unexpected keyword argument 'batch_size'\nINFO:tensorflow:Error reported to Coordinator: <type 'exceptions.TypeError'>, evaluate() got an unexpected keyword argument 'batch_size'\nTraceback (most recent call last):\n  File \"model.py\", line 178, in <module>\n    regressor.fit(X['train'], y['train'],monitors=[validation_monitor], logdir=LOG_DIR)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 166, in fit\n    monitors=monitors)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 578, in _train_model\n    max_steps=max_steps)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 280, in _supervised_train\n    None)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/supervised_session.py\", line 270, in run\n    run_metadata=run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/recoverable_session.py\", line 54, in run\n    run_metadata=run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/coordinated_session.py\", line 70, in run\n    self._coord.join(self._coordinated_threads_to_join)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 357, in join\n    six.reraise(_self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/coordinated_session.py\", line 66, in run\n    return self._sess.run(_args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitored_session.py\", line 107, in run\n    induce_stop = monitor.step_end(monitors_step, monitor_outputs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py\", line 396, in step_end\n    return self.every_n_step_end(step, output)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py\", line 687, in every_n_step_end\n    steps=self.eval_steps, metrics=self.metrics, name=self.name)\nTypeError: evaluate() got an unexpected keyword argument 'batch_size'\n@mouradmourafiq  \n### Environment info\n\nOperating System: \nubuntu14.04 python2.7 tensorflow with only CPU\n", "comments": ["This code is not part of the tensorflow project.  Please consider asking the author of that code, or post your request for assistance on StackOveflow.\n", "Did you solve the problem ?\n", "I met the exact same issue when using tflearn. Has anybody been able to fix it?\n", "i guess the tensorflow api has changed and that s why you get this error.\n"]}, {"number": 3989, "title": "Error Building Docker Image", "body": "Using `Dockerfile.devel-gpu` branched from 459c2fed498530b794c4871892fd68d1e6834ac6.\n\n```\nConfiguration finished\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 406\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 340, in _create_cuda_repository\n        _find_cudnn_lib_path(repository_ctx, cudnn_install_base..., ...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 249, in _find_cudnn_lib_path\n        fail(\"Cannot find %s or %s under %s\" ...))\nCannot find lib64/libcudnn.so.5 or libcudnn.so.5 under /usr/local/cuda.\n____Elapsed time: 1.274s\nThe command '/bin/sh -c ./configure &&     bazel build --local_resources 3072,3.0,1.0 -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl' returned a non-zero code: 1\n\n./build-all.sh returned exit code 1\n```\n", "comments": ["Please feel free to reach out to me if you need any help with this.\n", "@gunan Inside `nvidia/cuda:7.5-cudnn5-devel`, it looks like the `so` moved:\n\n```\n# ls -l /usr/lib/x86_64-linux-gnu | grep dnn\nlrwxrwxrwx 1 root root       29 Aug 12 05:11 libcudnn.so -> /etc/alternatives/libcudnn_so\nlrwxrwxrwx 1 root root       17 Jun 10 13:07 libcudnn.so.5 -> libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 60696704 Jun 10 13:07 libcudnn.so.5.1.3\nlrwxrwxrwx 1 root root       32 Aug 12 05:11 libcudnn_static.a -> /etc/alternatives/libcudnn_stlib\n-rw-r--r-- 1 root root 59715990 Jun 10 13:07 libcudnn_static_v5.a\n```\n\nwhereas nothing:\n\n```\n# ls /usr/local/cuda/lib64 | grep dnn\n```\n", "looking at our nightly build failures, this looks different.\nI will try to look into both today.\n", "@davidzchen asked us this question: https://github.com/NVIDIA/nvidia-docker/issues/172\nI think it's solved now? https://github.com/tensorflow/tensorflow/pull/3269\n", "I am seeing this issue on https://github.com/tensorflow/tensorflow/commit/459c2fed498530b794c4871892fd68d1e6834ac6 which I think includes the linked PR.\n", "Just modify this line:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu#L90\n\nIt was properly changed in the CI Dockerfile:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.gpu#L25\n", "@davidzchen to also get his input.\n", "Yes, from the discussion in NVIDIA/nvidia-docker#172 and #3269, the line:\n\n``` dockerfile\nENV CUDNN_INSTALL_PATH /usr/local/cuda\n```\n\nseems to be incorrect since in the Docker image, `libcudnn.so` is installed under `/usr/lib/x86_64-linux-gnu` and not `/usr/local/cuda/lib64`. The line should be changed to:\n\n``` dockerfile\nENV CUDNN_INSTALL_PATH /usr/lib/x86_64-linux-gnu\n```\n\nas we have done so in `tensorflow/tools/ci_build/Dockerfile.gpu`.\n", "I expect that users might have cuDNN at any of those two locations since we offer deb and tarball downloads. Ideally those two locations should be checked automatically and `CUDNN_INSTALL_PATH` could be reserved for unusual cuDNN install paths.\n", "> I expect that users might have cuDNN at any of those two locations since we offer deb and tarball downloads. \n\nShould not be relevant for the Docker image where the install location is controlled.\n", "True, but it's a maintenance burden, if you want to experiment with the image you have to remember to modify this value.\nFor instance, if you switch to `nvidia/cuda:8.0-cudnn5-devel`, cuDNN is in `/usr/local/cuda` again since we don't have public deb packages for cuDNN 5 on CUDA 8.0 yet.\n", "for cuda7.5 images, I have corrected all the paths as suggested by @flx42 \nClosing the issue now.\n", "I just hit this trying to build with cuda:8.0-cudnn5-devel, sounds more or less like the same issue?\n\n```\nNo Google Cloud Platform support will be enabled for TensorFlow\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nExtracting Bazel installation...\n____Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 412\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 344, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 233, in _find_cudnn_header_dir\n        fail(\"Cannot find cudnn.h under %s\" %...)\nCannot find cudnn.h under /usr/local/cuda-8.0/targets/x86_64-linux/lib.\nConfiguration finished\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 412\n        _create_cuda_repository(repository_ctx)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 344, in _create_cuda_repository\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\n    File \"/tensorflow/third_party/gpus/cuda_configure.bzl\", line 233, in _find_cudnn_header_dir\n        fail(\"Cannot find cudnn.h under %s\" %...)\nCannot find cudnn.h under /usr/lib/x86_64-linux-gnu.\n____Elapsed time: 0.391s\nThe command '/bin/sh -c ./configure &&     bazel bu\n```\n", "Submitted #4397.\n", "@flx42 I encountered with the same question like this.\r\nERROR: error loading package 'tensorflow/examples/android': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': error loading package 'external': The repository named 'local_config_cuda' could not be resolved.\r\n\r\nbut I don't get how to modify Dockerfile.devel-gpu,here is mine\r\nENV CI_BUILD_PYTHON python\r\nENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\nENV TF_NEED_CUDA 1\r\nENV TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,5.2,6.0,6.1\r\n\r\nanything is wrong?", "@wrs777 looks like a different issue. The original issue here should have been fixed."]}, {"number": 3988, "title": "Branch 131066649", "body": "", "comments": ["abandoning for a new push\n"]}, {"number": 3987, "title": "About tf.contrib.slim data prepare documentation", "body": "I have spend much time to understand how to prepare my data to feed into Slim Net. \nBut I still can't quiet understand how to prepare my training data.\nIs there a more detailed method for processing the training data, like Caffe ImageNet tutorial:http://caffe.berkeleyvision.org/gathered/examples/imagenet.html\n", "comments": ["This is a general question best suited to StackOverflow.   Please can you re-ask your question there.\n"]}, {"number": 3986, "title": "Freeze graph: node is not in graph (even though it's been named)", "body": "### Environment info\n\nOperating System: Ubuntu 14.04 LTS 64-bit\n\nInstalled version of CUDA and cuDNN: none\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`): fc9162975e52978d3af38549b570cc3cc5f0ab66\n2. The output of `bazel version`\n\n```\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n```\n### Steps to reproduce\n1. Copy the IPython Notebook for Assignment 6 of Udacity's deep learning course ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb))\n2. Change `saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))` to `saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]), name=\"saved_sample_output\")`\n3. Modify the code like so:\n\n``` python\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print('Initialized')\n  mean_loss = 0\n  # code omitted (no changes)\n  # new code below:\n  saver = tf.train.Saver(tf.all_variables())\n  saver.save(session, '/home/me/Documents/checkpoint.ckpt', write_meta_graph=False)\n  tf.train.write_graph(graph.as_graph_def(), '/home/me/Documents', 'graph.pb')\n```\n1. Run.\n2. Verify that `checkpoint.ckpt` and `graph.pb` have been successfully created in the directory.\n3. In the tensorflow source directory, run:\n\n```\nbazel build tensorflow/python/tools:freeze_graph && bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/home/me/Documents/graph.pb --input_checkpoint=/home/me/Documents/checkpoint.ckpt --output_graph=/home/me/Documents/frozen_graph.pb --output_node_names=saved_sample_output\n```\n### What have you tried?\n\nChecked the `graph.pb` file to make sure that node had actually been named properly. Seems like it was:\n\n```\n# other stuff\nnode {\n  name: \"saved_sample_output\"\n  op: \"Variable\"\n  attr {\n    key: \"container\"\n    value {\n      s: \"\"\n    }\n  }\n  attr {\n    key: \"dtype\"\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: \"shape\"\n    value {\n      shape {\n        dim {\n          size: 1\n        }\n        dim {\n          size: 64\n        }\n      }\n    }\n  }\n  attr {\n    key: \"shared_name\"\n    value {\n      s: \"\"\n    }\n  }\n}\n# etc.\n```\n\nI'm pretty much stumped with this one since [this](https://stackoverflow.com/questions/38958662/tensorflow-what-are-the-output-node-names-for-freeze-graph-py-in-the-model-wi) issue on StackOverflow says to pass in a name parameter for the node you want, which is what I did, to no avail (even without the name parameter, it still gave the same error).\n**Edit:** Got freeze_graph to run successfully with the `sample_prediction` node (changed `sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))` to `sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b), name=\"sample_prediction\")`). However, I still haven't figured out why that worked, and this didn't.\n### Logs or other output that would be helpful\n\n```\nTraceback (most recent call last):\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 134, in <module>\n    tf.app.run()\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 131, in main\n    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 120, in freeze_graph\n    sess, input_graph_def, output_node_names.split(\",\"))\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/graph_util.py\", line 232, in convert_variables_to_constants\n    inference_graph = extract_sub_graph(input_graph_def, output_node_names)\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/graph_util.py\", line 156, in extract_sub_graph\n    assert d in name_to_node_map, \"%s is not in graph\" % d\nAssertionError: saved_sample_output is not in graph\n```\n", "comments": ["I meet the similar issue.\n", "@petewarden I get the same error when I try to freeze_graph.py. I would appreciate if you could help me with this: \r\n\r\nWARNING: /home/pouyanj/.cache/bazel/_bazel_pouyanj/0e1d84e60e1e91548f16e1302589059b/external/protobuf/protobuf.bzl:90:19: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nWARNING: /home/pouyanj/.cache/bazel/_bazel_pouyanj/0e1d84e60e1e91548f16e1302589059b/external/protobuf/protobuf.bzl:96:28: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nWARNING: /home/pouyanj/.cache/bazel/_bazel_pouyanj/0e1d84e60e1e91548f16e1302589059b/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/profiling/instrumentation.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\r\nWARNING: /home/pouyanj/.cache/bazel/_bazel_pouyanj/0e1d84e60e1e91548f16e1302589059b/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/profiling/profiler.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\r\nWARNING: /home/pouyanj/.cache/bazel/_bazel_pouyanj/0e1d84e60e1e91548f16e1302589059b/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/bit_depth.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\r\nWARNING: /home/pouyanj/.cache/bazel/_bazel_pouyanj/0e1d84e60e1e91548f16e1302589059b/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/gemmlowp.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\r\nWARNING: /home/pouyanj/.cache/bazel/_bazel_pouyanj/0e1d84e60e1e91548f16e1302589059b/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/map.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\r\nWARNING: /home/pouyanj/.cache/bazel/_bazel_pouyanj/0e1d84e60e1e91548f16e1302589059b/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/output_stages.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\r\nINFO: Found 1 target...\r\nTarget //tensorflow/python/tools:freeze_graph up-to-date:\r\n  bazel-bin/tensorflow/python/tools/freeze_graph\r\nINFO: Elapsed time: 7.444s, Critical Path: 0.20s\r\nTraceback (most recent call last):\r\n  File \"/home/pouyanj/Desktop/tensorflow-r0.11/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 134, in <module>\r\n    tf.app.run()\r\n  File \"/home/pouyanj/Desktop/tensorflow-r0.11/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/home/pouyanj/Desktop/tensorflow-r0.11/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 131, in main\r\n    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\r\n  File \"/home/pouyanj/Desktop/tensorflow-r0.11/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 120, in freeze_graph\r\n    sess, input_graph_def, output_node_names.split(\",\"))\r\n  File \"/home/pouyanj/Desktop/tensorflow-r0.11/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/graph_util.py\", line 234, in convert_variables_to_constants\r\n    inference_graph = extract_sub_graph(input_graph_def, output_node_names)\r\n  File \"/home/pouyanj/Desktop/tensorflow-r0.11/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/graph_util.py\", line 158, in extract_sub_graph\r\n    assert d in name_to_node_map, \"%s is not in graph\" % d\r\nAssertionError: softmax is not in graph\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Me too!", "@kevinashaw just to be sure, did you check on `master`?", "Ping anyone?", "Closing due to lack of response.", "Hi all,\r\nI am getting this error\r\n```\r\nin extract_sub_graph\r\n    assert d in name_to_node_map, \"%s is not in graph\" % d\r\nAssertionError: predictions is not in graph,\r\n\r\n```\r\nwhat that mean sub graph?\r\n\r\nI am sure that the name of my output is correct,\r\n\r\nthis is my code for freezing:\r\n\r\n```\r\ndir = os.path.dirname(os.path.realpath(__file__))\r\ntf.train.write_graph(sess.graph_def, '/home/saria/Downloads/sentiment_analysis_tensorflow-master', 'har.pbtxt')\r\nsaver.save(sess,save_path = \"../har.ckpt\")\r\n\r\nfreeze_graph.freeze_graph(input_graph = \"../har.pbtxt\",  input_saver = \"\",\r\n             input_binary = False, input_checkpoint = \"../har.ckpt\", output_node_names = \"predictions\",\r\n             restore_op_name = \"save/restore_all\", filename_tensor_name = \"save/Const:0\",\r\n             output_graph = \"frozen_har.pb\", clear_devices = True, initializer_nodes = \"\")\r\n\r\ninput_graph_def = tf.GraphDef()\r\n\r\n```\r\nmay please some one help with this problem, I am get stuck in this error no ways for getting the right answer", "@saria85 \r\nWrite the graph as a text file using `as_text=True` in `write_graph`.\r\nOpen the graph file, it will have some node entries written in plane text.\r\nCheck the node name you are passing is in the graph file. When I had the same problem, the actual node name had some extra parts on the end.\r\n    ", "Bro check you node nameing convention. . . Naming of \"matmul function to output\" and \"x placeholder to input\". . .", "@SanthoshMKunthe Is there any possible to print all the node name in Graph? Checked the code and didn't find anything about the naming you mentioned. Thanks!", "Use json viewer. . . It helped me to solve mis naming problem. . . Its one of the solution, look for other possibilities to be sure. . . ", "I used this function to get all the node names, but more than 10000 nodes are really crazy for me to figure out the one that belong to me......  print(len([n.name for n in tf.get_default_graph().as_graph_def().node]))", "yes, the node name is incorrect according to the `.pb` file, you have to print the actual name used during the saving by `print model.output.op.name` same with input if that happened occasionally `model.input.op.name`, then you can use the printed name as an argument in the export function.\r\ni hope this solved the problem.", "@moss-khaldi how did you print the output node name while saving? It isn't available for estimators using the access method you've mentioned (model,output.op.name).", "@iamgroot42 Hi, after the building of the network use `print model.output.op.name` and by `model` i mean the name of the model you have used, i'd like to mention too that the methods i've used is concerned of the Keras API, so if you're using TensorFlow you may want to search for the appropriate function to display the names of the nodes.", "Oh. Keras wasn't mentioned in your comments anywhere; I assumed it's for Estimator. ", "Sorry, i just cleared my answer \ud83d\ude0a.", "I met similar issue, even after i print out all the node name by using `print([node.name for node in graph.as_graph_def().node])` and chose a name from them, the freeze_graph.py still return `AssertionError: MobileNet/Predictions/Softmax is not in graph`", "I got the same error, then when I printed the node names using\r\n`out_names = [x.op.name for x in model.outputs]` I got u'activation_1_1/Sigmoid' as a name but it did not work.\r\nThen I searched in the generated tmp.pb file for Sigmoid and the name I found is \"activation_1/Sigmoid\".\r\nI used it and it worked!", "@YoussefBenDhieb how did you search  in tmp.pb , can you please tell step by step.", "@dassi25 You open the tmp.pb file using an editor like Sublime or Visual Studio Code and you search for the name of the activation, in my case it is Sigmoid. Also, I had only one sigmoid activation in the graph, but even if you have many, usually it will be the last one.", "I faced the same problem, and the `saved_model_cli` command saved me. (https://www.tensorflow.org/guide/saved_model)\r\n\r\nSo if you are in a linux environment and has tensorflow installed, you can enter the following command.\r\n\r\n`saved_model_cli show --dir ./ --all`\r\n\r\nof course, use the directory path of yours\r\n\r\nIt will show something like below:\r\n\r\n    outputs['output'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 512)\r\n        name: some/kind/of/tensor/Add:0\r\n\r\nThen your name should be `some/kind/of/tensor/Add` without the numbering", "Just in case - I found the eager execution caused my problem.  ", "Hi all, \r\nIn my network, I make name of node with 'name = 'prob' as code below\r\n\r\nself.prob_output_layer = tf.nn.softmax(self.output_layer, name='prob')\r\n\r\n--> and we expect that node name should be 'prob'\r\n\r\nBut we need to check the file.pb \r\n\r\nnode {\r\n  name: \"dnn_01/Softmax\"\r\n  op: \"Softmax\"\r\n  input: \"dnn_01/fully_layer03/fully_layer03/Add_1\"\r\n  device: \"/device:GPU:0\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n}\r\n--> We found that node name is 'dnn_01/Softmax'\r\n\r\nTherefore, we need use 'dnn_01/Softmax' name when freezing ", " TF 2.3, my issue got solved by\r\n\r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n```\r\n\r\nI have no idea why this works.\r\nExplanation is welcome.\r\n", "I had the error, but my problem was that I was iterating over the `tf.GraphDef.node` and modifying that list at the same time (inserting and removing nodes) so be careful not to do that", "> I faced the same problem, and the `saved_model_cli` command saved me. (https://www.tensorflow.org/guide/saved_model)\r\n> \r\n> So if you are in a linux environment and has tensorflow installed, you can enter the following command.\r\n> \r\n> `saved_model_cli show --dir ./ --all`\r\n> \r\n> of course, use the directory path of yours\r\n> \r\n> It will show something like below:\r\n> \r\n> ```\r\n> outputs['output'] tensor_info:\r\n>     dtype: DT_FLOAT\r\n>     shape: (-1, 512)\r\n>     name: some/kind/of/tensor/Add:0\r\n> ```\r\n> \r\n> Then your name should be `some/kind/of/tensor/Add` without the numbering\r\n\r\nI think it's not working in Tensorflow 1.15.2", "> TF 2.3, my issue got solved by\r\n> \r\n> ```\r\n> import tensorflow.compat.v1 as tf\r\n> tf.disable_v2_behavior()\r\n> ```\r\n> \r\n> I have no idea why this works.\r\n> Explanation is welcome.\r\n\r\nYah its working thanks", "Hi Mr @phamdanglam1986, @InCogNiTo124, @sivasweta, @Nannigalaxy @wysohn \r\n\r\nI used command **saved_model_cli show --all --dir efficientnet_b0_classification_1/** with the model from tensorflow hub, and the output as below:\r\n\r\n```\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is: \r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['image_input'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, -1, -1, 3)\r\n        name: serving_default_image_input:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['probs'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1000)\r\n        name: StatefulPartitionedCall:0\r\n  Method name is: tensorflow/serving/predict\r\n\r\nDefined Functions:\r\n  Function Name: '__call__'\r\n    Option #1\r\n      Callable with:\r\n        Argument #1\r\n          image_input: TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='image_input')\r\n        Argument #2\r\n          DType: bool\r\n          Value: False\r\n        Argument #3\r\n          DType: NoneType\r\n          Value: None\r\n    Option #2\r\n      Callable with:\r\n        Argument #1\r\n          inputs: TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='inputs')\r\n        Argument #2\r\n          DType: bool\r\n          Value: False\r\n        Argument #3\r\n          DType: NoneType\r\n          Value: None\r\n```\r\n\r\nso how can I get the output_node_names? Please help me\r\n", "@hoaquocphan \r\n```\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['probs'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1000)\r\n        name: StatefulPartitionedCall:0\r\n```\r\n\r\nProbably it's StatefulPartitionedCall, but I am not entirely sure", "hi @wysohn \r\n\r\nit is ok with this model, but when I tried with other models from tensorflow hub  as below:\r\n```\r\nsaved_model_cli show --all --dir imagenet_mobilenet_v1_025_128_classification_4/\r\nsaved_model_cli show --all --dir imagenet_resnet_v1_50_classification_4/\r\nsaved_model_cli show --all --dir tf2-preview_inception_v3_classification_4/\r\n```\r\n\r\nthe output as below:\r\n\r\n```\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is: \r\n\r\nDefined Functions:\r\n  Function Name: '__call__'\r\n    Option #1\r\n      Callable with:\r\n        Argument #1\r\n          inputs: TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='inputs')\r\n        Argument #2\r\n          DType: bool\r\n          Value: True\r\n        Argument #3\r\n          DType: bool\r\n          Value: True\r\n        Argument #4\r\n          batch_norm_momentum: TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\r\n    Option #2\r\n      Callable with:\r\n        Argument #1\r\n          inputs: TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='inputs')\r\n        Argument #2\r\n          DType: bool\r\n          Value: True\r\n        Argument #3\r\n          DType: bool\r\n          Value: False\r\n        Argument #4\r\n          batch_norm_momentum: TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\r\n    Option #3\r\n      Callable with:\r\n        Argument #1\r\n          inputs: TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='inputs')\r\n        Argument #2\r\n          DType: bool\r\n          Value: False\r\n        Argument #3\r\n          DType: bool\r\n          Value: False\r\n        Argument #4\r\n          batch_norm_momentum: TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\r\n    Option #4\r\n      Callable with:\r\n        Argument #1\r\n          inputs: TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='inputs')\r\n        Argument #2\r\n          DType: bool\r\n          Value: False\r\n        Argument #3\r\n          DType: bool\r\n          Value: True\r\n        Argument #4\r\n          batch_norm_momentum: TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\r\n```\r\n\r\nso how can I get the  **output_node_names**? Please help me", "@hoaquocphan \r\n\r\n> hi @wysohn\r\n> \r\n> it is ok with this model, but when I tried with other models from tensorflow hub as below:\r\n> \r\n> ```\r\n> saved_model_cli show --all --dir imagenet_mobilenet_v1_025_128_classification_4/\r\n> saved_model_cli show --all --dir imagenet_resnet_v1_50_classification_4/\r\n> saved_model_cli show --all --dir tf2-preview_inception_v3_classification_4/\r\n> ```\r\n> \r\n> the output as below:\r\n> \r\n> ```\r\n> MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n> \r\n> signature_def['__saved_model_init_op']:\r\n>   The given SavedModel SignatureDef contains the following input(s):\r\n>   The given SavedModel SignatureDef contains the following output(s):\r\n>     outputs['__saved_model_init_op'] tensor_info:\r\n>         dtype: DT_INVALID\r\n>         shape: unknown_rank\r\n>         name: NoOp\r\n>   Method name is: \r\n> \r\n> Defined Functions:\r\n>   Function Name: '__call__'\r\n>     Option #1\r\n>       Callable with:\r\n>         Argument #1\r\n>           inputs: TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='inputs')\r\n>         Argument #2\r\n>           DType: bool\r\n>           Value: True\r\n>         Argument #3\r\n>           DType: bool\r\n>           Value: True\r\n>         Argument #4\r\n>           batch_norm_momentum: TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\r\n>     Option #2\r\n>       Callable with:\r\n>         Argument #1\r\n>           inputs: TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='inputs')\r\n>         Argument #2\r\n>           DType: bool\r\n>           Value: True\r\n>         Argument #3\r\n>           DType: bool\r\n>           Value: False\r\n>         Argument #4\r\n>           batch_norm_momentum: TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\r\n>     Option #3\r\n>       Callable with:\r\n>         Argument #1\r\n>           inputs: TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='inputs')\r\n>         Argument #2\r\n>           DType: bool\r\n>           Value: False\r\n>         Argument #3\r\n>           DType: bool\r\n>           Value: False\r\n>         Argument #4\r\n>           batch_norm_momentum: TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\r\n>     Option #4\r\n>       Callable with:\r\n>         Argument #1\r\n>           inputs: TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='inputs')\r\n>         Argument #2\r\n>           DType: bool\r\n>           Value: False\r\n>         Argument #3\r\n>           DType: bool\r\n>           Value: True\r\n>         Argument #4\r\n>           batch_norm_momentum: TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\r\n> ```\r\n> \r\n> so how can I get the **output_node_names**? Please help me\r\n\r\nFor that, I am not really sure how to figure that out. It might not be named tensor or something, but it's out of my knowledge. There is a quote in the documentation, however, and it might provide some clue\r\n\r\n> Key Point: Unless you need to export your model to an environment other than TensorFlow 2.x with Python, you probably don't need to export signatures explicitly. If you're looking for a way of  enforcing an input signature for a specific function, see the input_signature argument to tf.function.\r\n"]}, {"number": 3985, "title": "Refactored GPU installation from source issue (Ubuntu 16.04) ", "body": "It looks like the GPU installation from source got refactored and is failing to compile on my Ubuntu 16.04, CUDA 8.0, cuDNN 5.0, with GTX 1080 GPU.\n\nI had the GTX 1080 working with the build from a few days ago, but just pulled from the head and now cannot compile so I am not sure if this is the auto_configure for the GPU or not I saw going though.\n\n$ git rev-parse HEAD\n6d04d601e9e8758ec4642fa9d548b7321d804d63\n\n$ bazel version\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n\n$ ./configure\n\nGPU configured\n\nINFO: All external dependencies fetch successfully.\nConfiguration finished\n\n$ bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package  1>~/initial_output.txt 2>&1 \n[initial_output.txt](https://github.com/tensorflow/tensorflow/files/432724/initial_output.txt)\n\nGPU CUDA includes were not found so I added line to new file: CROSSTOOL.tpl @ line 66\n\n  cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include\"\n\n$ bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package  1>~/second_output.txt 2>&1 \n[second_output.txt](https://github.com/tensorflow/tensorflow/files/432726/second_output.txt)\n\nI think it is complaining about not finding \"crosstool_wrapper_driver_is_not_gcc\".\n\nI did a \"locate\" and my version seems to be at the following:\n$ locate crosstool_wrapper_driver_is_not_gcc\n/home/greg/serving/tensorflow/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n/home/greg/serving/tf_models/syntaxnet/tensorflow/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n/home/greg/tensorflow/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\n\nLet me know where the file is that has the path to the crosstool_wrapper.... and I can update the path to test that as a fix unless you know the root cause...\n\nEDIT: BTW, the build works fine without the GPU configuration\n", "comments": ["@Mazecreator - how did you configure your build using the `configure` script? Can you paste the output with the values you entered for CUDA version, etc.?\n\nAlso, what are the contents of the directory `bazel-tensorflow/external/local_config_cuda/crosstool`, including all directories?\n", "Sure, still had this on my system up:\n\ngreg@tensor:~/tensorflow$ ./configure\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \nNo Google Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /usr/local/lib/python2.7/dist-packages\n  /usr/lib/python2.7/dist-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\n\n/usr/local/lib/python2.7/dist-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-4.9\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n", "Understood, thanks.\n\nTo check: does the directory `bazel-tensorflow/external/local_config_cuda/crosstool` exist in your workspace?\n", "Sorta, looks like this is a symbolic link at that location but the file/directory doesn't exist.\n\ngreg@tensor:~/tensorflow/bazel-tensorflow/external/local_config_cuda$ ll\ntotal 16\ndrwxrwxr-x  2 greg greg 4096 Aug 23 10:32 ./\ndrwxrwxr-x 74 greg greg 4096 Aug 23 10:32 ../\nlrwxrwxrwx  1 greg greg  105 Aug 23 10:32 crosstool -> /home/greg/.cache/bazel/_bazel_greg/f51b04b6e5afee33e06cbb5163e8cd54/external/local_config_cuda/crosstool\nlrwxrwxrwx  1 greg greg  100 Aug 23 10:32 cuda -> /home/greg/.cache/bazel/_bazel_greg/f51b04b6e5afee33e06cbb5163e8cd54/external/local_config_cuda/cuda/\ngreg@tensor:~/tensorflow/bazel-tensorflow/external/local_config_cuda$ cat crosstool\ncat: crosstool: No such file or directory\ngreg@tensor:~/tensorflow/bazel-tensorflow/external/local_config_cuda$ cd crosstool\nbash: cd: crosstool: No such file or directory\n", "Interesting. I'll attempt to reproduce this.\n\nIn the meantime, if you set your CUDA version explicitly when running the `configure` script, do you still get the same problem?\n", "I tried setting CUDA to 8.0 and it compiled with no problems.  I used the default version for cuDNN.\n\nI don't know if this still required the manual edit of adding the \"cxx_builtin_include_directory\" to the  CROSSTOOL.tpl file.  If you think that may have been resolved with your update, I can try from scratch to see if that issue is also resolved.  Adding the 8.0 for CUDA in configure did fix the last problem.\n", "Understood. Thanks!\n\nI was able to reproduce this issue and will work on root causing the problem. I have a hunch that it may be related to an issue I mentioned in [https://github.com/tensorflow/tensorflow/issues/1157#issuecomment-235426495](this comment) but I'll verify to be sure.\n", "one thing that I noticed is that you have to run `./configure` again if you do `bazel clean` \n", "@Mistobaan Yes, you need to run `./configure` again if you run `bazel clean` since running `clean` will also delete the `@local_config_cuda` repository generated by `cuda_configure`, though in general, you should need to run `bazel clean` due to Bazel's incremental rebuilds. If you find an issue that is causing your workspace to become inconsistent, feel free to bring it to our attention. As mentioned in the documentation on [`bazel clean`](http://bazel.io/docs/bazel-user-manual.html#the-clean-command):\n\n> The clean command is provided primarily as a means of reclaiming disk space for workspaces that are no longer needed. However, we recognize that Bazel's incremental rebuilds might not be perfect; clean may be used to recover a consistent state when problems arise.\n> \n> Bazel's design is such that these problems are fixable; we consider such bugs a high priority, and will do our best fix them. If you ever find an incorrect incremental build, please file a bug report. We encourage developers to get out of the habit of using clean and into that of reporting bugs in the tools.\n\n@Mazecreator It seems likely that the reason why the undeclared inclusions error is appearing if the CUDA version is not set is due to the symlink-resolved include path of the CUDA headers not matching any of the `cxx_builtin_include_directory` entries in the CROSSTOOL config. While `/usr/local/cuda` is a symlink to `/usr/local/cuda-<version>`, the build only works if we have a `cxx_builtin_include_directory` entry for the `/usr/local/cuda-<version>` but not `/usr/local/cuda`.\n\nI'll look into whether we can add support for resolving symlinks for the `cxx_builtin_include_directory` entries in Bazel or whether the correct fix is to `readlink` and add the resolved directory to the CROSSTOOL config.\n", "Thanks,\n\nLet me  know if there is something you want me to try/test.\n", "An update on this: I opened bazelbuild/bazel#1685 to add a `realpath` method for getting the canonical path of a symlink in Skylark repositories I have a patch that is currently in review internally, which I tested with the repro scenario for this bug.\n\nOnce the patch for bazelbuild/bazel#1685 is in, I'll open a PR for the fix for this bug, but we may have to wait for a new Bazel release before we can merge it since it will use the new `realpath` method to resolve `/usr/local/cuda` to the canonical path before adding the `cxx_builtin_include_directory` entry for the include dir in CROSSTOOL.\n", "Having a similar issue, although on OS X (CUDA 7.5 and cuDNN4, although have 5 if necessary):\n\nError log:\n\n```\ntensorflow $ sudo bazel build -c opt --config=cuda //tensorflow/...\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nINFO: Elapsed time: 1.629s\n```\n\n./configure settings:\n\n```\ntensorflow $ ./configure\nPlease specify the location of python. [Default is /Library/Frameworks/Python.framework/Versions/2.7/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] y\nGoogle Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\n  /Library/Python/2.7/site-packages\nPlease input the desired Python library path to use.  Default is [/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\n\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 4\nPlease specify the location where cuDNN 4 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 3.0\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nINFO: Waiting for response from Bazel server (pid 14525)...\nWARNING: /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/external/boringssl_git/WORKSPACE:1: Workspace name in /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.\nINFO: All external dependencies fetched successfully.\nConfiguration finished\n\n```\n\nThanks for any assistance \n\nInterestingly, configuring the install to not use any symlinked CUDA libraries has the exact same result:\n\n```\n$ ./configure\nPlease specify the location of python. [Default is /Library/Frameworks/Python.framework/Versions/2.7/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] y\nGoogle Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\n  /Library/Python/2.7/site-packages\nPlease input the desired Python library path to use.  Default is [/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\n\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /Developer/NVIDIA/CUDA-7.5\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 4\nPlease specify the location where cuDNN 4 library is installed. Refer to README.md for more details. [Default is /Developer/NVIDIA/CUDA-7.5]: /usr/local/cuda\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 3.0\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n..\nINFO: Waiting for response from Bazel server (pid 14734)...\nWARNING: /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/external/boringssl_git/WORKSPACE:1: Workspace name in /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.\nINFO: All external dependencies fetched successfully.\nConfiguration finished\n```\n\n```\n$ sudo bazel build -c opt --config=cuda //tensorflow/...\nPassword:\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nINFO: Elapsed time: 0.446s\n```\n", "@trevorwelch Looks like you opened #4105 for the issue that you are seeing, which seems to be a separate issue from this one.\n"]}, {"number": 3984, "title": "ERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.", "body": "I am trying to build tensorflow from source\nWhen building the pip package with bazel, I got this error:   invalid command 'bdist_wheel'\n\nBut I have python 2.7.5 and wheel 0.29.0 in my linux, could you help me out\uff1f\n1. The commit hash (`git rev-parse HEAD`):6d04d601e9e8758ec4642fa9d548b7321d804d63\n2. The output of `bazel version`:0.3.1\n### Logs:\n\n[yxwang@gpu02 tensorflow]$ wheel version\nwheel 0.29.0\n[yxwang@gpu02 tensorflow]$ python -V\nPython 2.7.5\n[yxwang@gpu02 tensorflow]$ bazel-bin/tensorflow/tools/pip_package/build_pip_package  /tmp/tensorflow_pkg\nTue Aug 23 10:37:26 EDT 2016 : === Using tmpdir: /tmp/tmp.OMZ3S3VXj1\n/tmp/tmp.OMZ3S3VXj1 ~/local/tensorflow\nTue Aug 23 10:37:27 EDT 2016 : === Building wheel\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: setup.py --help [cmd1 cmd2 ...]\n   or: setup.py --help-commands\n   or: setup.py cmd --help\n\nerror: invalid command 'bdist_wheel'\n", "comments": ["Please can you provide the information requested in the TensorFlow issues template.\n", "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n\n### Environment info\n\nOperating System: Linux version 3.10.0-327.18.2.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) )\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n ls -l /usr/local/cuda-7.5/lib64/libcud*\n-rw-r--r--. 1 root root   322936 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\nlrwxrwxrwx. 1 root root       16 Jan 25  2016 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx. 1 root root       19 Jan 25  2016 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x. 1 root root   383336 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n-rw-r--r--. 1 root root   720192 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a\n-rwxr-xr-x. 1 root root 61453024 Feb 12  2016 /usr/local/cuda-7.5/lib64/libcudnn.so\n-rwxr-xr-x. 1 root root 61453024 Feb 12  2016 /usr/local/cuda-7.5/lib64/libcudnn.so.4\n-rwxr-xr-x. 1 root root 61453024 Feb 12  2016 /usr/local/cuda-7.5/lib64/libcudnn.so.4.0.7\n-rwxr-xr-x. 1 root root 60696704 Aug  7 09:45 /usr/local/cuda-7.5/lib64/libcudnn.so.5\n-rwxr-xr-x. 1 root root 60696704 Aug  7 09:45 /usr/local/cuda-7.5/lib64/libcudnn.so.5.1.3\n-rw-r--r--. 1 root root 62025862 Feb 12  2016 /usr/local/cuda-7.5/lib64/libcudnn_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`):6d04d601e9e8758ec4642fa9d548b7321d804d63\n2. The output of `bazel version`:0.3.1\n\n### Steps to reproduce\n1. ./configure (I removed the bazel fetch from it or it won't stop downloading such things like boost, which I have in my system)\n2. bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n3. bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n\n### What have you tried?\n1. I am trying to build tensorflow from source\n   When building the pip package with bazel, I got this error: invalid command 'bdist_wheel'\n\nBut I have python 2.7.5 and wheel 0.29.0 in my linux\n\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n[yxwang@gpu02 tensorflow]$ wheel version\nwheel 0.29.0\n[yxwang@gpu02 tensorflow]$ python -V\nPython 2.7.5\n[yxwang@gpu02 tensorflow]$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\nTue Aug 23 10:37:26 EDT 2016 : === Using tmpdir: /tmp/tmp.OMZ3S3VXj1\n/tmp/tmp.OMZ3S3VXj1 ~/local/tensorflow\nTue Aug 23 10:37:27 EDT 2016 : === Building wheel\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\nor: setup.py --help [cmd1 cmd2 ...]\nor: setup.py --help-commands\nor: setup.py cmd --help\n\nerror: invalid command 'bdist_wheel'\n", "I have met a new problem, at step 2\uff1a\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nI added the --config=cuda option, but there is an error: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\n\nhere is the log:\n\n```\n[yxwang@gpu05 tensorflow]$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nWARNING: Output base '/users2/yxwang/.cache/bazel/_bazel_yxwang/66b80900ec8be662b9abe25db0c533df' is on NFS. This may lead to surprising failures and undetermined behavior.\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nINFO: Elapsed time: 0.261s\n```\n", "Are you sure that wheel is installed in the same python version you're using? This looks like #348.\n", "You cannot remove the fetches from ./configure. We have some source dependencies, and we require them to build. Not sure that's the reason for the error, but it's not unlikely. \n", "Closing for inactivity. Please comment to reopen if you have new information.\n"]}, {"number": 3983, "title": "Installation failed on Mac OS X with protobuf 3.0.0b2", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nMac OS X 10.11.6\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nNo CUDA\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n# Mac OS X, CPU only, Python 2.7:\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0rc0-py2-none-any.whl\n\nlog:\nRequirement already satisfied (use --upgrade to upgrade): tensorflow==0.10.0rc0 from https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0rc0-py2-none-any.whl in ./tensorflow/lib/python2.7/site-packages\nRequirement already satisfied (use --upgrade to upgrade): mock>=2.0.0 in /usr/local/lib/python2.7/site-packages (from tensorflow==0.10.0rc0)\nRequirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /usr/local/lib/python2.7/site-packages/six-1.10.0-py2.7.egg (from tensorflow==0.10.0rc0)\nRequirement already satisfied (use --upgrade to upgrade): protobuf==3.0.0b2 in /usr/local/lib/python2.7/site-packages (from tensorflow==0.10.0rc0)\nRequirement already satisfied (use --upgrade to upgrade): wheel in ./tensorflow/lib/python2.7/site-packages (from tensorflow==0.10.0rc0)\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.10.1 in /usr/local/lib/python2.7/site-packages/numpy-1.11.1-py2.7-macosx-10.11-x86_64.egg (from tensorflow==0.10.0rc0)\nRequirement already satisfied (use --upgrade to upgrade): funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==0.10.0rc0)\nRequirement already satisfied (use --upgrade to upgrade): pbr>=0.11 in /usr/local/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==0.10.0rc0)\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /usr/local/lib/python2.7/site-packages/setuptools-26.0.0-py2.7.egg (from protobuf==3.0.0b2->tensorflow==0.10.0rc0)\n\nAnd the version of the protobuf is 3.0.0b2.\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   Traceback (most recent call last):\n   File \"<string>\", line 1, in <module>\n   File \"/Users/ttomato/tensorflow/lib/python2.7/site-packages/tensorflow/**init**.py\", line 23, in <module>\n     from tensorflow.python import *\n   File \"/Users/ttomato/tensorflow/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 52, in <module>\n     from tensorflow.core.framework.graph_pb2 import *\n   File \"/Users/ttomato/tensorflow/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\n     from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n   File \"/Users/ttomato/tensorflow/lib/python2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n     from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n   File \"/Users/ttomato/tensorflow/lib/python2.7/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n     from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n   File \"/Users/ttomato/tensorflow/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n     serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB2\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3')\n   TypeError: __init__() got an unexpected keyword argument 'syntax'\n### Steps to reproduce\n1. Pip install.\n   2.\n   3.\n### What have you tried?\n1. Tried a lot of times with \"pip install --upgrade portobuf\". And make sure the version of protobuf is correct.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Please see https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#protobuf-library-related-issues\n\nIf these instructions do not solve your problem, then please re-open, providing the **exact** sequence of commands you typed.\n", "(You almost certainly have a stale copy of python protobuf 2.0 installed somewhere, because that's what the error message indicates).\n"]}, {"number": 3982, "title": "Can not configure by new commit", "body": "the new version 6d04d601e9e8758ec4642fa9d548b7321d804d63\n./configure  on Mac platform\n\ncuda_configure.bzl:409:18: function 'repository_rule' does not exist.\n", "comments": ["Please can you provide the information requested in the TensorFlow issues template.  (software and OS versions, the exact sequence of command lines required to reproduce the problem)\n", "You probably need to update bazel to 0.3.0 or higher\n", "OK I  git pull  the tensorflow on Mac book. The System is OS X EI Capitan. With Nvidia GTX750\nCuda is 7.5 and Cudnn is 5\nthe repo commit is 6d04d60 \nI run the ./configure.  After the cuda setting,  there is an error that cuda_configure.bzl:409:18: function 'repository_rule' does not exist.\n\nThen I delete the all repo and git clone a new one. The error also exist. The branch is on master.\n", "What bazel version do you have?\n", "I brew the bazel on Mac. so the version is 2.0\nThe configuration is before compilation. And I have clone a new repo. The issue is also exist.\n", "The repo now needs bazel 0.3.0.  @davidzchen do you know who is in charge of bazel homebrew?\n", "Ok I will update it. The bazel version is 3.0 on homebrew. And thank you very much!\n", "https://github.com/Homebrew/homebrew-core/blob/8f35a6b808a98f5bfaee546644f80564bd95ae40/Formula/bazel.rb#L4 it looks like homebrew is already at 0.3.1, so you can upgrade.\n"]}, {"number": 3981, "title": "Clang host compiler support?", "body": "Hi all, \n\nI couldn't help but notice that currently TensorFlow is tightly coupled with GCC. Is there a plan to make it more flexible with the choice for host compiler? like clang? \nIs there any effort or plans for that?\n\nThanks,\nLuke\n", "comments": ["This is a general question best suited to StackOverflow.  Please can you re-ask there.\n", "Actually this question is probably not suited to StackOverflow as written...\n\nHowever, ./configure does allow you to specify the choice of the host compiler, and on Macs it uses clang anyway, so I suspect it already works.\n"]}, {"number": 3980, "title": "CUDA autoconf improvements", "body": "Some improvements to the new CUDA autoconf mechanism from the discussion in #3269 and #3966:\n- Autodetect `nvcc` version and:\n  - Set `CUDA_TOOLKIT_PATH` based on detected version\n  - Set `--expt-relaxed-constexpr` iff detected version is >= 7.5.\n- Set `cxx_builtin_include_directory` for CUDA headers based on detected include directories, similar to [`_get_cxx_inc_directories`](https://github.com/bazelbuild/bazel/blob/master/tools/cpp/cc_configure.bzl#L125)\n- Move remaining checks in `configure` script into `cuda_configure` so that the `configure` script only contains the user interface.\n", "comments": ["We'll also need to keep the alternative builds, such as the CMake build, in sync with any changes we make here (see #3996 for example).\n", "As mentioned in another Issue, `cxx_builtin_include_directory` needs to be set not only to the CUDA headers, but also to the include dirs of the users host compiler of choice.\n\nCurrently, dirs like `/usr/include` and `/usr/local/include` are hardcoded, but that is not good enough for self-built compilers in custom locations. This is not exotic and the setup should be supported because CUDA requires very specific GCC versions to work (e.g. CUDA 8 -> GCC 5.3) which are often not available on the distributions.\n", "@akors - Agreed. I have opened #4058 for the host compiler include directories issue.\n", "@davidzchen is this issue resolved?\r\nOr do we have more changes we are planning on this?", "The first part is done. I haven't found a good solution to the second part yet, but I do have a patch that is almost done for the third part.", "Since there has been no activity recently on this issue, I will close the issue.\r\nPlease feel free to reopen if there is more work here to be done."]}, {"number": 3979, "title": "Quantization ops missing from benchmark program", "body": "### Environment info\n\nOperating System: Mac OS X 10.11.4\nCUDA: No\nTF installed from source\n`git rev-parse HEAD`: `6d04d601e9e8758ec4642fa9d548b7321d804d63`\n\n```\nbazel version\nBuild label: 0.2.3-homebrew\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue May 17 15:07:52 2016 (1463497672)\nBuild timestamp: 1463497672\nBuild timestamp as int: 1463497672\n```\n### Problem\n\nI want to run inference on mobile and thought quantization would be a good idea. I also want to use the [benchmark program](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark) to see speed and memory consumption of different models before starting to train them.\n\nHowever the benchmark seems to be missing the quantization ops needed (it doesn't matter which quantization mode I use):\n\n```\nE tensorflow/tools/benchmark/benchmark_model.cc:72] Could not create TensorFlow Session: Not found: Op type not registered 'QuantizeV2'\n```\n\nI realize that I should modify this [BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/BUILD) file somehow to include the missing ops but I can't figure out Bazel's name of the built library or how to link it statically into the benchmark program. Or are the ops missing for a reason (incompatible with the normals ops etc)?\n\nI know that this is more of a question than a bug report, but I don't think I'm the only one with problem so perhaps you can treat is as a documentation bug?\n", "comments": ["you can see this issue https://github.com/tensorflow/tensorflow/issues/3619\nbut it just solve the issue partly.\n", "Closing as duplicate of #3619\n", "@lcwyylcwyy Thanks, that did the trick!\n"]}]