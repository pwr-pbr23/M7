[{"number": 36838, "title": "[ROCm] re-enable the test //tensorflow/python:auto_mixed_precision_test_gpu on ROCm", "body": "This PR is to re-enable the AMP unit test on the ROCm platform.\r\n\r\n------------------------------\r\n\r\n/cc @whchung @chsigg @nvining-work ", "comments": ["@gbaned, gentle ping", "gentle ping"]}, {"number": 36837, "title": "Tensorflow Lite Build Problem - Specialization for Eigen::internal::scalar_logistic_op<float> Removed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Lunux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source, branch r2.1\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): ./tensorflow/tools/ci_build/install/install_bazel.sh\r\n- GCC/Compiler version (if compiling from source): arm-linux-gnueabihf-gcc (Ubuntu/Linaro 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nCross-compiling Tensorflow Lite for Raspberry PI following published instructions.\r\n\r\nMultiple failures to find `Eigen::internal::scalar_logistic_op`:\r\n\r\n```In file included from tensorflow/lite/kernels/activations.cc:28:0:\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h: In function 'void tflite::optimized_ops::LstmCell(const tflite::LstmCellParams&, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, float*, const tflite::RuntimeShape&, float*, const tflite::RuntimeShape&, float*, const tflite::RuntimeShape&, float*, tflite::CpuBackendContext*)':\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:2938:48: error: 'scalar_logistic_op' is not a member of 'Eigen::internal'\r\n       input_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\r\n                                                ^~~~~~~~~~~~~~~~~~\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\ngit clone git@github.com:tensorflow/tensorflow.git\r\ncd tensorflow && git checkout r2.1\r\n./tensorflow/tools/ci_build/install/install_bazel.sh\r\n./tensorflow/lite/tools/make/download_dependencies.sh\r\n./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nEigen has moved to GitLab. The release for 2.1.0 has an error in `download_dependencies.sh` - the BitBucket repository URL is not found since `curl` is called with `-s`. I cloned the repository and found this using `git log`:\r\n\r\n```\r\ncommit 0f288e2ae6d343a978c4d5f6a951e20d7b69d96e\r\nAuthor: Rasmus Munk Larsen <rmlarsen@google.com>\r\nDate:   Mon Dec 2 17:00:58 2019 -0800\r\n\r\n    Revert the specialization for scalar_logistic_op<float> introduced in:\r\n    \r\n    https://bitbucket.org/eigen/eigen/commits/5ce45e1849c9c8352266830f6f8e628f26b99a9a\r\n    \r\n    While providing a 50% speedup on Haswell+ processors, the large relative error outside [-18, 18] in this approximation causes problems, e.g., when computing gradients of activation functions like softplus in neural networks.\r\n```\r\n\r\nI tried other releases of Eigen with the same problem.", "comments": ["@gadagashwini how can I help?", "To solve your immediate problem, the Tensorflow 2.1.0 release is pinned to Eigen sha 4e696901f873a2347f76d931cf2f701e31e15d05, so building with that will work.\r\n\r\nI think it would be nice if the `download_dependencies.sh` script were fixed, though.", "I did manage to fix the problem. I'll dig out the solution and see what I can do about submitting it.\r\n\r\nI may just add a patch to this issue. Advice welcome.", "[fix-eigen-archive-download-path.zip](https://github.com/tensorflow/tensorflow/files/4250361/fix-eigen-archive-download-path.zip)\r\n\r\nI patched `download_dependencies.sh` locally and use that in our build process.", "@dtsmith2001,\r\n\r\nAs you have managed to fix the problem. Would you mind submitting a PR for the same?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36837\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36837\">No</a>\n"]}, {"number": 36836, "title": "[ROCm][XLA:GPU] Merging xla postlayout assignment optimization pass", "body": "A continuation from the discussion [here](https://github.com/tensorflow/tensorflow/pull/35991#discussion_r370238036). Merging `OptimizeHloPostLayoutAssignment` pass such that:\r\n\r\n- `AMDGPUCompiler` can rely on its parent `GpuCompiler`'s `OptimizeHloPostLayoutAssignment` pass\r\n- `NVPTXCompiler` uses `GpuCompiler`'s routine as well as `Cuda` specific passes\r\n- `GpuCompiler` has the common functionality between `Cuda` and `ROCm`.\r\n\r\nThe benefit of this refactoring to to maximize code sharing between `Cuda` and `ROCm`. It will help eliminate subtle bugs caused by one pass adding to one platform but not another.\r\n\r\nNote: Alternatively I can move every pass to `GpuCompiler` and do runtime check to determine what platform it is. However, that will make the current inheritance hierarchy lose its original purpose in determining the platform dynamically through vtable.\r\n\r\n/cc: @whchung @deven-amd ", "comments": ["Working on addressing `Cuda` test failure now. Will push an update when done."]}, {"number": 36835, "title": "graph_transforms not found when running export_tflite_ssd_graph.py to convert ssd model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.1.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nFrom issue #31015 I'm trying to run:\r\n\r\npython ../models/research/object_detection/export_tflite_ssd_graph.py --pipeline_config_path pipeline.config --trained_checkpoint_prefix model.ckpt --output_directory my_frozen_graph --add_postprocessing_op True\r\n\r\nThe output is as follows:\r\n\r\nTraceback (most recent call last):\r\n  File \"../models/research/object_detection/export_tflite_ssd_graph.py\", line 96, in <module>\r\n    from object_detection import export_tflite_ssd_graph_lib\r\n  File \"/home/mello/venv36/lib/python3.6/site-packages/object_detection/export_tflite_ssd_graph_lib.py\", line 26, in <module>\r\n    from tensorflow.tools.graph_transforms import TransformGraph\r\nModuleNotFoundError: No module named 'tensorflow.tools.graph_transforms'\r\n\r\nThe section in export_tflite_ssd_graph_lib.py where the error occurs:\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.core.framework import attr_value_pb2\r\nfrom tensorflow.core.framework import types_pb2\r\nfrom tensorflow.core.protobuf import saver_pb2\r\nfrom tensorflow.tools.graph_transforms import TransformGraph\r\n\r\nI have tried binary install of tensorflow, both on ubuntu and on raspbian buster, have tried it with python 3.7 and 3.6, virtualenv or without.\r\nAlso have tried with tensorflow compiled using bazel.\r\n\r\nRegardless, I alwas get that tensorflow.tools.graph_transforms are missing.\r\n\r\nModel I'm using is ssd_mobilenet_v2_coco_2018_03_29\r\n\r\nI don't believe it has the to do with the model, but probably with the tensorflow install (but not at all sure...)\r\n\r\nThanks for your help!", "comments": ["It looks like:\r\n\r\nfrom tensorflow.tools.graph_transforms import TransformGraph\r\n\r\nworks for tensorflow 1.15, but not for 2.0 or 2.1.\r\n\r\nIs it possible to have this be part of the tensorflow 2.1 binary?\r\n", "You have to [build graph transform tool](https://github.com/tensorflow/tensorflow/issues/33352#issuecomment-556245567) for TF 2.X \r\nSee https://github.com/tensorflow/tensorflow/issues/33352#issuecomment-556245567 to know more"]}, {"number": 36834, "title": "Remove ppc64le manyinux2014 docker files.", "body": "We are now using the quay.io/pypa/manylinux2014_ppc64le container\r\nand the dockerfiles in https://github.com/tensorflow/build/pull/3/files\r\nto build on ppc64le.\r\n\r\nThis hack to install devtoolset7 into Ubuntu didn't really work for us.", "comments": []}, {"number": 36833, "title": "[tf2.1.0] model.save() throws TypeError when using tf.debugging.enable_check_numerics()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Redhat 8.1 (in Docker)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TF2.1.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: V100, 32GB\r\n\r\n**Describe the current behavior**\r\nwhen using `tf.debugging.enable_check_numerics()`, model.save(...) will throw some TypeError as below:\r\n```\r\nTypeError: Input 'resource' of 'AssignVariableOp' Op has type float32 that does not match expected type of resource.\r\n```\r\n\r\n**Code to reproduce the issue** \r\n```python\r\nimport tensorflow as tf\r\ntf.debugging.enable_check_numerics()\r\n\r\ndef build_and_compile_model():\r\n    \r\n    input = tf.keras.Input((20,))\r\n    y = tf.keras.layers.Dense(2)\r\n    model = tf.keras.Model(inputs=input, outputs=y)\r\n    \r\n    model.compile(\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n        metrics=['accuracy'])\r\n    \r\n    return model\r\n\r\nmodel = build_and_compile_model()\r\nmodel.save('test', save_format='tf')\r\n```\r\n\r\n**Other info / logs** \r\nTraceback logs as follows:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/tmp/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    467               as_ref=input_arg.is_ref,\r\n--> 468               preferred_dtype=default_dtype)\r\n    469         except TypeError as err:\r\n\r\n/tmp/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1289           \"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\" %\r\n-> 1290           (dtype.name, value.dtype.name, value))\r\n   1291     return value\r\n\r\nValueError: Tensor conversion requested dtype resource for Tensor with dtype float32: <tf.Tensor 'dense/kernel/Read/ReadVariableOp:0' shape=(20, 2) dtype=float32>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-597a577ec9b0> in <module>()\r\n      3 model = build_and_compile_model()\r\n      4 \r\n----> 5 model.save('test', save_format='tf')\r\n\r\n/tmp/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n   1006     \"\"\"\r\n   1007     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n-> 1008                     signatures, options)\r\n   1009 \r\n   1010   def save_weights(self, filepath, overwrite=True, save_format=None):\r\n\r\n/tmp/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    113   else:\r\n    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n--> 115                           signatures, options)\r\n    116 \r\n    117 \r\n\r\n/tmp/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n     76     # we use the default replica context here.\r\n     77     with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access\r\n---> 78       save_lib.save(model, filepath, signatures, options)\r\n     79 \r\n     80   if not include_optimizer:\r\n\r\n/tmp/site-packages/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    907   object_saver = util.TrackableSaver(checkpoint_graph_view)\r\n    908   asset_info, exported_graph = _fill_meta_graph_def(\r\n--> 909       meta_graph_def, saveable_view, signatures, options.namespace_whitelist)\r\n    910   saved_model.saved_model_schema_version = (\r\n    911       constants.SAVED_MODEL_SCHEMA_VERSION)\r\n\r\n/tmp/site-packages/tensorflow_core/python/saved_model/save.py in _fill_meta_graph_def(meta_graph_def, saveable_view, signature_functions, namespace_whitelist)\r\n    588     for concrete_function in saveable_view.concrete_functions:\r\n    589       concrete_function.add_to_graph()\r\n--> 590     saver_def = saver.to_proto()\r\n    591     meta_graph_def.saver_def.CopyFrom(saver_def)\r\n    592   graph_def = exported_graph.as_graph_def(add_shapes=True)\r\n\r\n/tmp/site-packages/tensorflow_core/python/training/saving/functional_saver.py in to_proto(self)\r\n    149         shape=[], dtype=dtypes.string, name=\"saver_filename\")\r\n    150     save_tensor = self._traced_save(filename_tensor)\r\n--> 151     restore_op = self._traced_restore(filename_tensor).op\r\n    152     return saver_pb2.SaverDef(\r\n    153         filename_tensor_name=filename_tensor.name,\r\n\r\n/tmp/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n/tmp/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    613       # This is the first call of __call__, so we have to initialize.\r\n    614       initializers = []\r\n--> 615       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    616     finally:\r\n    617       # At this point we know that the initialization is complete (or less\r\n\r\n/tmp/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    495     self._concrete_stateful_fn = (\r\n    496         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 497             *args, **kwds))\r\n    498 \r\n    499     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/tmp/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2387       args, kwargs = None, None\r\n   2388     with self._lock:\r\n-> 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2390     return graph_function\r\n   2391 \r\n\r\n/tmp/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2701 \r\n   2702       self._function_cache.missed.add(call_context_key)\r\n-> 2703       graph_function = self._create_graph_function(args, kwargs)\r\n   2704       self._function_cache.primary[cache_key] = graph_function\r\n   2705       return graph_function, args, kwargs\r\n\r\n/tmp/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2591             arg_names=arg_names,\r\n   2592             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2593             capture_by_value=self._capture_by_value),\r\n   2594         self._function_attributes,\r\n   2595         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n/tmp/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    976                                           converted_func)\r\n    977 \r\n--> 978       func_outputs = python_func(*func_args, **func_kwargs)\r\n    979 \r\n    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/tmp/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    437         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    438         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    440     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    441 \r\n\r\n/tmp/site-packages/tensorflow_core/python/eager/function.py in bound_method_wrapper(*args, **kwargs)\r\n   3204       if tf_inspect.ismethod(wrapped_fn):\r\n   3205         wrapped_fn = six.get_unbound_function(wrapped_fn)\r\n-> 3206       return wrapped_fn(weak_instance(), *args, **kwargs)\r\n   3207 \r\n   3208     # If __wrapped__ was replaced, then it is always an unbound function.\r\n\r\n/tmp/site-packages/tensorflow_core/python/training/saving/functional_saver.py in _traced_restore(self, file_prefix)\r\n    169       autograph=False)\r\n    170   def _traced_restore(self, file_prefix):\r\n--> 171     restore_ops = self.restore(file_prefix)\r\n    172     with ops.device(\"cpu:0\"):\r\n    173       with ops.control_dependencies(restore_ops.values()):\r\n\r\n/tmp/site-packages/tensorflow_core/python/training/saving/functional_saver.py in restore(self, file_prefix)\r\n    253     for device, saver in sorted(self._single_device_savers.items()):\r\n    254       with ops.device(device):\r\n--> 255         restore_ops.update(saver.restore(file_prefix))\r\n    256     return restore_ops\r\n\r\n/tmp/site-packages/tensorflow_core/python/training/saving/functional_saver.py in restore(self, file_prefix)\r\n    100                                           structured_restored_tensors):\r\n    101       restore_ops[saveable.name] = saveable.restore(\r\n--> 102           restored_tensors, restored_shapes=None)\r\n    103     return restore_ops\r\n    104 \r\n\r\n/tmp/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py in restore(self, restored_tensors, restored_shapes)\r\n    114       restored_tensor = array_ops.identity(restored_tensor)\r\n    115       return resource_variable_ops.shape_safe_assign_variable_handle(\r\n--> 116           self.handle_op, self._var_shape, restored_tensor)\r\n    117 \r\n    118 \r\n\r\n/tmp/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in shape_safe_assign_variable_handle(handle, shape, value, name)\r\n    298   return gen_resource_variable_ops.assign_variable_op(handle,\r\n    299                                                       value_tensor,\r\n--> 300                                                       name=name)\r\n    301 \r\n    302 \r\n\r\n/tmp/site-packages/tensorflow_core/python/ops/gen_resource_variable_ops.py in assign_variable_op(resource, value, name)\r\n    152   # Add nodes to the TensorFlow graph.\r\n    153   _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n--> 154         \"AssignVariableOp\", resource=resource, value=value, name=name)\r\n    155   return _op\r\n    156 AssignVariableOp = tf_export(\"raw_ops.AssignVariableOp\")(_ops.to_raw_op(assign_variable_op))\r\n\r\n/tmp/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    489           if input_arg.type != types_pb2.DT_INVALID:\r\n    490             raise TypeError(\"%s expected type of %s.\" %\r\n--> 491                             (prefix, dtypes.as_dtype(input_arg.type).name))\r\n    492           else:\r\n    493             # Update the maps with the default, if needed.\r\n\r\nTypeError: Input 'resource' of 'AssignVariableOp' Op has type float32 that does not match expected type of resource.\r\n```\r\n", "comments": ["@yufengm,\r\nI tried to reproduce the issue given above but I am facing an error stating `AttributeError: 'Dense' object has no attribute 'op'.` \r\nPlease find the Gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/919ae43998fbe508d97b44a29772875a/36833.ipynb). Thanks!", "@amahendrakar \r\nsorry, my bad. There was a typo in my provided code. It should be\r\n```python\r\ny = tf.keras.layers.Dense(2)(input)\r\n```", "Was able to reproduce the issue with TF 2.1 and TF-nightly. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/05abc701f4eab9ed24c94a9fdef6acb0/36833.ipynb). Thanks!", "@yufengm I think There is some `dtype` compatibility issue when you want to save the model in `tf` format. Please check the resource on TF website [here](https://www.tensorflow.org/api_docs/python/tf/debugging/enable_check_numerics). \r\n\r\nHowever, there is no issue when you save the model in `h5` format.\r\n\r\n> model.save('test', save_format='h5')\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36833\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36833\">No</a>\n", "This issue is still present, and saving in 'hd5' format is not an option if you wish to use TensorFlow Lite.", "> This issue is still present, and saving in 'hd5' format is not an option if you wish to use TensorFlow Lite.\r\n\r\n@jimblackler,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "I met the same issue with tensorflow 2.6 on Mac. ", "I encountered the same error message using Tensorflow 2.5.1. Unlike the provided code snippet I'm using TF-Estimators as a wrapper around a Keras model.\r\n\r\nHas anyone solved the problem?\r\n", "This could be caused by having `tf.debugging.experimental.enable_dump_debug_info(my_path)` in your code. If you do, try removing it."]}, {"number": 36832, "title": "Comments for alternate implementation in Adadelta Paper #36785", "body": "The original implementation of Adadelta by M.D Zeiler (https://arxiv.org/pdf/1212.5701.pdf) had a eta of 1.0 and epsilon of 1e-6 which is not matching with the function mentioned here. Hence commented the original implementation according to paper. Reference Issue #36785", "comments": ["Hi @tanzhenyu,  raised a PR #36849.Thanks", "@abhilash1910 Can you please resolve conflicts? Thanks!", "@gbaned,yes I can resolve the conflicts.  I raised a new PR to resolve this issue #36849. Should I close this pr?  ", "> @gbaned,yes I can resolve the conflicts. I raised a new PR to resolve this issue #36849. Should I close this pr?\r\n\r\nSure we can close it if you have another pr", "Yes @gbaned, thanks. ", "Hi @tanzhenyu @gbaned can you check the PR #36849 so that if anything is required to be added, I check. Thanks."]}, {"number": 36831, "title": "\"ValueError: No gradients provided for any variable\" in TF2.1 custom models", "body": "**System information** \r\n- OS Platform and Distribution: Arch Linux\r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de 2.1.0 \r\n\r\n\r\nI'm trying to train a custom model with tf.GradientTape, but it gives me this error:\r\n\r\n```\r\nValueError: No gradients provided for any variable: ['MyModel/conv2d/kernel:0', 'MyModel/conv2d/bias:0'].\r\n```\r\n\r\nI don't see anything wrong in my code, can you point out where is the problem?\r\n\r\n```python\r\ntrain_dir = config.TRAIN_DIR\r\ntrain_ds = tf.data.Dataset.list_files(str(train_dir / \"*\"))\r\ntrain_ds = (\r\n    train_ds.map(load_frames, num_parallel_calls=12)\r\n    .batch(batch_size)\r\n    .prefetch(buffer_size=batch_size)\r\n)\r\n\r\nmodel = MyModel()\r\n# Keep results for plotting\r\ntrain_loss_results = []\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\r\nmse_loss_fn = tf.keras.losses.MeanSquaredError()\r\nfor epoch in epochs:\r\n    epoch_loss_avg = tf.keras.metrics.Mean()\r\n    for inputs in train_ds:\r\n        with tf.GradientTape() as tape:\r\n            input_1, input_2, input_3 = inputs\r\n            predictions, warping_output = model(inputs, training=True)\r\n            rec_loss = mse_loss_fn(input_3, predictions)\r\n            \r\n        grads = tape.gradient(rec_loss, model.trainable_weights)\r\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n        epoch_loss_avg(grads)  # Add current batch loss\r\n\r\n    train_loss_results.append(epoch_loss_avg.result())\r\n    if epoch % 50 == 0:\r\n        print(\"Epoch {:03d}: Loss: {:.3f}\".format(epoch, epoch_loss_avg.result()))\r\n\r\n```\r\n\r\nwhere `MyModel` is for example\r\n\r\n```python\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, name=\"MyModel\", **kwargs):\r\n        super(MyModel, self).__init__(name=name, **kwargs)\r\n        self.conv = tf.keras.layers.Conv2D(\r\n            filters=32, kernel_size=7, strides=1, padding=\"same\"\r\n        )\r\n\r\n    def call(self, inputs, training=True, **kwargs):\r\n        input_1, input_2, input_3 = inputs\r\n        out = self.conv(input_1)\r\n        return out\r\n```\r\n", "comments": ["@Riccorl, Please provide the complete standalone code to replicate the reported issue. Thanks!", "If you use `tf.compat.v1.train.AdamOptimizer` the problem would most likely go away.\r\n\r\nCould you please check and respond ?\r\n\r\n", "I'll provide a working code as soon as possible, sorry for the delay.", "Hi, Did you find the solution of this problem?", "@ksrath0re @Riccorl  I believe this problem is very general and has two components\r\n\r\n1. There is some sort of mismatch or incompatibility between the APIs of `tf.compat.v1....`  and `tf.keras....`.  What is the exact nature of this incompatibility is not clear to me. I have been trying to implement some object detection techniques in TensorFlow and have been badly eroded by this problem.  I am quite sure that with tf.compat.v1.train.AdamOptimizer the above code will work i.e at least start running. But the OP will have to respond on this one.\r\n\r\n2. If you load a SavedModel and then try to fine-tune it, you are again likely to face this problem with quite the same error. This is often the case when you want to add some extra layers on top of a pretrained model and then do end-to-end training. In that case also using `tf.compat.v1....` in place of `tf.keras...` API will make the code running but you are likely to see the issue that the saved model's layers are not getting updated. \r\n\r\nThe fact that these GitHub issues (this and #34211 ) have not been addressed, ( except providing a CoLab GIST ) does speak something terribly unfortunate.", "> @ksrath0re @Riccorl I believe this problem is very general and has two components\r\n> \r\n>     1. There is some sort of mismatch or incompatibility between the APIs of `tf.compat.v1....`  and `tf.keras....`.  What is the exact nature of this incompatibility is not clear to me. I have been trying to implement some object detection techniques in TensorFlow and have been badly eroded by this problem.  I am quite sure that with tf.compat.v1.train.AdamOptimizer the above code will work i.e at least start running. But the OP will have to respond on this one.\r\n> \r\n>     2. If you load a SavedModel and then try to fine-tune it, you are again likely to face this problem with quite the same error. This is often the case when you want to add some extra layers on top of a pretrained model and then do end-to-end training. In that case also using `tf.compat.v1....` in place of `tf.keras...` API will make the code running but you are likely to see the issue that the saved model's layers are not getting updated.\r\n> \r\n> \r\n> The fact that these GitHub issues (this and #34211 ) have not been addressed, ( except providing a CoLab GIST ) does speak something terribly unfortunate.\r\n\r\nSi I tried the approach you mentioned of using  tf.compat.v1.train.AdamOptimizer and it works and execution starts. The problem is that the gradient values are None. I am using Tensorflow-gpu 2.0.0 version.\r\n\r\n> length of gradients :  268\r\n> gradients # 0  :  None\r\n> gradients # 1  :  None\r\n> gradients # 2  :  None\r\n> gradients # 3  :  None\r\n> gradients # 4  :  None\r\n> gradients # 5  :  None\r\n\r\n\r\nI am setting those values to Zero and continuing. How does that affect the computation?", "I wrote a [gist](https://colab.research.google.com/drive/1OLGnpj7gzbjaI-G0eOzmURhFxHESiIwH) on Colab but, there, I cannot reproduce the error anymore (neither with tensorflow 2.0 nor tensorflow 2.1). \r\n\r\nThe code here is essentially the same as the one provided in the OP (and identical to the original one at the time I had this error). Strange thing is that the same problem appeared on 3 different PC with different configuration and hardware.\r\n\r\n@ksrath0re The problem disappeared after changing everything from preprocessing to the train code, but honestly, since the same code now works in Colab, I don't know. We tried many different things (like putting `apply_gradients` inside the `tape` context) and eventually the train started correctly. ", "> > @ksrath0re @Riccorl I believe this problem is very general and has two components\r\n> > ```\r\n> > 1. There is some sort of mismatch or incompatibility between the APIs of `tf.compat.v1....`  and `tf.keras....`.  What is the exact nature of this incompatibility is not clear to me. I have been trying to implement some object detection techniques in TensorFlow and have been badly eroded by this problem.  I am quite sure that with tf.compat.v1.train.AdamOptimizer the above code will work i.e at least start running. But the OP will have to respond on this one.\r\n> > \r\n> > 2. If you load a SavedModel and then try to fine-tune it, you are again likely to face this problem with quite the same error. This is often the case when you want to add some extra layers on top of a pretrained model and then do end-to-end training. In that case also using `tf.compat.v1....` in place of `tf.keras...` API will make the code running but you are likely to see the issue that the saved model's layers are not getting updated.\r\n> > ```\r\n> > \r\n> > \r\n> > The fact that these GitHub issues (this and #34211 ) have not been addressed, ( except providing a CoLab GIST ) does speak something terribly unfortunate.\r\n> \r\n> Si I tried the approach you mentioned of using tf.compat.v1.train.AdamOptimizer and it works and execution starts. The problem is that the gradient values are None. I am using Tensorflow-gpu 2.0.0 version.\r\n> \r\n> > length of gradients :  268\r\n> > gradients # 0  :  None\r\n> > gradients # 1  :  None\r\n> > gradients # 2  :  None\r\n> > gradients # 3  :  None\r\n> > gradients # 4  :  None\r\n> > gradients # 5  :  None\r\n> \r\n> I am setting those values to Zero and continuing. How does that affect the computation?\r\n\r\nI am having the same issue. The gradients are continuously None. This kind of problem was reported in another issue ( I do not remember the issue nunber !! ) where the OP pointed out that the problem appears primarily due to a mixing of `tf.keras...` and `tf.math` operations. So, this further reinforces the belief that there is some gross compatibility issues in there.", "@Riccorl, Is this still an issue?", "> @Riccorl, Is this still an issue?\r\n\r\nAt the moment it's not an issue, I'll close it. I'll open it if I get this error again. Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36831\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36831\">No</a>\n"]}, {"number": 36830, "title": "Removed six dependency in tools/git since it breaks hermetic builds", "body": "[git_configure.bzl](https://github.com/tensorflow/tensorflow/blob/master/third_party/git/git_configure.bzl) uses the host python to run [tools/git/gen_git_source.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/git/gen_git_source.py)\r\n\r\nIt looks like in a cross compatibility migration (https://github.com/tensorflow/tensorflow/commit/c396546ca3d9424f7d902f4c5c604c209f9c0f0c) a `six` dependency was added. If building on a completely vanilla system, this will break if `six` is not installed on the host. Ideally, this should be done with python provided by bazel, but these rules are called during `tf_workspace`, `bind`, etc... So I think this approach is acceptable.\r\n\r\nSmall change, but also adds consistency! See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/git/gen_git_source.py#L171", "comments": ["I found evidence I'm not the only person facing this issue: https://github.com/google/mediapipe/issues/276\r\n\r\nEverytime I blow away my bazel cache I have to apply this patch", "@dmadisetti Can you please address Ubuntu Sanity errors? Thanks!", "Oh wow. I thought the tests were just due to flake, my bad. Surprised this removed six as a dependency all together in the other apis. Sanity checks passed locally", "Not completely sure this won't break internal py2 old workflows, but let's test", ":tada:\r\n\r\n@mihaimaruseac @gbaned  Looks to be good. Anything needed on my end?", "Nothing left to do here at the moment. It is pending internal integration, review and testing. If all goes well, it will be automerged, otherwise we'll report back."]}, {"number": 36829, "title": "[Keras] Fatal exception training a model with more outputs than targets from the generator", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n - Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n\r\nWith eager execution enabled, the number of targets generated by the generator and the number of outputs produced by the model have to be the same. If there is a difference between the two, training fails with the following error:\r\n\r\n```shell\r\nValueError: Unable to match target structure and sample_weight_modes structure:\r\n  ...\r\n    to  \r\n  ['...', '...']\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe example below should work whether using eager or non-eager execution.\r\n\r\n**Code to reproduce the issue** Provide a reproducible test case that is the\r\nbare minimum necessary to generate the problem.\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# This script works if v2 is disabled, but fails if v2 is enabled.\r\ntf.compat.v1.disable_v2_behavior()\r\n\r\n# Create a very simple generator.\r\nclass Generator(tf.keras.utils.Sequence):\r\n\tdef __len__(self):\r\n\t\treturn 1\r\n\r\n\tdef __getitem__(self, index):\r\n\t\treturn np.zeros((1, 100, 100)), np.zeros((1, 2))\r\ngenerator = Generator()\r\n\r\n# Create a simple model with two outputs, one has a loss attached to it the other does not.\r\ninputs = tf.keras.Input((100, 100))\r\nflattened = tf.keras.layers.Flatten()(inputs)\r\noutput_1 = tf.keras.layers.Dense(2, name='output_for_loss')(flattened)\r\noutput_2 = tf.keras.layers.Reshape((2, -1), name='some_other_output')(flattened)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=[output_1, output_2])\r\nmodel.compile(loss={'output_for_loss': tf.keras.losses.binary_crossentropy})\r\n\r\n# Train using the generator.\r\nmodel.fit(generator)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nComplete traceback:\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/util/nest.py\", line 329, in assert_same_structure\r\n    _pywrap_utils.AssertSameStructure(nest1, nest2, check_types,\r\nValueError: The two structures don't have the same nested structure.\r\n\r\nFirst structure: type=ndarray str=[[0. 0.]]\r\n\r\nSecond structure: type=tuple str=(None, None)\r\n\r\nMore specifically: Substructure \"type=tuple str=(None, None)\" is a sequence, while substructure \"type=ndarray str=[[0. 0.]]\" is not\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\", line 1076, in broadcast_sample_weight_modes\r\n    nest.assert_same_structure(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/util/nest.py\", line 334, in assert_same_structure\r\n    raise type(e)(\"%s\\n\"\r\nValueError: The two structures don't have the same nested structure.\r\n\r\nFirst structure: type=ndarray str=[[0. 0.]]\r\n\r\nSecond structure: type=tuple str=(None, None)\r\n\r\nMore specifically: Substructure \"type=tuple str=(None, None)\" is a sequence, while substructure \"type=ndarray str=[[0. 0.]]\" is not\r\nEntire first structure:\r\n.\r\nEntire second structure:\r\n(., .)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\", line 1087, in broadcast_sample_weight_modes\r\n    sample_weight_modes = nest.pack_sequence_as(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/util/nest.py\", line 504, in pack_sequence_as\r\n    return _pack_sequence_as(structure, flat_sequence, expand_composites)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/util/nest.py\", line 448, in _pack_sequence_as\r\n    raise ValueError(\r\nValueError: The target structure is of type `<class 'numpy.ndarray'>`\r\n  [[0. 0.]]\r\nHowever the input structure is a sequence (<class 'list'>) of length 2.\r\n  [None, None]\r\nnest cannot guarantee that it is safe to map one to the other.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 25, in <module>\r\n    model.fit(generator)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/training.py\", line 800, in fit\r\n    return func.fit(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 219, in fit\r\n    training_data_adapter, validation_adapter = _process_training_inputs(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 579, in _process_training_inputs\r\n    train_adapter = _process_inputs(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 693, in _process_inputs\r\n    adapter = adapter_cls(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\", line 945, in __init__\r\n    super(KerasSequenceAdapter, self).__init__(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\", line 752, in __init__\r\n    ) = self._canonicalize_peek(peek, kwargs.get(\"sample_weight_modes\"))\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\", line 806, in _canonicalize_peek\r\n    sample_weight_modes = broadcast_sample_weight_modes(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\", line 1093, in broadcast_sample_weight_modes\r\n    raise ValueError(\r\nValueError: Unable to match target structure and sample_weight_modes structure:\r\n  ...\r\n    to  \r\n  ['...', '...']\r\n```\r\n\r\nThis issue seems to be introduced in https://github.com/tensorflow/tensorflow/commit/ac20030c96d37e980333b604402ef6dba48ef5e2 .", "comments": ["@hgaiser \r\n\r\nI tried to reproduce the issue in colab with TF 2.1 by enabling and disabling v2_behavior. I am not seeing any error message.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d80d5b3723420971b154409e66cc6e86/untitled645.ipynb). Is this the expected behavior? Thanks!", "Hey, it looks like you didn't enable V2 in one of the two blocks. Could you\r\ntry that?\r\n\r\nEDIT: Upon further inspection, seems that in the second block you didn't call `tf.compat.v1.enable_v2_behavior` because it was missing parenthesis at the end.\r\n\r\nOn Tue, 18 Feb 2020, 10:55 ravikyram, <notifications@github.com> wrote:\r\n\r\n> @hgaiser <https://github.com/hgaiser>\r\n>\r\n> I tried to reproduce the issue in colab with TF 2.1 by enabling and\r\n> disabling v2_behavior. I am not seeing any error message.Please, find the\r\n> gist here\r\n> <https://colab.sandbox.google.com/gist/ravikyram/d80d5b3723420971b154409e66cc6e86/untitled645.ipynb>.\r\n> Is this the expected behavior? Thanks!\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/36829?email_source=notifications&email_token=AAFO22XPTY2IYEEORBVXLFTRDOV7VA5CNFSM4KWULM42YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMBKFLY#issuecomment-587375279>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAFO22RGEGCJSIYLIW47P3LRDOV7VANCNFSM4KWULM4Q>\r\n> .\r\n>\r\n", "I slightly changed your example:\r\n\r\nhttps://colab.research.google.com/gist/hgaiser/222f71b25e2cd3c7479931f59af5e757/untitled0.ipynb", "I tried to reproduce the issue in colab with [TF 2.1 gist](https://colab.sandbox.google.com/gist/ravikyram/4cf7cbc1f16674a7c0c96f58601a9017/untitled0.ipynb) ,[TF 2.2.0-dev20200218 gist](https://colab.sandbox.google.com/gist/ravikyram/bb51b125d818125a6d2b147ec69671f3/untitled649.ipynb) and was able to reproduce the issue.Thanks!", "Any progress with this? I need to train with multiple outputs and losses@@", "@hgaiser Thanks for the issue!\r\n\r\nThis is fixed in the latest tf-nightly: `pip install -U tf-nightly`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36829\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36829\">No</a>\n"]}, {"number": 36828, "title": "Build from Source Failure (Ubuntu 18.04 CUDA/cuDNN 10.1/7.6.5 Py 3.6 no AVX)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version:\r\nGPU 2.1\r\n- Python version:\r\n3.6\r\n- Installed using virtualenv? pip? conda?:\r\nNA - PIP3\r\n- Bazel version (if compiling from source):\r\n0.29.1\r\n- GCC/Compiler version (if compiling from source):\r\n7.4.0\r\n- CUDA/cuDNN version:\r\n10.1/7.6.5\r\n- GPU model and memory:\r\nGTX 1080 / 8GB\r\n\r\n\r\n**Describe the problem**\r\nThe build process fails with:\r\nERROR: /home/greg/tensorflow/tensorflow/core/kernels/BUILD:4843:1: output 'tensorflow/core/kernels/_objs/depth_space_ops_gpu/depthtospace_op_gpu.cu.pic.o' was not created\r\nERROR: /home/greg/tensorflow/tensorflow/core/kernels/BUILD:4843:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/greg/tensorflow/tensorflow/python/tools/BUILD:311:1 not all outputs were created or valid\r\nINFO: Elapsed time: 248.378s, Critical Path: 97.82s\r\nINFO: 853 processes: 853 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI have tried both of the following with similar results:\r\nbazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nbazel build  //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nNo other errors show in logs, can upload more of a snippet if needed, but no other reference to 'depthtospace_op_gpu.cu.pic' found leading to the failure for any hints.\r\n\r\nI tried running the docker install as we from the install page, trying to avoid all the setup:\r\nhttps://www.tensorflow.org/install/source\r\n\r\nI got this error from the docker system on the 2nd docker command:\r\ndocker run --runtime=nvidia -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=\"$(id -u):$(id -g)\" tensorflow/tensorflow:devel-gpu-py3 bash\r\ndocker: Error response from daemon: Unknown runtime specified nvidia.\r\nSee 'docker run --help'.\r\n\r\nI have build TensorFlow from source in the past, build 2.0b6 (no GPU) shortly after it was released and have no problems with GPU compiling of TensorFlow 1.X versions.  I have an old server without AVX support so I can't use the pre-compiled PIPs and haven't found one for my configuration with 2.1 yet.\r\n\r\nThanks.\r\n", "comments": ["I saw reference to Bazel 0.27 for TF 2.1 so loaded 0.27.2 and got similar results:\r\nERROR: /home/greg/tensorflow/tensorflow/core/kernels/BUILD:1257:1: output 'tensorflow/core/kernels/_objs/snapshot_op_gpu/snapshot_op_gpu.cu.pic.o' was not created\r\nERROR: /home/greg/tensorflow/tensorflow/core/kernels/BUILD:1257:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2186.132s, Critical Path: 166.24s\r\nINFO: 12252 processes: 12252 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nI have enclosed most of the output.\r\n\r\n[bazel0.27.2.txt](https://github.com/tensorflow/tensorflow/files/4219534/bazel0.27.2.txt)\r\n", "@Mazecreator Please look into [this](https://github.com/tensorflow/tensorflow/issues/31926#issuecomment-527584855) and let  me know if  it helps you. Thanks!", "Thanks @saikumarchalla , I got the docker nvidia toolkit installed and the code is in the process of compiling.  I will let you know if this build passes/fails.  Also, the instructions on the install webpage didn't seem right, I had to change the command-line to get it to run the container and did a pull request with the change:\r\nhttps://github.com/tensorflow/docs/pull/1469", "@saikumarchalla ,\r\n\r\nWow, the Docker solution was much easier once I got the Nvidia part installed!  Everything is working fine, no need to figure out the failure of compiling from source as this probably indicates some decency/compiler was not the exact version on my Ubuntu box.\r\n\r\nAll compiled successfully and PIP package installed and functional on that system.  Thank for the help!", "Closing this issue as it was resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36828\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36828\">No</a>\n", "I have this issue\r\n", "> I have this issue\r\n\r\n@surak,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 36826, "title": "tf.lite.TFLiteConverter error when I trying to convert keras .h5 Yolov3 model.", "body": "**System information**\r\ngoogle colab\r\ntf ver. '2.1.0'\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nyolo_keras_model= tf.keras.models.load_model(filepath=\"yolo.h5\", compile=False)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(yolo_keras_model)\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_1' has invalid shape '[None, None, None, 3]'.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# my .h5 file\r\n# https://drive.google.com/open?id=1R8PAoeYtkUyiN8XBUohLmmJYxR97p14o.\r\n```\r\n\r\n**Failure details**\r\nI read #22564, bit this decision is not clear to me. \r\nAlso, i try to use TFLiteConverter.from_concrete_functions()\r\n```\r\nyolo_model= tf.keras.models.load_model(filepath=\"yolo.h5\", compile=False)\r\nexport_dir = \"/saved_model\"\r\ntf.saved_model.save(yolo_model, export_dir)\r\nmodel = tf.saved_model.load(export_dir)\r\nconcrete_func = model.signatures[\r\n  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nconcrete_func.inputs[0].set_shape([1, 416, 416, 3])\r\ncoverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\ntflite_model = coverter .convert()\r\n```\r\nError\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in deref(weak_v)\r\n    482       if v is None:\r\n    483         raise AssertionError(\r\n--> 484             \"Called a function referencing variables which have been deleted. \"\r\n    485             \"This likely means that function-local variables were created and \"\r\n    486             \"not referenced elsewhere in the program. This is generally a \"\r\nAssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.\r\n```\r\nWhat i should do?  As i understand, this steps are necessary for quantization. ", "comments": ["There was a conflict of variables. \r\n```\r\nyolo_model= tf.keras.models.load_model(filepath=\"yolo.h5\", compile=False)\r\nexport_dir = \"/saved_model\"\r\ntf.saved_model.save(yolo_model, export_dir)\r\nmodel = tf.saved_model.load(export_dir)\r\nconcrete_func = model.signatures[\r\n  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nconcrete_func.inputs[0].set_shape([1, 416, 416, 3])\r\ncoverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\ntflite_model = coverter.convert()\r\n```\r\nThis work for me"]}, {"number": 36824, "title": "Add int8 and int16 support to tflu network tester.", "body": "Add support for other quantized types (int8, int16) to tflu network tester.\r\n\r\nSigned-off-by: SiCong Li <sicong.li@arm.com>", "comments": ["@petewarden Could you please review this patch? Thanks! :)", "@petewarden Could you please assign someone to review this patch? Thanks! :)", "@petewarden @njeffrie Could you please have a look into this patch? Thanks! ", "@sicong-li-arm Can you please check @njeffrie's comments and keep us posted. Thanks!", "> A couple of high level questions and comments:\r\n> \r\n> 1. What model do we have to test with int8 / int16 quantization?  I would prefer to add features along with a use case.  Typically this would mean adding an int8 and int16 model to test, alongside network_model.cc.\r\n> 2. Why are you adding quantization scale / zero point to input / output tensors? This information should always be encoded in the model itself.\r\n> 3. I'm a bit loathed to add another way to test each element in an array against an expected output. I see the need for a method to compare with templated types, but I feel that this should live in our common testing code, not in this test.  I recognize this is largely a product of my own messy attempts to write generic test helpers, so perhaps leaving it in here is OK until we clean up our testing story internally.\r\n\r\n@njeffrie @gbaned  Thank you for your review! Sorry again for the late reply. I'll keep a close watch to this PR this week.\r\n1. This patch was added alongside [this PR](https://github.com/tensorflow/tensorflow/pull/38873) where we added int16 support to softmax. Previously we'd like to extend the network tester test's ability to at least run a single int16 softmax layer. But I don't think this PR has a dependency on that one as we already support many other int16 and int8 ops right?\r\n2. Yep you're right. This was a misunderstanding on my side.", "@sicong-li-arm please resolve conflicts ,once done we can merge this PR ASAP.", "> @sicong-li-arm please resolve conflicts ,once done we can merge this PR ASAP.\r\n\r\n@rthadur Done. Thanks! :)"]}, {"number": 36823, "title": "decode_wav_op.cc:55 : Invalid argument: Bad file size for WAV: Expected 16 or 18, but got 40", "body": "**System information** \r\n\r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 31\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n - TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# from https://github.com/Athospd/wavesurfer/blob/master/data-raw/Glaucidium-minutissimum-22180.wav\r\nfile = tf.io.read_file(\"/tmp/Glaucidium-minutissimum-22180.wav\")\r\ntf.audio.decode_wav(file)\r\n```\r\n\r\nerror:\r\n\r\n```\r\n2020-02-17 14:20:35.070542: W tensorflow/core/framework/op_kernel.cc:1675] OP_REQUIRES failed at decode_wav_op.cc:55 : Invalid argument: Bad file size for WAV: Expected 16 or 18, but got40\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error\r\n\r\n**Code to reproduce the issue** \r\n\r\nSee above.\r\n\r\n**Other info / logs**\r\n\r\nFile is played without problems by e.g. `sox`.\r\n\r\nFile info:\r\n\r\n```\r\nInput File     : 'Glaucidium-minutissimum-22180.wav'\r\nChannels       : 1\r\nSample Rate    : 16000\r\nPrecision      : 16-bit\r\nDuration       : 00:00:46.29 = 740624 samples ~ 3471.68 CDDA sectors\r\nFile Size      : 1.48M\r\nBit Rate       : 256k\r\nSample Encoding: 16-bit Signed Integer PCM\r\n```\r\n\r\nAlso, the number of samples d\u00f2es not seem to be the problem, as after a simple resampling \r\n\r\n```\r\nsox Glaucidium-minutissimum-22180.wav -r 16000 o5.wav\r\n```\r\n\r\nthe new output is decoded without error.\r\n\r\nSource location is\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e4c9dedb31df127aa6f52050f70f0084fd3e4c93/tensorflow/core/lib/wav/wav_io.cc#L238\r\n\r\nbut it is not clear to me what exactly is going on, so it would be great if this could be investigated. Thanks!", "comments": ["The file is not a regular WAVE_FORMAT_PCM (this is currently the only one supported by TF), but instead an instance of WAVE_FORMAT_EXTENSIBLE https://docs.microsoft.com/en-us/windows/win32/api/mmreg/ns-mmreg-waveformatextensible ; see bytes 21 and 22 which contain `0xFFFE` instead of `0x0001`. Therefore, TensorFlow cannot handle it (but yes, the error message is suboptimal).", "Many thanks for clarifying!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36823\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36823\">No</a>\n"]}, {"number": 36822, "title": "exception in gradient computation for losses when combining Tensor and EagerTensor", "body": "**System information** - Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): - Linux Mint 19 \r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nThe following minimal example works when using the second implementation alternative, but raises an error for the first:\r\n```python\r\nip = layers.Input(10)\r\nwith tf.GradientTape() as tape:\r\n    t = tf.zeros((10, 10))\r\n    tape.watch(t)\r\n    # this line throws an error \r\n    result = keras.losses.MeanSquaredError()(t, ip)\r\n    # but this one works\r\n    # result = (t - ip)**2 / tf.cast(tf.reduce_prod(tf.shape(t)), tf.float32)\r\ntape.gradient(result, t)\r\n```\r\n\r\nThe error is\r\n```\r\n  File \"~/wrapper.py\", line 101, in <module>\r\n    print(tape.gradient(result, t))\r\n  File \"~/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\", line 1029, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"~/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\", line 77, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"~/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\", line 141, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"~/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py\", line 258, in _MeanGrad\r\n    sum_grad = _SumGrad(op, grad)[0]\r\n  File \"~/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py\", line 213, in _SumGrad\r\n    op.inputs[1])\r\n  File \"~/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\", line 3502, in reduced_shape\r\n    input_shape = input_shape.numpy()\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n**Describe the expected behavior**\r\nAs the second implementation works, i would also expect the first to work.\r\n\r\nI have not done exhaustive testing of different loss functions, but the same problem also occurs when replacing the `MeanSquaredError` by `CategoricalCrossEntropy`. The problem seems to be caused by the fact that only one of the arguments to the loss function is an `EagerTensor`.", "comments": ["I have tried on colab with TF version 2.1.0, 2.2.0-dev20200218 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/7085b1ad3a03380e559078aa220d186b/untitled646.ipynb). Thanks!", "Thank you @ngc92 . The equivalent of `result = keras.losses.MeanSquaredError()(t, ip)` will not be `result = (t - ip)**2 / tf.cast(tf.reduce_prod(tf.shape(t)), tf.float32)`.\r\n\r\nIt will be the following roughly:\r\n\r\n```\r\n      mean_per_sample = math_ops.mean((t - ip)**2 / math_ops.cast(math_ops.reduce_prod(array_ops.shape(t)), dtypes.float32), axis=-1)\r\n      total_loss = math_ops.reduce_sum(mean_per_sample)\r\n      num_present = math_ops.cast(array_ops.size(mean_per_sample), dtype=mean_per_sample.dtype)\r\n      result = math_ops.div_no_nan(total_loss, num_present, name='value')\r\n```\r\nHere is a detailed example for writing custom training loop: https://keras.io/guides/writing_a_training_loop_from_scratch/. Please check it out.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36822\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36822\">No</a>\n", "@pavithrasv \r\nMy main point was not that I wanted to find out how to do a custom training loop, but the fact that the function throws an exception when called with one `EagerTensor` and one `Tensor`, which I find to be surprising. The example was mostly to show that in other cases, tf is perfectly happy to mix those two."]}, {"number": 36821, "title": "Memory leak using generators with tf.data.experimental.AUTOTUNE", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** - Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): - OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: - TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): - Python version: - Bazel\r\nversion (if compiling from source): - GCC/Compiler version (if compiling from\r\nsource): - CUDA/cuDNN version: - GPU model and memory:\r\n\r\nWindows 10\r\nTensorflow 2.0\r\nRTX 2060\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nRead images from a generator using \"num_parallel_calls=AUTOTUNE\" is eating all memory\r\n\r\n**Describe the expected behavior**\r\n\r\nDon't use up all memory\r\n\r\n**Code to reproduce the issue** Provide a reproducible test case that is the\r\nbare minimum necessary to generate the problem.\r\n\r\n```\r\ndef my_generator(data_dir):\r\n    def my_func():\r\n        paths = pathlib.Path(data_dir).rglob('*.png')\r\n        for path in paths:\r\n            yield str(path)\r\n    return my_func\r\n\r\ndef process_path_test(file_path):\r\n    \"\"\"Read a tensorflow image from a png file and return the\r\n    image and filepath for writing out during test time\"\"\"\r\n    img_file = tf.io.read_file(file_path)\r\n    img = tf.io.decode_png(img_file, channels=1, dtype=tf.uint16)\r\n    return img, file_path\r\n\r\ndef some_func(img, file_path):\r\n    return img, file_path\r\n\r\np_dir = '//Some_path_to_images'\r\nAUTOTUNE = tf.data.experimental.AUTOTUNE\r\ntest_imgs = tf.data.Dataset.from_generator(my_generator(p_dir), tf.string).map(process_path_test)\r\ntest_imgs = test_imgs.map(some_func, num_parallel_calls=AUTOTUNE)\r\nmy_iter = iter(test_imgs)\r\nnext(my_iter)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@plooney  Could you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment,it helps us localizing the issue faster.\r\n", "```\r\nimport tensorflow as tf\r\nimport os \r\nimport png\r\nimport numpy as np\r\nimport pathlib\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nassert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\ndata_dir = 'pngs'\r\nif not os.path.isdir(data_dir): os.makedirs(data_dir)\r\n\r\ndef write_png(image_array, out_name):\r\n    \"\"\"Write out PNG images with dimensions IMG_WIDTH, IMG_HEIGHT.                                       \r\n    Expects an UInt16 image array and output filename.\"\"\"                                                \r\n    with open(out_name, 'wb') as png_file:\r\n        writer = png.Writer(256, 256, greyscale=True, bitdepth=16)                          \r\n        writer.write(png_file, image_array[:, :])\r\n        \r\n\r\ndef my_generator(data_dir):\r\n    def my_func():\r\n        paths = pathlib.Path(data_dir).rglob('*.png')\r\n        for path in paths:\r\n            if path.name.endswith('.png'):\r\n                yield str(path)\r\n    return my_func\r\n\r\ndef process_path_test(file_path):\r\n    \"\"\"Read a tensorflow image from a png file and return the\r\n    image and filepath for writing out during test time\"\"\"\r\n    img_file = tf.io.read_file(file_path)\r\n    img = tf.io.decode_png(img_file, channels=1, dtype=tf.uint16)\r\n    return img, file_path\r\n\r\ndef some_func(image, file_path):\r\n    image = tf.image.resize(image, size = (256,256))\r\n    image = tf.clip_by_value(image,-0.9999,0.9999)\r\n    image = tf.math.atanh(image)\r\n    #image = (ww/2)*image + wc\r\n    image = image/3000\r\n    return image, file_path\r\n\r\nfor i in range(0,1000):\r\n    arr = (2**16 -1)*np.random.random([256,256])\r\n    arr = arr.astype(np.uint16)\r\n    write_png(arr, 'pngs/my_png_{}.png'.format(str(i)))\r\n\r\nAUTOTUNE = tf.data.experimental.AUTOTUNE\r\ntest_imgs = tf.data.Dataset.from_generator(my_generator(data_dir), tf.string).map(process_path_test)\r\ntest_imgs = test_imgs.map(some_func, num_parallel_calls=AUTOTUNE).batch(1)\r\nmy_iter = iter(test_imgs)\r\nnext(my_iter)\r\nprint(\"KABOOM\")\r\n```", "@plooney What data are you using here. Please provide us the data and related colab notebook to reproduce this issue. Thanks!", "@gowthamkpr I have posted a self contained example that reproduces the problem while generating dummy data. If you run it in jupyter it will demonstrate the memory leak.", "@plooney on colab I couldn't reproduce the issue, see [here](https://colab.research.google.com/drive/1RtZ8dvRZTA33Kl4kwt0ao_2pzx0hNpe8).\r\n\r\nI was just monitoring the RAM use by eye.\r\n\r\nAlso one thing, why do you check for GPU use in this case ?", "@zaccharieramzi Checking for GPU use was a workaround for an issue using an RTX 2060 GPU that I needed. \r\n\r\nI consistently have the memory leak issue on my architecture.", "@plooney ,\r\nI was able to run the code without any issues on TF v2.7.Please find the gist of it [here](\r\nhttps://colab.research.google.com/gist/tilakrayal/d240e98d5ebe0e45101921d4b42df3ac/untitled142.ipynb).\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 36819, "title": "Keras fit generator - ValueError: Failed to find data adapter that can handle input", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** - Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or\r\nbinary): source\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6\r\n- Keras Version: 2.3.1\r\n\r\nWhen i fit the model, it shows me this error:\r\n`      Traceback (most recent call last):\r\n  File \"/home/castilho/PycharmProjects/Chargrid/Chargrid/main.py\", line 201, in <module>\r\n    target_path=target_path, prefix=prefix_splits, make_new_representation=False, train=True)\r\n  File \"/home/castilho/PycharmProjects/Chargrid/Chargrid/main.py\", line 117, in main\r\n    nn.train(representations_path=representations_path, target_path=target_path, training_filenames = data['train_imgs'], validation_filenames=data['val_imgs'])\r\n  File \"/home/castilho/PycharmProjects/Chargrid/Chargrid/neural_network.py\", line 150, in train\r\n    validation_steps=int(len(validation_filenames) // batch_size))\r\n  File \"/home/castilho/PycharmProjects/Chargrid/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/castilho/PycharmProjects/Chargrid/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 235, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/castilho/PycharmProjects/Chargrid/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 533, in _process_training_inputs\r\n    adapter_cls = data_adapter.select_data_adapter(x, y)\r\n  File \"/home/castilho/PycharmProjects/Chargrid/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\", line 998, in select_data_adapter\r\n    _type_name(x), _type_name(y)))\r\nValueError: Failed to find data adapter that can handle input: <class 'Chargrid.dataset_generator.RepresentationGenerator'>, <class 'NoneType'>\r\n\r\nProcess finished with exit code 1`\r\n\r\n\r\n\r\nI think this a bug in Keras. Below, is a part of the code when i fit the model.\r\n`       def train(self, representations_path: str, target_path: str, training_filenames: list, validation_filenames: list,\r\n              batch_size: int = 7):\r\n        \r\n        train_generator = RepresentationGenerator(representation_path=representations_path, target_path=target_path,\r\n                                                  filenames=training_filenames, batch_size=batch_size)\r\n        val_generator = RepresentationGenerator(representation_path=representations_path, target_path=target_path,\r\n                                                filenames=validation_filenames, batch_size=batch_size)\r\n        self.model_semantic.fit(x=train_generator, batch_size=7,\r\n                                steps_per_epoch=int(len(training_filenames) // batch_size),\r\n                                epochs=10,\r\n                                verbose=1,\r\n                                validation_data=val_generator,\r\n                                validation_steps=int(len(validation_filenames) // batch_size))\r\n        return 0`\r\n\r\n\r\nMy class RepresentationGenerator is:\r\n\r\n`from tensorflow_core.python.keras.utils.data_utils import Sequence\r\n\r\n\r\nclass RepresentationGenerator(Sequence):\r\n\r\n    def __init__(self, representation_path, target_path, filenames, batch_size):\r\n        self.filenames = np.array(filenames)\r\n        self.batch_size = batch_size\r\n        self.representation_path = representation_path\r\n        self.target_path = target_path\r\n\r\n    def __len__(self):\r\n        return (np.ceil(len(self.filenames) / float(self.batch_size))).astype(np.int)\r\n\r\n    def __getitem__(self, idx):\r\n        files_to_batch = self.filenames[idx * self.batch_size: (idx + 1) * self.batch_size]\r\n        batch_x, batch_y = [], []\r\n        for file in files_to_batch:\r\n            batch_x.append(np.load(self.representation_path + file + \".npy\", allow_pickle=True))\r\n            batch_y.append(np.load(self.target_path + file + \".npy\", allow_pickle=True))\r\n\r\n        return np.array(batch_x), np.array(batch_y)\r\n`\r\n\r\n\r\nWhen i fit the model with a generator, the variable Y assume as None and it shows the error above. How can i fix that?\r\n\r\n**System information**  \r\n- Have I written custom code (as opposed to using example directory):  Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04\r\n- TensorFlow backend (yes / no):  Yes\r\n- TensorFlow version:  2.1.0\r\n- Keras version:  2.3.1\r\n- Python version:  3.6\r\n- GPU model and memory:  NVIDIA Corporation GP108M [GeForce MX250]\r\n", "comments": ["@castilho25 \r\nLooks like code is incomplete. Request you to share colab link or simple standalone code to reproduce the issue in our environment. It helps in localizing the issue faster. Thanks!", "@castilho25 \r\n\r\nAny update on this issue please. Thanks!", "The problem was resolved.\r\n\r\nI imported Sequence from\r\n``from tensorflow_core.python.keras.utils.data_utils import Sequence`\r\n\r\nand the Sequence class should be imported from\r\n''from tensorflow.keras.utils import Sequence'", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36819\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36819\">No</a>\n"]}, {"number": 36818, "title": "How the Frozen model from tensor flow audio recognition is made and converted to tensor flow lite model?", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\nAfter following the steps train.py and freeze.py from the tutorial ,the structure of my frozen model looks like(https://imgur.com/a/JtNVkHw) which is different from the official frozen model conv_actions_frozen.pb(https://imgur.com/a/KJXExbV).\r\n\r\nWhen I Converted Frozen model to Tensorflow lite using the steps:\r\n\r\nimport tensorflow as tf\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\"./my_frozen_graph.pb\", input_arrays=['decoded_sample_data', 'decoded_sample_data:1'], output_arrays=['labels_softmax'])\r\nconverter.allow_custom_ops=True\r\ntflite_model = converter.convert()\r\nopen(\"output.tflite\", \"wb\").write(tflite_model)\r\nThe Tflite model obtained is getting crashed when tested in android studio and the structure of my tflite model (https://imgur.com/a/uceoHlo) is also different from the original speech commands tflite model(https://imgur.com/a/lWmxl9d).\r\n\r\nI suspect something went wrong in the creation of frozen model from the tutorial. Could someone kindly help in generating the frozen model for speech commands.\r\n\r\nTensorflow version-2.1.0\r\n\r\npython version - 3.7.3", "comments": []}, {"number": 36817, "title": "#37075 Readme Updated for Python3.", "body": "Issue : #36807\r\nIssue : #37075\r\nRequired changes made.", "comments": []}, {"number": 36816, "title": "Can tensorflow use multiple ps servers to store an embedding variable? ", "body": "Thank you for reading. \r\nCan tensorflow use multiple parameter servers to store an embedding variable? \r\nMy embedding is too big, the bandwidth of a server is not enough. \r\nWhich module should I use?\r\n\r\nLooking forward to your reply\uff01\r\nThanks\uff01", "comments": ["@Ljl-Jdsk, Tensorflow supports distribution strategy please take a look at [guide](https://www.tensorflow.org/guide/distributed_training#parameterserverstrategy) for parameter server strategy. Thanks!", "Yes, the following code can distribute v1, v2 and v3 to two different parameter servers. But now I want to separate the v1 and store it on two servers. Sorry, can you tell me the specific function? I \u2019m abroad and I can\u2019t open the guide.\r\n\r\ncluster_spec = {\r\n    \"ps\": [\"ps0:2222\", \"ps1:2222\"],\r\n    \"worker\": [\"worker0:2222\", \"worker1:2222\", \"worker2:2222\"]}\r\nwith tf.device(tf.train.replica_device_setter(cluster=cluster_spec)):\r\n  v1 = tf.Variable(...)  # assigned to /job:ps/task:0\r\n  v2 = tf.Variable(...)  # assigned to /job:ps/task:1\r\n  v3 = tf.Variable(...)  # assigned to /job:ps/task:0\r\n", "@Ljl-Jdsk, Please find the example for Parameter server strategy\r\n```\r\nstrategy = tf.distribute.experimental.ParameterServerStrategy()\r\nrun_config = tf.estimator.RunConfig(\r\n    experimental_distribute.train_distribute=strategy)\r\nestimator = tf.estimator.Estimator(config=run_config)\r\ntf.estimator.train_and_evaluate(estimator,...)\r\n```\r\nFor more read [here](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy). Thanks", "@Ljl-Jdsk, Did you get a chance to look at the above given link. Thanks!", "Closing this issue since the title of the issue is resolved. Thanks!"]}, {"number": 36815, "title": "2GB Protobuf limit", "body": "Greetings,\r\n\r\nI am training some deep learning models using Ubuntu Server 18.04 along with the following nvidia docker: nvcr.io/nvidia/tensorflow:20.01-tf1-py3\r\n\r\nTensorflow-gpu 1.15.2\r\nCUDA 10.2\r\nGPU TITAN RTX 24GB\r\n\r\nEverything is working very well and with a solid performance boost compared to my windows setup.\r\nHowever, it is limited to having a set window size of 328,328,328 - once I go with any larger window size the following happens:\r\n\r\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3166, in _as_graph_def\r\ngraph.ParseFromString(compat.as_bytes(data))\r\n\r\nAs far as I understand this has to do with the protbuf limit set at 2GB as default. If I print the length of the data being serialized it is just under 2GB at the working window_size and just over when it is not working.\r\n\r\nIf I run the same script in windows10 (with pip install of tensorflow-gpu==1.15.2), the protobuf error is not present. But it is running out of memory with higher resolutions as windows is using significant amounts of VRAM in background.\r\n\r\nMy question is: How do I raise the limit of the protobuf while using the dockerfile in question. I am not interested in simply lowerering the window size because the higher windowsize we choose, the better is the quality of the output in the end.\r\n\r\nI have tried looking into the ops.py and also the coded_stream.h - without knowing exactly what to change", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36815\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36815\">No</a>\n"]}, {"number": 36814, "title": "Can't import Layer from tensorflow.python.keras.engine.base_layer", "body": "I'm getting an error that says `from tensorflow.python.keras.engine.base_layer import Layer` while importing tensorflow in my app.\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): MacOS\r\n- TensorFlow installed from (source or\r\nbinary): binary\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nOn packaging an app using tensorflow with py2app, I get the following error while running the app: \r\n```\r\nFile \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/__init__.py\", line 36, in <module>\r\n    from tensorflow._api.v1 import compat\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/_api/v1/compat/__init__.py\", line 23, in <module>\r\n    from tensorflow._api.v1.compat import v1\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/_api/v1/compat/v1/__init__.py\", line 40, in <module>\r\n    from tensorflow._api.v1.compat.v1 import experimental\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/_api/v1/compat/v1/experimental/__init__.py\", line 11, in <module>\r\n    from tensorflow.python.ops.control_flow_v2_toggles import output_all_intermediates\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/ops/control_flow_v2_toggles.py\", line 24, in <module>\r\n    from tensorflow.python.ops import control_flow_util_v2\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/ops/control_flow_util_v2.py\", line 28, in <module>\r\n    from tensorflow.python.keras.engine import base_layer_utils\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\r\n    from tensorflow.python.keras import applications\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/keras/applications/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras import engine\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/keras/engine/__init__.py\", line 23, in <module>\r\n    from tensorflow.python.keras.engine.base_layer import Layer\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/keras/engine/base_layer.py\", line 58, in <module>\r\n    from tensorflow.python.keras.saving.saved_model import save as saved_model\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/keras/saving/saved_model/save.py\", line 30, in <module>\r\n    from tensorflow.python.keras.saving import saving_utils\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/keras/saving/__init__.py\", line 33, in <module>\r\n    from tensorflow.python.keras.saving.saved_model_experimental import export_saved_model\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/keras/saving/saved_model_experimental.py\", line 30, in <module>\r\n    from tensorflow.python.keras.utils import mode_keys\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/keras/utils/__init__.py\", line 38, in <module>\r\n    from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/keras/utils/multi_gpu_utils.py\", line 22, in <module>\r\n    from tensorflow.python.keras.engine.training import Model\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/keras/engine/training.py\", line 42, in <module>\r\n    from tensorflow.python.keras import metrics as metrics_module\r\n  File \"/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/keras/metrics.py\", line 34, in <module>\r\n    from tensorflow.python.keras.engine.base_layer import Layer\r\nImportError: cannot import name 'Layer' from 'tensorflow.python.keras.engine.base_layer' (/Users/harshitdwivedi/Desktop/aftershootscripts/dist/aftershoot.app/Contents/Resources/lib/python3.7/tensorflow_core/python/keras/engine/base_layer.py)\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe packaged app should work as expected.\r\n\r\n**Code to reproduce the issue** Provide a reproducible test case that is the\r\nbare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nself.model_path = r\"{}\".format(\r\n            pathlib.Path(os.getcwd()).joinpath('model'))\r\n        self.session = tf.Session(graph=tf.Graph())\r\n        tf.saved_model.loader.load(self.session, ['serve'], self.model_path)\r\n```\r\n\r\nhere's the setup.py that I've used with py2app: \r\n\r\n```\r\n\"\"\"\r\nThis is a setup.py script generated by py2applet\r\n\r\nUsage:\r\n    python setup.py py2app\r\n\"\"\"\r\n\r\nfrom setuptools import setup\r\n\r\nAPP = ['aftershoot.py']\r\nDATA_FILES = []\r\nOPTIONS = {\r\n        'packages' : ['PySide2', 'shiboken2', 'tensorflow_core', 'astor'],\r\n        'resources' : ['exposure/']\r\n        # 'optimize' : 1,\r\n        }\r\n\r\nsetup(\r\n    app=APP,\r\n    data_files=DATA_FILES,\r\n    options={'py2app': OPTIONS},\r\n    setup_requires=['py2app']\r\n)\r\n```", "comments": ["@harshithdwivedi,\r\nI tried to reproduce the error given above but was unable to do so, looks like the given code is incomplete. Please find the Gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/8aad3e50a6496ef82b2443dc629df54a/36814.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Can you try packaging the app with py2app and then try running the packaged app?\r\nThis problem doesn't arise with normally importing tensorflow", "@harshithdwivedi Can you please give the detailed steps to package the app with py2app.Thanks!", "yes the issue still exists.", "Yes The issue is still there please try fixing it asap.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue still exists. ", "This issue still persists.\n\nRegards\nGuramritpal Saggu\n\nOn Tue, 21 Apr 2020, 1:39 am tensorflow-butler[bot], <\nnotifications@github.com> wrote:\n\n> This issue has been automatically marked as stale because it has not had\n> recent activity. It will be closed if no further activity occurs. Thank you.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36814#issuecomment-616780962>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIUWZEHXBZNJIBMUAKIBNVDRNSTVZANCNFSM4KWNXHHA>\n> .\n>\n", "This also happens to me.  I am using Imageai and I downgraded to version 1.14", "This also happen to me when I import tensorflow in matlab api on Windows. \r\nNot sure if this could provide some clue to the bug.....\r\ntensorflow version 1.15.0\r\n```matlab\r\npy.importlib.import_module(\"tensorflow\")\r\n```\r\n\r\n> Error using metrics><module> (line 34)\r\nPython Error: ImportError: cannot import name 'Layer' from 'tensorflow.python.keras.engine.base_layer'\r\n(C:\\Users\\binxu\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py)\r\n\r\nAlthough I can import tensorflow fine in python. in the same environment. ", "> This also happen to me when I import tensorflow in matlab api on Windows.\r\n> Not sure if this could provide some clue to the bug.....\r\n> tensorflow version 1.15.0\r\n> \r\n> ```matlab\r\n> py.importlib.import_module(\"tensorflow\")\r\n> ```\r\n> \r\n> > Error using metrics> (line 34)\r\n> > Python Error: ImportError: cannot import name 'Layer' from 'tensorflow.python.keras.engine.base_layer'\r\n> > (C:\\Users\\binxu.conda\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py)\r\n> \r\n> Although I can import tensorflow fine in python. in the same environment.\r\n\r\nI found that the problem occurs when I was downgrading Tensorflow to version 1.14. What I did was to keep version 2.+ then I did not find this error.\r\n\r\nI needed to downgrade since everywhere it said that YOLO required Tensorflow 1.14. If that is your case you need to edit the YOLO code. everywhere it says import keras._whatever_, add tensorflow.keras._whatever_ \r\n\r\nIf if it not your case and you have limitation with Tensorflow try upgrading it and you should not get this erro.", "I also have same issue on Windows 10 Enterprise after packaging my python project using Pyinstaller.\r\nTf version=1.15", "Locate `tensorflow_core/python/keras/metrics.py` and on line 34 change the import to `from tensorflow_core.python.keras.engine.base_layer import Layer` - you might have to do this for every subsequent error thrown due to the mismatched library names.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36814\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36814\">No</a>\n", "in my problem,I have two  Anaconda versions in PATH, I delete the other Anaconda path in my PATH(windows system) , Solve the problem\r\n..\r\nfirst my error info:\r\n\"ImportError: cannot import name 'Layer' from partially initialized module 'tensorflow.python.keras.engine.base_layer' (most likely due to a circular import) (D:\\download\\Anaconda3-2021.05-python3.8\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py)\"\r\n\r\n\r\ndelete the other Anaconda path in my PATH(windows system) , Solve the problem\r\nOK..\r\n>>> import tensorflow\r\n2021-08-28 20:07:35.466805: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\nWARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\r\n>>>\r\n", "In my case I was trying to run tensorflow(2.0.0a0) in my django(3.0) app and while using tensorflow in the backend , I got the following error.\r\n![error](https://user-images.githubusercontent.com/36479349/131254101-d5ef9a77-5488-4e6c-b242-54cdb8731953.PNG)\r\n", "I also have this problem.How do you solve this problem in last?", "How did you solve the problem? I keep getting the same error"]}, {"number": 36813, "title": "utils", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@nikunjramani, Provide all the information asked in the Template. Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36812, "title": "Add sorted builtin support for autograph", "body": "This PR adds `sorted` builtin support for tensor in autograph.\r\nSince `sorted` has useful sort by comparison key, it might make sense to add `sorted`\r\nas the supported list of ops for autograph.", "comments": ["@lyonguyen8697 Can you please fix the build failures? Thanks!", "@lyonguyen8697 Can you please fix the build failures? Thanks!", "I think this will be tricky to address since there's a circular dependency, possibly by parallel_for. We're working on a broader effort to remove these circular deps, but in the mean time, I think this will need a workaround:\r\n * remove the parallel_for dep from the BUILD file (if tests fail, add the dependency to the tests alone) - bazel rejects circular imports outright\r\n * in the Pyhton file, use LazyLoader to import the module (look at the existing one for an example) - that will prevent import errors due to these cycles", "@mdanatg thanks for your help. So I removed `parallel_for` from BUILD file and added this code to the python file:\r\n`parallel_ops = lazy_loader.LazyLoader(\r\n    'parallel_ops', globals(),\r\n     'tensorflow.python.ops.parallel_for.control_flow_ops'\r\n)`\r\nAm I doing this correctly?", "It's still raise an import error. Look like the recently added `gen_spectral_ops` in 3a552b92f4ab783ca9984fe9a350aafec1e32349 that cause the error.\r\n\r\n`Traceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/autograph/operators/py_builtins_test.runfiles/org_tensorflow/tensorflow/python/autograph/operators/py_builtins_test.py\", line 395, in test_sorted_tensor\r\n    py_builtins.sorted_(iterable_1, key=lambda x: -x))), [3, 2, 1])\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/autograph/operators/py_builtins_test.runfiles/org_tensorflow/tensorflow/python/autograph/operators/py_builtins.py\", line 472, in sorted_\r\n    return _tf_sorted(iterable, key, reverse)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/autograph/operators/py_builtins_test.runfiles/org_tensorflow/tensorflow/python/autograph/operators/py_builtins.py\", line 483, in _tf_sorted\r\n    mapped = parallel_ops.vectorized_map(key, iterable)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/autograph/operators/py_builtins_test.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/autograph/operators/py_builtins_test.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/autograph/operators/py_builtins_test.runfiles/org_tensorflow/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 34, in <module>\r\n    from tensorflow.python.ops.parallel_for.pfor import PFor\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/autograph/operators/py_builtins_test.runfiles/org_tensorflow/tensorflow/python/ops/parallel_for/pfor.py\", line 53, in <module>\r\n    from tensorflow.python.ops import gen_spectral_ops\r\nImportError: cannot import name 'gen_spectral_ops'`", "Ok, that was somewhat expected. This is why we need to fix these terrible circular dependencies. Anyhow, this should be resolved by adding the pfor dependency (the one you had to remove from the BUILD file) to the test target only, that is, add it to `autograph/operators/BUILD`, under the rule with `name = \"py_builtins_test\"`.", "@allenlavoie do you know which dependency is still missing here? Pulling `//tensorflow/python/ops/parallel_for:control_flow_ops` alone gives an import error inside pfor.py:\r\n\r\n```\r\nopt/bin/tensorflow/python/autograph/operators/py_builtins_test.runfiles/org_tensorflow/tensorflow/python/ops/parallel_for/pfor.py\", line 53, in <module>\r\n    from tensorflow.python.ops import gen_spectral_ops\r\nImportError: cannot import name 'gen_spectral_ops'\r\n```", "Probably //tensorflow/python/ops/parallel_for:parallel_for makes more sense there? But @agarwal-ashish would know for sure."]}, {"number": 36811, "title": "#36693 Update mnist_with_summaries.py", "body": "Spelling of Placeholders at line 53 solved\r\n\r\nIssue : #36693", "comments": ["closing as it is not against master branch."]}, {"number": 36810, "title": "Gradient Support for Unique operator", "body": "I am trying to run a custom graph with only Unique op for a debugging.\r\n\r\n`with tf.GradientTape() as g:\r\ny = tf.unique(x)\r\ndy_dx = g.gradient(y, x)`\r\n\r\nThe Forward pass runs fine, but getting this error during Backward pass:\r\n\r\n> LookupError: gradient registry has no entry for: Unique\r\n\r\nGradient yet to be supported for Unique (or) there is no gradient for Unique ? Please help me understand why incase if its the second one.\r\n", "comments": ["@Prasandhmcw please provide us with complete executable code and the tensorflow version to replicate the issue faced by you in our environment.", "File : (please rename to .py)\r\n\r\n[unique.txt](https://github.com/tensorflow/tensorflow/files/4213491/unique.txt)\r\n\r\nTF version : \r\n`>>> tf.__version__\r\n'1.14.0'`", "i have replicated the code shared and issue persist, please find the gist [here](https://colab.sandbox.google.com/gist/Saduf2019/2299cbbb43b152bfdb731fc81e005086/36810.ipynb)", "@gowthamkpr the labels have been changed, does this mean Gradient for Unique is not yet supported ?", "I think you have to register this op as mentioned [here](https://stackoverflow.com/questions/51777535/gradient-registry-has-no-entry-for-a-tensorflow-custom-op) @Prasandhmcw ", "@gowthamkpr But this ain't a Custom Op. Its one of the TF's operator - https://www.tensorflow.org/api_docs/python/tf/unique", "I don't belive there's a gradient, but re-assigning to Yong Tang for double-checking.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36810\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36810\">No</a>\n"]}, {"number": 36808, "title": "Is Tensorflow impossible to predict multiple steps?", "body": "Thank you for reading. I'm not good at English. \r\n\r\nI am wondering how to predict and get future time series data after model training. I would like to get the values after N steps.\r\n\r\nSo, I used the time series in the Tensorflow tutorial to practice predicting the model.\r\n\r\n```\r\na = y_val[-look_back:] \r\nfor i in range(N-step prediction): #predict a new value n times.\r\n    tmp = model.predict(a.reshape(-1, look_back, num_feature)) #predicted value     \r\n    a = a[1:] #remove first     \r\n    a = np.append(a, tmp) #insert predicted value\r\n```\r\nThe results are predicted by linear graphs from nonlinear graphs as shown below.\r\n\r\n![1](https://imgur.com/7tenqRd.png)\r\n\r\nHow can I get the actual, right solution? or Is Tessorflow impossible to predict multiple steps?\r\n\r\n[full source](https://gist.github.com/Lay4U/96e0ba8d8c251046e89eae4bc5d40510)  (After the 25th line is my code.)\r\n\r\n[epoch=100](https://gist.github.com/Lay4U/3c530649934af39a6c6e9884a8eb14fd)", "comments": ["You did not train the model (no `.fit` method is called).", "> You did not train the model (no `.fit` method is called).\r\n\r\nThank you. Updated the .fit call.", "@Lay4U, Are you happy to close this issue as it is resolved. Thanks!", "> @Lay4U, Are you happy to close this issue as it is resolved. Thanks!\r\n\r\nNo.. The problem was updated and not solved. Still predict plots linear.", "I am not part of official TF development team, but I believe this issue tracker should be used to report bugs or feature requests. In this case there seems to be nothing wrong with TF, but instead you would like to get help about TF -- there are better places where to get it, for example on https://stackoverflow.com.\r\n\r\nBTW, you should probably not use `y_val_uni` on _input_, given it contains valid _outputs_, you should instead use one of `x_val_uni`. And after 100 steps where the model gets its own predictions (which are not particularly good) on input, the results are not very surprising.", "> I am not part of official TF development team, but I believe this issue tracker should be used to report bugs or feature requests. In this case there seems to be nothing wrong with TF, but instead you would like to get help about TF -- there are better places where to get it, for example on https://stackoverflow.com.\r\n> \r\n> BTW, you should probably not use `y_val_uni` on _input_, given it contains valid _outputs_, you should instead use one of `x_val_uni`. And after 100 steps where the model gets its own predictions (which are not particularly good) on input, the results are not very surprising.\r\n\r\nThank you for answer and when I use `x_val_uni` and after `20 steps` result is [here](https://gist.github.com/Lay4U/4471f4ed9b80a615de42be54fdaaa4ed)\r\n\r\nI think this method also not right how do you think?", "@Lay4U Please go through this [tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series). Closing this issue as it has been resolved. If you have any more questions and are related to bug/performance, build/install, docs or feature request related issues please create a new issue. If they are not related, please post them in stackoverflow. Thanks!"]}, {"number": 36807, "title": "wrong doc for categorical_hinge loss ", "body": "## Description of issue (what needs changing):\r\n\r\ndocument for tensorflow.keras.losses.categorical_hinge is wrong\r\n\r\n### Clear description\r\n\r\n```python\r\n@keras_export('keras.losses.categorical_hinge')\r\ndef categorical_hinge(y_true, y_pred):\r\n  \"\"\"Computes the categorical hinge loss between `y_true` and `y_pred`.\r\n  `loss = maximum(neg - pos + 1, 0)`\r\n  where `neg = sum(y_true * y_pred)` and `pos = maximum(1 - y_true)`\r\n  Args:\r\n    y_true: The ground truth values. `y_true` values are expected to be -1 or 1.\r\n      If binary (0 or 1) labels are provided they will be converted to -1 or 1.\r\n    y_pred: The predicted values.\r\n  Returns:\r\n    Categorical hinge loss values.\r\n  \"\"\"\r\n  y_pred = ops.convert_to_tensor_v2(y_pred)\r\n  y_true = math_ops.cast(y_true, y_pred.dtype)\r\n  pos = math_ops.reduce_sum(y_true * y_pred, axis=-1)\r\n  neg = math_ops.reduce_max((1. - y_true) * y_pred, axis=-1)\r\n  return math_ops.maximum(0., neg - pos + 1.)\r\n```\r\n\r\nShould be: `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`\r\n", "comments": ["Can i be assigned to this issue?\r\n", "@ayushmankumar7 feel free to send me a PR.", "Sure. Thanks", "Closing this issue since the associated PR has been merged. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36807\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36807\">No</a>\n"]}, {"number": 36806, "title": "how to use pretrain weight", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\n-----------------------------------------------------------------------------------------------------------\r\n\r\nHi, can somenoe guide me how to use quantization aware with a pretrain weight(32bit)?\r\nI have already trained a 32bit weight. Can I make it be a pretrain weight and train by quantization aware(tf.contrib.quantize.create_training_graph(quant_delay=300000))?\r\n\r\nThank you.", "comments": ["@aluds123  Please refer to the link shared to raise this issue [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow)"]}, {"number": 36805, "title": "Convert `std::vector<tensorflow::Tensor>` into `std::vector<float>`", "body": "How to convert Tensor `<type: float shae: [1, 512, 800, 1] >` into `std::vector<float>` ?\r\n\r\nI've tried [stackoverflow](https://stackoverflow.com/questions/44843368/convert-tensorflow-tensor-of-variable-size-to-stdvector-in-c) and some others  also, but failed\r\nThank you beforehand.", "comments": ["@Abdulazizbek, Can you provide the Tensorflow version that you are using and also sample code snippet. Thanks!", "Tensorflow version: Tensorflow C++ API 1.14\r\nBazel: 0.26\r\n\r\n```\r\nstring graphPath = \"weights/weights.pb\";\r\nstring image_path = \"1.jpg\";\r\n\r\n//detection tensor node names:\r\nstring input_det = \"input_images:0\";\r\nvector <string> out_det_layers = {\"Conv_6/Relu:0\", \"Conv_7/Sigmoid:0\",  \"concat_3:0\"};\r\n\r\nstd::unique_ptr<tensorflow::Session> session;\r\n LOG(INFO) << \"graphPath:\" << graphPath;\r\n\r\nStatus loadGraphStatus = loadGraph(graphPath, &session);\r\n\r\ncv::Mat image = cv::imread(image_path, cv::IMREAD_COLOR);\r\ncv::cvtColor(image, image, cv::COLOR_BGR2GRAY);\r\n\r\nTensor resized_tensor(DT_FLOAT, TensorShape({1,image.rows, image.cols, 1}));\r\nCVMat_to_Tensor(image, &resized_tensor, image.rows, image.cols);\r\ncout <<\"Resized tensor: \" << resized_tensor.DebugString()<<endl;\r\n// output\r\n\r\nvector<tensorflow::Tensor> outputs;\r\nvector <string> output_node = out_det_layers;\r\n\r\nStatus status_run = session->Run({{input_det, resized_tensor}}, {output_node}, {}, &outputs);\r\n\r\ncout << \"Output tensor size:\" << outputs.size() << std::endl;\r\n```\r\n// output\r\n```\r\nResized tensor: Tensor<type: float shape: [1,512,800,1] values: [[[0.737254858][0.737254858][0.729411721]]]...>\r\n```", "You can directly copy using data pointers. For int32_t use something similar to the code below.\r\n\r\n```\r\nauto tf_tensor_to_vector(tensorflow::Tensor tensor, int32_t tensorSize) {\r\n  int32_t* tensor_ptr = tensor.flat<int32_t>().data();\r\n  std::vector<int32_t> v(tensor_ptr, tensor_ptr + tensorSize);\r\n  return v;\r\n}\r\n```\r\n\r\nMore information is available in the tensor.h header file.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.h", "Thank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36805\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36805\">No</a>\n"]}]