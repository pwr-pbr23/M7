[{"number": 36682, "title": "unspecified errors while building tensorflow", "body": "\r\n**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): window 10 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary): build from source\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version: python 3.8.1\r\n- Bazel version (if compiling from source):0.29.1\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\n\r\nWhile building tensorflow/examples/label_image,build process fails with unspecified error code changing everytime i build. I think this error is due to lack of memory of my laptop. but can't find the reason of it with error code i got from it.\r\nAttaching three error codes i got while building.\r\n\r\nERROR: C:/users/gyuhwan/_bazel_gyuhwan/qrf5imf6/external/com_google_protobuf/BUILD:759:1: ProtoCompile external/com_google_protobuf/python/google/protobuf/field_mask_pb2.py failed (Exit -1). Note: Remote connection/protocol failed with: execution failed\r\nAction failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(\"C:\\users\\gyuhwan\\_bazel_gyuhwan\\qrf5imf6\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\external\\com_google_protobuf\\protoc.exe\" --python_out=bazel-out/x64_windows-py2-opt/bin/external/com_google_protobuf/python -Iexternal/com_google_protobuf/python -Ibazel-out/x64_windows-py2-opt/bin/external/com_google_protobuf/python bazel-out/x64_windows-py2-opt/bin/external/com_google_protobuf/python/google/protobuf/field_mask.proto): ?\u226a\uaf6d?\u317a? \u5ac4\uacd5??\uc10f\ubfc0?\ub4ec\ub572??\r\n\r\nERROR: C:/users/gyuhwan/tensor_flow/tensorflow/tensorflow/core/kernels/boosted_trees/BUILD:22:1: ProtoCompile tensorflow/core/kernels/boosted_trees/boosted_trees.pb.h failed (Exit -1). Note: Remote connection/protocol failed with: execution failed\r\nAction failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(\"C:\\users\\gyuhwan\\_bazel_gyuhwan\\qrf5imf6\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\external\\com_google_protobuf\\protoc.exe\" --cpp_out=bazel-out/x64_windows-py2-opt/bin -I. -Iexternal/com_google_protobuf/src -Ibazel-out/x64_windows-py2-opt/bin/external/com_google_protobuf/src tensorflow/core/kernels/boosted_trees/boosted_trees.proto): ?\u226a\uaf6d?\u317a? \u5ac4\uacd5??\uc10f\ubfc0?\ub4ec\ub572??\r\n (error: 5)\r\n\r\nERROR: C:/users/gyuhwan/tensor_flow/tensorflow/tensorflow/core/BUILD:2499:1: ProtoCompile tensorflow/core/protobuf/saved_model_pb2.py failed due to unexpected I/O exception: C:\\users\\gyuhwan\\_bazel_gyuhwan\\qrf5imf6\\execroot\\org_tensorflow\\bazel-out\\_tmp\\action_outs\\stderr-3 (?? ????? ??? ?? ??? ??? ????? ??? ? ? ????)\r\njava.io.FileNotFoundException: C:\\users\\gyuhwan\\_bazel_gyuhwan\\qrf5imf6\\execroot\\org_tensorflow\\bazel-out\\_tmp\\action_outs\\stderr-3 (?? ????? ??? ?? ??? ??? ????? ??? ? ? ????)\r\n        at java.base/java.io.FileOutputStream.open0(Native Method)\r\n        at java.base/java.io.FileOutputStream.open(Unknown Source)\r\n        at java.base/java.io.FileOutputStream.<init>(Unknown Source)\r\n        at java.base/java.io.FileOutputStream.<init>(Unknown Source)\r\n        at com.google.devtools.build.lib.vfs.AbstractFileSystem$ProfiledFileOutputStream.<init>(AbstractFileSystem.java:153)\r\n        at com.google.devtools.build.lib.vfs.AbstractFileSystem.createFileOutputStream(AbstractFileSystem.java:89)\r\n        at com.google.devtools.build.lib.vfs.AbstractFileSystem.getOutputStream(AbstractFileSystem.java:101)\r\n        at com.google.devtools.build.lib.vfs.Path.getOutputStream(Path.java:535)\r\n        at com.google.devtools.build.lib.vfs.Path.getOutputStream(Path.java:523)\r\n        at com.google.devtools.build.lib.util.io.FileOutErr$FileRecordingOutputStream.getOutputStream(FileOutErr.java:353)\r\n        at com.google.devtools.build.lib.util.io.FileOutErr$FileRecordingOutputStream.write(FileOutErr.java:460)\r\n        at com.google.devtools.build.lib.exec.local.LocalSpawnRunner$SubprocessHandler.start(LocalSpawnRunner.java:365)\r\n        at com.google.devtools.build.lib.exec.local.LocalSpawnRunner$SubprocessHandler.run(LocalSpawnRunner.java:192)\r\n        at com.google.devtools.build.lib.exec.local.LocalSpawnRunner.exec(LocalSpawnRunner.java:153)\r\n        at com.google.devtools.build.lib.exec.SpawnRunner.execAsync(SpawnRunner.java:225)\r\n        at com.google.devtools.build.lib.exec.AbstractSpawnStrategy.exec(AbstractSpawnStrategy.java:123)\r\n        at com.google.devtools.build.lib.exec.AbstractSpawnStrategy.exec(AbstractSpawnStrategy.java:88)\r\n        at com.google.devtools.build.lib.actions.SpawnActionContext.beginExecution(SpawnActionContext.java:41)\r\n        at com.google.devtools.build.lib.exec.ProxySpawnActionContext.beginExecution(ProxySpawnActionContext.java:60)\r\n        at com.google.devtools.build.lib.actions.SpawnContinuation$1.execute(SpawnContinuation.java:80)\r\n        at com.google.devtools.build.lib.analysis.actions.SpawnAction$SpawnActionContinuation.execute(SpawnAction.java:1344)\r\n        at com.google.devtools.build.lib.analysis.actions.SpawnAction.beginExecution(SpawnAction.java:314)\r\n        at com.google.devtools.build.lib.actions.Action.execute(Action.java:123)\r\n        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor$4.execute(SkyframeActionExecutor.java:851)\r\n        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor$ActionRunner.continueAction(SkyframeActionExecutor.java:985)\r\n        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor$ActionRunner.run(SkyframeActionExecutor.java:957)\r\n        at com.google.devtools.build.lib.skyframe.ActionExecutionState.runStateMachine(ActionExecutionState.java:116)\r\n        at com.google.devtools.build.lib.skyframe.ActionExecutionState.getResultOrDependOnFuture(ActionExecutionState.java:77)\r\n        at com.google.devtools.build.lib.skyframe.SkyframeActionExecutor.executeAction(SkyframeActionExecutor.java:577)\r\n        at com.google.devtools.build.lib.skyframe.ActionExecutionFunction.checkCacheAndExecuteIfNeeded(ActionExecutionFunction.java:760)\r\n        at com.google.devtools.build.lib.skyframe.ActionExecutionFunction.compute(ActionExecutionFunction.java:275)\r\n        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:451)        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n        at java.base/java.lang.Thread.run(Unknown Source)\r\n\r\n**Describe the expected behavior**\r\n\r\nit would be helpful if error code notice the user that error is because of insufficient memory or notification of expected performance of computer\r\n\r\n**Code to reproduce the issue**\r\n\r\nbazel build /tensorflow/examples/label_image/...\r\n\r\n**Other info / logs**\r\n\r\nCPU of my computer is i7 1065G7 and memory is 8G\r\n", "comments": ["@themusmus, Provide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "following steps of [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image](url)\r\nexecuted the\r\n`$ curl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" |\r\n  tar -C tensorflow/examples/label_image/data -xz`\r\ncode and run\r\n`$ bazel build tensorflow/examples/label_image/...`\r\nwhich keeps making errors wrote above", "Python 3.8 is a work in progress and not supported yet. Thanks!\r\nFor more info. duplicate #36790 \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36682\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36682\">No</a>\n"]}, {"number": 36681, "title": "How to build libhexagon_nn_skel.so ?", "body": "Hi\r\nTFLite hexagon delegate is based on hexagon nnlib, I can get nnlib source code from https://source.codeaurora.org/quic/hexagon_nn/nnlib, and I can build ibhexagon_nn_skel.so according to \"README.HOW_TO_BUILD\" in nnlib, but my build libhexagon_nn_skel.so can not be used for tflite, report some error.\r\nDoes  tflite modify the nnlib code ? how to build libhexagon_nn_skel.so using the nnlib source code ?\r\n", "comments": ["@huanyingjun Github is only meant for bug/performance, build/install, feature request or docs related issues. \r\nPlease post it in stack overflow as there is a wider community to respond. Thanks!", "Hello everyone, I post this here because I tried the same and I think it might be good to have another precedent.\r\n\r\nI have implemented tflite natively with c (no java) on an android service. The target platform is a Snapdragon 625 (Qualcomm Technologies, Inc MSM8953). Using the Hexagon delegate as mentioned in [https://www.tensorflow.org/lite/performance/hexagon_delegate](https://www.tensorflow.org/lite/performance/hexagon_delegate) I got the following warning:\r\n\r\n>WARNING: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\nINFO: Hexagon Delegate is not supported.\r\n\r\nI reach a point where I cannot build libhexagon_nn_skel.so for SD625 using the Hexagon SDK and the [nnlib](https://source.codeaurora.org/quic/hexagon_nn/nnlib) project. \r\n\r\nDo you have the libraries for this platform available? and if not supported at this moment, will it be supported in the future? or... what should I do to have the libraries I need?\r\n\r\nThank you!"]}, {"number": 36680, "title": "Image Augmentation erroring out with ImageDataGenerator.flow and multiple labels", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.1.0-rc0\r\n- **Python version**:3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**: Given source code ur\r\nl\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nGetting error : \"tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\" when running as python script.\r\n\r\nBut same code running fine in notebook mode\r\n\r\n### Source code / logs\r\nhttps://www.kaggle.com/anirbank/bengali-graphemes-multichannelcnn-with-dataaug", "comments": ["Please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/35100) and let me know if it helps. Thanks!", "@anirbankonar123 Please respond to the comment above. Thanks!", "Closing this issue as it has been inactive for more than 2 weeks. Please add additional comments and we can open the issue again. Thanks!"]}, {"number": 36679, "title": "tf.keras.layers.Softmax should respect image_data_format by default", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTensorflow allows one to set image_data_format of either channels_last (set by default, NHWC) or channels_first (NCHW) using `tf.keras.backend.set_image_data_format(data_format)`. Doing so nicely results in almost all layers of the `tf.keras.model` adjust appropriately by either bringing the channel last or first. Even the model.load_weights() work perfectly fine (and transparently), respecting user's choice of image_data_format, as it should. The story is somewhat different when it comes to `tf.keras.layers.Softmax`. Softmax layer/activation is often used in classification problems to normalize the output of the model and such normalization is applied w.r.t to an axis. Currently, Softmax implementation always sets the default value of axis = -1, which is the last axis. This works perfectly fine when image_data_format is channels_last but it for channels_first, one needs to explicitly set the axis to 1. \r\n\r\nThis feels like slightly out of sync compared to how all the other layers behave. They respect the image_data_format automatically and adjust accordingly. So why not Softmax? I am pretty sure 90% of the use cases of Softmax applies the function along the channels axis, so why not make it respect the data format automatically,too ?\r\n\r\nThis is also easy to miss for folks just getting started with TensorFlow. The docs does not mention anything remotely about this.\r\n\r\n**Will this change the current api? How?**\r\nYes. `tf.keras.layers.Softmax` would start respecting the default set_image_data_format set by user and hence axis would be -1 incase of NHWC and 1 incase of NCHW.\r\n\r\nThis would also allow one to easily specify `activation=\"softmax\"` in the last layer instead of adding one explicit line like :\r\n\r\n`model.add(tf.keras.layers.Softmax(axis=-1 if data_format==\"channels_last\" else 1))`\r\n\r\n**Who will benefit with this feature?**\r\nA big group of people who are either using Softmax in its default form (i.e. to normalize along channels axis) or someone who is getting started with TF and wants to have flexibility of changing image data layouts. This would render a uniform API experience and expectation to end-user, trusting that tensorflow's layers are respecting the global image data format without much hassle or individual treatment.\r\n\r\n**Any Other info.**\r\n", "comments": ["@dd1923 Looks like this is a duplicate of your another feature request https://github.com/tensorflow/tensorflow/issues/39230\r\n\r\nAs that issue was closed by a PR https://github.com/tensorflow/tensorflow/pull/47412, can we close this issue? Thanks!", "Does softmax now work with channels last and first?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Just seeing this now -- the reason why `Softmax` cannot default to a different `axis` value based on `image_data_format` is that it isn't an image-processing layer (unlike `Conv2D`), and it has no way of knowing whether the tensor it's processing represents image features or not.\r\n\r\nA rule like \"if the input tensor is 4D, and `image_data_format == 'channels_first'`, then use `axis=1`\" would be very error-prone. It is much better to let the user specify their own `axis`value. ", "Can it raise a warning instead? I am sure for most users would want to have axis = 1 when image_data_format is channels first. If they don't specify an axis and if image_data_format is channels first, can it raise a warning or message so that the user is atleast aware?"]}, {"number": 36678, "title": "Tensorflow installation document Korean translated page outdated", "body": "## URL(s) with the issue \r\n\r\nhttps://www.tensorflow.org/install?hl=ko\r\n\r\n## Description of issue (what needs changing)\r\n\r\nOn installation document/windows build from source page, it tells that \"TensorFlow\ub97c \ucef4\ud30c\uc77c\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\ub294 \ube4c\ub4dc \ub3c4\uad6c\uc778 Bazel 0.23.0\uc744 \uc124\uce58\ud569\ub2c8\ub2e4. C++\ub97c \ube4c\ub4dc\ud558\ub3c4\ub85d Bazel\uc744 \uc124\uc815\ud569\ub2c8\ub2e4.\" which means install Bazel 0.23.0 to compile Tensorflow. However, most recent version of Tensorflow which is r2.1, doesn't support Bazel 0.23.0. Instead, it uses 0.27.0~0.29.0. I checked English document and it tells me the version of Bazel that is needed. So, I think the Korean page should be renewed.\r\n\r\n\r\n", "comments": ["Hi @jungmin-lim, \r\n\r\nThanks for reporting this. The docs translations in the \r\nhttps://github.com/tensorflow/docs-l10n repository.  \r\n\r\nCan you copy this issue to that repository, and close this one? Thanks.\r\n\r\nAlso note that these translations are community owned. Since you seem to know how to correct this, I'm sure they'd love it if you can send a PR too.\r\n\r\nThanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36678\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36678\">No</a>\n"]}, {"number": 36677, "title": "Custom training loss with custom gradients", "body": "I am trying to write a custom loss in Tensorflow v2, for simplicity let's say that I'm using Mean Squared Error loss as follows,\r\n```\r\nloss_object = tf.keras.losses.MeanSquaredError()\r\n\r\n\r\ndef loss(model, x, y, training):\r\n  # training=training is needed only if there are layers with different\r\n  # behavior during training versus inference (e.g. Dropout).\r\n  y_ = model(x, training=training)\r\n  return loss_object(y_true=y, y_pred=y_)\r\n```\r\n\r\nNow I know that Tensorflow does [automatic differentiation](https://stackoverflow.com/questions/48219296/why-doesnt-keras-need-the-gradient-of-a-custom-loss-function).\r\n\r\nBut I want to specify my **custom gradient**, in the BackPropagation algorithm,\r\nif we use MSE, we have to do the following \r\n\r\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\frac{\\partial&space;E}{\\partial&space;w}&space;=&space;\\frac{\\partial&space;E}{\\partial&space;y_i}&space;\\frac{\\partial&space;y_i}{\\partial&space;net_i}&space;\\frac{\\partial&space;net_i}{\\partial&space;w}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial&space;E}{\\partial&space;w}&space;=&space;\\frac{\\partial&space;E}{\\partial&space;y_i}&space;\\frac{\\partial&space;y_i}{\\partial&space;net_i}&space;\\frac{\\partial&space;net_i}{\\partial&space;w}\" title=\"\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial y_i} \\frac{\\partial y_i}{\\partial net_i} \\frac{\\partial net_i}{\\partial w}\" /></a>\r\n\r\n<img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial&space;E}{\\partial&space;w}&space;=&space;(y&space;-&space;\\hat{y})*&space;D(activation)&space;*&space;x\" title=\"\\frac{\\partial E}{\\partial w} = (y - \\hat{y})* D(activation) * x\" />\r\n\r\nIs it possible in **Keras** to replace <a href=\"https://www.codecogs.com/eqnedit.php?latex=(y&space;-&space;\\hat{y})\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?(y&space;-&space;\\hat{y})\" title=\"(y - \\hat{y})\" /></a>  with <a href=\"https://www.codecogs.com/eqnedit.php?latex=(y&space;-&space;p)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?(y&space;-&space;p)\" title=\"(y - p)\" /></a> where `p` is a tensor that is passed during training before applying gradients.\r\n\r\n", "comments": ["@gadagashwini can you take a look at this.", "@rohanmohapatra Could you please provide with an implementable stand alone code so we could help you resolve the issue.", "@Saduf2019 Hi, what I'm interested is how is the gradient of the loss w.r.t to the `pred_y` calculated? I want to replace the that gradient with my custom gradient value.", "@rohanmohapatra, Tensorflow provides two ways  to add custom loss with Keras. Please refer [this doc](https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses). Thanks! ", "@gadagashwini Can I also change the value of the backprop gradient for the loss? I do not want to let Tensorflow use the AutoGrad for my loss. ", "I have the same issue - is there a way to define a custom gradient when using keras API?", "@rohanmohapatra,\r\nSorry for the delayed response. Can you please look into the Documentation of [Custom Gradients](https://www.tensorflow.org/api_docs/python/tf/custom_gradient) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36677\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36677\">No</a>\n"]}, {"number": 36676, "title": "TF2 SavedModel does not support multiple signatures? ", "body": "I used `tf.saved_model.save` and `tf.saved_model.load` to save and load TF2 SavedModel. According to [this link][1], I created a signature and this signature is `serving_default`. Then I try to add a new function with the signature decorator in class `Adder`. But after I loaded the model according to [this][2], I find that the signatures disappear in the model, i.e., `print(adder1.signatures)` prints no signature names. I don't find any information about how to use multiple signatures while saving models. So I think this may be a bug. If it is not, can anyone tell me how can I use multiple signatures in one model? Thank you very much. \r\n\r\nTensorflow `2.1.0`, on Google Colab. The code looks like this: \r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport numpy as np\r\nimport os\r\nimport pandas as pd\r\n\r\nclass Adder(tf.Module):\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32), tf.TensorSpec(shape=None, dtype=tf.float32)])# \r\n  def add(self, x, y):\r\n    return x + y ** 2 + 1\r\n  \r\n  @tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\r\n  def square(self, x):\r\n    return x ** 2\r\n\r\nto_export = Adder()\r\ntf.saved_model.save(\r\n    to_export, \r\n    '/tmp/adder'            \r\n)\r\n\r\nadder1 = tf.saved_model.load(\"/tmp/adder\")\r\nprint(adder1.signatures)\r\nadder1_sig = adder1.signatures[\"serving_default\"]\r\nadder1_sig(x = tf.constant(1.), y = tf.constant(2.))\r\n```\r\n\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/saved_model/save#used-in-the-notebooks\r\n  [2]: https://www.tensorflow.org/api_docs/python/tf/saved_model/load", "comments": ["If the signatures argument is omitted, obj will be searched for @tf.function-decorated methods. And as you have two such function so `print(adder1.signatures)` prints no signature names. You can use only one signature while saving models.\r\n```to_export = Adder()\r\ntf.saved_model.save(\r\n    to_export, \r\n    '/tmp/adder',\r\n    signatures=to_export.add\r\n)\r\nadder1 = tf.saved_model.load(\"/tmp/adder\")\r\nprint(adder1.signatures)\r\nadder1_sig = adder1.signatures[\"serving_default\"]\r\nadder1_sig(x = tf.constant(1.), y = tf.constant(2.))\r\n\r\n#or \r\n\r\nto_export = Adder()\r\ntf.saved_model.save(\r\n    to_export, \r\n    '/tmp/adder' ,\r\n    signatures=to_export.square        \r\n)\r\n\r\nadder1 = tf.saved_model.load(\"/tmp/adder\")\r\nadder1 = tf.saved_model.load(\"/tmp/adder\")\r\nprint(adder1.signatures)\r\nadder1_sig = adder1.signatures[\"serving_default\"]\r\nadder1_sig(x = tf.constant(1.))", "@khimraj , you're close!\r\n\r\nBut you can export multiple signatures. You just need to pass a dictionary as the `signatures` argument. See [here](https://www.tensorflow.org/guide/concrete_function#example_setting_the_signature_names). for example. "]}, {"number": 36675, "title": "[TF2] TRT Engine Ops are not garbage collected, resulting in incorrect reuse", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 10.0 cuDNN 7.6.5, TensorRT 6.0.1\r\n- GPU model and memory: RTX 2080 \r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen TRTEngineOps are created for different runs back to back, the first one is not properly cleared, resulting in potential reuse with an incorrect configuration.\r\n\r\nIn the reproducing case, running the dynamic input shapes TFTRT test creates and uses an fp16 engine op as expected, but the fp32 test does not create a new instance, instead reusing the one from before.  This behavior may result in the test failing, depending on the particular numerical results and tolerances.  In the case where the test passes, the bug can still be observed by parsing through the logs from the run (some snippets of mine are included below).\r\n\r\nI'm not sure if the issue is limited to TFTRT, or is the result of a larger problem with the resource manager in TF2.  This forced garbage collection is likely an attempted workaround for the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py#L372\r\n\r\n**Describe the expected behavior**\r\n\r\nCreated TRTEngineOps should be handled by the resource manager such that they aren't reused in a separate invocation.\r\n\r\n**Code to reproduce the issue**\r\n\r\nRunning the dynamic_input_shapes_test.py with increased verbosity will demonstrate the failure in the logs:\r\n`env TF_CPP_VMODULE=convert_nodes=2,trt_engine_op=1 python tensorflow/python/compiler/tensorrt/test/dynamic_input_shapes_test.py`\r\n\r\n**Other info / logs**\r\n\r\n(Full log attached)\r\nIn the first creation of a TRTEngineOp for the test, we can see that none are found in the TF-TRT cache, so one will be created:\r\n\r\n> 2020-02-11 18:21:13.440748: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at trt_engine_resource_ops.cc:183 : Not found: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0)\r\n> INFO:tensorflow:Could not find TRTEngineOp_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\r\n> I0211 18:21:13.440817 140386763036416 trt_convert.py:1074] Could not find TRTEngineOp_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\r\n> INFO:tensorflow:Assets written to: /tmp/dynamic_input_shapes_test0vdtk5_4/tmpsu9l_mk2/tmpd2ckucmp/assets\r\n> I0211 18:21:13.468731 140386763036416 builder_impl.py:775] Assets written to: /tmp/dynamic_input_shapes_test0vdtk5_4/tmpsu9l_mk2/tmpd2ckucmp/assets\r\n> ...\r\n> ...\r\n> 2020-02-11 18:21:13.624985: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:257] Constructing PartitionedCall/TRTEngineOp_0\r\n> 2020-02-11 18:21:13.625010: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:237] Constructing function handle\r\n> 2020-02-11 18:21:13.625334: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for PartitionedCall/TRTEngineOp_0 with input shapes: [[1,5,5,1]]\r\n> 2020-02-11 18:21:13.625404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n> 2020-02-11 18:21:13.829528: I tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:5645] Starting engine conversion, precision mode: FP16\r\n\r\n\r\nHowever, in the FP32 test run later, we do not see the warning about an engine op not being found in the cache, and there is no new engine op created:\r\n\r\n> \r\n> WARNING:tensorflow:From /home/mconley/TF2.1_test_env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> If using Keras pass *_constraint arguments to layers.\r\n> W0211 18:21:16.109585 140386763036416 deprecation.py:506] From /home/mconley/TF2.1_test_env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> If using Keras pass *_constraint arguments to layers.\r\n> INFO:tensorflow:Assets written to: /tmp/dynamic_input_shapes_test0vdtk5_4/tmp45kosn5r/tmpou7_kbrm/assets\r\n> I0211 18:21:16.131422 140386763036416 builder_impl.py:775] Assets written to: /tmp/dynamic_input_shapes_test0vdtk5_4/tmp45kosn5r/tmpou7_kbrm/assets\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).trt_engine_resources.TRTEngineOp_0._serialized_trt_resource_filename\r\n> W0211 18:21:16.308784 140386763036416 util.py:144] Unresolved object in checkpoint: (root).trt_engine_resources.TRTEngineOp_0._serialized_trt_resource_filename\r\n> ...\r\n> ...\r\n> 2020-02-11 18:21:16.378245: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:257] Constructing PartitionedCall/TRTEngineOp_0\r\n> 2020-02-11 18:21:16.378281: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:237] Constructing function handle\r\n> 2020-02-11 18:21:16.378781: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:515] Executing TRT engine: PartitionedCall/TRTEngineOp_0\r\n> 2020-02-11 18:21:16.379594: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:515] Executing TRT engine: PartitionedCall/TRTEngineOp_0\r\n> [logged_output.log](https://github.com/tensorflow/tensorflow/files/4189640/logged_output.log)\r\n> ", "comments": ["> This forced garbage collection is likely an attempted workaround for the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py#L372\r\n\r\nThat looks super fishy to me as well -- we should never need to rely on GC for *correctness*.\r\n\r\nIs this happening because we are creating key-ing TRT engines on the tensor shapes of the inputs and ignoring the dtypes?  I.e. the key does not capture enough information?\r\n\r\nIf so I'd suggest the following fix:\r\n\r\n * Make sure each `TRTEngineOp` has a `segment_func` with a (process wide) different name.\r\n * Key the `EngineContext` on the tensor shapes, dtypes, ID and `segment_func` name.", "@bixia1 @DEKHTIARJonathan Do either of you have cycles to pick this up?", "I ran the test without the forced garbage collection in tf_trt_integration_test_base.py, and did see TRT engines created for both F16 and F32. So the problem is gone in the current tree.  My commands are similar to this:\r\n\r\n$ blaze run -c opt tensorflow/python/compiler/tensorrt:gpu_dynamic_input_shapes_test --define use_experimental_tensorrt=1 --config=cuda  --test_env=TF_CPP_VMODULE=convert_nodes=2,trt_engine_op=1  --test_arg=--logtostderr>& t.log\r\n$ grep -e \"trt_engine_op\" -e \"DynamicInputShapesTest\" t.log\r\n[ RUN      ] DynamicInputShapesTest.testTfTrt_OfflineConversion_DynamicEngine_FP16_NoCalibration\r\nI0219 15:36:59.883159  143904 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,5,5,1]]\r\nI0219 15:37:00.669233  143905 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[10,5,5,1]]\r\nI0219 15:37:01.309641  143903 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,3,1,1]]\r\nI0219 15:37:01.961832  143903 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[2,9,9,1]]\r\nI0219 15:37:02.754980  143905 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,224,224,1]]\r\nI0219 15:37:03.625179  143902 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,128,224,1]]\r\nI0219 15:37:04.565040  143903 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,5,5,1]]\r\nI0219 15:37:05.345535  143904 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[10,5,5,1]]\r\nI0219 15:37:06.050820  143903 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,3,1,1]]\r\nI0219 15:37:06.776347  143903 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[2,9,9,1]]\r\nI0219 15:37:07.644002  143902 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,224,224,1]]\r\nI0219 15:37:08.518280  143902 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,128,224,1]]\r\n[       OK ] DynamicInputShapesTest.testTfTrt_OfflineConversion_DynamicEngine_FP16_NoCalibration\r\n[ RUN      ] DynamicInputShapesTest.testTfTrt_OfflineConversion_DynamicEngine_FP32_NoCalibration\r\nI0219 15:37:09.430716  143905 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,5,5,1]]\r\nI0219 15:37:09.614939  143904 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[10,5,5,1]]\r\nI0219 15:37:09.722950  143904 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,3,1,1]]\r\nI0219 15:37:09.859450  143905 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[2,9,9,1]]\r\nI0219 15:37:10.052555  143903 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,224,224,1]]\r\nI0219 15:37:10.234415  143902 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,128,224,1]]\r\nI0219 15:37:10.486743  143905 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,5,5,1]]\r\nI0219 15:37:10.672606  143905 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[10,5,5,1]]\r\nI0219 15:37:10.777662  143902 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,3,1,1]]\r\nI0219 15:37:10.913178  143905 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[2,9,9,1]]\r\nI0219 15:37:11.098235  143902 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,224,224,1]]\r\nI0219 15:37:11.295327  143905 trt_engine_op.cc:984] Building a new TensorRT engine for TRTEngineOp_0 with input shapes: [[1,128,224,1]]\r\n[       OK ] DynamicInputShapesTest.testTfTrt_OfflineConversion_DynamicEngine_FP32_NoCalibration\r\n[ RUN      ] DynamicInputShapesTest.testTfTrt_OfflineConversion_DynamicEngine_INT8_NoCalibration\r\n[       OK ] DynamicInputShapesTest.testTfTrt_OfflineConversion_DynamicEngine_INT8_NoCalibration\r\n[ RUN      ] DynamicInputShapesTest.testTfTrt_OfflineConversion_DynamicEngine_INT8_UseCalibration\r\n[       OK ] DynamicInputShapesTest.testTfTrt_OfflineConversion_DynamicEngine_INT8_UseCalibration\r\n[ RUN      ] DynamicInputShapesTest.testTfTrt_OfflineConversion_StaticEngine_FP16_NoCalibration\r\n[       OK ] DynamicInputShapesTest.testTfTrt_OfflineConversion_StaticEngine_FP16_NoCalibration\r\n[ RUN      ] DynamicInputShapesTest.testTfTrt_OfflineConversion_StaticEngine_FP32_NoCalibration\r\n[       OK ] DynamicInputShapesTest.testTfTrt_OfflineConversion_StaticEngine_FP32_NoCalibration\r\n[ RUN      ] DynamicInputShapesTest.testTfTrt_OfflineConversion_StaticEngine_INT8_NoCalibration\r\n[       OK ] DynamicInputShapesTest.testTfTrt_OfflineConversion_StaticEngine_INT8_NoCalibration\r\n\r\n", "@bixia1 if this issue is fixed at head, can you do the following:\r\n\r\n- Delete the explicit call to the Python GC in the test.\r\n- Add a test case that would have caught the original issue if feasible. ", "I am not sure that I understand this. While I see the TRT engine being created I don't see the cache being accessed. That is I don't things related to  message \" trt_convert.py:1074] Could not find TRTEngineOp_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime\" in the initial bug description.", "Thanks for your help with this bug!\r\nUnfortunately, I was still able to reproduce the issue as of commit 5a037e9954a9e9c883acc504a20ecb4a10c131d9\r\n\r\nAlso, adding in additional `gc.collect()` calls around each test did not seem to solve this problem; maybe this issue is on a different level than the python garbage collection?", "Small note, you could need to call multiple time gc.collect() to be sure all is collected from Python. I normally call it 3 times and up to now it was always enough to recover all the object.", "tfeher@ is going to work on it.", "@MattConley  It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version of TF 2.5 and retest with TensorRT 7.x. .Please let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36675\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36675\">No</a>\n"]}, {"number": 36674, "title": "Tensorflow error ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. I'm trying to reproduce simple examples.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n\r\n- TensorFlow installed from (source or binary):\r\nI used the following commands in command promt window:\r\npip install --user pip\r\npip install --user tensorflow\r\n\r\n- TensorFlow version (use command below): 2.1\r\n\r\n- Python version: 3.7.6\r\n\r\n- CUDA/cuDNN version: I've installed cuda 10.1\r\n\r\n- GPU model and memory: nvidia geforce gtx 650\r\n\r\n\r\n**Describe the current behavior**\r\nI can import tensorflow with no error messages (`import tensorflow as tf`), but I a error message appear when trying to run a simple line (`tf.set_random_seed(42)`)\r\n\r\nErro message:\r\n\r\n> ERROR:root:Internal Python error in the inspect module.\r\n> Below is the traceback from this internal error.\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\r\n>     exec(code_obj, self.user_global_ns, self.user_ns)\r\n>   File \"<ipython-input-16-f8b0a2f8392f>\", line 1, in <module>\r\n>     tf.set_random_seed(42)\r\n> AttributeError: module 'tensorflow' has no attribute 'set_random_seed'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\r\n>     stb = value._render_traceback_()\r\n> AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\r\n>     return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n>     return f(*args, **kwargs)\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n>     records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\inspect.py\", line 1502, in getinnerframes\r\n>     frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\inspect.py\", line 1460, in getframeinfo\r\n>     filename = getsourcefile(frame) or getfile(frame)\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\inspect.py\", line 696, in getsourcefile\r\n>     if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\inspect.py\", line 733, in getmodule\r\n>     if ismodule(module) and hasattr(module, '__file__'):\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n>   File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n>   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n>     from . _api.v2 import audio\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n>     from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\r\n>     exec(code_obj, self.user_global_ns, self.user_ns)\r\n>   File \"<ipython-input-16-f8b0a2f8392f>\", line 1, in <module>\r\n>     tf.set_random_seed(42)\r\n> AttributeError: module 'tensorflow' has no attribute 'set_random_seed'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\r\n>     stb = value._render_traceback_()\r\n> AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> ---------------------------------------------------------------------------\r\n> \r\n\r\n**Describe the expected behavior**\r\nTrying to reproduce this example https://peterroelants.github.io/posts/gaussian-process-kernel-fitting/\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ntf.set_random_seed(42)\r\n```\r\n\r\n**Other info / logs**\r\nI'm also using Jupyter. I installed it with `pip install --upgrade --user pip` then `pip install --user notebook`.\r\nI installed python with default settings and checked \"add to path\" option.\r\nI installed \"visual studio community 2019 - desktop develoment with c++\". with lastest \"window sdk\", \"MSVC v140 - VS 2015 C++ buildtools\", \"MSVC v141 - VS 2017 C++ buildtools\", \"MSVC v142 - VS 2019 C++ buildtools\". I'm not sure whether this are the correct so called \"redistributible microsoft visual c++ \" files.\r\n", "comments": ["tf.set_random_seed was removed in TensorFlow 2.X\r\n\r\nYou can use: `tf.random.set_seed(seed)`\r\nsee: https://www.tensorflow.org/api_docs/python/tf/random/set_seed\r\n\r\n", "After using `tf.random.set_seed(seed)`, the almost the sam error message appears,\r\n\r\n```\r\n File \"<ipython-input-27-f8b0a2f8392f>\", line 1, in <module>\r\n    tf.set_random_seed(42)\r\nAttributeError: module 'tensorflow' has no attribute 'set_random_seed'\r\n```\r\nThe code above is just the first part. Please, note how It changed a bit in the line that say \"File...\" to:\r\n\r\n```\r\nFile \"<ipython-input-28-3ec692838874>\", line 1, in <module>\r\n      tf.random.set_seed(seed)\r\nAttributeError: module 'tensorflow' has no attribute 'random'\r\n```\r\n\r\nI think is the same error. That line changes every time a run the code line.", "What is the output of `pip list`?", "I reinstalled everything from zero, but this time with python 3.7.0. Same problem I can't even see the version of tensorflow (`tf.__version__`)\r\npip list:\r\n\r\n```\r\n> C:\\Windows\\system32>pip list\r\n> Package                  Version\r\n> ------------------------ ----------\r\n> absl-py                  0.9.0\r\n> astor                    0.8.1\r\n> attrs                    19.3.0\r\n> backcall                 0.1.0\r\n> bleach                   3.1.0\r\n> cachetools               4.0.0\r\n> certifi                  2019.11.28\r\n> chardet                  3.0.4\r\n> colorama                 0.4.3\r\n> decorator                4.4.1\r\n> defusedxml               0.6.0\r\n> entrypoints              0.3\r\n> gast                     0.2.2\r\n> google-auth              1.11.0\r\n> google-auth-oauthlib     0.4.1\r\n> google-pasta             0.1.8\r\n> grpcio                   1.27.1\r\n> h5py                     2.10.0\r\n> idna                     2.8\r\n> importlib-metadata       1.5.0\r\n> ipykernel                5.1.4\r\n> ipython                  7.12.0\r\n> ipython-genutils         0.2.0\r\n> jedi                     0.16.0\r\n> Jinja2                   2.11.1\r\n> jsonschema               3.2.0\r\n> jupyter-client           5.3.4\r\n> jupyter-core             4.6.2\r\n> Keras-Applications       1.0.8\r\n> Keras-Preprocessing      1.1.0\r\n> Markdown                 3.2\r\n> MarkupSafe               1.1.1\r\n> mistune                  0.8.4\r\n> nbconvert                5.6.1\r\n> nbformat                 5.0.4\r\n> notebook                 6.0.3\r\n> numpy                    1.18.1\r\n> oauthlib                 3.1.0\r\n> opencv-python            4.2.0.32\r\n> opt-einsum               3.1.0\r\n> pandocfilters            1.4.2\r\n> parso                    0.6.1\r\n> pickleshare              0.7.5\r\n> pip                      10.0.1\r\n> prometheus-client        0.7.1\r\n> prompt-toolkit           3.0.3\r\n> protobuf                 3.11.3\r\n> pyasn1                   0.4.8\r\n> pyasn1-modules           0.2.8\r\n> Pygments                 2.5.2\r\n> pyrsistent               0.15.7\r\n> python-dateutil          2.8.1\r\n> pywin32                  227\r\n> pywinpty                 0.5.7\r\n> pyzmq                    18.1.1\r\n> requests                 2.22.0\r\n> requests-oauthlib        1.3.0\r\n> rsa                      4.0\r\n> scipy                    1.4.1\r\n> Send2Trash               1.5.0\r\n> setuptools               45.2.0\r\n> six                      1.14.0\r\n> tensorboard              2.1.0\r\n> tensorflow-gpu           2.1.0\r\n> tensorflow-gpu-estimator 2.1.0\r\n> termcolor                1.1.0\r\n> terminado                0.8.3\r\n> testpath                 0.4.4\r\n> tornado                  6.0.3\r\n> traitlets                4.3.3\r\n> urllib3                  1.25.8\r\n> wcwidth                  0.1.8\r\n> webencodings             0.5.1\r\n> Werkzeug                 1.0.0\r\n> wheel                    0.34.2\r\n> wrapt                    1.11.2\r\n> zipp                     2.2.0\r\n> You are using pip version 10.0.1, however version 20.0.2 is available.\r\n> You should consider upgrading via the 'python -m pip install --upgrade pip' command.\r\n```", "`ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.` is the main error.\r\n\r\nIn this case, please see https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156 and all the duplicated bugs", "Does intel i3 540 support AVX or AVX2?\r\nIf not i think that's the problem and I'll use another PC.\r\n", "I tried with anaconda 3.7. I guess the hardware isn't compatible.\r\n\r\nAnaconda Error message (same as Python):\r\n\r\n> ERROR:root:Internal Python error in the inspect module.\r\n> Below is the traceback from this internal error.\r\n> \r\n> ERROR:root:Internal Python error in the inspect module.\r\n> Below is the traceback from this internal error.\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n>     exec(code_obj, self.user_global_ns, self.user_ns)\r\n>   File \"<ipython-input-1-64156d691fe5>\", line 1, in <module>\r\n>     import tensorflow as tf\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n>     from tensorflow_core import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n>     from tensorflow.python.tools import module_util as _module_util\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n>     stb = value._render_traceback_()\r\n> AttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\r\n>     return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n>     return f(*args, **kwargs)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n>     records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\r\n>     frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\r\n>     filename = getsourcefile(frame) or getfile(frame)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\r\n>     if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\r\n>     if ismodule(module) and hasattr(module, '__file__'):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n>   File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n>   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n>     from . _api.v2 import audio\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n>     from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n>     exec(code_obj, self.user_global_ns, self.user_ns)\r\n>   File \"<ipython-input-1-64156d691fe5>\", line 1, in <module>\r\n>     import tensorflow as tf\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n>     from tensorflow_core import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n>     from tensorflow.python.tools import module_util as _module_util\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n>     stb = value._render_traceback_()\r\n> AttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n>     exec(code_obj, self.user_global_ns, self.user_ns)\r\n>   File \"<ipython-input-1-64156d691fe5>\", line 1, in <module>\r\n>     import tensorflow as tf\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n>     from tensorflow_core import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n>     from tensorflow.python.tools import module_util as _module_util\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n>     stb = value._render_traceback_()\r\n> AttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\r\n>     if (await self.run_code(code, result,  async_=asy)):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\r\n>     self.showtraceback(running_compiled_code=True)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2043, in showtraceback\r\n>     value, tb, tb_offset=tb_offset)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1385, in structured_traceback\r\n>     self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\r\n>     self, etype, value, tb, tb_offset, number_of_lines_of_context\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in structured_traceback\r\n>     formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\r\n> TypeError: can only concatenate str (not \"list\") to str\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n>     stb = value._render_traceback_()\r\n> AttributeError: 'TypeError' object has no attribute '_render_traceback_'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\r\n>     return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n>     return f(*args, **kwargs)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n>     records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\r\n>     frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\r\n>     filename = getsourcefile(frame) or getfile(frame)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\r\n>     if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\r\n>     if ismodule(module) and hasattr(module, '__file__'):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n>   File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n>   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n>     from . _api.v2 import audio\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n>     from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n>     exec(code_obj, self.user_global_ns, self.user_ns)\r\n>   File \"<ipython-input-1-64156d691fe5>\", line 1, in <module>\r\n>     import tensorflow as tf\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n>     from tensorflow_core import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n>     from tensorflow.python.tools import module_util as _module_util\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n>     module = self._load()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n>     module = _importlib.import_module(self.__name__)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n>     stb = value._render_traceback_()\r\n> AttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\r\n>     if (await self.run_code(code, result,  async_=asy)):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\r\n>     self.showtraceback(running_compiled_code=True)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2043, in showtraceback\r\n>     value, tb, tb_offset=tb_offset)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1385, in structured_traceback\r\n>     self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\r\n>     self, etype, value, tb, tb_offset, number_of_lines_of_context\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in structured_traceback\r\n>     formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\r\n> TypeError: can only concatenate str (not \"list\") to str\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n>     stb = value._render_traceback_()\r\n> AttributeError: 'TypeError' object has no attribute '_render_traceback_'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\NIP\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\NIP\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> ---------------------------------------------------------------------------\r\n> ImportError                               Traceback (most recent call last)\r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py in <module>\r\n>      57 \r\n> ---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n>      59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py in <module>\r\n>      27             return _mod\r\n> ---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n>      29     del swig_import_helper\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n>      23             try:\r\n> ---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>      25             finally:\r\n> \r\n> ~\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n>     241         else:\r\n> --> 242             return load_dynamic(name, filename, file)\r\n>     243     elif type_ == PKG_DIRECTORY:\r\n> \r\n> ~\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n>     341             name=name, loader=loader, origin=path)\r\n> --> 342         return _load(spec)\r\n>     343 \r\n> \r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> AttributeError                            Traceback (most recent call last)\r\n> ~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\r\n>    2039                         # in the engines. This should return a list of strings.\r\n> -> 2040                         stb = value._render_traceback_()\r\n>    2041                     except Exception:\r\n> \r\n> AttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> TypeError                                 Traceback (most recent call last)\r\n> ~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self, code_obj, result, async_)\r\n>    3341             if result is not None:\r\n>    3342                 result.error_in_exec = sys.exc_info()[1]\r\n> -> 3343             self.showtraceback(running_compiled_code=True)\r\n>    3344         else:\r\n>    3345             outflag = False\r\n> \r\n> ~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\r\n>    2041                     except Exception:\r\n>    2042                         stb = self.InteractiveTB.structured_traceback(etype,\r\n> -> 2043                                             value, tb, tb_offset=tb_offset)\r\n>    2044 \r\n>    2045                     self._showtraceback(etype, value, stb)\r\n> \r\n> ~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n>    1383         self.tb = tb\r\n>    1384         return FormattedTB.structured_traceback(\r\n> -> 1385             self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n>    1386 \r\n>    1387 \r\n> \r\n> ~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n>    1286             # Verbose modes need a full traceback\r\n>    1287             return VerboseTB.structured_traceback(\r\n> -> 1288                 self, etype, value, tb, tb_offset, number_of_lines_of_context\r\n>    1289             )\r\n>    1290         elif mode == 'Minimal':\r\n> \r\n> ~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py in structured_traceback(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\r\n>    1148         exception = self.get_parts_of_chained_exception(evalue)\r\n>    1149         if exception:\r\n> -> 1150             formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\r\n>    1151             etype, evalue, etb = exception\r\n>    1152         else:\r\n> \r\n> TypeError: can only concatenate str (not \"list\") to str", "I checked on another PC. It has an intel i5 9400f. I could run commands `import tensorflow` and `tensorflow.__version__` without problems. I think intel i3 540 doesn't support avx, but i'm not 100% sure if it is about the CPU.", "It is the CPU lacking the AVX support.\r\n\r\nClosing the issue as we determined what is the root cause.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36674\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36674\">No</a>\n"]}, {"number": 36673, "title": "Fix gpu test filters when running the ./configure script", "body": "Today if you run the ./configure script without setting the\r\nvariable TF_NEED_CUDA, in the resulting .tf_configure.bazelrc\r\nyou get:\r\n\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\n\r\nThis is incorrect because -gpu means exclude the gpu test. It should be\r\n-no_gpu.\r\n\r\nDebugging the problem I found when the code was switched from using\r\nos.env to using environ_cp, that the method system_specific_test_config\r\nwas never updated. So unless the environment variable TF_NEED_CUDA was\r\nset before running ./configure then answering yes for CUDA would not\r\nselect the correct test filter for gpus.\r\n\r\nWith this change the test filters are correct:\r\n\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only", "comments": []}, {"number": 36672, "title": "Add a raspberry pi docker image", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThere is already a tensoflow docker image for amd64. I'd like to see if there is interest other than me to have the same for armv7.\r\n\r\nI recently made one for tensorflow 2.1.0 here:\r\n\r\nhttps://github.com/fgervais/docker-tensorflow/blob/master/Dockerfile\r\n\r\nI think it would be nice to have an image like this published on the official docker hub repository.\r\n\r\nNote that this image does not need to be built on an actual raspberry pi. I build mine on my x86_64 system using:\r\n\r\n```\r\ndocker buildx build --platform linux/arm/v7 -t tensorflow:2.1.0-cp35 --load .\r\n```\r\n\r\nI'm guessing whatever CI system that builds the amd64 image could also build the armv7 image.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nOn the raspberry pi, the docker image is quite useful since a lot of pip wheels do not come in binary form and the cost of building, let say scipy, is quite high. Having the docker image ensures a plug-and-play experience.\r\n\r\nWhat do you think? Would this integrate well in tensorflow?", "comments": ["Just in case someone would want to use my image right now, run the following on the raspberry pi (tested only on the raspberry pi 4):\r\n\r\n```\r\ndocker run --rm francoisgervais/tensorflow:2.1.0-cp35 bash\r\n\r\npython3\r\nimport tensorflow as tf\r\nprint(tf.reduce_sum(tf.random.normal([1000, 1000])))\r\n```", "Inside the dockerfile you use this in the first build stage.\r\n`FROM debian@sha256:872b72a2b8487e4b91ae27855c7de1671635d3dc2cc0b89651103e55c74ed34a AS base`\r\nAfter finding said image on [dockerhub](https://hub.docker.com/layers/arm32v7/debian/stretch/images/sha256-872b72a2b8487e4b91ae27855c7de1671635d3dc2cc0b89651103e55c74ed34a), only two lines are executed.\r\nThe first involved adding this file. What is this?\r\n`ADD file:b7fc54d004d962f2cfb469a1aaa9e689e46dfa2554e0cf44c33981d688adc31b in / `\r\n", "@HamzaRahmani I base on the official debian:stretch docker image for arm32. The file add you refer to is added inside the official debian image. I don't have any info on how they generate their images.", "In case anyone is interested, I managed to build a devel-cpu docker images on my raspberry pi 4, see PR #39695 .  It's a good start for trying to natively build tensorflow on device.", "@fgervais,\r\nSorry for the delayed response. Can you please let us know if [this PR](https://github.com/tensorflow/tensorflow/pull/39695) has resolved your issue? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36672\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36672\">No</a>\n"]}, {"number": 36671, "title": "[ROCm] Fix for a test failure on the ROCm platform - 200211 - 1", "body": "The IR emitted by the AMDGPU backend, for the atomic-compare-and-swap operation, seems to have changed recently\r\n(perhaps by the new LLVM version being picked up as a recent frequent updates to the LLVM repo commit pointer)\r\n\r\nThe new IR results in a failure of a couple of subtests with the test `//tensorflow/compiler/xla/service/gpu/tests:gpu_kernel_tiling_test`.\r\n\r\nUpdating the regexp within the file-check pattern to account for the updated IR generation\r\n\r\n----------------\r\n\r\n/cc @whchung @jerryyin @cheshire \r\n", "comments": []}, {"number": 36670, "title": "Updated an example for reduce_euclidean_norm", "body": "Updated an incorrect example as reported in #36554", "comments": []}, {"number": 36669, "title": "Error while reading resource variable _AnonymousVar12 from Container: localhost", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7.7\r\n- TensorFlow installed from (source or binary): yes\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.6\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nError when trying to update tf.Variable in data.Dataset pipeline\r\n\r\n**Describe the expected behavior**\r\nNo error\r\n\r\n**Code to reproduce the issue**\r\n```\r\nds=tf.data.Dataset.from_tensor_slices(tf.constant([[9, 10, 11, 12]]))\r\ndef update(update):\r\n    ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\r\n    indices = tf.constant([[4], [3], [1] ,[7]])\r\n    updated = tf.tensor_scatter_nd_update(ref, indices, update)\r\n    return updated\r\nds=ds.map(update)\r\nfor i in ds:\r\n    print(i)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nFailedPreconditionError: {{function_node __inference_Dataset_map_update_674}} Error while reading resource variable _AnonymousVar12 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar12/N10tensorflow3VarE does not exist.\r\n\t [[{{node TensorScatterUpdate/ReadVariableOp}}]] [Op:IteratorGetNextSync]\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.1.0, 2.2.0-dev20200211 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/1a57753e62d5a898c3f565a6e53d0434/untitled641.ipynb). Thanks!", "Hi I am having  a similar issue any news?\r\n", "You need to put the variable outside of the user-defined function:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nds=tf.data.Dataset.from_tensor_slices(tf.constant([[9, 10, 11, 12]]))\r\nref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\r\ndef update(update):\r\n    indices = tf.constant([[4], [3], [1] ,[7]])\r\n    updated = tf.tensor_scatter_nd_update(ref, indices, update)\r\n    return updated\r\nds=ds.map(update)\r\nfor i in ds:\r\n    print(i)\r\n```\r\n\r\nIn your example, the Python object for the variable is destroyed when the tracing of the function completes, which results in the use-after-free error in your program.", "Closing - the comment above provides a solution.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36669\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36669\">No</a>\n"]}, {"number": 36668, "title": "TF 1.15.2: Metrics created in custom layer (self.add_metric) producing wrong output", "body": "(this seems to be the same issue as #32144, but for tf 1.15)\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15.2\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce GTX 1080Ti\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen creating a custom model class (inheriting from `tf.keras.Model`), with a custom layer class (tf.keras.layers.Layer) as attribute, and trying to define metrics (using `self.add_metric()`) in both the outer class and the inner layer, the metrics outputs seem to be swapped (keys and values do not match) when training with `model.fit`.\r\n\r\n**Describe the expected behavior**\r\n\r\nMetrics names and values should match.\r\nThis seems to be an issue only of tf 1. Did not observe this in tf 2.1.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nclass InnerLayer(layers.Layer):\r\n\r\n    def __init__(self):\r\n        super(InnerLayer, self).__init__()\r\n\r\n    def call(self, inputs):\r\n        # Should always print 1.0\r\n        self.add_metric(tf.constant(1.0), aggregation='mean',\r\n                        name='should_be_1')\r\n        return inputs\r\n\r\n\r\nclass OuterModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(OuterModel, self).__init__()\r\n        self.inner = InnerLayer()\r\n\r\n    def call(self, inputs):\r\n        # Should always print 2.0\r\n        self.add_metric(tf.constant(2.0), aggregation='mean',\r\n                        name='should_be_2')\r\n        return self.inner(inputs)\r\n\r\n\r\nmodel = OuterModel()\r\nmodel.compile('adam', loss='mse')\r\ndataset = tf.random.normal((100, 5))\r\nmodel.fit(dataset, dataset, epochs=10, steps_per_epoch=100, verbose=2)\r\n```\r\nOutput (with bug):\r\n\r\n```\r\nEpoch 1/10\r\n100/100 - 0s - loss: 0.0000e+00 - should_be_1: 2.0000 - should_be_2: 1.0000\r\nEpoch 2/10\r\n100/100 - 0s - loss: 0.0000e+00 - should_be_1: 2.0000 - should_be_2: 1.0000\r\nEpoch 3/10\r\n100/100 - 0s - loss: 0.0000e+00 - should_be_1: 2.0000 - should_be_2: 1.0000\r\nEpoch 4/10\r\n100/100 - 0s - loss: 0.0000e+00 - should_be_1: 2.0000 - should_be_2: 1.0000\r\nEpoch 5/10\r\n100/100 - 0s - loss: 0.0000e+00 - should_be_1: 2.0000 - should_be_2: 1.0000\r\nEpoch 6/10\r\n100/100 - 0s - loss: 0.0000e+00 - should_be_1: 2.0000 - should_be_2: 1.0000\r\nEpoch 7/10\r\n100/100 - 0s - loss: 0.0000e+00 - should_be_1: 2.0000 - should_be_2: 1.0000\r\nEpoch 8/10\r\n100/100 - 0s - loss: 0.0000e+00 - should_be_1: 2.0000 - should_be_2: 1.0000\r\nEpoch 9/10\r\n100/100 - 0s - loss: 0.0000e+00 - should_be_1: 2.0000 - should_be_2: 1.0000\r\nEpoch 10/10\r\n100/100 - 0s - loss: 0.0000e+00 - should_be_1: 2.0000 - should_be_2: 1.0000\r\n```", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/970adef6e8e9d6d31d69a647ec342023/untitled1.ipynb). Thanks!", "@kurka Thanks for the issue!\r\n\r\nThis is fixed in the latest tf-nightly: `pip install -U tf-nightly`\r\n\r\nUnfortunately we are not able to backport fixes like this to earlier versions", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36668\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36668\">No</a>\n"]}, {"number": 36667, "title": "Pip package for tensorflow 2.1 for Raspberry Pi (python 3.7)", "body": "Current versions of Raspbian use Python 3.7. Yet, the most updated pip packages (TF 2.1) provided by Google for the Raspberry pi are limited to python 3.5:\r\n\r\nhttps://www.tensorflow.org/install/pip\r\n\r\nPypi provides much older versions of TF (1.14) for python 3.7.  \r\n\r\nPlease update the version provided by either google or Pypi", "comments": ["As 2.2.0 is available, could you please also provide that for python 3.7? That would be great! Python 3.5 seems a bit outdated now. ", "@feranick \r\nAs we have 3.8/3.9 latest version, could you please verify if this is still an issue.Thanks!", "Still an issue here. Stuck with Python 3.7 (Raspbian 10), need TF 2.x. Python 3.8/3.9 is unfortunately not an option.", "You can build with Python 3.7 https://github.com/tensorflow/build/tree/master/raspberry_pi_builds#python-37\r\n\r\nAbout pre-built official wheels is a sort of dup https://github.com/tensorflow/tensorflow/issues/29704#issuecomment-780021283 /cc @mihaimaruseac ", "We can certainly build it, but not every hobbyist has the machine power to do quick cross compilation. Besides, as most RPi users may actually be using the official Raspberry PI OS, seeing that not being supported may create only confusion. ", "> You can build with Python 3.7 https://github.com/tensorflow/build/tree/master/raspberry_pi_builds#python-37\r\n\r\nRecently tried that, unfortunately the build-script uses the wrong NumPy version (NumPy 20.x for building, 19.x for running, which doesn't really work since it's considered as incompatible). Having official Tensorflow packages available for the RPi would entirely solve that problem for users.", "If want to edit the title and use this ticket to track @KopfKrieg issues building with python 2.7 ok.\n\nIf you just want updated pre-packaged wheels palese upvote and comment https://github.com/tensorflow/tensorflow/issues/29704\n\n\n", "P.s. as officially now is only support TF lite on Raspberry the official build guide is at https://www.tensorflow.org/lite/guide/build_arm", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36667\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36667\">No</a>\n", "For clarity: this is  a WON'T FIX, not a RESOLVED issue. Raspberry PI users on Raspberry PI OS will have to resort to compiling binaries themselves. "]}, {"number": 36666, "title": "Saving model with tf.keras.layers.RNN with unroll=True fails for save_format=tf", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- Mobile device if the issue happens on mobile device: -\r\n- TensorFlow installed from: binary (tf-nightly via docker)\r\n- TensorFlow version: GIT_VERSION = v1.12.1-23779-g96c5c8a 2.2.0-dev20200202\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: CPU only\r\n- GPU model and memory: CPU only\r\n\r\n**Describe the current behavior**\r\nSaving a `tf.keras.Sequential` model with `tf.keras.layers.RNN` with `unroll=True` fails for `save_format=tf`(but succeeds for `save_format=ht`).\r\n\r\n**Describe the expected behavior**\r\nSaving should succeed for `save_format=tf` as well.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential()\r\n\r\nmodel.add(tf.keras.layers.Input(shape=(1, 1,)))\r\n\r\ncell = tf.keras.layers.GRUCell(10)\r\n\r\nmodel.add(tf.keras.layers.RNN(cell, unroll=True))\r\n    \r\nmodel.save(\"test.tf\", save_format='tf') # fails\r\n#model.save(\"test.h5\", save_format='h5') # works\r\n```\r\n\r\n**Other info / logs**\r\nUnfortunately, saving as h5 is not an option (which would actually be my favorite), since it fails when having more than one cell (see #36093).\r\n\r\nTraceback in case of failure:\r\n```bash\r\n  File \"test.py\", line 11, in <module>\r\n    model.save(\"test.tf\", save_format='tf') # fails\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\", line 999, in save\r\n    signatures, options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\", line 138, in save_model\r\n    signatures, options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save.py\", line 78, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/save.py\", line 955, in save\r\n    checkpoint_graph_view)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/signature_serialization.py\", line 75, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/save.py\", line 142, in list_functions\r\n    self._serialization_cache)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2535, in _list_functions_for_serialization\r\n    .list_functions_for_serialization(serialization_cache))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/base_serialization.py\", line 91, in list_functions_for_serialization\r\n    fns = self.functions_to_serialize(serialization_cache)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\", line 79, in functions_to_serialize\r\n    serialization_cache).functions_to_serialize)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\", line 94, in _get_serialized_attributes\r\n    serialization_cache)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/model_serialization.py\", line 53, in _get_serialized_attributes_internal\r\n    serialization_cache))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\", line 103, in _get_serialized_attributes_internal\r\n    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 161, in wrap_layer_functions\r\n    original_fns = _replace_child_layer_functions(layer, serialization_cache)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 249, in _replace_child_layer_functions\r\n    serialization_cache).functions)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\", line 94, in _get_serialized_attributes\r\n    serialization_cache)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\", line 103, in _get_serialized_attributes_internal\r\n    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 171, in wrap_layer_functions\r\n    '{}_layer_call_and_return_conditional_losses'.format(layer.name))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 487, in add_function\r\n    self.add_trace(*self._input_signature)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 402, in add_trace\r\n    trace_with_training(True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 400, in trace_with_training\r\n    fn.get_concrete_function(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 531, in get_concrete_function\r\n    return super(LayerCall, self).get_concrete_function(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 953, in get_concrete_function\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 859, in _get_concrete_function_garbage_collected\r\n    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 505, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 2440, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 2771, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 2661, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 440, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 508, in wrapper\r\n    ret = method(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 170, in wrap_with_training_arg\r\n    lambda: replace_training_and_call(False))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/tf_utils.py\", line 59, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/smart_cond.py\", line 54, in smart_cond\r\n    return true_fn()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 169, in <lambda>\r\n    lambda: replace_training_and_call(True),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 165, in replace_training_and_call\r\n    return wrapped_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 550, in call_and_return_conditional_losses\r\n    return layer_call(inputs, *args, **kwargs), layer.get_losses_for(inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent.py\", line 734, in call\r\n    raise ValueError('Cannot unroll a RNN if the '\r\nValueError: Cannot unroll a RNN if the time dimension is undefined. \r\n- If using a Sequential model, specify the time dimension by passing an `input_shape` or `batch_input_shape` argument to your first layer. If your first layer is an Embedding, you can also use the `input_length` argument.\r\n- If using the functional API, specify the time dimension by passing a `shape` or `batch_shape` argument to your Input layer.\r\n\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.2.0-dev20200211 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2d485b1a2c3a6140d636d845ec760479/untitled640.ipynb). Thanks!", "I have tried in colab with TF nightly version(`2.4.0-dev20200712`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/3a016cc7fedf8af9ad0e27e536baa42b/untitled113.ipynb).Thanks!", "Same here (tensorflow==2.3.0):\r\n\r\n```python\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n\r\ndef get_model():\r\n    inputs = layers.Input(shape=(20, 80))\r\n    outputs = layers.LSTM(32, return_sequences=True, unroll=True)(inputs)\r\n    return keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n\r\ndef main():\r\n    model_dir = '/tmp/rnnt_toy/model'\r\n    model = get_model()\r\n    model.save(model_dir)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nOutput:\r\n\r\n```none\r\nValueError: Unrolling requires a fixed number of timesteps.\r\n```", "Same problem with me as well (as described by  @stefan-falk )\r\n\r\n\r\nValueError: Unrolling requires a fixed number of timesteps.", "This should be fixed at head!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36666\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36666\">No</a>\n", "I don't understand why this ticket is closed.", "@padoremu This was resolved in `tf-nightly`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/ae8b6ce7de7508c4ab601fd24c0be6d6/untitled.ipynb). \r\n\r\n@stefan-falk As mentioned by @k-w-w, this was also resolved in `tf-nightly`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/045f340fca21137c4199357e0b7add81/untitled64.ipynb). Thanks!"]}, {"number": 36665, "title": "TensorFlow 2.1 import 'DLL load failed error'", "body": "When I tried to import TensorFlow 2,1 in Anaconda environment after seeming successful installation using Aanacond Prompt, I got a 'DLL load failed error'. I tried several times from creating the virtual environment to installation tensorflow 2.1 again without success. The installation process went through fine without any error message. \r\n\r\nStrangely, the tensorflow 2.0 works fine but not tensorflow 2.1.  I am currently can run tensorflow 2.0 but would love to have 2.1 if I can find a fix for the 2.1 error.\r\n\r\nThanks for any help!\r\n\r\nBin\r\n--------------------------------------------------------------------------------\r\n\r\nname: Build/Installation Issue about: Use this template for build/installation\r\nissues labels: 'type:build/install'\r\n\r\n---\r\n\r\n<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["You didn't fill the issue template at all.\r\n\r\nMost likely you are on windows, in which case just searching for similar issues would have pointed you to other duplicated issues.\r\n\r\nClosing as duplicate.\r\n\r\n#36167 (comment)\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204"]}, {"number": 36664, "title": "Execution context management for TensorRT profiles", "body": "This PR adds a mechanism to handle multiple execution context for a TensorRT engine, which is a step towards enabling dynamic shape mode with multiple profiles.\r\n\r\nThis PR is not expected to change any existing behavior: inference in implicit batch mode, in explicit batch and in dynamic shapes mode remains the same.\r\n\r\nWhat changes is the engine lookup. Until now the [EngineContext](https://github.com/tfeher/tensorflow/blob/d8d6ddd802bdf20d6f38589147b265b7d08257cb/tensorflow/compiler/tf2tensorrt/utils/trt_lru_cache.h#L121-L142) object contained a reference to a singe [ICudaEngine](https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_cuda_engine.html) and a corresponding [IExecutionContext](https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_execution_context.html). We plan to have one execution context for each profile, therefore we introduce a vector of IExecutionContexts in EngineContext.\r\n\r\nCorrespondingly the [GetEngine](https://github.com/tfeher/tensorflow/blob/d8d6ddd802bdf20d6f38589147b265b7d08257cb/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc#L122-L126) function changes: it does not only return the pointer to the EngineContext but also gives the indexd of the IExecutionContext that can be used for inference. \r\n\r\nAn alternative method for handling multiple profiles would be to keep a single IExecutionContext, and [switch profile](https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_execution_context.html#aba0731b9fbc926c477010df818650b0a) before we enqueue the new inference task. This could lead to problems with asynchronous inference, because we would need to ensure there are \"no enqueued operations pending in the context prior to switching profiles\".\r\n\r\nDynamic shape mode is an experimental feature. The advantage of dynamic shapes is that we can use a single engine with multiple profiles to handle input data with different shapes. This saves memory: all input profiles for an engine can share the same weights. Input dimensions that do not influence the number of model parameters can be dynamic (batch size, sequence length, even image dimensions for fully convolutional networks). A single profile can handle a range of input parameters, or we could have multiple profiles each optimized for a certain input shape (e.g. one profile for each expected batch size). \r\n\r\nAlthough this PR implements the mechanism to handle multiple profiles and the corresponding execution contexts, one still needs a method to define these profiles. Such profile generation feature is implemented in PR #36729.\r\n ", "comments": ["@tfeher Can you please resolve conflicts? Thanks!", "@tfeher Still, conflicts are appearing. Can you please resolve? Thanks!"]}, {"number": 36663, "title": "Flatten layer behaves differently on different machine architectures for a 4 D tensor.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please run the flatten layer for a 4D tensor on different versions of CUDA. It will produce different vectors with a different order of terms.", "@shongi-yd, Could you please fill the template to understand the issue better. Thanks!", "@shongi-yd, Can you elaborate your issue with code snippet or any use case. Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36662, "title": "Issue with tf.estimator.export.ServingInputReceiver()", "body": "**System information**\r\n- I run my project on a Jupyter Notebook on GCP\r\n- TensorFlow version: 1.15\r\n- Python version: 3.5.3\r\n\r\n**Describe the current behavior**\r\nI followed every instruction to build a serving_input_receiver_fn(). Valliappa Lakshmanan provides his solution to me but it still does not work. I believe it is not the problem of the codes but some unknown issue.\r\n\r\n**Describe the expected behavior**\r\nI do not expect any error in creating and exporting this simple serving function. But when I run it, there are error messages. Despite the errors, I can still use the exported model for prediction.\r\n\r\n**Code to reproduce the issue**\r\ndef serving_input_receiver_fn():\r\n    inputs = {\r\n        'job' : tf.compat.v1.placeholder(dtype = tf.string, shape = [None]),\r\n        'marital' : tf.compat.v1.placeholder(dtype = tf.string, shape = [None]),\r\n        ......\r\n    }\r\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\r\n\r\n**Other info / logs**\r\nINFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'job': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'marital': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=string>, ...\r\nWARNING:tensorflow:Export includes no default signature!\r\n", "comments": ["@psyyip \r\n\r\nThis issue is more suitable for TensorFlow Serving repo. Please post it on Serving repo from [here.](https://github.com/tensorflow/serving/issues) Thanks!", "moved to TensorFlow Serving repo"]}, {"number": 36661, "title": "undefined reference to `tflite::DefaultErrorReporter()'", "body": "System information:  \r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): v1.4.0-19-ga52c8d9 1.4.1\r\nPython version: 2.7.12\r\nBazel version (if compiling from source): 0.8.1\r\nGCC/Compiler version (if compiling from source): g++ 5.4.0\r\nCUDA/cuDNN version: none\r\nGPU model and memory: none\r\nExact command to reproduce:\r\n\r\ng++ -I../tensorflow  -Llib -ltensorflow_cc -Llib -ltensorflowlite  test.cpp -o exec\r\n\r\n\r\nDescribe the problem\r\nI used this tutorial to build tensorflow [link](https://tuatini.me/building-tensorflow-as-a-standalone-project/).  Also this [link](https://github.com/FloopCZ/tensorflow_cc)\r\n\r\n\r\nAdditionally, I built tensorflow-lite with:\r\n\r\nbazel build //tensorflow/lite:libtensorflowlite.so\r\n\r\n\r\nI am using this code for testing tflite:\r\n\r\n  #include < cstdio >\r\n  #include \"tensorflow/lite/interpreter.h\"\r\n  #include \"tensorflow/lite/kernels/register.h\"\r\n  #include \"tensorflow/lite/model.h\"\r\n  #include \"tensorflow/lite/tools/gen_op_registration.h\"\r\n\r\n\r\n  using namespace tflite;\r\n\r\n  int main(int argc, char* argv[]) {\r\n  if (argc != 2) {\r\n  fprintf(stderr, \"minimal \\n\");\r\n  return 1;\r\n  }\r\n  const char* filename = argv[1];\r\n\r\n  std::unique_ptr<tflite::FlatBufferModel> model =\r\n  tflite::FlatBufferModel::BuildFromFile(filename);\r\n  return 0;\r\n  }\r\n\r\n\r\n\r\nHere is the error:\r\n\r\n/tmp/ccPKK7am.o: In function `main':\r\ntest.cpp:(.text+0x57): undefined reference to `tflite::DefaultErrorReporter()'\r\ntest.cpp:(.text+0x6d): undefined reference to `tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'\r\n/tmp/ccPKK7am.o: In function `std::default_delete<tflite::FlatBufferModel>::operator()(tflite::FlatBufferModel*) const':\r\ntest.cpp:(.text._ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_[_ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_]+0x1e): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'\r\ncollect2: error: ld returned 1 exit status\r\n", "comments": ["Could you try to separate build and link stage?\r\nFor example)\r\n$ g++ -I../tensorflow -ltensorflow_cc -c test.cpp\r\n$ g++ -Llib -ltensorflow_cc -Llib -o exec test.o -ltensorflowlite \r\nPlease make sure to provide your object file first than \"-ltensorflowlite \"", "I am getting this :\r\n./exec: error while loading shared libraries: libtensorflowlite.so: cannot open shared object file: No such file or directory\r\n\r\nMy lib/ \r\nlibtensorflow_cc.so                 libtensorflow_framework.so.2               libtensorflowlite.so-2.params           lite\r\nlibtensorflow_cc.so.2               libtensorflow_framework.so.2.1.0           libtensorflowlite.so.runfiles\r\nlibtensorflow_cc.so.2.1.0           libtensorflow_framework.so.2.1.0-2.params  libtensorflowlite.so.runfiles_manifest\r\nlibtensorflow_cc.so.2.1.0-2.params  libtensorflowlite.so                       libtensorflow.so.2.1.0-2.params\r\n\r\n\r\n", "You need to update LD_LIBRARY_PATH to make linker can find libtensorflowlite.so", "Thanks a lot...\r\nSo I have to:\r\n1. Include my library after object file\r\n2. Add path to LD_LIBRARY_PATH\r\n\r\nI wasted so many days on this. \r\nThank you once again", "System information:\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Tiny modification to existing minimal.cc file\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): not sure how to get this via c++, I cloned the latest tensorflow repository from here https://github.com/tensorflow/tensorflow\r\nPython version: 3.7.7\r\nBazel version (if compiling from source): not installed\r\nGCC/Compiler version (if compiling from source): g++  7.5.0\r\nCUDA/cuDNN version: none\r\nGPU model and memory: none\r\n\r\nExact command to reproduce:\r\n`g++ -o minimal minimal.o -Itensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a`\r\n\r\nI have build tensorflowlite for x86 following the instructions here : [link](https://www.tensorflow.org/lite/guide/build_arm64)\r\nBut instead of running `./build_aarch64_lib.sh`, I ran `./build_lib.sh` for x86\r\nI can see the generated `.a` files under : `tensorflow/lite/tools/make/gen/linux_x86_64/lib/` which are `benchmark-lib.a` and `libtensorflow-lite.a`\r\n\r\nI can see an executable already made for minimal.cc under  `tensorflow/lite/tools/make/gen/linux_x86_64/bin`, which I am able to run with a `.tflite` model and get the expected outputs.\r\n\r\nHowever I need to write a tweaked version of minimal.cc file for my model interpretation. How do I build this file ? Currently I see linker errors :\r\n`minimal.cc:(.text+0x6e): undefined reference to tflite::DefaultErrorReporter()\r\nminimal.cc:(.text+0x8a): undefined reference to tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)\r\nminimal.cc:(.text+0xe5): undefined reference to tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()\r\nminimal.cc:(.text+0x110): undefined reference to tflite::impl::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)`\r\n\r\nI have compiled the file via :\r\n`g++ -I. -Itensorflow/lite/tools/make/downloads/googletest/googletest/include/ -Itensorflow/lite/tools/make/downloads/absl/ -Itensorflow/lite/tools/make/downloads/flatbuffers/include/ -c tensorflow/lite/examples/minimal/minimal.cc` \r\n\r\nThings already done : \r\n`echo $LD_LIBRARY_PATH\r\n/home/rajveer/Downloads/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/`", "Resolved above via compiling in the `tensorflow/` root directory :\r\n`$ g++ tensorflow/lite/examples/minimal/minimal.cc -o minimal -std=c++11 -I. -Itensorflow/lite/tools/make/downloads/googletest/googletest/include/ -Itensorflow/lite/tools/make/downloads/absl/ -Itensorflow/lite/tools/make/downloads/flatbuffers/include/ -L/tf-lite/ -ltensorflow-lite -lrt -ldl -pthread`\r\n\r\nWhere linker path `/tf-lite` is the source directory containing `libtensorflow-lite.a` \r\n\r\nHope it helps others.\r\n\r\nHowever if I use the similar approach, I am not able to compile `label_image.cc` example under `tensorflow/lite/examples/label_image/label_image.cc` and  get undefined references for :\r\n`label_image.cc:(.text+0x55): undefined reference to \"tflite::evaluation::CreateGPUDelegate()\"`\r\n", "Hello,\r\n\r\n@rajveershringi @ashishmagar600 @jdduke @terryheo \r\n\r\nI'm trying to run a custom code written in C++ using TFLite. \r\nI.m trying to compile and run the code. But it's giving some errors.\r\n\r\nI also had undefined reference to `tflite::DefaultErrorReporter()' this error but managed to remove it.\r\n\r\nSomehow I've managed to install TensorflowLite from the source. \r\nI've put `tensorflow/tensorflow` directory in `/usr/local/include`. Other dependencies (absl,flatbuffers, etc) are also inside same directory. Using this command `bazel build //tensorflow/lite:libtensorflowlite.so` whcichever files (*.so, *.params, *.runfiles.manifest) are generated and put in `/usr/local/lib/` directory. \r\n\r\nAs said in this [comment](https://github.com/tensorflow/tensorflow/issues/36661#issuecomment-585034121) I tried to execute two commands\r\n```\r\ng++ -ltensorflow -c main.cc\r\ng++ -Llib -ltensorflow -o ./main.o -ltensorflowlite\r\n```\r\n\r\nAnd error was:\r\n```\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 1603 (>= sh_info of 1)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2710 (>= sh_info of 1)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2711 (>= sh_info of 1)\r\n/usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/9/../../../x86_64-linux-gnu/Scrt1.o: in function _start':\r\n(.text+0x24): undefined reference to main'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\nsecond time I did something like this:\r\n`g++ -ltensorflow -Llib main.cc -ltensorflowlite`\r\nAnd now error is :\r\n```\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 1603 (>= sh_info of 1)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2710 (>= sh_info of 1)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2711 (>= sh_info of 1)\r\n/usr/bin/ld: /tmp/ccR2jeyn.o: in function `main':\r\nmain.cc:(.text+0x314): undefined reference to `infer_float(tflite::impl::Interpreter*)'\r\n/usr/bin/ld: main.cc:(.text+0x32a): undefined reference to `infer_uint8(tflite::impl::Interpreter*)'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\nPlease help to correct me if I've done something wrong with the installation process or coding error.\r\n\r\ninfer_float and infer_uint8 are two functions declared in a header file. Then two separate .cc files contain the definition of each function. main.cc file has included header file and rest of code. \r\n\r\nHere's [link](https://github.com/pranv12/tflite_example_cc) to my repo which contains the code.", "@pranv12,\r\nCould you please create a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the issue template, so that we can track it there. Thanks!", "@amahendrakar \r\nThank you for your response. (And also sorry for late response.)\r\n\r\nI found the problem. The problem wasn't with TFLite but my .cc file compiling.\r\n\r\nI didn't compile all .cc files. \r\nI replaced command with `g++ -ltensorflow -Llib *.cc -ltensorflowlite` and it worked."]}, {"number": 36660, "title": "Add TensorRT optimization profiles", "body": "This PR adds optimization profile handling to TF-TRT and enables inference using the experimental dynamic shapes mode (where dynamic shapes mode =  explicit batch mode with dynamic input shapes).\r\n\r\nThis PR is not supposed to change the existing behavior in implicit batch mode. \r\n\r\nFor each unknown input dimension, a shape [optimization profile](https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_optimization_profile.html) describes what is the expected [min, max] range for that dimension, and also define an optimum value. \"The builder selects the kernels that result in the lowest runtime for the optimum input tensor dimensions, and are valid for all input tensor sizes in the valid range between minimum and maximum dimensions.\"\r\n\r\nTo create an engine with dynamic shapes one must define at least one optimization profile. In the current PR we define only a single optimization profile, using the shape of the concrete input data during the [first inference call](https://github.com/tfeher/tensorflow/blob/1ab228164fcbc648a8b885e2d05ccbf68375758f/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc#L599-L607).\r\n\r\nThe real advantage of dynamic shapes is to have a single engine that can execute the network with different input shapes. Unfortunately it is not possible to demonstrate this with the current PR. Although the `TrtShapeOptimizationProfile` class is designed to manage multiple profiles, we still need to enable profile generation mode during build phase, to take the full advatage of dynamic shapes mode. That feature is implemented in PR #36080.\r\n", "comments": ["@tfeher Can you please resolve conflicts? Thanks!"]}, {"number": 36659, "title": "Debug log implementation to use the STM32 HAL UART implementation. Re\u2026", "body": "\u2026directs (f)printf HAL_UART_Transmit using a UART_HandleTypeDef that must be provided as DEBUG_UART_HANDLE\r\n\r\nDebug log to be used with the STM32F4 HAL libraries instead of mbed. Tested working with the project [here](https://github.com/alxhoff/STM3240G-EVAL-TensorFlow) and the STM3240G-EVAL board, not using the TF make system though. I will look at completely integrating STM32 HAL into TF make soon.", "comments": ["I can't really make sense of why the Win build failed."]}, {"number": 36658, "title": "generate_projects target in the tf micro makefile fails for some target architectures", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: cygwin\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ARC\r\n\r\n**Describe the problem**\r\nThe generate_projects target in the tf micro makefile creates all projects, including the projects that are not valid for the given TARGET_ARCH. Only the projects valid for the provided TARGET_ARCH should be added to the ALL_PROJECT_TARGETS variable.\r\nPossible solutions:\r\n- add TARGET_ARCH specific conditions around filling the ALL_PROJECT_TARGETS list (note that this is not in line with the policy to keep TARGET_ARCH specific code in separate files)\r\n- Change the location of including the arch specific makefiles such that ALL_PROJECT_TARGETS can be filtered.\r\n- ...\r\n\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET_ARCH=arc generate_projects\r\n", "comments": ["Hi @JaccovG! We are checking to see if you still need help in this issue , Have you visited [TFlite documentation ](https://www.tensorflow.org/api_docs/python/tf/lite)from latest stable version TF 2.6  yet? Please create a new issue if the issue is replicating in newer version. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36658\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36658\">No</a>\n"]}, {"number": 36657, "title": "Hello World example for STM32F746 make error", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- TensorFlow installed from (source or binary): pip\r\n- Tensorflow version (commit SHA if source): 2.1.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): STM32F7\r\n\r\n**Describe the problem**\r\nI started following the guide here : https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world#deploy-to-STM32F746\r\nwhich required me to download TensorFlow's sources (I installed it previously using pip), and my goal is to deploy it to STM32F746. Except that the first step : \r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project`\r\ndoesn't work and throws me an error saying that it tried to download gemmlowp for some reason and that tensorflow is not recognized as an internal command... I get that I'm not supposed to run tensorflow outside of python but I'm just running **make** here... Shouldn't have any link whatsoever.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nUsed the first command from the \"Deploy to STM32F746\" guide `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project`\r\n\r\nEncountered first an error with unam -m tied to my OS or architecture, had to manually delete the makefile code regarding that and write it in stone; then I ran it again and got this :\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/micro/tools/make/downloads/gemmlowp\r\n'tensorflow' is not recognized as an internal or external command operable program or batch file\r\nmake: *** [tensorflow/lite/micro/tools/make/downloads/gemmlowp] Error 1\r\n```\r\n", "comments": ["@Nithraniel,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36657\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36657\">No</a>\n"]}, {"number": 36656, "title": "Profiler doesn't work correctly in tensorflow 2.1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos-release-7-6.1810.2.el7.centos.x86_64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nThe code below, taken from https://www.tensorflow.org/tensorboard/graphs, and used to profile a function call, seems to behave differently in tf 2.0 and tf 2.1. In 2.0, the memory and CPU usage can be inspected with `tensorboard`; in 2.1, they cannot as the radio buttons are grayed out. I am filing this as a tensorflow issue rather a tensorboard one since it seems to be independent of the tensorboard version. I also suspect it's due to the `tensorflow` pip package now including GPU support by default (the problem appears on a machine without a GPU).\r\n\r\n```\r\n# The function to be traced.\r\n@tf.function\r\ndef my_func(x, y):\r\n  # A simple hand-rolled layer.\r\n  return tf.nn.relu(tf.matmul(x, y))\r\n\r\n# Set up logging.\r\nstamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\nlogdir = 'logs/func/%s' % stamp\r\nwriter = tf.summary.create_file_writer(logdir)\r\n\r\n# Sample data for your function.\r\nx = tf.random.uniform((3, 3))\r\ny = tf.random.uniform((3, 3))\r\n\r\n# Bracket the function call with\r\n# tf.summary.trace_on() and tf.summary.trace_export().\r\ntf.summary.trace_on(graph=True, profiler=True)\r\n# Call only one tf.function when tracing.\r\nz = my_func(x, y)\r\nwith writer.as_default():\r\n  tf.summary.trace_export(\r\n      name=\"my_func_trace\",\r\n      step=0,\r\n      profiler_outdir=logdir)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should be the behaviour found in tf 2.0.\r\n\r\n**Other info / logs**\r\n\r\nScreenshots of the two tensorboard displays are shown below. Of note, running the code with tf 2.1 shows the errors below, which do not appear with tf 2.0. I do not have a GPU on my machine, but would still expect the profiler to work.\r\n\r\n```\r\n(...)\r\n2020-02-11 10:26:17.256467: I tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler session started.\r\n2020-02-11 10:26:17.256643: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.1'; dlerror: libcupti.so.10.1: cannot open shared object file: No such file or directory\r\n2020-02-11 10:26:17.256681: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2020-02-11 10:26:17.256701: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2020-02-11 10:26:17.393265: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2020-02-11 10:26:17.393327: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:88]  GpuTracer has collected 0 callback api events and 0 activity events.\r\n```\r\n![tf20profile](https://user-images.githubusercontent.com/45923989/74228945-58c22280-4cb9-11ea-935b-c24d4945f1e4.png)\r\n![tf21](https://user-images.githubusercontent.com/45923989/74228948-595ab900-4cb9-11ea-9609-3c04b6fbfb82.png)\r\n\r\n\r\n", "comments": ["@ksabr,\r\nI was able to run the code in TF 2.1 without any issues. Please find the Gist [here](https://colab.research.google.com/gist/amahendrakar/383565fa64dbb1602d3dccd2b00ce550/36656.ipynb). Thanks!", "After some investigation, this feels more like a tensorboard graph plugin problem. Notably even with the above GPU errors (about CUPTI etc), the CPU profile is generated (it seems very similar to what `tensorflow-cpu` produces without any error) and it can be even viewed in the Profile tab (but not in Graph tab).\r\n\r\nSo my guess is maybe the TF 2.1 profile uses some new or different features which the Graph tab does not unserstand. And yes, profile generated by TF 2.0 is available both in Profile and Graph tab.\r\n", "@ksabr,\r\nAny updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "After encountering the same problem today, I dug around a bit in the internals. Tensorboard has an endpoint that decodes the run metadata section of the protocol buffer containing the event logs. When running this on the logs of the above notebook, once with 2.1.0 and 2.0.0, I get different outputs:\r\n\r\n[run_metadata_v200.txt](https://github.com/tensorflow/tensorflow/files/4270483/run_metadata_v200.txt)\r\n[run_metadata_v210.txt](https://github.com/tensorflow/tensorflow/files/4270484/run_metadata_v210.txt)\r\n\r\nIt appears that the step_stats field in the run_metadata log is not filled out in 2.1, but is in 2.0.\r\n\r\nhttps://github.com/tensorflow/tensorboard/blob/799c5cf6ae1f5f8d78a09382689dcb9d0ec5f561/tensorboard/compat/proto/config.proto#L637-L640", "After digging deeper into the C++ code, it appears that between 2.0.0 and 2.1.0 the recording of this information was removed [in this commit](https://github.com/tensorflow/tensorflow/commit/0c9c859a7332c4d700ca131dd77944751d2feedc), so I don't think there is a \"quick fix\" for this."]}, {"number": 36655, "title": "Update wav_to_features.py", "body": "Remove the leading comma in the generated float array. With this leading comma, the generated C code can't be compiled.", "comments": []}, {"number": 36654, "title": "Tensorflow Lite Android Application Crashes on Inference A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x7759200000 in tid 26388 (mple.inpainting), pid 26388 (mple.inpainting)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung A30\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.5.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\nI am working on InPainting application on android using the TensorflowLite model but the application crashes during inference.\r\n\r\n**Describe the expected behavior**\r\nI am developing an Inpainting android application using tensorflow lite. The tensorflow lite model takes two inputs:\r\n\r\n1. Input Image (512x680x3)\r\n2. Mask Image(512x680x1)\r\n\r\nThese two input images are loaded from the asset folder in the form of Bitmaps and then converted into ByteBuffer. After that, they are passed to the tensorflow lite model and the model is supposed to give output in the form of ByteBuffer. Later this ByteBuffer is converted into a restored RGB image.\r\n\r\n**Code to reproduce the issue**\r\nMain Activity Code\r\n```\r\n protected void onCreate(Bundle savedInstanceState) {\r\n        super.onCreate(savedInstanceState);\r\n        setContentView(R.layout.activity_main);\r\n        Toolbar toolbar = findViewById(R.id.toolbar);\r\n        setSupportActionBar(toolbar);\r\n\r\n        FloatingActionButton fab = findViewById(R.id.fab);\r\n        fab.setOnClickListener(new View.OnClickListener() {\r\n            @Override\r\n            public void onClick(View view) {\r\n                Snackbar.make(view, \"Replace with your own action\", Snackbar.LENGTH_LONG)\r\n                        .setAction(\"Action\", null).show();\r\n            }\r\n        });\r\n\r\n        try {\r\n            tflite = new Interpreter(loadModelFile(MainActivity.this, \"converted_model.tflite\"));\r\n\r\n            Bitmap Image = getBitmapFromAsset(this,\"001.png\");\r\n            Bitmap Mask = getBitmapFromAsset(this,\"tmp_mask.png\");\r\n\r\n            ByteBuffer BImage = convertBitmapToByteBuffer(Image);\r\n            System.out.println(\"First Image Converted\");\r\n            ByteBuffer BMask = convertBitmapToByteBuffer1(Mask);\r\n            System.out.println(\"Second Image Converted\");\r\n\r\n            BImage.order(ByteOrder.nativeOrder());\r\n            BMask.order(ByteOrder.nativeOrder());\r\n\r\n            System.out.println(\"Bytes in BIMAGE \"+BImage.position());\r\n            System.out.println(\"Bytes in BMask \"+BMask.position());\r\n\r\n            Object[] Inputs = {BImage,BMask};\r\n\r\n\r\n            int OutputTensorIndex = 0;\r\n            int[] OutputShape =\r\n                    tflite.getOutputTensor(OutputTensorIndex).shape(); // {1, NUM_CLASSES}\r\n            DataType OutputDataType = tflite.getOutputTensor(OutputTensorIndex).dataType();\r\n            System.out.println(\"Output 1 Shape \"+ OutputShape[1]+\" \"+OutputShape[2]+\" +OutputShape[3]);\r\n            System.out.println(\"Output 1 DataType \" + OutputDataType);\r\n\r\n            TensorBuffer outputBuffer = TensorBuffer.createFixedSize(OutputShape, OutputDataType);\r\n            Map<Integer, Object> outputs = new HashMap<>();\r\n            System.out.println(\"Resizing Input\");\r\n            System.out.println(\"Bytes in Ouput\" + outputBuffer.getBuffer().position());\r\n            outputs.put(0,outputBuffer.getBuffer());\r\n            int[] input1 = new int[]{1,512,680,3};\r\n            int[] input2 = new int[]{1,512,680,1};\r\n            tflite.resizeInput(0,input1);\r\n            tflite.resizeInput(1,input2);\r\n            System.out.println(\"Resizing Input Done\");\r\n\r\n            tflite.runForMultipleInputsOutputs(Inputs, outputs);\r\n\r\n            Toast.makeText(this,\"Working\",Toast.LENGTH_LONG).show();\r\n            System.out.println(\"Inference Complete\");\r\n\r\n            tflite.close();\r\n\r\n        } catch (IOException e) {\r\n            Toast.makeText(this,\"Failed\",Toast.LENGTH_LONG).show();\r\n            e.printStackTrace();\r\n        }\r\n\r\n}\r\n```\r\nCode for getting Bitmap from Asset folder\r\n```\r\nprivate Bitmap getBitmapFromAsset(Context context, String filePath) {\r\n        AssetManager assetManager = context.getAssets();\r\n\r\n        InputStream istr;\r\n        Bitmap bitmap = null;\r\n        try {\r\n            istr = assetManager.open(filePath);\r\n            bitmap = BitmapFactory.decodeStream(istr);\r\n        } catch (IOException e) {\r\n            // handle exception\r\n        }\r\n        return bitmap;\r\n    }\r\n```\r\nCode for Converting Bitmap to ByteBuffer for RGB Input\r\n```\r\nprivate ByteBuffer convertBitmapToByteBuffer(Bitmap bitmap) {\r\n        int IMAGE_MEAN = 128;\r\n        float IMAGE_STD = 128.0f;\r\n        ByteBuffer byteBuffer = ByteBuffer.allocateDirect(4 * 1 * 512 * 680 * 3);\r\n        int[] intValues = new int[512 * 680];\r\n        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n        int pixel = 0;\r\n        for (int i = 0; i < 512; ++i) {\r\n            for (int j = 0; j < 680; ++j) {\r\n                final int val = intValues[pixel++];\r\n                byteBuffer.putFloat((((val >> 16) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n                byteBuffer.putFloat((((val >> 8) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n                byteBuffer.putFloat((((val) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n            }\r\n        }\r\n        return byteBuffer;\r\n    }\r\n```\r\nCode for Converting Bitmap to ByteBuffer for Single Channel Input\r\n\r\n```\r\nprivate ByteBuffer convertBitmapToByteBuffer1(Bitmap bitmap) {\r\n        int IMAGE_MEAN = 128;\r\n        float IMAGE_STD = 128.0f;\r\n        ByteBuffer byteBuffer1 = ByteBuffer.allocateDirect(4 * 1 * 512 * 680 * 1);\r\n        //byteBuffer1.order(ByteOrder.nativeOrder());\r\n        int[] intValues = new int[512 * 680];\r\n        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n        int pixel = 0;\r\n        for (int i = 0; i < 512; ++i) {\r\n            for (int j = 0; j < 680; ++j) {\r\n                final int val = intValues[pixel++];\r\n                //byteBuffer1.putFloat((((val >> 16) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n                //byteBuffer1.putFloat((((val >> 8) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n                byteBuffer1.putFloat((((val) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n            }\r\n        }\r\n        return byteBuffer1;\r\n    }\r\n```\r\n\r\n\r\n**Other info / logs**\r\n```\r\n2020-02-11 14:56:45.730 26572-26572/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n2020-02-11 14:56:44.696 26388-26388/com.example.inpainting A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x7759200000 in tid 26388 (mple.inpainting), pid 26388 (mple.inpainting)\r\n\r\n\r\n2020-02-11 14:56:45.730 26572-26572/? A/DEBUG: Build fingerprint: 'samsung/a30dd/a30:9/PPR1.180610.011/A305FDDU2ASF3:user/release-keys'\r\n2020-02-11 14:56:45.730 26572-26572/? A/DEBUG: Revision: '3'\r\n2020-02-11 14:56:45.730 26572-26572/? A/DEBUG: ABI: 'arm64'\r\n2020-02-11 14:56:45.730 26572-26572/? A/DEBUG: pid: 26388, tid: 26388, name: mple.inpainting  >>> com.example.inpainting <<<\r\n2020-02-11 14:56:45.730 26572-26572/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x7759200000\r\n2020-02-11 14:56:45.731 26572-26572/? A/DEBUG:     x0  000000770c7eb900  x1  0000007756164700  x2  ffffffffb35eb630  x3  00000077591fffc0\r\n2020-02-11 14:56:45.731 26572-26572/? A/DEBUG:     x4  000000770974fd80  x5  000000770c7eb680  x6  bf7ff19cbf7fec39  x7  bf7ff8c9bf5baee6\r\n2020-02-11 14:56:45.731 26572-26572/? A/DEBUG:     x8  bf800000bf800000  x9  bf709453bf7fdb6c  x10 bf7fffeebf7ffeac  x11 bf7fff54bf7ffa20\r\n2020-02-11 14:56:45.731 26572-26572/? A/DEBUG:     x12 bf800000bf800000  x13 bf7f9c97bf7fb93d  x14 00000000000000aa  x15 00000000000000aa\r\n2020-02-11 14:56:45.731 26572-26572/? A/DEBUG:     x16 0000007759593008  x17 00000077f5f60cf0  x18 0000000000000000  x19 fffffffffffffa00\r\n2020-02-11 14:56:45.731 26572-26572/? A/DEBUG:     x20 fffffffffffffd80  x21 0000000000000000  x22 0000007709750000  x23 0000000000000000\r\n2020-02-11 14:56:45.731 26572-26572/? A/DEBUG:     x24 000000770c7eb900  x25 0000000000000001  x26 0000000000005500  x27 0000000000000380\r\n2020-02-11 14:56:45.731 26572-26572/? A/DEBUG:     x28 0000000000000000  x29 0000007fe8dc5b90\r\n2020-02-11 14:56:45.731 26572-26572/? A/DEBUG:     sp  0000007fe8dc5af0  lr  0000007759423300  pc  00000077f5f60e18\r\n\r\n2020-02-11 14:56:46.346 26572-26572/? A/DEBUG: backtrace:\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #00 pc 000000000001de18  /system/lib64/libc.so (memcpy+296)\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #01 pc 000000000005b2fc  /data/app/com.example.inpainting-40kevdvzYiOK_rsH1RbKGA==/lib/arm64/libtensorflowlite_jni.so\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #02 pc 000000000005b008  /data/app/com.example.inpainting-40kevdvzYiOK_rsH1RbKGA==/lib/arm64/libtensorflowlite_jni.so\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #03 pc 000000000005a5a0  /data/app/com.example.inpainting-40kevdvzYiOK_rsH1RbKGA==/lib/arm64/libtensorflowlite_jni.so\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #04 pc 0000000000061ac8  /data/app/com.example.inpainting-40kevdvzYiOK_rsH1RbKGA==/lib/arm64/libtensorflowlite_jni.so\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #05 pc 000000000005a358  /data/app/com.example.inpainting-40kevdvzYiOK_rsH1RbKGA==/lib/arm64/libtensorflowlite_jni.so\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #06 pc 0000000000141aa8  /data/app/com.example.inpainting-40kevdvzYiOK_rsH1RbKGA==/lib/arm64/libtensorflowlite_jni.so\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #07 pc 0000000000144f88  /data/app/com.example.inpainting-40kevdvzYiOK_rsH1RbKGA==/lib/arm64/libtensorflowlite_jni.so\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #08 pc 000000000000eec8  /data/app/com.example.inpainting-40kevdvzYiOK_rsH1RbKGA==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+32)\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #09 pc 0000000000563be0  /system/lib64/libart.so (art_quick_generic_jni_trampoline+144)\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #10 pc 000000000055ae4c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #11 pc 00000000000d04e8  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #12 pc 00000000002838c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #13 pc 000000000027d8c8  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+968)\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #14 pc 000000000052b784  /system/lib64/libart.so (MterpInvokeStatic+204)\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #15 pc 000000000054d394  /system/lib64/libart.so (ExecuteMterpImpl+14612)\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #16 pc 000000000019b778  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.example.inpainting-40kevdvzYiOK_rsH1RbKGA==/base.apk_26388_26388 (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.run+156)\r\n2020-02-11 14:56:46.347 26572-26572/? A/DEBUG:     #17 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #18 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #19 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #20 pc 000000000052a280  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #21 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #22 pc 000000000019adee  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.example.inpainting-40kevdvzYiOK_rsH1RbKGA==/base.apk_26388_26388 (deleted) (org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs+10)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #23 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #24 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #25 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #26 pc 000000000052a280  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #27 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #28 pc 000000000001c28c  /dev/ashmem/dalvik-classes2.dex extracted in memory from /data/app/com.example.inpainting-40kevdvzYiOK_rsH1RbKGA==/base.apk!classes2.dex_26388_26388 (deleted) (com.example.inpainting.MainActivity.onCreate+736)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #29 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #30 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #31 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #32 pc 000000000052a280  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #33 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #34 pc 00000000004c641e  /system/framework/boot-framework.vdex (android.app.Activity.performCreate+24)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #35 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #36 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #37 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #38 pc 000000000052d2c4  /system/lib64/libart.so (MterpInvokeVirtualQuick+584)\r\n2020-02-11 14:56:46.348 26572-26572/? A/DEBUG:     #39 pc 0000000000550f94  /system/lib64/libart.so (ExecuteMterpImpl+29972)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #40 pc 00000000005eaccc  /system/framework/boot-framework.vdex (android.app.Activity.performCreate+2)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #41 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #42 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #43 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #44 pc 000000000052a280  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #45 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #46 pc 00000000004e6948  /system/framework/boot-framework.vdex (android.app.Instrumentation.callActivityOnCreate+6)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #47 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #48 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #49 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #50 pc 000000000052a280  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #51 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #52 pc 00000000004bd762  /system/framework/boot-framework.vdex (android.app.ActivityThread.performLaunchActivity+864)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #53 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #54 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #55 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #56 pc 000000000052b5c0  /system/lib64/libart.so (MterpInvokeDirect+296)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #57 pc 000000000054d314  /system/lib64/libart.so (ExecuteMterpImpl+14484)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #58 pc 00000000004bd37a  /system/framework/boot-framework.vdex (android.app.ActivityThread.handleLaunchActivity+72)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #59 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #60 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #61 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #62 pc 000000000052a280  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #63 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #64 pc 00000000005050cc  /system/framework/boot-framework.vdex (android.app.servertransaction.LaunchActivityItem.execute+114)\r\n2020-02-11 14:56:46.349 26572-26572/? A/DEBUG:     #65 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #66 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #67 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #68 pc 000000000052a280  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #69 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #70 pc 0000000000505d50  /system/framework/boot-framework.vdex (android.app.servertransaction.TransactionExecutor.executeCallbacks+198)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #71 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #72 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #73 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #74 pc 000000000052a280  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #75 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #76 pc 0000000000505c62  /system/framework/boot-framework.vdex (android.app.servertransaction.TransactionExecutor.execute+68)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #77 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #78 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #79 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #80 pc 000000000052a280  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #81 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #82 pc 00000000004bcb2c  /system/framework/boot-framework.vdex (android.app.ActivityThread$H.handleMessage+208)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #83 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #84 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #85 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #86 pc 000000000052a280  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #87 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #88 pc 0000000000c637de  /system/framework/boot-framework.vdex (android.os.Handler.dispatchMessage+42)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #89 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #90 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #91 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #92 pc 000000000052a280  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-02-11 14:56:46.350 26572-26572/? A/DEBUG:     #93 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #94 pc 0000000000c6c452  /system/framework/boot-framework.vdex (android.os.Looper.loop+406)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #95 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #96 pc 000000000025d0c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #97 pc 000000000027d8ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #98 pc 000000000052b784  /system/lib64/libart.so (MterpInvokeStatic+204)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #99 pc 000000000054d394  /system/lib64/libart.so (ExecuteMterpImpl+14612)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #100 pc 00000000004c27b8  /system/framework/boot-framework.vdex (android.app.ActivityThread.main+220)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #101 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #102 pc 000000000051ab14  /system/lib64/libart.so (artQuickToInterpreterBridge+1020)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #103 pc 0000000000563cfc  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #104 pc 000000000055ae4c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #105 pc 00000000000d04e8  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #106 pc 00000000004618ac  /system/lib64/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+104)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #107 pc 0000000000463300  /system/lib64/libart.so (art::InvokeMethod(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jobject*, _jobject*, unsigned long)+1440)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #108 pc 00000000003f2984  /system/lib64/libart.so (art::Method_invoke(_JNIEnv*, _jobject*, _jobject*, _jobjectArray*)+52)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #109 pc 000000000011e7e4  /system/framework/arm64/boot.oat (offset 0x114000) (java.lang.Class.getDeclaredMethodInternal [DEDUPED]+180)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #110 pc 000000000055ab88  /system/lib64/libart.so (art_quick_invoke_stub+584)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #111 pc 00000000000d04c8  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+200)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #112 pc 00000000002838c0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #113 pc 000000000027d8c8  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+968)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #114 pc 000000000052a280  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #115 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-02-11 14:56:46.351 26572-26572/? A/DEBUG:     #116 pc 00000000013d1462  /system/framework/boot-framework.vdex (com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run+22)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #117 pc 00000000002575cc  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3090282045+488)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #118 pc 000000000051ab14  /system/lib64/libart.so (artQuickToInterpreterBridge+1020)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #119 pc 0000000000563cfc  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #120 pc 0000000000e17090  /system/framework/arm64/boot-framework.oat (offset 0x41f000) (com.android.internal.os.ZygoteInit.main+2208)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #121 pc 000000000055ae4c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #122 pc 00000000000d04e8  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #123 pc 00000000004618ac  /system/lib64/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+104)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #124 pc 000000000046150c  /system/lib64/libart.so (art::InvokeWithVarArgs(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, std::__va_list)+424)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #125 pc 0000000000366214  /system/lib64/libart.so (art::JNI::CallStaticVoidMethodV(_JNIEnv*, _jclass*, _jmethodID*, std::__va_list)+652)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #126 pc 00000000000b8ebc  /system/lib64/libandroid_runtime.so (_JNIEnv::CallStaticVoidMethod(_jclass*, _jmethodID*, ...)+120)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #127 pc 00000000000bba78  /system/lib64/libandroid_runtime.so (android::AndroidRuntime::start(char const*, android::Vector<android::String8> const&, bool)+772)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #128 pc 000000000000498c  /system/bin/app_process64 (main+1200)\r\n2020-02-11 14:56:46.352 26572-26572/? A/DEBUG:     #129 pc 00000000000ae8f0  /system/lib64/libc.so (__libc_init+88)\r\n```\r\n", "comments": ["I recommend you to check addresses on backtrace with addr2line utility.\r\nIt's likely memory override by size mismatch. \r\n", "@terryheo I am Importing Tensorflow-Lite from **implementation 'org.tensorflow:tensorflow-lite:2.0.0'** instead of building the library so I think addr2line utility cannot be used in these circumstances.", "@nauyan, therefore you'd better build tflite by yourself. In this way, you can add more debug code to tflite.", "@terryheo I have tried the addr2line utility. I ran the following command:\r\n\r\n`$ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 0022b194`\r\n\r\nThe output I got is: \r\n\r\n```\r\nTfLiteDelegateCreate\r\n??:?\r\n```\r\n\r\n\r\n", "@terryheo Here I have run addr2line for all the adresses from error log\r\n\r\n```\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 0022b194\r\nTfLiteDelegateCreate\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 0022b2a7\r\nTfLiteDelegateCreate\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 00227d19\r\nTfLiteDelegateCreate\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 0022768e\r\nTfLiteDelegateCreate\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 002275e3\r\nTfLiteDelegateCreate\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 0022aaf8\r\nTfLiteDelegateCreate\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 0022ab30\r\nTfLiteDelegateCreate\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 0020e2fa\r\nJava_org_tensorflow_lite_TensorFlowLite_nativeSchemaVersion\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 0020b950\r\nJava_org_tensorflow_lite_TensorFlowLite_nativeSchemaVersion\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 00200ea3\r\nJava_org_tensorflow_lite_TensorFlowLite_nativeSchemaVersion\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 00200bce\r\nJava_org_tensorflow_lite_TensorFlowLite_nativeSchemaVersion\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 002054c5\r\nJava_org_tensorflow_lite_TensorFlowLite_nativeSchemaVersion\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 000080ef\r\nJava_org_tensorflow_lite_NativeInterpreterWrapper_allocateTensors\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 0000f0d2\r\nJava_org_tensorflow_lite_NativeInterpreterWrapper_delete\r\n??:?\r\n(base) nauyan@nauyan:~/Desktop/EsperSolutions/SystemCodes/DEC2019/InPainting/Android_APP_New$ $ANDROID_NDK_ROOT/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-addr2line -C -f -e /home/nauyan/Desktop/libtensorflowlite_jni.so 000d9fff\r\nJava_org_tensorflow_lite_TensorFlowLite_nativeSchemaVersion\r\n??:?\r\n```\r\n", "@terryheo any suggestions?", "I wondered if you used optimized build. You'd better use \"debug\" build not to strip symbols.\r\nIf you're using ARM64 target,\r\n\r\n```sh\r\nbazel build -c dbg --config=android_arm64 //tensorflow/lite/java:tensorflow-lite\r\n# Use bazel-bin/tensorflow/lite/java/tensorflow-lite.aar for your app build\r\n# Once you have fault address,\r\naddr2line <the fault address> -e bazel-out/android-arm64-v8a-dbg/bin/tensorflow/lite/java/libtensorflowlite_jni.so\r\n\r\n```\r\nIt should work for you.\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36654\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36654\">No</a>\n"]}, {"number": 36653, "title": "Results difference after converting a tensorflow op to TfLite", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: MacOS 10.15\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\nI converted a Tensorflow model with only one Tensorflow Keras layer _tf.keras.layers.Conv2D_ to TFLite and ran the TFLite model. There was no quantization when converting the model.\r\n\r\n**Describe the expected behavior**\r\nThe expected output of the TFLite model should be the same as the output of the Tensorflow model. However, the output of the TFLite model is slightly different from the result of the original Tensorflow model. I'm not sure whether this is a bug of TFLite framework or there was something wrong when I converted the Tensorflow model toTFLite.\r\n\r\n**Code to reproduce the issue**\r\n```Python\r\nimport tensorflow as tf\r\n\r\ninput = tf.keras.layers.Input(shape=(224, 224, 3), dtype='float32')\r\noutput = (filters=64, kernel_size=(7,7), strides=(2,2), padding='valid', data_format=\"channels_last\", activation='linear', dilation_rate=(1,1), bias_initializer='zeros')(input)\r\ntfmodel = tf.keras.Model(inputs=input, outputs=output)\r\n\r\nx = tf.random.normal([1,224, 224, 3], dtype=tf.float32)\r\ntf_y = tfmodel(x)\r\n\r\nconv = tf.lite.TFLiteConverter.from_keras_model(tfmodel)\r\nlite_bytes = conv.convert()\r\ninterp = tf.lite.Interpreter(model_content=lite_bytes)\r\n\r\ninput_index = interp.get_input_details()[0]['index']\r\noutput_index = interp.get_output_details()[0]['index']\r\ninterp.resize_tensor_input(input_index, [1, 224, 224, 3])\r\ninterp.allocate_tensors()\r\ninterp.set_tensor(input_index, x)\r\ninterp.invoke()\r\nlite_y = interp.get_tensor(output_index)\r\n```\r\n\r\n**Other info / logs**\r\nThe issue is similar when the Conv2D layer is replaced with BatchNormalization layer.\r\n\r\n", "comments": ["@MonicaGu I ran your code and find very very small mismatch (in the 7th decimal) in the results between keras  and tflite_model. Do you see significantly different results. Please check the [gist here]( https://colab.sandbox.google.com/gist/jvishnuvardhan/e7d0429feef15a434a36e5226c10f214/untitled819.ipynb). Thanks!", "@jvishnuvardhan In my case, I also found very small mismatch in the 7th or 6th decimal. I am not sure is this small mismatch inevitable? Or is this caused by a bug in the computation of Conv2D in TFLite? Thank you!", "It's expected behavior. Since floating point has limited precision, calculated results can be vary by order of execution. \r\nhttps://en.wikipedia.org/wiki/Floating-point_arithmetic#Accuracy_problems\r\nYou can see we're allowing 1e-5 (0.00001) errors in our TFLite kernel testing.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/conv_test.cc#L819\r\nSo the 7th or 6th decimal difference looks good.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36653\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36653\">No</a>\n"]}]