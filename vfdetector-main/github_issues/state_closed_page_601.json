[{"number": 35639, "title": "Pipeline & TypeError: can't pickle _thread.RLock objects", "body": "\r\n**System information**\r\n- colab script\r\n# Install the latest Tensorflow version.\r\n!pip3 install --upgrade tensorflow-gpu\r\n# Install TF-Hub.\r\n!pip3 install tensorflow-hub\r\n!pip3 install seaborn\r\n- Python version:\r\nPython 3.6.9 (default, Nov  7 2019, 10:44:02) \r\n[GCC 8.3.0] on linux\r\n\r\nUsing google universal sentence encoder ( see:[https://tfhub.dev/google/universal-sentence-encoder/4]( https://tfhub.dev/google/universal-sentence-encoder/4) )\r\nin a scikit Pipeline return following error:\r\nTypeError: can't pickle _thread.RLock objects\r\nI suppose the point is that it is not possible to clone the object with:\r\nfrom sklearn.base import clone\r\n\r\nHere you can find code to reproduce error:\r\n\r\n```\r\n\r\n!pip3 install --upgrade tensorflow-gpu\r\n!pip3 install tensorflow-hub\r\n!pip3 install seaborn\r\n\r\nmodule = hub.Module(href)\r\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\r\nmodel = hub.load(module_url)\r\nprint (\"module %s loaded\" % module_url)\r\ndef embed(input):\r\n  return model(input)\r\n\r\nclass UnivEmbedding( BaseEstimator, TransformerMixin ):\r\n  #Class Constructor \r\n  def __init__( self, module, use_light=True, verbose=False):\r\n       self.module = module\r\n       self.use_light= use_light\r\n       self.verbose= verbose \r\n\r\n  #Return self nothing else to do here   \r\n  def fit( self, X, y = None ):\r\n    return self \r\n\r\n  #Method that describes what we need this transformer to do\r\n  def transform( self, X, y = None ):\r\n    return embed(X) # universal_embedding (self.module, X, self.use_light, self.verbose)\r\n\r\n  def fit_transform( self, X, y = None ):  \r\n    if self.verbose: print(self.module)\r\n    return embed(X) #universal_embedding (self.module, X, self.use_light, self.verbose)\r\n\r\n  def get_params(self, deep=True):\r\n    return {\"module\": self.module, \"use_light\": self.use_light, \"verbose\": self.verbose}\r\n\r\n  def set_params(self, **parameters):\r\n    for parameter, value in parameters.items():\r\n        setattr(self, parameter, value)\r\n    return self\r\n\r\n\r\nuniv_emb= UnivEmbedding(module, use_light=False, verbose=False)\r\n\r\nclone (univ_emb)\r\n```\r\n\r\nIs it a bug? Can you help me?", "comments": ["@FabioMD1972 \r\n\r\nThis issue is more suitable for TensorFlow Hub repo. Please post it on hub repo from [here.](https://github.com/tensorflow/hub/issues) Thanks!", "Done, many thanks, F.", "Ask to TensorFlow Hub repo: [https://github.com/tensorflow/hub/issues/474](https://github.com/tensorflow/hub/issues/474)\r\n"]}, {"number": 35638, "title": "Use builtin compiler flags for CMSIS-NN and some more improvements", "body": "This PR fixes:\r\n1) Use builtin compiler flags for CMSIS-NN instead of those defined in cmsis.inc\r\n2) Adds a readme to the ext_lib folder to provide more details on CMSIS-NN optimizations\r\n3) Includes 3rd party (arm github) CMSIS-NN related files recursively instead of having to include each individual file manually as the CMSIS-NN github project evolves.", "comments": ["I took out the cmsis-nn/pooling.cc fix. We'll deliver it in a separate PR instead", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35638) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35638) for more info**.\n\n<!-- ok -->", "Gentle ping for review @petewarden @njeffrie :)"]}, {"number": 35637, "title": "tensorflow for arm64 issue", "body": "Hi,\r\n\r\nI have successfully built the tensorflow for arm64 and indeed it created libtensorflow-lite.a However I'm trying to import the tesorflow in one of my python script like\r\n\r\n$ cat test.py\r\nimport numpy\r\nimport tensorflow\r\n\r\nIt is showing No named modules something like.\r\n\r\n$ python3 test.py \r\nTraceback (most recent call last):\r\n  File \"test_imports/test_ai.py\", line 2, in <module>\r\n    import tensorflow\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\nDo we need to do any setup to python to picking the tensorflow here? or did I miss any step.\r\n\r\nAny help?\r\n", "comments": ["how did you build it?\r\nyou can't import a .a you must import a python library which usually is a dylib or so or dll", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35636, "title": "Failed to train with keras model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): \r\nsource\r\n- TensorFlow version (use command below): \r\ntf2.0.0\r\n- Python version:\r\n 3.5\r\n- CUDA/cuDNN version: \r\nCUDA10\r\n- GPU model and memory: \r\nnvidia V100  8*GPU 450G\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nI try to train a tf.keras.applications.MobileNetV2 on the dataset \"tf_flowers\". However, I have two styles of training loo: 1) keras style  2) tensorflow style.\r\nAs a result, keras style works but tensorflow style doesn't learn anything.\r\n\r\nresult:\r\nkeras style:\r\n![image](https://user-images.githubusercontent.com/33815430/71894611-b975b100-3189-11ea-8f9b-896069c057d2.png)\r\n\r\n\r\ntensorflow style:\r\n![image](https://user-images.githubusercontent.com/33815430/71894296-cc3bb600-3188-11ea-8ad4-e65d5b139ca1.png)\r\n\r\n\r\n**Describe the expected behavior**\r\nI expect Tensorflow style training loop could work. I am pretty sure my training loop is corrcet, \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nTensorflow style training loop:\r\n```python\r\nimport os, math, json\r\nimport numpy as np\r\nfrom matplotlib import pyplot as plt\r\nimport tensorflow as tf\r\n\r\nprint(\"Tensorflow version \" + tf.__version__)\r\n# tf.enable_eager_execution()\r\nAUTO = tf.data.experimental.AUTOTUNE\r\n\r\nHAS_COLAB_TPU = 'COLAB_TPU_ADDR' in os.environ\r\nassert not HAS_COLAB_TPU, \"Please select a GPU backend for this notebook. Pre-trained models in tf.keras.applications.* are not yet TPU-compatible\"\r\n\r\nGCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192-2/*.tfrec'\r\nIMAGE_SIZE = [192, 192]\r\n\r\nBATCH_SIZE = 64 # 128 works on GPU too but comes very close to the memory limit of the Colab GPU\r\nEPOCHS = 5\r\n\r\nVALIDATION_SPLIT = 0.19\r\nCLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # do not change, maps to the labels in the data (folder names)\r\n\r\n# splitting data files between training and validation\r\nfilenames = tf.io.gfile.glob(GCS_PATTERN)\r\nsplit = int(len(filenames) * VALIDATION_SPLIT)\r\ntraining_filenames = filenames[split:]\r\nvalidation_filenames = filenames[:split]\r\n\r\nprint(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\r\nvalidation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE\r\nsteps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE\r\nprint(\"With a batch size of {}, there will be {} batches per training epoch and {} batch(es) per validation run.\".format(BATCH_SIZE, steps_per_epoch, validation_steps))\r\n\r\ndef read_tfrecord(example):\r\n    features = {\r\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\r\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\r\n    }\r\n    example = tf.io.parse_single_example(serialized=example, features=features)\r\n    image = tf.image.decode_jpeg(example['image'], channels=3)\r\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\r\n    # image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size will be needed for TPU\r\n    image = tf.image.resize(image, (192, 192))\r\n    class_label = example['class']\r\n    return image, class_label\r\n\r\ndef load_dataset(filenames):\r\n  # read from TFRecords. For optimal performance, use \"interleave(tf.data.TFRecordDataset, ...)\"\r\n  # to read from multiple TFRecord files at once and set the option experimental_deterministic = False\r\n  # to allow order-altering optimizations.\r\n\r\n  option_no_order = tf.data.Options()\r\n  option_no_order.experimental_deterministic = False\r\n\r\n  dataset = tf.data.Dataset.from_tensor_slices(filenames)\r\n  dataset = dataset.with_options(option_no_order)\r\n  #dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=16)\r\n  dataset = dataset.interleave(tf.data.TFRecordDataset, cycle_length=16, num_parallel_calls=AUTO) # faster\r\n  dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\r\n  return dataset\r\n\r\n\r\ndef get_batched_dataset(filenames):\r\n  dataset = load_dataset(filenames)\r\n  # dataset = dataset.cache() # This dataset fits in RAM\r\n  # dataset = dataset.repeat()\r\n  dataset = dataset.batch(BATCH_SIZE) # drop_remainder will be needed on TPU\r\n  dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\r\n  # should shuffle too but this dataset was well shuffled on disk already\r\n  # For proper ordering of map/batch/repeat/prefetch, see Dataset performance guide: https://www.tensorflow.org/guide/performance/datasets\r\n  return dataset\r\n\r\n# instantiate the datasets\r\ntrain_ds = get_batched_dataset(training_filenames)\r\ntest_ds = get_batched_dataset(validation_filenames)\r\n\r\n\r\n\r\npretrained_model = tf.keras.applications.MobileNetV2(input_shape=[*IMAGE_SIZE, 3], include_top=False, weights='imagenet')\r\npretrained_model.trainable = True\r\n\r\nmodel = tf.keras.Sequential([\r\n  pretrained_model,\r\n  tf.keras.layers.GlobalAveragePooling2D(),\r\n  tf.keras.layers.Dense(5, activation=\"softmax\")\r\n])\r\n\r\n\r\n\r\n\r\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\r\noptimizer = tf.keras.optimizers.SGD()\r\n\r\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\r\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n\r\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\r\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\r\n\r\n\r\n## ----- training loop -----##\r\n@tf.function\r\ndef train_step(images, labels):\r\n  # tf.print(images)\r\n  with tf.GradientTape() as tape:\r\n    predictions = model(images)\r\n    loss = loss_object(labels, predictions)\r\n  gradients = tape.gradient(loss, model.trainable_variables)\r\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n  train_loss(loss)\r\n  train_accuracy(labels, predictions)\r\n  template = 'Epoch {}, Loss: {}, Accuracy: {}'\r\n\r\n@tf.function\r\ndef test_step(images, labels):\r\n\r\n  predictions = model(images)\r\n  t_loss = loss_object(labels, predictions)\r\n\r\n  test_loss(t_loss)\r\n  test_accuracy(labels, predictions)\r\n  print(test_loss.result())\r\n\r\nEPOCHS = 20\r\n\r\nfor epoch in range(EPOCHS):\r\n  # Reset the metrics at the start of the next epoch\r\n  train_loss.reset_states()\r\n  train_accuracy.reset_states()\r\n  test_loss.reset_states()\r\n  test_accuracy.reset_states()\r\n  for images, labels in train_ds:\r\n    # tf.print(images)\r\n    train_step(images, labels)\r\n\r\n  for test_images, test_labels in test_ds:\r\n    test_step(test_images, test_labels)\r\n\r\n  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n  print(template.format(epoch+1,\r\n                        train_loss.result(),\r\n                        train_accuracy.result()*100,\r\n                        test_loss.result(),\r\n                        test_accuracy.result()*100))\r\n```\r\nkeras style:\r\n\r\n```python\r\nimport os, math, json\r\nimport numpy as np\r\nfrom matplotlib import pyplot as plt\r\nimport tensorflow as tf\r\n\r\n\r\nprint(\"Tensorflow version \" + tf.__version__)\r\n# tf.enable_eager_execution()\r\nAUTO = tf.data.experimental.AUTOTUNE\r\n\r\nHAS_COLAB_TPU = 'COLAB_TPU_ADDR' in os.environ\r\nassert not HAS_COLAB_TPU, \"Please select a GPU backend for this notebook. Pre-trained models in tf.keras.applications.* are not yet TPU-compatible\"\r\n\r\nGCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192-2/*.tfrec'\r\nIMAGE_SIZE = [192, 192]\r\n\r\nBATCH_SIZE = 64 # 128 works on GPU too but comes very close to the memory limit of the Colab GPU\r\nEPOCHS = 5\r\n\r\nVALIDATION_SPLIT = 0.19\r\nCLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # do not change, maps to the labels in the data (folder names)\r\n\r\n# splitting data files between training and validation\r\nfilenames = tf.io.gfile.glob(GCS_PATTERN)\r\nsplit = int(len(filenames) * VALIDATION_SPLIT)\r\ntraining_filenames = filenames[split:]\r\nvalidation_filenames = filenames[:split]\r\n\r\nprint(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\r\nvalidation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE\r\nsteps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE\r\nprint(\"With a batch size of {}, there will be {} batches per training epoch and {} batch(es) per validation run.\".format(BATCH_SIZE, steps_per_epoch, validation_steps))\r\n\r\ndef read_tfrecord(example):\r\n    features = {\r\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\r\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\r\n    }\r\n    example = tf.io.parse_single_example(serialized=example, features=features)\r\n    image = tf.image.decode_jpeg(example['image'], channels=3)\r\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\r\n    # image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size will be needed for TPU\r\n    image = tf.image.resize(image, (192, 192))\r\n    class_label = example['class']\r\n    return image, class_label\r\n\r\ndef load_dataset(filenames):\r\n  # read from TFRecords. For optimal performance, use \"interleave(tf.data.TFRecordDataset, ...)\"\r\n  # to read from multiple TFRecord files at once and set the option experimental_deterministic = False\r\n  # to allow order-altering optimizations.\r\n\r\n  option_no_order = tf.data.Options()\r\n  option_no_order.experimental_deterministic = False\r\n\r\n  dataset = tf.data.Dataset.from_tensor_slices(filenames)\r\n  dataset = dataset.with_options(option_no_order)\r\n  #dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=16)\r\n  dataset = dataset.interleave(tf.data.TFRecordDataset, cycle_length=16, num_parallel_calls=AUTO) # faster\r\n  dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\r\n  return dataset\r\n\r\n\r\ndef get_batched_dataset(filenames):\r\n  dataset = load_dataset(filenames)\r\n  # dataset = dataset.cache() # This dataset fits in RAM\r\n  dataset = dataset.repeat()\r\n  dataset = dataset.batch(BATCH_SIZE) # drop_remainder will be needed on TPU\r\n  dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\r\n  # should shuffle too but this dataset was well shuffled on disk already\r\n  # For proper ordering of map/batch/repeat/prefetch, see Dataset performance guide: https://www.tensorflow.org/guide/performance/datasets\r\n  return dataset\r\n\r\n# instantiate the datasets\r\ntrain_ds = get_batched_dataset(training_filenames)\r\ntest_ds = get_batched_dataset(validation_filenames)\r\n\r\n\r\n\r\npretrained_model = tf.keras.applications.MobileNetV2(input_shape=[*IMAGE_SIZE, 3], include_top=False, weights='imagenet')\r\npretrained_model.trainable = True\r\n\r\nmodel = tf.keras.Sequential([\r\n  pretrained_model,\r\n  tf.keras.layers.GlobalAveragePooling2D(),\r\n  tf.keras.layers.Dense(5, activation=\"softmax\")\r\n])\r\n\r\nmodel.compile(\r\n  optimizer=tf.keras.optimizers.SGD(lr=0.01),\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['sparse_categorical_accuracy']\r\n)\r\n\r\nmodel.summary()\r\n\r\nhistory = model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\r\n                    validation_data=test_ds, validation_steps=validation_steps)\r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["----------------------SOLVED----------------------- \r\nSolution\uff1aadd the argument \"training=True\" when training the keras.application.. For example:\r\n```python\r\nmodel = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,weights=\"imagenet\",include_top=False)\r\n\r\npred = model(inputs, training=True)\r\n```\r\nThe reason might be caused by \"batchnorm\" layer. Those model which has BN layers works well in keras training loop, \"model.fit()\", and nothing to takecare. However, they cannot learn anything by costume training loop if you forget to set training=True in model()"]}, {"number": 35635, "title": "Model to json", "body": "I added code to let users directly convert their model to json\r\n", "comments": ["@generationXcode Can you please fix the build failures? Thanks!", "Also, regarding the test errors: click on \"Details\" to the right and you will see a page with all the information about the failing tests. This PRs breaks all of them, please make sure you run at least pylint (ideally you should run all affected tests) before submitting the PR for review.\r\n\r\nI know you didn't run it because in the output you can see:\r\n\r\n```\r\n2. do_pylint: Python 3 pylint\r\n  FAIL\r\n```\r\n\r\n(and the real error some hundreds of lines above).\r\n\r\nPS: @rthadur is a human being just like me and you. [Please be respectful](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md)", "Ohh, sorry... I didn't mean it that way...   I was just wondering.\r\nI read the code of conduct.", "@generationzcode Any update on this PR, please. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "Hi, I'm so sorry... I'm kind of busy rn, so is it okay if I take it up in a few weeks?", "If no one else takes it, sure."]}, {"number": 35634, "title": "EarlyStopping should restore weights on end of training, not end of epoch", "body": "## System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin-19.2.0-x86_64-i386-64bit\r\n- TensorFlow installed from (source or binary): Pipenv\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n## Current behavior\r\n\r\nThe EarlyStopping callback will restore the best weights only if it itself stops training. For example, if a model is set to train for 40 epochs, uses early stopping with a patience of 5, and the best weights occur in epoch 37, then the best weights will not be restored, and the model will have the weights from epoch 40.\r\n\r\n## Expected behavior\r\n\r\nEarlyStopping should, when initialized with `restore_best_weights=True`, always restore the best weights when training stops, regardless of the reason why training stopped.\r\n\r\n## Code to reproduce the issue\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom numpy.random import RandomState\r\nfrom tensorflow.keras.callbacks import EarlyStopping\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.models import Sequential\r\n\r\nN_CLASSES, N_SAMPLES = 5, 500\r\n\r\nfor seed in [1, 2]:\r\n    print(f\"=== Random seed is {seed} ===\")\r\n\r\n    tf.random.set_seed(seed)\r\n    rng = RandomState(seed)\r\n\r\n    x_train = rng.standard_normal(size=(N_SAMPLES, 10))\r\n    x_test = rng.standard_normal(size=(N_SAMPLES, 10))\r\n    y_train = rng.random_integers(N_CLASSES, size=N_SAMPLES)\r\n    y_test = rng.random_integers(N_CLASSES, size=N_SAMPLES)\r\n\r\n    model = Sequential([Dense(32), Dense(N_CLASSES)])\r\n    model.compile(\"adam\", \"categorical_crossentropy\", [\"accuracy\"])\r\n    early_stopping = EarlyStopping(\r\n        \"val_accuracy\", patience=10, verbose=1, restore_best_weights=True\r\n    )\r\n    history = model.fit(\r\n        x_train,\r\n        y_train,\r\n        epochs=20,\r\n        callbacks=[early_stopping],\r\n        verbose=0,\r\n        validation_data=(x_test, y_test),\r\n    )\r\n    best_acc = max(history.history[\"val_accuracy\"])\r\n    _, eval_acc = model.evaluate(x_test, y_test, verbose=0)\r\n    print(f\"Best accuracy in training: {best_acc}. In evaluation: {eval_acc}\\n\")\r\n```\r\n\r\n### Output from example:\r\n\r\n```txt\r\n=== Random seed is 1 ===\r\nRestoring model weights from the end of the best epoch.\r\nEpoch 00011: early stopping\r\nBest accuracy in training: 0.19200000166893005. In evaluation: 0.19200000166893005\r\n\r\n=== Random seed is 2 ===\r\nBest accuracy in training: 0.15199999511241913. In evaluation: 0.14000000059604645\r\n\r\n```\r\n\r\n## Other info / logs\r\n\r\n- Numpy version: 1.18.1", "comments": ["See this issue in keras-team/keras for several people confirming the issue: https://github.com/keras-team/keras/issues/12511", "Was able to reproduce the issue. Please find the [Gist](https://colab.sandbox.google.com/gist/amahendrakar/3143abc3aeaa7962f38a97097464c5ed/35634.ipynb) here. Thanks!", "@Stigjb current behavior isn't clear at all, I agree with you. But it is not wrong. Just consider a situation: you train your model for 20 epochs, EarlyStopping callback isn't triggered and you want to continue the training process. If your *Expected behavior* was applied and the best weights were restored, then the training would continue from e.g. 15th epoch, but not from 20th as you would expect.\r\n\r\nSo I believe that it isn't the bug, but it definitely should be documented somewhere, e.g. in the description of `restore_best_weights` argument.", "I think the best solution to this is to also add a `restore_best_weights` option to `Model.fit` that you can have this behavior on early termination and regular termination", "> I think the best solution to this is to also add a `restore_best_weights` option to `Model.fit` that you can have this behavior on early termination and regular termination\r\n\r\nIn a similar fashion, we just hijacked the original callback and enforced the callback to replace the weights from the \"best\" epoch in the end of the training process.\r\n\r\n```python\r\nfrom tensorflow.keras.callbacks import EarlyStopping\r\n\r\nclass ReturnBestEarlyStopping(EarlyStopping):\r\n    def __init__(self, **kwargs):\r\n        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\r\n\r\n    def on_train_end(self, logs=None):\r\n        if self.stopped_epoch > 0:\r\n            if self.verbose > 0:\r\n                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\r\n        elif self.restore_best_weights:\r\n            if self.verbose > 0:\r\n                print('Restoring model weights from the end of the best epoch.')\r\n            self.model.set_weights(self.best_weights\r\n```", "> @Stigjb current behavior isn't clear at all, I agree with you. But it is not wrong. Just consider a situation...[discussion of continued training]...I believe that it isn't a bug, but it definitely should be documented somewhere...\r\n\r\nFailing to restore the weights is not the behavior of least surprise, which by some definitions makes it a bug no matter what the documentation says.  At best, it is ill-advised.\r\n\r\nI'm not fond of the idea of putting it into Model.fit either.  I think it does belong in EarlyStopping because that class already encapsulates the concepts of tracking something other than \"normal\" weights and metrics and making choices based on it.  \r\n\r\nIn my mind, the _only_ correct default behavior is to always restore the best weights when restore_best_weights is True.  If we want to make it possible to override this sensible default behavior for the sake of resuming training, we should add yet another flag to the EarlyStopping constructor.  Anything else leads to behavior most people won't expect.  Don't force people into documentation archeology, digging around until they discover a surprise.", "I'm wondering in \"on_epoch_end\" function should not \r\n`self.best_weights = self.model.get_weights()`\r\nbe like\r\n`self.best_weights = self.model.get_weights().copy()`\r\n?", "> I'm wondering in \"on_epoch_end\" function should not\r\n> `self.best_weights = self.model.get_weights()`\r\n> be like\r\n> `self.best_weights = self.model.get_weights().copy()`\r\n> ?\r\n\r\nYou could... But why? \ud83d\ude03 ", "> You could... But why? \ud83d\ude03\r\n\r\nI'm not sure how get_weights() function works. Imagine if it returns only a pointer to the weights, then what happens if model is trained more? It might be able to change the weights. No?", "> > You could... But why? \ud83d\ude03\r\n> \r\n> I'm not sure how get_weights() function works. Imagine if it returns only a pointer to the weights, then what happens if model is trained more? It might be able to change the weights. No?\r\n\r\nOk, I get it.  Maybe it's an extra precaution, but if this the case then the original, quite ill-adviced, implementation of `EarlyStopping` is also wrong / buggy: https://github.com/tensorflow/tensorflow/blob/e5cc10bcb5140ef8478a58c2ab260c5dcb2b1315/tensorflow/python/keras/callbacks.py#L1742\r\n\r\nI doubt that such a huge bug resides there for almost 3 years.....\r\n\r\nThis seems to be the actual implementation of `get_weights()`:   https://github.com/tensorflow/tensorflow/blob/e5cc10bcb5140ef8478a58c2ab260c5dcb2b1315/tensorflow/python/keras/engine/base_layer.py#L1853", "> I doubt that such a huge bug resides there for almost 3 years.....\r\n\r\nYou are right. Get_weight function does not need copy(). Interestingly I have reasons to believe that the \"restore best weights\" is not working for me, but I have already checked and it seems using get_weight().copy() is not the solution. \r\n\r\nDuring training, the running values of a loss function and another metric are getting saved in training logs. We use those values to stop training. We can also evaluate the model after training ends and get the best values coming from the best model. I expect the minimum value in logs matches the final restored model evaluation values, but it never happens in my case. \"model.evaluate\" function reports the last epochs training results, not the best model. I made a custom early_stopping function that stops training when loss value reaches a certain goal. Still, the \"model.evaluate\" output is different from the best values stored in the training logs, which is odd. Tensorflow is powerful but still has a lot of bugs in my limited experience.\r\n\r\nFor example, in the following plot, I was monitoring mean absolute percentage error (MAPE) to stop training and patience was 10000 epochs. At epoch 39294 training stoped and early stopping supposed to restore the best weights. Model.evaluate reports a MAPE of 103% while the best MAPE is ~85% and happening at epoch 29294.\r\n![image](https://user-images.githubusercontent.com/18602635/92831230-250cbf80-f3a4-11ea-978a-45a1106004d1.png)\r\n\r\nIn the following plot, I used my own custom early stopping callback that stops training immediately after MAPE becomes smaller than 90%. So there is no patience involved. Still, MAPE values I get from model.evaluate after training is different from the minimum MAPE I find in logs, and the loss in model accuracy is not negligible.\r\n![image](https://user-images.githubusercontent.com/18602635/92832066-2e4a5c00-f3a5-11ea-848e-668d7de06b8c.png)", "@Stigjb , Tried to reproduce your issue but got shape error, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/c0630d82e37522b119a7764cfb49be7d/35634.ipynb).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35634\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35634\">No</a>\n"]}, {"number": 35633, "title": "i want to do distributed model training in tensorflow.keras  with multiple ram and one main memory ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., centos7):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):source\r\nTensorFlow version (use command below):2.0.0\r\nPython version:3.6.3\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n", "@shan1322 \r\nCan you please go through the [link1 ](https://www.tensorflow.org/guide/distributed_training), [link2 ](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) and see if it helps you. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@shan1322 \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35631, "title": "ValueError: Expected scalar shape for softmax_cross_entropy_with_logits_sg/Reshape_2:0, saw shape: (?,).", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): pip install tensorflow-gpu==1.12.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: 2G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"D:/workspace/nlp_plat/nlp-platform/algorithm/text_classification/base/workflow_dl.py\", line 769, in <module>\r\n    _tet_fit()\r\n  File \"D:/workspace/nlp_plat/nlp-platform/algorithm/text_classification/base/workflow_dl.py\", line 726, in _tet_fit\r\n    wf_dl.fit()\r\n  File \"D:/workspace/nlp_plat/nlp-platform/algorithm/text_classification/base/workflow_dl.py\", line 183, in fit\r\n    self._fit()\r\n  File \"D:/workspace/nlp_plat/nlp-platform/algorithm/text_classification/base/workflow_dl.py\", line 271, in _fit\r\n    model = self.Graph(self.params)\r\n  File \"D:\\workspace\\nlp_plat\\nlp-platform\\algorithm\\text_classification\\text_cnn\\graph.py\", line 15, in __init__\r\n    super().__init__(params)\r\n  File \"D:\\workspace\\nlp_plat\\nlp-platform\\algorithm\\text_classification\\base\\graph_dl.py\", line 43, in __init__\r\n    self.build()\r\n  File \"D:\\workspace\\nlp_plat\\nlp-platform\\algorithm\\text_classification\\base\\graph_dl.py\", line 55, in build\r\n    self.build_optimize()\r\n  File \"D:\\workspace\\nlp_plat\\nlp-platform\\algorithm\\text_classification\\base\\graph_dl.py\", line 202, in build_optimize\r\n    self.optimize = tf.contrib.layers.optimize_loss(self.loss, global_step=self.global_step, learning_rate=self.learning_rate, optimizer=\"Adam\")\r\n  File \"D:\\code\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\optimizers.py\", line 155, in optimize_loss\r\n    contrib_framework.assert_scalar(loss)\r\n  File \"D:\\code\\anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\check_ops.py\", line 1264, in assert_scalar\r\n    % (tensor.name, shape))\r\nValueError: Expected scalar shape for softmax_cross_entropy_with_logits_sg/Reshape_2:0, saw shape: (?,).\r\n\r\n\r\n", "comments": ["self.loss must be scalar, shape is (1), you can use 'tf.reduce_sum' or 'tf.reduce_mean'", "@yongzhuo Hi, I also met this problem. I tried  'tf.reduce_mean', but it does not work", "@yongzhuo Do you have other advice? Thanks~"]}, {"number": 35630, "title": "OutOfRangeError Unknown Error when extracting particular zip file", "body": "Please be aware this issue was originally posted in tensorflow/datasets but I got directed here as it seems that the issue is related to the GFile implementation:\r\nhttps://github.com/tensorflow/datasets/issues/1337\r\n\r\n\r\n**Short description**\r\nWhen the download of imagenet_resized finished and tfds starts extracting/writing records, the program crashes.\r\n\r\nYou can reproduce this error by downloading the particular zip file manually and extracting it with tensorflow:\r\n\r\nhttp://www.image-net.org/image/downsample/Imagenet32_train_npz.zip\r\n\r\n**Environment information**\r\n* Operating System: Windows 10\r\n* Python version: 3.7\r\n* tensorflow-datasets version: 1.3.2\r\n* tensorflow-gpu version: 2.0.0\r\n\r\n**Reproduction instructions**\r\nWithout TFDS:\r\n```\r\nimport zipfile\r\nimport tensorflow.compat.v2 as tf\r\n\r\npath = 'path/to/file.zip'\r\nwith tf.io.gfile.GFile(path, 'rb') as fobj:\r\n  z = zipfile.ZipFile(fobj)\r\n  for member in z.infolist():\r\n    extract_file = z.open(member)\r\n    print(member.filename)\r\n```\r\n\r\n\r\nWith TFDS:\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\n\r\nimagenet_data, info = tfds.load(name=\"imagenet_resized/32x32\", with_info=True, as_supervised=True)\r\n```\r\n\r\n**Link to logs**\r\n```\r\nDl Size...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3414/3414 [22:47<00:00,  2.60 MiB/s]\r\n\r\n\r\n\r\n0 examples [00:00, ? examples/s]Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\contextlib.py\", line 130, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py\", line 199, in incomplete_dir\r\n    yield tmp_dir\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 333, in download_and_prepare\r\n    download_config=download_config)\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 1008, in _download_and_prepare\r\n    max_examples_per_split=download_config.max_examples_per_split,\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 871, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 1033, in _prepare_split\r\n    total=split_info.num_examples, leave=False):\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tqdm\\_tqdm.py\", line 1005, in __iter__\r\n    for obj in iterable:\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\image\\imagenet_resized.py\", line 141, in _generate_examples\r\n    for fname, fobj in archive:\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\core\\download\\extractor.py\", line 179, in iter_zip\r\n    z = zipfile.ZipFile(fobj)\r\n  File \"C:\\Program Files\\Python37\\lib\\zipfile.py\", line 1225, in __init__\r\n    self._RealGetContents()\r\n  File \"C:\\Program Files\\Python37\\lib\\zipfile.py\", line 1288, in _RealGetContents\r\n    endrec = _EndRecData(fp)\r\n  File \"C:\\Program Files\\Python37\\lib\\zipfile.py\", line 259, in _EndRecData\r\n    fpin.seek(0, 2)\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\", line 167, in seek\r\n    offset += self.size()\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\", line 102, in size\r\n    return stat(self.__name).length\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\", line 727, in stat\r\n    return stat_v2(filename)\r\n  File \"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\", line 744, in stat_v2\r\n    pywrap_tensorflow.Stat(compat.as_bytes(path), file_statistics)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: C:\\Users\\[username]\\tensorflow_datasets\\downloads\\image-net.org_image_downs_Image_train_npzlCJjN-zBsDCdn80BZxJ6qtyTFYcDX7y1OSUjXtuuxPw.zip; Unknown error\r\n\r\nProcess finished with exit code 1\r\n```\r\n**Expected behavior**\r\nNo error", "comments": ["Per instructions from @Conchylicultor I redownloaded the file after which I got the same error. I also tried their minimal example which results in the same error:\r\n\r\n```\r\npath = 'C:\\Users\\[username]\\tensorflow_datasets\\downloads\\image-net.org_image_downs_Image_train_npzlCJjN-zBsDCdn80BZxJ6qtyTFYcDX7y1OSUjXtuuxPw.zip'\r\nwith tf.io.gfile.GFile(path, 'rb') as fobj:\r\n  z = zipfile.ZipFile(fobj)\r\n  for member in z.infolist():\r\n    extract_file = z.open(member)\r\n    print(member.filename)\r\n```", "Confirmed not be an issue on Ubuntu 18.04", "@BrEr thanks for the update. It seems that this is not an issue with tfds.\r\nCould you link the url of the zip file that make `tf.io.gfile` crash ?", "@BrEr ,\r\nCould you try instruction from @Conchylicultor  usage of tf.io.gfile ?thanks!", "> @BrEr ,\r\n> Could you try instruction from @Conchylicultor usage of tf.io.gfile ?thanks!\r\n\r\nI did try GFile:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/35630#issuecomment-571468035", "@Conchylicultor the url, as extracted from source is:\r\n\r\nhttp://www.image-net.org/image/downsample/Imagenet32_train_npz.zip\r\n\r\nfound in:\r\n\r\nhttps://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/image/imagenet_resized.py", "Any updates on this? I still did not figure this issue out.", "@BrEr The reason for the error is that The `txt` file was missing from the `setup.py`. Its been fixed in tfds-nightly. Please check it and confirm it. \r\n\r\nFor more information, please take a look at the following [issue](https://github.com/tensorflow/datasets/issues/1304). Thanks!", "The issue described in the linked issue is different from what I am experiencing. It also does not fix the issue. 8x8 is working.", "@gowthamkpr \r\nFrom what I understand of the issue, `tf.io.gfile` fails to extract the following file on windows. This seems unrelated to TFDS. I do not have access to windows so cannot reproduce the issue.\r\n\r\n@BrEr You should update the issue description with better title/description. The current one makes it look like the issue is related to TFDS which it is not, so it's not surprising that TF team do not look at it.\r\n\r\nUpdate your comment with the exact minimal reproductions instructions (where to download the file,...), informations on which TF version, OS, full stacktrace (pasted as gist),...\r\nDo not mention TFDS if the bug can be reproduced with TF only. Try to keep instructions as minimal as possible. Something like:\r\n\r\nProblem encounter:\r\n...\r\n\r\nSystem information:\r\n...\r\n\r\nReproduction instruction:\r\n\r\nDownload: http://www.image-net.org/image/downsample/Imagenet32_train_npz.zip\r\n```\r\nimport zipfile\r\nimport tensorflow.compat.v2 as tf\r\n\r\npath = 'path/to/file.zip'\r\nwith tf.io.gfile.GFile(path, 'rb') as fobj:\r\n  z = zipfile.ZipFile(fobj)\r\n  for member in z.infolist():\r\n    extract_file = z.open(member)\r\n    print(member.filename)\r\n```\r\n\r\n", "I filed this bug in Tensorflow because you asked me to in the original issue in TFDS .  https://github.com/tensorflow/datasets/issues/1337\r\n\r\nI don't know if the issue is not related to TFDS, for me it only occurs with one specific dataset (Imagenet resized 32x32 and 64x64) using TFDS. I haven't been able to reproduce the issue in any other way.\r\n\r\nI have updated the instruction a bit. \r\n", "@BrEr Thanks for updating the description. Please also update the description to add the stracktrace that you get when running without TFDS.\r\n\r\n> I don't know if the issue is not related to TFDS, for me it only occurs with one specific dataset (Imagenet resized 32x32 and 64x64) using TFDS. I haven't been able to reproduce the issue in any other way.\r\n\r\nI though you were able to reproduce the issue by running the code snippet that you added. Could you confirm and add the full stacktrace that you're getting when you running the code ? \r\nIf so, the code snippet do not depends on TFDS, so no change in TFDS will be able to solve this particular error.\r\n", "@BrEr Can you please reply to @Conchylicultor comment above. Thanks!", "Changed the file extractor.py in line 131\r\n\r\n```\r\n    with tf.io.gfile.GFile(path_or_fobj, 'rb') as f_obj:\r\n```\r\ninto \r\n```\r\n    with open(path_or_fobj, 'rb') as f_obj:\r\n```\r\nwhich becomes\r\n```\r\n@contextlib.contextmanager\r\ndef _open_or_pass(path_or_fobj):\r\n  if isinstance(path_or_fobj, six.string_types):\r\n    with open(path_or_fobj, 'rb') as f_obj:\r\n      yield f_obj\r\n  else:\r\n    yield path_or_fobj\r\n```\r\nand everything works by using \r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\n\r\ntfds.load('imagenet_resized/32x32')\r\n```", "@Xinihiko, thank you for the update. This seems to confirm this is a `tf.io.gfile` bug.", "Chiming in because I am receiving this same `OutOfRangeError` on Windows 10 with:\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\ncoco_data = tfds.load('coco/2017') \r\n```\r\n\r\nThe offending file is `tensorflow_datasets\\downloads\\images.cocodataset.org_zips_train2017aai7WOpfj5nSSHXyFBbeLp3tMXjpA_H3YD4oO54G2Sk.zip`.\r\n\r\nI can provide the full traceback if you'd like, but it's the same as the one above and I'd rather not spam the issue.", "The above mentioned URL is not working, can you please update the URL.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35630\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35630\">No</a>\n"]}, {"number": 35629, "title": "TPU support in TF2.1 release candidate 2", "body": "I'm trying to run prototype of a code for training a model on TPU, with bfloat16 precision. I'm doing it in google colab notebook.\r\n\r\nTo do it I install tensorflow-2.1.0rc2 and run the following code:\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\n\r\ndef create_model():\r\n    model = tf.keras.models.Sequential()\r\n\r\n    model.add(tf.keras.layers.Conv2D(128, (3, 3), input_shape=[32,32,3]))\r\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n    model.add(tf.keras.layers.Activation('elu'))\r\n\r\n    model.add(tf.keras.layers.Flatten())\r\n    model.add(tf.keras.layers.Dense(10))\r\n    model.add(tf.keras.layers.Activation('softmax', dtype='float32'))\r\n\r\n    return model\r\n\r\n#this is for bfloat16 precision\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.config.experimental_connect_to_host(resolver.master())\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n    model = create_model()\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy])\r\n```\r\n\r\nAfter running this code I receive an error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n\r\n<ipython-input-4-4b29fabfb5e2> in <module>()\r\n     18 resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n     19 tf.config.experimental_connect_to_host(resolver.master())\r\n---> 20 tf.tpu.experimental.initialize_tpu_system(resolver)\r\n     21 strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n     22 \r\n\r\n3 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: '__inference__tpu_init_fn_14' is neither a type of a primitive operation nor a name of a function registered in binary running on n-88be52b9-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n```\r\n\r\n\r\nHere is notebook with code:\r\nhttps://colab.research.google.com/drive/1SuCZ7AsoT7SZSQBgxQUf_L41ibYpFT0a\r\n\r\nThis code worked on TF2.0, but ONLY without bfloat16 support(see block with comment in code). What can i do, if I need a TPU on TF2 with bfloat16 support?", "comments": ["Was able to reproduce the issue. Please find the [Gist](https://colab.sandbox.google.com/gist/amahendrakar/328df74ea466f392b6e33e38ca19c1a5/35629.ipynb) here. Thanks!", "I have found, that before using TF2 in Colab I need to run in notebook a sort of marco(?):\r\n`%tensorflow_version 2.x`\r\nAfter that I need to install a latest release of TF2.1 and run a model code I presented before.\r\nWith this macro the TPU is going to work fine.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35629\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35629\">No</a>\n"]}, {"number": 35628, "title": "How many items does tf.gradients return?", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/gradients?version=stable\r\n\r\n## Description of issue (what needs changing):\r\nIt's unclear how many list items are returned from `tf.gradients`.\r\n\r\nThe second paragraph states that \"It returns a list of Tensor of length `len(xs)` where each tensor is the `sum(dy/dx)` for y in `ys`.\" The \"Returns\" section says, \"A list of `sum(dy/dx)` for each x in `xs`.\"\r\n\r\nSo... which one is it? `sum(dy/dx)` for x in `xs` or `sum(dy/dx)` for y in `ys`? Besides the inconsistency, the summation notation in this documentation is ambiguous. When it says \"`sum(dy/dx)` for x in `xs`\" does that mean `dy/dx` is summed over the `ys` axis and there is one element produced for each `xs` or the other way around?\r\n\r\nA clarifying example would help and a statement along the lines of \"returns a list of <whatever> with as many elements as `xs`\" (or `ys` -- I don't know).\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@sharvil Are you planning to submit PR to update the docs? Thanks!", "@jvishnuvardhan, no I'm not planning to submit a PR.", "@sharvil , @jvishnuvardhan and @MarkDaoust. Please review the above PR of mine. I have solved the problem with the doc.", "Issue resolved by PR #36206 and @sharvil and @alextp has approved it. Please close this issue if everything is resolved, @MarkDaoust , @jvishnuvardhan and @sharvil.", "@sharvil,\r\nClosing this issue based on [your comment](https://github.com/tensorflow/tensorflow/pull/36206#issuecomment-579969531) in PR #36206.\r\n\r\nPlease feel free to re-open if mistaken. Thanks!"]}, {"number": 35627, "title": "Tensorflow thirdparty libraries are clashing with other same libraries in the project", "body": "**System information**\r\nLinux\r\ntensorflow-2.0.0 \r\nBazel 0.26.1\r\nBuilt using virtual environment, python 3\r\n\r\n**Describe the problem**\r\nBuilt **libtensorflow_cc.so & libtensorflow_framework.so** using bazel 0.26.1.\r\nAnd when linked our existing c++ binary to these libraries, libcurl or opencv calls are clashing with the symbols in tensorflow libraries.\r\n\r\nHow exactly I can built & include these tenforflow libraries in a larger project without exporting third party symbols. \r\n\r\nIs --config=monolithic the right way ? \r\n", "comments": ["This thread may help #1924 #14267 ", "Thanks for the reply @ymodak. I tried --monolithic option and it resolved the symbols conflicts issue.\r\n\r\nBut it seems to be breaking  TensorRT integration ?? I get these error while running my code ::\r\n\r\nNot found: No attr named 'BFC' in NodeDef:\r\n\u00a0\u00a0\u00a0\u00a0 [[{{node gaussian_blurring/image_input}}]]\r\n\u00a0\u00a0\u00a0\u00a0 [[gaussian_blurring/image_input]]Aborted (core dumped)\r\n\r\nOr could the error be because of something else I missed. I'm linking against -ltensorflow_cc and -lprotobuf only.", "What version of cuda/cudnn and tensorrt are you using?\r\n", "--monolithic option helped resolving symbols clash issue. I have resolved the other issues by removing gpu calls from our binary.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35627\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35627\">No</a>\n"]}, {"number": 35626, "title": "TFLite 2.0.0 compilation errors in block_map.cc", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Debian buster\r\n  uname -a output: Linux freewave-ib 4.14.79-g3438de3474 #22 PREEMPT Mon Nov 4 20:16:15 UTC 2019 armv7l GNU/Linux\r\n- Mobile device: N/A\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: none of the above\r\n- Bazel version (if compiling from source): Don't know\r\n- GCC/Compiler version (if compiling from source): g++ (Debian 8.3.0-6) 8.3.0, \r\n  GNU Make 4.2.1\r\n  Built for arm-unknown-linux-gnueabihf\r\n- CUDA/cuDNN version: Don't know\r\n- GPU model and memory: none\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen building the Pip package for TFLite, I ran across two bugs in the file `tensorflow/lite/experimental/ruy/block_map.cc` that caused the compilation to exit with an error. \r\n\r\n- The first bug was a missing header file,` profiling/instrumentation.h`.\r\n- The second bug was that the object `gemmlowp` was not declared before it was used.\r\n- There may be additional bugs as well. I stopped looking after I found these two, since I don't know \r\n  the code well enough to know what to look for.\r\n\r\n(Peripheral to this issue, the two relevant scripts needed minor edits before they would work. I've included this information here for completeness, but the C++ source file is the larger, more important issue.\r\n1. I edited the curl commands in `download_dependencies.sh`. The curl commands in lines 63 and 65 were edited to `curl -Lsk`, and the star-zip in line 65 was wrapped in double quotes, `\"*zip\"` .\r\n2. I inserted the line `TENSORFLOW_TARGET=rpi` in `build_pip_package.sh`, just before line 42, to ensure that it compiled for an armv7l.\r\n)\r\n\r\n**Commands used to replicate the issue**\r\n\r\n```\r\ngit clone -b master --depth=1 http://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\n./tensorflow/lite/tools/make/download_dependencies.sh\r\n./tensorflow/lite/tools/pip_package/build_pip_package.sh\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nI'm only including the relevant part of stdout/stderr. I can include the entire output upon request.\r\n\r\n1. The missing header file: output from build_pip_package.sh\r\n```\r\n[...]\r\npython3 setup.py bdist --plat-name=linux-armv7l bdist_wheel --plat-name=linux-armv7l\r\nrunning bdist\r\nrunning bdist_dumb\r\nrunning build\r\nrunning build_py\r\nrunning build_ext\r\nmake: Entering directory '/ptp/tensorflow'\r\ng++ -O3 -DNDEBUG -fPIC  --std=c++11 -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/ptp/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/ptp/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/ -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/ruy/block_map.cc -o /ptp/tensorflow/tensorflow/lite/tools/make/gen/linux_armv7l/obj/tensorflow/lite/experimental/ruy/block_map.o\r\ntensorflow/lite/experimental/ruy/block_map.cc:27:10: fatal error: profiling/instrumentation.h: No such file or directory\r\n #include \"profiling/instrumentation.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nmake: *** [tensorflow/lite/tools/make/Makefile:254: /ptp/tensorflow/tensorflow/lite/tools/make/gen/linux_armv7l/obj/tensorflow/lite/experimental/ruy/block_map.o] Error 1\r\nmake: Leaving directory '/ptp/tensorflow'\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 188, in <module>\r\n    'build_py': CustomBuildPy,\r\n  File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 145, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/lib/python3.7/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib/python3.7/distutils/dist.py\", line 966, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/lib/python3.7/distutils/command/bdist.py\", line 143, in run\r\n    self.run_command(cmd_name)\r\n  File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/lib/python3.7/distutils/command/bdist_dumb.py\", line 81, in run\r\n    self.run_command('build')\r\n  File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/lib/python3.7/distutils/command/build.py\", line 135, in run\r\n    self.run_command(cmd_name)\r\n  File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 123, in run\r\n    self.run_command('build_ext')\r\n  File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 115, in run\r\n    make()\r\n  File \"setup.py\", line 95, in make\r\n    subprocess.check_call(make_args(quiet=False))\r\n  File \"/usr/lib/python3.7/subprocess.py\", line 347, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['make', 'SHELL=/bin/bash', 'BUILD_WITH_NNAPI=false', '-C', '/ptp/tensorflow/tensorflow/lite/tools/pip_package/../../../..', '-f', 'tensorflow/lite/tools/make/Makefile', '-j', '1']' returned non-zero exit status 2.\r\n```\r\n\r\n2. The undeclared object: output from build_pip_package.sh\r\n```\r\n[...]\r\n+ python3 setup.py bdist --plat-name=linux-armv7l bdist_wheel --plat-name=linux-armv7l\r\nrunning bdist\r\nrunning bdist_dumb\r\nrunning build\r\nrunning build_py\r\nrunning build_ext\r\nmake: Entering directory '/ptp/tensorflow'\r\ng++ -O3 -DNDEBUG -fPIC  --std=c++11 -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/ptp/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/ptp/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/ -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/ruy/block_map.cc -o /ptp/tensorflow/tensorflow/lite/tools/make/gen/linux_armv7l/obj/tensorflow/lite/experimental/ruy/block_map.o\r\ntensorflow/lite/experimental/ruy/block_map.cc: In function \u2018void ruy::GetBlockByIndex(const ruy::BlockMap&, int, ruy::SidePair<int>*)\u2019:\r\ntensorflow/lite/experimental/ruy/block_map.cc:36:3: error: \u2018gemmlowp\u2019 has not been declared\r\n   gemmlowp::ScopedProfilingLabel label(\"GetBlockByIndex\");\r\n   ^~~~~~~~\r\ntensorflow/lite/experimental/ruy/block_map.cc: In function \u2018void ruy::MakeBlockMap(int, int, int, int, int, int, int, int, ruy::Path, int, ruy::BlockMap*)\u2019:\r\ntensorflow/lite/experimental/ruy/block_map.cc:273:3: error: \u2018gemmlowp\u2019 has not been declared\r\n   gemmlowp::ScopedProfilingLabel label(\"MakeBlockMap\");\r\n   ^~~~~~~~\r\ntensorflow/lite/experimental/ruy/block_map.cc: In function \u2018void ruy::GetBlockMatrixCoords(ruy::Side, const ruy::BlockMap&, int, int*, int*)\u2019:\r\ntensorflow/lite/experimental/ruy/block_map.cc:412:3: error: \u2018gemmlowp\u2019 has not been declared\r\n   gemmlowp::ScopedProfilingLabel label(\"GetBlockMatrixCoords\");\r\n   ^~~~~~~~\r\nmake: *** [tensorflow/lite/tools/make/Makefile:254: /ptp/tensorflow/tensorflow/lite/tools/make/gen/linux_armv7l/obj/tensorflow/lite/experimental/ruy/block_map.o] Error 1\r\nmake: Leaving directory '/ptp/tensorflow'\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 188, in <module>\r\n    'build_py': CustomBuildPy,\r\n  File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 145, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/lib/python3.7/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib/python3.7/distutils/dist.py\", line 966, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/lib/python3.7/distutils/command/bdist.py\", line 143, in run\r\n    self.run_command(cmd_name)\r\n  File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/lib/python3.7/distutils/command/bdist_dumb.py\", line 81, in run\r\n    self.run_command('build')\r\n  File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/lib/python3.7/distutils/command/build.py\", line 135, in run\r\n    self.run_command(cmd_name)\r\n  File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 123, in run\r\n    self.run_command('build_ext')\r\n  File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 115, in run\r\n    make()\r\n  File \"setup.py\", line 95, in make\r\n    subprocess.check_call(make_args(quiet=False))\r\n  File \"/usr/lib/python3.7/subprocess.py\", line 347, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['make', 'SHELL=/bin/bash', 'BUILD_WITH_NNAPI=false', '-C', '/ptp/tensorflow/tensorflow/lite/tools/pip_package/../../../..', '-f', 'tensorflow/lite/tools/make/Makefile', '-j', '1']' returned non-zero exit status 2.\r\n\r\n```\r\n", "comments": ["@rdepew could you try it with Docker build?\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package\r\n\r\n`make BASE_IMAGE=debian:buster PYTHON=python3 TENSORFLOW_TARGET=rpi docker-build`", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@rdepew \r\n\r\nAny update on this issue please. Thanks!", "Let's close this. Please reopen this if you still have the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35626\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35626\">No</a>\n"]}, {"number": 35625, "title": "RuntimeError: Invalid quantization params for op RESHAPE at index 36 in subgraph 0", "body": "I download source code of SSD from github.com. I want to convert the model to run in edgetpu.\r\nWhen I convert saved model to tflite, then run 'edgetpu_compiler XXX.tflite' for running in tpu. But it tell me that not quantization, I change parameter for quantization.Such as \r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8], \r\nbut occur error: RuntimeError: Invalid quantization params for op RESHAPE at index 36 in subgraph 0\r\n\r\n**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source):1.15\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\nimport numpy as np\r\n\r\nckpt_filename = '/home/lanjunc/python_project/SSD-Tensorflow/convert/model'\r\n\r\nimg = Image.open('./dog.jpg')\r\nimg = img.resize((300, 300))\r\ninput_data = np.array(img, dtype=np.float32)\r\ndef representative_dataset_gen():\r\n    for i in range(1):\r\n        yield [input_data]\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(ckpt_filename,\r\n                                                     # tag_set=['test_saved_model1'], signature_key='signature_test1',\r\n                                                     input_shapes={\"Placeholder\":[300,300,3]})\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]  # DEFAULT, OPTIMIZE_FOR_SIZE, OPTIMIZE_FOR_LATENCY\r\n\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n# converter.target_spec.supported_ops = [\r\n#                                        tf.lite.OpsSet.SELECT_TF_OPS]\r\n#\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = representative_dataset_gen\r\n\r\ntflite_model = converter.convert()\r\n\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nRuntimeError: Invalid quantization params for op RESHAPE at index 36 in subgraph 0\r\n```\r\n\r\n", "comments": ["I change my code\r\n\r\nimg1 = Image.open('./dog.jpg')\r\n    img1 = img1.resize((300, 300))\r\n    input_data = np.array(img1, dtype=np.float32)\r\n\r\n\r\n    def representative_dataset_gen():\r\n        for i in range(1):\r\n            yield [input_data]\r\n\r\n    # tf.contrib.quantize.create_eval_graph(isess.graph)\r\n    toco_converter = tf.lite.TFLiteConverter.from_session(isess, input_tensors=[image_pre], output_tensors=localisations)\r\n    toco_converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    toco_converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n    toco_converter.post_training_quantize = False\r\n    toco_converter.inference_type = tf.uint8\r\n    toco_converter.inference_input_type = tf.uint8\r\n    toco_converter.quantized_input_stats = {'ssd_preprocessing_train/resize_image/Reshape': (0., 255.)}\r\n    toco_converter.representative_dataset = representative_dataset_gen\r\n    model_tflite_binary = toco_converter.convert()\r\n    open(\"converted_model.tflite\", \"wb\").write(model_tflite_binary)\r\n\r\n-------------------------------------------\r\nbut occur the error:\r\n\r\nArray ssd_300_vgg/conv1/conv1_1/Relu, which is an input to the Conv operator producing the output array ssd_300_vgg/conv1/conv1_2/Relu, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.", "I saw it:\r\nhttps://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba\r\nIt says that To execute entirely on specialized hardware that does not support floating point operations at all (for example, some machine learning accelerators, including the Edge TPU), you can specify a flag in order to output only integer operations:\r\n\r\nWhen this flag is used and an operation has no integer quantizable counterpart, the TensorFlow Lite Converter will throw an error.\r\nI want to run the model in edge tpu.I know the tpu only recognizes quantion model.If the model has the ops which can't be quantized, the model can't run in the tpu. Yes?", "I compiler source code , and I try\r\n\r\nbazel run --config=opt tensorflow/lite/toco:toco -- --input_file=/home/lanjunc/python_project/SSD-Tensorflow/convert/pb/ssd_frozen_model.pb --output_file=/home/lanjunc/python_project/SSD-Tensorflow/convert/ssd_tpu_compiler.tflite --input_shapes=300,300,3 --input_arrays=Placeholder --output_arrays='ssd_300_vgg/softmax/Reshape_1,ssd_300_vgg/softmax_1/Reshape_1,ssd_300_vgg/softmax_2/Reshape_1,ssd_300_vgg/softmax_3/Reshape_1,ssd_300_vgg/softmax_4/Reshape_1,ssd_300_vgg/softmax_5/Reshape_1,ssd_300_vgg/block4_box/Reshape,ssd_300_vgg/block7_box/Reshape,ssd_300_vgg/block8_box/Reshape,ssd_300_vgg/block9_box/Reshape,ssd_300_vgg/block10_box/Reshape,ssd_300_vgg/block11_box/Reshape,ssd_preprocessing_train/strided_slice' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --default_ranges_min=0 --default_ranges_max=255\r\n\r\noccur error:\r\n\r\n2020-01-07 20:31:16.292889: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 254 operators, 369 arrays (0 quantized)\r\n2020-01-07 20:31:16.295754: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 254 operators, 369 arrays (0 quantized)\r\n2020-01-07 20:31:16.364710: I tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc:202] Replaced sub-network with Dilated Conv2D op outputting \"ssd_300_vgg/conv6/Conv2D\".\r\n2020-01-07 20:31:16.437956: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 78 operators, 176 arrays (0 quantized)\r\n2020-01-07 20:31:16.439118: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 73 operators, 170 arrays (0 quantized)\r\n2020-01-07 20:31:16.439996: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 72 operators, 168 arrays (0 quantized)\r\n2020-01-07 20:31:16.440905: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 72 operators, 168 arrays (0 quantized)\r\n2020-01-07 20:31:16.441421: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 72 operators, 168 arrays (0 quantized)\r\n2020-01-07 20:31:16.441973: W tensorflow/lite/toco/tooling_util.cc:1288] Fixing constant output array ssd_preprocessing_train/strided_slice by inserting a copy. This is not optimal.\r\n2020-01-07 20:31:16.442080: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 73 operators, 170 arrays (0 quantized)\r\n2020-01-07 20:31:16.442621: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 73 operators, 170 arrays (0 quantized)\r\n2020-01-07 20:31:16.443268: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 73 operators, 170 arrays (0 quantized)\r\nUnimplemented: this graph contains an operator of type Cast for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).", "@conglanjun Can you please share a standalone code to reproduce your issue? Thanks!", "I saw \r\nhttps://github.com/tensorflow/tensorflow/issues/20955\r\n\r\n```\r\nsuharshs commented on Aug 18, 2018\r\n\r\nRegarding cast, why does the model have a Cast operation?\r\nMake sure you are converting a eval graph and not a train graph which can sometimes have spurious unsupported operations.\r\n\r\nAre you trying to quantize the model using the tool here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize\r\n\r\nyou should train with that which should place quantization ops in the graph to collect info. That second error you get means that the graph is not getting the correct quantization operations in the graph.\r\n\r\nHow are you making your fake quantized frozen graph to provide to tflite conversion?\r\n```\r\nbut https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize 404 not found\r\ntf 2.0 remove the pah?\r\n\r\n```\r\nraninbowlalala commented on Aug 20, 2018\r\n@suharshs\r\nFor cast op, I want to use dummy quantization for deeplabv3+ (mobilenetv2) model named \"mobilenetv2_coco_voc_trainaug\" which download from https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md. I use tensorboard to see the graph and there are \"cast\" op.\r\n\r\nFor training a quantization model. I am sorry I forgot to add \"tf.contrib.quantize.create_eval_graph()\", convert model successfully after I add this command. Thank you very much!\r\n```\r\nI should add \"tf.contrib.quantize.create_eval_graph()\"?\r\n", "I saw\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/contrib/quantize/README.md\r\nI should train my model, using \"Quantization-aware training\".\r\nAnd make pb file using create_eval_graph().\r\n\r\nAnd I use the SSD code is \r\nhttps://github.com/balancap/SSD-Tensorflow\r\nI want to use this model to run in EdgeTpu.\r\n", "I update new message:\r\nI use tf.contrib.quantize.create_training_graph() in training, save to ckpt\r\nI load ckpt, then use tf.contrib.quantize.create_eval_graph(), save to saved model.\r\n\r\nI use the saved model to convert to tflite. Code is:\r\n```\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\nimport numpy as np\r\n\r\nckpt_filename = '/home/lanjunc/python_project/SSD-Tensorflow/convert/model'\r\n# ckpt_filename = '../notebooks/ssd_300_vgg_test'\r\nimg = Image.open('./dog.jpg')\r\nimg = img.resize((300, 300))\r\ninput_data = np.array(img, dtype=np.float32)\r\ndef representative_dataset_gen():\r\n    for i in range(1):\r\n        yield [np.random.randn(300, 300, 3)]\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(ckpt_filename, input_shapes={\"input\":[300,300,3]})\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]  # DEFAULT, OPTIMIZE_FOR_SIZE, OPTIMIZE_FOR_LATENCY\r\n\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = representative_dataset_gen\r\n\r\ntflite_model = converter.convert()\r\n\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n`2020-01-08 17:05:57.434156: F tensorflow/lite/toco/import_tensorflow.cc:2471] Check failed: status.ok() Neither input_content (0) nor float_val (294911) have the right dimensions (294912) for this float tensor\r\n\t (while processing node 'ssd_300_vgg/conv3/conv3_1/weights_quant/FakeQuantWithMinMaxVars')\r\nAborted (core dumped)`\r\n\r\nI think input shape is error?\r\nQuantization result input is \"dimensions (294912)\"?\r\nBut the input shape is [300,300,3]. never changed!", "I saw the source code https://git.codingcafe.org/Mirrors/tensorflow/tensorflow/blob/f4cb5978667ccf6396e4a779e3a482766959e5dd/tensorflow/contrib/lite/toco/import_tensorflow.cc\r\n```\r\n    return Status(\r\n        false,\r\n        absl::StrCat(\"Neither input_content (\",\r\n                     input_tensor.tensor_content().size() / sizeof(float),\r\n                     \") nor float_val (\", input_tensor.float_val_size(),\r\n                     \") have the right dimensions (\", input_flat_size,\r\n                     \") for this float tensor\"));\r\n\r\n```\r\ninput_flat_size != input_tensor.float_val_size()\r\nhow can I control the size?", "@conglanjun Can you please share a standalone code to reproduce your issue? Thanks!", "https://drive.google.com/open?id=18IoP7I_TcixoJtje2vDsbFtpOYDTl-Fv\r\nyou can download it.", "@jvishnuvardhan \r\nI tell you the code.\r\nTraining the model, use the script, SSD-Tensorflow/train_ssd_network.py\r\nrun\r\n```\r\nDATASET_DIR=./tfrecords\r\nTRAIN_DIR=./logs/\r\nCHECKPOINT_PATH=./checkpoints/ssd_300_vgg.ckpt\r\npython train_ssd_network.py \\\r\n    --train_dir=${TRAIN_DIR} \\\r\n    --dataset_dir=${DATASET_DIR} \\\r\n    --dataset_name=pascalvoc_2007 \\\r\n    --dataset_split_name=train \\\r\n    --model_name=ssd_300_vgg \\\r\n    --checkpoint_path=${CHECKPOINT_PATH} \\\r\n    --save_summaries_secs=60 \\\r\n    --save_interval_secs=600 \\\r\n    --weight_decay=0.0005 \\\r\n    --optimizer=adam \\\r\n    --learning_rate=0.001 \\\r\n    --batch_size=32 --max_number_of_steps=1\r\n\r\n```\r\nI add --max_number_of_steps=1, I think run 1 step can update all weights, and all operation can be quantized.\r\nI add the code\r\n`\r\ng = tf.get_default_graph()\r\ntf.contrib.quantize.create_training_graph(input_graph=g, quant_delay=200)\r\n`\r\nin SSD-Tensorflow/train_ssd_network.py.\r\nIt will product model such as 'SSD-Tensorflow/logs/model.ckpt-5'\r\nThen I run SSD-Tensorflow/notebooks/ssd_notebookv_eval.py\r\nI have add 'tf.contrib.quantize.create_eval_graph()' in it.\r\nI'll create saved model in SSD-Tensorflow/notebooks/model.\r\nThen I want to create tflite.\r\nRun 'SSD-Tensorflow/convert/tf_lite_convert.py'\r\noccur the error:\r\nRuntimeError: Invalid quantization params for op RESHAPE at index 80 in subgraph 0.\r\nI download tensorflow source code, want to chang tensorflow code for fixing the error.\r\nI change the file:\r\n'tensorflow/tensorflow/tensorflow/lite/tools/optimize/quantize_model.cc'\r\n```\r\n} else if (tensor_property.restriction) {\r\n    const auto scale_and_zp = tensor_property.restricted_value;\r\n    // Apply to output.\r\n    output_tensor->quantization = absl::make_unique<QuantizationParametersT>();\r\n    output_tensor->quantization->scale.push_back(scale_and_zp.first);\r\n    output_tensor->quantization->zero_point.push_back(scale_and_zp.second);\r\n    output_tensor->type = TensorType_INT8;\r\n}\r\n```\r\nas\r\n```\r\n} else if (tensor_property.restriction) {\r\n    const auto scale_and_zp = tensor_property.restricted_value;\r\n    // Apply to output.\r\n    output_tensor->quantization = absl::make_unique<QuantizationParametersT>();\r\n    output_tensor->quantization->scale.push_back(scale_and_zp.first);\r\n    output_tensor->quantization->zero_point.push_back(scale_and_zp.second);\r\n    output_tensor->type = TensorType_INT8;\r\n    // patch: read min/max from the input tensor\r\n    const int input_tensor_idx = op->inputs[property.inputs[0].first];\r\n    TensorT* input_tensor = subgraph->tensors[input_tensor_idx].get();\r\n    output_tensor->quantization->min.push_back(input_tensor->quantization->min[0]);\r\n    output_tensor->quantization->max.push_back(input_tensor->quantization->max[0]);\r\n}\r\n```\r\nWhy can come in the case:'else if (tensor_property.restriction) {'?\r\nI add max and min in it.\r\nBut other error:\r\n```\r\nerror_reporter->Report(\r\n          \"Invalid quantization params for op %s at index %d \"\r\n          \"in subgraph %d, scale.size:%d, zero_point.size:%d, min.size:%d, max.size:%d\",\r\n          EnumNameBuiltinOperator(op_code), op_idx, subgraph_idx, input_tensor->quantization->scale.size(), input_tensor->quantization->zero_point.size(), input_tensor->quantization->min.size(), input_tensor->quantization->max.size());\r\n      \r\n```\r\nI printf it. show that min.size()==0,max.size()==0\r\nI don't know why there isn't max,min in input_tensor.\r\nwhen Quantizing train model, and eval model occur the error?\r\nI set min and max default value is [-127, 128].\r\n```\r\nif(input_tensor->quantization->min.size() == 0 || input_tensor->quantization->max.size() == 0){\r\n        input_tensor->quantization->min.push_back(-128);\r\n        input_tensor->quantization->max.push_back(127);\r\n}\r\n```\r\nOther error:RuntimeError: Unsupported input type UINT8 for input tensor 0 of type FLOAT32\r\nhow my model have float32? I have qunitized.\r\nI find the source code.\r\n```\r\nfor (int i = 0; i < subgraph->inputs.size(); ++i) {\r\n      TensorT* tensor = subgraph->tensors[subgraph->inputs[i]].get();\r\n      // TODO(suharshs): Add support for this case if it ever comes up.\r\n      if (tensor->type == TensorType_FLOAT32 && input_type != tensor->type) {\r\n        error_reporter->Report(\r\n            \"Unsupported input type %s for input tensor %d of type %s.\",\r\n            EnumNameTensorType(input_type), subgraph->inputs[i],\r\n            EnumNameTensorType(tensor->type));\r\n        return kTfLiteError;\r\n      }\r\n      const int32_t input_idx =\r\n          SetInputType(model, subgraph, subgraph->inputs[i], input_type);\r\n      if (input_idx < 0) {\r\n        continue;\r\n      }\r\n      subgraph->inputs[i] = input_idx;\r\n    }\r\n```\r\nIt show that // TODO(suharshs): Add support for this case if it ever comes up.\r\nMaybe, the process which I quantize the model, is error!\r\nCan help me?\r\nI want to train my own model, quantize it,product tflite, runing the model in EgeTpu.", "I had a similar problem with tflite post-training quantization for the SSD + mobilenetv3 model on TensorFlow 1.15.0. I resolved the issue by upgrading to TensorFlow 2.1.0 then saving the model as v1 compatible.\r\n\r\n```\r\nimport tensorflow.compat.v1 as tf\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_quant_model = converter.convert()\r\n```", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35625\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35625\">No</a>\n"]}, {"number": 35624, "title": "[ROCm] Misc updates for the ROCm platform", "body": "see individual commit descriptions for details \r\n\r\n--------------------\r\n\r\n/cc @whchung @chsigg ", "comments": ["@gbaned, gentle ping", "> @gbaned, gentle ping\r\n\r\n@deven-amd This is failing while import internally. Please stay tuned. Thanks!", "@gbaned, gentle ping"]}, {"number": 35623, "title": "Linking error when building libtensorflow_cc.so", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: `v2.1.0-rc2` branch\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.5\r\n- GPU model and memory: RTX 2070 8GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nMy project uses TensorFlow in C++, so I'm trying to get C++ related `.so` compiled. The problem now is that in the linking stage, there are lots of undefined references, see the following for example and the attached log for more.\r\n\r\n```\r\nmlir_passthrough_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.29+0x2f8): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(std::function<tensorflow::Status (tensorflow::shape_inference::InferenceContext*)>)'\r\nmlir_passthrough_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.29+0x324): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\nbazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcuda_stub.pic.a(cuda_stub.pic.o): In function `cudaError_enum (*(anonymous namespace)::LoadSymbol<cudaError_enum (*)(cudaError_enum, char const**)>(char const*))(cudaError_enum, char const**)':\r\ncuda_stub.cc:(.text._ZN12_GLOBAL__N_110LoadSymbolIPF14cudaError_enumS1_PPKcEEET_S3_+0x2a): undefined reference to `tensorflow::Env::Default()'\r\nbazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcuda_stub.pic.a(cuda_stub.pic.o): In function `cudaError_enum (*(anonymous namespace)::LoadSymbol<cudaError_enum (*)(int*)>(char const*))(int*)':\r\ncuda_stub.cc:(.text._ZN12_GLOBAL__N_110LoadSymbolIPF14cudaError_enumPiEEET_PKc+0x2a): undefined reference to `tensorflow::Env::Default()'\r\nbazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcuda_stub.pic.a(cuda_stub.pic.o): In function `cudaError_enum (*(anonymous namespace)::LoadSymbol<cudaError_enum (*)(char*, int, int)>(char const*))(char*, int, int)':\r\ncuda_stub.cc:(.text._ZN12_GLOBAL__N_110LoadSymbolIPF14cudaError_enumPciiEEET_PKc+0x2a): undefined reference to `tensorflow::Env::Default()'\r\nbazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcuda_stub.pic.a(cuda_stub.pic.o): In function `cudaError_enum (*(anonymous namespace)::LoadSymbol<cudaError_enum (*)(int)>(char const*))(int)':\r\ncuda_stub.cc:(.text._ZN12_GLOBAL__N_110LoadSymbolIPF14cudaError_enumiEEET_PKc+0x2a): undefined reference to `tensorflow::Env::Default()'\r\nbazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcuda_stub.pic.a(cuda_stub.pic.o): In function `cudaError_enum (*(anonymous namespace)::LoadSymbol<cudaError_enum (*)(CUctx_st*)>(char const*))(CUctx_st*)':\r\ncuda_stub.cc:(.text._ZN12_GLOBAL__N_110LoadSymbolIPF14cudaError_enumP8CUctx_stEEET_PKc+0x2a): undefined reference to `tensorflow::Env::Default()'\r\nbazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcuda_stub.pic.a(cuda_stub.pic.o):cuda_stub.cc:(.text._ZN12_GLOBAL__N_110LoadSymbolIPF14cudaError_enumPP8CUctx_stEEET_PKc+0x2a): more undefined references to `tensorflow::Env::Default()' follow\r\nbazel-out/k8-opt/bin/tensorflow/core/distributed_runtime/librequest_id.pic.a(request_id.pic.o): In function `tensorflow::GetUniqueRequestId()':\r\nrequest_id.cc:(.text._ZN10tensorflow18GetUniqueRequestIdEv+0x9): undefined reference to `tensorflow::random::New64()'\r\ncollect2: error: ld returned 1 exit status\r\nINFO: Elapsed time: 33.036s, Critical Path: 32.21s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nbazel build --config=opt \\\r\n                //tensorflow:libtensorflow_cc.so \\\r\n                //tensorflow:libtensorflow_framework.so \\\r\n                //tensorflow:install_headers\r\n```\r\n\r\n.tf_configure.bazelrc:\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/abcdabcd987/.pyenv/versions/3.7.2/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/abcdabcd987/.pyenv/versions/3.7.2/lib/python3.7/site-packages\"\r\nbuild --python_path=\"/home/abcdabcd987/.pyenv/versions/3.7.2/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_CUDA_VERSION=\"10.1\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"\"\r\nbuild --action_env TF_CUDA_PATHS=\"/usr/local/cuda-10.1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda-10.1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"7.5\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/x86_64-linux-gnu-gcc-7\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[tensorflow_cc-link.txt.gz](https://github.com/tensorflow/tensorflow/files/4028250/tensorflow_cc-link.txt.gz)\r\n\r\n", "comments": ["Try adding --noincompatible_do_not_split_linking_cmdline to your bazel build command. ", "Just realized that `bazel` doesn't invoke the correctly versioned one. If I use `/usr/bin/bazel-0.29.1-linux-x86_64`, it works. I ran into this issue because I'm compiling different versions of TensorFlow and have multiple installation of bazel. I also ran into some linking issues when compiling `v1.15.0` and using `/usr/bin/bazel-0.26.1-linux-x86_64` fixed that.\r\n\r\nThanks for the help though @Arjuna197  :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35623\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35623\">No</a>\n", "Actually, I found manually specifying bazel version only fix linking issues for `v1.15.0`, not for 2.x, as in 2.x there is a `.bazelversion` that forces bazel to invoke the specific version. And I found that for 2.x, I can build it on Manjaro (Arch Linux) but not Ubuntu 18.04. `--noincompatible_do_not_split_linking_cmdline` can indeed fix the linking errors on Ubuntu 18.04. Thanks @Arjuna197 !"]}, {"number": 35622, "title": "Contributing: Fix a typo", "body": "nput --> input", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35622) for more info**.\n\n<!-- need_sender_cla -->", "Can you fix more than one typo in a file please? One letter change causes hours of CI to run and that warms the planet and melts the ice.", "@mihaimaruseac What is the minimum number of typos?", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35622) for more info**.\n\n<!-- ok -->", "There is no minimum number but we prefer fixing every typo in a file/directory instead of just one. See [`CONTRIBUTING.md`](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#general-guidelines-and-philosophy-for-contribution)", "It would be nice if changes to comments alone did not trigger this lengthy CI process.", "@mihaimaruseac my mistake. It is always important to read the contributing guide before any PR."]}, {"number": 35621, "title": "Replace --xla_gpu_disable_autotune option with --xla_gpu_autotune_level", "body": "XLA performs autotuning for convolutions and GEMMs. for each algorithm\r\nit runs some checks to check for out of bounds access or functional errors.\r\nThe latter in particular can take a lot of time, increasing compile time\r\nby orders of magnitude. This hurts end-to-end execution time.\r\n\r\n--xla_gpu_autotune_level enables clients to set a level.\r\n        0: don't autotune\r\n        1: autotune with uninitialized data\r\n        2: autotune with initialized data\r\n        3: autotune with initialized data, and reinitialize for inplace case\r\n        4: autotune with initialized data, reinitialize, and check\r\nThe deafult is 4, not changing the current behaviour.\r\n\r\nChange some tests accordingly.", "comments": ["Can someone review this PR please?", "> Can someone review this PR please?\r\n\r\nDone. Sorry for the delay!", "Is there anything I can do on y end to resolve the unsuccessful check?"]}, {"number": 35620, "title": "Update version numbers for TensorFlow 2.1.0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 1 -> 1\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.1.0-rc2\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.1.0rc2\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 35619, "title": "[Intel MKL] reverted the commit 782e12b7aa42015263370c7593df780dd917c\u2026", "body": "\u2026776 which fixes for Fixed a bug in mkl_conv2d constant filter caching because it causes performance regression in a few models", "comments": []}, {"number": 35618, "title": "Tensorflow 2.1.0 - DLL load failed", "body": "### System information\r\n- **OS Platform and Distribution**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu==2.1.0rc2\r\n- **TensorFlow version (use command below)**: 2.1.0rc2\r\n- **Python version**: 3.7.4\r\n- **CUDA/cuDNN version**: CUDA 10.1, cuDNN 7.6.5 associated with CUDA 10.1\r\n- **GPU model and memory**: GTX 1060 6GB\r\n\r\n### Describe the problem\r\nI installed the pre-release using `pip install tensorflow-gpu==2.1.0rc2`. I installedCUDA 10.1 with cuDNN 7.6.5. When running `import tensorflow`, I get a \r\n\r\n> ImportError: DLL load failed: The specified module could not be found.\r\n\r\nIt probably means there is an issue with the CUDA installation, or a bug somewhere. Tensorflow 2.1.0 is supposed to be compatible with CUDA 10.1. \r\n\r\nIn my system environment variables, path contains:\r\n\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\lib64\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\include\r\n\r\n\r\n### Source code / logs\r\n  ```\r\nFile \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.2.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\johndoe\\AppData\\Local\\Continuum\\anaconda3\\envs\\dummy_env_name\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\johndoe\\AppData\\Local\\Continuum\\anaconda3\\envs\\dummy_env_name\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\johndoe\\AppData\\Local\\Continuum\\anaconda3\\envs\\dummy_env_name\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\johndoe\\AppData\\Local\\Continuum\\anaconda3\\envs\\dummy_env_name\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n```", "comments": ["@ReinforcedMan \r\nDid you follow the instruction from [TensorFlow website](https://www.tensorflow.org/install/source_windows). You can have a look on similar issue #22794 and #10033 . let us know if that helps. Looks like you need to add path for CUDA and CuDNN.Thanks!", "@ravikyram \r\nI did not follow the instructions to build the package. I installed the 2.1.0rc2 version from pipy, and couldn't get it to work with any cuda version (neither 10.0 or 10.1), even though it is supposed to support cuda 10.1 as stated in the release notes. \r\n\r\nWhen trying to build the package like you linked, I get an error in the c++ code when building with bazel. \r\n\r\nI don't have any more days to allocate to this and am thus giving up. I'll wait for a clean 2.1.0 release. Thank you for your help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35618\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35618\">No</a>\n", "I'm reopening since the problem still appears with the version of tensorflow 2.1.0 on pipy that you get with \r\n\r\n`pip install tensorflow`\r\n\r\nI have cuda 10.1 and cudnn 7.6.5 for cuda 10.1. \r\nIn my system path I have bin/, libnvvp/ and CUPTI\\lib64. It worked perfectly for tensorflow 2.0.0, and I adapted the paths for the new cuda version.\r\n", "Can you try using `tensorflow` instead of `tensorflow-gpu`? It should have the GPU build too (and if it works it will point out a failure in the way we release these two)", "I tried both\r\n\r\n```\r\npip install tensorflow\r\npip install tensorflow-gpu\r\n```\r\n\r\nbut neither seem to be working for me ...\r\n\r\nAlso: Maybe something of note is that in the [test build of the pipy version](https://source.cloud.google.com/results/invocations/88229a56-37fe-41cc-947a-787023dce60a/targets/tensorflow%2Fgithub%2Fwindows%2Fgpu_py36_full%2Fcontinuous/log) the CUPTI path is set as pointing to 10.1/../libx64, where it seems to have been renamed to 10.1/../lib64 without the x in the cuda installation.", "i fix this by downgrading to\r\n\r\n`pip install tensorflow=2.0`", "@ReinforcedMan can you make a symlink from `CUPT/libx64` to `CUPT/lib64`?", "@mihaimaruseac I already added the two versions in my Path variable, I don't think that's the cause of it. I was just saying that in the tensorflow test build I think you guys might have the wrong path, but it still works as you also give the CUDA/v10.0/../libx64 in addition to the 10.1. \r\n\r\nSorry for being unclear. ", "I am having the same issue when I pip install tensorflow. I do not have an nvidia graphics card and want to use a CPU tensorflow, NOT GPU. I get:\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\nFailed to load the native TensorFlow runtime.", "I am using an HP laptop from 2 years ago.", "Is there a workaround for this issue??", "@nectario try `tensorflow-cpu` package? If the issue still persists, I would recommend opening a new one and filling in the template as it looks like you have a different issue.", "This happens even when I install tensorflow-cpu. Below is the complete stack trace:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Development/Projects/TensorFlow_2.0_Hierarchical_Attention/QuestionAnswerContextAttention.py\", line 8, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "The issue seems to be the same and it happens on two completely different computers of mine.", "Please identify which DLL fails to load and please open a different issue as they look to be different", "Done.\r\nIssue: #35749", "I posted my solution in the @nectario post.\r\nhttps://github.com/tensorflow/tensorflow/issues/35749#issuecomment-573313402", "The solution proposed by @galocen in #35749 seems to resolve the issue for me.\r\n> I solved it today downloading and installing visual studio 2015-2019 x86 and x64 from [here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n", "@ReinforcedMan \r\n\r\nPlease, let us know if the issue still persists. Thanks!", "I have the same issue and my laptop is not with GPU. I tried with CPU version but the issue still exists. So, have returned back to 2.0.", "I still don't know what DLL fails to load", "@ravikyram @mihaimaruseac \r\n\r\nThe solution proposed by @galocen in #35749 solves the issue for me. So if anyone has this issue,\r\nmake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads). \r\n\r\nThank you all for your help, this issue is now solved, closing it. \r\n\r\n@aeropia: Your issue seems different, feel free to open a new one describing it in more details. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35618\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35618\">No</a>\n", "@ReinforcedMan Are you using vs2015 or a higher version?", "> @ravikyram @mihaimaruseac\r\n> \r\n> The solution proposed by @galocen in #35749 solves the issue for me. So if anyone has this issue,\r\n> make sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n> \r\n> Thank you all for your help, this issue is now solved, closing it.\r\n> \r\n> @aeropia: Your issue seems different, feel free to open a new one describing it in more details.\r\n\r\nIt also solved my problem!!", "Installing the redistributable didn't solve the error for me. Does anyone else have the same problem?", "@Ansreng360 \r\n\r\nThere are at least 4 possible scenarios:\r\n\r\n* You need to install the MSVC 2019 redistributable\r\n* Your CPU does not support AVX2 instructions\r\n* Your CPU/Python is on 32 bits\r\n* There is a library that is in a different location/not installed on your system that cannot be loaded.\r\n\r\nPlease identify which is your case and open a new issue *only if* there is a fifth case", "I had similar DLL load failed error when I was importing tensorflow 2.1 in my latest Anaconda environment. I tried several times from creating the virtual environment to installation tensorflow 2.1 without success.  The installation process went through fine without any error message. The tensorflow 2.0 works fine but not tensorflow 2.1.  Thanks for any insight! ", "@bduan007 \r\n\r\nCan you please raise a new issue by filling [build/installation issue template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md).Thanks!", "Sure!\u00a0 Can you forward me the link that I shared the issue?\u00a0 I could not easily find my comments and don't want to recreate the wheels.\u00a0 Thanks! Bin\n\n\n\n\n\nOn Monday, February 10, 2020, 12:25:35 AM EST, ravikyram <notifications@github.com> wrote: \n\n\n\n\n\n@bduan007\n\nCan you please raise a new issue by filling build/installation issue template.Thanks!\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe.\n", "> > @ravikyram @mihaimaruseac\r\n> > The solution proposed by @galocen in #35749 solves the issue for me. So if anyone has this issue,\r\n> > make sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n> > Thank you all for your help, this issue is now solved, closing it.\r\n> > @aeropia: Your issue seems different, feel free to open a new one describing it in more details.\r\n> \r\n> It also solved my problem!!\r\n\r\n\r\n\r\n> @ravikyram @mihaimaruseac\r\n> \r\n> The solution proposed by @galocen in #35749 solves the issue for me. So if anyone has this issue,\r\n> make sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n> \r\n> Thank you all for your help, this issue is now solved, closing it.\r\n> \r\n> @aeropia: Your issue seems different, feel free to open a new one describing it in more detail.\r\n\r\nI don't think this is closed. Tensorflow PyPI distribution should not rely on a manual install of some Microsoft software utility that needs admin rights to install. (Scalability and so)", "We cannot bundle everything ourselves (especially so since some packages have licenses prohibiting bundling together). So you have to install dependencies out of bound.", "I ran through this issue too during an update of mt TF environment. The installation of latest Visual C redistribuable tools solved the issue for me too obviously.\r\nThis solution is well documented in build from source for windows [https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2019](https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2019)\r\nBUT it should also be documented as a requirement in the PIP based installation process and even better checked during the pip install if it is possible to echoe an explicite warning if msvcp140_1.dll has not been found. It would save time and nerves on many people that are not building from sources. ", "> i fix this by downgrading to\r\n> \r\n> `pip install tensorflow=2.0`\r\n\r\nproblem solved!", "Solution is to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n\r\nLocking conversation to prevent spammy \"works for me\" and fake solutions of \"downgrade to TF 2.0\""]}, {"number": 35617, "title": "erfcx special function", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): -\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI would like to see the `erfcx(x) = exp(x**2) * erfc(x)` function (e.g., `scipy.special.erfcx`) implemented in Tensorflow. The naive implementation `exp(x**2) * erfc(x)` fails for large `x`.\r\n\r\nThis is useful when one needs to work with truncated normal distributions. \r\n\r\n`erfcx` is available in many numerical packages, such as [Matlab](https://www.mathworks.com/help/matlab/ref/erfcx.html), [Julia](https://juliamath.github.io/SpecialFunctions.jl/latest/functions_list/#SpecialFunctions.erfcx), [SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.erfcx.html) [R](https://www.rdocumentation.org/packages/pracma/versions/1.9.9/topics/erf), and others.\r\n\r\nAn alternative is to implement `log_erfc(x) = log(erfc(x))`, from which one can obtain `erfcx(x)` accurately. The function `log_erfc(x)` is implemented in [GSL](https://github.com/ampl/gsl/blob/644e768630841bd085cb7121085a688c4ff424d0/specfunc/erfc.c#L441).\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone working with truncated normal distributions.\r\n", "comments": ["In the meantime is there a workaround to use this function? Hopefully without degrading performance too much. Note that I intend to run this model on the cpu only, so maybe that helps.", "One thing I tried is this:\r\n\r\n```python\r\n@tf.function\r\ndef erfcx1(x):\r\n    return tf.convert_to_tensor(scipy.special.erfcx(A))\r\n\r\n# no tf.function annotation\r\ndef erfcx2(x):\r\n    return tf.convert_to_tensor(scipy.special.erfcx(A))\r\n```\r\n\r\nInterestingly, the `@tf.function` annotation here seems to degrade performance:\r\n\r\n```python\r\nA = tf.random.uniform((50,60))\r\n%timeit erfcx1(A) # 95 \u00b5s \u00b1 12.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n%timeit erfcx2(A) # 63.9 \u00b5s \u00b1 6.07 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n```\r\n\r\nWhy?", "@cossio  I checked it in tf-nightly and the performance is low if we decorate the function with tf-nightly", "@cossio the decorated function won't work as expected: this just remembers each Python input and rebuilds the graph. I recommend to read up on this e.g. [here](https://www.tensorflow.org/guide/function). Actually, try to put in a tensor, not a Python number and the decorated should fail.\r\n\r\nerfcx was though added now in the nightlies of TensorFlow Probability", "@cossio,\r\n[tfp.math.erfcx ](https://www.tensorflow.org/probability/api_docs/python/tfp/math/erfcx) is available now. The [documentation](https://www.tensorflow.org/probability/api_docs/python/tfp/math/erfcx) states:\r\n\r\n> Note: This API is new and only available via pip install tfp-nightly.\r\n\r\nThank you @mayou36 for pointing it out. ", "Just to add, there is the difference that this does not (yet) work for complex arguments (but this is WIP: https://github.com/tensorflow/probability/issues/1265)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Bump", "@cossio,\r\nAs per [this comment](https://github.com/tensorflow/tensorflow/issues/35617#issuecomment-829345082), this issue is related to **`Tensorflow Probability`**, not actual **`Tensorflow`**. Can we close this issue as it is being tracked in [TF Probability Repo](https://github.com/tensorflow/probability/issues/1265)? Thanks!", "Closed in favor of https://github.com/tensorflow/probability/issues/1265"]}, {"number": 35616, "title": "Typo Error in `l07c01_saving_and_loading_models.ipynb`", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/ManishAradwad/examples/blob/9f7d80aff8214b358e4aea0b83f2648748990c4b/courses/udacity_intro_to_tensorflow_for_deep_learning/l07c01_saving_and_loading_models.ipynb#L579\r\n\r\n`The differnece in output should be zero:`\r\n\r\n## Description of issue (what needs changing):\r\n\r\ndiffernece should be difference\r\n\r\n### Submit a pull request?\r\n\r\nYes\r\n", "comments": ["@jvishnuvardhan Is this issue is solved? ", "@abhiverma1924 There is a PR https://github.com/tensorflow/examples/pull/138 opened to resolve this issue. Thanks. ", "Thanks. In the process of merging that PR so will close the issue in this repo."]}, {"number": 35615, "title": "tf.argmax(x, axis=-1) breaks TFLite exporter", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\n**Code to reproduce the issue**\r\n```python\r\ndef build_model():\r\n    from tensorflow.keras.layers import Lambda, Input\r\n    from tensorflow.keras.models import Model\r\n\r\n    a = Input(shape=(16, 16, 4))\r\n    b = Input(shape=(4,))\r\n\r\n    def f(inputs):\r\n        a, b = inputs\r\n        print('b.shape', b.shape)\r\n        b = tf.one_hot(tf.argmax(b, axis=-1), 4)\r\n        return tf.einsum('bhwc,bc->bhw', a, b)\r\n\r\n    x = Lambda(f)([a, b])\r\n    model = Model(inputs=[a, b], outputs=x)\r\n    return model\r\n\r\n_model = build_model()\r\nprint(_model.predict([np.ones((1, 16, 16, 4)), np.ones((1, 4))]))\r\n\r\ntf.saved_model.save(_model, 'argmax_model')\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('argmax_model')\r\ngraph = converter.convert()\r\n```\r\n\r\n**Describe the current behavior**\r\n_model.predict` returns the correct result.\r\n\r\nThe TFLite converter falls over with the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  ...\r\n  File \"/Users/stefan/Documents/dc-form-model/src/scratch.py\", line 29, in <module>\r\n    graph = converter.convert()\r\n  File \"/Users/stefan/.virtualenvs/dc-form-model/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 446, in convert\r\n    **converter_kwargs)\r\n  File \"/Users/stefan/.virtualenvs/dc-form-model/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 449, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/Users/stefan/.virtualenvs/dc-form-model/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-01-06 18:33:19.342565: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 18 operators, 36 arrays (0 quantized)\r\n2020-01-06 18:33:19.343757: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 18 operators, 36 arrays (0 quantized)\r\n2020-01-06 18:33:19.344782: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:460] Check failed: input_flat_size == RequiredBufferSizeForShape(output_shape) (16 vs. 4)Input cannot be reshaped to requested dimensions for Reshape op with output \"PartitionedCall/model/lambda/einsum/Reshape_1\". Are your input shapes correct?\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00000001151fa5c0 (most recent call first):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52 in execute\r\n  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 299 in run\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89 in main\r\n  File \"/usr/local/bin/toco_from_protos\", line 8 in <module>\r\n```\r\n\r\n**Describe the expected behavior**\r\nExpected for it to work correctly.\r\n\r\nIf I replace `tf.argmax(b, axis=-1)` with `tf.argmax(b, axis=1)`, which indexes exactly the same axis but with a non-negative index (`b` is a rank-2 tensor), then the export finishes successfully.\r\n\r\n", "comments": ["@tailsu You need to enable `tensorflow` supported ops as follows. \r\n\r\n`converter.target_spec.supported_ops =[tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]`\r\n\r\nWith that modification, code works without any error. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/cc7a88b02d9e078170552441d9581967/untitled732.ipynb) is the gist for your reference.\r\n\r\nPlease close the issue if it was resolved for you. Thanks!", "I see in your gist that you set\r\n\r\n```\r\nconverter.experimental_new_converter = True\r\n```\r\n\r\nI'm guessing that the bug is inside the old TOCO converter which is already deprecated in the tf-nightly version. I'm guessing this means that the bug won't be fixed.", "@tailsu I am not sure what you meant by \r\n\r\n> I'm guessing this means that the bug won't be fixed.\r\n\r\nThis bug was fixed in the recent `tf-nightly`  by this `converter.target_spec.supported_ops =[tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]`.\r\n\r\nIf you use recently `tf-nightly`, we don't need to use `converter.experimental_new_converter = True` as that is selected by default. "]}, {"number": 35614, "title": "Segmentation fault when inferencing in C++", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n/\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version (use command below):\r\n2.1\r\n- Python version:\r\n3.6.9\r\n- Bazel version (if compiling from source):\r\nbazel 0.27.2\r\n- GCC/Compiler version (if compiling from source):\r\ngcc 8.3.0\r\n- CUDA/cuDNN version:\r\n10.1\r\n- GPU model and memory:\r\ngtx 950m\r\n\r\n\r\n**Describe the current behavior**\r\nI have trained a model (sub-classed keras.Model) and saved it via the SavedModel-API in Python.\r\nI want to include this model in a c++ program. To this end, I have compiled the tensorflow_cc library and linked it to my project by using:\r\n\r\n`bazel build --config=opt --config=cuda //tensorflow:libtensorflow_cc.so`\r\n\r\nIn c++, I can load the model, but upon running the session for inference, the program suffers a segmentation fault. I have done this in a similar manner before, but not since moving to eager execution. My model's call method is decorated with @tf.function. Unfortunately, I am unable to provide further details since I can't build tensorflow with debug symbols to get a proper backtrace of the issue.\r\n \r\n**Describe the expected behavior**\r\nThe model should run a forward pass in C++. \r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```c++\r\n#ifdef __GNUC__\r\n#pragma GCC diagnostic push\r\n#pragma GCC diagnostic ignored \"-Winvalid-partial-specialization\"\r\n#include <tensorflow/core/public/session.h>\r\n#include <tensorflow/core/protobuf/meta_graph.pb.h>\r\n#include <tensorflow/core/public/session_options.h>\r\n#include <tensorflow/cc/saved_model/loader.h>\r\n#include <tensorflow/cc/saved_model/tag_constants.h>\r\n#pragma GCC diagnostic pop\r\n#else\r\n#include <tensorflow/core/public/session.h>\r\n#include <tensorflow/core/protobuf/meta_graph.pb.h>\r\n#include <tensorflow/core/public/session_options.h>\r\n#include <tensorflow/cc/saved_model/loader.h>\r\n#include <tensorflow/cc/saved_model/tag_constants.h>\r\n#endif\r\n\r\n#include <string>\r\n#include <Eigen/Dense>\r\n\r\nusing namespace tensorflow;\r\n\r\nint main(){\r\n\r\n    const std::string pathToGraph = \"/path/to/saved/model/\";\r\n\r\n    Status status;\r\n    SavedModelBundle bundle;\r\n    status = LoadSavedModel(SessionOptions(), RunOptions(), pathToGraph, {tensorflow::kSavedModelTagServe}, &bundle);\r\n    Session* session = bundle.GetSession();\r\n\r\n    if (!status.ok())\r\n    {\r\n        throw std::runtime_error(\"Could not create Tensorflow session: \" + status.ToString());\r\n    }\r\n\r\n    if (!status.ok())\r\n    {\r\n        throw std::runtime_error(\"Error reading graph definition from \" + pathToGraph + \": \" + status.ToString());\r\n    }\r\n\r\n    GraphDef def = bundle.meta_graph_def.graph_def();\r\n    std::string outputlayer = \"StatefulPartitionedCall:0\";\r\n\r\n    tensorflow::Tensor input_tensor_state(tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 6}));\r\n    tensorflow::Tensor input_tensor_action(tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 3}));\r\n\r\n    auto input_tensor_state_mapped = input_tensor_state.flat<float>().data();\r\n    auto input_tensor_action_mapped = input_tensor_action.flat<float>().data();\r\n\r\n    Eigen::Matrix<float, 6, 1> X = Eigen::Matrix<float, 6, 1>::Zero();\r\n    Eigen::Matrix<float, 3, 1> U = Eigen::Matrix<float, 3, 1>::Zero();\r\n\r\n    memcpy(input_tensor_state_mapped, X.data(), X.size()*sizeof(float));\r\n    memcpy(input_tensor_action_mapped, U.data(), U.size()*sizeof(float));\r\n\r\n    std::vector<tensorflow::Tensor> outputs;\r\n    \r\n    Status run_status = session->Run({{\"serving_default_input_1:0\", input_tensor_state},\r\n                                      {\"serving_default_input_2:0\", input_tensor_action}}, {outputlayer}, {},\r\n                                     &outputs); // The program segfaults here\r\n\r\n\r\n}\r\n\r\n```\r\n\r\n**Other info / logs**\r\nI am also including the saved model in question.\r\n[NeuralModel.zip](https://github.com/tensorflow/tensorflow/files/4027127/NeuralModel.zip)\r\n\r\n", "comments": ["Can you minimize the code and the model and provide both?", "Sorry, I just saw you provided the model too.", "@mihaimaruseac Hello! What exactly do you want me to minimize? Do you mean the code used for model training?", "Hi. Please ignore the request to minimize code, didn't look properly", "@mihaimaruseac Any idea in what direction I could go to solve the issue?", "Can you try running under Valgrind? Or identifying the line of the crash?\r\n\r\nI didn't get a chance to look into this, the earliest I can get to it would be mid February", "Have you solved the problem yet? If you did it, could you tell me? Thanks!", "@LemonRay Yeah I did solve it for my case. The pointer to the session stored in the SavedModelBundle is a unique_ptr and you can't use it the way I did. I ended up accessing it through the SavedModelBundle like this:\r\n\r\n```cpp\r\nStatus status;\r\nSavedModelBundle bundle\r\nstatus = LoadSavedModel(SessionOptions(), RunOptions(), pathToGraph, {tensorflow::kSavedModelTagServe}, &bundle);\r\n... //define inputs and outputs\r\nStatus run_status = bundle.session->Run({{\"serving_default_input_1:0\", input_tensor_state}, {\"serving_default_input_2:0\", input_tensor_action}}, {outputlayer}, {}, &outputs);\r\n```\r\nHope this is applicable to your problem. ", "Can we close this if it is fixed?", "I'll have a try! Thanks for your reply."]}, {"number": 35613, "title": "Fix lite/micro cmsis-nn kernels that were not refactored in #27019", "body": "Fix for #35612 ", "comments": []}, {"number": 35612, "title": "lite/micro/kernels/cmis-nn ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 2274eacd794d7e501849567811637b7921e52820\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ARM cmis-nn\r\n\r\n**Describe the problem**\r\n\r\nIssue reported by \"On-Device AI Co., Ltd. \" on tensorflow/sig-micro gitter\r\nLite/micro examples using CMIS-NN kernels no longer compile.\r\n\r\nRoot cause:\r\nThe PR: Lite: Kernel_util refactored #27019 did not refactor the cmis-nn specific add and mul kernels. Will submit PR request with the missing changes.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n```make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge TAGS=\"cmsis-nn\" micro_speech_bin\r\n\r\n......\r\n\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -fno-rtti -DPART_apollo3 -DAM_PACKAGE_BGA -DAM_PART_APOLLO3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DNDEBUG -DTF_LITE_MCU_DEBUG_LOG -D __FPU_PRESENT=1 -DARM_MATH_CM4 -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m4 -mthumb -mfpu=fpv4-sp-d16 -mfloat-abi=hard -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -ggdb -O3 -DARM_MATH_DSP -DARM_MATH_LOOPUNROLL -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/ -Itensorflow/lite/micro/tools/make/downloads/CMSIS_ext/ -Itensorflow/lite/micro/tools/make/downloads/gcc_embedded//arm-none-eabi/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/mcu/apollo3/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/CMSIS/AmbiqMicro/Include/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/bsp -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/devices/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/utils/ -Itensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/Core/Include -Itensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/NN/Include -Itensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/DSP/Include -Itensorflow/lite/micro/tools/make/downloads/kissfft -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/examples/example1_edge_test/src/tf_accelerometer/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/examples/example1_edge_test/src/tf_adc/ -c tensorflow/lite/micro/kernels/cmsis-nn/add.cc -o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/kernels/cmsis-nn/add.o\r\ntensorflow/lite/micro/kernels/cmsis-nn/add.cc: In function 'TfLiteStatus tflite::ops::micro::add::CalculateOpData(TfLiteContext*, TfLiteAddParams*, const TfLiteTensor*, const TfLiteTensor*, TfLiteTensor*, tflite::ops::micro::add::OpData*)':\r\ntensorflow/lite/micro/kernels/cmsis-nn/add.cc:89:7: error: 'CalculateActivationRangeUint8' was not declared in this scope\r\n       CalculateActivationRangeUint8(params->activation, output,\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/micro/kernels/cmsis-nn/add.cc:89:7: note: suggested alternative: 'CalculateActivationRange'\r\n       CalculateActivationRangeUint8(params->activation, output,\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n       CalculateActivationRange\r\ntensorflow/lite/micro/kernels/cmsis-nn/add.cc:93:7: error: 'CalculateActivationRangeInt8' was not declared in this scope\r\n       CalculateActivationRangeInt8(params->activation, output,\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/micro/kernels/cmsis-nn/add.cc:93:7: note: suggested alternative: 'CalculateActivationRange'\r\n       CalculateActivationRangeInt8(params->activation, output,\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n       CalculateActivationRange\r\n.....\r\n```", "comments": ["Fixed by #35613 "]}, {"number": 35610, "title": "Add docs for tf.strings.lower and tf.strings.regex_replace", "body": "Funny enough it even showed as TODO in docs and on the website\r\n![image](https://user-images.githubusercontent.com/775466/71825842-02037080-309d-11ea-8803-c225c002d318.png)\r\n", "comments": ["Fixed line split, thanks for force-run! ", "Windows build failing is not related to introduced change. cc @mihaimaruseac \r\n\r\nDoes it make sense not even to suppress this part? \r\nFeels like it's pretty common :)", "It is fixed by adc3a1b1bec28849f62076e9b4be5c5963e5e5e7 so after a rebase on master it should work.\r\n\r\nBut in any case, this is blocked now on copybara and internal import, nothing needed for you to do at the moment\r\n", "Sorry for the delay, do you mind resolving the conflicts @lc0 ?", "@yifeif just rebased and resolved conflicts"]}, {"number": 35609, "title": "Anyone knows this error? cuda_dnn.cc:86] Check failed: narrow == wide (-1236407296 vs. 3058560000)checked narrowing failed; values not equal post-conversion", "body": "2020-01-06 13:21:24.480792: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2020-01-06 13:21:24.742605: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-06 13:21:28.135955: F tensorflow/stream_executor/cuda/cuda_dnn.cc:86] Check failed: narrow == wide (-1236407296 vs. 3058560000)checked narrowing failed; values not equal post-conversion\r\n", "comments": ["@ersanliqiao Did you figure out cause of this issue? I encountered the same error.", "Yes, I have the answer. The code is checking conversion of some dimension to what is effectively a signed int quantity.  The 3058560000 number is too big. This won't fit in a signed 32-bit integer quantity, which can hold a maximum of 2147483647 safely.", "Was there a solution or is it just to use smaller images? Why does TF have this limitation?"]}, {"number": 35608, "title": "Beginner models", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):2.0\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nperfectly normal\r\n**Will this change the current api? How?**\r\nYes. The beginner models can be a different api in tensorflow itself. Like `tf.BeginnerModel`.\r\n**Who will benefit with this feature?**\r\nBeginners in machine learning who want to write less code and acheive a lot\r\n**Any Other info.**\r\nThis will make it easier to use tensorflow as a beginner", "comments": ["For beginners: The best place to start is with the user-friendly Sequential API. You can create models by plugging together building blocks.\r\nSee https://www.tensorflow.org/overview and https://colab.sandbox.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb", "Thats one way to look at it, but another way is that absolute beginners to this topic will want to see some quick results as well. Maybe they don't know coding that well, or they want to try it out before getting their hands dirrty with relatively tougher methods of making models (`tf.Sequential`)", "What part of using `tf.Sequential` is relatively tough?", "@WilliamHYZhang  Good question... I think beginners would like to start off by getting results quickly to see how cool and powerful this technologie is. But in tf.Sequential you have to explicitly state all the layers in the model, the optimizer, the activation and many other things. These would be very confusing to a beginner who might not know much about this field. It would also give them the satisfaction of knowing that they didn't copy and paste everything from the tutorial.\ud83d\ude0a\ud83d\udd96", "We have added a variety of new tutorials for beginners.\r\nYou may try ML basics with Keras that covers a variety of concepts.\r\nSee https://www.tensorflow.org/tutorials\r\nThanks!"]}]