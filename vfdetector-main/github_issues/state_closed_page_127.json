[{"number": 51146, "title": "Google Developers Certification Plugin", "body": "The Google Developers Certification Plugin is not available on Android Studio Arctic Fox 2020.3.1\r\n![dd](https://user-images.githubusercontent.com/40315618/128174071-8755f5fb-5161-4991-9151-df731784dda8.PNG)\r\n", "comments": ["@idee24 Could you please refer to the **`Plugin description`** in this [documentation](https://www.tensorflow.org/certificate) of **`Review Candidate Handbook`** where it is recommended that Certification Plugin is available in Pycharm .Please take a look on that link to  try Google Developers certification [plugin](https://www.tensorflow.org/extras/cert/TF_Certificate_Candidate_Handbook.pdf) in Pycharm   and let us know if it helps? Thank you!", "This helped. My issue has been resolved thanks."]}, {"number": 51143, "title": "tf.saved_model.save fails when there is tf.reshape in tf.keras.Model call method", "body": "**Current behavior**\r\ntf.saved_model.save fails to save tf.keras.Model if there is tf.reshape operation in Model call method.\r\n\r\nI have debugged it and I found that during saving the Model, SimpleModel.call is called twice. The first time everything is fine and **batch_size = 1,** but in the second call **batch_size = None** and this is why it fails.\r\n\r\n**Expected behavior**\r\nModel should be saved as Savedmodel\r\n\r\n**Tested with**\r\nTF 2.4.1 and TF 2.5 installed by PIP\r\nWindows 10\r\nUbuntu 18.04\r\nNvidia GPU\r\n\r\n**Example code for easy reproduction**\r\n```\r\nimport tensorflow as tf\r\n\r\nclass SampleModel(tf.keras.Model):\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n\r\n        self.dense_1 = tf.keras.layers.Dense(\r\n            units=10,\r\n            activation=tf.nn.relu\r\n        )\r\n\r\n    def call(self, inputs, **kwargs):\r\n        batch_size, height, width, filters = inputs.get_shape().as_list()\r\n        x = tf.reshape(inputs, [batch_size, height * width * filters])\r\n        x = self.dense_1(x)\r\n\r\n        return x\r\n\r\ndef get_model():\r\n    input_shape = (10, 10, 3)\r\n    input = tf.keras.layers.Input(shape=input_shape, batch_size=1)\r\n\r\n    x =  tf.keras.layers.Conv2D(\r\n        filters=10,\r\n        kernel_size=3\r\n    )(input)\r\n\r\n    sample_model = SampleModel()\r\n    output = sample_model(x)\r\n    model = tf.keras.Model(inputs=input, outputs=output)\r\n\r\n    return model\r\n\r\nif __name__ == \"__main__\":\r\n    model = get_model()\r\n\r\n    tf.saved_model.save(\r\n        obj=model,\r\n        export_dir=\"./savemodel_test\"\r\n    )\r\n\r\n\r\n```", "comments": ["@kacper-kleczewski Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "Ok, thanks.\r\nIssue added to keras repo:\r\nhttps://github.com/keras-team/keras/issues/15066", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51143\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51143\">No</a>\n"]}, {"number": 51142, "title": "Tensorflow 2.x, Unable to debug a tensorflow projects, can only print tensor.shape but not get values of the tensor", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@tricoffee \r\nPlease share code for us to replicate the issue or if possible share a colab gist with the error reported.\r\nYou may refer to [this link](https://stackoverflow.com/questions/33633370/how-to-print-the-value-of-a-tensor-object-in-tensorflow).\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51142\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51142\">No</a>\n"]}, {"number": 51141, "title": "Failed precondition: Could not find variable bias_w. This could mean that the variable has been deleted.", "body": "tf2.5\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef weightVariables(shape, name):\r\n    initial = tf.random.truncated_normal(shape=shape, mean=0, stddev=0.05, dtype=tf.dtypes.float64)\r\n    return tf.Variable(initial, name=name)\r\n\r\nc_proto = tf.compat.v1.ConfigProto()\r\nc_proto.gpu_options.allow_growth = True\r\n\r\nwith tf.compat.v1.Session(config=c_proto) as sess:\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n\r\n    while True:\r\n        biasWeight = weightVariables([10], name='bias_w')\r\n        print(sess.run(biasWeight))\r\n\r\n```\r\n\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.\r\n  (0) Failed precondition: Could not find variable bias_w. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Container localhost does not exist. (Could not find resource: localhost/bias_w)\r\n         [[node bias_w/Read/ReadVariableOp (defined at test5.py:42) ]]\r\n  (1) Failed precondition: Could not find variable bias_w. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Container localhost does not exist. (Could not find resource: localhost/bias_w)\r\n         [[node bias_w/Read/ReadVariableOp (defined at test5.py:42) ]]\r\n         [[bias_w/Read/ReadVariableOp/_1]]\r\n\r\n```", "comments": ["```\r\nimport tensorflow as tf\r\n\r\ndef weightVariables(shape, name):\r\n    initial = tf.random.truncated_normal(shape=shape, mean=0, stddev=0.05, dtype=tf.dtypes.float64, name='bias_w')\r\n    return initial\r\n\r\nc_proto = tf.compat.v1.ConfigProto()\r\nc_proto.gpu_options.allow_growth = True\r\n\r\nwith tf.compat.v1.Session(config=c_proto) as sess:\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n\r\n    while True:\r\n        biasWeight = weightVariables([10], name='bias_w')\r\n        print(sess.run(biasWeight))\r\n```\r\n\r\nThis code works normally if I remove the `tf.Variable` in the function of `weightVariables`.But that includes the weight that I want to train. So I think `tf.Variable`  is necessary.  Also the while loop stands for different batches of data, so I think that is also necessary. So how should I modify the code.", "Some simplification but same error\r\n```\r\nimport tensorflow as tf\r\n\r\ndef weightVariables(shape, name):\r\n    initial = tf.random.truncated_normal(shape=shape, mean=0, stddev=0.05, dtype=tf.dtypes.float64, name=name)\r\n    return tf.Variable(initial)\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    biasWeight = weightVariables([10], name='bias_w')\r\n    print(sess.run(biasWeight))\r\n```", "Tensorflow makes me so frustrated because it has wasted me lots time bebuging those that should not be the error.\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\n\r\nv1 = tf.Variable(tf.random.truncated_normal(shape=[4, 3], mean=0, stddev=1), name='v1')\r\nwith tf.compat.v1.Session() as sess:\r\n\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    print(sess.run(v1))\r\n```\r\n\r\n`TypeError: Can not convert a ResourceVariable into a Tensor or Operation.\r\n`\r\n\r\n\r\n\r\n\r\n", "very bad product experience", "```\r\nimport tensorflow as tf\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n\r\n    v1 = tf.Variable(tf.random.truncated_normal(shape=[4, 3], mean=0, stddev=1), name='v1')\r\n    print(sess.run(v1))\r\n```\r\n\r\n```\r\n\r\n  (0) Failed precondition: Could not find variable v1. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Container localhost does not exist. (Could not find resource: localhost/v1)\r\n         [[node v1/Read/ReadVariableOp (defined at test5.py:52) ]]\r\n  (1) Failed precondition: Could not find variable v1. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Container localhost does not exist. (Could not find resource: localhost/v1)\r\n         [[node v1/Read/ReadVariableOp (defined at test5.py:52) ]]\r\n         [[v1/Read/ReadVariableOp/_1]]\r\n\r\n```", "@Saduf2019 Please explain to me what each of the error means", "@sjtusmartboy ,\r\nplease modify likewise to define your variable before initializing it ;- \r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n    v1 = tf.Variable(tf.random.truncated_normal(shape=[4, 3], mean=0, stddev=1), name='v1')\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    print(sess.run(v1))\r\n\r\n```\r\n    ", "@arghyaganguly Yeah, that solves the question, is this the new requirement in tf 2.5? Any instructions?", "@sjtusmartboy ,from Tensorflow 2.0, eager execution is enabled by default which might be the reason the below approach works -\r\n\r\napproach 1)defining the variable outside the session  :- \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nv1 = tf.Variable(tf.random.truncated_normal(shape=[4, 3], mean=0, stddev=1), name='v1')\r\nwith tf.compat.v1.Session() as sess:\r\n\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    print(sess.run(v1))\r\n    \r\n```    \r\nresults in below error :- \r\n```  \r\nTypeError: Can not convert a ResourceVariable into a Tensor or Operation.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py in __init__(self, fetches, contraction_fn)\r\n    309         raise TypeError('Fetch argument %r has invalid type %r, '\r\n    310                         'must be a string or Tensor. (%s)' %\r\n--> 311                         (fetch, type(fetch), str(e)))\r\n    312       except ValueError as e:\r\n    313         raise ValueError('Fetch argument %r cannot be interpreted as a '\r\n\r\nTypeError: Fetch argument <tf.Variable 'v1:0' shape=(4, 3) dtype=float32> has invalid type <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>, must be a string or Tensor. (Can not convert a ResourceVariable into a Tensor or Operation.)    \r\n  ```  \r\nto avoid this please turn off eager execution in tf2.x by the below approach :-\r\n ```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\nv1 = tf.random.truncated_normal(shape=[4, 3], mean=0, stddev=1)\r\nwith tf.compat.v1.Session() as sess:\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    print(sess.run(v1))\r\n ```\r\nelse ,\r\nif you don't want to turn off eager execution in tf2.x, defining the variable inside the session does the trick. \r\n ```\r\nimport tensorflow as tf\r\nwith tf.compat.v1.Session() as sess:\r\n    v1 = tf.Variable(tf.random.truncated_normal(shape=[4, 3], mean=0, stddev=1), name='v1')\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    print(sess.run(v1))\r\n ```\r\n     \r\n \r\n\r\n    ", "@arghyaganguly Thanks very much for your kind answer", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51141\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51141\">No</a>\n"]}, {"number": 51140, "title": "[determinism] Add unimplemented exception to tf.image.adjust_contrast", "body": "This current PR adds and tests determinism-unimplemented exception-throwing for `tf.image.adjust_contrast`.\r\n\r\nThis PR is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#adjust-contrast).\r\n\r\nCC @reedwm, @sanjoy, @nluehr", "comments": ["@reedwm, I have added an exception to `AdjustContrastOp`, but have not tested it. I think this path is inaccessible from the Python API. I'm not actually certain that this path introduces nondeterminism. Do you think we should (A) remove that exception-throwing case, or (B) test that (1) the path is nondeterministic and that (2) the exception gets thrown when executing the C++ op directly?", "@duncanriach  Can you please resolve conflicts? Thanks!", "@gbaned Conflicts resolved.", "This has been merged in 7d9c7679264c998afe6c9ef3873e5752e2b4454a, but for some reason this PR hasn't been marked as merged, and the commit message looks different from normal merge commits. Not sure why that happened, but closing this PR since the changes are in."]}, {"number": 51139, "title": "KeyError: u'SelectV2'", "body": "**I trained one model using tf 1.13, but when i load the model , the KeyError: u'SelectV2' occured , is there anything wrong with the procedure?**\r\n\r\nFile \"/usr/lib/python2.7/site-packages/tensorflow/contrib/predictor/predictor_factories.py\", line 153, in from_saved_model\r\n    config=config)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/predictor/saved_model_predictor.py\", line 153, in __init__\r\n    loader.load(self._session, tags.split(','), export_dir)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 197, in load\r\n    return loader.load(sess, tags, import_scope, **saver_kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 350, in load\r\n    **saver_kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 278, in load_graph\r\n    meta_graph_def, import_scope=import_scope, **saver_kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1696, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 391, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 158, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\nKeyError: u'SelectV2'", "comments": ["@yydxlv We see that you are using older version of tensorflow 1.13 which is officially considered as end of life, We recommend that you upgrade to latest stable version of TF(2.5) and let us know if the issue still persists in newer versions .Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51138, "title": "Fix tf.tile crash when n is large", "body": "This PR tries to fix the issue raised in #46911 where tf.tile\r\nwill crash when n is large. This PR add additional check\r\nto make sure an error message is rendered (instead of crash).\r\n\r\nThis PR fixes #46911.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @mihaimaruseac. The PR has been updated. Please take a look."]}, {"number": 51137, "title": "TFLite conversion notebook will no longer install dependencies", "body": "## URL(s) with the issue:\r\n\r\nDoc Link: https://github.com/tensorflow/examples/blob/master/lite/examples/gesture_classification/ml/README.md\r\n\r\nNotebook link (also on doc page): \r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/gesture_classification/ml/tensorflowjs_to_tflite_colab_notebook.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\nUpdated notebook that will install the dependencies, and run successfully to convert a Tf 1.x model to a TFLite model. \r\n\r\n### Clear description\r\n\r\nFollowing the steps for the TFLite example project you are recommended to use a python notebook (linked above). I have used it many times before, however it has recently stopped working. I was using it on Collab as per the instructions. \r\n\r\n\r\n### Usage example\r\n\r\nThe following command: `!pip3 install tensorflow==1.14.0 keras==2.2.4 tensorflowjs==0.6.4 --force-reinstall`\r\n\r\nResults in: \r\n\r\n```\r\nERROR: Cannot install keras==2.2.4 and tensorflowjs==0.6.4 because these package versions have conflicting dependencies.\r\n\r\nThe conflict is caused by:\r\n    The user requested keras==2.2.4\r\n    tensorflowjs 0.6.4 depends on keras==2.2.2\r\n\r\nTo fix this you could try to:\r\n1. loosen the range of package versions you've specified\r\n2. remove package versions to allow pip attempt to solve the dependency conflict\r\n\r\n```\r\n", "comments": ["@xunkai @lintian06 could you take a look at this report?", "Thank you for the report! You can add the flag --use-deprecated=legacy-resolver, and it is pip's version change.\r\n```\r\n!pip3 install tensorflow==1.14.0 keras==2.2.4 tensorflowjs==0.6.4 --force-reinstall --use-deprecated=legacy-resolver\r\n```", "Hi @lintian06. You're welcome for the report. \r\n\r\nThe fix did work for the initial error, however the next part of the notebook would then end up in an error unfortunately. \r\n\r\n\r\nCommand:\r\n```\r\nimport traceback\r\nimport logging\r\nimport tensorflow.compat.v1 as tf\r\nimport keras.backend as K\r\nimport os\r\n\r\nfrom google.colab import files\r\n\r\nfrom keras import Model, Input\r\nfrom keras.applications import MobileNet\r\nfrom keras.engine.saving import load_model\r\n\r\nfrom tensorflowjs.converters import load_keras_model\r\n\r\nlogging.basicConfig(level=logging.INFO)\r\nlogger = logging.getLogger(__name__)\r\n```\r\n\r\nError:\r\n```\r\nAttributeError: module 'tensorflow._api.v1.compat.v2.compat' has no attribute 'v1'\r\n```\r\n\r\nFortunately I was able to find the solution to this error myself here: https://stackoverflow.com/questions/67703514/tensorflow-issue-google-colab-tensorflow-api-v1-compat-v2-has-no-attribute\r\n\r\n\r\n\r\n", "Closing this issue since it's resolved now. Feel free to reopen if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51137\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51137\">No</a>\n"]}, {"number": 51136, "title": "Add lstm support for TFLM", "body": "Signed-off-by: Niranjan Yadla <nyadla@cadence.com>", "comments": ["@petewarden @advaitjain \r\nPlease review and let me know your comments..", "It looks like you might need to remove some of the functions you added to the portable version from the tensorflow/lite/kernels/internal/tensor_utils.h file. Here's a failing build:\r\n\r\nhttps://source.cloud.google.com/results/invocations/101404ff-682f-445d-b20e-d938616a6880/targets/%2F%2Ftensorflow%2Flite%2Fjava%2Fdemo%2Fapp%2Fsrc%2Fmain:TfLiteCameraDemo/log\r\n\r\n```\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/lite/kernels/internal/BUILD:759:11: C++ compilation of rule '//tensorflow/lite/kernels/internal:kernel_utils' failed (Exit 1): clang failed: error executing command external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target ... (remaining 77 argument(s) skipped)\r\nIn file included from tensorflow/lite/kernels/internal/kernel_utils.cc:19:\r\n./tensorflow/lite/kernels/internal/tensor_utils.h:142:13: error: redefinition of 'ApplyReluToVector'\r\ninline void ApplyReluToVector(const float* __restrict__ vector, int v_size,\r\n            ^\r\n./tensorflow/lite/kernels/internal/portable_tensor_utils.h:121:13: note: previous definition is here\r\ninline void ApplyReluToVector(const float* __restrict__ vector, int v_size,\r\n            ^\r\nIn file included from tensorflow/lite/kernels/internal/kernel_utils.cc:19:\r\n./tensorflow/lite/kernels/internal/tensor_utils.h:150:13: error: redefinition of 'ApplyRelu1ToVector'\r\ninline void ApplyRelu1ToVector(const float* __restrict__ vector, int v_size,\r\n            ^\r\n./tensorflow/lite/kernels/internal/portable_tensor_utils.h:129:13: note: previous definition is here\r\ninline void ApplyRelu1ToVector(const float* __restrict__ vector, int v_size,\r\n            ^\r\nIn file included from tensorflow/lite/kernels/internal/kernel_utils.cc:19:\r\n./tensorflow/lite/kernels/internal/tensor_utils.h:175:13: error: redefinition of 'ApplySignbitToVector'\r\ninline void ApplySignbitToVector(const float* __restrict__ vector, int v_size,\r\n            ^\r\n./tensorflow/lite/kernels/internal/portable_tensor_utils.h:137:13: note: previous definition is here\r\ninline void ApplySignbitToVector(const float* __restrict__ vector, int v_size,\r\n            ^\r\nIn file included from tensorflow/lite/kernels/internal/kernel_utils.cc:19:\r\n./tensorflow/lite/kernels/internal/tensor_utils.h:192:13: error: redefinition of 'ApplyActivationToVector'\r\ninline void ApplyActivationToVector(const float* __restrict__ vector,\r\n            ^\r\n./tensorflow/lite/kernels/internal/portable_tensor_utils.h:145:13: note: previous definition is here\r\ninline void ApplyActivationToVector(const float* __restrict__ vector,\r\n            ^\r\n4 errors generated.\r\n```", "Closed, since the PR has been pulled in https://github.com/tensorflow/tensorflow/commit/d76dffffa4b6b17066d5b049afd5323a807965ec"]}, {"number": 51135, "title": "Documentation link broken at projector.tensorflow.org", "body": "## URL(s) with the issue:\r\nhttps://projector.tensorflow.org/\r\n\r\n## Description of the issue (what needs changing):\r\n\r\nThe question mark icon in the top right of navbar is supposed to be a hyperlink to the documentation.\r\nThe icon is a hyperlink to https://www.tensorflow.org/get_started/embedding_viz , which is broken.\r\n", "comments": ["@fz-29 \r\nWe are looking in to this and will update you at the earliest.", "This is fixed now, I think this issue should be closed to avoid confusion. ", "Moving this to closed status as its resolved."]}, {"number": 51134, "title": "TFLite fails to convert when the Model has regex_split_with_offsets", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n```\r\ndef tokenize(self, x):\r\n    x = tf_text.case_fold_utf8(x)\r\n    x = tf_text.normalize_utf8(x, \"NFD\")\r\n    x = tf.strings.regex_replace(x, r\"\\p{Mn}\", \"\")\r\n    x = tf.strings.regex_replace(x, r\"\\p{Cc}|\\p{Cf}\", \" \")\r\n    x,_ ,_ = tf_text.regex_split_with_offsets(\r\n        x, self._delim_regex_pattern, self._keep_delim_regex_pattern,\r\n        \"BasicTokenizer\")\r\n    return x\r\n```\r\nTesting with a simpler delim and keep_delim inputs will also result in the conversion error.\r\nThe full model is in the following colab gist.\r\n#### Reference To [TensorFlow Model, and tflite conversion Colab] [Colab gist](https://colab.research.google.com/drive/1JRSaXPEy7_osjKj-JnYoYNo5pPhvbal3?usp=sharing)\r\n\r\nIn the gist, the model is shown to work before and after saving to SavedModel. \r\nThe code attempts to convert to tflite from SavedModel. \r\n\r\n### 3. Failure during conversion\r\nTFLite Fails to convert when model contains regex_split_with_offsets ops. \r\ntensorflow_text is imported, \r\nSelect Ops flex option is enabled. such as:\r\n`\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\r\n]`\r\n\r\nIn the example, there are other tf_text ops before problematic regex_split_with_offsets. \r\nWhen commenting away the line with tf_text.regex_split_with_offsets, it will convert successfully. \r\n\r\n### 4. (optional) Any other info / logs\r\nThe error prompt says the  \"Graph does not contain node: \". Full trace logs is available below and in the gist.\r\n\r\n```\r\n ---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    293                                                  debug_info_str,\r\n--> 294                                                  enable_mlir_converter)\r\n    295       return model_str\r\n\r\n4 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n     37       debug_info_str,\r\n---> 38       enable_mlir_converter)\r\n     39 \r\n\r\nException: Graph does not contain node: \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-82-51fa59e54032> in <module>()\r\n      5   tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n      6 ]\r\n----> 7 tflite_model = converter.convert()\r\n      8 with open(tfLite_filepath, 'wb') as f:\r\n      9   f.write(tflite_model)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    911     converter_kwargs.update(quant_mode.converter_flags())\r\n    912 \r\n--> 913     result = _convert_saved_model(**converter_kwargs)\r\n    914     if self.experimental_new_quantizer:\r\n    915       calibrate_and_quantize, flags = quant_mode.quantizer_flags(\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in convert_saved_model(saved_model_dir, saved_model_version, saved_model_tags, saved_model_exported_names, **kwargs)\r\n    725       None,  # input_data, unused\r\n    726       None,  # debug_info_str, unused\r\n--> 727       enable_mlir_converter=True)\r\n    728   return data\r\n    729 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    295       return model_str\r\n    296     except Exception as e:\r\n--> 297       raise ConverterError(str(e))\r\n    298 \r\n    299   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: Graph does not contain node: \r\n\r\n```", "comments": ["I can reproduce this issue. Thanks for the report!", "Ragged Tensors are not a simple tensor type. Returning objects should be one of TensorFlow Lite types. Just returning a Ragged Tensor object is not supported.\r\n\r\nFor example,\r\n```\r\n   x,_ ,_ = tf_text.regex_split_with_offsets(\r\n        x, self._delim_regex_pattern, self._keep_delim_regex_pattern,\r\n        \"BasicTokenizer\")\r\n    return {\"values\": x.values, \"shapes\": x.shapes}\r\n```\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51134\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51134\">No</a>\n", "Thank you for your hints. For those interested, I am able to successfully convert to tflite after introducing the line after regex_split_with_offsets with, \r\n```x = x.to_tensor()```"]}, {"number": 51133, "title": "Portable double promotion warning fix", "body": "", "comments": ["@petewarden @njeffrie \r\nPlease review and let me know comments ", "@nyadla-sys  Can you please resolve conflicts? Thanks!", "@nyadla-sys  Any update on this PR? Please. Thanks!", "these changes are already part of tensorflow lite github"]}, {"number": 51132, "title": "Conv2d native FP16 compute", "body": "This patch enables use of native FP16 hardware acceleration for Conv2D op, if available on system.\r\nIt is tested on ARM Neoverse N1 CPU. With native FP16 vectored instruction up to 45% improvement is\r\nobserved on CNN models (without/very minimal accuracy loss). It is showing very good results in\r\nmixed precision training as well. This will further improve with improved GEMM kernel for FP16.\r\n\r\nBy default native FP16 Conv2D ops are disabled. \r\nTo enable it, environment variable `TF_CONV2D_USE_FP16_ACCUMULATE` must be set to 1.\r\ni.e. without setting environment variable or in absence of environment variable there is no change\r\nin execution of Conv2D operation.\r\n\r\nDefault behavior is disabled. i.e. without environment variable\r\n```\r\n# export TF_CONV2D_USE_FP16_ACCUMULATE=0\r\n# python <TensorFlow Application>.py\r\n```\r\ndoes the same.\r\n\r\nTo Enable native FP16 accumulate\r\n```\r\n# export TF_CONV2D_USE_FP16_ACCUMULATE=1\r\n# python <TensorFlow Application>.py\r\n```\r\n\r\nSystem Software Configuration used to test/build:\r\nUbuntu 20.04.2 LTS\r\nGCC: gcc-11 (Ubuntu 11.1.0-1ubuntu1~20.04) 11.1.0\r\n", "comments": ["I'd still like to see isolated benchmarks, and accuracy numbers for the models tested.\r\n\r\nIt turns out the FP16 Conv2D op without dilations and with `VALID` padding on CPU currently uses a matmul instead of `SpatialConvolution`, as do point-wise convolutions.  The matmul currently does only use f16 (see [here](https://github.com/tensorflow/tensorflow/blob/6529765814de33bca75a68c43f99d50a4d8624a5/tensorflow/core/kernels/conv_ops.cc#L122)).\r\n\r\n", "I did accuracy check using CNN model trained using mixed precision for float16.\r\nBase model/source is taken from : Implementing AlexNet CNN Architecture Using TensorFlow 2.0+ and Keras | by Richmond Alake | Towards Data Science\r\n\r\nFollowing are the changes to base code base source:\r\n# added float16_mixed precision policy as per following link (https://www.tensorflow.org/guide/mixed_precision)\r\n\r\nfrom tensorflow.keras import mixed_precision\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_global_policy(policy)\r\n\r\n>>> Compute dtype: float16\r\n>>> Variable dtype: float32\r\n\r\n# replaced tensorboad callback with model check point to save most accurate model\r\n\r\ncheckpoint_filepath = '/tmp/checkpoint'\r\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint_filepath,\r\n    save_weights_only=True,\r\n    monitor='val_accuracy',\r\n    mode='max',\r\n    save_best_only=True)\r\n\r\nmodel.fit(train_ds,\r\n        epochs=20,\r\n        validation_data=val_ds,\r\n        validation_freq=1, verbose=0,\r\n        callbacks=[model_checkpoint_callback])\r\n\r\n# saved model to disk\r\nmodel.save('alexnet_cifar10.h5', save_format='h5')\r\n\r\n# did accuracy check using saved model\r\nmodel = tf.keras.models.load_model('alexnet_cifar10.h5')\r\ntest_loss, test_acc = model.evaluate(test_ds, verbose=2)\r\nprint(f'accuracy = {(test_acc*100):4.2f} %')\r\n\r\nFollowing are results for 3 different training cycles + accuracy checks\r\nTest cycle : 1\r\n\r\nAccuracy check on GPU\r\n100/100 - 8s - loss: 0.7470 - accuracy: 0.7416\r\naccuracy = 74.16 %\r\nAccuracy check on CPU\r\n100/100 - 11s - loss: 0.7470 - accuracy: 0.7413\r\naccuracy = 74.13 %\r\nAccuracy check on CPU with FP16 ACCUMULATE\r\n100/100 - 6s - loss: 0.7471 - accuracy: 0.7418\r\naccuracy = 74.18 %\r\n\r\nTest cycle : 2\r\n\r\nAccuracy check on GPU\r\n100/100 - 8s - loss: 0.7517 - accuracy: 0.7421\r\naccuracy = 74.21 %\r\nAccuracy check on CPU\r\n100/100 - 10s - loss: 0.7517 - accuracy: 0.7418\r\naccuracy = 74.18 %\r\nAccuracy check on CPU with FP16 ACCUMULATE\r\n100/100 - 6s - loss: 0.7517 - accuracy: 0.7424\r\naccuracy = 74.24 %\r\n\r\nTest cycle : 3\r\n\r\nAccuracy check on GPU\r\n100/100 - 8s - loss: 0.7521 - accuracy: 0.7463\r\naccuracy = 74.63 %\r\nAccuracy check on CPU\r\n100/100 - 13s - loss: 0.7522 - accuracy: 0.7462\r\naccuracy = 74.62 %\r\nAccuracy check on CPU with FP16 ACCUMULATE\r\n100/100 - 6s - loss: 0.7522 - accuracy: 0.7460\r\naccuracy = 74.60 %\r\n\r\nModel summary:\r\n\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nconv2d (Conv2D)              (None, 55, 55, 96)        34944\r\n_________________________________________________________________\r\nbatch_normalization (BatchNo (None, 55, 55, 96)        384\r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 27, 27, 96)        0\r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 27, 27, 256)       614656\r\n_________________________________________________________________\r\nbatch_normalization_1 (Batch (None, 27, 27, 256)       1024\r\n_________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2 (None, 13, 13, 256)       0\r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 13, 13, 384)       885120\r\n_________________________________________________________________\r\nbatch_normalization_2 (Batch (None, 13, 13, 384)       1536\r\n_________________________________________________________________\r\nconv2d_3 (Conv2D)            (None, 13, 13, 384)       1327488\r\n_________________________________________________________________\r\nbatch_normalization_3 (Batch (None, 13, 13, 384)       1536\r\n_________________________________________________________________\r\nconv2d_4 (Conv2D)            (None, 13, 13, 256)       884992\r\n_________________________________________________________________\r\nbatch_normalization_4 (Batch (None, 13, 13, 256)       1024\r\n_________________________________________________________________\r\nmax_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0\r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 9216)              0\r\n_________________________________________________________________\r\ndense (Dense)                (None, 4096)              37752832\r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 4096)              0\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 4096)              16781312\r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 4096)              0\r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 10)                40970\r\n=================================================================\r\nTotal params: 58,327,818\r\nTrainable params: 58,325,066\r\nNon-trainable params: 2,752\r\n\r\nTest sequence:\r\n\r\n# Training on GPU\r\nexport CUDA_VISIBLE_DEVICES=0\r\nexport TF_CONV2D_USE_FP16_ACCUMULATE=0\r\nrun training\r\n\r\n# Accuracy check on GPU\r\nexport CUDA_VISIBLE_DEVICES=0\r\nexport TF_CONV2D_USE_FP16_ACCUMULATE=0\r\nrun accuracy check\r\n\r\n# Accuracy check on CPU with FP32\r\nexport CUDA_VISIBLE_DEVICES=-1\r\nexport TF_CONV2D_USE_FP16_ACCUMULATE=0\r\necho \"Accuracy check on CPU\"\r\nrun accuracy check\r\n\r\n# Accuracy check on CPU with FP16 ACCUMULATE\r\nexport CUDA_VISIBLE_DEVICES=-1\r\nexport TF_CONV2D_USE_FP16_ACCUMULATE=1\r\nrun accuracy check\r\n", "@cantonios  Can you please review this PR ? Thanks!", "> I'd still like to see isolated benchmarks\r\n\r\nMicrobenchmarks for the affected convolution operation(s), and overall model inference numbers.", "Executed microbenchmark to exercise all possible paths of Conv2D op.\r\nConv2D can have three different paths (tensorflow/core/kernels/conv_ops.cc:81)\r\n1.\t1x1 filter\r\n2.\tInput image and filters are of same dimensions\r\n3.\tFilter 3x3 and above\r\n\r\nBenchmark is time measurement over Conv2D operation using timeit.\r\nInput image is always 1x512x512x3\r\nFilter in first case  : 1x1x3x1\r\nFilter in second case : 512x512x3x1 (Image and filter dimensions are same)\r\nFilter in third case \r\na.\t1x3x3x3\r\nb.\t7x7x3x64\r\n\r\nExample data:\r\ndtype=np.float16 or dtype=np.float32\r\na = np.random.rand(1,512,512,3).astype(dtype)\r\nf = np.random.rand(1,1,3,1).astype(dtype)\r\n\r\nTimeit is used over following operation with all the filters: \r\n\r\n_ = tf.nn.conv2d(image, filter, strides=[1, 1], padding='VALID')\r\n\r\nFollowing are the results in % gain/loss using 4 N1 cores\r\n\r\nFP16 vs FP16 ACCUMULATE\r\n        \t   1x1\tmxn\t        3x3\t        7x7\r\nFP16 ACC\t   3.77%\t0.00%\t29.81%\t37.39%\r\n\r\nFP32 vs FP16 and FP16 ACCUMULATE\r\n        \t        1x1\t        mxn\t         3x3\t        7x7\r\nFP16\t        12.73%\t31.17%\t-12.21%\t-10.47%\r\nFP16 ACC\t        16.98%\t31.17%\t13.95%\t23.01%\r\n\r\nNote: Without using FP16 accumulate flag SpatialConvolution (else part of condition) shows -ve gain. ", "This change is only affecting `functor::SpatialConvolution`.  Two of those three paths use `functor::MatMulConvFunctor`, *not* `SpatialConvolution`.  I would therefore expect only one of the convolution cases to actually show any gain/loss.\r\n\r\nIs that what your first benchmark results show?  The first 3.77% is actually just noise, so it's essentially (+ is a performance gain):\r\n\r\n|                                                                           |MatMulConvFunctor|           |SpatialConvolution|              |\r\n|--------------------------------------------|--------------------|-------|-------------------|--------|\r\n|  Filter size                                                         |  1x1                            | mxn  | 3x3                         | 7x7       |\r\n| TF_CONV2D_USE_FP16_ACCUMULATE = 0 |   baseline                 |           | baseline                 |              |\r\n| TF_CONV2D_USE_FP16_ACCUMULATE = 1 |  +0%                        |  +0% | +30%                     |  +40%  |\r\n\r\nI don't know if I understand the second set of benchmarks though.  Is the summary that without this flag, float16 convolutions with SpatialConvolution are slower than float32 convolutions, but with it float16 convolutions are faster?", "Yes your understanding is correct. When flag is not used packet flow is F16 -> F32 -> SpatialConvolution -> F16.", "Alright, then the second set of benchmarks are a bit misleading.  Yes, float16 is slower than float32 without the macro because we are essentially doing a float32 convolution plus extra casting float16 -> float32 -> float16.  The main gain is to skip the casting.\r\n\r\nAre you able to run the same benchmarks on an intel CPU?", "Along with casting ARMv8-FP16 is also losing processing power of NEON engine. In case of F32, 4 variables are processed in a go while in case of F16 8 variables can be processed in a go.\r\n\r\nYes benchmark is Functional on x86_64, But FP16 results are low as Intel is not having native FP16 packet support. Intel can take advantage of upcast to FP32 as emulated Eigne::half will be worse.\r\n", "> Along with casting ARMv8-FP16 is also losing processing power of NEON engine. In case of F32, 4 variables are processed in a go while in case of F16 8 variables can be processed in a go.\r\n\r\nRight, but this is not reflected in\r\n\r\n> FP16 12.73% 31.17% -12.21% -10.47%\r\n\r\nsince both cases are doing computation in float32.  That's all I mean... that the speed gain for fp16 is only due to the results in the first set of benchmarks (which does already include removing the cast + increased ISA throughput).\r\n\r\n> Yes benchmark is Functional on x86_64\r\n\r\nDo you have numbers (relative or absolute)?", "-ve gain is showing this in 3x3 and 7x7.\r\n1x1 and mxn are not using SpatialConvolution. These are going through matmul and tensor size/memory usage is half of F32.", "@asharma-ampere still awaiting intel benchmark results, if you have them.", "@asharma-ampere Any update on this PR? Please. Thanks!", "I tried same micro benchmark on Skylake 8160 (on 4 CPU cores).\r\nAs expected results are showing loss only. \r\n\r\nTF Build flags: \r\nbazel build --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --config=opt --copt=\"-O3\" --copt=\"-march=skylake-avx512\" --config=nogcp --config=nonccl --copt=-Wformat --copt=-Wformat-security --copt=-fstack-protector --copt=-fPIC --copt=-fpic --linkopt=-znoexecstack --linkopt=-zrelro --linkopt=-znow --linkopt=-fstack-protector --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\nFP32 vs FP16 and FP32 vs FP16 ACC\r\n|        | 1x1      | mxn      | 3x3      | 7x7      |\r\n|------- | -------- | -------- | -------- | -------- |\r\n|FP16    | -76.26%  | -87.27%  | -29.15%  | -20.77%  |\r\n|FP16 ACC| -76.21%  | -87.38%  | -77.34%  | -99.21%  |\r\n\r\nFP16 vs FP16 ACC\r\n|        | 1x1      | mxn      | 3x3      | 7x7      |\r\n|------- | -------- | -------- | -------- | -------- |\r\n|FP16 ACC| 0.21%    | -0.93%   | -68.02%  | -99.00%  |\r\n", "I assume the 1x1 and mxn results in the second table are just noise.\r\n\r\nThat is quite a significant loss if we try to do the convolutions in fp16.  This leads me to believe we should be doing the matmul versions (1x1, mxn)  in f32 on Intel (and arm - without native fp16 support) as well.\r\n\r\nThanks, this has been very helpful."]}, {"number": 51131, "title": "recompute_grad + keras: \"TypeError: Cannot convert a symbolic Keras input/output to a numpy array.\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro, Version 21H1\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.9.0\r\n\r\n**Describe the current behavior**\r\ntf.recompute_grad does not accept tf.keras.layers objects.\r\nInstead, I get: \"TypeError: Cannot convert a symbolic Keras input/output to a numpy array.\"\r\n\r\n**Describe the expected behavior**\r\nThe API documentation explicitly mentions tf.keras.layers objects.\r\nAlso, according to several comments on GitHub, it should work with keras layers.\r\n\r\n**Contributing**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Conv2D, Input\r\nfrom tensorflow.keras.models import Model\r\n\r\ndef block(x):\r\n    x = Conv2D(filters = 32, kernel_size = (3, 3))(x)\r\n    x = Conv2D(filters = 32, kernel_size = (3, 3))(x)\r\n    return x\r\nblock = tf.recompute_grad(block)\r\n\r\ninput = Input(shape = (128, 128, 3))\r\nblock_1 = block(input)\r\nblock_2 = block(block_1)\r\nmodel = Model(input, block_2)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n> ---------------------------------------------------------------------------\r\n> TypeError                                 Traceback (most recent call last)\r\n> <ipython-input-15-d90add487c0c> in <module>\r\n>      10 \r\n>      11 input = Input(shape = (128, 128, 3))\r\n> ---> 12 block_1 = block(input)\r\n>      13 block_2 = block(block_1)\r\n>      14 model = Model(input, block_2)\r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\ops\\custom_gradient.py in __call__(self, *a, **k)\r\n>     259 \r\n>     260   def __call__(self, *a, **k):\r\n> --> 261     return self._d(self._f, a, k)\r\n>     262 \r\n>     263 \r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\ops\\custom_gradient.py in decorated(wrapped, args, kwargs)\r\n>     213 \r\n>     214     if context.executing_eagerly():\r\n> --> 215       return _eager_mode_decorator(wrapped, args, kwargs)\r\n>     216     else:\r\n>     217       return _graph_mode_decorator(wrapped, args, kwargs)\r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\ops\\custom_gradient.py in _eager_mode_decorator(f, args, kwargs)\r\n>     456   flat_result = nest.flatten(result)\r\n>     457   # TODO(apassos) consider removing the identity below.\r\n> --> 458   flat_result = [gen_array_ops.identity(x) for x in flat_result]\r\n>     459 \r\n>     460   input_tensors = [ops.convert_to_tensor(x) for x\r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\ops\\custom_gradient.py in <listcomp>(.0)\r\n>     456   flat_result = nest.flatten(result)\r\n>     457   # TODO(apassos) consider removing the identity below.\r\n> --> 458   flat_result = [gen_array_ops.identity(x) for x in flat_result]\r\n>     459 \r\n>     460   input_tensors = [ops.convert_to_tensor(x) for x\r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py in identity(input, name)\r\n>    3954       pass\r\n>    3955     try:\r\n> -> 3956       return identity_eager_fallback(\r\n>    3957           input, name=name, ctx=_ctx)\r\n>    3958     except _core._SymbolicException:\r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py in identity_eager_fallback(input, name, ctx)\r\n>    3974 \r\n>    3975 def identity_eager_fallback(input, name, ctx):\r\n> -> 3976   _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [])\r\n>    3977   _inputs_flat = [input]\r\n>    3978   _attrs = (\"T\", _attr_T)\r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in args_to_matching_eager(l, ctx, allowed_dtypes, default_dtype)\r\n>     271 \r\n>     272       if tensor is None:\r\n> --> 273         tensor = ops.convert_to_tensor(\r\n>     274             t, dtype, preferred_dtype=default_dtype, ctx=ctx)\r\n>     275 \r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py in wrapped(*args, **kwargs)\r\n>     161         with Trace(trace_name, **trace_kwargs):\r\n>     162           return func(*args, **kwargs)\r\n> --> 163       return func(*args, **kwargs)\r\n>     164 \r\n>     165     return wrapped\r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n>    1564 \r\n>    1565     if ret is None:\r\n> -> 1566       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n>    1567 \r\n>    1568     if ret is NotImplemented:\r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n>     337                                          as_ref=False):\r\n>     338   _ = as_ref\r\n> --> 339   return constant(v, dtype=dtype, name=name)\r\n>     340 \r\n>     341 \r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name)\r\n>     262     ValueError: if called on a symbolic tensor.\r\n>     263   \"\"\"\r\n> --> 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n>     265                         allow_broadcast=True)\r\n>     266 \r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n>     274       with trace.Trace(\"tf.constant\"):\r\n>     275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n> --> 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n>     277 \r\n>     278   g = ops.get_default_graph()\r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n>     299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):\r\n>     300   \"\"\"Implementation of eager constant.\"\"\"\r\n> --> 301   t = convert_to_eager_tensor(value, ctx, dtype)\r\n>     302   if shape is None:\r\n>     303     return t\r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n>      96       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n>      97   ctx.ensure_initialized()\r\n> ---> 98   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n>      99 \r\n>     100 \r\n> \r\n> ~\\.conda\\envs\\histology\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\keras_tensor.py in __array__(self)\r\n>     252 \r\n>     253   def __array__(self):\r\n> --> 254     raise TypeError(\r\n>     255         'Cannot convert a symbolic Keras input/output to a numpy array. '\r\n>     256         'This error may indicate that you\\'re trying to pass a symbolic value '\r\n> \r\n> TypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.\r\n", "comments": ["@Danny4 \r\n\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo. To know more refer to:https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n\r\nI have reproduced the code with minimal changes.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/823a4d13ebd2baa005248c6b0aba4533/untitled161.ipynb)\r\nCould you please refer the similar issues [link](https://github.com/tensorflow/tensorflow/issues/47311) .Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51131\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51131\">No</a>\n"]}, {"number": 51130, "title": "Update sqlite to sqlite-amalgamation-3360000", "body": "This PR updates sqlite to sqlite-amalgamation-3360000 (latest version).\r\n\r\nThis PR is loosely related to #51124 (not directly related).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 51129, "title": "Fix double-promotion warnings", "body": "Signed-off-by: Niranjan Yadla <nyadla@cadence.com>", "comments": ["I've created cl/388510144. Closing this in favor of that CL."]}, {"number": 51128, "title": "[Intel oneDNN] static analysis scan fixes based on r2.6 release tag", "body": "Few fixes based on r2.6 release tag to fix some static code analyzer complains", "comments": ["@penpornk  Can you please review this PR ? Thanks!", "@nammbash Can you please check @penpornk's comments and keep us posted ? Thanks!", "@nammbash Any update on this PR? Please. Thanks!", "> @nammbash Any update on this PR? Please. Thanks!\r\n\r\n@gbaned  I did go ahead and provide some insights into what the static analysis tool complained about here: \r\nhttps://github.com/tensorflow/tensorflow/pull/51128#discussion_r722458318", "@penpornk Can you please assist on above comments from @nammbash. Thanks!", "@penpornk Any update on this PR? Please. Thanks!", "This PR is outdated. Please close it. "]}, {"number": 51127, "title": "No information in official doc regarding implementation of class_weight in customized model.fit() method", "body": "Hi,\r\nI have customized the standard model.fit() method based on the help/steps described in the official [page](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). Now, I would like to use the class_weight in this customized function but I do not see any help regarding class_weight implementation in official [page](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#supporting_sample_weight_class_weight) under the heading [Supporting sample_weight & class_weight](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#supporting_sample_weight_class_weight). Kindly help me to understand how to implement class_weight in this case?\r\n\r\n\r\nBest Regards", "comments": ["Hi,\r\nAny update on this issue?\r\n\r\n\r\nBest Regards", "@n33lkanth It looks like your Issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. As ([previously announced](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999/)), all future development of Keras is expected to happen in the [github.com/keras-team/keras](github.com/keras-team/keras) repository. If your issue lies with the TF-Core area please comment back with your explanation and we can look into it further. Thanks!", "Hi @saikumarchalla ,\r\nThank you for your feedback. I raised this issue here as it is related to the TensorFlow official documentation. Do you mean that the doc I mentioned in previous post is not Tensorflow official doc?", "Yes this has been moved to Keras repo , please check the related github page here https://github.com/keras-team/keras-io/blob/master/guides/customizing_what_happens_in_fit.py , any updated information will be submitted in keras repo. Thank you ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51127\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51127\">No</a>\n", "Dear @rthadur,\r\nThank you for the response. However, the question still remains the same. The [link](https://github.com/keras-team/keras-io/blob/master/guides/customizing_what_happens_in_fit.py) shared by you discusses the usage of _sample_weight_ only. There is no information about the usage of _class_weight_ in the customized model.fit() method.\r\n\r\n\r\nBest Regards"]}, {"number": 51126, "title": "Question: Can we get all the intermidiate tensors when using gpu(opencl) delegate??", "body": "Hi guys, i wanna ask if there is any method that we can get the intermidiate tensors on gpu(opencl) delegate.   Which means, when delegate invoke is finished , all the intermidiate tensors' cl_mem are NAIVE stored and we can WRITE into a cpu ptr which is hold by TfliteTensor form the marched tensor id on CPU.   \r\n\r\nI see there is TensorTieFactory for subgraph global input & output's conversion. However I'm not really clear some details. \r\nCould you help me ? Thanks a lot.!\r\n\r\n\r\n", "comments": ["@impjdi could you take a look at this?", "Not easily achieved at the code level, as we implement https://arxiv.org/abs/2001.03288 buffers used by intermediate tensors are reused and overwritten.\r\n\r\nHowever, one quick hack you can do is the following:\r\n- Assume the output of a CONV_2D is to be inspected.  Let that tensor be A.\r\n- Let the rest of the network intact, but attach a ADD 0 or MUL 1 to A, which is essentially a no-op.  Let its output be B.\r\n- Declare B to be the graph output when you use tflite_converter.\r\n\r\nIn case TFLite tries to do something smart and prunes out ADD 0 or MUL 1, change it to ADD 0.0000001 or MUL 0.99999999.", "> Not easily achieved at the code level, as we implement https://arxiv.org/abs/2001.03288 buffers used by intermediate tensors are reused and overwritten.\r\n> \r\n> However, one quick hack you can do is the following:\r\n> \r\n> * Assume the output of a CONV_2D is to be inspected.  Let that tensor be A.\r\n> * Let the rest of the network intact, but attach a ADD 0 or MUL 1 to A, which is essentially a no-op.  Let its output be B.\r\n> * Declare B to be the graph output when you use tflite_converter.\r\n> \r\n> In case TFLite tries to do something smart and prunes out ADD 0 or MUL 1, change it to ADD 0.0000001 or MUL 0.99999999.\r\n\r\nI really appreciate your reply, i still got one more question. If we cannot get intermidiate buffers from internal op of graph. How can we debug the result of ops in graph if graph's result is wrong\uff1f(**assume op single test is passed, but op may not be fully tested or there comes some platform-related bugs.**)\r\n\r\nBTW,  i figure we can use MemoryStrategy::NAIVE when create gpu buffers so that buffers won't be reused. Am I wrong?\r\n", "@impjdi Please take a look.  ", "> How can we debug the result of ops in graph if graph's result is wrong\uff1f\r\n\r\nWhile we can't rule out the possibility of a bug in the shader, it's often FP16 not being good enough (precision-wise), to do the math computation.  See whether FP32 works fine; if it does, you know it's a precision issue.\r\n\r\nIn our team, we bisect the graph and inspect the output.  Unfortunately, it's not easily achieved with an automatic tool, but you need to visually inspect the graph and find the right cut off point by yourself.\r\n\r\n> i figure we can use MemoryStrategy::NAIVE when create gpu buffers so that buffers won't be reused.\r\n\r\nOh, I didn't know naive was one of the possible options.  If it's the same \"naive\" as in the paper (and I assume it is as it's the same dude who wrote the paper and implemented it), it may do the work.  I have never used it myself though.\r\n\r\n", "@impjdi  thank you so much !  \r\n**one last question about gpu layout:**\r\n\r\nseems like gpu(opencl) uses (C/4)HWB(C4) layout converted from BHWC(CPU layout)\uff0cwhy choose this layout\uff1f maybe BHW(C4) or BH(C/4)W(C4) will be better?  These are my reasons: \r\n\r\n1. data from different batches are independent; \r\n2. channel data usually are accumulated together in conv ops which costs most of inference time in most CNN.  \r\n\r\nSo Batch-size being outermost will reduce Scattered-access of threads in the same warp/wave as much as possible and Channel-size being innermost will reduce loop times when doing channel-accumulate. Plus, Channel-size being innermost will be more flexible in boundary process.\r\n\r\n", "> @impjdi thank you so much !\r\n> **one last question about gpu layout:**\r\n> \r\n> seems like gpu(opencl) uses (C/4)HWB(C4) layout converted from BHWC(CPU layout)\uff0cwhy choose this layout\uff1f maybe BHW(C4) or BH(C/4)W(C4) will be better? These are my reasons:\r\n> \r\n> 1. data from different batches are independent;\r\n> 2. channel data usually are accumulated together in conv ops which costs most of inference time in most CNN.\r\n> \r\n> So Batch-size being outermost will reduce Scattered-access of threads in the same warp/wave as much as possible and Channel-size being innermost will reduce loop times when doing channel-accumulate. Plus, Channel-size being innermost will be more flexible in boundary process.\r\n\r\n@impjdi please take a look.  thank you so much.", "While I don't have any documentation to share, that's what the owner of OpenCL profiled and determined to be the fastest performing memory layout.  This usually goes with the parallelization of the workload and how GPU threads run.", "@FdyCN Is this still an issue?\r\nPlease refer to the [comment](https://github.com/tensorflow/tensorflow/issues/51126#issuecomment-896324362) above.\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51125, "title": "WARNING:tensorflow:AutoGraph could not transform <function> ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, following this tutorial: https://www.tensorflow.org/text/tutorials/text_generation\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary (via pip)\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: NVIDIA RTX A4000, 16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nTensorflow throws a warning when mapping a function to a dataset. The stacktrace told me to report this (see full stack trace below):\r\n\r\n\r\n```\r\nWARNING: AutoGraph could not transform <function split_input_target at 0x7f24c012aa60> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function split_input_target at 0x7f24c012aa60>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nNo warning\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): No\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nCode taken from [tutorial](https://www.tensorflow.org/text/tutorials/text_generation):\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers.experimental import preprocessing\r\n\r\nimport numpy as np\r\nimport os\r\nimport time\r\n\r\npath_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\r\n\r\nwith open(path_to_file, 'rb') as f:\r\n    text = f.read().decode(encoding='utf-8')\r\n\r\nvocab = sorted(set(text))\r\n\r\n# process the text\r\n\r\nexample_texts = ['abcdefg', 'xyz']\r\n# unicode_split = tokenizer, split by unicode character\r\nchars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\r\n\r\n# StringLookup maps strings to integers, vocab list is single chars\r\nids_from_chars = preprocessing.StringLookup(\r\n            vocabulary=list(vocab), mask_token=None)\r\n\r\nids = ids_from_chars(chars)\r\n\r\n# inverted operation\r\n\r\nchars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\r\n    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\r\n\r\nchars = chars_from_ids(ids)\r\n# tf & numpy way of joining a list:\r\ntf.strings.reduce_join(chars, axis=-1).numpy()\r\n\r\ndef text_from_ids(ids):\r\n  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\r\n\r\n\r\n# create training dataset\r\n\r\nall_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\r\n\r\nids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\r\n\r\nfor ids in ids_dataset.take(10):\r\n    print(chars_from_ids(ids).numpy().decode('utf-8'))\r\n\r\nseq_length = 100\r\nexamples_per_epoch = len(text) // (seq_length+1)\r\n\r\nsequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\r\n\r\nfor seq in sequences.take(5):\r\n  print(text_from_ids(seq).numpy())\r\n\r\n# create input and label pairs\r\n# input: current character\r\n# label: next character\r\n\r\ndef split_input_target(sequence):\r\n    input_text = sequence[:-1]\r\n    target_text = sequence[1:]\r\n    return input_text, target_text\r\n\r\nsplit_input_target(list(\"Tensorflow\"))\r\n\r\ndataset = sequences.map(split_input_target)\r\n```\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\nFull error:\r\n\r\n```\r\nINFO:tensorflow:Converted call: <function split_input_target at 0x7f24c012aa60>\r\n    args: (<tf.Tensor 'args_0:0' shape=(101,) dtype=int64>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function split_input_target at 0x7f24c012aa60>\r\n    args: (<tf.Tensor 'args_0:0' shape=(101,) dtype=int64>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f24c012aa60>: default rule\r\nNot allowed: <method-wrapper '__call__' of function object at 0x7f24c012aa60>: default rule\r\nINFO:tensorflow:Not allowed: <function split_input_target at 0x7f24c012aa60>: default rule\r\nNot allowed: <function split_input_target at 0x7f24c012aa60>: default rule\r\nINFO:tensorflow:<function split_input_target at 0x7f24c012aa60> is not cached for subkey ConversionOptions[{}]\r\n<function split_input_target at 0x7f24c012aa60> is not cached for subkey ConversionOptions[{}]\r\nINFO:tensorflow:Error transforming entity <function split_input_target at 0x7f24c012aa60>\r\nTraceback (most recent call last):\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/parser.py\", line 154, in parse_entity\r\n    original_source = inspect_utils.getimmediatesource(entity)\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\", line 151, in getimmediatesource\r\n    lines, lnum = inspect.findsource(obj)\r\n  File \"/usr/lib/python3.8/inspect.py\", line 798, in findsource\r\n    raise OSError('could not get source code')\r\nOSError: could not get source code\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 432, in converted_call\r\n    converted_f = _convert_actual(target_entity, program_ctx)\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 274, in _convert_actual\r\n    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 286, in transform\r\n    return self.transform_function(obj, user_context)\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 470, in transform_function\r\n    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 346, in transform_function\r\n    node, source = parser.parse_entity(fn, future_features=future_features)\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/parser.py\", line 156, in parse_entity\r\n    raise ValueError(\r\nValueError: Unable to locate the source code of <function split_input_target at 0x7f24c012aa60>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nError transforming entity <function split_input_target at 0x7f24c012aa60>\r\nTraceback (most recent call last):\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/parser.py\", line 154, in parse_entity\r\n    original_source = inspect_utils.getimmediatesource(entity)\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\", line 151, in getimmediatesource\r\n    lines, lnum = inspect.findsource(obj)\r\n  File \"/usr/lib/python3.8/inspect.py\", line 798, in findsource\r\n    raise OSError('could not get source code')\r\nOSError: could not get source code\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 432, in converted_call\r\n    converted_f = _convert_actual(target_entity, program_ctx)\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 274, in _convert_actual\r\n    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 286, in transform\r\n    return self.transform_function(obj, user_context)\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 470, in transform_function\r\n    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 346, in transform_function\r\n    node, source = parser.parse_entity(fn, future_features=future_features)\r\n  File \"/redacted//.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/parser.py\", line 156, in parse_entity\r\n    raise ValueError(\r\nValueError: Unable to locate the source code of <function split_input_target at 0x7f24c012aa60>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nWARNING:tensorflow:AutoGraph could not transform <function split_input_target at 0x7f24c012aa60> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function split_input_target at 0x7f24c012aa60>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function split_input_target at 0x7f24c012aa60> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function split_input_target at 0x7f24c012aa60>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n```\r\n", "comments": ["@mrwunderbar666 \r\nI was able to reproduce the issue in tf2.5 with no warning. Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/98bdf8b059c2f8d8b081e6b7276d7699/untitled160.ipynb).Thanks", "@UsharaniPagadala \r\nI guess it is because I ran my code snipped in python repl. No error when executed as file.\r\nNevertheless, the warning told me to report this to the tensorflow devs", "@mrwunderbar666 \r\n\r\nCould you please refer similar issues [#30895](https://github.com/tensorflow/tensorflow/issues/30895) and [#28316](https://github.com/tensorflow/tensorflow/issues/28316) and let us know if it helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51125\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51125\">No</a>\n"]}, {"number": 51124, "title": "Upgrade Sqlite3 to fix CVE-2021-20227", "body": "sqlite 3.34.0", "comments": ["The latest master branch of tensorflow uses sqlite 3.35.5 so I think this vulnerability CVE-2021-20227 does not apply any more:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace2.bzl#L359\r\n\r\nHaving said that, it will not hurt to upgrade sqlite to the latest version (3.36.0). I have created a PR #51130 for that.", "@tachezhou1989 We see that the PR is merged .Could you please move this ticket to closed status ?Thanks! "]}, {"number": 51123, "title": "Need help: How to build tensorflow-lite with \"Pyro\" version of Yocto, for ARM platform", "body": "Hi,\r\n\r\nI need to build tensorflow-lite on \"Pyro\" version of Yocto for one of my Projects, for ARM platform\r\nCan someone point me to fetch the \"*.bb\" file for the same ? Or the steps to build the same from scratch ?\r\n\r\nThanking in advance", "comments": ["@terryheo could you take a look at this?", "There is no *official* recipe for Yocto.\r\nRegarding cross compilation, please check the following.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_arm", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51123\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51123\">No</a>\n"]}, {"number": 51122, "title": "Issue with tf.nn.conv2d restarting kernel", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n\r\n- TensorFlow installed from (source or binary):\r\nPip install\r\n\r\n- TensorFlow version (use command below):\r\nv2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n\r\n- Python version:\r\n3.8\r\n\r\n- CUDA/cuDNN version:\r\nCUDA 11.2; cuDNN 8.2\r\n\r\n- GPU model and memory:\r\nNvdia RTX 3060Ti 8GB\r\n\r\n**Describe the current behavior**\r\nI am running the exmple script found on the tf.nn.conv2d documentaion page (found here: https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\r\n\r\nUpon running the final line in spyder the kernel immediately restarts with no error message displayed\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx_in = np.array([[\r\n  [[2], [1], [2], [0], [1]],\r\n  [[1], [3], [2], [2], [3]],\r\n  [[1], [1], [3], [3], [0]],\r\n  [[2], [2], [0], [1], [1]],\r\n  [[0], [0], [3], [1], [2]], ]])\r\nkernel_in = np.array([\r\n [ [[2, 0.1]], [[3, 0.2]] ],\r\n [ [[0, 0.3]],[[1, 0.4]] ], ])\r\nx = tf.constant(x_in, dtype=tf.float32)\r\nkernel = tf.constant(kernel_in, dtype=tf.float32)\r\ntf.nn.conv2d(x, kernel, strides=[1, 1, 1, 1], padding='VALID')\r\n```\r\n", "comments": ["@mdtm1g14 I tried to run your code on **Colab** & **Spyder** with **`TF v2.5`** , and didn't face  issue as reported here .Please find the [gist](https://colab.research.google.com/gist/sushreebarsa/b3bc8a9bfce3e3c522c324f6f6e8dafa/untitled354.ipynb) of colab & snapshot of the output in spyder for your reference.Thank you!\r\n\r\n![IMG-20210805-WA0004](https://user-images.githubusercontent.com/84765720/128505340-e9d0bb5d-2f4b-406b-b612-f2ae4c68c571.jpeg)\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51122\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51122\">No</a>\n"]}, {"number": 51121, "title": "FAILED: Build did NOT complete successfully", "body": "**System information**\r\n\r\n- OS Platform and Distribution: 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5.2\r\n- Python version: 3.8.10\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 10.2 / 8.2.2\r\n- GPU model and memory: Quadro K2000M / 2GB\r\n\r\n**Configure**\r\n\r\n`cat .tf_configure.bazelrc`\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild --config=tensorrt\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda-10.2\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"3.0\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/cuda-10.2/lib64:/usr/local/lib\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/x86_64-linux-gnu-gcc-9\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-Wno-sign-compare\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_env=LD_LIBRARY_PATH\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\n```\r\n\r\n**Build a TensorFlow package builder with GPU support:**\r\n\r\nbazel build --config=cuda [--config=option] //tensorflow/tools/pip_package:build_pip_package\r\n\r\nThe error:\r\n\r\n```\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=190\r\nINFO: Reading rc options for 'build' from /home/kevin/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/kevin/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /home/kevin/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=tensorrt --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-10.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0 --action_env LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:/usr/local/lib --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda\r\nINFO: Found applicable config definition build:short_logs in file /home/kevin/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/kevin/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:tensorrt in file /home/kevin/tensorflow/.bazelrc: --repo_env TF_NEED_TENSORRT=1\r\nINFO: Found applicable config definition build:cuda in file /home/kevin/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:cuda in file /home/kevin/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:linux in file /home/kevin/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/kevin/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/toolchains/archive/d781e89e2ee797ea7afd0c8391e761616fc5d50d.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/cdf6d36e9a5c07770160ebac25b153481c37a247.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: /home/kevin/.cache/bazel/_bazel_kevin/34b262a4f3c2d5656d47ec6453465346/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nERROR: Skipping '[--config=option]': no such target '//:[--config=option]': target '[--config=option]' not declared in package '' defined by /home/kevin/tensorflow/BUILD\r\nERROR: no such target '//:[--config=option]': target '[--config=option]' not declared in package '' defined by /home/kevin/tensorflow/BUILD\r\nINFO: Elapsed time: 0.089s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\nI see URLs not available and return 404.\r\n\r\nhttp://mirror.tensorflow.org/github.com/tensorflow/toolchains/archive/d781e89e2ee797ea7afd0c8391e761616fc5d50d.tar.gz\r\nhttp://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/cdf6d36e9a5c07770160ebac25b153481c37a247.tar.gz\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51121\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51121\">No</a>\n"]}, {"number": 51120, "title": "ValueError(Missing predictions) in tf.estimator", "body": "I defined the two spec for eval and prediction with tf.estimator, My goal is to get **predictions**'s content, like[1,1,1], not printed like \"Tensor(shape=(),dtype=float32)\"\r\n- here is the code\r\n```python\r\ndef model_fn():\r\n    ...\r\n    predictions = tf.argmax(logits,axis=-1,name=\"pred_node\") \r\n    pred = {'pred':prediction}\r\n    acc = tf.metrics.accuracy(labels,predictions)\r\n    eval_metrics={\"acc\":acc}\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        eval_spec = tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            loss=total_loss,\r\n            eval_metrics=eval_metrics,\r\n            scaffold_fn=scaffold_fn\r\n        )\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        eval_spec = tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            predictions=pred\r\n        )\r\n    return eval_spec\r\n```\r\nand when do eval mode using `estimator.eval()`, it can print the content of *eval_metrics*,acc(0.966) however, when i use `estimator.predict()`, raised ValueError(Missing predictions).\r\n\r\nI also tried these ways to get tensor predictions:\r\n- using session to `session.run(predictions)`, and `tf.print(predictions)`, failed\r\n- restore model.ckpt to model.pb to `tf.get_default_graph.get_tensor_by_name()`,I got all of argmax nodes' output, Then i found the node that calculates `predictions` not in the graph ????\r\n\r\nFinally I found in [estimator source code](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/model_fn.py) that:\r\n```python\r\nRaises:\r\n    ValueError: If:\r\n      - predictions is None and we are in predict mode.\r\n      - predictions `Tensor` is not in default_graph or else it is a dict of\r\n        `Tensor` where at least one is not in default_graph.\r\n    TypeError:  If predictions is not a `Tensor` or dict of `Tensor`.\r\n  \"\"\"\r\n```\r\nbut i got acc(0.996969907284) using predictions, which means this tensor is not NONE. So it must be the second cause, **not in default_graph**, the same as my second try.\r\n\r\nSo how should I deal with this? how to add this argmax node to graph?\r\n\r\nThanks in advance for answering.", "comments": ["@waynamigo \r\nCould you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51118, "title": "Cherrypick saved model cli fix", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51118) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 51117, "title": "Update version numbers for TensorFlow 2.6.0-rc2", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 6 -> 6\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.6.0-rc1\" found in source directory \n\"tensorflow/\". Good.\nWARNING: Below are potentially instances of lingering old version string \n\"2.6.0rc1\" in source directory \"tensorflow/\" that are not updated by this \nscript. Please check them manually!\ntensorflow/tools/pip_package/setup.py:106:2.6.0rc1\n```", "comments": []}, {"number": 51116, "title": "Trivial typo", "body": "Hope to be edited. Thanks.", "comments": ["It looks like your PR relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. Thankyou.\r\n@fchollet, @qlzh727"]}, {"number": 51115, "title": "TFLite flex delegate on x86_64 failed to link", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.8.10\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 4.1.0\r\n- GCC/Compiler version (if compiling from source): 11.1.0\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: RTX 2080Ti 11GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am trying to compile the C++ API of flex delegate on x86_64 desktop but failed due to link errors (see below). But if i compile the same target with option `--config=android_arm64` everything runs smoothly. I am wondering if the C++ API of flex delegate is supported and if it is how to avoid the link errors. The log file is attached below. Please kindly check it. Thanks!\r\n\r\nCompiling\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`bazel build --compilation_mode=opt --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 --experimental_ui_max_stdouterr_bytes=1073741819 -- //tensorflow/lite/delegates/flex:libtensorflowlite_flex.so`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[tflite_flex.log](https://github.com/tensorflow/tensorflow/files/6919287/tflite_flex.log)\r\n", "comments": ["@thaink could you take a look at this?", "Can you add `--config=monolithic` and try again.\r\nBeside, If you are building for x86_64 PC, it is true not to add `--config=android_arm64`.\r\nHowever for Android, you should use `--config=android_arm64` or `--config=android_x86_64` depends on the target platform.", "Yeah, adding `--config=monolithic` works for me. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51115\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51115\">No</a>\n"]}, {"number": 51114, "title": "[doc fix] Inline help of input_length of the embedding layer", "body": "To my understanding, the input_length needs to be specified for the embedding layer only if it is upstream of the flatten and then the dense layers, which means those layers are downstream of the embedding layer.", "comments": ["@HunderlineK Can you please check @qlzh727's comments and keep us posted ? Thanks!", "And if you do edit this there, try using unambiguous wording. \r\n\r\n\"input_length may be required if you pass this layer's output to a layer that required a fixed size (like Flatten + Dense)\"", "@gbaned @MarkDaoust @qlzh727  Thanks, will update in the Keras repository with an unambiguous wording."]}]