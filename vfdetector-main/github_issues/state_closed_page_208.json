[{"number": 48424, "title": "TypeError: apply_gradients() got an unexpected keyword argument 'global_step' in tf_agents.agents.ReinforceAgent", "body": "<em>First of all I tryed everything in my code and it didn't changed the outcome i dont know where is this global_step coming from not from my code then i belive the problem may be with Tensor if its not i am sorry for posting as Bug Issue but before canceling it Could you at least tell me how can i train my model??\r\nedit: other agents work with the same layout ex DqnAgent worked well \r\n</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock and non stock\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 / ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NoN\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): NON\r\n- GCC/Compiler version (if compiling from source):  NON\r\n- CUDA/cuDNN version:  NON gpu also doesn't work\r\n- GPU model and memory: GTX1070 6GB\r\n\r\n**Describe the current behavior**\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tf_agents\\agents\\reinforce\\reinforce_agent.py in _train(self, experience, weights)\r\n    286                                           self.train_step_counter)\r\n    287 \r\n--> 288     self._optimizer.apply_gradients(\r\n    289         grads_and_vars, global_step=0)\r\n    290 \r\n\r\nTypeError: apply_gradients() got an unexpected keyword argument 'global_step'\r\n\r\n**Describe the expected behavior**\r\n\r\ntrain_loss = tf_agent.train(experience,global_step= tf.Variable(1, name=\"global_step\"))\r\nprint(train_loss)\r\n\r\n*** train_los***\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```py\r\nimport pyxinput\r\nimport time\r\nimport cv2\r\nfrom PIL import ImageGrab\r\nimport numpy as np\r\nimport keyboard\r\nimport tensorflow\r\nimport tf_agents\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport torch\r\n#from tf_agents.networks import actor_distribution_networ\r\n\r\n\r\nfrom tf_agents.policies import random_py_policy\r\n\r\nTensod_spec = tf_agents.specs.BoundedArraySpec(\r\n   (15,), dtype= np.float32 ,  name=\"XimputSpecs\"   , minimum=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], maximum=[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\r\n)\r\n\r\n\r\n\r\nTensod_spec2 = tf_agents.specs.TensorSpec(\r\n   [ 440 , 600 , 1], dtype= tf.int32 , name=\"ScreenSpecs\"\r\n)\r\n\r\n\r\nTensor_reward_spe = tf_agents.specs.TensorSpec(\r\n   [1,1], dtype= tf.int32 , name=\"Reward\"\r\n)\r\n\r\nFromEnv = tf_agents.specs.BoundedTensorSpec(shape=(440 , 600 , 1), dtype='uint8', name='observation', minimum=0, maximum=255)\r\nFromEnv2 = tf_agents.specs.BoundedTensorSpec(shape=(1 , 440 , 600 , 1), dtype=tf.int32, name='observation', minimum=0, maximum=255)\r\n\r\n\r\nfullscreen = [110,130,710,570]\r\n\r\nscreenpil = ImageGrab.grab(bbox=fullscreen)\r\nshowprint = np.array(screenpil)\r\ngrayscreen = cv2.cvtColor(showprint, cv2.COLOR_BGR2GRAY)\r\nscreenrect = cv2.cvtColor(grayscreen, cv2.COLOR_GRAY2BGR)\r\ngrayscreen = grayscreen.reshape( 440, 600, 1)\r\n\r\ntime_step_spec2 = tf_agents.trajectories.time_step.time_step_spec(\r\n   observation_spec= FromEnv ,\r\n   #reward_spec = Tensor_reward_spec\r\n)\r\n\r\ntime_step_spec = tf_agents.trajectories.time_step.time_step_spec(\r\n    observation_spec= FromEnv ,\r\n    \r\n    #reward_spec = Tensor_reward_spec\r\n)\r\n\r\n\r\nactor_net = tf_agents.networks.actor_distribution_network.ActorDistributionNetwork(\r\n   input_tensor_spec =FromEnv,\r\n   output_tensor_spec =  tf_agents.specs.tensor_spec.from_spec(Tensod_spec),\r\n   activation_fn = 'relu' ,\r\n   #conv_layer_params = [(25, 40, 2)] ,\r\n   fc_layer_params=(50,25,15) ,\r\n   #dtype =  'int32'\r\n)\r\nprint(actor_net)\r\n\r\ntrain_step_counter =tf.dtypes.cast(1, tf.int32)\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.003)\r\n\r\ntf_agent = tf_agents.agents.ReinforceAgent(\r\n   time_step_spec = time_step_spec,\r\n   action_spec = tf_agents.specs.tensor_spec.from_spec(Tensod_spec),\r\n   actor_network=actor_net,\r\n   optimizer=optimizer,\r\n   normalize_returns=True,\r\n   #train_step_counter= tf.Variable(1, name=\"global_step\")\r\n   )\r\ntf_agent.initialize()\r\n\r\ngrayscreen2 = grayscreen\r\ngrayscreen2 = grayscreen2.reshape(1 , 440, 600, 1)\r\ntime_step2 = tf_agents.trajectories.time_step.TimeStep(\r\n   step_type = tf_agents.trajectories.time_step.StepType.FIRST,\r\n   reward = tf.dtypes.cast(1, tf.float32) ,\r\n   discount = tf.dtypes.cast(1, tf.float32),\r\n   observation = grayscreen2\r\n)\r\n\r\npolicy_state = tf_agent.policy.get_initial_state(batch_size=1)\r\n\r\npolicy_step = tf_agent.policy.action(time_step2, policy_state)\r\nprint(policy_step)\r\n\r\n\r\nobserve = time_step2.observation\r\n#print(observe.dtype)\r\n#observe = observe.astype(int)\r\n#print(observe.shape)\r\n\r\nexperience = tf_agents.trajectories.trajectory.Trajectory(\r\naction= tf.compat.v2.Variable([tf.compat.v2.Variable(policy_step.action),tf.compat.v2.Variable(policy_step.action),tf.compat.v2.Variable(policy_step.action)]),\r\nreward = tf.compat.v2.Variable([[tf.compat.v2.Variable(time_step2.reward),tf.compat.v2.Variable(time_step2.reward),tf.compat.v2.Variable(time_step2.reward)]]),\r\nstep_type = tf.compat.v2.Variable([[tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.FIRST),tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.MID),tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.LAST)]]),\r\nobservation = tf.compat.v2.Variable([tf.compat.v2.Variable(observe),tf.compat.v2.Variable(observe),tf.compat.v2.Variable(observe)]),\r\npolicy_info = tf_agent.policy.info_spec,\r\nnext_step_type = tf.compat.v2.Variable([[tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.MID),tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.LAST),tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.LAST)]]),\r\ndiscount= tf.compat.v2.Variable([[tf.dtypes.cast(1, tf.float32),tf.dtypes.cast(1, tf.float32),tf.dtypes.cast(1, tf.float32)]]), \r\n\r\n)\r\n\r\ntrain_loss = tf_agent.train(experience)\r\nprint(train_loss)\r\n```\r\n\r\n`\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nFull erro\r\n\r\n``` py\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-16-ed3ca44dc79e> in <module>\r\n      1 #\r\n----> 2 train_loss = tf_agent.train(experience,global_step= tf.Variable(1, name=\"global_step\"))\r\n      3 print(train_loss)\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tf_agents\\agents\\tf_agent.py in train(self, experience, weights, **kwargs)\r\n    516 \r\n    517     if self._enable_functions:\r\n--> 518       loss_info = self._train_fn(\r\n    519           experience=experience, weights=weights, **kwargs)\r\n    520     else:\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tf_agents\\utils\\common.py in with_check_resource_vars(*fn_args, **fn_kwargs)\r\n    183         # We're either in eager mode or in tf.function mode (no in-between); so\r\n    184         # autodep-like behavior is already expected of fn.\r\n--> 185         return fn(*fn_args, **fn_kwargs)\r\n    186       if not resource_variables_enabled():\r\n    187         raise RuntimeError(MISSING_RESOURCE_VARIABLES_ERROR)\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tf_agents\\agents\\reinforce\\reinforce_agent.py in _train(self, experience, weights)\r\n    286                                           self.train_step_counter)\r\n    287 \r\n--> 288     self._optimizer.apply_gradients(\r\n    289         grads_and_vars, global_step=0)\r\n    290 \r\n\r\nTypeError: apply_gradients() got an unexpected keyword argument 'global_step'\r\n```\r\n`\r\n\r\nThank you for your time\r\n", "comments": ["Found This error is caused because of the OPTMIZER i am tring to find a working one you ned an optmizer that has apply_gradients if i find a good one i will post an example here ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48424\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48424\">No</a>\n"]}, {"number": 48423, "title": "Update bot_config.yml", "body": "", "comments": []}, {"number": 48422, "title": "[WIP] [XLA:GPU] cuBLASLT integration in XLA", "body": "", "comments": ["@AyanmoI  This PR is in draft, any update on this? Please. Thanks!", "> @AyanmoI This PR is in draft, any update on this? Please. Thanks!\r\n\r\nNot yet. Needs further testing and bug fixes.", "@AyanmoI This PR is in draft, any update on this? Please. Thanks!", "Closing this PR for now."]}, {"number": 48420, "title": "[CherryPick:r2.5] Use merge_call for optimizer distributed_apply if strategy doesn't support merge_call free path.", "body": "In principle even if the strategy doesn't support merge_call free path, distributed_apply can still be called w/o merge_call. However it would be slower than calling it with merge_call in pure eager mode, mainly because MS uses multiple python threads to run replica function, and those python threads are running sequentially.\n\nPiperOrigin-RevId: 367305737\nChange-Id: Ie2316fd62131c60eff605cf9f51e044c09ec9abc", "comments": []}, {"number": 48419, "title": "[ROCm] Minor updates to the XLA / LLVM for AMDGPU backend", "body": "## 1  Fix for ROCm CSB breakage - 210329\r\n\r\nThe following commit breaks the XLA unit-testcases on the ROCm platform\r\n\r\ncb3e0b2#diff-3916e937a690715a62df34b7dae19bbdd92d7c79b9d6712ba3605b8d208c8a7fR7\r\n\r\nThe above commit updates the LLVM pointer, which in turn picks a lot of changes in the AMDGPU backend implementation in LLVM. One of those changes is to use code-object-v4, by default, for the generated HSACOs (AMDGPU binaries). This is okay if you also use the corresponding HIP runtime, which is ROCm 4.1 or higher. But with ROCm 4.0.1 or older, the newer HSACO files do not get loaded properly by the older HIP runtime, resulting in errors like the following error\r\n\r\n```\r\ntensorflow/compiler/xla/service/gpu/tests/reduction_layout_normalizer_test.cc:62: Failure\r\nValue of: RunAndCompare(hlo_text, ErrorSpec{1e-5, 1e-5})\r\n  Actual: false (Internal: Failed to load HSACO: hipError_t(303))\r\nExpected: true\r\n```\r\n\r\nThis commit fixes the issue by changing the default code object version to v3, when TF is built with ROCm 4.0.1 or older.\r\n\r\n## 2  Adding support for enbale_ftz flag (flush denormals to zero) in the LLVM backend for AMDGPUs \r\n\r\n----------------------\r\n\r\nthanks to @ekuznetsov139 for helping out on the fixes.\r\n\r\n------------------------------\r\n\r\n/cc @cheshire @chsigg \r\n", "comments": []}, {"number": 48418, "title": "Deadlock when setting interop thread thread to 1 with a tf Dataset and tf function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux / colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): 2.4.1\r\n- TensorFlow version (use command below):\r\n- Python version: Colab\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nFor test purposes we usually set the nbr of threads to 1 for the interop and intra op threadings. When using the following code (to reduce batch size from an already batched dataset) we get a deadlock.\r\n\r\nWe only get the deadlock if:\r\n- inter nbr thread set to 1\r\n- and  \r\n-  _slicer is wrapped in tf function.\r\n\r\n**Describe the expected behavior**\r\nNo deadlocks\r\n**Standalone code to reproduce the issue**\r\n````\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\n@tf.function\r\ndef _slicer(dd, slice_):\r\n    return tf.nest.map_structure(lambda val: val[slice_[0]:slice_[1]], dd)\r\ndef get_small_batch_gen(ds: tf.data.Dataset, batch_size):\r\n    def gen_small_batch():\r\n        for batch in ds:\r\n            big_batch_size = tf.nest.flatten(tf.nest.map_structure(lambda x: x.shape[0], batch))\r\n            for i in range(big_batch_size[0] // batch_size):\r\n                yield _slicer(batch, tf.constant([i * batch_size, (i + 1) * batch_size]))\r\n    return tf.data.Dataset.from_generator(gen_small_batch, output_signature=ds.element_spec)\r\nsize = 102\r\nds = tf.data.Dataset.from_tensor_slices(({i: tf.RaggedTensor.from_row_splits(np.random.choice(10, size=size),\r\n                                                                                  tf.range(size + 1))\r\n                                              for i in ['a', 'b', 'c']}, tf.range(size))).batch(10)\r\nnext(iter(get_small_batch_gen(ds, 5)))\r\n`````\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@ymodak \r\nI ran the code shared and it runs infinite mode, please find the [gist here.](https://colab.research.google.com/gist/Saduf2019/4878e2cce2ea1786df07545b9f46becc/untitled583.ipynb)", "`tf.data.Dataset.from_generator` uses `PyFunc` op in its implementation, which uses an interop threadpool for its execution. if the Python method that it executes invokes any TF ops, these will need additional interop threadpool. This is unlikely to be changed.\r\n\r\nIf you would like to run your tests with a singleton interop threadpool, you should avoid using `tf.data.Dataset.from_generator` (or any other source of `PyFunc` such as `tf.py_function` or `tf.numpy_function`).", "ok thank you for the clarification ! so i'll need to use at least nbr interop thread i have nbr of python generator. \r\n\r\nWould there be any way to make that error more user friendly and explicitely raise instead of the deadlock ?"]}, {"number": 48417, "title": "[r2.5] Adding missing Windows libtensorflow files.", "body": "", "comments": []}, {"number": 48416, "title": "[r2.4] Adding missing Windows libtensorflow files.", "body": "", "comments": []}, {"number": 48415, "title": "[CherryPick:r2.5]Use the standard lib ast.unparse instead of astunparse, available in Python 3.9+.", "body": "PiperOrigin-RevId: 367434930\nChange-Id: Ieab1c5b76e54f00986ea4e6467714b1ee71199d4", "comments": []}, {"number": 48414, "title": "TF-TRT implement Range and RangeOptimal optimization profile strategies", "body": "This PR implements Range and RangeOptimal optimization profile strategies for TF-TRT dynamic shape mode.\r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.\r\n\r\nTracker: #45481 ", "comments": ["@tfeher  Can you please check @bixia1's comments and keep us posted ? Thanks!", "@tfeher Any update on this PR? Please. Thanks!"]}, {"number": 48413, "title": "License for using the TF logo", "body": "I would like to add the TF Logo to a commercial product. When can I obtain a copy of the licence for the logo?\r\n", "comments": ["Hi @johnpeach please drop me an email at ewj at google.com and I'll route you in the right direction.\r\n"]}, {"number": 48412, "title": "tf.keras.applications.EfficientNetBXs already contain a rescaling layer to preprocess input. It's confusing!", "body": "https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/applications/efficientnet.py#L735-L737\r\n`tf.keras.applications.efficientnet.preprocess_input `does nothing at all! \r\n`tf.keras.applications.EfficientNetBXs` already contain a rescaling layer to preprocess input. It's confusing!\r\n`tf.keras.applications.EfficientNetBXs` and `tf.keras.applications.efficientnet.preprocess_input`'s design and behavior are different from other tf.keras.applications modules.", "comments": ["Please take a look at this https://github.com/tensorflow/tensorflow/issues/45243#issuecomment-740845533 it applies to efficientnet as well.", "@UsharaniPagadala \r\n\r\nOS Platform: Linux Ubuntu 16.04\r\nDistribution: Tensorflow 2.3.0\r\n\r\n \r\n![image](https://user-images.githubusercontent.com/16998373/114114452-774e9880-9913-11eb-886a-bc958313dbc8.png)\r\n\r\n`tf.keras.applications.EfficientNetBX` already contains rescaling and normalization layers, the inputs pixel values will be scaled from [0, 255] to [-1, 1].  \r\n\r\n", "The [`preprocess_input`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet/preprocess_input?version=nightly) docs are updated in nightly version.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48412\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48412\">No</a>\n"]}, {"number": 48411, "title": "Having trouble building tf wheel from r2.5 branch", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 2.5\r\n- Python version: py36\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm building  from tf source r2.5 branch, getting the following error while trying to build the wheel:\r\n[91mERROR: Could not find a version that satisfies the requirement tf-estimator<2.6.0,>=2.5.0rc0 (from tf-nightly==2.5.0rc0) (from versions: none)\r\nERROR: No matching distribution found for tf-estimator<2.6.0,>=2.5.0rc0 (from tf-nightly==2.5.0rc0)\r\n\r\nFrom what I can see in this commit : https://github.com/tensorflow/tensorflow/commit/ecf0809b8f292636b07bcf10db71861709fd2802\r\nhas changed the dependency \r\nfrom:\r\n 'tf-estimator-nightly == 2.5.0.dev2021032501',\r\nto:\r\n 'tf-estimator >= 2.5.0rc0 , < 2.6.0',\r\n\r\nbut tf-estimator 2.5.0rc0 doesn't seem to be available.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["It looks like a mistypo of tensorflow_estimator, which has 2.5.0rc0", "https://github.com/tensorflow/tensorflow/pull/48488 has fixed the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48411\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48411\">No</a>\n"]}, {"number": 48410, "title": "TFLiteConverter: Support QuantizeAndDequantizeV4", "body": "This PR adds support for [`tf.quantization.quantize_and_dequantize_v2`](https://www.tensorflow.org/api_docs/python/tf/quantization/quantize_and_dequantize_v2) to the TFLite converter.\r\n\r\nUnder the hood it is implemented as a canonicalizer that converts `QuantizeAndDequantizeV2` (used by `tf.quantization.quantize_and_dequantize`) to `QuantizeAndDequantizeV4` (used by `tf.quantization.quantize_and_dequantize_v2`). This can be done since both ops share the same kernel implementation in the forward pass. Secondly the pattern in the TFLite converter is changed to handle `QuantizeAndDequantizeV4` including a unittest that verifies the behaviour.\r\n\r\nThis PR is a follow up on #47225 /cc @smit-hinsu", "comments": ["@liufengdb @teijeong @jpienaar could you review this PR?", "Thanks for the fast review!\r\n\r\n>  are there any differences between 2 and 4 representationally?\r\n\r\nNot as far as I can tell. They share the same implementation in the forward pass:\r\nhttps://github.com/tensorflow/tensorflow/blob/a86ff7943a1468fe21311d7274de8be78c4375ce/tensorflow/core/kernels/quantize_and_dequantize_op.cc#L419-L422\r\nso the only difference is in the gradient op which is not relevant in this case."]}, {"number": 48409, "title": "ImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from :pip install\r\n- TensorFlow version:1.13.1\r\n- Python version:3.5\r\n- Installed using: virtualenv, pip and conda?:\r\n- Bazel version (if compiling from source): none\r\n- CUDA/cuDNN version:10 and 7.4\r\n- GPU model and memory: GF -GTX1070Ti\r\n\r\n\r\n**PROBLEM**\r\nI am trying to download and use tensorflow-gpu version 1.13.1 and apparently I've done something wrong and I don't really know where. I am using cuda and cudnn versi\u00f3n that are compatible acording to \"https://www.tensorflow.org/install/source_windows\".\r\n\r\n\r\n**COMMANDS**\r\nconda create -n tensorflow1 pip python=3.5\r\npip install --upgrade tensorflow-gpu==1.13.1\r\npython\r\n>>>import tensorflow as tf\r\n\r\n\r\n\r\n**LOG**\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Users\\germa\\anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\r\n   * For TF-GPU - See point 1\r\n   * For TF-CPU - See point 2\r\n-----------------------------------------------------------------------------------------------\r\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\r\n\r\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\r\n| TF  | CUDA |\r\n| :-------------: | :-------------: |\r\n| 2.4.0  | 11.0 |\r\n| 2.1.0 - 2.3.0  | 10.1 |\r\n| 1.13.1 - 2.0  | 10.0  |\r\n| 1.5.0 - 1.12.0 | 9.0 |\r\n\r\n  * If you have above configuration and using _**Windows**_ platform -\r\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\r\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\r\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\r\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\r\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\r\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\r\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\r\n\r\n-----------------------------------------------------------------------------------------------\r\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\r\n\r\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\r\n\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\r\n\r\n   * Try Google Colab to use TensorFlow.\r\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\r\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\r\n      * All you need is a good internet connection and you are all set.\r\n   * Try to build TF from sources by changing CPU optimization flags.\r\n\r\n*Please let us know if this helps.*\r\n", "@Germanferraris,\r\n\r\nTensorFlow **1.x** is not actively supported. Could you please update TensorFlow to the latest stable version **v2.4.1** and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48409\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48409\">No</a>\n"]}, {"number": 48408, "title": "Accuracies saved in history.history after model.fit() are different than the ones printed on-screen during training", "body": "**System information**\r\nUsing Colab with Keras 2.4.0 and TensorFlow 2.4.1\r\n\r\n**Describe the current behavior**\r\nAfter calling \"history = model.fit(...)\", values saved in history.history['loss'] and history.history['accuracy'] are different from the ones printed on-screen at every epoch during training.\r\n\r\n**Describe the expected behavior**\r\nValues should be the same, as is with previous tf versions and with other parameters (like val_accuracy)\r\n\r\n**Standalone code to reproduce the issue**\r\nShared link: https://colab.research.google.com/drive/14Uogeq8wRlZlinaKLbkFr_Bl2aLzUJuy?usp=sharing\r\n(slightly modified version of MNIST tutorial, see the block right after calling model.fit())\r\n\r\nSee also original discussion: https://stackoverflow.com/questions/67001654/accuracy-in-history-dictionary-different-from-what-printed-on-screen", "comments": ["@malessandrini This is a known regression issue and is tracked with this issue https://github.com/tensorflow/tensorflow/issues/48033\r\n\r\nDuplicate of #https://github.com/tensorflow/tensorflow/issues/48033", "@malessandrini This was resolved in recent tf-nightly. I couldn't reproduce the error anymore. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/ea9c97436cf9949b21df32a17eb42969/copia-di-beginner.ipynb).\r\n\r\n`loss` and accuracy from `model.fit`\r\n\r\n```\r\nEpoch 1/5\r\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.3012 - accuracy: 0.9124\r\nEpoch 2/5\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.1457 - accuracy: 0.9561\r\nEpoch 3/5\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.1091 - accuracy: 0.9670\r\nEpoch 4/5\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0888 - accuracy: 0.9726\r\nEpoch 5/5\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0768 - accuracy: 0.9756\r\n```\r\n\r\n`loss` and `accuracy` from `history` object\r\n```\r\n`loss`          :  [0.3012, 0.1457, 0.1091, 0.0888, 0.0768]\r\n`accuracy` :  [0.9124, 0.9561, 0.967, 0.9726, 0.9756]\r\n```\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48408\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48408\">No</a>\n"]}, {"number": 48407, "title": "GPU delegate run tensorflow-lite C++ example label_image more slowly than CPU", "body": "I compile tensorflow-lite and the example label_image in tensorflow-lite source code success,\r\nI did run with delegates of GPU and also CPU with ADB with running comands:\r\n     CPU:  ./label_image -m tflite_model_int8.tflite -i grace_hopper.bmp -l labels.txt -j 1\r\n     GPU:  ./label_image -m tflite_model_int8.tflite -i grace_hopper.bmp -l labels.txt -g 1\r\nBoth seems runs success, but average time or GPU : 60.00 ms, average time or CPU : 40.00 ms\r\nIs it right? GPU is more slow than CPU for 50% ?\r\nIf Not, what should I do to make GPU runs at good performance?\r\n\r\n\r\nSystem information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 9.0\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: BBK S5d (SOC: Qualcomm Snapdragon 660)\r\nTensorFlow installed from (source or binary): build by myself\r\nTensorFlow version (use command below): tensorflow-2.4.1\r\nPython version: Python 3.8.5\r\nBazel version (if compiling from source): 3.1.0\r\nGCC/Compiler version (if compiling from source): g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nSOC: Qualcomm Snapdragon 660\r\n\r\nCould anyone help? Or give a advice?\r\nThank you so much~\r\n\r\n", "comments": ["@limdlh \r\nPlease share simple stand alone code such that we can replicate the issue faced or if possible share a colab gist.", "@impjdi @lintian06 could you take a look at this?", "@limdlh Assuming all the nodes in you quantized models could be delegated to GPU, then you are comparing CPU int speed and GPU fp16 speed (because GPU delegate has to dequantized your model to floating point).", "@freedomtan I did compare CPU, GPU and also DSP for both int and fp16, both GPU and DSP is not obviously faster than CPU no matter int or fp16.", "It's hard to tell without taking a look at the model.", "@impjdi Hello, both model is here: https://github.com/limdlh/share", "@impjdi I also tested in DSP(Hexagon), it's still roghly same slowly as GPU", "Hm, you have a pretty large `FULLY_CONNECTED` at the end which may not be too efficient when run on the GPU.  Try to cut the network right after the final `CONV_2D` and see whether it's indeed the culprit.", "@impjdi I did cut the FULLY_CONNECTED, but DSP(Hexagon) and GPU are still slow as CPU.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Sorry for the late turnaround.  Have a lot of things on my plate.\r\n\r\nThat's a quite disappointing news; I suspected things to be slow on those end, but maybe there is more to it.  Do you have the adb log?", "I ran your full network (`tflite_model_not_quant.tflite`) on a Pixel 4:\r\n\r\nCPU: 38.018285ms\r\nGPU: 5.6015285ms\r\n\r\nAFAIK Pixel 4 has Qualcomm Snapdragon 855 & Adreno 640.  So I don't think it's the delegate or the model.\r\n\r\nI'm not familiar with the `label_image` thing.  How is the inference engine used?  It should look like:\r\n\r\n```\r\n  // done once.\r\n  auto model = FlatBufferModel::BuildFromFile(model_path);\r\n  unique_ptr<Interpreter> gpu_inference;\r\n  InterpreterBuilder(*model, op_resolver)(&gpu_inference);\r\n  TfLiteGpuDelegateOptionsV2 o = TfLiteGpuDelegateOptionsV2Default();\r\n  o.inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED;\r\n  o.inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY;\r\n  o.inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_MEMORY_USAGE;\r\n  o.inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO;\r\n  auto* gpu_delegate = TfLiteGpuDelegateV2Create(&o);\r\n  gpu_inference->ModifyGraphWithDelegate(gpu_delegate);  // check whether it fails!  i'm just being lazy\r\n\r\n  // now for each input tensor...\r\n  gpu_inference->Invoke();  // check whether it fails!  i'm just being lazy\r\n```", "@impjdi Thanks for your reply, but I find in the [TFLite docment ](https://tensorflow.google.cn/lite/performance/gpu#c-until-2.3.0), it use TfLiteInterpreterOptions, not TfLiteGpuDelegateOptionsV2, is it the reason why my code fail to accelerate?\r\nAnd how about Hexagon(DSP), could you still give me a sample code to use Hexagon to accelerate?\r\nThank you so much~", "@Saduf2019 Hello, why remove assignment? should I add a new issue for Hexagon(DSP) accelerate?", "@limdlh \r\nThis issue will be handled by @impjdi  and @lintian06, hence removed my assignment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I'm not familiar with `TfLiteInterpreterOptions`.  I suggest using the code snippet I pasted above.", "@impjdi Does your code pasted above also could used for Hexagon(DSP) ?\r\nIf not, could you give me a sample code ?\r\nNow I still want a sample code for Hexagon, not for GPU.", "No, I don't use how to use Hexagon the hexagon delegate.  Doesn't it work as documented in https://www.tensorflow.org/lite/performance/hexagon_delegate ?", "@impjdi The code sample in https://www.tensorflow.org/lite/performance/hexagon_delegate runs roghly same slowly as CPU.\r\nShould I new another issue in github for hexagon delegate?", "yeah, it's a different owner\n\nOn Fri, May 7, 2021 at 17:51 limdlh ***@***.***> wrote:\n\n> @impjdi <https://github.com/impjdi> The code sample in\n> https://www.tensorflow.org/lite/performance/hexagon_delegate runs roghly\n> same slowly as CPU.\n> Should I new another issue in github for hexagon delegate?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/48407#issuecomment-834913237>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACKKUT4UTOHGGLS66ROGVKTTMSDJBANCNFSM42TDDOSA>\n> .\n>\n"]}, {"number": 48406, "title": "Testcode runs on multiple gpus instead of one", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 1.9\r\n- Python version: 3.6.13\r\n- Installed using virtualenv? pip? conda?: conda 4.6.11\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: GeForce GTX TITAN X - 12G\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm running a simple test script from the tensorflow documentation, to see if my setup is working correctly. I'm using conda to run tensorflow 1.9. When I'm running my test script, even though I'm specifying the device, it is running on all gpus, instead of the specified one. I can't figure out what the issue is. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/device:GPU:3'):\r\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n  c = tf.matmul(a, b)\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nprint(sess.run(c))\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@staehlmich ,\r\n\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48406\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48406\">No</a>\n"]}, {"number": 48405, "title": "Old version of CUDA support ?", "body": "Hi, I wonder if possible to release newest/nightly tensorflow-gpu compatible for old version CUDA ( 9.0 10.0). I want to use tf2.4 (or higher) on a CUDA 10.0 machine since the driver version are not compatible with newer version CUDA. Upgrading driver is impossible for some reason and and it's very inconvenient to build from source. \r\n\r\nIt's very important to `pip install tensorflow-gpu # maybe with something like +cuda101` to install tensorflow gpu with certain cuda version. Especially when projects are moved to another machine when older version tensorflow can handle newer version tensorflow codes....\r\n\r\nThanks for you guys open source such a great library, I think this feature might be very useful to all users.\r\n", "comments": ["@gitlabspy,\r\nA similar query was answered in issue [#42895](https://github.com/tensorflow/tensorflow/issues/42895).\r\n\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/42895#issuecomment-685874736) from a member of the TensorFlow team and check if it helps. Thanks!", "@amahendrakar Thanks for your response. He mentioned new cuda can not be compatible with older version of TF, so there is no way to build new TF (>2.4) for older cuda ?", "To reach the maximum performance available, each version of TF is tied to a specific CUDA version.\r\n\r\nYou can try compiling from source to get access to other versions of CUDA (newer/older) but you will have to fix compile errors/bugs that result from the mismatch yourself, as we don't have developers to handle every possible combination."]}, {"number": 48404, "title": "Update tf.keras.callbacks.CSVLogger to work with cloud storage", "body": "Small bug in CSVLogger. Change the `open` function to `file_io.FileIO` to make it work with files stored on the cloud and not only locally.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48404) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@qlzh727 what corner case are you referring to?", "@GabboM can you please check sanity build failures ?", "@GabboM  Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned the Ubuntu Sanity has 1 failing test:\r\n```\r\nFound 1 non-allowlisted pylint errors:\r\ntensorflow/python/keras/callbacks.py:22: [W0611(unused-import), ] Unused import io\r\n```\r\nIt seems it's just complaining for `io` not being called", "Hey @gbaned @qlzh727, any updates?"]}, {"number": 48403, "title": "Is SavedModel in tf 1.15 same as SavedModel in tf 2.x from the API prospective?", "body": "Hi Team,\r\n\r\nAs it is noted in https://www.tensorflow.org/hub/tf2_saved_model:\r\n\r\n\r\n**Users of TensorFlow 1 can update to TF 1.15 and then use the same APIs. Older versions of TF1 do not work.**\r\n\r\nDoes that means there's no difference between SavedModel format of tf 1.15(not tf.1.14 or other versions behind it) and tf 2.x?\r\n\r\nRegards,\r\nZan", "comments": ["@pmixer \r\nThere is no significant difference, please let us know if you are facing any issues, else please open this issue in [stackover flow](https://stackoverflow.com/questions/tagged/tensorflow) and move this to closed status as its not a bug or feature request.", "@Saduf2019 \r\nThank you Saduf! That's cool, I'm just trying to verify my assumption before diving into the details for working on the related project. Thx for your support!"]}, {"number": 48402, "title": "tflite qat model inference data_type", "body": "I am confused with tflite QAT model. Is it running with uint8 data_type or int8 data_type? \r\nWith this link https://www.tensorflow.org/lite/guide/hosted_models , I download the quant models and find it with uint8 weights.\r\nWith this link https://www.tensorflow.org/lite/performance/quantization_spec#int8_quantized_operator_specifications, I found tflite quantization use int8 data_type.\r\n\r\nWhich is correct?", "comments": ["I'll say both :-) As far as I can remember, quant models at TFLite hosted models are mostly old QAT ones using UINT8. Nowadays, int8 is the default type.", "@freedomtan thanks for your replay :-). \r\nI am confused what's the reason tflite choose to use int8 rather than uint8, performance or convinent? And where can I download a example int8 qat model to test.\r\nAnother question, if qat use int8 for default data_type, then post-training-quantize models and qat models can running the same operation kernels(realize with int8)?", "There's a note on the link: https://www.tensorflow.org/lite/performance/quantization_spec#int8_quantized_operator_specifications\r\n\r\n`Note: In the past our quantization tooling used per-tensor, asymmetric, uint8 quantization. New tooling, reference kernels, and optimized kernels for 8-bit quantization will use this spec.`\r\n\r\nAs far as I know, old TFLite converter (TF 1.x) dtype was uint8, new TFLite converter (TF 2.x) dtype is int8.\r\nTFLite kernel ops usually supports both dtypes. (int8 and uint8)\r\nBut new converter only supports TFLITE_BUILTINS_INT8 OpsSet: https://www.tensorflow.org/api_docs/python/tf/lite/OpsSet\r\n\r\n> I am confused what's the reason tflite choose to use int8 rather than uint8, performance or convinent? And where can I download a example int8 qat model to test.\r\n\r\nI think it's more easy to represent symmetric quantization which we usually used for dynamic-range quantization. (weight quantization) See more details here : https://www.tensorflow.org/lite/performance/quantization_spec#symmetric_vs_asymmetric\r\n\r\n", "@Xhark Thanks! \r\nI am confused about tensorflow quantization. As far as I know, tf quantizetion methods divided into two categories , post-training-quantization and quantize-aware-training.\r\nIn  post-training-quantization there are: \r\nmethod 1\u3001dynamic-range quantization\r\nmethod 2\u3001integer-only quantization\uff08need some datasets\uff09\r\nmethod 3\u3001Float16 quantization\r\nIn quantize-aware-training there is just one method(quantize during training, here I call it method 4).\r\n\r\nThen,\r\nI convert two tflite models through method 4 and method 2 from the same fp32 model. When  running this two quantized model with tflite, will they use the same operations in framework ? will they have the same performance?\r\n\r\nI know the model generated by method 1 is slower because it activations produced by immediate layer may convert to fp32, it take some times. \r\nBut the models generated by method 2 and method 4, I wonder that their attributes(model size, data_type, running kernels) should be same except the model accuracy. \r\n", "AFAIK, method2 and method4 should generate same model structure for the full-integer quantization case. (weight values and quantization parameter values might be different.) : So it should have the same performance.\r\n\r\nBut method4 (QAT) supports selective quantization which quantize only specific layers:\r\nhttps://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide#quantize_some_layers", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48402\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48402\">No</a>\n"]}, {"number": 48401, "title": "How to build libtensorflowlite.so with TensorFlow ops supported for armdf?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: tf-nightly\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.5\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: GTX 1070ti\r\n\r\n\r\n\r\n**Describe the problem**\r\nI need Tensorflow OPS to run tflite model inference in C++.\r\nBut how to build libtensorflowlite.so (for armdf) with TensorFlow ops ?\r\nIn [document](https://www.tensorflow.org/lite/guide/ops_select), it mentioned:\r\n_Add the TensorFlow ops delegate library dependency to the build dependencies: tensorflow/lite/delegates/flex:delegate_\r\nSo how to do this? \r\nModify BUILD file in directory tensorflow_src/tensorflow/lite/ ?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@thaink could you take a look?", "Actually, what I need is a C++ library that support interpreting TF ops as libtensorflowlite_flex_jni.so did on Android.", "@zhangguanqun if you use the jni how about building tensorflow-lite.aar and tensorflow-lite-select-tf-ops.aar?", "@thaink I did build tensorflow-lite.aar, and it works fine on my Android app.\r\nNow I want to run my TFLite model on Raspberry Pi using pure C++ interface.", "You can add \"tensorflow/lite/delegates/flex:delegate\" to the deps field of the rule \"tensorflowlite\" in tensorflow/lite/BUILD", "@thaink Thank u, I will try it.", "@thaink @abattery \r\nBuild report following errors:\r\n`no such package '@local_cuda//': The repository '@local_cuda' could not be resolved and referenced by '@cub_archive//:cub'\r\n`\r\n\r\nHere is my building command:\r\n`bazel build --config=elinux_armhf -c opt //tensorflow/lite:libtensorflowlite.so`\r\n\r\nFrom errors above, it means that directory \"local_cuda\" is referenced by target \"cub\", but it is not found.\r\nIn file bazel-tensorflow-src/external/cub_archive/BUILD.bazel, there is:\r\n```\r\ncc_library(\r\n    name = \"cub\",\r\n    hdrs = glob([\"cub/**\"]),\r\n    deps = [\"@local_cuda//:cuda_headers\"],\r\n)\r\n```\r\nBut the dependency dir \"local_cuda\" dose not exist in bazel-tensorflow-src/external/, that's why the error occurs. \r\nIn bazel-tensorflow-src/external/, there is a directory  called \"local_config_cuda\", and I find the build target \"cuda_headers\" in local_config_cuda/cuda/BUILD. **So is the deps wrong in \"/tensorflow_src/third_party/cub.BUILD\" ? Maybe deps should be \"@local_config_cuda//cuda:cuda_headers\"?**", "After change deps from  `[\"@local_cuda//:cuda_headers\"]` to `[\"@local_config_cuda//cuda:cuda_headers\"]` in file bazel-tensorflow-src/external/cub_archive/BUILD.bazel, I got the following errors:\r\n```\r\nERROR: /home/zhangguanqun/tensorflow-master/tensorflow/core/kernels/BUILD:4919:18: C++ compilation of rule '//tensorflow/core/kernels:scatter_op_gpu' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command /home/zhangguanqun/.cache/bazel/_bazel_zhangguanqun/9a90fa5c1777f045d849f4cfb9aa189a/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 141 argument(s) skipped)\r\naarch64-linux-gnu-gcc: error: unrecognized command line option '-Xcuda-fatbinary=--compress-all'\r\naarch64-linux-gnu-gcc: error: unrecognized command line option '--no-cuda-include-ptx=all'; did you mean '--no-chkp-instrument-calls'?\r\naarch64-linux-gnu-gcc: error: unrecognized command line option '--cuda-include-ptx=sm_61'\r\naarch64-linux-gnu-gcc: error: unrecognized command line option '--cuda-gpu-arch=sm_61'\r\naarch64-linux-gnu-gcc: error: unrecognized command line option '-nvcc_options=relaxed-constexpr'\r\naarch64-linux-gnu-gcc: error: unrecognized command line option '-nvcc_options=ftz=true'\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\n\r\n```\r\nTensorflow source version: 2.5.0-dev20210210\r\nCUDA version: 10.2\r\nWhat should I do?\r\n@thaink @abattery", "@zhangguanqun What platform are you building on? I am not sure about this error, too.", "@thaink\r\nUbuntu 18.04 Desktop", "Did you run ./configure?", "@thaink \r\nYes, I have already successfully build tensorflow-lite.aar with the same TF source and the AAR file is works fine on my Android App.", "Or, can I build libtensorflowlite.so without CUDA?\r\nIt seems like all errors are related to CUDA.", "Right. cub_archive is only pulled in if if_cuda is true.\r\nSo it should not be included here.\r\nCould you double check  and share the content of .tf_configure.bazelrc file?", "does adding --config=monolithic help?", "@thaink \r\nDo you mean run ./configure and set with CUDA=False?\r\nI run configure file with CUDA setting true, here is my .tf_configure.bazelrc file:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/zhangguanqun/anaconda3/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/zhangguanqun/anaconda3/lib/python3.8/site-packages\"\r\nbuild --python_path=\"/home/zhangguanqun/anaconda3/bin/python3\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda-10.2\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu/\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/x86_64-linux-gnu-gcc-7\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-Wno-sign-compare\r\nbuild --action_env ANDROID_NDK_HOME=\"/home/zhangguanqun/Android/android-ndk-r21b\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"21\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"30.0.3\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"30\"\r\nbuild --action_env ANDROID_SDK_HOME=\"/home/zhangguanqun/Android/Sdk\"\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_env=LD_LIBRARY_PATH\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\n```\r\nShould I configure CUDA = false?", "Adding --config=monolithic does not help.", "@thaink \r\nUPDATE:\r\nIf I set `with CUDA to False` in runing configure file, .tf_configure.bazelrc file change to\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3.6/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-Wno-sign-compare\r\nbuild --action_env ANDROID_NDK_HOME=\"/home/zhangguanqun/Android/android-ndk-r21b\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"21\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"30.0.3\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"30\"\r\nbuild --action_env ANDROID_SDK_HOME=\"/home/zhangguanqun/Android/Sdk\"\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\n```\r\n\r\nThan I try to rebuild libtensorflowlite.so with the same command, but got some errors with OPEN-SSL :(\r\n```\r\nERROR: /home/zhangguanqun/.cache/bazel/_bazel_zhangguanqun/9a90fa5c1777f045d849f4cfb9aa189a/external/com_github_grpc_grpc/BUILD:1659:16: C++ compilation of rule '@com_github_grpc_grpc//:grpc_secure' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command /home/zhangguanqun/.cache/bazel/_bazel_zhangguanqun/9a90fa5c1777f045d849f4cfb9aa189a/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 68 argument(s) skipped)\r\nIn file included from /usr/include/openssl/evp.h:16,\r\n                 from /usr/include/openssl/x509.h:18,\r\n                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,\r\n                 from external/com_github_grpc_grpc/src/core/lib/http/httpcli_security_connector.cc:35:\r\n/usr/include/openssl/bio.h:690:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0'\r\n DEPRECATEDIN_1_1_0(int BIO_get_port(const char *str, unsigned short *port_ptr))\r\n ^~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/include/openssl/asn1.h:23,\r\n                 from /usr/include/openssl/objects.h:15,\r\n                 from /usr/include/openssl/evp.h:28,\r\n                 from /usr/include/openssl/x509.h:18,\r\n                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,\r\n                 from external/com_github_grpc_grpc/src/core/lib/http/httpcli_security_connector.cc:35:\r\n/usr/include/openssl/bn.h:183:43: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?\r\n int BN_abs_is_word(const BIGNUM *a, const BN_ULONG w);\r\n                                           ^~~~~~~~\r\n                                           SHA_LONG\r\n/usr/include/openssl/bn.h:186:39: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?\r\n int BN_is_word(const BIGNUM *a, const BN_ULONG w);\r\n                                       ^~~~~~~~\r\n                                       SHA_LONG\r\n/usr/include/openssl/bn.h:214:22: error: 'BN_ULONG' was not declared in this scope\r\n int BN_num_bits_word(BN_ULONG l);\r\n                      ^~~~~~~~\r\n/usr/include/openssl/bn.h:214:22: note: suggested alternative: 'SHA_LONG'\r\n int BN_num_bits_word(BN_ULONG l);\r\n                      ^~~~~~~~\r\n                      SHA_LONG\r\n/usr/include/openssl/bn.h:266:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?\r\n BN_ULONG BN_mod_word(const BIGNUM *a, BN_ULONG w);\r\n ^~~~~~~~\r\n SHA_LONG\r\n/usr/include/openssl/bn.h:267:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?\r\n BN_ULONG BN_div_word(BIGNUM *a, BN_ULONG w);\r\n ^~~~~~~~\r\n SHA_LONG\r\n/usr/include/openssl/bn.h:268:28: error: 'BN_ULONG' has not been declared\r\n int BN_mul_word(BIGNUM *a, BN_ULONG w);\r\n                            ^~~~~~~~\r\n/usr/include/openssl/bn.h:269:28: error: 'BN_ULONG' has not been declared\r\n int BN_add_word(BIGNUM *a, BN_ULONG w);\r\n                            ^~~~~~~~\r\n/usr/include/openssl/bn.h:270:28: error: 'BN_ULONG' has not been declared\r\n int BN_sub_word(BIGNUM *a, BN_ULONG w);\r\n                            ^~~~~~~~\r\n/usr/include/openssl/bn.h:271:28: error: 'BN_ULONG' has not been declared\r\n int BN_set_word(BIGNUM *a, BN_ULONG w);\r\n                            ^~~~~~~~\r\n/usr/include/openssl/bn.h:272:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?\r\n BN_ULONG BN_get_word(const BIGNUM *a);\r\n ^~~~~~~~\r\n SHA_LONG\r\n/usr/include/openssl/bn.h:288:37: error: 'BN_ULONG' has not been declared\r\n int BN_mod_exp_mont_word(BIGNUM *r, BN_ULONG a, const BIGNUM *p,\r\n                                     ^~~~~~~~\r\n/usr/include/openssl/bn.h:323:24: error: variable or field 'BN_consttime_swap' declared void\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                        ^~~~~~~~\r\n/usr/include/openssl/bn.h:323:24: error: 'BN_ULONG' was not declared in this scope\r\n/usr/include/openssl/bn.h:323:24: note: suggested alternative: 'SHA_LONG'\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                        ^~~~~~~~\r\n                        SHA_LONG\r\n/usr/include/openssl/bn.h:323:46: error: expected primary-expression before '*' token\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                                              ^\r\n/usr/include/openssl/bn.h:323:47: error: 'a' was not declared in this scope\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                                               ^\r\n/usr/include/openssl/bn.h:323:57: error: expected primary-expression before '*' token\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                                                         ^\r\n/usr/include/openssl/bn.h:323:58: error: 'b' was not declared in this scope\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                                                          ^\r\n/usr/include/openssl/bn.h:323:61: error: expected primary-expression before 'int'\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                                                             ^~~\r\n/usr/include/openssl/bn.h:332:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_0_9_8'\r\n DEPRECATEDIN_0_9_8(int\r\n ^~~~~~~~~~~~~~~~~~\r\n/usr/include/openssl/bn.h:403:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_0_9_8'\r\n DEPRECATEDIN_0_9_8(int BN_get_params(int which)) /* 0, mul, 1 high, 2 low, 3\r\n ^~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/include/openssl/objects.h:15,\r\n                 from /usr/include/openssl/evp.h:28,\r\n                 from /usr/include/openssl/x509.h:18,\r\n                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,\r\n                 from external/com_github_grpc_grpc/src/core/lib/http/httpcli_security_connector.cc:35:\r\n/usr/include/openssl/asn1.h:555:7: error: expected constructor, destructor, or type conversion before 'unsigned'\r\n const unsigned char *ASN1_STRING_get0_data(const ASN1_STRING *x);\r\n       ^~~~~~~~\r\nIn file included from /usr/include/openssl/x509.h:22,\r\n                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,\r\n                 from external/com_github_grpc_grpc/src/core/lib/http/httpcli_security_connector.cc:35:\r\n/usr/include/openssl/ec.h:274:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_2_0'\r\n DEPRECATEDIN_1_2_0(int EC_GROUP_get_curve_GFp(const EC_GROUP *group, BIGNUM *p,\r\n ^~~~~~~~~~~~~~~~~~\r\n/usr/include/openssl/ec.h:543:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_2_0'\r\n DEPRECATEDIN_1_2_0(int EC_POINT_get_affine_coordinates_GFp(const EC_GROUP *group,\r\n ^~~~~~~~~~~~~~~~~~\r\n/usr/include/openssl/ec.h:631:1: error: expected constructor, destructor, or type conversion before 'size_t'\r\n size_t EC_POINT_point2oct(const EC_GROUP *group, const EC_POINT *p,\r\n ^~~~~~\r\nIn file included from /usr/include/openssl/x509.h:25,\r\n                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,\r\n                 from external/com_github_grpc_grpc/src/core/lib/http/httpcli_security_connector.cc:35:\r\n/usr/include/openssl/rsa.h:239:1: error: expected constructor, destructor, or type conversion before 'int'\r\n int RSA_generate_key_ex(RSA *rsa, int bits, BIGNUM *e, BN_GENCB *cb);\r\n ^~~\r\nIn file included from /usr/include/openssl/dsa.h:25,\r\n                 from /usr/include/openssl/x509.h:26,\r\n                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,\r\n                 from external/com_github_grpc_grpc/src/core/lib/http/httpcli_security_connector.cc:35:\r\n/usr/include/openssl/dh.h:142:1: error: expected constructor, destructor, or type conversion before 'int'\r\n int DH_generate_parameters_ex(DH *dh, int prime_len, int generator,\r\n ^~~\r\nIn file included from /usr/include/openssl/x509.h:26,\r\n                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,\r\n                 from external/com_github_grpc_grpc/src/core/lib/http/httpcli_security_connector.cc:35:\r\n/usr/include/openssl/dsa.h:103:1: error: expected constructor, destructor, or type conversion before 'int'\r\n int DSA_sign(int type, const unsigned char *dgst, int dlen,\r\n ^~~\r\n/usr/include/openssl/dsa.h:127:1: error: expected constructor, destructor, or type conversion before 'int'\r\n int DSA_generate_parameters_ex(DSA *dsa, int bits,\r\n ^~~\r\nIn file included from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,\r\n                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,\r\n                 from external/com_github_grpc_grpc/src/core/lib/http/httpcli_security_connector.cc:35:\r\n/usr/include/openssl/x509.h:728:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0'\r\n DEPRECATEDIN_1_1_0(ASN1_TIME *X509_CRL_get_nextUpdate(X509_CRL *crl))\r\n ^~~~~~~~~~~~~~~~~~\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 724.445s, Critical Path: 115.39s\r\nINFO: 3503 processes: 148 internal, 3355 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nMy OpenSSL version is: 1.1.1  11 Sep 2018\r\n", "right. cuda should be config to no. \r\nI am was able to reproduce a similar build error, let me check.", "@thaink  \ud83d\ude4f\r\nMaybe I should downgrade openssl.", "I hope this is helpful.\r\n\r\n1. According to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh#L39\r\nyou need to hide the system openssl. And you also need to modify /usr/include/curl/curlbuild.h\r\n\r\n2. \"--config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl\" is required for Flex build.\r\n\r\n3. To avoid #1, it's better to use container build tool \"tensorflow/tools/ci_build/ci_build.sh\" and create your own build script.", "@terryheo Thanks, advice 1st did help, errors about openssl did not exist, but, at the end of building, linking error reported:\r\n```\r\nERROR: /home/zhangguanqun/tensorflow-master/tensorflow/lite/BUILD:887:24: Linking of rule '//tensorflow/lite:libtensorflowlite.so' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command /home/zhangguanqun/.cache/bazel/_bazel_zhangguanqun/9a90fa5c1777f045d849f4cfb9aa189a/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc ... (remaining 1 argument(s) skipped)\r\nstderr (/home/zhangguanqun/.cache/bazel/_bazel_zhangguanqun/9a90fa5c1777f045d849f4cfb9aa189a/execroot/org_tensorflow/bazel-out/_tmp/actions/stderr-7638) exceeds maximum size of --experimental_ui_max_stdouterr_bytes=1048576 bytes; skipping\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2182.429s, Critical Path: 474.31s\r\nINFO: 4542 processes: 493 internal, 4049 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "How about increase the value of --experimental_ui_max_stdouterr_bytes?", "@thaink \r\nYes, I have increated max_stdouterr_bytes to 1073741819, and rebuilding is doing now, this may take several tens of minutes.", "Finally made it !\r\nThanks to @abattery @terryheo @thaink, Appreciate.\r\n \r\nCmd that successfully build libtensorflowlite.so:\r\n`bazel build --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl --config=elinux_armhf --experimental_ui_max_stdouterr_bytes=1073741819 -c opt //tensorflow/lite:libtensorflowlite.so`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48401\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48401\">No</a>\n", "@zhangguanqun how did you solve the problem below? \r\n\"  The repository '@local_cuda' could not be resolved and referenced by '@cub_archive//:cub' \" \r\nif i comment out the cuda, is it right when i start the model?"]}, {"number": 48400, "title": "Added Lite option to EfficientNets.", "body": "### Description\r\nThis PR modifies the existing code for EfficientNet inside `tf.keras.applications` so that it allows to use the Lite [1] variants of the model. The Lite variants were introduced in the original EfficientNet repository and had the following differences:\r\n* Removed squeeze-and-excite (SE).\r\n* Replaced swish with RELU6.\r\n* Fixed the stem and head while scaling models up.\r\n\r\nI am also referencing my helper repository [2], that uses existing Tensorflow code, for direct conversion of `.ckpt` files to Keras `.h5` variants.\r\n\r\n### Motivation\r\nThe existing Lite variants of EfficientNets are only available in original repository and Tensorflow Hub [3]. These are written in Tensorflow 1.x and hence do not allow for flexible use with Keras.\r\n\r\nThe option to add Lite variants has been requested few times in Tensorflow Ecosystem:\r\nhttps://github.com/tensorflow/tensorflow/issues/45091 \r\nhttps://github.com/tensorflow/hub/issues/751 \r\n\r\nThis PR addresses the first issue and partially solves the second one.\r\n\r\n### Usage / Changes\r\nIn the original version one could call EfficientNet models as such:\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.applications.efficientnet.EfficientNetB0()\r\n```\r\n\r\nWith this PR it would be possible to do:\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel_lite = tf.keras.applications.efficientnet.EfficientNetB0(lite=True)\r\n```\r\nThis behaviour is not introducing any breaking changes to existing codebase and is very similar to `minimalistic` option in MobilenetV3Small and Large.\r\n\r\n### Todos:\r\nThis PR is sadly a not finished work and to make it fully compatible with existing codebase I would need help from someone on the Tensorflow team. Most notably there are two things to be discussed:\r\n\r\n1. Add converted weights to Remote Storage + add lite variants weights hashes so they can be automatically loaded. I can provide the converted weights (top and notop variants) or they can be obtained by my helper repository [2].\r\n2. Prevent the user from creating lite variants of the models B5 and higher. These are not supported in the original repository, yet still the user can pass the `lite=True` flag via kwargs. This will throw rather cryptic error that the number of layers does not match in architecture vs weight file. Perhaps a different error message would be better?\r\n\r\n### References\r\n[1] https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite \r\n[2] https://github.com/sebastian-sz/efficientnet-lite-keras\r\n[3] https://tfhub.dev/s?q=efficientnet%20lite", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48400) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "This is promising!", "@sebastian-sz  Can you please resolve conflicts? Thanks!\r\n", "@gbaned \r\nHi! Thank you for the notice. \r\nI updated the repository, the conflicts should be resolved.\r\n\r\nEDIT: \r\nAlso, I'm wondering it these Lite models should not be in a separate file. The cost would be some duplication but I guess all variants would be easier to test and maintain?\r\n\r\n", "@gbaned \r\nHello, there has been almost 2 months with not much activity. I know that there are other, more important PRs, going through. I don't mind waiting for a review, but could you tell me, whether you would be interested in functionality presented here? \r\nThanks!", "@sebastian-sz  Sorry for the delay, it is under review phase. Thank you!", "@gbaned Understood, thanks for the info!", "Sorry for the long wait. I was fully occupied by other issue for quite a while, and wasn't able to review this PR.\r\n\r\nAlso to verify the correctness of this PR, I was trying to setup some internal test to let the model run the imagenet dataset (mainly to verify the accuracy number). Do u have the converted weight file available so that we can use it for verification?", "No problem, thanks for looking into this.\r\n\r\nYes I have the converted weights on Google Drive:\r\nhttps://drive.google.com/file/d/1rcUoNjpUYhoOIgz2VFsfVGFFyNQK5PQ4/view?usp=sharing\r\n\r\nOne more thing:\r\nThese weights are not hosted anywhere, and are not referenced in the code. Calling\r\n`m = tf.keras.applications.efficientnet.EfficientNetB0(lite=True)` \r\nwill raise error. However,\r\n```python\r\nm = tf.keras.applications.efficientnet.EfficientNetB0(lite=True, weights=None)\r\nm.load_weights(...)\r\n```\r\nwill work.", "@sebastian-sz  Any update on this PR? Please. Thanks!", "@gbaned \r\nHello, I added the weights in my previous comment on Google Drive. As I understood @qlzh727 wanted to verify the metrics on Imagenet. Is there something more I should do at this moment? Thanks!", "@qlzh727 Any update on this PR? Please. Thanks!", "@qlzh727 Given current separation of Keras and TF should I close this PR and resubmit it in the keras repository?", "Sorry for the long wait. Yes, the keras code has been removed to keras-team/keras, and this PR will need to be ported there. Thanks.", "Understood, I am closing this PR for now. Given more time I will reopen it in the Keras repo."]}, {"number": 48399, "title": "[tflite] make build of tflite cmd line tools work again", "body": "This is to revert 5ba9567, which removed a workaround in absl to\r\nmake cross-building android stuff on macOS work. Without this,\r\nwhen building `benchmark_model` with\r\n```\r\nbazel build --config android_arm64 \\\r\n  //tensorflow/lite/tools/benchmark:benchmark_model\r\n```\r\nI saw\r\n\r\n```\r\n/bin/external/com_google_absl/absl/base/liblog_severity.a bazel-out/arm64-v8a-opt/bin/external/com_google_absl/absl/numeric/libint128.a bazel-out/arm64-v8a-opt/bin/external/ruy/ruy/profiler/libprofiler.a bazel-out/arm64-v8a-opt/bin/external/ruy/ruy/profiler/libinstrumentation.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++abi.a -latomic -Wl,--no-export-dynamic -Wl,--gc-sections -Wl,--as-needed -s -pie -lm '-Wl,--rpath=/data/local/tmp/' -lm -ldl -ldl -ldl -ldl -lEGL -lGLESv2 -ldl -lm -pthread -framework Foundation -pthread -pthread -pthread -pthread -lm -llog -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -llog -pthread -pthread -pthread -static-libgcc -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64 -target aarch64-none-linux-android -no-canonical-prefixes -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a '--sysroot=external/androidndk/ndk/platforms/android-28/arch-arm64')\r\nERROR: /Users/freedom/work/tf-py3/tensorflow/lite/tools/benchmark/BUILD:30:10: Linking of rule '//tensorflow/lite/tools/benchmark:benchmark_model' failed (Exit 1): clang failed: error executing command external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang -o bazel-out/arm64-v8a-opt/bin/tensorflow/lite/tools/benchmark/benchmark_model ... (remaining 397 argument(s) skipped)\r\nexternal/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64/lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/bin/ld: -f may not be used without -shared\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/lite/tools/benchmark:benchmark_model failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.293s, Critical Path: 0.04s\r\nINFO: 2 processes: 2 internal.\r\n```", "comments": ["@chsigg @multiverse-tf could you review this PR?", "@freedomtan, thx for the fix!\r\n\r\n@mihaimaruseac, could you help review this PR? I don't have enough background about your [commit](https://github.com/tensorflow/tensorflow/commit/5ba956765f3cd21c72c9900dcb658ea041555517) which is to be reverted in this PR. Thx!", "Sorry, internal testing showed that this patch is no longer needed", "@mihaimaruseac No problem, I could see cross-compiling for Android on macOS is kinda corner case."]}, {"number": 48398, "title": "Add extra point in ./tensorflow/examples README file", "body": "## Description\r\nThis PR fixes #48394 \r\n\r\n## Changes made\r\nI added an extra point to refer course assignments and all other things. Just a minor change though! \r\n\r\n## Notes for reviewers\r\nThis is actually a small change but review and suggest changes to do if any. Thanks", "comments": []}, {"number": 48396, "title": "/micro_speech/recognize_commands_test.cc bug", "body": "Hi, I tried to follow the book TinyML chapter 07 and ran `recognize_command_test` and got an error shown in the img below..\r\nI think in `recognize_commands_test.cc` line 158\r\n`const int bad_dims[] = {2, 1, 3};`  \r\nhas to be changed to \r\n`const int bad_dims[] = {2, 1, 4};` \r\n\r\n![Screenshot from 2021-04-08 16-43-54](https://user-images.githubusercontent.com/46836844/113991238-3875ff80-988d-11eb-9e81-c853ec79b31c.png)\r\n", "comments": ["> Hi, I tried to follow the book TinyML chapter 07 and ran `recognize_command_test` and got an error shown in the img below..\r\n> I think in `recognize_commands_test.cc` line 158\r\n> `const int bad_dims[] = {2, 1, 3};`\r\n> has to be changed to\r\n> `const int bad_dims[] = {2, 1, 4};`\r\n> \r\n> ![Screenshot from 2021-04-08 16-43-54](https://user-images.githubusercontent.com/46836844/113991238-3875ff80-988d-11eb-9e81-c853ec79b31c.png)\r\n\r\noh,, nevermind I figured it out!! There's no problem", "Hey, can someone close this issue :) (sorry if this sounds mean)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48396\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48396\">No</a>\n"]}, {"number": 48394, "title": "Add link and more info in tf/examples/README", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Here's a brief about the issue...\r\nIn this repo (tensorflow/tensorflow/examples) the README is enough with examples and all other things, but still it has a statement \"This directory is not actively maintained\" which is fine, but I feel a statement must be included along with it... \"Refer this repo for other examples\" so that newcomers will know and refer to that link", "And TF community, please assign me this issue, I want to work on it! :)", "@alphaX86 \r\n Please feel free to submit a PR.Thanks!"]}, {"number": 48393, "title": "Fix cub BUILD file for local_config_cuda reference", "body": "On Linux x86_64, while building TF 2.5.0-rc0 for cuda 10.2, I encountered an error that says \"no such package @local_cuda referenced by cub\" (don't have an exact error). This PR proposes a fix for this.\r\nBut I'm surprised how this is working for everyone as the same code is also present on the master branch. And if it is working fine, then what is wrong with my environment.\r\n\r\nCreating new PR against master branch. Closing the earlier one https://github.com/tensorflow/tensorflow/pull/48310 as it was earlier created against r2.5 branch and then after changing base branch, it started showing n number of commits from master too.", "comments": ["I get the following error when trying the bazel build with this PR \r\n```\r\n#13 155.3 ERROR: /opt/tensorflow/tensorflow/core/lib/db/BUILD:15:11: //tensorflow/core/lib/db:sqlite depends on @org_sqlite//:org_sqlite in repository @org_sqlite which failed to fetch. no such package '@org_sqlite//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/www.sqlite.org/2021/sqlite-amalgamation-3340100.zip, https://www.sqlite.org/2021/sqlite-amalgamation-3340100.zip] to /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/org_sqlite/temp10485635226286372172/sqlite-amalgamation-3340100.zip: Unknown host: www.sqlite.org\r\n```\r\n\r\n[full traceback](https://github.com/tensorflow/tensorflow/files/6280582/traceback.txt)\r\n", "> \r\n> \r\n> I get the following error when trying the bazel build with this PR\r\n> \r\n> ```\r\n> #13 155.3 ERROR: /opt/tensorflow/tensorflow/core/lib/db/BUILD:15:11: //tensorflow/core/lib/db:sqlite depends on @org_sqlite//:org_sqlite in repository @org_sqlite which failed to fetch. no such package '@org_sqlite//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/www.sqlite.org/2021/sqlite-amalgamation-3340100.zip, https://www.sqlite.org/2021/sqlite-amalgamation-3340100.zip] to /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/org_sqlite/temp10485635226286372172/sqlite-amalgamation-3340100.zip: Unknown host: www.sqlite.org\r\n> ```\r\n> \r\n> [full traceback](https://github.com/tensorflow/tensorflow/files/6280582/traceback.txt)\r\n\r\nCursory look at sqlite.org download page shows no version 3.34 link (the version it looks like is being asked for here)", "@gbaned are you able to reproduce this error?\r\n\r\n> \r\n> \r\n> I get the following error when trying the bazel build with this PR\r\n> \r\n> ```\r\n> #13 155.3 ERROR: /opt/tensorflow/tensorflow/core/lib/db/BUILD:15:11: //tensorflow/core/lib/db:sqlite depends on @org_sqlite//:org_sqlite in repository @org_sqlite which failed to fetch. no such package '@org_sqlite//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/www.sqlite.org/2021/sqlite-amalgamation-3340100.zip, https://www.sqlite.org/2021/sqlite-amalgamation-3340100.zip] to /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/org_sqlite/temp10485635226286372172/sqlite-amalgamation-3340100.zip: Unknown host: www.sqlite.org\r\n> ```\r\n> \r\n> [full traceback](https://github.com/tensorflow/tensorflow/files/6280582/traceback.txt)\r\n\r\n", "@chsigg  Can you please assist on above comments from @RainierBarrett. Thanks!", "@npanpaliya @gbaned @chsigg Is there a fix to the errors @RainierBarrett and I are getting? (Sorry to pester you, but we need this container for a project.)", "@jennyfothergill - Are you using sqlite from the system? \r\nI am able to download https://www.sqlite.org/2021/sqlite-amalgamation-3340100.zip from the browser. So, this should not be an issue.", "@npanpaliya I am building in a docker container. (original issue https://github.com/tensorflow/tensorflow/issues/48380) How would I resolve this in my container build file?", "The problem that @jennyfothergill is facing is unrelated to my PR. @cheshire , @chsigg - Could you review this PR?", "Verified that this solves the problem for CUDA 10.1 and checking the diff and other usages of cuda_headers this is obviously correct."]}, {"number": 48392, "title": "TensorBoard can't support read data from Kerberos cluster hdfs.", "body": "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.8\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): \r\n      tensorboard            2.4.1    \r\n      tensorboard-plugin-wit 1.8.0    \r\n      tensorflow             2.3.1    \r\n      tensorflow-estimator   2.3.0  \r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nsudo su hdfs\r\nexport KERB_TICKET_CACHE_PATH=/tmp/krb5cc_996\r\necho 'hdfs' | kinit hdfs@TEST-KDC\r\n\r\nbash-4.2$ hdfs dfs -ls /tmp/tensorflow/mnist/\r\nFound 4 items\r\ndrwxr-xr-x   - hdfs supergroup          0 2021-03-31 18:03 /tmp/tensorflow/mnist/input_data\r\ndrwxr-xr-x   - hdfs supergroup          0 2021-04-07 19:49 /tmp/tensorflow/mnist/working_dir\r\ndrwxr-xr-x   - hdfs supergroup          0 2021-03-31 19:25 /tmp/tensorflow/mnist/working_dir_1131\r\ndrwxr-xr-x   - hdfs supergroup          0 2021-03-31 19:39 /tmp/tensorflow/mnist/working_dir_231\r\n\r\nbash-4.2$ hdfs dfs -ls /tmp/tensorflow/mnist/working_dir\r\nFound 17 items\r\n-rw-r--r--   1 hdfs supergroup        222 2021-04-07 19:49 /tmp/tensorflow/mnist/working_dir/checkpoint\r\n-rw-r--r--   1 hdfs supergroup     656044 2021-03-31 20:09 /tmp/tensorflow/mnist/working_dir/events.out.tfevents.1617217664.cdhhakerberos-cdh-core-kudu-0.novalocal\r\n-rw-r--r--   1 hdfs supergroup     657680 2021-04-01 02:10 /tmp/tensorflow/mnist/working_dir/events.out.tfevents.1617239301.cdhhakerberos-cdh-core-kudu-0.novalocal\r\n-rw-r--r--   1 hdfs supergroup     657796 2021-04-07 20:49 /tmp/tensorflow/mnist/working_dir/events.out.tfevents.1617824847.cdhhakerberos-cdh-core-kudu-0.novalocal\r\n-rw-r--r--   1 hdfs supergroup     328014 2021-04-07 19:47 /tmp/tensorflow/mnist/working_dir/graph.pbtxt\r\n-rw-r--r--   1 hdfs supergroup   39295624 2021-03-31 19:07 /tmp/tensorflow/mnist/working_dir/model.ckpt-0.data-00000-of-00001\r\n-rw-r--r--   1 hdfs supergroup        994 2021-03-31 19:07 /tmp/tensorflow/mnist/working_dir/model.ckpt-0.index\r\n-rw-r--r--   1 hdfs supergroup     138970 2021-03-31 19:07 /tmp/tensorflow/mnist/working_dir/model.ckpt-0.meta\r\n\r\n\r\nCLASSPATH=`hadoop classpath --glob` tensorboard --logdir hdfs://default/tmp/tensorflow/mnist/ --host `hostname` --verbosity 0 --logger_levels other.logger:DEBUG --stderrthreshold debug\r\n\r\n**Describe the expected behavior**\r\nI expect show data in Tensorboard. But TensorBoard show \"No dashboards are active for the current data set.\"\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n2021-04-07 22:22:10.690895: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nI0407 22:22:13.750683 139694758954816 plugin_event_multiplexer.py:106] Event Multiplexer initializing.\r\nI0407 22:22:13.750985 139694758954816 plugin_event_multiplexer.py:125] Event Multiplexer done initializing\r\nI0407 22:22:13.751172 139694758954816 data_ingester.py:124] Launching reload in a daemon thread\r\nI0407 22:22:13.752043 139693371959040 data_ingester.py:98] TensorBoard reload process beginning\r\nI0407 22:22:13.752586 139693371959040 plugin_event_multiplexer.py:201] Starting AddRunsFromDirectory: hdfs://default/tmp/tensorflow/mnist/\r\nI0407 22:22:13.756613 139693371959040 plugin_event_multiplexer.py:207] Done with AddRunsFromDirectory: hdfs://default/tmp/tensorflow/mnist/\r\nI0407 22:22:13.756979 139693371959040 data_ingester.py:102] TensorBoard reload process: Reload the whole Multiplexer\r\nI0407 22:22:13.757233 139693371959040 plugin_event_multiplexer.py:212] Beginning EventMultiplexer.Reload()\r\nI0407 22:22:13.757594 139693371959040 plugin_event_multiplexer.py:256] Reloading runs serially (one after another) on the main thread.\r\nI0407 22:22:13.757895 139693371959040 plugin_event_multiplexer.py:265] Finished with EventMultiplexer.Reload()\r\nI0407 22:22:13.758112 139693371959040 data_ingester.py:107] TensorBoard done reloading. Load took 0.006 secs\r\nTensorBoard 2.4.1 at http://cdhhakerberos-cdh-master1-0.novalocal:6006/ (Press CTRL+C to quit)\r\nI0407 22:22:14.644470 139693380351744 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:14] \"GET / HTTP/1.1\" 200 -\r\nI0407 22:22:14.728922 139693380351744 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:14] \"GET /index.js HTTP/1.1\" 200 -\r\nI0407 22:22:14.752892 139693388744448 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:14] \"GET /font-roboto/oMMgfZMQthOryQo9n22dcuvvDin1pK8aKteLpeZ5c0A.woff2 HTTP/1.1\" 200 -\r\nI0407 22:22:15.515960 139693380351744 application.py:439] Plugin listing: is_active() for scalars took 0.000 seconds\r\nI0407 22:22:15.516464 139693380351744 application.py:439] Plugin listing: is_active() for custom_scalars took 0.000 seconds\r\nI0407 22:22:15.516749 139693380351744 application.py:439] Plugin listing: is_active() for images took 0.000 seconds\r\nI0407 22:22:15.516995 139693380351744 application.py:439] Plugin listing: is_active() for audio took 0.000 seconds\r\nI0407 22:22:15.517701 139693380351744 application.py:439] Plugin listing: is_active() for debugger-v2 took 0.000 seconds\r\nI0407 22:22:15.518152 139693380351744 application.py:439] Plugin listing: is_active() for graphs took 0.000 seconds\r\nI0407 22:22:15.518453 139693380351744 application.py:439] Plugin listing: is_active() for distributions took 0.000 seconds\r\nI0407 22:22:15.518700 139693380351744 application.py:439] Plugin listing: is_active() for histograms took 0.000 seconds\r\nI0407 22:22:15.518924 139693380351744 application.py:439] Plugin listing: is_active() for text took 0.000 seconds\r\nI0407 22:22:15.519140 139693380351744 application.py:439] Plugin listing: is_active() for pr_curves took 0.000 seconds\r\nI0407 22:22:15.519388 139693380351744 application.py:439] Plugin listing: is_active() for profile_redirect took 0.000 seconds\r\nI0407 22:22:15.519608 139693380351744 application.py:439] Plugin listing: is_active() for hparams took 0.000 seconds\r\nI0407 22:22:15.519902 139693380351744 application.py:439] Plugin listing: is_active() for mesh took 0.000 seconds\r\nI0407 22:22:15.520128 139693380351744 application.py:439] Plugin listing: is_active() for timeseries took 0.000 seconds\r\nI0407 22:22:15.525379 139693397137152 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] \"GET /icon_bundle.svg HTTP/1.1\" 200 -\r\nI0407 22:22:15.526228 139693380351744 application.py:439] Plugin listing: is_active() for projector took 0.006 seconds\r\nI0407 22:22:15.526491 139693380351744 application.py:439] Plugin listing: is_active() for whatif took 0.000 seconds\r\nI0407 22:22:15.529370 139693388744448 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] \"GET /data/environment HTTP/1.1\" 200 -\r\nI0407 22:22:15.530524 139693380351744 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] \"GET /data/plugins_listing HTTP/1.1\" 200 -\r\nI0407 22:22:15.541091 139693380351744 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] \"GET /data/runs HTTP/1.1\" 200 -\r\nI0407 22:22:15.542141 139693397137152 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] \"GET /data/environment HTTP/1.1\" 200 -\r\nI0407 22:22:15.544588 139693388744448 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] \"GET /font-roboto/RxZJdnzeo3R5zSexge8UUZBw1xU1rKptJj_0jans920.woff2 HTTP/1.1\" 200 -\r\nI0407 22:22:15.587946 139693388744448 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] \"GET /data/runs HTTP/1.1\" 200 -\r\nI0407 22:22:15.699429 139693388744448 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] \"GET /font-roboto/d-6IYplOFocCacKzxwXSOJBw1xU1rKptJj_0jans920.woff2 HTTP/1.1\" 200 -\r\nI0407 22:22:15.701413 139693397137152 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] \"GET /font-roboto/vPcynSL0qHq_6dX7lKVByXYhjbSpvc47ee6xR_80Hnw.woff2 HTTP/1.1\" 200 -\r\nI0407 22:22:18.765110 139693371959040 data_ingester.py:98] TensorBoard reload process beginning\r\nI0407 22:22:18.765607 139693371959040 plugin_event_multiplexer.py:201] Starting AddRunsFromDirectory: hdfs://default/tmp/tensorflow/mnist/\r\nI0407 22:22:18.765993 139693371959040 plugin_event_multiplexer.py:207] Done with AddRunsFromDirectory: hdfs://default/tmp/tensorflow/mnist/\r\nI0407 22:22:18.766208 139693371959040 data_ingester.py:102] TensorBoard reload process: Reload the whole Multiplexer\r\nI0407 22:22:18.766429 139693371959040 plugin_event_multiplexer.py:212] Beginning EventMultiplexer.Reload()\r\nI0407 22:22:18.766694 139693371959040 plugin_event_multiplexer.py:256] Reloading runs serially (one after another) on the main thread.\r\nI0407 22:22:18.766852 139693371959040 plugin_event_multiplexer.py:265] Finished with EventMultiplexer.Reload()\r\nI0407 22:22:18.766979 139693371959040 data_ingester.py:107] TensorBoard done reloading. Load took 0.002 secs\r\n\r\n", "comments": ["@bd050705 as per the [documentation](https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/hadoop.md) you need to set `KRB5CCNAME` as the path of Kerberos ticket cache file. For example:\r\n```bash\r\nexport KRB5CCNAME=/tmp/krb5cc_10002\r\n```\r\n\r\nCan you give it a try?", "yes, I have try export KRB5CCNAME=\"/tmp/krb5cc_996\", but it also not working on TensorBoard load data from hdfs.\r\nMy Tensorflow can write file to hdfs on kerberos mode. I download it to local system, It can be show on TensorBoard. But TensorBoard can't show the data from hdfs. I wonder is TensorBoard not support file I/O from kerberos cluster HDFS?", "@bd050705 tensorboard uses tensorflow's file system modules for io from hdfs. If writes are supported then reads should ideally be working as well. The logs don't seem to provide the info on the error. Can you attach a log dump where tensorboard raises an error (if applicable)?", "Thank you for your reply. I also can't find any error log from tensorboard debug log. Only Tensorboard UI show \"No dashboards are active for the current data set\". But log had print \"plugin_event_multiplexer.py:201] Starting AddRunsFromDirectory: hdfs://default/tmp/tensorflow/mnist/\" and \"Done with AddRunsFromDirectory: hdfs://default/tmp/tensorflow/mnist/\", it seem to io_wrapper.GetLogdirSubdirectories(path) is empty, so bellow code \"logger.info(\"Adding run from directory %s\", subdir)\" didn't be print out? but if I download this directory to local, events.out data can be display on tensorboard normally.\r\n\r\nhttps://github.com/tensorflow/tensorboard/blob/2.4.1/tensorboard/backend/event_processing/plugin_event_multiplexer.py\r\n        logger.info(\"Starting AddRunsFromDirectory: %s\", path)\r\n        for subdir in io_wrapper.GetLogdirSubdirectories(path):\r\n            logger.info(\"Adding run from directory %s\", subdir)\r\n            rpath = os.path.relpath(subdir, path)\r\n            subname = os.path.join(name, rpath) if name else rpath\r\n            self.AddRun(subdir, name=subname)\r\n        logger.info(\"Done with AddRunsFromDirectory: %s\", path)", "I found that in tensorflow 1.13.1 it had add kerberos support for HDFS, but tensorflow 2.3.1 is not, and my tensorboard use 2.4.1. I also test use tensorflow 2.3.1 read and write kerberos hdfs and result is fail. I wonder is tensorflow not support kerberos for HDFS?\r\n\r\nAdd Kerberos support for HDFS file system #5398\r\n\r\nError log for tensorflow 2.3.1 listdir for kerberos HDFS\r\n\r\n> NotFoundError                             Traceback (most recent call last)\r\n> <ipython-input-20-00718e3957b5> in <module>\r\n>       4 #with gfile.Open(\"hdfs://192.168.140.140:8020/tmp\",'rb') as f:\r\n>       5 #    f.readline()\r\n> ----> 6 tf.io.gfile.listdir('hdfs://default/tmp/')\r\n>       7 #tf.io.gfile.mkdir('hdfs://default/tmp/gfile')\r\n>       8 #pathexist = gfile.Exists('hdfs://default/tmp')\r\n> \r\n> /usr/local/lib64/python3.6/site-packages/tensorflow/python/lib/io/file_io.py in list_directory_v2(path)\r\n>     692         node_def=None,\r\n>     693         op=None,\r\n> --> 694         message=\"Could not find directory {}\".format(path))\r\n>     695 \r\n>     696   # Convert each element to string, since the return values of the\r\n> \r\n> NotFoundError: Could not find directory hdfs://default/tmp/\r\n> \r\n\r\nTensorflow 1.13.1\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/core/platform/hadoop/hadoop_file_system.cc\r\nline 58-77\r\n\r\n>   std::function<hdfsFS(hdfsBuilder*)> hdfsBuilderConnect;\r\n>   std::function<hdfsBuilder*()> hdfsNewBuilder;\r\n>   std::function<void(hdfsBuilder*, const char*)> hdfsBuilderSetNameNode;\r\n>   std::function<int(const char*, char**)> hdfsConfGetStr;\r\n>   std::function<void(hdfsBuilder*, const char*kerbTicketCachePath)>\r\n>       hdfsBuilderSetKerbTicketCachePath;\r\n>   std::function<int(hdfsFS, hdfsFile)> hdfsCloseFile;\r\n>   std::function<tSize(hdfsFS, hdfsFile, tOffset, void*, tSize)> hdfsPread;\r\n>   std::function<tSize(hdfsFS, hdfsFile, const void*, tSize)> hdfsWrite;\r\n>   std::function<int(hdfsFS, hdfsFile)> hdfsHFlush;\r\n>   std::function<int(hdfsFS, hdfsFile)> hdfsHSync;\r\n>   std::function<hdfsFile(hdfsFS, const char*, int, int, short, tSize)>\r\n>       hdfsOpenFile;\r\n>   std::function<int(hdfsFS, const char*)> hdfsExists;\r\n>   std::function<hdfsFileInfo*(hdfsFS, const char*, int*)> hdfsListDirectory;\r\n>   std::function<void(hdfsFileInfo*, int)> hdfsFreeFileInfo;\r\n>   std::function<int(hdfsFS, const char*, int recursive)> hdfsDelete;\r\n>   std::function<int(hdfsFS, const char*)> hdfsCreateDirectory;\r\n>   std::function<hdfsFileInfo*(hdfsFS, const char*)> hdfsGetPathInfo;\r\n>   std::function<int(hdfsFS, const char*, const char*)> hdfsRename;\r\n> \r\n\r\nTensorflow 2.3.1\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/core/platform/hadoop/hadoop_file_system.cc\r\nline 50-68\r\n\r\n>  std::function<hdfsFS(hdfsBuilder*)> hdfsBuilderConnect;\r\n>   std::function<hdfsBuilder*()> hdfsNewBuilder;\r\n>   std::function<void(hdfsBuilder*, const char*)> hdfsBuilderSetNameNode;\r\n>   std::function<int(const char*, char**)> hdfsConfGetStr;\r\n>   std::function<int(hdfsFS, hdfsFile)> hdfsCloseFile;\r\n>   std::function<tSize(hdfsFS, hdfsFile, tOffset, void*, tSize)> hdfsPread;\r\n>   std::function<tSize(hdfsFS, hdfsFile, const void*, tSize)> hdfsWrite;\r\n>   std::function<int(hdfsFS, hdfsFile)> hdfsHFlush;\r\n>   std::function<int(hdfsFS, hdfsFile)> hdfsHSync;\r\n>   std::function<tOffset(hdfsFS, hdfsFile)> hdfsTell;\r\n>   std::function<hdfsFile(hdfsFS, const char*, int, int, short, tSize)>\r\n>       hdfsOpenFile;\r\n>   std::function<int(hdfsFS, const char*)> hdfsExists;\r\n>   std::function<hdfsFileInfo*(hdfsFS, const char*, int*)> hdfsListDirectory;\r\n>   std::function<void(hdfsFileInfo*, int)> hdfsFreeFileInfo;\r\n>   std::function<int(hdfsFS, const char*, int recursive)> hdfsDelete;\r\n>   std::function<int(hdfsFS, const char*)> hdfsCreateDirectory;\r\n>   std::function<hdfsFileInfo*(hdfsFS, const char*)> hdfsGetPathInfo;\r\n>   std::function<int(hdfsFS, const char*, const char*)> hdfsRename;", "@bd050705 yes, seems like the related code was removed in https://github.com/tensorflow/tensorflow/commit/2cad3742600f3e903bc5a2d1a60ef4b571e7c8f9#diff-b6e19034b00093723ad953a6ffb8528b9d311a86a650be0bb7ad58900bfb03f9 due to:\r\n```cc\r\n// KERB_TICKET_CACHE_PATH will be deleted in the future, Because KRB5CCNAME is\r\n// the build in environment variable of Kerberos, so KERB_TICKET_CACHE_PATH\r\n// and related code are unnecessary.\r\n```", "@bd050705 just to test the writes as well, can you please try this and let me know the output:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nbody = b\"1234567\"\r\ntf.io.write_file(\"hdfs://default/testfile.txt\")\r\n\r\ncontent = tf.io.read_file(\"hdfs://default/testfile.txt\")\r\nprint(\"CONTENT: {}\".format(content))\r\nassert content == body\r\n", "But I can't find testfile.txt file on hdfs\r\n[root@cdhhakerberos-cdh-core-kudu-0 ~]# hdfs dfs -ls /\r\nFound 4 items\r\ndrwxr-xr-x   - hbase hbase               0 2021-04-01 00:25 /hbase\r\ndrwxrwxr-x   - solr  solr                0 2021-03-31 01:21 /solr\r\ndrwxrwxrwt   - hdfs  supergroup          0 2021-03-31 18:03 /tmp\r\ndrwxr-xr-x   - hdfs  supergroup          0 2021-03-31 01:43 /user\r\n\r\n\r\nimport tensorflow as tf\r\n\r\nbody = b\"1234567\"\r\ntf.io.write_file(\"hdfs://default/testfile.txt\", b\"1234567\")\r\n\r\ncontent = tf.io.read_file(\"hdfs://default/testfile.txt\")\r\nprint(\"CONTENT: {}\".format(content))\r\nassert content == body\r\n\r\n\r\nCONTENT: Tensor(\"ReadFile_6:0\", shape=(), dtype=string)\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-35-7d2943abc0e0> in <module>\r\n      6 content = tf.io.read_file(\"hdfs://default/testfile.txt\")\r\n      7 print(\"CONTENT: {}\".format(content))\r\n----> 8 assert content == body\r\n\r\nAssertionError: ", "I hope you set the necessary environment variables before running this. As per [docs](https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/hadoop.md).", "Thank you for your remind and reply. After I add \"export LD_LIBRARY_PATH=/usr/java/jdk1.8.0_181-cloudera/jre/lib/amd64/server/\" it work successfully. But on disable Kerberos cluster didn't need add this environment variables it can work normally. ", "Welcome. So is the tensorboard issue solved as well?", "Yes! Thank you very much.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48392\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48392\">No</a>\n"]}]