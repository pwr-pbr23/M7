[{"number": 4491, "title": "Disable flaky rendezvous_test", "body": "b/30476344\n", "comments": []}, {"number": 4490, "title": "Add manual tag to dnn_sampled_softmax_classifier_test for now to unblock", "body": "builds\n", "comments": ["@caisq, thanks for your PR! By analyzing the annotation information on this pull request, we identified @ilblackdragon, @tensorflower-gardener and @itsmeolivia to be potential reviewers\n", "Referencing https://github.com/tensorflow/tensorflow/issues/4463\nReferencing https://github.com/tensorflow/tensorflow/issues/4416\n", "Bug filed and assigned for permanent fix.\n"]}, {"number": 4489, "title": "Fix gif build on Windows", "body": "It fails on Windows because \"'unistd.h': No such file or directory\". \"unistd.h\" is included in some source files but never used. I can manually comment out these includes then the target is buildable. Since it's a new_http_archive, the only fix I can think of is to write a genrule to remove those include statments\nIdeally, we want the include could be configurable, but that's not true in the source code yet. @mrry @damienmg @dslomov\n", "comments": ["@meteorcloudy, thanks for your PR! By analyzing the annotation information on this pull request, we identified @martinwicke and @vrv to be potential reviewers\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n", "Iirc we still build bazel with msys. I am sure we still needs msys to run.\n\nOn Tue, Sep 20, 2016, 8:32 PM Derek Murray notifications@github.com wrote:\n\n> ## _@mrry_ commented on this pull request.\n> \n> In gif.BUILD https://github.com/tensorflow/tensorflow/pull/4489:\n> \n> >  prefix_dir = \"giflib-5.1.4/lib\"\n> > +prefix_dir_windows = \"windows/giflib-5.1.4/lib\"\n> > +\n> > +genrule(\n> > -  name = \"srcs_without_unistd\",\n> > -  srcs = [prefix_dir + \"/\" + source for source in SOURCES],\n> > -  outs = [prefix_dir_windows + \"/\" + source for source in SOURCES],\n> > -  cmd = \"for f in $(SRCS); do \" +\n> > -        \"  sed 's/#include <unistd.h>//g' $$f > $(@D)/%s/$$(basename $$f);\" % prefix_dir_windows +\n> \n> @martinwicke https://github.com/martinwicke: IIUC running Bazel on\n> Windows currently requires msys, but the generated binaries would not\n> depend on it (since they're compiled using MSVC).\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4489, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ADjHf7JQZcsyw1qDRN1SYDVJJYah0edkks5qsHsegaJpZM4KBo8K\n> .\n", "It's interesting since \u200bgiflib (allegedly) is included in major browsers,\nwho was we know do work on Windows. GIF hasn't changed much so maybe a 9\nyear old source isn't so bad?\n"]}, {"number": 4488, "title": "Fuse MirrorPadding into Conv2D to reduce memory usage and decrease latency", "body": "This is similar to the work for fusing bilinear filtering into Conv2D, but this version supports nearest neighbor lookups as part of im2col, so that mirror padding without a bilinear resize present can be efficiently fused.\n", "comments": ["Why PR?\n", "> Why PR?\n\nBecause this is aimed at iOS, so it's more straightforward to test changes directly in github. I won't make this into a habit, and I've asked cwhipkey@ if he minds the extra reviewing burden.\n"]}, {"number": 4487, "title": "config parameter in tf.contrib.learn.Estimator is not working!", "body": "I'm using TensorFlow 0.10 on Ubuntu (GPU). I have an estimator as follow:\n\n```\n  estimator = tf.contrib.learn.Estimator(\n      model_fn=model_fn,\n      model_dir=MODEL_DIR,\n      config=tf.contrib.learn.RunConfig(\n          save_checkpoints_secs=1))\n```\n\nI want to save checkpoints in every second. \nThe problem is that after running, checkpoints are saved every 300 iterations and `config` parameter does not work!\n", "comments": ["@martinwicke Martin - can you take a look at this?  Thanks.\n", "This should work better in 0.11. It was a bug. I'll close this, hopefully this is fixed.\n"]}, {"number": 4486, "title": "why GrpcSession not support RunMetadata in r0.10", "body": "", "comments": ["This support was added after r0.10 was branched; it will be in r0.11, or you can get it now from a nightly build.\n"]}, {"number": 4485, "title": "Unable to retrieve flower_photos.tgz using curl command", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:Ubuntu 12.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nI have installed TensorFlow using docker by running the following command $ docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\nI am trying to retrieve the set of images for TensorFlow for Poets using the following command curl -O http://download.tensorflow.org/example_images/flower_photos.tgz but the below mentioned error is being thrown\n\n~/tf_files$ curl http://download.tensorflow.org/example_images/flower_photo.tgz\n<?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message></Error>\n", "comments": ["Is it \"photo\" or \"photos\"?\n", "```\nhholst@fb-hholst3:/tmp$ curl -O http://download.tensorflow.org/example_images/flower_photos.tgz\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  218M  100  218M    0     0  66.2M      0  0:00:03  0:00:03 --:--:-- 66.3M\nhholst@fb-hholst3:/tmp$ file flower_photos.tgz \nflower_photos.tgz: gzip compressed data, last modified: Wed Feb 10 20:53:41 2016, from Unix\nhholst@fb-hholst3:/tmp$ curl -O http://download.tensorflow.org/example_images/flower_photo.tgz\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   127  100   127    0     0    476      0 --:--:-- --:--:-- --:--:--   481\nhholst@fb-hholst3:/tmp$ file flower_photo.tgz \nflower_photo.tgz: XML 1.0 document text\nhholst@fb-hholst3:/tmp$ cat flower_photo.tgz \n<?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message></Error>hholst@fb-hholst3:/tmp$ \nhholst@fb-hholst3:/tmp$\n```\n"]}, {"number": 4484, "title": "A typo word on you website", "body": "On you website, you typed east instead of each, version r0.10\n", "comments": ["@NickYi1990, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks, you can send a PR to the r0.10 branch to fix this, since it's already fixed at master.\n", "Thanks, Got it!\n"]}, {"number": 4483, "title": "source install transorflow  configure  error", "body": "i install Tensorflow on Ubuntu16.04 GTX1080  CUDA 8.0 CUDNN 5.0\nwhen git Tensorflow configure occure this issue:\n\nchenl@FS-S07:~/tensorflow$ ./configure \n~/tensorflow ~/tensorflow\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\nNo Google Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /usr/local/lib/python2.7/dist-packages\n  /usr/lib/python2.7/dist-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\n\n/usr/local/lib/python2.7/dist-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: \n.\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nERROR: /home/chenl/tensorflow/tensorflow/tensorflow.bzl:571:26: Traceback (most recent call last):\n    File \"/home/chenl/tensorflow/tensorflow/tensorflow.bzl\", line 565\n        rule(attrs = {\"srcs\": attr.label_list...\"), <3 more arguments>)}, <2 more arguments>)\n    File \"/home/chenl/tensorflow/tensorflow/tensorflow.bzl\", line 571, in rule\n        attr.label_list(cfg = \"data\", allow_files = True)\nexpected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead: data.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.\n\nwho can help me > Thank you very much \n", "comments": ["# **## configure again the same issue**\n\nchenl@FS-S07:~/tensorflow$ ./configure \n~/tensorflow ~/tensorflow\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /usr/local/lib/python2.7/dist-packages\n  /usr/lib/python2.7/dist-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\n\n/usr/local/lib/python2.7/dist-packages\nDo you wish to build TensorFlow with GPU support? [y/N] Y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.0\nPlease specify the location where cuDNN 5.0 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nInvalid path to cuDNN  toolkit. Neither of the following two files can be found:\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.0\n/usr/local/cuda-8.0/libcudnn.so.5.0\n.5.0\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.0.5\nPlease specify the location where cuDNN 5.0.5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n\nSending SIGTERM to previous Bazel server (pid=18929)... done.\n.\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\n**ERROR**: /home/chenl/tensorflow/tensorflow/tensorflow.bzl:571:26: Traceback (most recent call last):\n    File \"/home/chenl/tensorflow/tensorflow/tensorflow.bzl\", line 565\n        rule(attrs = {\"srcs\": attr.label_list...\"), <3 more arguments>)}, <2 more arguments>)\n    File \"/home/chenl/tensorflow/tensorflow/tensorflow.bzl\", line 571, in rule\n        attr.label_list(cfg = \"data\", allow_files = True)\nexpected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead: data.\n**ERROR:** com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.\nConfiguration finished\n", "Work-around: Try the r0.10 branch. `git checkout r0.10` and retry compile.\n", "thanks @hholst80  I had the same issue and switching to the the r0.10 branch worked for me.\n", "I also had the same issue. But when I swtiched to the r0.10 branch, other warnings and errors come...\n\n```\nWARNING: /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/protobuf/WORKSPACE:1: Workspace name in /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nWARNING: /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/highwayhash/WORKSPACE:1: Workspace name in /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.\nWARNING: /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/gemmlowp/WORKSPACE:1: Workspace name in /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/gemmlowp/WORKSPACE (@__main__) does not match the name given in the repository's definition (@gemmlowp); this will cause a build error in future versions.\nWARNING: /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/re2/WORKSPACE:1: Workspace name in /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nWARNING: /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/boringssl_git/WORKSPACE:1: Workspace name in /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.\nERROR: /home/jinhyung/tensorflow/tensorflow/core/platform/default/build_config/BUILD:56:1: no such package '@jpeg_archive//': Error downloading from http://www.ijg.org/files/jpegsrc.v9a.tar.gz to /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/jpeg_archive: Error downloading http://www.ijg.org/files/jpegsrc.v9a.tar.gz to /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/jpeg_archive/jpegsrc.v9a.tar.gz: www.ijg.org and referenced by '//tensorflow/core/platform/default/build_config:platformlib'.\nERROR: /home/jinhyung/tensorflow/tensorflow/core/platform/default/build_config/BUILD:56:1: no such package '@jpeg_archive//': Error downloading from http://www.ijg.org/files/jpegsrc.v9a.tar.gz to /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/jpeg_archive: Error downloading http://www.ijg.org/files/jpegsrc.v9a.tar.gz to /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/external/jpeg_archive/jpegsrc.v9a.tar.gz: www.ijg.org and referenced by '//tensorflow/core/platform/default/build_config:platformlib'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\nConfiguration finished\n```\n", "@f8281113 I solve this problem by updating bazel to the latest version. I could install master branch after the updating.\n", "@all Thank you replay .. I  slove this problem too.  I just update nvidia-cuda-toolkit and  nvidia-cuda-dev; **command:**  sudo apt-get install nvidia-cuda-toolkit nvidia-cuda-dev \n", "Beaause when i install torach also happed an  error: (https://github.com/torch/distro/issues/130) ,just update: sudo apt-get install nvidia-cuda-toolkit nvidia-cuda-dev\nthis issue slove too ;But i don't know why..maybe is my nvidvi-card is GTX1080 ......\n", "In Linux, I found the solution was that the config script looks in \"/usr/local/cuda\"\r\nbut the files were in /usr/local/cuda-8.0 (The version I just installed). \r\n\r\nPerhaps something could be added to the config script that incorporates the idea that new cuda versions might have default installations at places like \"/usr/local/cuda-*\" \r\n\r\nHeck, I would even take a side note in the script, \"The default tensorflow location for cuda might vary from what ever the hack NVIDIA is doing now\""]}, {"number": 4482, "title": "WIP: DO NOT MERGE: Upgrade numpy to 1.11.0", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the annotation information on this pull request, we identified @mrry, @ebrevdo and @keveman to be potential reviewers\n", "Closing PR. Changes will be pushed from internal once it passes review.\n"]}, {"number": 4481, "title": "GradOutput in Torch", "body": "Hi all, \nI wonder that is there any function that return the gradOutput (gradient w.r.t. the output of the module) of a specific layer like Torch? \nEx: \nassume f,g are simple linear functions\nf(x)=x'\ng(x')=y\nI want to directly get the value dy/df(x), the gradient of output\ninstead of the gradient of variable in f\n", "comments": ["just use the tf.gradients. solved\n"]}, {"number": 4480, "title": "`UnimplementedError: File system scheme` in sess.restore() with the path including colons.", "body": "I found that we get\n\n```\ntensorflow.python.framework.errors.UnimplementedError: File system scheme {{ part of path }} not implemented\n```\n\nWhen we try to restore the model from the directory which has colons in its path.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n- https://github.com/ibab/tensorflow-wavenet/pull/28\n- https://github.com/carpedm20/variational-text-tensorflow/commit/4dee0ce68814f212d2ece7a9b289c41cc4c35cd6\n- I have searched here tensorflow's issues with the words such as \"colon\", \"File system scheme\", or \"UnimplementedError\" but I couldn't find someone note this.\n### Environment info\n- Mac OS X 10.11.6\n- Python 3.5.2\n- Tensorflow 0.10.0 (from pip, https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0-py3-none-any.whl )\n- Installed version of CUDA and cuDNN: None\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n``` python\nimport tensorflow as tf\n\ns = tf.Session()\nsaver = tf.train.Saver()\nsaver.restore(sess, \"path/to/somewhere/with:colon/model.ckpt-0\") # where the path is exist with valid checkpoint.\n```\n### What other attempted solutions have you tried?\n- Renamed the directory containing the checkpoint => fixed.\n### Logs or other output that would be helpful\n\n```\nTraceback (most recent call last):\n  File \"./generate.py\", line 94, in <module>\n    main()\n  File \"./generate.py\", line 57, in main\n    saver.restore(sess, args.checkpoint)\n  File \"/Users/***/.virtualenvs/***/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1126, in restore\n    if not file_io.get_matching_files(save_path):\n  File \"/Users/***/.virtualenvs/***/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 53, in get_matching_files\n    return pywrap_tensorflow.GetMatchingFiles(compat.as_bytes(filename), status)\n  File \"/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py\", line 66, in __exit__\n    next(self.gen)\n  File \"/Users/***/.virtualenvs/***/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\", line 450, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors.UnimplementedError: File system scheme {{ path/to/somewhere/with }} not implemented\n```\n\nNote `.generate.py` is the user code and {{ path/to/somewhere/with }} was actual path of the model, but terminated before the colon.\n", "comments": ["You can specify your path as \"file://path/to/somewhere/with:colon/model.ckpt-0\" instead of \"path/to/somewhere/with:colon/model.ckpt-0\".\n", "I see. Thank you!\n"]}, {"number": 4479, "title": "the nightly binaries of TF is 404 error", "body": "hello, in the Installation , when i try to install the nightly build \"Linux GPU: Python 2\" , it tells \"404 error\". how should i do ?\n", "comments": []}, {"number": 4478, "title": "Frame of the Variable", "body": "Hello\n\nI am using tf.scan for implementing memory augmented network and when trying to run tf.initialize_all_variables() getting the error about frame of the variables:\n\n```\nAll inputs to node scan_1/while/Variable_13/Assign must be from the same frame.\n```\n\nAs I understood this frame is some inner identification of the variables, so what should I do to get rid of this error? Scan works if I do not define any variables outside the step, but I need at least weights-biases and loss-accuracy variables.\n", "comments": ["@ebrevdo Hi! I saw that you solved same problem for while_loop in https://github.com/tensorflow/tensorflow/issues/3114, maybe here just the same fix is needed?\n", "It sounds like you are trying to use external tensors in scan (ie,\nvariables that are not passed as arguments)\n\nOn Mon, Sep 19, 2016 at 10:59 PM, Linara Adilova notifications@github.com\nwrote:\n\n> @ebrevdo https://github.com/ebrevdo Hi! I saw that you solved same\n> problem for while_loop in #3114\n> https://github.com/tensorflow/tensorflow/issues/3114, maybe here just\n> the same fix is needed?\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4478#issuecomment-248209238,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHNG-DCv0Eim56u8zMAGe_t9i1FVKks5qr3ZagaJpZM4KBDCg\n> .\n", "Yes, I am, because I have weights variables that need to be initialized outside.\nIn my loop I just iterate over the input sequence, but I do not need to iterate weights and I cannot initialize them every step - what is the way to solve this?\nIf needed, I can provide code (at least critical parts of it)\n", "Please provide the critical part of the code.\n\nOn Sep 20, 2016 1:07 PM, \"Linara Adilova\" notifications@github.com wrote:\n\n> Yes, I am, because I have weights variables that need to be initialized\n> outside.\n> In my loop I just iterate over the input sequence, but I do not need to\n> iterate weights and I cannot initialize them every step - what is the way\n> to solve this?\n> If needed, I can provide code (at least critical parts of it)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4478#issuecomment-248418571,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim4WoEhF4_GRyv1zA6G0QeEhrjaPqks5qsD0PgaJpZM4KBDCg\n> .\n", "p.s. is this something for which you can use tf.get_variable?\n\nOn Sep 20, 2016 2:15 PM, wrote:\n\nPlease provide the critical part of the code.\n\nOn Sep 20, 2016 1:07 PM, \"Linara Adilova\" notifications@github.com wrote:\n\nYes, I am, because I have weights variables that need to be initialized\noutside.\nIn my loop I just iterate over the input sequence, but I do not need to\niterate weights and I cannot initialize them every step - what is the way\nto solve this?\nIf needed, I can provide code (at least critical parts of it)\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/tensorflow/tensorflow/issues/4478#issuecomment-248418571,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtim4WoEhF4_GRyv1zA6G0QeEhrjaPqks5qsD0PgaJpZM4KBDCg\n.\n", "> Please provide the critical part of the code.\n\n```\ndef step(prev_step, cur_input):\n        layer_1 = tf.add(tf.matmul(cur_input, weights['h1']), biases['b1'])\n        layer_1 = tf.nn.sigmoid(layer_1)\n\n        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n        layer_2 = tf.nn.sigmoid(layer_2)\n\n        layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n        layer_3 = tf.nn.sigmoid(layer_3)\n\n        key = tf.nn.tanh(tf.add(tf.matmul(layer_3, weights['out']), biases['out']))\n\n        # calculate alpha gateway depending on the key\n        alpha = tf.nn.softmax(tf.add(tf.matmul(key, weights['alpha']), biases['alpha'])) # (batch_size, 1)\n        # calculate write weights as combination of previously read rows and least used rows\n        w_weights = tf.sigmoid(alpha) * prev_step[1] # (batch_size, mem_height) \n\n    # n-th smallest element's index in previous usage_weights\n    nth_smallest = tf.nn.top_k(-1*prev_step[2], k=number_of_reads, sorted=True)[1] # (batch_size, 1)\n\n    linear_index = tf.reshape(nth_smallest, [batch_size]) + (mem_height * tf.range(0,batch_size))\n    linear_w_weights = tf.reshape(w_weights, [-1])\n    ref = tf.Variable(linear_w_weights, trainable=False)\n    least_used_update = tf.reshape((1 - tf.sigmoid(alpha)), [batch_size])\n    w_weights = tf.stop_gradient(tf.reshape(tf.scatter_add(ref, linear_index, least_used_update, use_locking=True), \n                           [batch_size, mem_height]))\n\n    # put to 0 least used row in memory\n    linear_memory = tf.reshape(prev_step[0], [batch_size*mem_height, mem_width])\n    ref1 = tf.Variable(linear_memory, trainable=False)\n    memory = tf.stop_gradient(tf.reshape(tf.scatter_update(ref1, linear_index, tf.zeros([batch_size, mem_width]), use_locking=True),\n                        (batch_size,mem_height,mem_width))) # (batch_size,mem_height,mem_width)\n    # update memory state with write weights\n    # (batch_size, mem_height, mem_width)\n    memory = memory + \\\n            tf.batch_matmul(tf.reshape(w_weights, (batch_size,mem_height,1)), tf.reshape(key, (batch_size,1,mem_width)))\n        normed_key = key / tf.sqrt(tf.reduce_sum(tf.square(key))) # (batch_size, mem_width)\n        # (batch_size, mem_height, mem_width)\n        normed_memory = tf.div(memory, tf.sqrt(tf.reduce_sum(tf.square(memory), 1, keep_dims=True)))\n        # calculate similarities to each memory row\n        # (batch_size, mem_height)\n        similarity = tf.reshape(tf.batch_matmul(normed_memory, tf.reshape(normed_key, (batch_size,mem_width,1))),\n                                (batch_size,mem_height))\n        # calculate read weights as softmax probability distribution\n        r_weights = tf.nn.softmax(similarity) # (batch_size, mem_height)\n        # retrieve memory\n        retrieved_memory = tf.reshape(tf.batch_matmul(tf.reshape(r_weights, (batch_size,1,mem_height)), memory),\n                                      (batch_size,mem_width)) # (batch_size, mem_width)\n        # put retrieved memory through output layer to get prediction\n        do_output = tf.add(tf.matmul(retrieved_memory, weights['do']), biases['do'])\n        prediction = tf.nn.softmax(do_output) # (batch_size, n_classes)\n        # calculate usage weights\n        u_weights = gamma * prev_step[2] + r_weights + w_weights # (batch_size, mem_height)\n\n        return (memory, r_weights, u_weights, prediction) \n\ndef model(elems):\n        _, _, _, predictions = tf.scan(step, elems, \n                                    initializer=(np.zeros((batch_size, mem_height, mem_width)).astype(np.float32), \n                                      np.random.rand(batch_size, mem_height).astype(np.float32),\n                                      np.random.rand(batch_size, mem_height).astype(np.float32),\n                                      np.zeros((batch_size, n_classes)).astype(np.float32)), \n            parallel_iterations=1, back_prop=True, swap_memory=False)\n        return predictions\n\nweights = {\n  'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=stddev)),\n  'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=stddev)),\n  'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3],stddev=stddev)),\n  'out': tf.Variable(tf.random_normal([n_hidden_3, mem_width],stddev=stddev)),\n  'do': tf.Variable(tf.random_normal([mem_width, n_classes],stddev=stddev)),\n  'alpha': tf.Variable(tf.random_normal([mem_width, 1],stddev=stddev))\n}\nbiases = {\n  'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n  'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n  'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n  'out': tf.Variable(tf.random_normal([mem_width])),\n  'do': tf.Variable(tf.random_normal([n_classes])),\n  'alpha': tf.Variable(tf.random_normal([1]))\n}\n\ninput_data = tf.placeholder(\"float32\",[n_classes*n_examples, batch_size, n_input], name='input_data')\nlabels = tf.placeholder(\"float32\",[n_classes*n_examples, batch_size, n_classes],name='labels')\n\npredictions = model(input_data)\n```\n\nAnd after this calling initialize_all_variables() to run causes that error\n\n> p.s. is this something for which you can use tf.get_variable?\n\nWhat exactly this call will do?\n", "I'm having the same error message that is reported in the issue(s). I have a RNN like structure that has some building blocks (component neural networks) that are passed in by the user. Here is a minimal example:\n\n```\nimport tensorflow as tf\ntf.reset_default_graph()\n\ndef initialize(shape):\n    init = tf.random_normal(shape, mean=0, stddev=0.1, dtype=tf.float32)\n    return init\n\ndef test_rnn_with_external(input, hiddens, external_fct):\n    \"\"\"\n    A simple rnn that makes the standard update, then\n    feeds the new hidden state through some external\n    function.\n    \"\"\"\n    dim_in = input.get_shape().as_list()[-1]\n    btsz = input.get_shape().as_list()[1]\n    shape = (dim_in + hiddens, hiddens)\n    _init = initialize(shape)\n    W = tf.get_variable(\"rnn_w\", initializer=_init)\n    _init = tf.zeros([hiddens])\n    b = tf.get_variable(\"rnn_b\", initializer=_init)\n\n    def _step(previous, input):\n        concat = tf.concat(1, [input, previous])     \n        h_t = tf.tanh(tf.add(tf.matmul(concat, W), b))\n\n        h_t = external_fct(h_t)\n\n        return h_t\n\n    h_0 = tf.zeros([btsz, hiddens])\n    states = tf.scan(_step,\n                     input,\n                     initializer=h_0,\n                     name=\"states\")\n    return states\n\n# the external function, relying on the templating mechanism.\ndef ext_fct(hiddens):\n    \"\"\"\n    \"\"\"\n    def tmp(input):\n        shape = (hiddens, hiddens)\n        _init = initialize(shape)\n        W = tf.get_variable(\"ext_w\", initializer=_init)\n        b = 0\n        return tf.add(tf.matmul(input, W), b, name=\"external\")\n    return tf.make_template(name_=\"external_fct\", func_=tmp)\n\n# run from here on\nt = 5\nbtsz = 4\ndim = 2\nhiddens = 3\n\nx = tf.placeholder(tf.float32, shape=(t, btsz, dim))\next = ext_fct(hiddens)\n\nstates = test_rnn_with_external(x, hiddens, external_fct=ext)\n\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_all_variables())\n```\n\nwith the error ending in:\n\n```\nInvalidArgumentError: All inputs to node external_fct/ext_w/Assign must be from the same frame.\n```\n\nWith `Frame`, I would associate an area on the stack. So I thought that maybe `tf.make_template` does something very wired, and thus it is not useable here. The external function can be rewritten a bit and then called more directly, like so:\n\n```\nimport tensorflow as tf\ntf.reset_default_graph()\n\ndef initialize(shape):\n    init = tf.random_normal(shape, mean=0, stddev=0.1, dtype=tf.float32)\n    return init\n\ndef test_rnn_with_external(input, hiddens, external_fct):\n    dim_in = input.get_shape().as_list()[-1]\n    btsz = input.get_shape().as_list()[1]\n    shape = (dim_in + hiddens, hiddens)\n    _init = initialize(shape)\n    W = tf.get_variable(\"rnn_w\", initializer=_init)\n    _init = tf.zeros([hiddens])\n    b = tf.get_variable(\"rnn_b\", initializer=_init)\n\n    def _step(previous, input):\n        \"\"\"\n        \"\"\"\n        concat = tf.concat(1, [input, previous])     \n        h_t = tf.tanh(tf.add(tf.matmul(concat, W), b))\n\n        h_t = external_fct(h_t, hiddens)\n\n        return h_t\n\n    h_0 = tf.zeros([btsz, hiddens])\n    states = tf.scan(_step,\n                     input,\n                     initializer=h_0,\n                     name=\"states\")\n    return states\n\ndef ext_fct_new(input, hiddens):\n    \"\"\"\n    \"\"\"\n    shape = (hiddens, hiddens)\n    _init = initialize(shape)\n    W = tf.get_variable(\"ext_w_new\", initializer=_init)\n    b = 0\n    return tf.add(tf.matmul(input, W), b, name=\"external_new\")\n\nt = 5\nbtsz = 4\ndim = 2\nhiddens = 3\nx = tf.placeholder(tf.float32, shape=(t, btsz, dim))\n\nstates = test_rnn_with_external(x, hiddens, external_fct=ext_fct_new)\n\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_all_variables())\n```\n\nHowever, still the same error `InvalidArgumentError: All inputs to node ext_w_new/Assign must be from the same frame.`\n\nOf course, moving contents of the external function into the `_step` part (and `tf.get_variable`ing before) works. But then the flexibility (necessary in the original code) is gone.\n\nWhat am I doing wrong?\n", "This is probably related to #2211\n", "Can you do this without the templating mechanism?\n\nOn Oct 21, 2016 2:55 PM, \"Christian\" notifications@github.com wrote:\n\n> I'm having the same error message that is reported in the issue(s). I have\n> a RNN like structure that has some building blocks (component neural\n> networks) that are passed in by the user. Here is a minimal example:\n> \n> import tensorflow as tf\n> tf.reset_default_graph()\n> \n> def initialize(shape):\n>     init = tf.random_normal(shape, mean=0, stddev=0.1, dtype=tf.float32)\n>     return init\n> \n> def test_rnn_with_external(input, hiddens, external_fct):\n>     \"\"\"\n>     A simple rnn that makes the standard update, then\n>     feeds the new hidden state through some external\n>     function.\n>     \"\"\"\n>     dim_in = input.get_shape().as_list()[-1]\n>     btsz = input.get_shape().as_list()[1]\n>     shape = (dim_in + hiddens, hiddens)\n>     _init = initialize(shape)\n>     W = tf.get_variable(\"rnn_w\", initializer=_init)\n>     _init = tf.zeros([hiddens])\n>     b = tf.get_variable(\"rnn_b\", initializer=_init)\n> \n> ```\n> def _step(previous, input):\n>     concat = tf.concat(1, [input, previous])\n>     h_t = tf.tanh(tf.add(tf.matmul(concat, W), b))\n> \n>     h_t = external_fct(h_t)\n> \n>     return h_t\n> \n> h_0 = tf.zeros([btsz, hiddens])\n> states = tf.scan(_step,\n>                  input,\n>                  initializer=h_0,\n>                  name=\"states\")\n> return states\n> ```\n> \n> # the external function, relying on the templating mechanism.\n> \n> def ext_fct(hiddens):\n>     \"\"\"\n>     \"\"\"\n>     def tmp(input):\n>         shape = (hiddens, hiddens)\n>         _init = initialize(shape)\n>         W = tf.get_variable(\"ext_w\", initializer=_init)\n>         b = 0\n>         return tf.add(tf.matmul(input, W), b, name=\"external\")\n>     return tf.make_template(name_=\"external_fct\", func_=tmp)\n> \n> # run from here on\n> \n> t = 5\n> btsz = 4\n> dim = 2\n> hiddens = 3\n> \n> x = tf.placeholder(tf.float32, shape=(t, btsz, dim))\n> ext = ext_fct(hiddens)\n> \n> states = test_rnn_with_external(x, hiddens, external_fct=ext)\n> \n> sess = tf.InteractiveSession()\n> sess.run(tf.initialize_all_variables())\n> \n> with the error ending in:\n> \n> InvalidArgumentError: All inputs to node external_fct/ext_w/Assign must be from the same frame.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4478#issuecomment-255474721,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim27Ix4gaRbkMdbrS6mw9SglCr7iJks5q2TS5gaJpZM4KBDCg\n> .\n", "No, does not work without templating either. The second code part in the previous post tries this, leading to the same error message.\n", "Yuan pushed improved error messaging today. Can you try using a fresh build\nof tensorflow tomorrow and report back the more informative error messages?\n\nOn Oct 25, 2016 2:11 PM, \"Christian\" notifications@github.com wrote:\n\n> No, does not work without templating either. The second code part in the\n> previous post tries this, leading to the same error message.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4478#issuecomment-256177104,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim5R7AQ7ygprwKcS9m1qh_vETglJ1ks5q3nCIgaJpZM4KBDCg\n> .\n", "I have the same problem. Passing an initializer function instead of a tensor/constant as `tf.get_variable(name, shape, initializer=init_fun)` works as a work-around, though.\n", "@akosiorek Thank you! I didn't try your approach, but a similar one suggested by @mrry here: http://stackoverflow.com/questions/42564698/invalidargumenterror-the-node-has-inputs-from-different-frames. A `tf.constant_initializer` resolves the described problem.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 4477, "title": "Branch 133630671", "body": "", "comments": ["Jenkins, test this please\n", "@tensorflow-jenkins test this please  (GPU flaky)\n"]}, {"number": 4476, "title": "build errors on missing \"//base\"", "body": "When running ./configure on OS X with CUDA support enabled with bazel 0.3.1-homebrew, these errors occur:\n\n```\nERROR: /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:692:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core:ios_tensorflow_test_lib'.\nERROR: /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_proto_lib_no_rtti_lite_runtime': target 'android_proto_lib_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /Users/jmhodges/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\n```\n\nThis seems to be because of targets like \"ios_tensorflow_test_lib\" and, so on, depending on a `//base` and `//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime` that do not exist.\n\nOff hand, I'm not able to figure out what this was trying to refer to. Perhaps it used to exist?\n", "comments": ["(Added bazel version to ticket: 0.3.1-homebrew)\n", "This seems to have been introduced in ed87884e50e1a50f7dc7b36dc7a7ff225442bee0. Perhaps, it's a Google internal thing that just slipped in?\n", "Same deal with `//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime`. It just doesn't exist.\n", "Looks like this only happens when you try to build with CUDA.\n", "@jmhodges Yes, those are some references that weren't meant to be interpreted by the external version of TF.\n\nThis is a duplicate of #4312, where you can find workarounds for the errors you're seeing -- essentially just revert/comment out the new functions.\n", "Ah, it's been 10 days since that ticket. Is there no fix I could send up to handle this?\n", "(I'm trying to confirm #4145 now that configure has some nice new CUDA smarts, and it's a little bit harder to confirm with these changes not in master. I'm sure I'm not the only external contributor trying to help and having to rediscover this!)\n"]}, {"number": 4475, "title": "Add auto-generated version_info.cc to .gitignore.", "body": "This file is generated in-place during the CMake build, which risks it being checked into the repository.\n", "comments": ["@mrry, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @rasbt and @girving to be potential reviewers\n"]}, {"number": 4474, "title": "Feature request: Patch extracting given location implementation needed.", "body": "Hello there,\n## Feature\n\nI consider a lot but finally decide put this request here.\nI know there are some matrix operation and image operations like `image_patch_extract` and `image.extract_glimpse()`.\nBut what I want is extract the patches given several locations.\n\nInput Tensor: \n\n``` python\n         batch image: [batch_size, image_height, image_width, image_channel]\n         batch locations: [batch_size, num_patches, 2]\n```\n\nOutput Tensor: \n\n``` python\n        [batch_size, num_patches, patch_height, patch_width, image_channel]\nor \n        [batch_size * num_patches, patch_height, patch_width, image_channel]\n```\n## Reference:\n\nGeorgis have done the similar thing and he made the a patch in v0.8.0 and only cpu supports.\n\n```\nhttps://github.com/trigeorgis/tensorflow\n```\n\nBut when I use it in v0.10.0, it requires me to define a shape function. \nI really want to use it in the future, so I am glad it can be added as a new feature.\n## Other solution I tried\n\nI have tried use `extract_glimpse()` instead.\n\n``` python\noutput_list = [[] for _ in range(batch_size)]   # create n_patch list\nlocations = locations/image_size  # normalize the location\nfor j in range(n_patch):\n    patch_one = tf.image.glimpse(batch_image, tf.constant([20, 20]), locations[:, j, :], centered=False)\n    for i in range(batch_size):\n          output_list[i].append(patch_one[j])  # add tensor to each list\npatches = tf.pack(output_list)  # pack the list to tensor, size = [batch_size, n_patch, patch_size, patch_height, image_channel]\n```\n", "comments": ["Noticed this was open while searching for something that could do this. Looks like TensorFlow now has an [extract_image_patches](https://www.tensorflow.org/versions/master/api_docs/python/tf/extract_image_patches) op. Does this solve the request?"]}, {"number": 4473, "title": "Make the gradient computation available in the C api", "body": "When writing TensorFlow bindings for other languages, a major difficulty is that the C api does not give a simple way to add gradient computations to a graph. In the case of the [tensorflow-ocaml](https://github.com/LaurentMazare/tensorflow-ocaml) bindings, we ended up re-implementing back-propagation and having to register gradients for many operators. Providing support for this in the C api would allow us to remove all this code.\n\nThere is already a [todo](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/c/c_api.h#L505) for this in the code but having a proper issue would make this easier to track.\n", "comments": ["We are actually already working on this and will have it included in the release notes when it is public (no eta yet though). So I will close this for now.\n"]}, {"number": 4472, "title": "Revised CMake rules for SWIG wrapper generation.", "body": "Instead of generating a separate `python_deps.so` and `_pywrap_tensorflow.so`, we now generate a single `_pywrap_tensorflow.so` containing the entire runtime, like the Bazel build. This simplifies the linkage on other platforms.\n", "comments": ["@tensorflow-jenkins test this please.\n", "@gunan PS. Is there some way to configure the presubmits to only run the CMake build if the changes are all under `tensorflow/contrib/cmake`? Sorry that this wastes so much time on the Jenkins cluster!\n", "@gunan - I think the workspace for the CMake build might have some leftover files in it that the ci_build doesn't have permission to delete. Is that possible?\n", "I think cmake build is configured to do exactly that, only run if 1-everything else passes, 2-it changes something in cmake folder. \nI think what we want to do is just not wait for other tests, run anytime there is a change in cmake folder.\n\nFor the 2nd question. It runs inside a docker container. Inside that container, it runs as root. \nDdid you see some unexpected behaviour? I can check if there is a permissions issue on the cmake test.\n", "Jenkins, test this please.\n", "AFAIK, the build [start by deleting any leftover build directory](http://stackoverflow.com/questions/39582974/regarding-the-tensor-shape-is-1) and this seems to be failing with `Permission denied` errors. Do we deprivilege before running a build? (Is that what [`with_the_same_user`](https://github.com/tensorflow/tensorflow/blob/56eb8376260ef0ddb6389920dabc7f636d21cbff/tensorflow/tools/ci_build/builds/with_the_same_user) is meant to do?) \n"]}, {"number": 4471, "title": "Add `tf.write_file` op", "body": "Tensorflow currently has a `tf.read_file` op, as documented [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard1/tf.read_file.md).\n\nCan we have a `tf.write_file` as well?\n\nIt could be defined as something like:\n\n``` python\ndef write_file(tensor, filename, name=None):\n    def write_data(data):\n        with open(filename, \"wb\") as handle:\n            handle.write(data)\n\n    with tf.name_scope(\"WriteFile\" or name):\n        tensor = tf.convert_to_tensor(tensor, name=\"Values\")\n        return tf.py_func(write_data, tensor)\n```\n\nThere are some usecases where this would be helpful, such as generating images and audio and then writing them to disk. Presumably, this would be done after an op such as `encode_jpeg` or `encode_audio`.\n\nIf this already exists in some form, please close this \u2013 my apologies, but some googling as well as searching on Github did not find it.\n", "comments": ["Contributions welcome!  As a first cut I think the above will work (albeit might not be the most efficient), and we need to use TensorFlow's `file_io` instead of the raw `with open(...) as ...` -- this is to support many platform and file systems.\n", "I am currently working on this.\nWould you point me where is the `read_file` op implemented ? I looked at \n**tensorflow/python/ops/io_ops.py** but I can't see it there.\n\nThanks,\nMoustafa\n", "@malzantot I believe the op is implemented [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/whole_file_read_ops.cc#L100), using helper function [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/whole_file_read_ops.cc#L30).\n\nYou'll probably want to use [NewWritableFile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/env.h#L103) in place of NewRandomAccessFile and use the resulting file's [Append](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.h#L248) member function.\n"]}, {"number": 4470, "title": "Fix path for `get_regularization_losses`", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@cancan101, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener and @lukaszkaiser to be potential reviewers\n", "@tensorflow-jenkins Test this please!\n"]}, {"number": 4469, "title": "Mac GPU installation", "body": "After I run a python3 script I get the following statements and do not know where the 3 errors are coming from. I am using cudnn v5.0 but obviously I have gone wrong somewhere along the installation pipeline. Any help would be fantastic.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally\nnumber of elements at final reshape = %d. 61440\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] OS X does not support NUMA - returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GT 750M\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9255\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.28GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:354] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:361] error retrieving driver version: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got \"\"\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:321] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nF tensorflow/core/kernels/conv_ops.cc:457] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\nAbort trap: 6\n```\n", "comments": ["you are using macbook? \n", "yes\n", "Please post this to stackoverflow. Thanks.\n", "+1\n", "@mattdns100689 I'm having the same issue. Do you have the link to stackoverflow?\n", "@estebanbouza http://stackoverflow.com/questions/39685176/tensor-flow-mac-gpu-installation\n"]}, {"number": 4468, "title": "Allow crop_to_bounding_box to work batchwise", "body": "Right now `crop_to_bounding_box` only works with single images. \n", "comments": ["This can be done with tensor slicing.\n", "can you give an example of your solution, please? I am running into this problem atm.. Thanks", "sorry I got it. I suppose you meant the solution is to reimplement `crop_image_to_bounding_box` which uses the slice operator:\r\n\r\n```python\r\ndef _is_tensor(x):\r\n    \"\"\"Returns `True` if `x` is a symbolic tensor-like object.\r\n    Args:\r\n      x: A python object to check.\r\n    Returns:\r\n      `True` if `x` is a `tf.Tensor` or `tf.Variable`, otherwise `False`.\r\n    \"\"\"\r\n    return isinstance(x, (tf.Tensor, tf.Variable))\r\n\r\n\r\ndef crop_to_bounding_box(image, offset_height, offset_width, target_height,\r\n                         target_width):\r\n    _, height, width, depth = image.get_shape().as_list()\r\n\r\n    cropped = tf.slice(image,\r\n                       tf.pack([0, offset_height, offset_width, 0]),\r\n                       tf.pack([-1, target_height, target_width, -1]))\r\n\r\n    cropped_shape = [None if _is_tensor(i) else i\r\n                     for i in [None, target_height, target_width, depth]]\r\n    cropped.set_shape(cropped_shape)\r\n\r\n    return cropped\r\n```"]}, {"number": 4467, "title": " Why 'tf.python_io.TFRecordWriter' is so SLOW and STORAGE-CONSUMING in TensorFlow?", "body": "I'm going to write to TFRecord file using this [code](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/examples/how_tos/reading_data/convert_to_records.py#L68) The problem is that this process is very slow, such that it's not feasible to write a large dataset even in days! It's just a writer that serialize to disk. Why it's so slow?! Another problem is that the size of the output file is 10 times greater than the original file!\n\ndoes anyone know any way to speed up the process of TFRecordWriter and compress the result?\n", "comments": ["Regarding compression, see https://github.com/tensorflow/tensorflow/blob/4addf4b5806cd731949c6582a83f5824599cd1ef/tensorflow/python/lib/io/tf_record.py#L26\n", "@fwalch   thanks,  but the most important thing is the speed.  I have to split the file to 100000 slice and using multiprocessing to process each.  \nis there any optimize method for speed up ?  \n", "is there a c++ demo for tfrecord? maybe faster?\n", "I'm also having trouble reading from TFRecord. I only get 25MB/s instead of the 250MB/s I could get out of the SSD, single threaded.\n", "@ericyue I'm having some succes with FixedLengthRecordReader. It reads at 150-160MB/s from SSD\n", "I'm having a similar problem; TFRecordWriter doesn't appear to be the issue, rather it's the protobuf library. BytesList() is slow, and SerializeToString() is mega-slow. Writing 1 MiB:\n\n```\n%%prun\ndef quantize(x):\n    return x.astype(np.int8).tostring()\nfeats = {'':tf.train.Feature(bytes_list=tf.train.BytesList(value=quantize(np.ones((2**10, 2**10)))))}\nexample = tf.train.Example(features=tf.train.Features(feature=feats))\nwriter = tf.python_io.TFRecordWriter('/dev/null')\nwriter.write(example.SerializeToString())\n```\n\n```\n         15729132 function calls (15729090 primitive calls) in 6.355 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        2    1.679    0.839    2.488    1.244 encoder.py:260(RepeatedFieldSize)\n        1    1.292    1.292    2.881    2.881 encoder.py:711(EncodeRepeatedField)\n  1048580    0.701    0.000    1.013    0.000 encoder.py:372(EncodeVarint)\n  3145731    0.535    0.000    0.535    0.000 containers.py:202(__getitem__)\n  3145748    0.440    0.000    0.440    0.000 {method 'write' of '_io.BytesIO' objects}\n        1    0.366    0.366    0.841    0.841 containers.py:261(extend)\n  1048576    0.359    0.000    0.471    0.000 type_checkers.py:100(CheckValue)\n3145753/3145751    0.289    0.000    0.289    0.000 {len}\n  2097157    0.283    0.000    0.283    0.000 encoder.py:82(_VarintSize)\n  1048592    0.150    0.000    0.150    0.000 {chr}\n  1048621    0.113    0.000    0.113    0.000 {isinstance}\n        1    0.109    0.109    6.355    6.355 <string>:2(<module>)\n        6    0.021    0.003    0.021    0.003 {method 'extend' of 'list' objects}\n     16/6    0.003    0.000    0.862    0.144 python_message.py:474(init)\n      9/4    0.003    0.000    2.495    0.624 python_message.py:1035(ByteSize)\n      5/1    0.003    0.001    5.383    5.383 python_message.py:1077(InternalSerialize)\n        1    0.003    0.003    0.003    0.003 {_pywrap_tensorflow.PyRecordWriter_WriteRecord}\n        1    0.003    0.003    0.003    0.003 {method 'astype' of 'numpy.ndarray' objects}\n        1    0.002    0.002    0.002    0.002 {numpy.core.multiarray.copyto}\n        1    0.001    0.001    0.001    0.001 {method 'getvalue' of '_io.BytesIO' objects}\n        1    0.000    0.000    0.000    0.000 {method 'tostring' of 'numpy.ndarray' objects}\n     10/5    0.000    0.000    0.018    0.004 python_message.py:1233(MergeFrom)\n        1    0.000    0.000    0.000    0.000 {_pywrap_tensorflow.PyRecordWriter_New}\n       21    0.000    0.000    0.000    0.000 python_message.py:1362(__init__)\n       12    0.000    0.000    0.000    0.000 python_message.py:790(ListFields)\n        8    0.000    0.000    0.000    0.000 python_message.py:429(MakeSubMessageDefault)\n       12    0.000    0.000    0.000    0.000 python_message.py:775(_IsPresent)\n       12    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n        5    0.000    0.000    0.017    0.003 containers.py:280(MergeFrom)\n        5    0.000    0.000    0.000    0.000 containers.py:541(__getitem__)\n       18    0.000    0.000    0.000    0.000 python_message.py:1381(Modified)\n      4/1    0.000    0.000    5.382    5.382 encoder.py:760(EncodeField)\n       40    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n    21/12    0.000    0.000    0.000    0.000 python_message.py:1317(Modified)\n        5    0.000    0.000    0.000    0.000 python_message.py:1397(__init__)\n      2/1    0.000    0.000    0.000    0.000 python_message.py:1141(IsInitialized)\n        6    0.000    0.000    0.000    0.000 python_message.py:421(MakeRepeatedScalarDefault)\n        1    0.000    0.000    4.130    4.130 encoder.py:818(EncodeField)\n        4    0.000    0.000    0.000    0.000 type_checkers.py:162(CheckValue)\n        1    0.000    0.000    5.384    5.384 python_message.py:1071(SerializePartialToString)\n        1    0.000    0.000    0.003    0.003 <string>:2(quantize)\n        1    0.000    0.000    1.246    1.246 encoder.py:351(FieldSize)\n        1    0.000    0.000    0.002    0.002 numeric.py:136(ones)\n        5    0.000    0.000    0.000    0.000 python_message.py:1406(Modified)\n        2    0.000    0.000    0.000    0.000 python_message.py:660(field_setter)\n        6    0.000    0.000    0.000    0.000 containers.py:237(__init__)\n       21    0.000    0.000    0.000    0.000 {_weakref.proxy}\n      5/2    0.000    0.000    2.488    1.244 encoder.py:307(FieldSize)\n        1    0.000    0.000    0.004    0.004 containers.py:595(MergeFrom)\n        1    0.000    0.000    0.000    0.000 tf_record.py:57(__init__)\n        8    0.000    0.000    0.000    0.000 python_message.py:537(_GetFieldByName)\n        5    0.000    0.000    0.000    0.000 python_message.py:1332(_UpdateOneofState)\n        6    0.000    0.000    0.000    0.000 containers.py:192(__init__)\n       10    0.000    0.000    0.000    0.000 python_message.py:1004(SetListener)\n        2    0.000    0.000    0.000    0.000 {setattr}\n        1    0.000    0.000    0.000    0.000 python_message.py:271(_IsMapField)\n        5    0.000    0.000    0.000    0.000 containers.py:206(__len__)\n        1    0.000    0.000    0.000    0.000 {numpy.core.multiarray.empty}\n        1    0.000    0.000    0.003    0.003 tf_record.py:78(write)\n        2    0.000    0.000    0.000    0.000 python_message.py:379(MakeMessageMapDefault)\n        1    0.000    0.000    0.000    0.000 {_codecs.utf_8_decode}\n        1    0.000    0.000    5.384    5.384 python_message.py:1057(SerializeToString)\n        2    0.000    0.000    0.000    0.000 python_message.py:651(getter)\n        1    0.000    0.000    0.003    0.003 pywrap_tensorflow.py:183(WriteRecord)\n        2    0.000    0.000    0.000    0.000 containers.py:525(__init__)\n        2    0.000    0.000    0.000    0.000 descriptor.py:118(GetOptions)\n        1    0.000    0.000    0.000    0.000 {method 'decode' of 'str' objects}\n        2    0.000    0.000    0.000    0.000 containers.py:586(__len__)\n        5    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}\n       12    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n       12    0.000    0.000    0.000    0.000 python_message.py:792(<lambda>)\n        1    0.000    0.000    0.000    0.000 compat.py:27(as_bytes)\n        1    0.000    0.000    0.000    0.000 utf_8.py:15(decode)\n        3    0.000    0.000    0.000    0.000 containers.py:589(__iter__)\n        4    0.000    0.000    0.000    0.000 {iter}\n        1    0.000    0.000    0.000    0.000 python_message.py:277(_IsMessageMapField)\n        1    0.000    0.000    0.000    0.000 pywrap_tensorflow.py:182(<lambda>)\n        2    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n\n```\n\n```\ndef quantize(x):\n    return x.astype(np.int8).tostring()\nwriter = tf.python_io.TFRecordWriter('/dev/null')\n%time q = quantize(np.ones((2**10, 2**10))) #fast\n%time bl = tf.train.BytesList(value=q) #slow\n%time example = tf.train.Example(features=tf.train.Features(feature={'':tf.train.Feature(bytes_list=bl)})) #fast\n%time s = example.SerializeToString() #very slow\n%time writer.write(s) #fast\n```\n\n```\nCPU times: user 8 ms, sys: 0 ns, total: 8 ms\nWall time: 5.11 ms\nCPU times: user 440 ms, sys: 4 ms, total: 444 ms\nWall time: 440 ms\nCPU times: user 20 ms, sys: 0 ns, total: 20 ms\nWall time: 20.4 ms\nCPU times: user 2.21 s, sys: 16 ms, total: 2.23 s\nWall time: 2.22 s\nCPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 2.81 ms\n```\n\ni'm using the docker image tensorflow/tensorflow:latest-gpu with nvidia-docker on ubuntu 16.04\n", "Try installing a protobuf-python wheel that we have built that uses optimized C++ code internally: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues\n", "@vrv thanks! I missed that.\nthe pip command succeeded, except for some SSL warnings, but now:\n\n```\nroot@58fb67668f0f:/notebooks# cat test.py\nimport tensorflow as tf\nroot@58fb67668f0f:/notebooks# gdb python\nGNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1\n...\nReading symbols from python...Reading symbols from /usr/lib/debug//usr/bin/python2.7...done.\ndone.\n(gdb) run test.py\nStarting program: /usr/bin/python test.py\nwarning: Error disabling address space randomization: Operation not permitted\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:1562] Unable to load cuDNN DSO\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nDuring startup program terminated with signal SIGSEGV, Segmentation fault.\n(gdb) bt\nNo stack.\n(gdb)\n```\n\nlooks like I have a cuDNN problem too, but that was present already and I assume it is unrelated. If I pip uninstall protobuf, the cuda messages still print before protobuf throws an importerror.\n", "@victor-shepardson: We're not sure why but the binary protobuf wheel we build doesn't work for everyone and results in segfaults for some, but it does work for others.\n\nYou might need to figure out how to build the optimized python protobuf package on your own.  Hopefully the github.com/google/protobuf folks can help.\n", "I'm going to close this, since it looks like the problem is related to not using the optimized protobuf/python package, and isn't strictly related to TF.  Thanks for the helpful performance debugging @victor-shepardson !\n", "thanks , I will try the c++ version of protobuf", "can you show me a demo code for using FixedLengthRecordReader to read tfrecord files? I think the tfrecord are not fixed length , how to do this ? @titusnicolae ", "According to google/protobuf#1332 the wheel on pypi for protobuf 3.2.0 now includes the C++ implementation and uses it by default (although maybe not yet on OS X). I was able to verify this on linux x86_64:\r\n```\r\n$ pip show protobuf\r\nName: protobuf\r\nVersion: 3.2.0\r\nSummary: Protocol Buffers\r\nHome-page: https://developers.google.com/protocol-buffers/\r\nAuthor: protobuf@googlegroups.com\r\nAuthor-email: protobuf@googlegroups.com\r\nLicense: New BSD License\r\nLocation: /home/ed/.pyenv/versions/3.6.1/envs/py36/lib/python3.6/site-packages\r\nRequires: six, setuptools\r\n$ python -c \"from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)\"\r\ncpp\r\n```", "Unfortunately this doesn't seem the case on Windows (x64):\r\n```\r\n$ pip show protobuf\r\nName: protobuf\r\nVersion: 3.5.1\r\nSummary: Protocol Buffers\r\nHome-page: https://developers.google.com/protocol-buffers/\r\nAuthor: protobuf@googlegroups.com\r\nAuthor-email: protobuf@googlegroups.com\r\nLicense: 3-Clause BSD License\r\nLocation: c:\\users\\patrik\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\r\nRequires: six, setuptools\r\n\r\n$ python -c \"from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)\"\r\npython\r\n```", "I'm on Windows these days and I get the same result on cpython 3.6 :(", "On Windows it seems like the protobuf is still the old version. I'm getting extremely slow speeds writing TFRecords even with multiprocessing, at this point I'd be better off using something like Keras sequence.", "TFRecordWriter was incredibly slow for me. The only thing that worked for me was converting the output tensors I wanted to save to numpy arrays, and then saving using pyarrow. Here is some code if it helps anybody:\r\n\r\n```python\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport shutil, os\r\nimport numpy as np\r\n\r\n# For faster writing of tensorflow output\r\nclass ParquetWriter():\r\n  # context manager stuff\r\n  \"\"\" dir_path: directory to put parquet files\r\n    cols: column names for schema\r\n    chunk_size: how many rows per file = chunk_size*buffer_size\r\n    buffer_size: how many rows to buffer before writing (not exact, if using add_rows)\r\n    overwrite: delete existing directory/file if found  \r\n  \"\"\"\r\n  def __init__(self, dir_path, cols, chunk_size=10, buffer_size=5000, overwrite=True):\r\n    self.chunk_size = chunk_size\r\n    self.buffer_size = buffer_size\r\n    self.overwrite = overwrite\r\n    self.dir_path = dir_path\r\n    self.cols = cols\r\n    self.chunk = 0\r\n    self.file = 0\r\n    self.writer = None\r\n    self.new_buffer()\r\n  def __enter__(self): \r\n      return self\r\n  def __exit__(self, exc_type, exc_value, exc_traceback):\r\n    if self.writer:\r\n      self.flush()\r\n      self.writer.close()\r\n  # clears buffer (does not write)\r\n  def new_buffer(self):\r\n    self.buffer = []\r\n    for i in range(len(self.cols)):\r\n      self.buffer.append([])\r\n    self.buffered = 0\r\n  # actually perform write to parquet file\r\n  def flush(self):\r\n    print(\"parquet write chunk {}, rows {}\".format(self.chunk, self.buffered))\r\n    # convert to chunked pyarrow arrays, instead of list of numpy arrays\r\n    for i in range(len(self.buffer)):\r\n      bi = self.buffer[i]\r\n      # apparently pyarrow can't do > 1d arrays; but doesn't complain if its a list\r\n      if len(bi[0].shape) > 1:\r\n        self.buffer[i] = np.vstack(bi).tolist()\r\n      else:\r\n        self.buffer[i] = pa.chunked_array(bi)\r\n    tbl = pa.Table.from_arrays(self.buffer, names=self.cols)\r\n    # create new file\r\n    if not self.chunk:\r\n      # create/overwrite existing directory/file\r\n      if not self.file and self.overwrite:\r\n        if os.path.exists(self.dir_path):\r\n          if os.path.isfile(self.dir_path):\r\n            os.remove(self.dir_path)\r\n          else:\r\n            shutil.rmtree(self.dir_path)\r\n        os.makedirs(self.dir_path)\r\n      self.writer = pq.ParquetWriter(os.path.join(self.dir_path, \"chunk_{}.part\".format(self.file)), tbl.schema)\r\n    self.writer.write_table(tbl)\r\n    self.chunk += 1\r\n    self.new_buffer()\r\n    # finished file\r\n    if self.chunk == self.chunk_size:\r\n      print(\"finished file {}\".format(self.file))\r\n      self.chunk = 0\r\n      self.file += 1\r\n      self.writer.close()\r\n      self.writer = None\r\n  # buffer/write multiple rows\r\n  def add_rows(self, rcols):\r\n    assert len(rcols) == len(self.cols), \"Doesn't match column schema length\"\r\n    self.buffered += len(rcols[0])\r\n    for i in range(len(self.buffer)):\r\n      self.buffer[i].append(rcols[i])\r\n    if self.buffered >= self.buffer_size:\r\n      self.flush()\r\n```\r\n\r\nUse it like so:\r\n```python\r\nwith ParquetWriter(\"some_directory_path\", [\"col1\",\"col2\"]) as f:\r\n  # perform inference here\r\n  f.add_rows([out_col1.numpy(), out_col2.numpy()])\r\n```", "I was having the same problem writing a large dataset -- the writing speed would get slower the longer the process ran (starting at 1iteration/sec and going up to 11sec/iteration). \r\n\r\nThe way I solved it was to spin up sub-processes in a loop to write smaller blocks of the data. Ideally the blocks would be small enough so that the writing would remain at 1iteration/second. \r\n\r\nFor example my slow code originally was something like:\r\n```\r\npool = multiprocessing.Pool(n_processes)\r\nprocess_blocked_data = split(data, n_processes) \r\nfor block in process_blocked_data: \r\n    pool.start(write_tfrecord, args=(block))\r\npool.join()\r\n```\r\nAnd I fixed it by splitting up the data even more:\r\n```\r\n# split the data into even smaller blocks\r\nfactor = 100 \r\nblocked_data = split(data, factor) \r\n\r\nfor block in blocked_data: \r\n   # assign smaller block to each process\r\n   pool = multiprocessing.Pool(n_processes)\r\n   process_blocks = split(block, n_processes) \r\n   for smaller_pblock in process_blocks: \r\n       pool.start(write_tfrecord, args=(smaller_pblock))\r\n   pool.join()\r\n```\r\n\r\nDoing it this way reduced the total write time from 11+hours to 1hour. \r\n\r\nMy best guess is that the writing slows down the longer the process is run because something is written/added to the graph at each write command which ends up leading to a very large graph which slows write speeds down significantly. \r\n\r\nFull Code: https://github.com/gebob19/TFRecords_4_videos"]}, {"number": 4466, "title": "Android: Why is RandomShuffleQueue needed for testing?", "body": "Hello, \n\nI want to test a learnt model under Python on android, I'm on Tensorflow 0.9.\nTo do so, I freezed my graph to have a single pb file with the graph and weight. I used Queues to manage my learning batches.\n\nWhen running my session on Android, I specify the input tensor by its name \"input_node\", which is the data layer as input in my network. \nX = tf.reshape(X, [-1, W, H, 1], name=\"input_node\")\nand call the \"output_node\" layer : \noutput = tf.reshape(h_fc11, shape=[-1, 8], name=\"output_node\")\n\nThe batch generation is done before, so it should not been used when testing.\nBut I have the following error:\ntensorflow_jni.cc:312 Error during inference: Invalid argument: No OpKernel was registered to support Op 'RandomShuffleQueue' with these attrs\n[[Node: shuffle_batch/random_shuffle_queue = RandomShuffleQueue[capacity=10750, component_types=[DT_FLOAT, DT_FLOAT], container=\"\", min_after_dequeue=10000, seed=0, seed2=0, shapes=[[10000], [8]], shared_name=\"\"]()]]\n\nIt seems that the batch generation layer is called (my images are 100x100 and I have 8 outputs), but I don't know why.\n\nWhen testing the same model with the same input/output layers though the image_labelling.cc directly on Mac (building with Bazel), I don't have the error.\n\nI do not understand why the RandomShuffleQueue is needed when testing. Am I missing something to specify the part of the graph I want to use? Are all the layers of the graph verified even if not used?\n\nThanks.\n", "comments": ["@petewarden Could you take a look at this? Thanks.\n", "@catherineHR All nodes are verified when loading on Android, but you can use the [strip_unused](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/strip_unused.py) script to remove the unused and unsupported nodes from your graph.\n", "Closing; please reopen with details if the problem persists.\n"]}, {"number": 4465, "title": "Distributed training optimization", "body": "After playing with the current distributed training implementation for a while, I think it views each GPU as a separate worker.However, It is common now to have 2~4 GPUs in one box. Isn't it better to adopt the single box multi-GPU methodology to compute average gradients in single box first and then sync up across multiple nodes? This way it ease the I/O traffic a lot, which is always the bottleneck in data parallelism.  \n", "comments": ["I think the example implementations might be one GPU per worker, but nothing prevents you from defining a worker having 8 GPUs, replicating the model across each GPU in a worker, averaging gradients before sending it to parameter servers.  TF is low-level enough that these are all possible.  To make the system easy to understand, the examples are simpler than what is possible.\n\nI suspect higher-level libraries will be added over time that canonicalizes these types of model deployments that are becoming more popular.\n"]}, {"number": 4464, "title": "[mnist]Fix the array type bug", "body": "Fix the `PS_ARRAY` and `WORKER_ARRAY`  bug. They should be arrays, but in fact they are strings. The problem is the wrong way for assigning bash array. \nThe log info before fixing:\n\n```\n2 worker process(es) running in parallel...\nWorker 0: \n  WORKER HOST: localhost:2223 localhost:2224 \n  log file: /tmp/worker0.log\nWorker 1: \n  WORKER HOST: \n  log file: /tmp/worker1.log\n```\n\nAfter:\n\n```\n2 worker process(es) running in parallel...\nWorker 0: \n  WORKER HOST: localhost:2223\n  log file: /tmp/worker0.log\nWorker 1: \n  WORKER HOST: localhost:2224\n  log file: /tmp/worker1.log\n```\n\nI made a mistake on my branch, so I have to open a new PR to fix the bug. \n@caisq  Plz check\n", "comments": ["@DjangoPeng, thanks for your PR! By analyzing the annotation information on this pull request, we identified @andrewharp and @caisq to be potential reviewers\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 4463, "title": "Installation links in README.md is wrong", "body": "https://github.com/tensorflow/tensorflow#installation\n\nThe links to the nightly images are invalid and returns 404.\n", "comments": ["https://github.com/tensorflow/tensorflow/issues/4416 but also CPU links are broken.\n", "@caisq. The GPU ones seem to still be broken and it has been 3 days since #4416 comment. Any thoughts?\n", "The build is blocked by a single failing test that's taken a little long to fix. I'll tag it for now to let the build pass.\n", "The Linux GPU whl links should be working now.\n", "@hholst80  please let us know if this resolves your problem so we can close the issue.\n", "All the links except \"Linux CPU-only\" and \"Android\" are broken again.\nThe links are to 0.11.0rc0 while the actual most recent build from the history is 0.10.0\n\nIt looks like it's the same sort of issue, where there has been no passing build since the version number change.\n", "@gunan, @caisq, was this related to the jenkins upgrade that caused breakage in our jenkins jobs?\n", "No, this is a different issue.\n\nThis is due to the breakage in nightly builds.\nToday CPU only nightly builds passed, so the links should now work.\nOnce the GPU builds also pass, we will update nightly links to be independent of release versions.\n", "#4783 should fix mac-cpu links\n"]}, {"number": 4462, "title": "[done] update zeros_initializer()", "body": "I'm reading source code this day, and I find there is a `#TODO` in [array_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L263) saying that move `zeros_initializer()` to [init_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/init_ops.py#L64). \n\nSo, I think I can help update the change. Not only move `zeros_initializer()` to `init_ops.py`, but also update the reference files which call `array_ops.zeros_initializer()`.\n\nBTW,  I list all of them as below:\n- init_ops.py\n- array_ops.py\n- rnn_cell.py\n\n**3 files in contrib/..**\n- feature_column.py\n- histogram_ops.py\n- rnn_cell.py\n\n@girving  Could you have a look? \n", "comments": ["Can one of the admins verify this patch?\n", "@martinwicke plz check\n", "@martinwicke Plz check\n", "Jenkins, test this please\n", "You need another init_op import in histogram_ops. I guess they didn't have one.\n", "@martinwicke You're right. I forgot importing it. Now, it works\n", "Jenkins, test this please!\n", "@martinwicke hhhh...You're so fast! There is another file needed to import init_ops\n", "Heh.\nOn Tue, Sep 20, 2016 at 21:24 Jingtian Peng notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke hhhh...You're so fast!\n> There is another file needed to import init_ops\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> \n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4462#issuecomment-248507953,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_Q_-FWAIq0DO3-z2F4QuAp7pXrqoks5qsLF7gaJpZM4KAjUB\n> .\n", "Jenkins, test this please. (Jenkins won't listen to you, sorry)\n", "Hhhh..I try it, but failed. \u256e(\u256f\u25bd\u2570)\u256d\n", "@martinwicke Ok, alphabetically...You all have good habits\n", "We have pedantic linters. :)\n", "Yey!\n"]}]