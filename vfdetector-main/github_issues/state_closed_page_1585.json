[{"number": 5363, "title": "Convert retrained data into .pb format for ios camera example?", "body": "I retrained image data using the tutorial at ((https://www.tensorflow.org/versions/r0.9/how_tos/image_retraining/index.html)) I did all the steps until `bazel build tensorflow/examples/image_retraining:retrain`. I was wondering how to turn this image training data into a .pb file that I can use in the ios camera example.\r\n\r\nThank you for your help!", "comments": ["This question is better asked on StackOverflow which is centered around how to do things. GitHub issues is primarily for bugs or feature requests. Thank you!\n"]}, {"number": 5362, "title": "CUDA_TOO_MANY_PEERS exception when launching on p2.16xlarge on aws", "body": "This issue is identical to the issue discussed [here](https://devtalk.nvidia.com/default/topic/970010/cuda-peer-resources-error-when-running-on-more-than-8-k80s-aws-p2-16xlarge-/).\r\n\r\nWhen launching tensorflow to train on more than 8 gpu instances, tensorflow will cause a CUDA_TOO_MANY_PEERS error in the driver. The Tesla K80 shows up as 2 gpu instances internally so even though the p2.16xlarge technically only has 8 gpus it looks to the driver like it has 16. It seems that the cuda p2p system limits any gpu to only connect to a maximum 8 other gpus via p2p, but when the graph is launched, tensorflow (quite understandably) attempts to create a p2p connection between every gpu in the graph.\r\n\r\nIs there some way to disable p2p or is there a way via intelligent graph construction to limit the number of connections any given gpu requires, or even use a resource pool for gpu p2p connections?\r\n\r\n```bash\r\nubuntu@host:~/workspace/nn$ nvidia-smi\r\nWed Nov  2 20:53:20 2016       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 361.93.02              Driver Version: 361.93.02                 |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:00:0F.0     Off |                    0 |\r\n| N/A   45C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           Off  | 0000:00:10.0     Off |                    0 |\r\n| N/A   39C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla K80           Off  | 0000:00:11.0     Off |                    0 |\r\n| N/A   50C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla K80           Off  | 0000:00:12.0     Off |                    0 |\r\n| N/A   43C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  Tesla K80           Off  | 0000:00:13.0     Off |                    0 |\r\n| N/A   50C    P8    57W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Tesla K80           Off  | 0000:00:14.0     Off |                    0 |\r\n| N/A   41C    P8    70W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Tesla K80           Off  | 0000:00:15.0     Off |                    0 |\r\n| N/A   51C    P0    56W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Tesla K80           Off  | 0000:00:16.0     Off |                    0 |\r\n| N/A   43C    P0    71W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   8  Tesla K80           Off  | 0000:00:17.0     Off |                    0 |\r\n| N/A   46C    P0    56W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   9  Tesla K80           Off  | 0000:00:18.0     Off |                    0 |\r\n| N/A   39C    P0    71W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|  10  Tesla K80           Off  | 0000:00:19.0     Off |                    0 |\r\n| N/A   49C    P0    58W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|  11  Tesla K80           Off  | 0000:00:1A.0     Off |                    0 |\r\n| N/A   40C    P0    73W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|  12  Tesla K80           Off  | 0000:00:1B.0     Off |                    0 |\r\n| N/A   49C    P0    58W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|  13  Tesla K80           Off  | 0000:00:1C.0     Off |                    0 |\r\n| N/A   40C    P0    70W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|  14  Tesla K80           Off  | 0000:00:1D.0     Off |                    0 |\r\n| N/A   49C    P0    60W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|  15  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n| N/A   41C    P0    69W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\nubuntu@host:~/workspace/nn$ python3 deploy_local.py \r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:0f.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3bb3f120\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:10.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3bf86790\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:11.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3c3cf0c0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:12.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4a7fe070\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 4 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:13.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4ac4dd20\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 5 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:14.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4b0a16a0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 6 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:15.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4b4f9220\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 7 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:16.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4b9545d0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 8 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:17.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4bdb3920\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 9 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:18.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4c216790\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 10 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:19.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4c67d450\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 11 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:1a.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4cae8580\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 12 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:1b.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x28999af0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 13 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:1c.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x28e0bf00\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 14 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:1d.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x29282290\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 15 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:1e.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nE tensorflow/core/common_runtime/direct_session.cc:132] Internal: Internal: failed to enable peer access from 0x1c961680 to 0x18f1ac60: CUDA_ERROR_TOO_MANY_PEERS\r\nTraceback (most recent call last):\r\n  File \"deploy_local.py\", line 39, in <module>\r\n    n_test_batches=4,\r\n  File \"/home/ubuntu/workspace/nn/nn/model/network.py\", line 271, in train\r\n    test_steps=test_steps, save_steps=save_steps, load_all=load_all, debug=debug, **kwargs)\r\n  File \"/home/ubuntu/workspace/nn/nn/train/train.py\", line 86, in train_model\r\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)) as sesh:\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 1138, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 502, in __init__\r\n    self._session = tf_session.TF_NewSession(opts, status)\r\n  File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors.InternalError: Failed to create session.\r\n```", "comments": ["@zhengxq, it seems like the recommended way to go about this is to use distributed and give each process on the same host half of the available GPUs using CUDA_VISIBLE_DEVICES as discussed on the link article. Could you confirm?\n", "I can confirm that deploying as a cluster to half of the devices per worker does not reduce the number of p2p connections attempted when starting a session.\n\n```\ncreating ec2 cluster\ncreating security group\nlaunching instances\nwaiting for ip addresses to be assigned\ncluster config: {\n  \"ps_hosts\": [\n    \"35.162.204.77\"\n  ],\n  \"worker_hosts\": [\n    \"35.162.204.77\",\n    \"35.162.204.77\"\n  ]\n}\nsetting rules for security group.\nwaiting for cluster instances to boot up.\nstarting ps host at: 35.162.204.77\nstarting worker host at: 35.162.204.77\nstarting master worker host at: 35.162.204.77\n35.162.204.77: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\n35.162.204.77: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\n35.162.204.77: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\n35.162.204.77: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\n35.162.204.77: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\n35.162.204.77: INFO: starting parameter server 0\n35.162.204.77: INFO: starting master worker.\n35.162.204.77: INFO: starting training worker 1\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:0f.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7bd5093eb0\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:0f.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:0f.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b7838bed0\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b6e672bb0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:10.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7bd54e5d30\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:10.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b6eaa38b0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:10.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b78778410\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:11.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7bd59365b0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:11.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b6eed7d10\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:11.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b78b66470\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:12.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7bd5d8add0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:12.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b6f310590\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:12.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b78f55830\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 4 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:13.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7bd61e3400\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 4 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:13.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b65446460\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 4 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:13.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b79346310\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 5 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:14.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7bd663fad0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 5 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:14.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b65837ee0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 5 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:14.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b79738150\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 6 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:15.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7bd6aa04e0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 6 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:15.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b65c2b780\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 6 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:15.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b79b2b630\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 7 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:16.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7bd6f04a60\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 7 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:16.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b6601f5b0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 7 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:16.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b79f1f670\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 8 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:17.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 11.04GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7bd736cfc0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 8 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:17.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b66414bf0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 8 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:17.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b7a314800\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 9 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:18.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 11.04GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7bd77d9480\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 9 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:18.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b6680b590\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 9 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:18.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b7a70acf0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 10 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:19.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 11.04GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7bd7c49a40\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 10 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:19.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b66c03120\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 10 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:19.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b7ab02880\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 11 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1a.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 11.04GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f79f80be5b0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 11 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1a.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b66ffced0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 11 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1a.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b7aefc630\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 12 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1b.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 11.04GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f79f85369e0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 12 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1b.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b673f71f0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 12 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1b.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b7b2f6950\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 13 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1c.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 11.04GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f79f89b2d40\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 13 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1c.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b677f23c0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 13 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1c.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b7b6f1b20\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 14 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1d.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f79f8e32fe0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 14 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1d.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.98GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b67bee8f0\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 14 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1d.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.98GiB\n35.162.204.77: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f7b7baee050\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 15 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1e.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.99GiB\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 15 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1e.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.98GiB\n35.162.204.77: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.162.204.77: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 15 with properties:\n35.162.204.77: name: Tesla K80\n35.162.204.77: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.162.204.77: pciBusID 0000:00:1e.0\n35.162.204.77: Total memory: 11.17GiB\n35.162.204.77: Free memory: 10.98GiB\nTraceback (most recent call last):\n  File \"/Users/alexatknit/Workspace/nn/deploy_local.py\", line 42, in <module>\n    n_test_batches=4,\n  File \"/Users/alexatknit/Workspace/nn/nn/model/network.py\", line 293, in train_cluster\n    n_test_batches=n_test_batches, test_steps=test_steps, save_steps=save_steps, load_all=load_all, **kwargs)\n  File \"/Users/alexatknit/Workspace/nn/nn/train/deploy.py\", line 81, in deploy_to_cluster\n    **kwargs).result()\n  File \"/usr/local/lib/python3.5/site-packages/KnitPyUtils-0.0.39-py3.5.egg/knitutils/util/executor.py\", line 25, in result\n    raise self._result\n  File \"/usr/local/lib/python3.5/site-packages/KnitPyUtils-0.0.39-py3.5.egg/knitutils/util/executor.py\", line 119, in _worker\n    result = func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/KnitPyUtils-0.0.39-py3.5.egg/knitutils/parallel/cluster.py\", line 83, in _submit\n    return self.proxy.submit(fn, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/KnitPyUtils-0.0.39-py3.5.egg/knitutils/util/server.py\", line 202, in method\nsecurity group removed.\n    raise convert_to_error(kind, result)\nmultiprocessing.managers.RemoteError: \n---------------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/KnitPyUtils-0.0.39-py3.4.egg/knitutils/util/server.py\", line 122, in handle\n    result = ('#RETURN', func(*args, **kwargs))\n  File \"/usr/local/lib/python3.4/dist-packages/KnitPyUtils-0.0.39-py3.4.egg/knitutils/util/functions.py\", line 48, in wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/KnitPyUtils-0.0.39-py3.4.egg/knitutils/parallel/cluster.py\", line 39, in submit\n    return self.future_pool.submit(fn, *args, **kwargs).result()\n  File \"/usr/local/lib/python3.4/dist-packages/KnitPyUtils-0.0.39-py3.4.egg/knitutils/parallel/future.py\", line 75, in result\n    raise convert_to_error(kind, result)\nmultiprocessing.managers.RemoteError: \n---------------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/KnitPyUtils-0.0.39-py3.4.egg/knitutils/parallel/future.py\", line 18, in _deploy_worker\n    result = (\"#RETURN\", fn(*args, **kwargs))\n  File \"/usr/local/lib/python3.4/dist-packages/KnitNN-0.5.29-py3.4.egg/nn/train/train.py\", line 398, in train_master\n    server = tf.train.Server(cluster, job_name='worker', task_index=0)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/server_lib.py\", line 152, in __init__\n    self._server_def.SerializeToString(), status)\n  File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors.InternalError: Internal: failed to enable peer access from 0x7f7b8c286310 to 0x7f7b8e226740: CUDA_ERROR_TOO_MANY_PEERS\nscheduling instances for termination: ['35.162.204.77']\nwaiting for termination to complete.\n```\n", "@alexatknit, could you confirm that you have set CUDA_VISIBLE_DEVICES? TensorFlow should not even see the invisible devices in that case. \n\nEven if it is not set, this is a bit weird. Before trying to enabling peer-access to each other GPU, TF calls cuDeviceCanAccessPeer, only if it returns True, TF proceeds to call the next cuCtxEnablePeerAccess. It seems to cuDeviceCanAccessPeer returns True even if the next cuCtxEnablePeerAccess will fail with CUDA_ERROR_TOO_MANY_PEERS.\n\n+@benbarsdell, our friend from NVIDIA. Could you confirm whether cuCtxEnablePeerAccess should fail in this case? \n", "I have not altered CUDA_VISIBLE_DEVICES, I'll try it out.\n", "> cuDeviceCanAccessPeer returns True even if the next cuCtxEnablePeerAccess will fail with CUDA_ERROR_TOO_MANY_PEERS\n\nThis is correct.\n", "@benbarsdell, would it be too big a feature request to have future Cuda drivers return False in that case? \n", "I've set CUDA_VISIBLE_DEVICES and still have the issue\n", "@alexatknit, could you share your command line, your CUDA_VISIBLE_DEVICES setting, and the resulting logs? If the log is too large, feel free to upload somewhere such as pastebin.com and share the URL here. \n", "My cluster is launched remotely, I set the environment variable in python, my workers and parameter server are launched from a python process listening to a specific port. It looks like the separate processes all have their own environment variables, so unless those need to be set before importing tensorflow in the first place they should be set based on:\n\n``` python\ngpu_devices = [device.split(':')[-1] for device in devices if 'gpu' in device]\nif len(gpu_devices) > 0:\n  environ['CUDA_VISIBLE_DEVICES'] = ','.join(gpu_devices)\n```\n", "Depending on where you set it, it might be too late. It needs to be set the first time Cuda driver is loaded. \n\nThe safest thing to try is to set it in the script that launches your worker and ps. \n", "There are far too many variables associated with launching a model, I would need to rework my entire system to try that. My listener process seems to be importing tensorflow during the depickle phase which loads the driver, and this state is passed to the child processes.\n", "@zheng-xq\n\n> would it be too big a feature request to have future Cuda drivers return False in that case?\n\ncuDeviceCanAccessPeer only considers the topology of the system, independent of any other state; I believe that changing this behavior could potentially break existing codes. There are also other reasons why calls to cuCtxEnablePeerAccess may still fail.\n", "Alright, I updated the architecture to support setting environment variables before deserialization and it appears that it still fails:\n\n```\ncreating ec2 cluster\nlaunching instances\nwaiting for ip addresses to be assigned\ncluster config: {\n  \"worker_hosts\": [\n    \"35.160.255.186\",\n    \"35.160.255.186\"\n  ],\n  \"ps_hosts\": [\n    \"35.160.255.186\"\n  ]\n}\nsetting rules for security group.\nwaiting for cluster instances to boot up.\nstarting ps host at: 35.160.255.186\nstarting worker host at: 35.160.255.186\nstarting master worker host at: 35.160.255.186\n35.160.255.186: env = {}\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\n35.160.255.186: env = {'CUDA_VISIBLE_DEVICES': '0,1,2,3,4,5,6,7'}\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\n35.160.255.186: env = {'CUDA_VISIBLE_DEVICES': '8,9,10,11,12,13,14,15'}\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\n35.160.255.186: I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\n35.160.255.186: INFO: starting parameter server 0\n35.160.255.186: INFO: starting master worker.\n35.160.255.186: INFO: starting training worker 1\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:17.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.11GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e64e3f6b0\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:0f.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:0f.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e60e3c540\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ebd0934f0\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:18.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.11GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e6522b7e0\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:10.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ebd4e4f80\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:10.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e612288e0\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:0f.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 10.99GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e65619840\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:11.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ebd9357e0\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:11.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e61616d00\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:10.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 10.99GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e65a08fc0\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:12.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ebdd89f40\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:12.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e61a060c0\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 4 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:11.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 10.99GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e65df9aa0\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 4 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:13.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ebe1e2600\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 4 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:13.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e61df67e0\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 5 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:12.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 10.99GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e661eb520\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 5 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:14.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ebe63ec40\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 5 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:14.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e621e8260\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 6 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:13.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 10.99GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e665dedc0\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 6 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:15.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ebea9f5a0\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 6 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:15.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4e625dbb00\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 7 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:14.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 10.99GiB\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 7 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:16.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ebef03bb0\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 7 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:16.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.05GiB\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1 2 3 4 5 6 7\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y Y Y Y Y Y Y Y\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   Y Y Y Y Y Y Y Y\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 2:   Y Y Y Y Y Y Y Y\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 3:   Y Y Y Y Y Y Y Y\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 4:   Y Y Y Y Y Y Y Y\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 5:   Y Y Y Y Y Y Y Y\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 6:   Y Y Y Y Y Y Y Y\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 7:   Y Y Y Y Y Y Y Y\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:17.0)\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:00:18.0)\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:00:0f.0)\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:00:10.0)\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:4) -> (device: 4, name: Tesla K80, pci bus id: 0000:00:11.0)\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:5) -> (device: 5, name: Tesla K80, pci bus id: 0000:00:12.0)\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:6) -> (device: 6, name: Tesla K80, pci bus id: 0000:00:13.0)\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:7) -> (device: 7, name: Tesla K80, pci bus id: 0000:00:14.0)\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 8 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:17.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 565.56MiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ebf36c090\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 9 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:18.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 504.81MiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ebf7d8550\n35.160.255.186: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job ps -> {0 -> 127.0.0.1:40842}\n35.160.255.186: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -> {0 -> 127.0.0.1:50425, 1 -> localhost:52197}\n35.160.255.186: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:206] Started server with target: grpc://localhost:52197\n35.160.255.186: INFO: training model res_net_v10_20161103_2_fcn.0\n35.160.255.186: INFO: setting up device: '/job:worker/task:1'\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 10 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:19.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.11GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ebfc48b00\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 11 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:1a.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.11GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ce40bd6e0\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 12 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:1b.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.11GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ce4535a80\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 13 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:1c.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.11GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ce49b1e00\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 14 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:1d.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.11GiB\n35.160.255.186: W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7f4ce4e320b0\n35.160.255.186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n35.160.255.186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 15 with properties:\n35.160.255.186: name: Tesla K80\n35.160.255.186: major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n35.160.255.186: pciBusID 0000:00:1e.0\n35.160.255.186: Total memory: 11.17GiB\n35.160.255.186: Free memory: 11.11GiB\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/KnitPyUtils-0.0.40-py3.4.egg/knitutils/parallel/future.py\", line 18, in _deploy_worker\n    result = (\"#RETURN\", fn(*args, **kwargs))\n  File \"/usr/local/lib/python3.4/dist-packages/KnitPyUtils-0.0.40-py3.4.egg/knitutils/parallel/cluster.py\", line 23, in _run_worker\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/KnitNN-0.5.30-py3.4.egg/nn/train/train.py\", line 406, in train_master\n    server = tf.train.Server(cluster, job_name='worker', task_index=0)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/server_lib.py\", line 152, in __init__\n    self._server_def.SerializeToString(), status)\n  File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors.InternalError: Internal: failed to enable peer access from 0x7f4e79688120 to 0x7f4e7aa4ebb0: CUDA_ERROR_TOO_MANY_PEERS\nscheduling instances for termination: ['35.160.255.186']\nwaiting for termination to complete.\n```\n", "If we change it from an error to a warning, would that make people happy? :)\n", "(I'm going to send out a change that does this, and then we can refine it if it turns out to be bad).  I think enough people are bitten by this that don't really care, that we should just fix it for the common case.\n", "@vrv does the change make sure to prioritize the p2p connections that are in the graph? I wouldn't want to have an issue where gpus 8-15 can't talk to each other even though they're the only ones the graph is deployed to in that particular tf session. \n\nedit: (I guess that can be controlled by CUDA_VISIBLE_DEVICES)\n", "Yeah, I would try to use CUDA_VISIBLE_DEVICES if you really don't plan on using the GPUs in your graph.   All the change will do is not fail when it currently fails, so things might get a bit slower, but at least it will tell you via the warning logs, instead of just failing the job.\n"]}, {"number": 5361, "title": "Object of type 'path' has no field \"realpath\".", "body": "i get the latest code, installed **bazel 0.3.2**. but still meet errors when run **./configure** for build from source.\r\n\r\nroot@scopephotos:/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow# ./configure \r\n/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow /home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\n/usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with GPU support? [y/N] y\r\nGPU support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\r\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: **5.1.3**\r\nPlease specify the location where cuDNN 5.1.3 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: \r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.\r\n**ERROR:** com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/third_party/gpus/cuda_configure.bzl\", line 517\r\n\t\t_create_cuda_repository(repository_ctx)\r\n\tFile \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/third_party/gpus/cuda_configure.bzl\", line 432, in _create_cuda_repository\r\n\t\t_cuda_toolkit_path(repository_ctx, cuda_version)\r\n\tFile \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/third_party/gpus/cuda_configure.bzl\", line 148, in _cuda_toolkit_path\r\n\t\tstr(repository_ctx.path(cuda_toolkit...)\r\n\tFile \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/third_party/gpus/cuda_configure.bzl\", line 148, in str\r\n\t\trepository_ctx.path(cuda_toolkit_path).realpath\r\n**Object of type 'path' has no field \"realpath\".**\r\n", "comments": ["Please provide the information on the template. What operating system, compiler, etc. are you using? \n", "the latest source code. Ubuntu 16.04. Cuda 7.5, cudnn 5.1.3.\n", "A few more things...\n- What version of bazel?\n- Did you try upgrading bazel?\n- Did you try starting from a clean sandbox?\n", "I install bazel 0.3.2. my previous version is 0.3.1, as I see another post has the similar issue and is fixed by update to bazel 0.3.2. But it does not work for me. I use **git pull** to update the code. all the local code are up to date.\n", "I run the following cmd and the issues are fixed.\n\n sudo apt-get update && sudo apt-get install bazel\n\n sudo apt-get upgrade bazel\n"]}, {"number": 5360, "title": "how to build iOS static library using bazel", "body": "I'm working with tag `v0.11.0.rc2`.\r\n\r\nFollowing the instructions here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/README.md I have successfully built tensorflow for iOS using the makefile approach.  However, I have wondered about the first line in the readme\r\n>The recommended way to build TensorFlow from source is using the Bazel open-source build system\r\n\r\nI can't find any targets in the bazel BUILD files that would build `libprotobuf-lite.a`, `libprotobuf.a`, and `libtensorflow-core.a` that the makefile approach builds.\r\n\r\nDoes anyone know how to build the static libs for iOS using Bazel?  Is there a reason/advantage to do so?", "comments": ["Sorry that is a general statement that doesn't apply to iOS. For iOS you currently need to use the makefile. Sorry for the confusion.\n", "Is it still the case that Make is the only way to build for iOS, or can bazel now be used? If no bazel, what is this: https://github.com/tensorflow/tensorflow/commit/78c9dec5a62e74389608c709027fb8eabdf2bef0 ?", "@petewarden, I believe this still applies. Right?", "i successfully built ios  file : tensorflow\\contrib\\makefile\\gen\\lib\\libtensorflow-core.a\r\n\r\nbut don't know where the header files are...\r\n\r\nany one knows where to find h files ?\r\n\r\nhere is my technical paths:\r\nhttps://github.com/tensorflow/tensorflow/issues/11126\r\nhttps://github.com/tensorflow/tensorflow/issues/11231\r\n\r\n"]}, {"number": 5359, "title": "Replace expunge with expunge_async in bazel clean", "body": "https://github.com/tensorflow/tensorflow/issues/5357", "comments": ["Can one of the admins verify this patch?\n", "@Randl, thanks for your PR! By analyzing the history of the files in this pull request, we identified @drpngx, @vrv and @keveman to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Signed CLA\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@jart, related to #5253.\n", "This change might be problematic (generate some race conditions), and is not solving the underlying problem.\n", "Thanks @damienmg. I'll close this and we should either follow up on the referenced bug, or at bazelbuild/bazel, since this is more about bazel?\n"]}, {"number": 5358, "title": "Branch 137959663", "body": "", "comments": ["@vrv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @danmane, @keveman and @dsmilkov to be potential reviewers.\n"]}, {"number": 5357, "title": "Bazel fails to clean", "body": "Bazel fails to clean with following message\r\n```\r\nWARNING: Output base '/home/username/.cache/bazel/_bazel_username/acf2af86672e1552dfd6edd47d54a950' is on NFS. This may lead to surprising failures and undetermined behavior.\r\n.\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\nERROR: /home/username/.cache/bazel/_bazel_username/acf2af86672e1552dfd6edd47d54a950/server (Directory not empty).\r\n```\r\nReplacement of `bazel clean --expunge` with `bazel clean --expunge_async` [here](https://github.com/tensorflow/tensorflow/blob/master/configure#L25) fixes the issue.\r\n### Environment info\r\nOperating System:\r\nUbuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n$ ls -l /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcud*\r\n-rw-r--r-- 1 root root 558720 Sep 15 02:02 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Sep 15 02:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root     19 Sep 15 02:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root 415432 Sep 15 02:02 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root 775162 Sep 15 02:02 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n``` \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n```\r\n$ git rev-parse HEAD\r\n7443479c9261b893d00c923bf53d924bbae9bc64\r\n```\r\n2. The output of `bazel version`\r\n```\r\n$ bazel version\r\nWARNING: Output base '/home/username/.cache/bazel/_bazel_chaimb/acf2af86672e1552dfd6edd47d54a950' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nBuild label: 0.3.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\r\nBuild timestamp: 1475861110\r\nBuild timestamp as int: 1475861110\r\n```\r\n", "comments": ["Thank you for reporting this. I'm not sure if this is a recommended option to use always (@jart could you comment), but if it is, then a pull request would be most welcome.\n", "`--expunge_async` is basically just a way to not block on bazel doing its work, it's not the reason for the underlying error, can you report a bug to https://github.com/bazelbuild/bazel?\n", "@Randl: please feel free to close this bug once you've filed a bug with bazel, and reference it here.  Thanks!\n"]}, {"number": 5356, "title": "Add log1p", "body": "This PR adds the `log1p` function and partially addresses #3682. It also uses `log1p` in the computation of the sigmoid cross entropy to improve numerical stability (I had issues with rare events).", "comments": ["@tillahoffmann, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @vrv to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Looks like using `log1p` slightly changes the loss used in some tests. Happy for me to change the expected values, @vrv?\n", "Yes, seems reasonable to me.  I don't know why the tests are written like that anyway.\n", "Could just change them to perform an equivalent computation using numpy routines.\n", "@zakariahaque can you comment?\n", "@tillahoffmann, thanks for the change.  Feel free to update the loss in the tests.\n@vrv all the test needs to check is that the correct loss was used for that type of head/target_column.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5355, "title": "R0.11", "body": "", "comments": ["@hardingpj, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @yifeif to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 5354, "title": "failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED on a AWS p2.xlarge instance", "body": "Hi,\r\n\r\nI have been running docker images on a **Centos 7.0 AWS p2.xlarge** instance. I have previously installed on it:\r\nCUDA: **cuda-repo-rhel7-8.0.44-1.x86_64.rpm**\r\nNVIDIA **drivers 361.42**\r\n\r\nI have also installed **nvidia-docker** [following instructions](https://github.com/NVIDIA/nvidia-docker)\r\n\r\nI have successfully run all notebooks from Docker images (as fas as I've tried tensorflow/tensorflow:latest-devel-gpu and tensorflow/tensorflow:latest-gpu):\r\n\r\nRunning **tensorflow** version within the docker containter: **0.11.0rc2**\r\n**Bazel** version: Build label: **0.3.2**\r\nroot@de73edc73418:~# nvidia-smi -l\r\nWed Nov  2 12:02:54 2016       \r\n+------------------------------------------------------+                       \r\n| NVIDIA-SMI 361.42     Driver Version: 361.42         |                       \r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n| N/A   57C    P0    70W / 149W |  10948MiB / 11519MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\nHowever when I try to launch a [Single GPU computing example with tensorflow](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_MultiGPU/multigpu_basics.py) and get the following error:\r\n\r\n`I tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_3: /job:localhost/replica:0/task:0/gpu:0\r\nMatMul_4: /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_4: /job:localhost/replica:0/task:0/gpu:0\r\nMatMul_5: /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_5: /job:localhost/replica:0/task:0/gpu:0\r\nMatMul_6: /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_6: /job:localhost/replica:0/task:0/gpu:0\r\nMatMul_7: /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_7: /job:localhost/replica:0/task:0/gpu:0\r\nMatMul_8: /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_8: /job:localhost/replica:0/task:0/gpu:0\r\nMatMul_9: /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_9: /job:localhost/replica:0/task:0/gpu:0\r\nAddN: /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:819] AddN: /job:localhost/replica:0/task:0/cpu:0\r\nE tensorflow/stream_executor/cuda/cuda_blas.cc:367] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\nW tensorflow/stream_executor/stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nE tensorflow/stream_executor/cuda/cuda_blas.cc:367] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\nW tensorflow/stream_executor/stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n`\r\n\r\nNot sure if is something related to Nvidia drivers, OS or some library mismatch. Any idea?\r\n", "comments": ["Working installing cudnn and adding it to the path :).\n\ntar -zxf cudnn-8.0-linux-x64-v5.1.tgz\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include\nsudo cp cuda/lib64/libcudnn\\* /usr/local/cuda/lib64\nsudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64\"\nexport CUDA_HOME=/usr/local/cuda\n", "'Ran out of memory' exception masked 'failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED' exception when running a large tensorflow calculation. \n\nWhen running a smaller matrix operation, CUBLAS exception arises again\n", "- Can you run TensorFlow binary from outside the Docker container? \n- CUBLAS_STATUS_NOT_INITIALIZED is a classic CUDA runtime not setup properly problem. It's best to remove the complication of docker if you can. \n- Avoid using latest tags, instead use specific versions. You can also try nightly if you want a bleeding edge version, but you probably want to name or tag the container id once you get a working config if you are using nightly.\n", "thank you Aselle\n- Yes, no problem with it\n- Unfortunately that's part of my approach. I am using nvidia-docker to decouple the whole thing and running from containers in AWS.\n- I will follow your advice and it'll give a try to previous versions.\n", "I really have no ability to test this, so I am marking it as community support. You might try asking on stackoverflow which might have more AWS docker users.\n- Make sure you docker pull to make sure your container is up to date.\n- Make sure to upgrade your nvidia-docker install.\n- I would also try making sure you can run a non-tensorflow cublas program (perhaps one of the nvidia demos).\n", "thx @aselle. I'll give a try on stackoverflow\n", "if you're still having trouble, try adding `/usr/local/cuda/extras/CUPTI/lib64` to your `LD_LIBRARY_PATH`. I had the same error and this fixed it (I was on mac though, so verify that directory on your system)\r\n\r\nOn mac it was `/usr/local/cuda/extras/CUPTI/lib`", "Thanks @yufengg !!! How did you come up with this fix ?", "Hi @yufengg -- Is this ```/usr/local/cuda/extras/CUPTI/lib64``` applicable for MacbookPro (with Nvidia GPU) as well? Thanks.\r\n\r\nI'm getting this same error:\r\n```\r\n[I 23:59:13.425 NotebookApp] Kernel restarted: 82276eab-ae23-4637-8050-b40a3097613e\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\r\n[I 00:00:40.514 NotebookApp] Saving file at /15_Style_Transfer.ipynb\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GT 750M\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9255\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.52GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_blas.cc:372] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\nW tensorflow/stream_executor/stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n[I 00:02:40.823 NotebookApp] Saving file at /15_Style_Transfer.ipynb\r\n```", "Hi @laventura, I'm not sure if you found a solution yet, but switching from Python 2.7 to Python 3.5 worked for me when I was getting that error. ", "I  added /usr/local/cuda/extras/CUPTI/lib64 to my LD_LIBRARY_PATH,but don't work.\r\nAnd my environment is:\r\nUbuntu 16.04    Tensorflow1.0.0  CUDA8.0   Cudnn5.1\r\nHow to fix this problem? \r\n@yufengg ", "I ran into this problem when I'm running <https://github.com/davidsandberg/facenet/> inside docker image `tensorflow/tensorflow:latest-gpu-py3`.\r\n\r\nHowever, the jupyter can run without any problem.\r\n\r\nUPDATE:\r\n\r\nAfter I set `per_process_gpu_memory_fraction` from 1 to 0.5, this error gone.\r\n\r\n```python\r\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\r\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True))\r\n```\r\n\r\nIt seems if set `per_process_gpu_memory_fraction` to 1 it will require the entire GPU memory, which will fail because my `Xorg`, `chrome` have already used part of them.", "I faced the same issue and none of the above method worked out for me. What I realized is the memory allocation is failing. I reduced my GPU memory percentage from per_process_gpu_memory_fraction=0.95 to per_process_gpu_memory_fraction=0.8 to get past it.", "yes this seems to be out-of-memory being misreported. btw it happens on non-aws machines too so i dont think it has to do with aws. ", "@laventura \r\nHI,I come across the same problem ,and this method `/usr/local/cuda/extras/CUPTI/lib64` doesn't work for me.\r\nHave you solved this problem now?", "Setting `per_process_gpu_memory_fraction` does fix the problem. Though the fraction does not have to be so low. As xorg uses only 5MB, I was able to get away with setting the fraction to 0.99", "Maybe the following command helps:\r\n\r\n    sudo rm -rf .nv/\r\n\r\nGood luck.", "I try @hzxie 's suggestion and CUBLAS__STATUS_NOT_INITIALIZED changes to OOM.", "OOM is out of memory - so if you now turn down per_process_gpu_memory_fraction or put your net on a diet you may get past it.", "I try @hzxie 's suggestion and it really works!! thanks", "I experienced the same problem. I confirm that @hzxie 's suggestion works like a charm. Thanks!", "Thank you VERY VERY MUCH @hzxie. ", "@hzxie   A very good suggestion!\r\nIn my case, it is \r\n`sudo rm -rf ~/.nv/`\r\n\r\n\r\n", "\r\n@yufengg\r\n\r\nThis solved my issue... Thanks...", "> Hi,\r\n> \r\n> I have been running docker images on a **Centos 7.0 AWS p2.xlarge** instance. I have previously installed on it:\r\n> CUDA: **cuda-repo-rhel7-8.0.44-1.x86_64.rpm**\r\n> NVIDIA **drivers 361.42**\r\n> \r\n> I have also installed **nvidia-docker** [following instructions](https://github.com/NVIDIA/nvidia-docker)\r\n> \r\n> I have successfully run all notebooks from Docker images (as fas as I've tried tensorflow/tensorflow:latest-devel-gpu and tensorflow/tensorflow:latest-gpu):\r\n> \r\n> Running **tensorflow** version within the docker containter: **0.11.0rc2**\r\n> **Bazel** version: Build label: **0.3.2**\r\n> root@de73edc73418:~# nvidia-smi -l\r\n> Wed Nov 2 12:02:54 2016\r\n> +------------------------------------------------------+\r\n> | NVIDIA-SMI 361.42 Driver Version: 361.42 |\r\n> |-------------------------------+----------------------+----------------------+\r\n> | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\r\n> | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |\r\n> |===============================+======================+======================|\r\n> | 0 Tesla K80 Off | 0000:00:1E.0 Off | 0 |\r\n> | N/A 57C P0 70W / 149W | 10948MiB / 11519MiB | 0% Default |\r\n> +-------------------------------+----------------------+----------------------+\r\n> \r\n> However when I try to launch a [Single GPU computing example with tensorflow](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_MultiGPU/multigpu_basics.py) and get the following error:\r\n> \r\n> `I tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_3: /job:localhost/replica:0/task:0/gpu:0 MatMul_4: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_4: /job:localhost/replica:0/task:0/gpu:0 MatMul_5: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_5: /job:localhost/replica:0/task:0/gpu:0 MatMul_6: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_6: /job:localhost/replica:0/task:0/gpu:0 MatMul_7: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_7: /job:localhost/replica:0/task:0/gpu:0 MatMul_8: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_8: /job:localhost/replica:0/task:0/gpu:0 MatMul_9: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_9: /job:localhost/replica:0/task:0/gpu:0 AddN: /job:localhost/replica:0/task:0/cpu:0 I tensorflow/core/common_runtime/simple_placer.cc:819] AddN: /job:localhost/replica:0/task:0/cpu:0 E tensorflow/stream_executor/cuda/cuda_blas.cc:367] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED W tensorflow/stream_executor/stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support E tensorflow/stream_executor/cuda/cuda_blas.cc:367] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED W tensorflow/stream_executor/stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support `\r\n> \r\n> Not sure if is something related to Nvidia drivers, OS or some library mismatch. Any idea?\r\n\r\nAdministrator Rights\u3002\r\nI open pycharm in Admin, and it works and no wrong!(Although I don't know why"]}, {"number": 5353, "title": "Changing Bazel dependencies", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@fomiciov, thanks for your PR! By analyzing the history of the files in this pull request, we identified @meteorcloudy, @martinwicke and @jart to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "For future reference, please do not use git_repository. Always use {new_,}http_archive.\n", "This PR was opened by mistake, but thanks for the advice @jart \ud83d\udc4d \n"]}, {"number": 5352, "title": "Missing information in inception_train.py", "body": "\r\nI wanted to fine-tune inception network using my dataset, which is way too big. Converting to TFRecords format is not the best option because it uses a lot of GB in hard disk. I used a placeholder for images and a placeholder for labels and here https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py\r\nI converted the command in line 336 in    _, loss_value = sess.run([train_op, loss], feed_dict={images_placeholder: images , labels_placeholder: labels}). I think I achieved fine-tuning, so you could add it to the documentation. \r\n                              \r\n                              ", "comments": ["It's difficult to follow exactly what you are requesting and what you did. We'd appreciate any improvements to the documentation based on your experience as a pull request. Thank you!\n", "Automatically closing due to lack of recent activity, we will reopen if further information becomes available. Thanks!\n"]}, {"number": 5351, "title": "Installing Tensor flow on multiple machines (on GCP).", "body": "i am trying to install tensorflow on two machines(2 instances on Google Cloud Platform) how i can do that?\r\nor it is not supported only thing is you can write programs in tensorflow in distributed manner. ", "comments": ["Please re-ask this question on StackOverflow which is a better forum for asking questions about how to use TensorFlow. GitHub in mainly for bugs and problems. Thanks!\n"]}, {"number": 5350, "title": "Sampled and regular softmax should use the same weight matrix shape", "body": "As it is now, the softmax samplers [sampled_softmax_loss](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sampled_softmax_loss) and [nce_loss](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#nce_loss) require a `|C|x|H|` weight matrix, while the logits from [softmax](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#softmax) come from a `|H|x|C|`-sized one. This discrepancy is problematic on two levels:\r\n1. it makes for an inconsistent API;\r\n1. it results in a performance hit, because if one uses a sampled method for training and softmax for testing (as is usual when the number of classes is huge), one has to call `tf.transpose` somewhere, which does not work well with sparse input (see #4138).\r\n\r\nIn my case, I can choose between 21150 / 18300 or 22350 / 11900 train / test wps, depending on whether the `tf.transpose` happens on the sampled softmax loss during training or on regular softmax during testing. In either case, the performance is suboptimal.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nAs mentioned above, #4138. The fix there, however, is in the client code, not the API in question.\r\n\r\n### Environment info\r\nOperating System: `Linux 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt20-1+deb8u3 (2016-01-17) x86_64 GNU/Linux`\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n`/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44`; I don't have cuDNN at the moment\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: the official GPU install for Python 3.4\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`: `0.11.0rc1`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI cannot do that right now, but one can experiment with e.g. the PTB example.\r\n\r\n### What other attempted solutions have you tried?\r\nN/A", "comments": ["@lukaszkaiser, could you evaluate and decide whether this is something we want to do. It may be a difficult thing to change at this point given that the ubiquity of use of both of these. It may be worth providing a transposed version of softmax to allow people to optimize this case?\n", "@aselle I think out of the two, softmax has the \"logical\" dimensions, not the sampled functions. I preferred a new version / `transposed` parameter to the sampled functions to changing the softmax signature.\n\nI also had a quick look at the code, and I am wondering if the problem might be that `embedding_lookup` is not able to perform a lookup on other than the first axis? I am not familiar with its implementation, but at least numpy can index along any of the axes, so it shouldn't be difficult to fix that.\n", "I did encounter this problem, and the transpose does somtimes incur cost. I don't know how large of a problem this is. I'm unassigning myself as I don't know the details of any softmax code, so I'm not sure how hard it'd be to provide an optional transpose argument.\n", "Can you think of anybody who would be more appropriate?\n", "I think Stephan Gouws wrote the original NCE code, but I'm not sure who's working on it now, sorry.\n", "@gouwsmeister, could you add anything further?\n", "I did write the original code, but I have not kept up with the recent changes, so I may not be the best person to help with this. But I believe this was indeed originally because embedding_lookup indexes along the first dimension, as you mention. Also, to me CxH seems like the \"logical\" dimensions for the weight matrix, but I guess that's to some extent a matter of opinion :)\n", "Hey, I also encountered the issue  cf https://github.com/tensorflow/tensorflow/issues/5890\r\nI really think this is a major issue since once you decide to use sampled_softmax for training (to be quick), of course you need the full softmax to be quick too at inference time.\r\nAny update on this at the API level ?\r\nMany thanks.\r\n", "A simple workaround is to transpose the weight matrix once after training, assign it to a variable and use  that for testing. This way the transpose is computed only once.", "I agree with @DavidNemeskey here. I started working with tensorflow only recently and the discrepancy in the dimensions of the weight matrices passed to `sampled_softmax_loss` and `nce_loss` as compared to `softmax` was initially confusing. If it's not a priority to change the api, maybe a note in the documentation could be added about this?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 5349, "title": "Include libcurl into the bazel build", "body": "Currently tensorflow serving project will build gcs_file_system target which may fail, with the following message:\r\n```\r\nERROR: /home/heliangliang/work/tensorflow/tensorflow/core/platform/cloud/BUILD:49:1: C++ compilation of rule '//tensorflow/core/platform/cloud:http_request' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-canonical-system-headers ... (remaining 100 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from ./tensorflow/core/platform/cloud/http_request.h:22:0,\r\n                 from tensorflow/core/platform/cloud/http_request.cc:16:\r\ntensorflow/core/platform/cloud/http_request.cc: In member function 'virtual tensorflow::Status tensorflow::HttpRequest::Init()':\r\ntensorflow/core/platform/cloud/http_request.cc:219:30: error: 'CURL_HTTP_VERSION_2_0' was not declared in this scope\r\n                              CURL_HTTP_VERSION_2_0);\r\n```\r\n\r\nThe reason is gcs_file_system dependens on libcurl 7.33.0+, and this is higher than the default package versions of many Linux distros.\r\n\r\nIt's better to include libcurl as bazel build library instead of using the system one.", "comments": ["For Linux we are supporting Ubuntu 14   and newer which has 7.35.  RHEL 6 currently does have an older version at 7.19, but that is 6 years old at this point and ending its production phase 1. We don't generally promise to make changes to support older Linux systems, but maybe this is an easy change.   @jart could you take this?\n", "I would love to integrate libcurl into the bazel build. I'll do this at some point in the next two months.\n", "Thanks @jart and thanks for the suggestion @llhe\n", "Even RHEL7 / Centos7 only has libcurl 7.29 as default.  As a result, building from r0.11 source and selecting GCP support during configure will fail when building the pip package.\r\n\r\n", "This feature would be very helpful. I had to manually install the latest version of libcurl on Centos 7 to get it to work.", "I've written up a change internally that makes Bazel build libcurl from scratch. This is one of the most challenging vendor libraries we've dealt with so far. libcurl has a pretty large set of configure defines. We're getting more eyes on the BUILD file we've written to make sure we're doing it correctly. I can't make any promises this is going to work. But with a little bit of luck, this change might be good enough to be merged. Then no one should hopefully need to worry about curl again.", "@jart What about using `mkdir tmp && cp to tmp && pushd tmp && configure --prefix=target-bazel-out-path && make && make install && popd && rm -rf tmp`? It's much easier to make the integration.", "I wish it were that simple @llhe. The problem with building it that way inside a genrule is that Bazel wouldn't be building it anymore. It would be delegating the build to autotools, m4, make, libtool and friends. That may or may not work depending on how the system is configured. So we usually end up writing our own build code for these things, e.g. [jpeg.BUILD](https://github.com/tensorflow/tensorflow/blob/master/third_party/jpeg.BUILD), with tightly controlled versioning and configuration. That way builds should be guaranteed to work on the systems we support, so long as Bazel works, and the only tool/library the user needs to worry about installing is Bazel.", "Can I see your build file, I am struggling with libcurl myself", "@GregBowyer Here is an example that I use to build another 3rd party poco library which is also `configure-make-make install` pattern, hope this can help:\r\n```py\r\nPOCO_HEADERS = [                                                                \r\n        \"include/Poco/Process.h\",                                               \r\n        \"include/Poco/MemoryStream.h\", \r\n        ......\r\n]\r\nPOCO_LIBRARIES = [                                                              \r\n        \"lib/libPocoNetSSL.a\",                                                  \r\n        \"lib/libPocoCrypto.a\",                                                  \r\n        \"lib/libPocoNet.a\",                                                     \r\n        \"lib/libPocoUtil.a\",                                                    \r\n        \"lib/libPocoXML.a\",                                                     \r\n        \"lib/libPocoJSON.a\",                                                    \r\n        \"lib/libPocoFoundation.a\",                                              \r\n]                                                                               \r\n                                                                                \r\ngenrule(                                                                        \r\n    name = 'make',                                                              \r\n    srcs = glob(                                                                \r\n        [\"**/*\"],                                                               \r\n        exclude = [\"**/*.psd\", \"**/*.txt\"], # supress bazel complain            \r\n    ),                                                                          \r\n    outs = POCO_HEADERS + POCO_LIBRARIES,                                       \r\n    cmd = \"pushd $$(dirname $(location configure)); workdir=$$(mktemp -d -t poco-build.XXXXXXXXXX); cp -aL * $$workdir; pushd $$workdir; ./configure --static --cflags='-fPIC -Wl,-Bdynamic -lpthread -Wl,-Bdynamic -ldl -Wl,-Bdynamic -lrt' --omit=ApacheConnector,Data,MongoDB,PageCompiler,CppParser,CppUnit,NetSSL_Win,PocoDoc,PDF,ProGen,SevenZip,Zip --no-sharedmemory --no-sharedlibs --no-fpenvironment --no-tests --no-samples --prefix=$$workdir/install; make -j10; make install; popd; popd; cp -a $$workdir/install/* $(@D); rm -rf $$workdir\",\r\n)                                                                               \r\n                                                                                \r\ncc_library(                                                                     \r\n    name = \"poco\",                                                              \r\n    hdrs = POCO_HEADERS,                                                        \r\n    srcs = POCO_LIBRARIES,                                                      \r\n    includes = [\"include\"],                                                     \r\n    visibility = [\"//visibility:public\"],                                       \r\n    linkopts = [                                                                \r\n        \"-lssl\",                                                                \r\n        \"-lcrypto\",                                                             \r\n    ],                                                                          \r\n) \r\n```", "Would adding libcurl dependency work in windows?\r\nOr does windows make things infeasible here?", "I strongly recommend against running configure, make, and make install inside a genrule.", "@gunan @jart I'm not suggesting to push this into trunk, but provides a quick hacking solution for whom has this problem now. Of course, comment out the gcs rule is much more easier.\r\n\r\nWe use this approach internally because the library we need to integrate is **really nasty** to convert to normal Bazel rule and we don't want to invest too much time on this because we don't use bazel except tensorflow.", "yeah I am not running make in my BUILD ", "This should be fixed now. See https://github.com/yifeif/tensorflow/commit/bb890ea5fa4c8a2100a295e93c5bf37b3c88b55a. So happy I was able to help you guys! Thank you for your patience.", "Whoops, re-opening because that commit hasn't been merged into master yet."]}, {"number": 5348, "title": "Change sourceforge links from the unstable mirror to the primary site", "body": "This change address this issue: https://github.com/tensorflow/tensorflow/issues/5347", "comments": ["@llhe, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke, @caisq and @mrry to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "PR https://github.com/tensorflow/tensorflow/pull/5346 should have addressed the same issue. Thank you for the pull request all the same, @llhe \n"]}, {"number": 5347, "title": "Change http://ufpr.dl.sourceforge.net links for http_archive to another more stable one", "body": "TensorFlow build dependens on external links, for example:\r\n```python\r\n  native.new_http_archive(                                                      \r\n    name = \"gif_archive\",                                                       \r\n    url = \"http://ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz\",\r\n    sha256 = \"34a7377ba834397db019e8eb122e551a49c98f49df75ec3fcc92b9a794a4f6d1\",\r\n    strip_prefix = \"giflib-5.1.4/lib\",                                          \r\n    build_file = str(Label(\"//:gif.BUILD\")),                                    \r\n  )\r\n```\r\nLooks like http://ufpr.dl.sourceforge.net is just a SF mirror located in Brazil, and not quite stable, see #3929, #4085. I also encountered this some times. Considering change to another stable one, for example: http://downloads.sourceforge.net\r\n\r\nA list of SF sites/mirrors:\r\nhttps://github.com/fink/fink-mirrors/blob/master/sourceforge", "comments": ["By the way, I can't access it both in China and US-West, now, so the reason is that mirror is down instead of my network connection issue.\n", "Thanks for reporting the issue! @gunan, I don't know if there was a specific reason we chose than link, but the fix seems to make sense.\n", "#5346 already addressed this.\nTo the best of my knowledge, debian links should also be ok in China.\n"]}, {"number": 5346, "title": "Fix sourceforge links for TF external dependencies.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke, @caisq and @kirilg to be potential reviewers.\n", "tensorflow/tensorboard/backend:server_test  flaked.\nJenkins, test this please.\n", "Why rollback this change? ufpr.dl.sourceforge.net is not reachable again.\r\n", "Actually this change seems to have been clobbered by a bad merge. \r\n\r\nWe have some redundancy now though, and we mirror the package and shouldn't need the sourceforge server at all. Do you see issues with it?", "@martinwicke , Yes, it broke the cmake build.  Later on, it was fixed by 6446895aa6e35b88c0344ac1f976b06826617cbb"]}, {"number": 5345, "title": "tensor layer", "body": "Does tensorflow support or has tensor layer function. If it has than how can we implement it.\r\nThank You", "comments": ["Generally speaking, usage questions are more appropriately asked on Stack Overflow. However, you might consider looking at tf.slim, tf.contrib.layers, tf.learn, etc. Thanks!\n"]}, {"number": 5344, "title": "Import error: AttributeError: 'module' object has no attribute 'Default'", "body": "### Environment info\r\nOperating System:  Ubuntu 14.04 LTS\r\n\r\nInstalled version of CUDA and cuDNN: CUDA-8.0 and cuDNN v5.1\r\n\r\nHi everyone:\r\n   I have met an ImportError and do not what happened? Before this, I use tensorflow normally for about one month.\r\n   Can anyone help me? Thanks a lot!\r\n\r\nThe error info as following (in Ipython).\r\n\r\n```python\r\nIn [2]: import tensorflow as tf\r\n```\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcurand.so.8.0 locally\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-41389fad42b5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py in <module>()\r\n     21 from __future__ import print_function\r\n     22 \r\n---> 23 from tensorflow.python import *\r\n     24 \r\n     25 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py in <module>()\r\n     61 \r\n     62 # Protocol buffers\r\n---> 63 from tensorflow.core.framework.graph_pb2 import *\r\n     64 from tensorflow.core.framework.node_def_pb2 import *\r\n     65 from tensorflow.core.framework.summary_pb2 import *\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py in <module>()\r\n      7 from google.protobuf import message as _message\r\n      8 from google.protobuf import reflection as _reflection\r\n----> 9 from google.protobuf import symbol_database as _symbol_database\r\n     10 from google.protobuf import descriptor_pb2\r\n     11 # @@protoc_insertion_point(imports)\r\n\r\n/usr/local/lib/python2.7/dist-packages/google/protobuf/symbol_database.py in <module>()\r\n    163 \r\n    164 \r\n--> 165 _DEFAULT = SymbolDatabase(pool=descriptor_pool.Default())\r\n    166 \r\n    167 \r\n\r\nAttributeError: 'module' object has no attribute 'Default'\r\n```", "comments": ["What .whl did you use to install?\nDid you use a virtualenv (try one if you haven't already)?\nDId you try `pip install --upgrade protobuf`?\n", "Hi @aselle :\nI use to install tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl and run tensorflow normally for one month. This error suddenly occured. Now, I have tried to reinstall the tensorflow, but when I run ./configure, I got this error:\n\n```\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/home/common/tools/tensorflow/third_party/gpus/cuda_configure.bzl\", line 517\n        _create_cuda_repository(repository_ctx)\n    File \"/home/common/tools/tensorflow/third_party/gpus/cuda_configure.bzl\", line 432, in _create_cuda_repository\n        _cuda_toolkit_path(repository_ctx, cuda_version)\n    File \"/home/common/tools/tensorflow/third_party/gpus/cuda_configure.bzl\", line 148, in _cuda_toolkit_path\n        str(repository_ctx.path(cuda_toolkit...)\n    File \"/home/common/tools/tensorflow/third_party/gpus/cuda_configure.bzl\", line 148, in str\n        repository_ctx.path(cuda_toolkit_path).realpath\nObject of type 'path' has no field \"realpath\".\n```\n\nI am not sure what happened exactly?\n", "So you switched from using a precompiled binary to compiling from source? Correct?\nDoes it work if you go back to the old whl that was working?\nDid you try upgrading your version of bazel? You did not provide the output of bazel --version?\n", "Hi @aselle :\nI have tried to reinstall tensorflow from compiling the source, but it did not work. I do not why, but I noticed that tensorflow can be installed by pip with CUDA 8.0 and cuDNN v5.1. It works for me.\nI tried to upgrading my version of bazel and got the different error.\nAlthough pip works for me, but I still do not know why does the my tensorflow crash?\nI will close this issue, if anyone need it, I will reopen it.\nThanks for your time and help!\n", "Hi, @JohnnyY8  How do you solved your problem? I just installed latest tensorflow with CUDA 8.0 and cuDNN 5.1 using the Pip installation method. And I met the same problem.\n", "@CTTC Hi:\n     I installed tensorflow with CUDA 8.0 AND cuDNN 5.1 from source, but got that error. Then I noticed the latest tensorflow can be installed by pip, and it works for me. Do you use pip and got the same error as mine?\n", "@CTTC Have you installed tensorflow successfully by any ways?\n", "@JohnnyY8 I haven't solved this problem yet. I have tried both ways. And pip installation didn't work for me.\n", "@CTTC Pip installlation works for me, but I still do not know the root reason about this problem. I have reopened this issue, hope anyone can provide some suggestions. If you solved this problem, please share with me the method. Thank you!\n", "The original comment includes `No module named tensorflwo`.  This is not surprising, since it's called tensorflow, not tensorflwo.  Was that the problem?\n", "@girving Hi:\nI am sorry for my mistake, but my original code is writed and run correctly. Suddenly got this error.\nI want to reinstall tensorflow from source, it did not work, but pip installation works.\n@CTTC has the same problem, I do not know what happened exactly?\n", "@JohnnyY8 Can you reformat your original post so that the code and code output is easier to read?  Indenting by 4 spaces causes code formatting.  Also, can you say what the current error you are experiencing is?  The \"Object of type 'path' has no field \"realpath\".\" was fixed (https://github.com/tensorflow/tensorflow/issues/5319), and I'm having trouble following the thread to know if that was the final issue.\n", "@girving Hi\uff1a\nI have reformated the original post, is that easier to read? I am sorry that I do not know the right code format. Maybe @aselle  will try the #5319 , hope it will work.\n", "@JohnnyY8 Yep, much easier to read.  Thank you!\n\nIf understand correctly that #5319 was the last error, let me know if upgrading past that fixes the problem.  If something else was the last error, also let me know.\n", "@girving Hi:\nThanks for your help!\n"]}, {"number": 5343, "title": "ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory", "body": "I have installed cuda 7.5, not 8, but when I get this error when I try to import tensflow (having installed ts binaries for 7.5, gpu, linux).  I have tried un- and re-installing everything repeatedly.  I may have at one time attempted to install cuda 8 in the past, but 7.5 is in my $path.\r\n\r\nthe complete error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 23, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\n```", "comments": ["What exact binary (url, whl file) did you install? Many (all @gunan?) new binaries will be Cuda 8.0 only. What you are seeing indicates that you downloaded a cuda 8 binary.\n", "Correct, starting 0.11.0rc1, all our prebuilt packages are now built for cuda8.\nWith cuda7.5, you either need to install the 0.11.0rc0 wheel file, or build from sources.\n", "Thanks @gunan.\n", "Was yesterday 0rctober 1?\n\nOn Tue, Nov 1, 2016 at 8:56 PM, Andrew Selle notifications@github.com\nwrote:\n\n> Thanks @gunan https://github.com/gunan.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5343#issuecomment-257765598,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAL3MKj2JbJaG9HgUb8rSC9YP5itNBk0ks5q6An7gaJpZM4Kmxh4\n> .\n", "I am using jupyter notebook to use keras which has tensorflow as backend. I have `libcudart.so.8.0` in `usr/local/cuda-8.0/` as showed by below:\n\n```\n$ locate libcudart.so.8.0\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\n```\n\nbut still, when the import part of keras run, I got \n\n```\n/home/allan/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py in swig_import_helper()\n     22         if fp is not None:\n     23             try:\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n     25             finally:\n     26                 fp.close()\n\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\n```\n\nCould anyone help?\n", "There is very little information here.\nCould you create a new issue, filling in all the fields required by the template?\n", "Perhaps you need to make a symbolic link from `/usr/local/cuda` to `/usr/local/cuda-8.0`? That seemed to fix the issue for me", "Traceback (most recent call last):\r\n  File \"evaluate.py\", line 3, in <module>\r\n    import transform, numpy as np, vgg, pdb, os\r\n  File \"src/transform.py\", line 1, in <module>\r\n    import tensorflow as tf, pdb\r\n  File \"/home/prateek/.local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/prateek/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/prateek/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/prateek/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/home/prateek/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n", "If you are still seeing this problem, please file a new issue.\r\nFrom your error message, it does not look related to me.", "I have installed `cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb` but as I try to import tensorflow I get\r\n```\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\n```\r\n\r\nI do not have a `/usr/local/cuda/` directoryafter installation\r\n\r\n```\r\nstefan@stefan-pc:~$ ls -la /usr/local/\r\ntotal 44\r\ndrwxr-xr-x 11 root root 4096 Feb  9 19:19 .\r\ndrwxr-xr-x 13 root root 4096 Feb  2 19:32 ..\r\ndrwxr-xr-x  3 root root 4096 Feb  9 19:34 bin\r\ndrwxr-xr-x  3 root root 4096 Feb  9 19:19 cuda-8.0\r\n...\r\n```\r\n\r\nThe `cuda-8.0` directory only contains\r\n\r\n```\r\nstefan@stefan-pc:~$ ls -la /usr/local/cuda-8.0/\r\ntotal 12\r\ndrwxr-xr-x  3 root root 4096 Feb  9 19:19 .\r\ndrwxr-xr-x 11 root root 4096 Feb  9 19:19 ..\r\ndrwxr-xr-x  3 root root 4096 J\u00e4n  9 20:43 targets\r\n```\r\n\r\nWhich I think comes from cuDNN.\r\n\r\nI don't quite understand why it does not find `libcudart.so.8.0`. Not sure what the `.deb` file installed if not CUDA.\r\n\r\n", "@silentsnooc,   you probably should open a new issue with full context of your install environment ( we have an issue template). However, here is generic information:\r\n\r\n1. `dpkg -L <package>` will list files installed by installing a deb. You can also use `find` or `locate` to find the file if that doesn't help.  \r\n\r\n2. If you can't find libcudart.so.8.0, install CUDA properly (possibly manually). If you cannot run NVidia CUDA demos, CUDA is not installed properly. Sometimes it is better to use the NVidia shell script install method rather than a debian package to install both the NVidia driver and the CUDA toolkit. I find that works more reliably. However, if you do this, you must be careful to remove all .deb related cuda information. I'd suggest finding a good tutorial online on installing NVidia kernel drivers and cuda toolkit and following them. \r\n\r\n3. Try TensorFlow. If it can't find find  libcudart.so.8.0, add the directory that contains it to LD_LIBRARY_PATH environment variable.\r\n\r\nYour issue is likely  a  generic CUDA installation issue rather than a TensorFlow specific problem.  Installing nvidia drivers and cuda on Linux is harder than one would like. Sometimes it goes smoothly, but often it does not.\r\n\r\nThanks!\r\n\r\n", "@aselle Hi! Thank you very much for that information. I'll take a look at it. It's frustrating because I just setup the whole thing on another machine but like you said: sometimes it goes smoothly and sometimes not. I'll try to go with the shell approach that you suggested.", "One possible case I've found is that the environment (IDE) you run your Python does not have access to the file. If you try import tensorflow in CLI and it works, then probably you should consider this possibility.", "Double check that you have exported the LD_LIBRARY_PATH\r\n`export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64`\r\n\r\nWas fighting this issue too and that solved it for me. Existed and everything lol", "I also am getting the ImportError but I have located libcudart.so.8.0 inside /usr/local/cuda-8.0/lib64 and I have the symlink cuda pointing to cuda-8.0.\r\n\r\nI have received a warning when installing cuda 8 that I needed to have at least nvidia driver 361, which I ignored because I have 367.57", "Import Error can have a lot of different underlying issues. We will need to see the full error message.\r\nCould you file a new issue, by filling out the full template?", "No problem... Yesterday I tried to install the graphics driver that came with cuda, but my gtx 970 stopped working with ubuntu, today I'm trying to figure out why, but If I come across that error again, I'll file a new issue, thanks", "i got same error, but with caffe !, i installed cuda 8 and for some application it worked fine but for caffe this issue got me confusing, if someone helped this please share your solutions :\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#5>\", line 1, in <module>\r\n    import caffe\r\n  File \"/Caffe/python/caffe/__init__.py\", line 1, in <module>\r\n    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer\r\n  File \"/Caffe/python/caffe/pycaffe.py\", line 13, in <module>\r\n    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\r\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n", "Sorry, I forgot to come back here, the problem was that I forgot to export the path to cuda lib, here is the solution:\r\n\r\nhttp://askubuntu.com/questions/889015/cant-install-cuda-8-but-have-correct-nvidia-driver-ubuntu-16", "For me, the only thing that works is to type\r\nsudo ldconfig /usr/local/cuda/lib64", "sudo ldconfig /usr/local/cuda/lib64  works for me.", "sudo ldconfig /usr/local/cuda/lib64 works for me.\r\nOS: LinuxMint 18 (Sarah)\r\ncuda_version: 8.0\r\ncuDNN_version: 5.1", "sudo ldconfig /usr/local/cuda/lib64 works for me as well, but i have to do it EVERY time i've restarted my computer.. Any tips how to avoid this?", "````sudo ldconfig /usr/local/cuda/lib64```` works for me also, but i need to do it once in a while (no matter if i restart or not). weird !!", "@sedghi same here... ldconfig helped, but after a while (hours, days) theres no effect...", "@maxss280's method works for me! In fact I have written some path settings, but with wrong syntax (forgot to add keyword \"export\"). So, this step, checking if environment variables are valid, is very important.\r\nA good reference to this: https://unix.stackexchange.com/questions/117467/how-to-permanently-set-environmental-variables\r\n\r\nBTW, when something must be redone for each time computer restarted, I think writing the command into .bashrc file may be a kind of solution.", "@maxss280 Works!\r\n`export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64`\r\nor\r\n`export LD_LIBRARY_PATH=/usr/local/cuda/lib64`\r\n\r\n\r\n", "I have **Cuda 9.0 , cudnn 7.0 on Ubuntu 16.04**.\r\nI built Tensorflow 1.6 since the release notes states it has pre-compiled binaries for cuda 9.0 and cudnn 7.0.\r\n\r\n`ldconfig -v: libcublas.so.9.0`\r\n\r\nupon running:\r\n\r\n> import tensorflow as tf\r\n\r\nI still get the error:\r\n`ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n`\r\nMy Questions:\r\n\r\n**1. Shouldn't Tensorflow 1.6 work with cuda 9.0/ cudnn 7.0 ?\r\n2. Is there any way I can continue using cuda 9.0 ?? I really cannot use 8.0**", "The prebuilt binaries 1.6 binaries are indeed using cuda 9.0.\r\nWhy did you need to \"build\" tf, as you already have cuda 9.0?\r\n\r\nto install the release candidates, you will need to run:\r\n`pip install tensorflow-gpu --pre --upgrade`", "I'm not sure why it is looking for libcublas.so.8.0. Is there any cache stored from which `sudo -H pip install tensorflow-gpu` builds the package?  I do uninstall tensorflow-gpu each time using `sudo -H pip uninstall tensorflow-gpu` each time. So I'm not sure what's going on...\r\n\r\nI built from source using bazel too. Still the same error!!!!! \r\n\r\nAny thoughts??\r\n", "On my computer I had :\r\n\r\n    cudnn5.1,   \r\n    cuda8.0\r\n\r\n\r\nWhen importing tensorflow, I had the same error ImportError: \r\n\r\n> libcublas.so.8.0: cannot open shared object file: No such file or\r\n> directory\r\n\r\nand even \r\n\r\n> libcublas.so.9.0: cannot open shared object file: No such file or\r\n> directory\r\n\r\n\r\nTo overcome the second issue I installed `tensorflow 1.4` instead of 1.6 \r\n and for the first I did\r\n$ export PATH=\"$PATH:/usr/local/cuda-8.0/bin\" \r\n$ export LD_LIBRARY_PATH=\"/usr/local/cuda-8.0/lib64\"\r\n\r\n\r\nBut then I had another issue : \r\n\r\n> libcudnn.so.6: cannot open shared object file: No such file or\r\n> directory\r\n\r\nThis was because I had cudnn5.1 as required but actually it needs cudnn6\r\n\r\nHere are the steps to uninstall cudnn5.1 and install cudnn6:\r\n\r\n1- Uninstall cudnn 5.1\r\n    rm -f /usr/include/cudnn.h\r\n    rm -f /usr/lib/x86_64-linux-gnu/libcudnn\r\n    rm -f /usr/local/cuda-/lib64/libcudnn\r\n\r\n2- Install cudnn6\r\n\r\nAfter having subscribed to nvdia, go to here https://developer.nvidia.com/rdp/cudnn-download and download cudnn6 for cuda8 and then go to the folder where you downloaded the cudnn and do :\r\n\r\n    $ tar xvzf cudnn-8.0-linux-x64-v5.1-ga.tgz\r\n    $ sudo cp -P cuda/include/cudnn.h /usr/local/cuda/include\r\n    $ sudo cp -P cuda/lib64/libcudnn /usr/local/cuda/lib64\r\n    $ sudo chmod a+r /usr/local/cuda/include/cudnn.h \r\n    /usr/local/cuda/lib64/libcudnn*\r\n\r\nNow you should have tensorflow\r\n\r\nTry it on typing in the console:\r\n\r\n    $python\r\n    import tensorflow\r\n\r\nIf you want to work in anaconda and the error persists , try :\r\n\r\n    $jupyter notebook --generate-config\r\n\r\nthen you can find the name of the directory where you have your config file (Ill call it\r\n\r\n) and open /jupyter_notebook_config.py and add at the top :\r\n\r\n    import os\r\n    c = get_config()\r\n    os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-8.0/lib64:usr/local/cuda-8.0/lib64/libcudart.so.8.0'\r\n    c.Spawner.env.update('LD_LIBRARY_PATH')\r\n\r\nNow it should work...\r\n\r\nHere are the websites that helped me:\r\n\r\nhttps://askubuntu.com/questions/952075/how-to-upgrade-tensorflow-to-v1-3-cudnn-cuda-upgrade\r\nhttps://developer.nvidia.com/rdp/cudnn-download\r\nhttps://medium.com/@ikekramer/installing-cuda-8-0-and-cudnn-5-1-on-ubuntu-16-04-6b9f284f6e77\r\nhttps://stackoverflow.com/questions/43984135/tensorflow-gpu-can-not-be-called-from-jupyterhub-jupyter-notebook-why\r\n", "maybe this was because the path usr/local/lib dont have the .so  file ,so you can cp the .so file to /usr/local/lib then ldconfig.\r\n- sudo cp  /lusr/local/cuda-9.0/libcudart.so.9.0 /usr/local/lib \r\n- sudo ldconfig\r\nthese steps works", "@NHQ , if you're using Ubuntu 16.04 or Ubuntu 18.04 and want to get TensorFlow with GPU support installed, there is a deb package for that in the Lambda Stack repository.\r\n\r\nYou can install the repository and the package with this line:\r\n\r\n\r\n    LAMBDA_REPO=$(mktemp) && \\\r\n    wget -O${LAMBDA_REPO} https://lambdal.com/static/files/lambda-stack-repo.deb && \\\r\n    sudo dpkg -i ${LAMBDA_REPO} && rm -f ${LAMBDA_REPO} && \\\r\n    sudo apt-get update && sudo apt-get install -y lambda-stack-cuda\r\n\r\nWhat it does:\r\n\r\n 1. Download and installs the Lambda Stack Repository (essentially adds a file to /etc/apt/sources.list.d/)\r\n 2. Updates apt and installs the `lambda-stack-cuda` package.\r\n 3. Installs CUDA, Drivers, CuDNN, and TensorFlow with CuDNN and GPU support into the proper system level directories. You won't need to modify your `LD_LIBRARY_PATH` or `PATH` as the shared libraries are placed in directories that `ld` already checks at link time.", "I have solve the problem. I think it is because the tensorflow ==1.4 , so I uninstall tensorflow-gpu==1.4 in coach (any ohter env)and then install the latest tensorflow gpu and the problem done.\r\n\r\npip3 uninstall tensorflow-gpu\r\npip3 install tensorflow-gpu\r\n\r\nI was using Carla in Coach and running demos.\r\nIt is because that I have to use cuda9.0 and the tensorflow-gpu1.4 can't support cuda9.0, however the tensorflow-gpu 1.8 does. This is the key problem.", "> I have installed `cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb` but as I try to import tensorflow I get\r\n> \r\n> ```\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n>     _pywrap_tensorflow = swig_import_helper()\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n>   File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\n> ```\r\n> I do not have a `/usr/local/cuda/` directoryafter installation\r\n> \r\n> ```\r\n> stefan@stefan-pc:~$ ls -la /usr/local/\r\n> total 44\r\n> drwxr-xr-x 11 root root 4096 Feb  9 19:19 .\r\n> drwxr-xr-x 13 root root 4096 Feb  2 19:32 ..\r\n> drwxr-xr-x  3 root root 4096 Feb  9 19:34 bin\r\n> drwxr-xr-x  3 root root 4096 Feb  9 19:19 cuda-8.0\r\n> ...\r\n> ```\r\n> The `cuda-8.0` directory only contains\r\n> \r\n> ```\r\n> stefan@stefan-pc:~$ ls -la /usr/local/cuda-8.0/\r\n> total 12\r\n> drwxr-xr-x  3 root root 4096 Feb  9 19:19 .\r\n> drwxr-xr-x 11 root root 4096 Feb  9 19:19 ..\r\n> drwxr-xr-x  3 root root 4096 J\u00e4n  9 20:43 targets\r\n> ```\r\n> Which I think comes from cuDNN.\r\n> \r\n> I don't quite understand why it does not find `libcudart.so.8.0`. Not sure what the `.deb` file installed if not CUDA.\r\n\r\n\r\nTry:  conda install -c anaconda cudatoolkit\r\n", "Hello,\r\n\r\nI had the same issue. I fixed it by adding the below command to the '**.bashrc**' file.\r\n\r\n> export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.0/lib64/\r\n\r\n**System configuration:**\r\n\r\n```\r\nUbuntu 16.04 LTS\r\nTensorflow GPU 2.0beta1\r\nCuda 10.0\r\ncuDNN 7.6.0 for Cuda 10.0\r\n```\r\n\r\nI  used conda to configure my system."]}, {"number": 5342, "title": "Gradient for gather_nd is not implemented", "body": "Hi all,\r\n\r\nwhen I use `tf.gather_nd`, It gives me the error \"NotImplementedError: Gradient for gather_nd is not implemented\".\r\n\r\nI can temporarily solve this problem to flatten both of `params` and `indices` and then use `tf.gather`. But it isn't the best way to do so.\r\n\r\nIs there any plan to support that? Thanks!", "comments": ["Yes, we are planning to support that. The patch will be imminently committed. @narphorium is implementing it.\n", "@aselle Oh great. Thanks for your quick reply. I'm looking forward to it.\n", "@lan2720 \u3002\u3002how to flatten\uff0c I meet the same problem and I am a newer to tensorflow . Hope some advice for me . Thanks\n", "@tfka Yes, to use tf.gather you just need to reshape your params and indices.\n\nFor example \n\n```\nparams = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15]]\nindices = [[2, 3, 1], [4, 0, 2], [3, 4, 0]]\n# After some operation\nnew_params = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\nnew_indices = [2, 3, 1, 4+5, 0+5, 2+5, 3+10, 4+10, 0+10]\n```\n", "Hi,\r\nIs it fixed yet ? I'm using gather_nd() and I don't get any warning, but my gradient descent does not seem to work properly, so I guess it could come from there...", "gather_nd gradients are available on master, and should be in the latest minor release:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/cb17d1b0e2b581b5da1d9597b7929ba764749d38/tensorflow/python/ops/array_grad.py"]}, {"number": 5341, "title": "TensorBoard not working with 0.11 RC1", "body": "All I see is a blank page when I go to localhost:6006\r\n\r\nBrowser: Chrome (Version 54.0.2840.71 (64-bit))\r\nHere's the full stack trace:\r\n\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET / HTTP/1.1\" 200 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /webcomponentsjs/webcomponents-lite.min.js HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /dist/bazel-html-imports.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /lib/css/global.css HTTP/1.1\" 200 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /dist/tf-tensorboard.html HTTP/1.1\" 200 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /polymer/polymer.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /iron-icons/iron-icons.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-tabs/paper-tabs.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-dialog/paper-dialog.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-toolbar/paper-toolbar.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-button/paper-button.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-checkbox/paper-checkbox.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-header-panel/paper-header-panel.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /lodash/lodash.min.js HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-slider/paper-slider.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-input/paper-input.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /d3/d3.js HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-styles/paper-styles.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-dropdown-menu/paper-dropdown-menu.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-menu/paper-menu.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-item/paper-item.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /iron-collapse/iron-collapse.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /plottable/plottable.js HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /plottable/plottable.css HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-icon-button/paper-icon-button.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-toggle-button/paper-toggle-button.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /lodash/lodash.min.js HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /graphlib/dist/graphlib.core.js HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /dagre/dist/dagre.core.js HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /lodash/lodash.min.js HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /graphlib/dist/graphlib.core.js HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /iron-flex-layout/iron-flex-layout.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /iron-list/iron-list.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-item/all-imports.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-progress/paper-progress.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-radio-group/paper-radio-group.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] code 404, message Not Found\r\n127.0.0.1 - - [02/Nov/2016 08:59:41] \"GET /paper-tooltip/paper-tooltip.html HTTP/1.1\" 404 -\r\n", "comments": ["Upgraded to 0.11 RC2 and I still see the same errors.\n", "@dsmilkov  could you please take a look at this issue?\n", "I tried to reproduce this and couldn't. I tried the following things:\nLinux - Build the pip package against master (not 0.11 rc2 due to bazel version compatibility issues), install resultant pip package, and launch TensorBoard. No problem.\nMac - Build the pip package against 0.11 rc2, install resultant pip package, and launch TensorBoard. No problem.\nMac - Bazel build tensorboard, and launch tensorboard from bazel-bin. No problem.\n\n@jkschin Please provide a more detailed reproduction.\n", "@danmane Linux 14.04\n\n```\ngit checkout master (commit number a50743836)\n./configure\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\nsudo pip uninstall tensorflow\nsudo pip install /tmp/tensorflow_pkg/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl\npython\nimport tensorflow as tf\ntf.__version__ == '0.11.0rc2'\n```\n\nBazel details\n\n```\nBuild label: 0.3.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\nBuild timestamp: 1475861110\nBuild timestamp as int: 1475861110\n```\n\nRan \n\n```\npython cifar10_train.py\ntensorboard --logdir=/tmp/cifar10_train\n```\n\nGot this still\n\n```\n127.0.0.1 - - [04/Nov/2016 13:49:15] \"GET / HTTP/1.1\" 200 -\n127.0.0.1 - - [04/Nov/2016 13:49:15] code 404, message Not Found\n127.0.0.1 - - [04/Nov/2016 13:49:15] \"GET /webcomponentsjs/webcomponents-lite.min.js HTTP/1.1\" 404 -\n127.0.0.1 - - [04/Nov/2016 13:49:15] \"GET /dist/bazel-html-imports.html HTTP/1.1\" 200 -\n127.0.0.1 - - [04/Nov/2016 13:49:15] \"GET /lib/css/global.css HTTP/1.1\" 200 -\n127.0.0.1 - - [04/Nov/2016 13:49:15] \"GET /dist/tf-tensorboard.html HTTP/1.1\" 200 -\n127.0.0.1 - - [04/Nov/2016 13:49:15] code 404, message Not Found\n127.0.0.1 - - [04/Nov/2016 13:49:15] \"GET /paper-tabs/paper-tabs.html HTTP/1.1\" 404 -\n127.0.0.1 - - [04/Nov/2016 13:49:15] code 404, message Not Found\n127.0.0.1 - - [04/Nov/2016 13:49:15] \"GET /polymer/polymer.html HTTP/1.1\" 404 -\n127.0.0.1 - - [04/Nov/2016 13:49:15] code 404, message Not Found\n127.0.0.1 - - [04/Nov/2016 13:49:15] code 404, message Not Found\n127.0.0.1 - - [04/Nov/2016 13:49:15] \"GET /iron-icons/iron-icons.html HTTP/1.1\" 404 -\n127.0.0.1 - - [04/Nov/2016 13:49:15] \"GET /numericjs_numeric_min_js/file/numeric.min.js HTTP/1.1\" 404 \n```\n", "Hi @jkschin,\n\nI had the very same problem and my colleague just realised that we are calling tensorboard in different ways, and it was automatically solved. Instead of running tensorboard by\n\n`tensorboard --logdir=path/to/log`\n\ntry running the tensorboard.py file in the tensorflow library. In my case:\n\n`cd /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard`\n`python tensorboard.py --logdir=path/to/log`\n\nI hope it works for you as well!\n", "Had the same issue with both 0.11 and 0.11 RC2. @scanyameres's workaround worked for me. ", "I ran into the same problem, after following the instructions at [https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#using-pip](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#using-pip), specifically:\r\n\r\n```\r\n# Mac OS X, GPU enabled, Python 3.4 or 3.5:\r\n(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.11.0-py3-none-any.whl\r\n```\r\n\r\nThe file ```dist/bazel-html-imports.html``` was not in ```/usr/local/lib/python3.5/site-packages/tensorflow/tensorboard/dist```. I copied a few files from github (```package.json```, ```bower.json```, etc.) and ran bower and npm install and somehow got it working \u2013 numericjs was also missing.\r\n\r\nI can't make sense of the build system fast enough to contribute a useful pull request, but from browsing the changes, I'd guess @dsmilkov may be able to help.\r\n\r\nThe workaround by @scanyameres may work if there are two installations of tf \u2013 a default one using python3.x and the python2.7 version without this problem.", "Most of our TensorBoard build system stuff is going to be replaced in the next couple of weeks. I would recommend against expending too much mental effort into trying to figure it out or contributing until then. The new build is going to be amazing and beautiful. You're going to love it.", "I'm on Windows, v0.12.0rc0 GPU (tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl) and I face the same issue.", "@tomzx at this moment only the [nightly builds](http://ci.tensorflow.org/view/Nightly/job/nightly-win/) have [the fix](https://github.com/tensorflow/tensorflow/pull/5844) for this issue, which will also be in the `0.12.0rc-1`. The other alternative is building it yourself. ", "Judging from the last comments, this should be resolved in 0.12.1"]}, {"number": 5340, "title": "Rename argument names in tf.train.exponential_decay()", "body": "The current function signature reads: \r\n\r\n```python\r\ntf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)\r\n```\r\n\r\nWhen passing the parameters by keyword name (for example after reading a configuration file), it's pretty weird that the first parameter is called `learning_rate` since the function can be used for other decaying parameters as well.\r\n\r\nA better name would be `initial_value`.", "comments": ["@martinwicke, who would be good for this. It seems close to the argument ordering work you were telling me about yesterday.\n", "It would be lovely to change that, I agree. I am not sure that we'll get to it -- we're accumulating work fast, and this seems to be a slightly lesser problem than the others.\n", "We did not get to this before 1.0 and now it's too late. It's not a terrible bug, so I'll close this.", "Taking about cleaning up the API, an exponential decay op should also not live in `tf.train`. Maybe worth cleaning up and reorganizing some ops for TF 2.0?", "Why not create a new exponential_decay function with the correct argument names and at the correct location now, and leave the current one for compatibility? This would be a non-breaking change that would solve this issue. ", "Yeah I should do that.", "We are working on cleaning up the API, this could be one of the things we do (make an alias and soft-deprecate the old form).\r\n\r\n@aselle FYI"]}, {"number": 5339, "title": "Branch 137889948", "body": "", "comments": ["@vrv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @martinwicke and @petewarden to be potential reviewers.\n"]}, {"number": 5338, "title": "Failed to run MNIST example on Android", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n#4623\r\n#4451\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`): 3d35376a66cde4f3e614c746d3c8708d15caa1b5\r\n2. The output of `bazel version`: Build label: 0.3.2\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n- Train using [MNIST tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/mnist/pros/index.html)\r\n- `tf.train.write_graph(..., mnist_graph.txt, as_text=True)`\r\n- freeze graph `bazel-bin/.../freeze_graph --input_graph=.../mnist_graph.txt --input_checkpoint=.../mnist_model.ckpt --output_graph=.../mnist_graph_frozen.pb --output_node_names=\"output_node\"`\r\n- strip graph `bazel-bin/..bazel-bin/.../strip_unused --input_graph=.../mnist_graph_frozen.pb --output_graph=.../mnist_graph_frozen_stripped.pb --input_node_names=\"input_node\" --output_node_names=\"output_node\" --input_binary=true`\r\n- Java code: basically I took part of TensorFlowImageListener and part of TensorFlowImageClassifier\r\n  - Remove the image preprocessing part, instead pass in the raw mnist datapoint of shape [784] as floatValues\r\n  - `inferenceInterface.fillNodeFloat(\u201dinput_node\", 1, 28, 28, 1, floatValues); //When training I was reshaping the datapoint to 28x28. `\r\n  - `inferenceInterface.runInference(new String[] {\"output_node\"})`\r\n  - `inferenceInterface.readNodeFloat(\"output_node\", outputs\")`\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n10-23 19:16:51.918: I/native(10721): tensorflow_inference_jni.cc:126 Creating session.\r\n10-23 19:16:51.967: I/native(10721): tensorflow_inference_jni.cc:137 Tensorflow graph loaded from: file:///android_asset/mnist_graph_frozen_stripped.pb\r\n10-23 19:16:51.967: I/native(10721): tensorflow_inference_jni.cc:140 Initialization done in 137.556ms\r\n10-23 19:16:51.967: I/MainActivity(10721): Tensorflow model initialized\r\n10-23 19:16:51.988: I/native(10721): tensorflow_inference_jni.cc:87 Found session variables for 6f51f9109840554e\r\n10-23 19:16:51.988: I/native(10721): tensorflow_inference_jni.cc:87 Found session variables for 6f51f9109840554e\r\n10-23 19:16:52.194: I/native(10721): tensorflow_inference_jni.cc:197 End computing. Ran in 205ms (205ms avg over 1 runs)\r\n**10-23 19:16:52.194: E/native(10721): tensorflow_inference_jni.cc:202 Error during inference: Invalid argument: You must feed a value for placeholder tensor 'Placeholder' with dtype float\r\n10-23 19:16:52.194: E/native(10721): \t \\[\\[Node: Placeholder = Placeholder \\[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]**\r\n10-23 19:16:52.194: I/native(10721): tensorflow_inference_jni.cc:87 Found session variables for 6f51f9109840554e\r\n10-23 19:16:52.194: I/native(10721): tensorflow_inference_jni.cc:87 Found session variables for 6f51f9109840554e\r\n**10-23 19:16:52.195: E/native(10721): tensorflow_inference_jni.cc:159 Output [output_node] not found, aborting!**\r\n\r\n**Thanks for reading this. Please let me know if there's any other information I need to provide.** ", "comments": ["I would try loading the graph in python and see if you can feed it there to see if the problem is with your baked trained model or with your Java usage. @petewarden might have more precise suggestions. Could there be an errant Placeholder that is in the mnist_model you created that you could just feed with a dummy value just to get the ball rolling?\n", "@aselle Thanks for the suggestions! I haven't fully sorted it out yet, but yes you are right. The 'Placeholder' was referring to the drop out layer that's not given a name. Will update and close the issue once I fix it.\n", "Thanks for the update, good luck.\n", "@aselle I finally got it running.\n\nFirst I tried to work around with that dropout layer in the model, by feeding the `Placeholder` (the keep probability) with a value `{1f}`, but I ended up getting `In[0] is not a matrix` at the `MatMul`that uses the dropout as input. I gave up after trying a few things to solve it.\n\nThe final working model was a clean one with manually assigned parameters.\n\nI guess understanding the error messages has been the biggest obstacle for me throughout the process. It is especially hard for people who are not working with the C++ API (I don't know if it's possible to debug and see what's happening in the C code below JNI). It's tricky to figure out that `You must feed a value for placeholder tensor XXX` happens when some placeholder exists in the graph but was not fed with a value, while `FeedInputs: unable to find feed output` happens when some placeholder does not exist in the graph but you are feeding value to it. Thanks again for helping out. Closing the issue.\n"]}, {"number": 5337, "title": "How to install Tensorflow on windows for Ananconda?", "body": "![image](https://cloud.githubusercontent.com/assets/12981723/19903991/c07a065c-a03e-11e6-8265-512ad1077b41.png)\r\n", "comments": ["As far as I know ( @mrry  please correct me if I'm wrong), Windows support still requires you to install manually using CMake see commint on the PR https://github.com/tensorflow/tensorflow/pull/4778.\nAlso see http://stackoverflow.com/questions/37522676/install-tensorflow-on-windows-with-anaconda\n", "There are no binary packages for Windows (yet), so as Andrew says you'll have to build it yourself. It's currently possible to build a TensorFlow PIP package on Windows using Bazel or (if you want GPU support) CMake.\n\nThere's an example script for using Bazel on Windows [here](https://github.com/tensorflow/tensorflow/blob/816ecb7a342c25c19daa8b19627346e1fb38e56f/tensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh) and instructions for using CMake on Windows [here](https://github.com/tensorflow/tensorflow/blob/816ecb7a342c25c19daa8b19627346e1fb38e56f/tensorflow/contrib/cmake/README.md).\n"]}, {"number": 5336, "title": "Regression: PyUnicodeUCS4_AsUTF8String on 0.11.0rc0 to 0.11.0rc2", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttp://stackoverflow.com/questions/33795982/undefined-symbol-pyunicodeucs4-fromstringandsize-with-tensorflow-on-heroku\r\n### Environment info\r\nOperating System:\r\nHeroku\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\nConfirmed working on https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\nNov 01 15:05:13 i23d-staging app/worker.1:      import tensorflow as tf \r\nNov 01 15:05:13 i23d-staging app/worker.1:    File \"/app/.heroku/python/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module> \r\nNov 01 15:05:13 i23d-staging app/worker.1:      from tensorflow.python import * \r\nNov 01 15:05:13 i23d-staging app/worker.1:    File \"/app/.heroku/python/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module> \r\nNov 01 15:05:13 i23d-staging app/worker.1:      from tensorflow.python import pywrap_tensorflow \r\nNov 01 15:05:13 i23d-staging app/worker.1:    File \"/app/.heroku/python/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module> \r\nNov 01 15:05:13 i23d-staging app/worker.1:      _pywrap_tensorflow = swig_import_helper() \r\nNov 01 15:05:13 i23d-staging app/worker.1:    File \"/app/.heroku/python/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper \r\nNov 01 15:05:13 i23d-staging app/worker.1:      _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description) \r\nNov 01 15:05:13 i23d-staging app/worker.1:  ImportError: /app/.heroku/python/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyUnicodeUCS4_AsUTF8String \r\n```", "comments": ["Just to be clear, how is this reproduced? Just importing tensorflow as \n\n``` python\nimport tensorflow as tf\n```\n\ncauses the problem?  This only occurs on Heroku? What version of Python?\n", "@aselle Yeah. So far the target platform I tested was only heroku. The python version is 2.7.10. \n", "+1 on Centos 7. Python 2.7.12\n\n$ python\nPython 2.7.12 (default, Aug 31 2016, 14:30:10) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import tensorflow as tf\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/usr/local/python-2.7.12/lib/python2.7/site-packages/tensorflow/**init**.py\", line 23, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/usr/local/python-2.7.12/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 49, in <module>\n> > >     from tensorflow.python import pywrap_tensorflow\n> > >   File \"/usr/local/python-2.7.12/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n> > >     _pywrap_tensorflow = swig_import_helper()\n> > >   File \"/usr/local/python-2.7.12/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n> > >     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n> > > ImportError: /usr/local/python-2.7.12/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyUnicodeUCS4_AsUTF8String\n", "Could you try the nightly build and also other older versions and see if we can narrow it down to just that build?\n\nIt looks like your version of Python does not have UCS4 unicode checked in, see e.g.\nhttps://groups.google.com/forum/#!topic/comp.lang.python/9dThp1h3TMM\n\nCan you do?\n\n```\n>>> import sys\n>>> print sys.maxunicode\n```\n\nand see what it returns (see e.g. http://stackoverflow.com/questions/1446347/how-to-find-out-if-python-is-compiled-with-ucs-2-or-ucs-4)\n", "It looks like sys.maxunicode == 65535, which I would guess is UCS2.\n\nI did try back to 0.11.0rc0 and saw the same issue. We've been running with a custom-compiled version, since we needed CUDA 8 support.\n\nIt's a little difficult to try a custom version without disrupting others' work, but I can try--we're running TF pretty much 24/7 now. \n", "I believe in general it is unlikely that our binary which is designed to work on Ubuntu will work on CentOS 7, because the build options on Python are possibly different between the two. Thus, on CentOS, you might need to use a custom built version. It appears CentOS7 and Ubuntu use different unicode options... @gunan might know better.  You could probably build a custom python just for tensorflow if you want to use the Ubuntu binary, but that might just be asking for trouble.\n", "We never tested our binary with CentOS, therefore the distributed pip package has no guarantees.\nAs @aselle suggested, rebuilding from source, or using docker images on CentOS should work for you.\n\nNot sure about heroku platform. Again, it is never tested.\nTherefore no guarantees on it.\n", "@gunan @aselle Heroku also returns 65535. For us it's not a big deal and we already switched to EC2. But it does look like a regression since `0.10.0rc0` works.\n", "A quick google search showed that heroku instances are built on top of ubuntu 14.04.\nUsing docker, I tried the following:\n\n$ docker run -it ubuntu:14.04 bash\n$ apt-get update\n$ apt-get install python python-dev python-pip -y\n$ pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl\n$ python -c \"import tensorflow; print(tensorflow.**version**)\"\n\nWhich results in:\n0.11.0rc2\n\nLooks like in the stackoverflow question @mrry was able to root cause the problem and provide a solution for the platform in question.\nHowever, since we do not have official support for this platform, and issue does not affect ubuntu version we are supporting, and the one before that, I will close this issue.\n\nI am really sorry for the inconvenience, but looks like the solution for heroku and centOS is to build the pip package from sources.\n"]}, {"number": 5335, "title": "Is there any way to add classes in inception?", "body": "Hi there,\r\nI'm working on retrain example; I have TensorFlow built from source on Ubuntu 14.04, and the examples on flowers work great. However, the process strips away the Inception's final layer and removes all 1,000 existing categories, which means it can now identify 5 species of flowers, but can no longer identify pandas, for example. https://www.tensorflow.org/versions/r0.8/how_tos/image_retraining/index.html\r\n\r\nHow can I add the 5 flower categories to the existing 1,000 categories from ImageNet (and add training for those 5 new flower categories) so that I have 1,005 categories that a test image can be classified as? In other words, be able to identify both those pandas and sunflowers?\r\n\r\nThanks", "comments": ["This question is more important for StackOverflow, since it concerns using Inception so closing for now. @shlens might have a quick answer. Barring anything else, why not just try adding 5 to the original labels and see if that works.\n"]}, {"number": 5334, "title": "tensorboard raises `FailedPreconditionError`", "body": "## Description\r\nTensorboard always raises the exception `FailedPreconditionError` in my environment. I have it running with parameter `logdir` pointing to a directory that is tracking active experiments. Tensorboard raises an exception after a few minutes with a Traceback similar to the following:\r\n```bash\r\ntensorboard --debug --logdir .\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so locally\r\nINFO:tensorflow:TensorBoard is in debug mode.\r\nINFO:tensorflow:Starting TensorBoard in directory /media/data/sarroff/experiments/source_separation/logdir3\r\nINFO:tensorflow:TensorBoard path_to_run is: {'/media/data/sarroff/experiments/source_separation/logdir3': None}\r\nINFO:tensorflow:TensorBoard is tag: 33\r\nStarting TensorBoard 33 on port 6006\r\n(You can navigate to http://127.0.1.1:6006)\r\nINFO:tensorflow:Adding events from directory /media/data/sarroff/experiments/source_separation/logdir3/train/161031224434\r\nINFO:tensorflow:Constructing EventAccumulator for /media/data/sarroff/experiments/source_separation/logdir3/train/161031224434\r\nDEBUG:tensorflow:Opening a record reader pointing at /media/data/sarroff/experiments/source_separation/logdir3/train/161031224434/events.out.tfevents.1477968276.eltopo\r\nDEBUG:tensorflow:No more events in /media/data/sarroff/experiments/source_separation/logdir3/train/161031224434/events.out.tfevents.1477968276.eltopo\r\nINFO:tensorflow:No path found after /media/data/sarroff/experiments/source_separation/logdir3/train/161031224434/events.out.tfevents.1477968276.eltopo\r\nINFO:tensorflow:Multiplexer done loading. Load took 49.9 secs\r\n\r\n... <snip> ...\r\n\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/tensorboard/backend/server.py\", line 126, in _ReloadForever\r\n    ReloadMultiplexer(multiplexer, path_to_run)\r\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/tensorboard/backend/server.py\", line 100, in ReloadMultiplexer\r\n    multiplexer.AddRunsFromDirectory(path, name)\r\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/summary/event_multiplexer.py\", line 178, in AddRunsFromDirectory\r\n    for subdir in GetLogdirSubdirectories(path):\r\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/summary/event_multiplexer.py\", line 396, in <genexpr>\r\n    subdir\r\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/summary/impl/io_wrapper.py\", line 50, in ListRecursively\r\n    for dir_path, _, filenames in gfile.Walk(top):\r\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 415, in walk\r\n    listing = list_directory(top)\r\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 391, in list_directory\r\n    file_list = get_matching_files(os.path.join(compat.as_str_any(dirname), \"*\"))\r\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 260, in get_matching_files\r\n    compat.as_bytes(filename), status)]\r\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\nFailedPreconditionError: /media/data/sarroff/experiments/source_separation/logdir3/train/161031224434/model.ckpt-263550.index\r\n```\r\nThe accompanying exception message mentions a different file every time and the exception is raised at some arbitrary time after execution. It is therefore not easy to replicate my issue deterministically.\r\n\r\n## Environment\r\nUbuntu 16.04LTS\r\nTensorFlow commit 1fcd6d1294564066c6f92b121a3aaf4ed186dc1a\r\n\r\n\r\n## Proposed Cause\r\nThis is just a guess; I'm not too familiar with the code: The exception is raised in `FileSystem::GetMatchingPaths`. Perhaps, while traversing the directory, some files get incorrectly marked as directories. If so, then the error probably happens during the call to `opendir` in `PosixFileSystem::GetChildren`. Error codes which are mapped to `FailedPreconditionError` are listed in `tensorflow/core/platform/posix/error.cc`. A likely error code causing the exception is `ENOTDIR`. I don't have an explanation as to why files would occasionally be incorrectly marked as directories. Perhaps there is a problem unique to my environment.\r\n\r\nI've also noticed the recent Issue https://github.com/tensorflow/tensorflow/issues/4921, which specifically mentions problems with `FileSystem::GetMatchingPaths`. Perhaps this issue is connected with that one.", "comments": ["Some questions\n- Could you try it with a simpler path like in /tmp/foo? e.g. Can you \n\n```\nls  -l media/data/sarroff/experiments/source_separation/logdir3/train/161031224434/model.ckpt-263550.index\n```\n\n(i.e. does the file exist)?\n- Do you have a simple model that you are using to produce that? \n- Can you provide a minimal reproducible test case?\n- What version of Python?\n", "I'm using Python 2.7.11. I confirmed that the file exists and is a normal file with normal access permissions.\n\nUnfortunately I am so far unable to reproduce the issue when saving checkpoints for a minimal test case. (The problem persists when tensorboard is given a logdir for checkpoints produced by the non-minimal model that I'm currently training.)\n\nSince it's unclear whether this is actually a TF issue I'll close and reopen if I discover more pertinent information. Thanks!\n", "Thank you. Best of luck.\n"]}]