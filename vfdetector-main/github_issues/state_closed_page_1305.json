[{"number": 13958, "title": "Upgrade gRPC", "body": "Update commit ID of grpc, update bazel.rc to force non-use of ares resolver, and also stop patching gRPC. This should enable regular updates of gRPC since separate patches will no longer be needed.\r\n\r\n~~Some changes below will be brought in separately by another commit and should rebase out cleanly.~~\r\n", "comments": ["Can one of the admins verify this patch?", "Will wait until the internal changes are pushed and rebase.", "Waiting on #13976 ", "Cc: @nicolasnoble @ncteisen\r\n", "@vjpai  #13976 is merged, but it creates a conflict with your PR. Can you fix the conflict ?", "Will do\n\nOn Wed, Oct 25, 2017, 2:39 PM Benoit Steiner <notifications@github.com>\nwrote:\n\n> @vjpai <https://github.com/vjpai> #13976\n> <https://github.com/tensorflow/tensorflow/pull/13976> is merged, but it\n> creates a conflict with your PR. Can you fix the conflict ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13958#issuecomment-339479070>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIdEkGT1Op9yHQixBn8KJ-Eu0ovHoX9Jks5sv6qLgaJpZM4QFKvw>\n> .\n>\n", "Ok, I think conflicts are resolved. I can haz test? (I tested it locally on bazel but not cmake, Windows, etc)\r\n", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "There are a lot of falures that look like:\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n\r\nCould that be due to the fact that this PR started to link abseil in TF, and the abseil version of the functions conflict the the ones provided by TF ?\r\n", "It's indeed very strange. The abseil functions and classes have their own namespace (absl::) so I don't think that there would be a build conflict; it is definitely something that needs some investigation. Right now gRPC has a dependence on abseil for future use but does not currently use any of the abseil functions.", "Jenkins, test this please\r\n", "Jenkins, test this please", "Jenkins, test this please", "Linux XLA failures are worrying. Just want to double check.\r\nJenkins, test this please.", "Sorry, haven't had the chance to check back in on this for a few days, but the # of failures seems high and widespread. I will try to take a look today or tomorrow. My feeling was to try to bisect the gRPC commit point or possibly push it forward since I know there have been a few bug fixes in gRPC meanwhile as well. I guess one thing that made me wonder in particular was the number of failures coming from targets that I wouldn't expect to have any gRPC component like compiler (though maybe I'm wrong about that) while it looks like the gRPC-specific unit tests were fine, afaict.", "I am also surprised, I am rerunning some other tests, but XLA tests have failed on multiple runs, so I think the failures are legitimate. Did not take a hard look into why they might be though.", "I've tried a few different speculations and don't believe either worked: updating the gRPC commit point and commenting out the unneeded absl dependence. Still investigating. Can you suggest a good test to consider as a signal for what's working or not? I've been trying with `bazel test //tensorflow/contrib/all_reduce:all` but I don't know how representative of a test that is.", "I think any of the failing tests are good.\r\nYou just need to make sure you are running with XLA support enabled, on a machine that has an nvidia GPU and cuda to replicate our test setup.\r\nJenkins, test this please.", "How about  `tensorflow/compiler/tests:xla_device_test`?\r\nIt seems to be a simple fast test that could help in your investigation.", "I speculated that maybe c-ares was being included even though it isn't supposed to be, but that's not it either. I confirmed this by putting a `#error` directive in ares specific code and confirming that I trigger it on a native gRPC build but not on a TF-specific gRPC build (since this PR defines `GRPC_ARES=0` in `tools/bazel.rc`).\r\n\r\n", "Ok, I now know the problem. gRPC depends on protobuf as does TF. We need to change to make gRPC depend on protobuf_headers only (which we wanted to do anyway for other reasons). Otherwise there's some sort of diamond-dependence problem which is destroying serialization. I have tested this by bringing in my own branch and confirmed that this is an issue (and hopefully the only issue) and will now get this resolved at gRPC master and then revisit this PR. Thanks for the help!\r\n", "The tests that were failing for me previously are now passing if I pin to my branch at grpc/grpc#13257 . I am going to push a commit now that is purely WIP because it is linked to vjpai/grpc instead of grpc/grpc , so please don't merge it. I think it will work (fingers crossed).\r\n", "Jenkins, test this please.", "I'm surprised that that's an issue now. We also explicitly depend on the same version of protobuf.", "Hmm, looks like things are moving though! Maybe we take protobuf at a different commit point?", "Jenkins, test this please", "Ubuntu contrib issue is a known flaky failure. Merging.", "Looks like with the latest prod push, and us also adding absl dependency there is a collision.\r\nI will try to see what is going on.", "For us, the absl dependence is not really important. We are not actively\nusing it yet. We have the dependence in place for future expansion, and\nthere is a strong feeling that we will actually be rolling it back and not\nusing it for the next quarter or so. So, feel free to change the commit\npoint of that dependence as you need to.\n\nOn Fri, Nov 3, 2017, 8:02 PM gunan <notifications@github.com> wrote:\n\n> Looks like with the latest prod push, and us also adding absl dependency\n> there is a collision.\n> I will try to see what is going on.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13958#issuecomment-341867360>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIdEkNFQyZgCkFkuRfHbdTS3Pa4PdxnFks5sy9PYgaJpZM4QFKvw>\n> .\n>\n"]}, {"number": 13957, "title": "TimeDistributed (keras) wrapper broken in 1.4rc1", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4rc1\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nTwo issues I've found when testing the TimeDistributed wrapper in release *1.4rc1*. I figure I'd wrap them (pun intended?) in one issue since they're both occur on the TimeDistributed wrapper. See simple code examples below which both work fine in version *1.3*.\r\n\r\n### Source code / logs\r\n```\r\ndef td():\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Dense(8, input_shape=(16,)))\r\n    model.add(tf.keras.layers.Dense(4))\r\n    model.summary()\r\n    \r\n    frame_input = tf.keras.layers.Input(shape=(10, 16))\r\n    x = tf.keras.layers.TimeDistributed(model)(frame_input)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    \r\n    full_model = tf.keras.models.Model(inputs=frame_input, outputs=x)\r\n    full_model.summary()\r\n```\r\nproduces this trace:\r\n\r\n```\r\n     x = tf.keras.layers.TimeDistributed(model)(frame_input)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py\", line 252, in __call__\r\n    output = super(Layer, self).__call__(inputs, **kwargs)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 575, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/wrappers.py\", line 238, in call\r\n    output_shape = self._compute_output_shape(input_shape).as_list()\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/wrappers.py\", line 193, in _compute_output_shape\r\n    child_input_shape).as_list()\r\nAttributeError: 'NoneType' object has no attribute 'as_list'\r\n```\r\nand...\r\n\r\n```\r\ndef td2():\r\n    vgg16_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(100, 100, 3))\r\n    \r\n    frame_input = tf.keras.layers.Input(shape=(10, 100, 100, 3))\r\n    \r\n    x = tf.keras.layers.TimeDistributed(vgg16_model)(frame_input)\r\n    model = tf.keras.models.Model(inputs=frame_input, outputs=x)\r\n    model.summary()\r\n```\r\nproduces this trace:\r\n\r\n```\r\n    x = tf.keras.layers.TimeDistributed(vgg16_model)(frame_input)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py\", line 252, in __call__\r\n    output = super(Layer, self).__call__(inputs, **kwargs)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 575, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/wrappers.py\", line 234, in call\r\n    y = self.layer.call(inputs, **kwargs)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 1920, in call\r\n    output_tensors, _, _ = self._run_internal_graph(inputs, masks)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 2084, in _run_internal_graph\r\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 171, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 835, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 499, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 187, in __call__\r\n    name=self.name)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 631, in conv2d\r\n    data_format=data_format, name=name)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2959, in create_op\r\n    self._add_op(ret)\r\n  File \"/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2599, in _add_op\r\n    \"is already used\" % op.name)\r\nValueError: cannot add op with name block1_conv1/convolution as that name is already used\r\n```\r\n", "comments": ["This is a bug in convolutional layers that was introduced in relation to eager mode; the same issue as [here](https://github.com/tensorflow/tensorflow/issues/13822#issuecomment-338880215). It has already been fixed and the fix will be backported into TF 1.4.", "Awesome, thanks for the quick reply. Does that explain the first issue though? (Perhaps I should've split them into separate reports.)"]}, {"number": 13956, "title": "tf.contrib.layers.flatten has neither \"name\" nor \"reuse\" parameter.", "body": "I think `tf.contrib.layers.flatten` is just a special case of `tf.reshape`. \r\n\r\nHowever, while `tf.reshape` has `name` parameter, `tf.contrib.layers.flatten` doesn't have one. Also neither of them has `reuse` parameter.  \r\n\r\nIs `tf.contrib.layers.flatten` a deprecated API?", "comments": ["Just found the flatten has been added to core layers `tf.layers` recently.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/80ed8afc02b16adb209bffeb551e7f0c435985f6\r\n\r\nI was curious why reshape is not added into core layers as well.\r\n", "@fchollet can you comment or redirect? Thanks.", "There was an attempt to introduce `Reshape` in `tf.layers` at the same time as `Flatten`, but it was not approved; if I recall correctly this was to limit API surface.\r\n\r\nYou could use `tf.keras.layers.Reshape` instead."]}, {"number": 13955, "title": "Fix a typo of \"Jenkins\".", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13954, "title": "make \"smart_cond\" api public and reusable", "body": "Fix #13903 \r\n\r\nMove the implementation of \"smart_cond\" and \"constant_value\" from tensorflow/python/layers/utils to tensorflow/python/ops/control_flow_ops and add corresponding test and expose smart_cond to tensorflow/python/ops/standard_ops.\r\n\r\n@martinwicke should I implement the API in this way or not? As I think ops should be lower level API than layers and smart_cond should be a kind of ops. It's my first time to contribute to TensorFlow. Please feel free to correct me if there's any problem with the design and codes.\r\n\r\nTips, I closed the former PR #13938 and create a new since there's some signature/info problem with that PR.", "comments": ["Can one of the admins verify this patch?", "@netheril96 Good point, I changed the naming schema. By the way @drpngx do you think we should remove the `fn1` `fn2` parameters in `cond` API? They seems redundant.", "Jenkins, test this please.", "The XLA tests are failing because of this:\r\n```\r\nFAIL: //tensorflow/compiler/tests:conv3d_test_gpu (shard 5 of 5) (see /var/lib/jenkins/workspace/tensorflow-pull-requests-xla/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local_linux-py3-opt/testlogs/tensorflow/compiler/tests/conv3d_test_gpu/shard_5_of_5/test.log).\r\nINFO: From Testing //tensorflow/compiler/tests:conv3d_test_gpu (shard 5 of 5):\r\n==================== Test output for //tensorflow/compiler/tests:conv3d_test_gpu (shard 5 of 5):\r\nRunning test tensorflow/compiler/tests/conv3d_test_gpu --test_device=XLA_GPU --types=DT_FLOAT,DT_DOUBLE,DT_INT32,DT_INT64,DT_BOOL,DT_COMPLEX64 on GPU 4\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-xla/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/compiler/tests/conv3d_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/conv3d_test.py\", line 24, in <module>\r\n    from tensorflow.compiler.tests.xla_test import XLATestCase\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-xla/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/compiler/tests/conv3d_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/xla_test.py\", line 35, in <module>\r\n    from tensorflow.python.ops import variables\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-xla/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/compiler/tests/conv3d_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/variables.py\", line 27, in <module>\r\n    from tensorflow.python.ops import control_flow_ops\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-xla/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/compiler/tests/conv3d_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops.py\", line 75, in <module>\r\n    from tensorflow.python.ops import variables\r\nImportError: cannot import name 'variables'\r\n```\r\n\r\nSome missing dependency?", "@drpngx The problem is that `variables` already import `control_flow_ops` and `control_flow_ops` cannot import `variables`. I already resolved it without loop dependencies. And It's my first pull request to TensorFlow and can you please tell me how I can test them? just comment `Jenkins, test this please.` and does it work from my comment?", "Jenkins, test this please.", "Someone from tensorflow needs to trigger the builds, sorry you can't do it yourself.", "Same sycl issue, let's try again.", "Jenkins. test this please.", "@drpngx @martinwicke Seems that it needs to resolve a merge conflict.", "@martinwicke @drpngx Please retest the function again as there ware some merge conflicts before. Thanks", "Jenkins, test this please.", "Seems that docker build failed:\r\n`Build timed out (after 90 minutes). Marking the build as failed.\r\ntensorflow/tools/ci_build/ci_build.sh: line 130: 16143 Terminated docker build -t ${DOCKER_IMG_NAME} -f \"${DOCKERFILE_PATH}\" \"${DOCKER_CONTEXT_PATH}\"\r\nERROR: docker build failed. Dockerfile is at /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/tensorflow/tools/ci_build/Dockerfile.cpu`", "Infra failure. You passed other GPU Py3 tests though, so this is fine to merge. @sb2nov FYI.", "@sguada WDYT of @GeorgyZhou argument?\r\n\r\n@GeorgyZhou please pull rebase and push again", "@sguada please take another look at this.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sync and fix the failing tests", "@GeorgyZhou can you fix the failing test? constant_value is apparently used in contexts which require the current behavior.", "Here's the log:\r\n\r\n  File \"/tmp/botexec/bazel-out/k8-opt/bin/tensorflow/python/keras/backend_test.runfiles/org_tensorflow/tensorflow/python/layers/normalization.py\", line 544, in call\r\n    training_value = utils.constant_value(training)\r\n  File \"/tmp/botexec/bazel-out/k8-opt/bin/tensorflow/python/keras/backend_test.runfiles/org_tensorflow/tensorflow/python/layers/utils.py\", line 220, in constant_value\r\n    return control_flow_ops.smart_constant_value(pred)\r\n  File \"/tmp/botexec/bazel-out/k8-opt/bin/tensorflow/python/keras/backend_test.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops.py\", line 2103, in smart_constant_value\r\n    raise TypeError('`pred` must be a Tensor or a Python bool.')\r\nTypeError: `pred` must be a Tensor or a Python bool.\r\n", "Sure. I will resubmit soon today", "Thanks for lint correction! @martinwicke ", "Anybody can merge it or can review it? @drpngx @martinwicke", "Thanks for the ping!"]}, {"number": 13953, "title": "Tensorboard not displaying event files", "body": "Hi all\r\nFirst time posting, please forgive and correct me if i do anything wrong or stupid.\r\n\r\nI have an issue where I am training a model, but tensorboard isn't displaying any data from the event files. I've done everything in my knowledge. Yes I read the tensorboard ReadMe FAQ. [https://github.com/tensorflow/tensorboard/blob/master/README.md#frequently-asked-questions](url)\r\n\r\nI start the train process with the folowing command (cmd started in folder with the directories \"tf_files\" and \"flower_photos\" int it):\r\n`python retrain.py --bottleneck_dir=tf_files/bottlenecks --model_dir=tf_files/models/\"mobilenet_0.50_224\" --summaries_dir=tf_files/training_summaries/\"mobilenet_0.50_224\" --output_graph=tf_files/retrained_graph.pb --output_labels=tf_files/retrained_labels.txt --architecture=\"mobilenet_0.50_224\" --image_dir=flower_photos`\r\n\r\n![retrain_start_1](https://user-images.githubusercontent.com/9570377/31962587-4074134c-b8fe-11e7-95ab-53b565cfa96e.png)\r\n![retrain_start_2](https://user-images.githubusercontent.com/9570377/31962591-4317b784-b8fe-11e7-94f2-aebbd8bd11ef.png)\r\n![retrain_finish](https://user-images.githubusercontent.com/9570377/31962594-459de2da-b8fe-11e7-8511-76a502fc7763.png)\r\n\r\nThe retraining finishes successfully.\r\n\r\nTensorboard i start with the command:\r\n`tensorboard --logdir i:/temp/learningtf/tf_files/training_summaries`\r\n\r\n![start_tensorboard](https://user-images.githubusercontent.com/9570377/31962693-8ddea9c6-b8fe-11e7-8876-b40bda7349b0.png)\r\n\r\n\r\nThe command above with `--inspect` give me the following output:\r\n\r\n![tensorboard_inspect_1](https://user-images.githubusercontent.com/9570377/31962725-a472b268-b8fe-11e7-948b-75c0adb866b1.png)\r\n![tensorboard_inspect_2](https://user-images.githubusercontent.com/9570377/31962727-a6f84458-b8fe-11e7-9b3a-0f4e41a8ebe0.png)\r\n\r\n\r\nAs you can see, tensorboard doesn't display anything. Im quite confused.\r\n![tensorboard_webinterface](https://user-images.githubusercontent.com/9570377/31962837-faed81b8-b8fe-11e7-8f14-d7407a4b9130.png)\r\n\r\nHere are the event files I'm talking about. If these are not the files you need, post a reply and tell my which files i need to upload, please.\r\n[the_i_hope_right_files.zip](https://github.com/tensorflow/tensorflow/files/1412114/the_i_hope_right_files.zip)\r\n\r\nIf you need eny further Info on this case, please let me know.\r\nThanks for any help in advance.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@droidben I have same problem after upgrading to TF 1.4. I have found the solution is to run tensorboard on the same drive! Previously, I run across drives without any problem.\r\n\r\nMore details has been posted at https://github.com/tensorflow/tensorboard/issues/695", "@ybsave You are genus! "]}, {"number": 13952, "title": "tensorflow + tensorboard on cifs results in PermissionDeniedError", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nUsing stock example script mnist.py from https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: using standard tensorflow/tensorflow:1.1.0-gpu image\r\n- **TensorFlow version (use command below)**: using standard tensorflow/tensorflow:1.1.0-gpu image\r\n- **Python version**: using standard tensorflow/tensorflow:1.1.0-gpu image\r\n- **Bazel version (if compiling from source)**: tensorflow/tensorflow:1.1.0-gpu\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: Tesla K80\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am running stock  mnist.py from https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial on  tensorflow/tensorflow:1.1.0-gpu and running tensorboard in parallel on the same container. Everything works well if logdir is located on local ssd.\r\nIf i am configuring logdir to be on samba directory and using tensorboard during job execution (specifically if i am using Embeddings), the job fails at writing temp checkpoint files (see logs below). If I am not using tensorboard embeddings during job's execution, the job finishes successfully.\r\n\r\n### Source code / logs\r\n2017-10-24 17:39:31.165783: W tensorflow/core/framework/op_kernel.cc:1152] Permission denied: ./model.ckpt-500.index.tempstate18351127508205389812\r\nTraceback (most recent call last):\r\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/external/tb_sample/mnist.py\", line 164, in <module>\r\n    main()\r\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/external/tb_sample/mnist.py\", line 156, in main\r\n    mnist_model(learning_rate, use_two_fc, use_two_conv, hparam)\r\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/external/tb_sample/mnist.py\", line 136, in mnist_model\r\n    saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), i)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1391, in save\r\n    {self.saver_def.filename_tensor_name: checkpoint_file})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.PermissionDeniedError: ./model.ckpt-500.index.tempstate18351127508205389812\r\n         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, conv1/B/_41, conv1/B/Adam/_43, conv1/B/Adam_1/_45, conv1/W/_47, conv1/W/Adam/_49, conv1/W/Adam_1/_51, conv2/B/_53, conv2/B/Adam/_55, conv2/B/Adam_1/_57, conv2/W/_59, conv2/W/Adam/_61, conv2/W/Adam_1/_63, fc1/B/_65, fc1/B/Adam/_67, fc1/B/Adam_1/_69, fc1/W/_71, fc1/W/Adam/_73, fc1/W/Adam_1/_75, fc2/B/_77, fc2/B/Adam/_79, fc2/B/Adam_1/_81, fc2/W/_83, fc2/W/Adam/_85, fc2/W/Adam_1/_87, test_embedding/_89, train/beta1_power/_91, train/beta2_power/_93)]]\r\n\r\nCaused by op u'save/SaveV2', defined at:\r\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/external/tb_sample/mnist.py\", line 164, in <module>\r\n    main()\r\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/external/tb_sample/mnist.py\", line 156, in main\r\n    mnist_model(learning_rate, use_two_fc, use_two_conv, hparam)\r\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/external/tb_sample/mnist.py\", line 114, in mnist_model\r\n    saver = tf.train.Saver()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1086, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 689, in build\r\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 276, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 219, in save_op\r\n    tensors)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 780, in save_v2\r\n    tensors=tensors, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nPermissionDeniedError (see above for traceback): ./model.ckpt-500.index.tempstate18351127508205389812\r\n         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, conv1/B/_41, conv1/B/Adam/_43, conv1/B/Adam_1/_45, conv1/W/_47, conv1/W/Adam/_49, conv1/W/Adam_1/_51, conv2/B/_53, conv2/B/Adam/_55, conv2/B/Adam_1/_57, conv2/W/_59, conv2/W/Adam/_61, conv2/W/Adam_1/_63, fc1/B/_65, fc1/B/Adam/_67, fc1/B/Adam_1/_69, fc1/W/_71, fc1/W/Adam/_73, fc1/W/Adam_1/_75, fc2/B/_77, fc2/B/Adam/_79, fc2/B/Adam_1/_81, fc2/W/_83, fc2/W/Adam/_85, fc2/W/Adam_1/_87, test_embedding/_89, train/beta1_power/_91, train/beta2_power/_93)]]", "comments": ["@dandelionmane @jart should this be filed directly against's dandelion's project?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Do you have any recommendations for now how to avoid those errors or there is no way to store and consume the output from cifs?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing due to inactivity. If it's still a problem let us know and I'll reopen. Might also be worth checking StackOverflow.", "But nobody even tried to answer -)\r\n", "[StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) is the best place to get TensorFlow support. There's a larger community there and better searchability. "]}, {"number": 13951, "title": "help with classifier.predict and predicted_classes", "body": "### System information\r\n- custom code: no, it is the one in https://www.tensorflow.org/get_started/estimator\r\n- system: Apple\r\n- OS: Mac OsX 10.13\r\n- TensorFlow version: 1.3.0\r\n- Python version: 3.6.3\r\n- GPU model: AMD FirePro D700 (actually, two such GPUs)\r\n\r\n\r\n### Describe the problem\r\nDear all,\r\nI am running the simple iris program:\r\nhttps://www.tensorflow.org/get_started/estimator\r\nunder python 3.6.3 and tensorflow 1.3.0.\r\nThe program executes correctly, apart from the very last part, i.e. the one related to the confusion matrix.\r\nIn fact, the result I get for the confusion matrix is:\r\nNew Samples, Class Predictions:    [array([b'1'], dtype=object), array([b'2'], dtype=object)]\r\nrather than the expected output:\r\nNew Samples, Class Predictions:    [1 2]\r\nHas anything about confusion matrix changed in the latest release?\r\nIf so, how should I modify that part of the code?\r\nThank you very much for your help!\r\nBest regards\r\nIvan\r\n\r\n\r\n\r\n### Source code / logs\r\nhttps://www.tensorflow.org/get_started/estimator\r\n", "comments": ["Hi, I have the same issue with:\r\nWindows 10 Pro, TensorFlow version: 1.3.0, Python version: 3.5.2\r\nAny help is welcome. Thanks!", "I think it has just type change.b'1 is equal to 1 and b'2 is eqaul to 2.", "tf.estimator.DNNClassifier returns four predictions: logits, probabilities, class_ids, classes.\r\nclass_id is integer and classes is string"]}, {"number": 13950, "title": "Error in documentation: programmers_guide/variables.md", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: No.\r\n- **TensorFlow installed from (source or binary)**: binary.\r\n- **TensorFlow version (use command below)**:  r1.3\r\n- **Python version**: Irrelevant\r\n- **Bazel version (if compiling from source)**: Irrelevant\r\n- **CUDA/cuDNN version**: Irrelevant\r\n- **GPU model and memory**: Irrelevant\r\n- **Exact command to reproduce**: Irrelevant\r\n\r\n### Indentation is wrong\r\nThe example code snippets from the bottom of chapter **PROGRAMMERS' GUIDE** / **Variables**:\r\n``` python\r\nwith tf.variable_scope(\"model\") as scope:\r\n  output1 = my_image_filter(input1)\r\nwith tf.variable_scope(scope, reuse=True):\r\n  output2 = my_image_filter(input2)\r\n\r\n```\r\nshould be:\r\n``` python\r\nwith tf.variable_scope(\"model\") as scope:\r\n  output1 = my_image_filter(input1)\r\n  with tf.variable_scope(scope, reuse=True):\r\n    output2 = my_image_filter(input2)\r\n\r\n```\r\n", "comments": []}, {"number": 13949, "title": "Merge pull request #1 from tensorflow/master", "body": "Updated on 2017/10/24", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@flysky1991 It looks like there is no change to the codebase. Can you double check your pull request, and also sign the CLA ? Thanks."]}, {"number": 13948, "title": "Dockerfile devel: Fixed broken bazel LICENSE url", "body": "no more .txt extension to the bazel LICENSE file\r\nthis results in HTTP 404 error during docker build", "comments": ["Can one of the admins verify this patch?", "Actually after fixing this bazel LICENSE url the bazel build runs but it issues following error:\r\n```\r\nStep 17/21 : RUN ./configure &&     bazel build -c opt tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl\r\n ---> Running in aee485194515\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nNo GPU support will be enabled for TensorFlow\r\nConfiguration finished\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nExtracting Bazel installation...\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\n____Loading package: tensorflow/tools/pip_package\r\n____Loading...\r\n____Loading package: tensorflow/tensorboard\r\n____Loading package: tensorflow\r\n____Loading package: tensorflow/core\r\n____Loading package: tensorflow/models/embedding\r\n____Loading package: tensorflow/models/image/mnist\r\n____Loading package: tensorflow/python\r\n____Loading package: @bazel_tools//tools/cpp\r\n____Loading package: tensorflow/models/rnn/ptb\r\n____Loading package: tools/defaults\r\n____Loading package: @local_jdk//\r\n____Loading package: tensorflow/tensorboard/backend\r\n____Loading package: tensorflow/tensorboard/lib/python\r\nERROR: /tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_icons//': Error cloning repository: https://github.com/polymerelements/iron-icons.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-icons.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-icons.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-icons.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-icons.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: Loading failed; build aborted.\r\n____Elapsed time: 3.482s\r\nThe command '/bin/sh -c ./configure &&     bazel build -c opt tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl' returned a non-zero code: 1\r\nFAIL: docker build of sebastien/tensorflow:0.9-devel with Dockerfile /tmp/tmp.aR8ZXgxsaB/Dockerfile failed\r\n```\r\nConsidering the Dockerfile.devel of the master branch the Bazel version may have to be upgraded from 0.2.1 to 0.5.0\r\nAnd may be also the `--recursive` option should be removed from the `RUN git clone --recursive https://github.com/tensorflow/tensorflow.git && \\`\r\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Dockerfile.devel & Dockerfile.devel-gpu are not working on r0.9\r\nBut this was too early for a pull request I need to review this properly and see if I can really fix this issue before creating a pull request\r\nmy bad"]}, {"number": 13947, "title": "Tensorflow import fails with Segmentation fault error", "body": "Hello\r\n\r\nI have pip3 installed tensorflow CPU only\r\nPython version:  3.5.2\r\nOS:LSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch\r\nDistributor ID: RedHatEnterpriseServer\r\nDescription:    Red Hat Enterprise Linux Server release 6.8 (Santiago)\r\nRelease:        6.8\r\nCodename:       Santiago\r\n\r\nInstall command: pip3 install tensorflow\r\n\r\necho $LD_LIBRARY_PATH\r\n:/usr/local/lib:/grapps/hadoop/instantclient/instantclient_11_2:/usr/local/lib:/opt/glibc-2.14/lib\r\n\r\n\r\nPython 3.5.2 (default, Mar 23 2017, 07:51:35)\r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-17)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>>\r\n>>> import tensorflow\r\nSegmentation fault\r\n\r\n\r\nAlso tried to import scipy,numpy and matplotlib before importing tensorflow as per suggestion seens in some other cases https://github.com/tensorflow/tensorflow/issues/2034. \r\n\r\nBut nothing solved the issue. \r\n\r\nThanks for the advice \r\n\r\nAbraham \r\n\r\n\r\n\r\n", "comments": ["I'm sorry but we aren't able to provide support for Red Hat Linux. Tagging with community support.", "Please try to test with latest tested builds here https://www.tensorflow.org/install/source#tested_build_configurations, which will resolve the issue. Thank you "]}, {"number": 13946, "title": "Create Model file for Object (face) Recognition in C++", "body": "I am doing research on face Recognition using tensorflow.\r\nCan I please know how how to use the code to create model file for face recognition.\r\n\r\n**Task (required in C++)**\r\nI have 5 folders, each folder has 4 images of a particular person.\r\nI would like to train the images and generate a model file from scratch.\r\nI would then like to use the model file for recognizing one of the 5 persons.\r\n\r\n**Steps followed**\r\n1. https://www.tensorflow.org/tutorials/image_retraining\r\n\r\n**Issues**\r\n> 1. Its in Python , but I wanted in C++\r\n> 2. It is not training the model file from scratch\r\n> \r\n\r\n2.https://github.com/tensorflow/models/tree/master/research/inception/inception\r\n\r\n**Issue Resolved**\r\n> Model can be trained from scratch. \r\n>\r\n**Unresolved Issue**\r\n> Its in Python , but I wanted in C++\r\n> \r\n\r\nPlease help me in creating the model file in C++", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I am sorry to say but haven't received any response in Stackoverflow yet - https://stackoverflow.com/questions/46909787/tensorflow-create-model-file-for-object-face-recognition-in-c"]}, {"number": 13945, "title": "ImportError: cannot import name gen_checkpoint_ops", "body": "### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: via pip\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 2.7.12\r\n- **Exact command to reproduce**: `from tensorflow.contrib.tensorboard.plugins import projector`\r\n\r\n### Describe the problem\r\nBeen following [this tuturial from TF website](https://www.tensorflow.org/versions/r0.12/how_tos/embedding_viz/) to visualize embeddings using Tensorboard. When running the code to add metadata, the following error log returns:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-39-40557054c972> in <module>()\r\n----> 1 from tensorflow.contrib.tensorboard.plugins import projector\r\n      2 \r\n      3 # Use the same LOG_DIR where you stored your checkpoint.\r\n      4 summary_writer = tf.train.SummaryWriter(LOG_DIR)\r\n      5 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py in <module>()\r\n     20 \r\n     21 # Add projects here, they will show up under tf.contrib.\r\n---> 22 from tensorflow.contrib import bayesflow\r\n     23 from tensorflow.contrib import cloud\r\n     24 from tensorflow.contrib import compiler\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/bayesflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=unused-import,line-too-long\r\n---> 24 from tensorflow.contrib.bayesflow.python.ops import csiszar_divergence\r\n     25 from tensorflow.contrib.bayesflow.python.ops import entropy\r\n     26 from tensorflow.contrib.bayesflow.python.ops import monte_carlo\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/bayesflow/python/ops/csiszar_divergence.py in <module>()\r\n     24 # go/tf-wildcard-import\r\n     25 # pylint: disable=wildcard-import\r\n---> 26 from tensorflow.contrib.bayesflow.python.ops.csiszar_divergence_impl import *\r\n     27 # pylint: enable=wildcard-import\r\n     28 from tensorflow.python.util.all_util import remove_undocumented\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/bayesflow/python/ops/csiszar_divergence_impl.py in <module>()\r\n     40 import numpy as np\r\n     41 \r\n---> 42 from tensorflow.contrib import framework as contrib_framework\r\n     43 from tensorflow.contrib.bayesflow.python.ops import monte_carlo_impl as monte_carlo\r\n     44 from tensorflow.python.framework import ops\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/__init__.py in <module>()\r\n     87 # pylint: disable=unused-import,wildcard-import\r\n     88 from tensorflow.contrib.framework.python.framework import *\r\n---> 89 from tensorflow.contrib.framework.python.ops import *\r\n     90 # pylint: enable=unused-import,wildcard-import\r\n     91 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/__init__.py in <module>()\r\n     22 # pylint: disable=wildcard-import\r\n     23 from tensorflow.contrib.framework.python.ops.arg_scope import *\r\n---> 24 from tensorflow.contrib.framework.python.ops.checkpoint_ops import *\r\n     25 from tensorflow.contrib.framework.python.ops.ops import *\r\n     26 from tensorflow.contrib.framework.python.ops.prettyprint_ops import *\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/checkpoint_ops.py in <module>()\r\n     20 import math\r\n     21 \r\n---> 22 from tensorflow.contrib.framework.python.ops import gen_checkpoint_ops\r\n     23 from tensorflow.contrib.util import loader\r\n     24 from tensorflow.python.framework import dtypes\r\n\r\nImportError: cannot import name gen_checkpoint_ops\r\n\r\n```\r\n\r\nThe files gen_checkpoint_ops.py and gen_checkpoint_ops.pyc exist in the specified directory.\r\n\r\nWould appreciate any hint.\r\n", "comments": ["Restarted the session and suddenly it worked :)", "me too get the same error how do you fix it?"]}, {"number": 13944, "title": "How can I use all of cpu cores in android ", "body": "Hi, \r\n     Thank your for your work.\r\n     In my app, I use tensorflow to classify.  But CPU usage are lower, just about 30%. My device has 4 cores.  I want to use all CPUs to reduce cost time. I try to modify intra_op_parallelism_threads and  inter_op_parallelism_threads, when compile the libtensorflow_inference.so.  And use TF_SetConfig to set  TF_SessionOptions.\u3000But it doesn't work. \r\n     Can you give me some advice?  thank you.\r\n ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Hi @xunkaixin . Did you find any solution? I am facing the same problem. "]}, {"number": 13943, "title": "Doubt in definition of state returned by dynamic_rnn for LSTM cell in tensorflow", "body": "### Describe the problem\r\nCan someone tell me whether the state returned by the dynamic_rnn function for a LSTM cell is (h,c) or is (c,h)? c - cell state h - hidden state.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13942, "title": "Support grayscale .bmp images", "body": "The current implementation only supports 3 (RGB) or 4 (ARGB) channel BMP images. However, grayscale images are also often used for machine learning stuff. Therefore it would be great to be able to read 1 channel BMP images.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/decode_bmp_op.cc#L71", "comments": ["@rmlarsen FYI (please feel free to cc anyone else who may be interested)", "Any news?", "I think grayscale bmp is just 1 byte per pixel? Created a PR #14296 for the support.", "Cool, thx. ", "Thanks @yongtang!"]}, {"number": 13941, "title": "add a note on numpy compatibility", "body": "`np.mean` has a `dtype` parameter that could be used to specify the output type. By default this is `dtype=float64`. On the other hand, `tf.reduce_mean` has an aggressive type inference from `input_tensor`. This should be clear in the Numpy compatibility section.\r\nhttps://github.com/tensorflow/tensorflow/issues/13885 ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 13940, "title": "Feature request: tfdbg support for \"Session.partial_run_setup\" and \"Session.partial_run\"", "body": "Hello,\r\n         When I debug tensorflow codes written with \"partial_run_setup\" and \"partial_run\" using tfdbg, it output \"partial_run_setup is not implemented for debug-wrapper sessions\". I want to know how to debug tensorflow codes with \"partial_run_setup\" and \"partial_run\". Thank you. ", "comments": ["@Qilewuqiong tfdbg doesn't support partial runs yet. So currently, to debug partial runs, you can use other debugging tools from tensorflow, including the following\r\n* [tf.Print op](https://www.tensorflow.org/api_docs/python/tf/Print) See use example at: https://www.quora.com/How-does-the-tf-Print-statement-work-for-TensorFlow\r\n* [tf.add_check_numerics_op](https://www.tensorflow.org/api_docs/python/tf/add_check_numerics_ops)", "@caisq Thanks for your help.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."]}, {"number": 13939, "title": "Quantization make graph slower during inference.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: using TF source code (GPU build), can provide docker to reproduce environment conditions if necessary\r\n- **TensorFlow version**: using r1.3 branch, version 1.3.1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.6.1\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: GeForce GTX 1080 Ti, 11170 MB\r\n- **Exact command to reproduce**: \r\n/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph  --in_graph=/quantization/VGG16/frozen_model.pb   --outputs=\"Validation_segmentation/Validation/decoder/Softmax\" --out_graph=/quantization/VGG16/optimized_model.pb   --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"384,1248,3\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes'\r\n\r\n\r\n### Describe the problem\r\nHi, I compressed a graph using transform_graph tool but the resulting graph is actually slower during inference. I am compressing a graph similar to the one presented in this article: https://arxiv.org/pdf/1612.07695.pdf, which has VGG16 as an encoder in input and a classification decoder with a Softmax in output. Inference uses same python script for both graph (original and quantized) and make an average of 100 inferences. Original graph takes ~0.1s for inference, quantized graph takes 70s! If I perform quantization without quantize_nodes, inference takes ~0.3s.\r\n\r\nI understand that this quantization is still in a work in progress and maybe was more aimed at improving inference on mobile devices, but I'm surprised that it is actually so much slower, so that's why I'm logging it as a bug here. (I posted this on stackoverflow but didn't get any answer...)\r\n\r\nThe graph takes ~500Mb, let me know if I should attach it to this ticket (or include an external link?)\r\n\r\n### Source code / logs\r\n[quantization_logs.txt](https://github.com/tensorflow/tensorflow/files/1409825/quantization_logs.txt)\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1409824/tf_env.txt)\r\n[inference.py.txt](https://github.com/tensorflow/tensorflow/files/1409864/inference.py.txt)\r\n\r\n", "comments": ["I have the same problem as this. I have a small graph of 30MB (7.5MB after quantization). On 1080Ti quantization increases inference time from 8ms to 5s. And on DrivePX2 embedded device it increases from 100ms -> 800ms.\r\n\r\nI observe that on both devices, after quantization the graph is processed on the CPU. By inspecting nvidia-smi I can see that the GPU memory is allocated but no processing occurs on it.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It seems library for quantization is gemmlowp and it is C++ library and CPU only.", "Closing this, since I think it's a dupe of #2807. We are still actively working on quantization, but with the goal of deploying it in TF Lite for mobile."]}, {"number": 13938, "title": "make \"smart_cond\" api public and reusable", "body": "Fix #13903 .\r\n\r\nMove the implementation of \"smart_cond\" and \"constant_value\" from `tensorflow/python/layers/utils` to `tensorflow/python/ops/control_flow_ops` and add corresponding test and expose `smart_cond` to `tensorflow/python/ops/standard_ops`.\r\n\r\n@martinwicke should I implement the API in this way or not? As I think ops should be lower level API than layers and `smart_cond` should be a kind of ops. It's my first time to contribute to TensorFlow. Please feel free to correct me if there's any problem with the design and codes.", "comments": ["Can one of the admins verify this patch?", "Oops, seems that my first PR could not be that smooth. It seems that the sanity checks failed at GPG signature verification. ", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "Can one of the admins verify this patch?"]}, {"number": 13937, "title": "Use := instead of ?= to set MAKEFILES_DIR to fix Android build flakiness", "body": "This change should fix Android build flakiness we see with error:\r\nfatal error: google/protobuf/stubs/common.h: No such file or directory", "comments": ["Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 13936, "title": "Standardised caps on Virtualenv.", "body": "Standardised capitalisation of the name Virtualenv.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 13935, "title": "Fix mnist_softmax tutorial: W should be randomly initialized, should \u2026", "body": "Fix mnist_softmax tutorial: W should be randomly initialized, should not be initialized as zeros. Otherwise, you will get a symmetric NN.\r\n```\r\n# This is wrong and misleading. W should not initialized as zeros. \r\n# If you do so, you will get a symmetric network. \r\nW = tf.Variable(tf.zeros([784, 10]))\r\n```\r\nInstead, W should be randomly initialized\r\n```\r\nW = tf.Variable(tf.random_normal([784, 10]))\r\n```\r\nActually, there is another mistake:\r\n```\r\n# This is not how we use mini-batch in practical environment\r\nfor _ in range(1000):\r\n    batch_xs, batch_ys = mnist.train.next_batch(100)\r\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\r\n```\r\nInstead, mini-batch should be used as follows:\r\n```\r\nfor _ in range(1000):\r\n   for batch in range(550):\r\n      batch_xs, batch_ys = mnist.train.next_batch(100)\r\n      sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\r\n```\r\nBut maybe for simplicity, we can ignore this mistake", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@eeandrew this no-hidden-layer network, multinomial or softmax logistic regression, is convex in the parameters, so initialization to zeros is fine. Symmetry-breaking is a problem in networks with hidden layers.", "In terms of not breaking the docs this looks fine to me?  (Though I don't think I'm technically supposed to approve code sample changes.)", "Hi thanks for the PR.\r\n\r\nThis PR would only make sense on `master`, otherwise it would only be fixed in 1.4. \r\n\r\nBut I'm not sure this one is worth pursuing:\r\n\r\n* @JesseLivezey if has a good (if subtle), point. \r\n* We'll be moving and modernizing these examples shortly."]}, {"number": 13934, "title": "Branch 173194115", "body": "", "comments": ["The Android build failure is preexisting and the Linux XLA build failure is unrelated tool failure. Should be okay to merge, @benoitsteiner ", "Ping @benoitsteiner \r\n@tensorflow-jenkins test this please", "@caisq: My apologies, I didn't notice your PR earlier. #13976 is a superset of your PR, isn't it ?", "@benoitsteiner OK. Thanks for the update. I'll close this PR, then."]}, {"number": 13933, "title": "gradient registry has no entry for: FloorMod", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nA small reproducible example has been provided.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nFrom docker gpu image\r\n- **TensorFlow version (use command below)**:\r\n1.3.0\r\nv1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: \r\n3.5.2\r\n- **CUDA/cuDNN version**:\r\n V8.0.61\r\n- **GPU model and memory**:\r\nGeForce GTX 1080, 8GB\r\n\r\n\r\n### Describe the problem\r\nThe mod operation claims to have no gradient defined. When running the below code, I receive these messages:\r\n```\r\nLookupError: gradient registry has no entry for: FloorMod\r\n```\r\nand\r\n```\r\nLookupError: No gradient defined for operation 'mod' (op type: FloorMod)\r\n```\r\n\r\n### Source code / logs\r\nA minimal reproducible example is found here:\r\n```\r\nimport tensorflow as tf\r\n\r\nsess = tf.InteractiveSession()\r\na = tf.placeholder(dtype=tf.float32, shape=[5, 2])\r\n\r\n# b = snt.Linear(output_size=4)(a)\r\nW = tf.Variable(tf.zeros([2, 10]))\r\nb = tf.Variable(tf.zeros([10]))\r\nb = tf.matmul(a, W) + b\r\n\r\nloss = tf.reduce_sum(b) % 2\r\nupdate_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\r\n\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(update_op, {a: [[1, 2], [3, 4]]})\r\n```\r\n\r\nThis results in the following traceback:\r\n```\r\n---------------------------------------------------------------------------\r\nLookupError                               Traceback (most recent call last)\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\r\n    511             try:\r\n--> 512               grad_fn = ops.get_gradient_function(op)\r\n    513             except LookupError:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in get_gradient_function(op)\r\n   1835     op_type = op.type\r\n-> 1836   return _gradient_registry.lookup(op_type)\r\n   1837 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/registry.py in lookup(self, name)\r\n     92       raise LookupError(\r\n---> 93           \"%s registry has no entry for: %s\" % (self._name, name))\r\n\r\nLookupError: gradient registry has no entry for: FloorMod\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nLookupError                               Traceback (most recent call last)\r\n<ipython-input-1-7b9ad04151d6> in <module>()\r\n     10 \r\n     11 loss = tf.reduce_sum(b) % 2\r\n---> 12 update_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\r\n     13 \r\n     14 sess.run(tf.global_variables_initializer())\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    313         aggregation_method=aggregation_method,\r\n    314         colocate_gradients_with_ops=colocate_gradients_with_ops,\r\n--> 315         grad_loss=grad_loss)\r\n    316 \r\n    317     vars_with_grad = [v for g, v in grads_and_vars if g is not None]\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\r\n    384         gate_gradients=(gate_gradients == Optimizer.GATE_OP),\r\n    385         aggregation_method=aggregation_method,\r\n--> 386         colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n    387     if gate_gradients == Optimizer.GATE_GRAPH:\r\n    388       grads = control_flow_ops.tuple(grads)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\r\n    514               raise LookupError(\r\n    515                   \"No gradient defined for operation '%s' (op type: %s)\" %\r\n--> 516                   (op.name, op.type))\r\n    517         if loop_state:\r\n    518           loop_state.EnterGradWhileContext(op, before=False)\r\n\r\nLookupError: No gradient defined for operation 'mod' (op type: FloorMod)\r\n```\r\n\r\n\r\n\r\nAny idea on a work-around? I need to use modulo as part of my loss function.\r\nI use it to convert some coordinates from global space to their relative position with a specific grid-cell (ask me if you want a better explanation - I don't suppose it's particularly relevant though).\r\n\r\n\r\n__Thank you!__", "comments": ["The gradient of FloorMod is 0 almost everywhere, so even if a gradient function were registered I'm not sure it would do what you are hoping. Is your goal to have the gradient be zero, or something else?", "I wasn't intending to use FloorMod, but floating point mod. The example code I gave was probably a little misleading in regards to my desired usage! \r\n\r\nI managed to get around it by writing a little helper function (in case anyone is interested).\r\n\r\n```\r\ndef _modulo(self, x, y):\r\n    '''\r\n    A *mostly* differentiable modulo function! Builds and returns the ops for mod(x, y)\r\n    '''\r\n    divided = x / y\r\n    remainder = tf.round(\r\n        y * (divided - tf.cast(tf.cast(divided, tf.int32), tf.float32))\r\n    )\r\n    return remainder\r\n```", "Closing this out, since I don't think there's anything more to add to this issue."]}, {"number": 13932, "title": "Non-determinism from `tf.data.Dataset.map` with random ops", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes -- please see the minimal reproducible example script below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12, Linux CentOS 7 (4.6.6-300.el7.centos.x86_64)\r\n- **TensorFlow installed from (source or binary)**: `pip3 install tf-nightly` (also happens when built from source)\r\n- **TensorFlow version (use command below)**: v1.3.0-rc1-3690-g9b9cbbe 1.5.0-dev20171023\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: N/A since nightly build reproduces the issue (but when built from source, I use 0.6.1-homebrew)\r\n- **CUDA/cuDNN version**: a GPU is not needed to reproduce the issue (however, it has also been tested with CUDA 8.0.61 / cuDNN 7.0.1)\r\n- **GPU model and memory**: N/A -- a GPU is not needed to reproduce the issue (however, it has also been tested with Tesla K80s)\r\n- **Exact command to reproduce**: See minimal reproducible example below\r\n\r\n### Describe the problem\r\nThe new `tf.data.Dataset` API contains a `map` function with a `num_parallel_calls` parameter, which allows elements to be processed in parallel by multiple threads.  Although not explicitly mentioned in the API docs, prior discussions (such as a comment from [today](https://github.com/tensorflow/tensorflow/issues/13847#issuecomment-338772693)) have indicated that the `map` function should be deterministic (w.r.t. the graph seed) even if `num_parallel_calls > 1`.  I have observed that if the function being mapped contains only non-random ops, then this determinism is observed (see step 2 below).  However, if the the function being mapped contains a random op, the results become non-deterministic for all values of `num_parallel_calls > 1`.  This is unexpected, and prevents training experiments from being reproducible, unless `num_parallel_calls == 1`.  Also, please note that the example below serves as a minimal example to reproduce the issue.  The real scenario involves running data augmentation during training.\r\n\r\n### Source code / logs\r\n1. `pip3 install tf-nightly`\r\n2.  Run the following code to observe that `map` functions with only *non-random* ops are *deterministic* for *all* values of `num_parallel_calls`, which is the *expected* behavior:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef test(threads):\r\n  np.random.seed(42)\r\n  tf.set_random_seed(42)\r\n  images = np.random.rand(100, 64, 64, 3).astype(np.float32)\r\n\r\n  def get_data():\r\n    dataset = tf.data.Dataset.from_tensor_slices(images)  # some initial dataset\r\n    dataset = dataset.map(lambda x: x * 2, num_parallel_calls=threads)  # this works fine always\r\n    dataset = dataset.batch(32)\r\n    x = dataset.make_one_shot_iterator().get_next()\r\n    return x\r\n\r\n  # execution 1\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch1 = sess.run(x)\r\n\r\n  # clear out everything\r\n  tf.reset_default_graph()\r\n\r\n  # execution 2\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch2 = sess.run(x)\r\n\r\n  # results should be equivalent\r\n  assert np.allclose(x_batch1, x_batch2)\r\n\r\ntest(1)  # works with 1 thread!\r\ntest(15)  # works with >1 threads!\r\n```\r\n\r\n3. Run the following code to observe that `map` functions with *random* ops are deterministic if `num_parallel_calls == 1`, but are *non-deterministic* for values of `num_parallel_calls > 1`, which seems to me to be an *unexpected* behavior:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef test(threads):\r\n  np.random.seed(42)\r\n  tf.set_random_seed(42)\r\n  images = np.random.rand(100, 64, 64, 3).astype(np.float32)\r\n\r\n  def get_data():\r\n    dataset = tf.data.Dataset.from_tensor_slices(images)  # some initial dataset\r\n    # ONLY DIFFERENCE IS THE BELOW LINE:\r\n    dataset = dataset.map(lambda image: tf.image.random_hue(image, 0.04, seed=42), num_parallel_calls=threads)\r\n    # ONLY DIFFERENCE IS THE ABOVE LINE ^^^:\r\n    dataset = dataset.batch(32)\r\n    x = dataset.make_one_shot_iterator().get_next()\r\n    return x\r\n\r\n  # execution 1\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch1 = sess.run(x)\r\n\r\n  # clear out everything\r\n  tf.reset_default_graph()\r\n\r\n  # execution 2\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch2 = sess.run(x)\r\n\r\n  # results should be equivalent\r\n  assert np.allclose(x_batch1, x_batch2)\r\n\r\ntest(1)  # works with 1 thread!\r\ntest(15)  # fails with >1 threads!\r\n```\r\n\r\n4. Observe that swapping out the `map` line above with an entirely different random op such as `dataset = dataset.map(lambda x: x * tf.random_normal([64, 64, 3], seed=42), num_parallel_calls=threads)` is also *non-deterministic* for values of `num_parallel_calls > 1`.", "comments": ["@mrry ", "Additionally, I would like to note that for steps 3 and 4, an op-level seed *must* be set on the random ops used within the map function, regardless of whether or not a graph-level seed is set.  This appears to be an inconsistent behavior with that of the documentation for [`tf.set_random_seed()`](https://www.tensorflow.org/api_docs/python/tf/set_random_seed):\r\n\r\n> 2. If the graph-level seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the graph-level seed so that it gets a unique random sequence.", "I'm not familiar with tensorflow codes, but I tried to trace this. Looks like if we can't assign the exact thread in thread pool to run for each input element, we can't make sure the parallel map functions with random ops are deterministic. However, assigning thread sounds counterintuitive to the nature of thread pool.", "Unfortunately, this is \"expected behavior\" due to the way `tf.random_uniform()` (used inside `tf.image.random_hue()`) and the other RNG ops are implemented. The parallel invocations of map will race to access the mutable RNG state inside the op, and different invocations will see a non-deterministically chosen element of the same sequence. Currently, the only way to ensure deterministic results from `Dataset.map()` that contains an RNG op is to set `num_parallel_calls=1`.\r\n\r\nIn principle, you could slice your `map()` function so that the random number generation in a serial fashion, and the compute-intensive part of the op in a parallel map. For example, it's possible to do this manually for `tf.image.random_hue()`, because it is simply a composition of `tf.adjust_hue(..., tf.random_uniform(...))`:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef test(threads):\r\n  np.random.seed(42)\r\n  tf.set_random_seed(42)\r\n  images = np.random.rand(100, 64, 64, 3).astype(np.float32)\r\n\r\n  def get_data():\r\n    dataset = tf.data.Dataset.from_tensor_slices(images)\r\n    # Perform the random number generation in a single-threaded map().\r\n    dataset = dataset.map(\r\n        lambda image: (image, tf.random_uniform([], -0.04, 0.04, seed=42)),\r\n        num_parallel_calls=1)\r\n    # Perform the compute-intensive hue adjustment in a multi-threaded map().\r\n    dataset = dataset.map(\r\n        lambda image, adjustment: tf.image.adjust_hue(image, adjustment),\r\n        num_parallel_calls=threads)\r\n    dataset = dataset.batch(32)\r\n    x = dataset.make_one_shot_iterator().get_next()\r\n    return x\r\n\r\n  # execution 1\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch1 = sess.run(x)\r\n\r\n  # clear out everything\r\n  tf.reset_default_graph()\r\n\r\n  # execution 2\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch2 = sess.run(x)\r\n\r\n  # results should be equivalent\r\n  assert np.allclose(x_batch1, x_batch2)\r\n\r\ntest(1)  # works with 1 thread!\r\ntest(15)  # works with >1 threads!\r\n```\r\n\r\nHowever, this manual approach might not scale to a real program. In our CNN benchmarks, we've been using a sequence number to deterministically map \"random\" perturbations onto input images. In future we might consider doing this kind of slicing automatically, but that's probably some way off.\r\n\r\nHope this helps though!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Hi @mrry ,\r\n\r\nI just stumbled on this behaviour.\r\nI wanted to understand whether this was something that could (and would) be fixed in the future?\r\n\r\nIf not, I think it would be nice to have a warning in the docs, especially since there is a `deterministic` keyword in the [docs of `map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map). I could submit a PR for that if needed.", "@zaccharieramzi @mrry \r\nI encountered this nondeterminism in `map` as well, when it is used with the random augmentation function like `tf.image.random_brightness` and `num_parallel_calls` > 1.\r\n\r\nI tried setting `deterministic = True` but it didn't work.\r\n\r\nBy the way, I've already called a function for deterministic results as below.\r\n```python\r\ndef seed_everything(seed_value):\r\n    tf.random.set_seed(seed_value)\r\n    os.environ['TF_DETERMINISTIC_OPS'] = '1' \r\n```\r\n\r\nAnd the tf version I'm using is tf-nightly-gpu `2.5.0-dev20201130`.", "@xfffrank,\r\n\r\nThe [work-around](https://github.com/tensorflow/tensorflow/issues/13932#issuecomment-341263301) suggested by @mrry can be extended using the stateless random image ops. For example, an early stage in your `tf.data.Dataloader` pipeline could append a (deterministic) random seed to each example using a single-threaded (num_parallel_calls=1) `map`. Then, any subsequent stateless random image op in a parallel stage (num_parallel_calls > 1) could use the seed associated with the example. This would require you replacing `tf.image.random_brightness` with `tf.image.stateless_random_brightness` in your example.\r\n\r\nThe advantage of using the relatively newly added stateless random image ops in this way is that you only have to inject one random number per-example into the pipeline and that one random number can be used for all the stateless random image ops (as the op's `seed` parameter)."]}, {"number": 13931, "title": "TF 1.4 (CUDA 9/CUDNN7) Chrome Tracing Timeline broken", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ArchLinux Latest\r\n- **TensorFlow installed from (source or binary)**:Source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.7\r\n- **CUDA/cuDNN version**: 9 /7\r\n- **GPU model and memory**: 2* Nvidia1080ti (11GB)\r\n- **Exact command to reproduce**: \r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\ntraining_loss_, _ = sess.run(op_list,                                          \r\n      feed_dict=feed_dict,\r\n      options=run_options,run_metadata=run_metadata)\r\ntl=timeline.Timeline(run_metadata.step_stats)\r\nchrome_trace = tl.generate_chrome_trace_format()\r\nwith open(filename, 'w') as f:\r\n              f.write(chrome_trace)\r\n\r\n\r\n### Describe the problem\r\nAfter switching to TF 1.4 compiled with CUDA 9 /CUDNN7 from TF 1.3 with CUDA 8 and CUDNN 6, chrome trace timeline stopped working, otherwise the code runs as before. See the attached screenshots (TF1.4 / TF1.3).\r\n\r\n### Source code / logs\r\n![image](https://user-images.githubusercontent.com/24719485/31917426-7a1b3828-b84f-11e7-8352-b82088b353bb.png)\r\n![image](https://user-images.githubusercontent.com/24719485/31917461-9f9dd3d0-b84f-11e7-874d-c9c6dadf3bc8.png)\r\n", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @prb12: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @prb12: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @prb12: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @prb12: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 13930, "title": "Build failure CUDA8 / cudNN8-v7", "body": "### System information\r\n- **Fedora 26 x64**:\r\n- **TensorFlow installed from source**:\r\n- **TensorFlow version commit 53e7541cf7efa61ba22c9f042e07031d87c8f145 (oct 23 11:46)**:\r\n- **Python version 3.6**: \r\n- **Bazel version 0.5.4**:\r\n- **CUDA 8.0**\r\n- **cuDNN 8.0 v7**:\r\n- **GPU model GTX 1060**:\r\n- **bazel build -c opt --copt=-march=native --config=cuda //tensorflow/tools/pip_package:build_pip_package**:\r\n\r\nHave tried `bazel clean`.\r\n```\r\n.....\r\nexternal/protobuf_archive/python/google/protobuf/pyext/message.cc: In instantiation of 'bool google::protobuf::python::CheckAndGetInteger(PyObject*, T*) [with T = long unsigned int; PyObject = _object]':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/message.cc:698:60:   required from here\r\nexternal/protobuf_archive/python/google/protobuf/pyext/message.cc:635:20: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nINFO: From Compiling tensorflow/core/lib/strings/numbers.cc:\r\ntensorflow/core/lib/strings/numbers.cc: In function 'std::__cxx11::string tensorflow::strings::HumanReadableNumBytes(tensorflow::int64)':\r\ntensorflow/core/lib/strings/numbers.cc:424:8: warning: '%lld' directive output may be truncated writing between 1 and 19 bytes into a region of size between 7 and 8 [-Wformat-truncation=]\r\n string HumanReadableNumBytes(int64 num_bytes) {\r\n        ^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/lib/strings/numbers.cc:424:8: note: directive argument in the range [0, 9223372036854775807]\r\nIn file included from /usr/include/stdio.h:939:0,\r\n                 from /usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/cstdio:42,\r\n                 from /usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/ext/string_conversions.h:43,\r\n                 from /usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/basic_string.h:6347,\r\n                 from /usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/string:52,\r\n                 from ./tensorflow/core/lib/strings/numbers.h:19,\r\n                 from tensorflow/core/lib/strings/numbers.cc:15:\r\n/usr/include/bits/stdio2.h:65:44: note: '__builtin_snprintf' output between 3 and 22 bytes into a destination of size 8\r\n        __bos (__s), __fmt, __va_arg_pack ());\r\n                                            ^\r\nINFO: From Compiling external/nccl_archive/src/all_gather.cu.cc:\r\n/usr/local/cuda-8.0/bin/../targets/x86_64-linux/include/math_functions.h(8897): error: cannot overload functions distinguished by return type alone\r\n\r\n/usr/local/cuda-8.0/bin/../targets/x86_64-linux/include/math_functions.h(8901): error: cannot overload functions distinguished by return type alone\r\n\r\n2 errors detected in the compilation of \"/tmp/tmpxft_00006246_00000000-7_all_gather.cu.cpp1.ii\".\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/nccl_archive/BUILD:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_gather.cu.pic.o' was not created.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/nccl_archive/BUILD:33:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 186.358s, Critical Path: 25.29s\r\n```", "comments": ["Can you try updating to the latest version of bazel? That's sometimes the issue. If that doesn't help I'll try to repro.", "First pulled 5487545cfba94bff67f494a038e4314518982b45, then upgraded to bazel 0.7.0. After both steps, the build error was the following: \r\n```\r\nINFO: From Compiling external/nccl_archive/src/libwrap.cu.cc:\r\n/usr/local/cuda-8.0/bin/../targets/x86_64-linux/include/math_functions.h(8897): error: cannot overload functions distinguished by return type alone\r\n\r\n/usr/local/cuda-8.0/bin/../targets/x86_64-linux/include/math_functions.h(8901): error: cannot overload functions distinguished by return type alone\r\n\r\n2 errors detected in the compilation of \"/tmp/tmpxft_00005318_00000000-7_libwrap.cu.cpp1.ii\".\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/nccl_archive/BUILD:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/libwrap.cu.pic.o' was not created.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/nccl_archive/BUILD:33:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 136.580s, Critical Path: 21.28s\r\n```\r\nThe error has changed a bit from above", "Fedora 26 comes with GCC 7, but CUDA 8 only supports gcc5 (and CUDA 9 supports gcc6).\r\nI had installed CUDA 8 successfully some time ago, but forgotten the above requirement. \r\n\r\nTo install GCC 5 on Fedora follow these instructions (scroll down the page): https://jasonqsy.github.io/compile-tensorflow-with-gpu-support-on-fedora.html\r\nThe instructions are probably applicable for other distros or CUDA9 and gcc6 as well though I haven't tried it.\r\n\r\nThen, when running `.configure`, remember to specify the location for gcc5.4. Doing that solved the above compilation errors for me.\r\n\r\nIt might be a good idea to add GCC Version to the issue template, or even check that the gcc path provided is compatible with the specified CUDA version when configuring the tensorflow installation.", "Thank you for the detailed explanation and instructions!", "@gunan FYI (see @tsoernes last comment)", "@yifeif Could you add GCC/Compiler into our new issue template?", "Sure, we can add it to the template file, and to the checker. cc @aselle FYI. Will send a PR first."]}, {"number": 13929, "title": "Add links and fix typos", "body": "Add links to the docs in GitHub, to make it easier for contributors to find them. Also fix some typos in the names of GitHub and TensorFlow, and standardise capitalisation in headings.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}]