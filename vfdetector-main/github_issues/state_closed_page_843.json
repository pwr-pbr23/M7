[{"number": 28230, "title": "[tf.keras.layers.ReLU] Layer returns 0 if threshold is negative and max_value is set", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.2 LTS**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version (use command below): **1.12.2**\r\n- Python version: **3.6.7**\r\n- Bazel version (if compiling from source): **0.17.2**\r\n- GCC/Compiler version (if compiling from source): **gcc version 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04)** \r\n- CUDA/cuDNN version:  **no gpu**\r\n- GPU model and memory: **no gpu**\r\n\r\n**The expected behavior**\r\nAccording to the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU#class_relu) of _tf.keras.layers.ReLU_ (formatted for readability): \r\n\r\n> With default values, it returns element-wise max(x, 0).\r\n> \r\n> Otherwise, it follows: \r\n> f(x) = max_value for x >= max_value\r\n> f(x) = x for threshold <= x < max_value\r\n> f(x) = negative_slope * (x - threshold) otherwise\r\n\r\n**Current behavior**\r\n_tf.keras.layers.ReLU()_ returns 0 when:\r\n\r\n1. threshold is set to xt < 0\r\n2. max_value is set\r\n3. the layer is fed with a value between xt and 0\r\n\r\nFor example, evaluating:\r\n```\r\nt1 = tf.Variable(-0.5, dtype=tf.float32)\r\ntf.keras.layers.ReLU(max_value=1.0, threshold=-1.0)(t1)\r\n```\r\n\r\nresults in 0, although I think, it should return -0.5.\r\n\r\n**Code to reproduce the issue**\r\n\r\nYou can run the following script to see the problem (you'll need to install matplotlib).\r\n`test_relu`  computes ReLU values in the manner described in the documentation.\r\n\r\n```\r\nfrom matplotlib import pyplot as plt\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\nsession = tf.Session()\r\nxs = np.linspace(-3, 3, 100)\r\n\r\nt1 = tf.Variable(0, dtype=tf.float32)\r\ntf_ys = [session.run(tf.keras.layers.ReLU(max_value=1.0, threshold=-1.0)(t1), feed_dict={t1: x})\r\n         for x in xs]\r\n\r\n\r\ndef test_relu(x, max_value, threshold, negative_slope):\r\n    if x >= max_value:\r\n        return max_value\r\n    elif threshold <= x and x < max_value:\r\n        return x\r\n    else:\r\n        return negative_slope * (x - threshold)\r\n\r\n\r\ntest_ys = [test_relu(x, 1, -1, 0) for x in xs]\r\nplt.subplot(2, 1, 1)\r\nplt.plot(xs, tf_ys)\r\nplt.ylabel('tf.keras.layers.ReLU')\r\nplt.subplot(2, 1, 2)\r\nplt.plot(xs, test_ys)\r\nplt.ylabel('test_relu')\r\nplt.xlabel('x')\r\nplt.show()\r\n```\r\nThe script shows the following plots. You can see the difference between expected _test_relu_ and actual ReLU values.\r\n![relu](https://user-images.githubusercontent.com/9303779/56865174-2fa30080-69cb-11e9-9d0e-b6b49e3b0530.png)\r\n\r\nI guess, the problem might be caused by [this line](https://github.com/tensorflow/tensorflow/blob/6b634657d8ff1355132c3838271e4f569d1ffaba/tensorflow/python/keras/backend.py#L3491) of TensorFlow source code, where output is clipped to 0 regardless of the threshold.", "comments": ["@pwws When I executed the below code, it returned the output as -0.5 as expected.  Please let me know if you still have any  concern.\r\n```\r\nlayer = tf.keras.layers.ReLU(threshold= -1.0)\r\noutput = layer([-0.5, -1.0, 1.0,6.0])\r\nlist(output.numpy())\r\n\r\n```\r\n", "Hey, @saikumarchalla, honestly I haven't used TensorFlow for a while, so no - I have no concerns - and am glad the problem has been fixed :+1: ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28230\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28230\">No</a>\n", "Hi there, @saikumarchalla.\r\nThis issue does not seem to be resolved. When max_value is set, the threshold parameter for _negative_ values does not work.\r\n\r\nlayer = tf.keras.layers.ReLU(threshold= -1, max_value=5)\r\noutput = layer([-0.5, -1.0, 1.0, 6.0])\r\nlist(output.numpy())\r\n\r\ngive:\r\n[0.0, -0.0, 1.0, 5.0]\r\n\r\nexpected output:\r\n[-0.5, -0.0, 1.0, 5.0]"]}, {"number": 28229, "title": "tf.batch_gather with higher dimensional indices for one batch", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.batch_gather` is currently able to handle an arbitrary number of batch dimensions, but there is only one dimension of the indices of a single batch. I needed higher dimensional indices for one batch for my purposes, so I extended the `tf.batch_gather` function (see https://github.com/VincentStimper/tf-batch-gather-extension).\r\n\r\n**Will this change the current api? How?**\r\nOne would need to pass an axis to `tf.batch_gather` since this cannot be inferred form the dimensions of `params` and `indices`, but axis could be `None` by default causing the function to act as originally implemented.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone with specific requirements to gather elements from tensors by indices.\r\n\r\n\r\n", "comments": ["So you want something like batch_dims in gather_nd? Look at [the docs for gather_nd](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L3527) on the tree and see if it solves your problem.", "No, in `tf.gather_nd` you slice multiple axis of the `params` tensor, but I want to slice only along one axis, but with a higher dimensional index to get a higher dimensional result. This can be done in the `tf.gather` function, which is unfortunatly not able to deal with batch axes. To understand what I mean have a look at my repo https://github.com/VincentStimper/tf-batch-gather-extension where I implemented an solution, please.", "Yeah, it's very useful, thanks! @VincentStimper ", "There is tf.batch_gather / tf.gather(..., batch_dims=...). I think those should solve your feature request.", "Sorry, I meant gather_nd with batch_dimensions."]}, {"number": 28228, "title": "Documentation for C APIs", "body": "Is there any detailed documentation for C APIs besides  [version example](https://www.tensorflow.org/install/lang_c#build) and [c_api.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h)? For example, creating a session and tensors, running queries, get tensor result, etc.\r\n\r\n\r\n", "comments": ["Look at the [tests](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api_test.cc) for usage examples, they are pretty thorough."]}, {"number": 28227, "title": "Self-train MobileNet on ImageNet", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): latest\r\n- Are you willing to contribute it (Yes/No): Yes if I can\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nHi! Thank you very much for these wonderful libraries!\r\nI want to have a mobilenet with alpha=0.125 (or possibly smaller) to make it even more light-weight. Since there is no pretrained models (only with alpha=0.25 and so on), I want to first pretrain it on mobilenet, and then use it for fine-tuning. Therefore, I would appreciate it if I could be given the code you use to pretrain it on mobilenet. Thank you very much!\r\n\r\n**Will this change the current api? How?**\r\nNO\r\n\r\n**Who will benefit with this feature?**\r\neveryone who wants to train on ImageNet by themselves\r\n\r\n**Any Other info.**\r\nit seems that to achieve this, it is only needed to open source some simple scripts :)\r\n\r\nThanks so much!\r\n\r\n", "comments": ["Hi apologies for the delay in getting to this, we have released the scripts we used to make the pretrained models here: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1_train.py\r\nand \r\nhttps://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1_eval.py", "WOW Thank you so much!"]}, {"number": 28226, "title": "InvalidArgumentError: logits and labels must have the same first dimension", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): 1.13\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nI have created a dataset and I am trying to feed it to my model. Here is the shape:\r\n`<DatasetV1Adapter shapes: ((1, ?, ?, ?), (1, 50)), types: (tf.uint8, tf.float32)>`\r\n\r\nI constructed it pretty simply from a pandas.DataFrame:\r\n```python\r\nfilenames_ds = tf.data.Dataset.from_tensor_slices(categ_img[:1000]['image_name'])\r\nlabels_ds    = tf.data.Dataset.from_tensor_slices(categ_img[:1000]['category_label'])\r\nimages_ds    = filenames_ds.map(lambda x: tf.image.decode_jpeg(tf.read_file(x)))\r\nlabels_ds    = labels_ds.map(lambda x: tf.one_hot(x, NUM_CATEGORIES))\r\nds = tf.data.Dataset.zip((images_ds, labels_ds))\r\nds = ds.apply(tf.contrib.data.batch_and_drop_remainder(1))\r\n```\r\n\r\nI am feeding it to a ResNet-based network. However, I keep getting this error:\r\n```\r\nInvalidArgumentError: logits and labels must have the same first dimension, got logits shape [1,50] and labels shape [50]\r\n```\r\n\r\nI also tried to create the dataset slightly differently (this line replaces the old labels_ds line):\r\n```python\r\nlabels_ds    = labels_ds.map(lambda x: tf.expand_dims(tf.one_hot(x, NUM_CATEGORIES), axis=0))\r\n```\r\n\r\nSo the shape is now:\r\n`<DatasetV1Adapter shapes: ((1, ?, ?, ?), (1, 1, 50)), types: (tf.uint8, tf.float32)>`\r\n\r\nBut the error is exactly the same. Nothing changed.\r\n\r\nI don't know what to do exactly. Here is the model, in case it matters:\r\n```python\r\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(None, None, 3))\r\nfor layer in base_model.layers:\r\n    layer.trainable = False\r\nx = base_model.output\r\nx = GlobalAveragePooling2D()(x)\r\n    \r\nfor fc in FC_LAYERS:\r\n    x = Dense(fc, activation='relu')(x)\r\n    x = Dropout(DROPOUT)(x)\r\n    \r\noutput    = Dense(NUM_CATEGORIES, activation='softmax', name='fully-connected')(x)\r\nmodel     = Model(inputs=base_model.input, outputs=output)\r\noptimizer = tf.keras.optimizers.SGD(lr=LEARNING_RATE)\r\ncce       = tf.keras.losses.CategoricalCrossentropy()\r\n    \r\nmodel.compile(optimizer, loss=cce)\r\nreturn model\r\n```\r\n\r\nI already asked on [StackOverflow](https://stackoverflow.com/questions/55855543/invalidargumenterror-logits-and-labels-must-have-the-same-first-dimension?noredirect=1#comment98380254_55855543) but no answer.\r\n\r\nCan someone tell me if I am doing something wrong (very likely) or if this is a bug please?\r\n\r\nThank you for your help.\r\nThis is my first Github issue, so do not hesitate to tell me if I need to improve, adapt, change something.\r\n\r\nThanks guys.", "comments": ["@Bornlex Please refer the links [1](https://stackoverflow.com/questions/34519423/tensorflow-error-logits-and-labels-must-be-same-size), [2](https://stackoverflow.com/questions/49161174/tensorflow-logits-and-labels-must-have-the-same-first-dimension). This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n\r\n", "Of course. Thank you for your answer.\r\n\r\nI found out that I might have missed something.\r\nThe following line made things work:\r\n\r\n```python\r\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\r\n```\r\n\r\nI have no idea why though. Thank you anyway."]}, {"number": 28225, "title": "matmul gives different results for tensor based on shape", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro N, 64-bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: No GPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nGiven a tensor X with shape (1, 1, 256), matmul gives slightly different output for matmul(X, W) and matmul(tf.tile(X, [1, 2, 1]), W)\r\n\r\n**Describe the expected behavior**\r\n\r\nThe 60 elements of the output of matmul(X, W) should match exactly the first 60 outputs of matmul(tf.tile(X, [1, 2, 1]), W)\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport platform\r\n\r\nprint(\"Python version\", platform.python_version())\r\nprint(\"Tensorflow version\", tf.__version__)\r\nprint(\"Numpy version\", np.version.version)\r\n\r\nX = tf.random.normal((1, 1, 256), dtype = tf.float32, stddev = 0.0001, seed = 1)\r\nW = tf.random.normal((1, 256, 60), dtype = tf.float32, stddev = 0.0001, seed = 1)\r\n\r\ns = tf.Session()\r\na = tf.matmul(X, W)\r\nb = tf.matmul(tf.tile(X, [1, 2, 1]), W)\r\na, b = s.run([a, b])\r\nprint(a[0][0][0], b[0][0][0])\r\n```\r\n\r\nFor me, the output is:\r\n\r\nPython version 3.6.8\r\nTensorflow version 1.13.1\r\nNumpy version 1.16.2\r\n2019-04-28 12:27:37.641095: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n5.1921496e-08 5.1921518e-08\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI understand that floating point approximation might play a role here but should the output depend on the shape like this?\r\n", "comments": ["I can reproduce this with Tensorflow 2.0.0 and `dtype=tf.float64`\r\nChoose X with shape (a,b) and W with shape (b, c).\r\nNow, I reduce the c dimension of the W tensor using `tf.boolean_mask` to filter out some columns. The results then differ by up to 1e-14.\r\nInterestingly, this happens only to some values but not to others. \r\nWhile I can imagine it has to do with some precision problems, there are two reasons it shouldn't happen:\r\n- X is a design matrix in my case, thus containing only the values 0 and 1 for which I don't expect such differences when performing `matmul` operations.\r\n- As @fast-reflexes pointed out, the shape shouldn't make any difference.\r\n\r\nAny ideas how to solve this?", "@fast-reflexes Is this still an issue for you? I tried running your code in `TF1.x` as well as  in `TF2.x`. Results are not exact but very close (till 13th or 14th decimal if dtype is `tf.float32`) and the results are even closer if dtype is `tf.float64`. \r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/92196f377c28f50d277baa947070ae68/untitled.ipynb) with `TF1.x` and [gist here](https://colab.research.google.com/gist/jvishnuvardhan/69da3900897f72df79272cf088e959ca/untitled996.ipynb) with `TF2.5`. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28225\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28225\">No</a>\n"]}, {"number": 28224, "title": "[tflite] tflite file with single ADD op produces duplicated outputs", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n\r\n\r\n- TensorFlow version (use command below): r1.13\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI have created `.tflite` with single `ADD` op. It has two inputs and one output.\r\nWhen reading this `.tflite` with interpreter(e.g. `tensorflow.lite.python`)\r\n\r\n```py\r\nimport sys\r\n\r\nimport numpy as np\r\n\r\nfrom tensorflow.lite.python import interpreter as interpreter_wrapper\r\n\r\ninterpreter = interpreter_wrapper.Interpreter(model_path=sys.argv[1])\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nprint(input_details)\r\nprint(output_details)\r\n```\r\n\r\n```\r\n[{'name': 'input0', 'index': 0, 'shape': array([2, 5], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'input1', 'index': 1, 'shape': array([2, 5], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n[{'name': 'output0', 'index': 2, 'shape': array([2, 5], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'output0', 'index': 2, 'shape': array([2, 5], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n```\r\n\r\nCode using  C++ interpreter also reports duplicated outputs(2 2), even though outout of ADD(builtin code 0) shows one output.\r\n\r\n```\r\nInterpreter has 3 tensors and 1 nodes\r\nInputs: 0 1\r\nOutputs: 2 2\r\n\r\nTensor   0 input0               kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  2 5\r\nTensor   1 input1               kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  2 5\r\nTensor   2 output0              kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  2 5\r\n\r\nNode   0 Operator Builtin Code   0\r\n  Inputs: 0 1\r\n  Outputs: 2\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n`get_output_details()` returns unique list of outputs.\r\n\r\n**Code to reproduce the issue**\r\n\r\nUse attached `.tflite` file to reproduce the issue.\r\n\r\n[add.tflite.zip](https://github.com/tensorflow/tensorflow/files/3124805/add.tflite.zip)\r\n\r\n\r\n", "comments": ["If you load your .tflite model with Netron(https://lutzroeder.github.io/netron/), it will show exactly two outputs. I'm wondering how you build & convert this model. Could you share the code for the model and the command you used to convert the model?", "@haozha111 I have created `.tflite` using python binding(generated from `r1.14` schema.fbs) with our own converter.\r\n\r\nHere is a code snippet for serializing `ADD` op.\r\n\r\n``` \r\ndef SerializeOpAdd(serializer, x_id, y_id, output_id):\r\n\r\n    opcode_id = serializer.RegisterBuiltinOpcode(\r\n        tflite.BuiltinOperator.BuiltinOperator.ADD)\r\n\r\n    # Inputs\r\n    num_inputs = 2\r\n    tflite.Operator.OperatorStartInputsVector(serializer.builder, num_inputs)\r\n    serializer.builder.PrependInt32(y_id)\r\n    serializer.builder.PrependInt32(x_id)\r\n    inputs = serializer.builder.EndVector(num_inputs)\r\n\r\n    # Outputs\r\n    num_outputs = 1\r\n    tflite.Operator.OperatorStartOutputsVector(serializer.builder, num_outputs)\r\n    serializer.builder.PrependInt32(output_id)\r\n    outputs = serializer.builder.EndVector(num_outputs)\r\n\r\n    # Options\r\n    activation_function = 0  # 'NONE'\r\n    tflite.AddOptions.AddOptionsStart(serializer.builder)\r\n    tflite.AddOptions.AddOptionsAddFusedActivationFunction(\r\n        serializer.builder, activation_function)\r\n    tf_options = tflite.AddOptions.AddOptionsEnd(serializer.builder)\r\n\r\n    tflite.Operator.OperatorStart(serializer.builder)\r\n    tflite.Operator.OperatorAddInputs(serializer.builder, inputs)\r\n    tflite.Operator.OperatorAddOutputs(serializer.builder, outputs)\r\n    tflite.Operator.OperatorAddBuiltinOptionsType(\r\n        serializer.builder, tflite.BuiltinOptions.BuiltinOptions.AddOptions)\r\n    tflite.Operator.OperatorAddBuiltinOptions(serializer.builder, tf_options)\r\n    tflite.Operator.OperatorAddOpcodeIndex(serializer.builder, opcode_id)\r\n    op = tflite.Operator.OperatorEnd(serializer.builder)\r\n\r\n    serializer.operators.append(op)\r\n\r\n    return op\r\n``` \r\n\r\nWhole part of our converter is W.I.P. and difficult to cut out for creating minimal & reproducing the issue atm. ", "May I know why do you need to create a converter yourself and not using the TOCO? Given that the graph visualization of your model has two outputs, I wonder if it's a bug in your converter implementation?", "@haozha111 Ah, it was a bug in my converter. Sorry for disturbing you.\r\n\r\nFYI, there was a mismatch in writing out output vector in my serializer. And it looks tflite interpreter treat it as a valid `.tflite` file. \r\n\r\n```\r\n        # len(inputs) = 2, len(outputs) = 1  \r\n        # [Inputs]\r\n        tflite.SubGraph.SubGraphStartInputsVector(self.builder, len(inputs))\r\n        for i in reversed(inputs):\r\n            self.builder.PrependInt32(i)\r\n        tf_inputs = self.builder.EndVector(len(inputs))\r\n\r\n        # [Outputs]\r\n        tflite.SubGraph.SubGraphStartOutputsVector(self.builder, len(outputs))\r\n        for o in reversed(outputs):\r\n            self.builder.PrependInt32(o)\r\n        tf_outputs = self.builder.EndVector(len(inputs))  # <-----\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28224\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28224\">No</a>\n"]}, {"number": 28223, "title": "Import Error while building on Windows with cudnn64_7.dll", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows10 1809\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N?A\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:r1.14\r\n- Python version:3.6.8\r\n- Installed using virtualenv? pip? conda?:No\r\n- Bazel version (if compiling from source):0.24.1\r\n- GCC/Compiler version (if compiling from source):visual studio 2015\r\n- CUDA/cuDNN version:10.1/7.5.1\r\n- GPU model and memory:780ti 3g\r\n\r\nAlthough I already add cuda & cudnn locations to PATH , and when I run configure.py it do catch up the cuda &cudnn locations. But while compling I still get a wired Import Error.\r\n\r\n//configure.py:\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\ly0ko\\AppData\\Local\\Programs\\Python\\Python36\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\ly0ko\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\ly0ko\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.5\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: -march=native\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\n//build ERROR Massage:\r\n\r\nERROR: D:/programming/tf/tensorflow/tensorflow/python/keras/api/BUILD:12:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/ly0ko/_bazel_ly0ko/6bo2um6w/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\libnvvp;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\nodejs\\;C:\\Program Files (x86)\\GtkSharp\\2.12\\bin;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2019.1\\;C:\\Users\\ly0ko\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\;C:\\Users\\ly0ko\\AppData\\Local\\Programs\\Python\\Python36\\;C:\\Users\\ly0ko\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ly0ko\\AppData\\Roaming\\npm;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1;C:\\msys64;C:\\msys64\\usr\\bin;\r\n    SET PYTHON_BIN_PATH=C:/Users/ly0ko/AppData/Local/Programs/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/ly0ko/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TF_NEED_TENSORRT=0\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.exe  --apidir=bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api --apiname=keras --apiversion=1  --package=tensorflow.python,tensorflow.python.keras --output_package=tensorflow.python.keras.api bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/activations/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/densenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/inception_resnet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/inception_v3/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/mobilenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/mobilenet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/nasnet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/resnet50/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/vgg16/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/vgg19/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/xception/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/backend/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/callbacks/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/constraints/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/boston_housing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/cifar10/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/cifar100/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/fashion_mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/imdb/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/reuters/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/estimator/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/initializers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/layers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/layers/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/losses/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/metrics/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/mixed_precision/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/mixed_precision/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/models/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/optimizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/optimizers/schedules/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/preprocessing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/preprocessing/image/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/preprocessing/sequence/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/preprocessing/text/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/regularizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/wrappers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/wrappers/scikit_learn/__init__.py\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\ly0ko\\AppData\\Local\\Temp\\Bazel.runfiles_9jiiji0f\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\self_check.py\", line 87, in preload_check\r\n    ctypes.WinDLL(build_info.cudnn_dll_name)\r\n  File \"C:\\Users\\ly0ko\\AppData\\Local\\Programs\\Python\\Python36\\lib\\ctypes\\__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\ly0ko\\AppData\\Local\\Temp\\Bazel.runfiles_9jiiji0f\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\ly0ko\\AppData\\Local\\Temp\\Bazel.runfiles_9jiiji0f\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\ly0ko\\AppData\\Local\\Temp\\Bazel.runfiles_9jiiji0f\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"\\\\?\\C:\\Users\\ly0ko\\AppData\\Local\\Temp\\Bazel.runfiles_9jiiji0f\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\self_check.py\", line 97, in preload_check\r\n    % (build_info.cudnn_dll_name, build_info.cudnn_version_number))\r\nImportError: Could not find 'cudnn64_.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN  from this URL: https://developer.nvidia.com/cudnn\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n\r\n\r\n", "comments": ["@ly0koS Request you to please check all the below points and let us know .Thanks!\r\n\r\n1)Stackoverflow links: Please refer these\r\nhttps://stackoverflow.com/questions/48698536/tensorflow-gpu-import-tensorflow-importerror-could-not-find-cudnn64-7-dll\r\n\r\nhttps://stackoverflow.com/questions/51824219/importerror-could-not-find-cudnn64-7-dll/51825974\r\n\r\n2)The error suggests this. Did you try finding cudnn and putting it in your path?\r\nif not  then use below method\r\nyou should rename the cudnn64_6.dll or cudnn64_8.dll as cudnn64_7.dll in the below path of your system\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\r\n\r\n3)Please go through the steps present in the below link for installing tensorflow\r\n https://www.tensorflow.org/install/gpu\r\n\r\n4)Software requirements:\r\nNVIDIA\u00ae GPU drivers \u2014CUDA 10.0 requires 410.x or higher.\r\n\r\nHardware requirements:\r\nNVIDIA\u00ae GPU card with CUDA\u00ae Compute Capability 3.5 or higher.\r\n\r\n\r\n", "> @ly0koS Request you to please check all the below points and let us know .Thanks!\r\n> 1)Stackoverflow links: Please refer these\r\n> https://stackoverflow.com/questions/48698536/tensorflow-gpu-import-tensorflow-importerror-could-not-find-cudnn64-7-dll\r\n> https://stackoverflow.com/questions/51824219/importerror-could-not-find-cudnn64-7-dll/51825974\r\n> 2)The error suggests this. Did you try finding cudnn and putting it in your path?\r\n> if not  then use below method\r\n> you should rename the cudnn64_6.dll or cudnn64_8.dll as cudnn64_7.dll in the below path of your system\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\r\n> 3)Please go through the steps present in the below link for installing tensorflow\r\n> https://www.tensorflow.org/install/gpu\r\n> 4)Software requirements:\r\n> NVIDIA\u00ae GPU drivers \u2014CUDA 10.0 requires 410.x or higher.\r\n> Hardware requirements:\r\n> NVIDIA\u00ae GPU card with CUDA\u00ae Compute Capability 3.5 or higher.\r\n\r\n1)These are neither my problem and I did try all of them before I made the issue.\r\n\r\n2)I do put the cudnn files in my path.\r\nI use \"where\" in terminal and it returns C:\\C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\r\n\r\n3)I just want to build tensorflow-gpu with AVX2 SSE enabled.\r\n\r\n4)I meet all these requirements", "@ly0koS Can you please downgrade your tensorflow to 1.12 and follow the steps as per [link](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12), if you are particular to use 1.14 then please downgrade CUDA to 10.0", "> \r\n> \r\n> @ly0koS Can you please downgrade your tensorflow to 1.12 and follow the steps as per [link](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12), if you are particular to use 1.14 then please downgrade CUDA to 10.0\r\n\r\nThanks,downgrade to 1.12 fixed my problem!", "@ly0koS  Closing this issue as downgrading to TF1.12 resolved this error", "go with 'pip install tensorflow-gpu' in a fresh new env to be detected . Since, one env has many versions, so it makes it confusing."]}, {"number": 28222, "title": "[cmake] please update contrib/cmake for tensorflow 2.0 alpha", "body": "The current cmake files are outdated.", "comments": ["which specific files are outdated according to you?", "On master branch:\r\n```\r\n(sid)debian@debian-2:~/tensorflow/tensorflow/contrib/cmake$ cmake -Bx\r\n-- The C compiler identification is GNU 8.3.0\r\n-- The CXX compiler identification is GNU 8.3.0\r\n-- Check for working C compiler: /usr/lib/ccache/cc\r\n-- Check for working C compiler: /usr/lib/ccache/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/lib/ccache/c++\r\n-- Check for working CXX compiler: /usr/lib/ccache/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Looking for pthread_create\r\n-- Looking for pthread_create - not found\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed\r\n-- Performing Test GCC_OPENMP_SUPPORT\r\n-- Performing Test GCC_OPENMP_SUPPORT - Success\r\n-- /home/debian/tensorflow/tensorflow/contrib/cmake/x/abseil_cpp/src/abseil_cpp_build\r\n-- Found PythonInterp: /usr/bin/python (found version \"2.7.16\")\r\n-- Found PythonLibs: /usr/lib/powerpc64le-linux-gnu/libpython2.7.so (found version \"2.7.16\")\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/tpu/ops\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:612 (include)\r\n\r\n\r\n-- Found SWIG: /usr/bin/swig3.0 (found version \"3.0.12\")\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/home/debian/tensorflow/tensorflow/contrib/cmake/x/CMakeFiles/CMakeOutput.log\".\r\nSee also \"/home/debian/tensorflow/tensorflow/contrib/cmake/x/CMakeFiles/CMakeError.log\".\r\n```\r\n\r\nOn `r2.0` branch:\r\n```\r\n(sid)debian@debian-2:~/tensorflow/tensorflow/contrib/cmake$ cmake -Bx\r\n-- The C compiler identification is GNU 8.3.0\r\n-- The CXX compiler identification is GNU 8.3.0\r\n-- Check for working C compiler: /usr/lib/ccache/cc\r\n-- Check for working C compiler: /usr/lib/ccache/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/lib/ccache/c++\r\n-- Check for working CXX compiler: /usr/lib/ccache/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Looking for pthread_create\r\n-- Looking for pthread_create - not found\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed\r\n-- Performing Test GCC_OPENMP_SUPPORT\r\n-- Performing Test GCC_OPENMP_SUPPORT - Success\r\n-- /home/debian/tensorflow/tensorflow/contrib/cmake/x/abseil_cpp/src/abseil_cpp_build\r\n-- Found PythonInterp: /usr/bin/python (found version \"2.7.16\")\r\n-- Found PythonLibs: /usr/lib/powerpc64le-linux-gnu/libpython2.7.so (found version \"2.7.16\")\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/tpu/ops\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:611 (include)\r\n\r\n\r\n-- Found SWIG: /usr/bin/swig3.0 (found version \"3.0.12\")\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/home/debian/tensorflow/tensorflow/contrib/cmake/x/CMakeFiles/CMakeOutput.log\".\r\nSee also \"/home/debian/tensorflow/tensorflow/contrib/cmake/x/CMakeFiles/CMakeError.log\".\r\n```\r\n\r\nOn `r1.13` branch: looks good.", "@cdluminate Contrib is deprecated in 2.0. Here is the [link](https://github.com/tensorflow/tensorflow/releases) for 2.0 release. Thanks!", "@gadagashwini  How to using  cmake in 2.0? It does not released yet but our full product has been update into tensorflow 2.0", "@cdluminate We don't provide support for the CMake build, instead use Bazel. Please find the instructions mentioned in the [Tenosrflow](https://www.tensorflow.org/install/source) website.Thanks! "]}, {"number": 28221, "title": "Non stateful scatter min max", "body": "", "comments": []}, {"number": 28220, "title": "tflite_interpreter.run() showing error with GPU delegate", "body": "I am trying to implement the GPU support on Android for Yolov3 model.  Without the GPU support its working fine but whenever I am creating a GPU delegate and adding as the interpreter option and finally creating a tflite interpreter that can run to GPU. But tflite interpreter got stuck in runForMultipleInputsOutputs() call (Never returning from this)\r\n\r\nWhile using tflite_interpreter.run() I am getting the following error, can anyone Please tell me how to resolve this issue?\r\n\r\n**_Error:_**\r\n_04-28 01:28:54.862 21858-21916/com.amitshekhar.tflite E/AndroidRuntime: FATAL EXCEPTION: Thread-4\r\n    Process: com.amitshekhar.tflite, PID: 21858\r\n    java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 13, 13, 3, 12] and a Java object with shape [1, 7].\r\n        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:270)\r\n        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:141)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:161)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:249)_\r\n", "comments": ["@spaul13 It looks like you haven't used a template to create this issue(mention the versions used). Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28219, "title": "tf.tuple can't be indexed with tensor in graph mode", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0.dev20190427\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nthe tf.tuple can't be indexed by tensor in graph mode.\r\n\r\n**Describe the expected behavior**\r\n\r\ntf.tuple should support index with tensor in graph mode.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf;\r\n\r\n@tf.function\r\ndef func(a):\r\n    i = tf.constant(0);\r\n    return a[i];\r\n\r\nt = tf.tuple([tf.constant(0),tf.constant(1)]);\r\na = func(t);\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["You can use `tf.gather()` or `tf.gather_nd()`.", "I tried the following code and failed.\r\n\r\n```python\r\nimport numpy as np;\r\nimport tensorflow as tf;\r\n\r\n@tf.function\r\ndef get(t, indices):\r\n    return tf.gather(t, indices);\r\n\r\nt = tf.tuple([tf.constant(np.random.normal(size = (3,4))), tf.constant(np.random.normal(size=(4,5)))]);\r\nindices = tf.constant([0]);\r\na = get(t,indices);\r\n```\r\n\r\nThe error message is\r\n>ValueError: Tried to convert 'params' to a tensor and failed. Error: Dimension 0 in both shapes must be equal, but are 3 and 4. Shapes are [3,4] and [4,5].", "@jsimsa is there any known method for handling this?\r\nThe `Pack` Operation is raising this error, in `tf.gather()` and `tf.slice()`, Since the dimension of the tensors are different. Also, python like indexing doesn't support the use of Tensors.", "The problem is that `t` is not a `Tensor` but a list of tensors. Wrapping it in `convert_to_tensor` does the trick:\r\n\r\n```\r\nimport tensorflow as tf;\r\n\r\ntf.enable_v2_behavior()\r\n\r\n@tf.function\r\ndef func(a):\r\n    i = tf.constant(0);\r\n    return a[i];\r\n\r\nt = tf.tuple([tf.constant(0),tf.constant(1)]);\r\nprint(t[0].numpy())  # prints 0\r\na = func(tf.convert_to_tensor(t));\r\nprint(a.numpy())  # prints 0\r\n```", "tf.tuple as a tensorflow utility should support index by tensor. otherwise, it has no difference from python's tuple. without index by tensor function, the following problem can't be solved.\r\n\r\n```python\r\nimport numpy as np;\r\nimport tensorflow as tf;\r\n\r\n@tf.function\r\ndef get(t, index):\r\n    return t[index];\r\n\r\nt = tf.tuple([tf.constant(np.random.normal(size = (3,4))), tf.constant(np.random.normal(size=(4,5)))]);\r\nindex = tf.constant(0);\r\na = get(t,index);\r\n```", "Closing this issue since jsimsa has provided a workaround. Feel free to reopen this issue if have further questions or perhaps open a feature request to add this functionality if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28219\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28219\">No</a>\n"]}, {"number": 28218, "title": "Detecting Bounding Boxes along with class labels realtime", "body": "I am using TF Lite Android Image Classifier App provided with this project with GPU support for Mobilenetv1. I want to know how I can detect the bounding boxes of inferred/detected objects along with the class labels?\r\n\r\nCan anyone please tell me what modifications I have to make?", "comments": ["You may want to start from the object detection example first https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android", "@freedomtan Thanks a lot for the reply. It (https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) is working for the input size 224 when I am changing the resized bitmap size from 224 to 416 or 608 I am getting the following error \r\n\r\n**_04-30 22:00:51.810 26345-26363/org.tensorflow.lite.examples.detection E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 26345\r\n    java.lang.ArrayIndexOutOfBoundsException\r\n        at android.graphics.Bitmap.checkPixelsAccess(Bitmap.java:1718)\r\n        at android.graphics.Bitmap.getPixels(Bitmap.java:1659)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:156)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:215)\r\n        at android.os.Handler.handleCallback(Handler.java:790)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)_**\r\ncan anyone please tell me how to resolve this issue?", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28217, "title": "tflite interpreter getting stuck at runForMultipleInputsOutputs function", "body": "I am initializing tflite interpreter to run the object detection inference with GPU delegate option for Yolov3 (adding a GPU delegate) for enabling GPU support on Android Platform (on Pixel 2). But when I run **tflite.runForMultipleInputsOutputs**(inputArray, outputMap);\r\n\r\nIts getting stuck there not returning anything while without the GPU delegate that functions working well. Can anyone please tell me why it's getting stuck for indefinitely?\r\n\r\n[e.g., If few ops are not performed by GPU that must be performed by CPU this might increase the time of inference but it should provide the output]", "comments": ["@spaul13 It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 28216, "title": "Migrating to 2.0 with confusing MultiRNNCell", "body": "While upgrading my project to tensorflow 2.0, this part of the code is confusing me.\r\n\r\n## In tensorflow 1.13.1\r\n```\r\nnum_layers = 2\r\nnum_units = 128\r\n\r\nrnn = tf.contrib.rnn.MultiRNNCell(\r\n            [\r\n                tf.contrib.rnn.BasicLSTMCell(\r\n                    num_units,\r\n                    forget_bias=0.0,\r\n                    state_is_tuple=False,\r\n                )\r\n                for _ in range(num_layers)\r\n            ],\r\n            state_is_tuple=False,\r\n        )\r\n\r\n# x.shape == (?, 128)  and state.shape == (?, 512)\r\n\r\nx, state = rnn(x, state)\r\n\r\n# x.shape == (?, 128)  and state.shape == (?, 512)\r\n```\r\nEverey thing is working fine.\r\n\r\n\r\n## Upgraded to tensorflow 2.0-alpha0\r\n\r\nAs documentation: \r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells\r\n\r\n```\r\ncells = [\r\n        tf.keras.layers.LSTMCell(num_units) for _ in range(num_layers)\r\n    ]\r\nrnn = tf.keras.layers.RNN(cells)\r\n\r\n# x.shape == (?, 128)  and state.shape == (?, 512)\r\n\r\nx, state = rnn(x, state)\r\n\r\n\r\nbut here I got an error and I think is related to shapes of `x` and `state`:\r\n\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 285, in <module>\r\n    main()\r\n  File \"run.py\", line 253, in main\r\n    channels=parameters.channels,\r\n  File \"/home/nicoo/py/aocr/400/aocr/model/model.py\", line 168, in __init__\r\n    training=not self.forward_only,\r\n  File \"/home/nicoo/py/aocr/400/aocr/model/seq2seq_model.py\", line 82, in __init__\r\n    hidden_features,\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 554, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/nicoo/py/aocr/400/aocr/model/Decoder.py\", line 39, in call\r\n    cell_output, state = self.cell(x, state)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 750, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 538, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1603, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 619, in build\r\n    self.cell.build(step_input_shape)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py\", line 151, in wrapper\r\n    output_shape = fn(instance, input_shape)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 153, in build\r\n    cell.build(input_shape)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py\", line 151, in wrapper\r\n    output_shape = fn(instance, input_shape)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 2022, in build\r\n    constraint=self.kernel_constraint)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 349, in add_weight\r\n    aggregation=aggregation)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\", line 607, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 145, in make_variable\r\n    aggregation=aggregation)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 213, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 176, in _variable_v1_call\r\n    aggregation=aggregation)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 155, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2488, in default_variable_creator\r\n    import_scope=import_scope)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 217, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 294, in __init__\r\n    constraint=constraint)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 406, in _init_from_args\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 127, in <lambda>\r\n    shape, dtype=dtype, partition_info=partition_info)\r\n  File \"/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\", line 499, in __call__\r\n    scale /= max(1., (fan_in + fan_out) / 2.)\r\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\r\n```", "comments": ["`tf.contrib.rnn.MultiRNNCell` is an `RNNCell`, while `tf.keras.layers.RNN` is an `RNN API` (like `tf.nn.dynamic_rnn`) instead of a kind of `RNNCell`. An `RNNCell` defines how **a single time step** is computed, while an `RNN API` loops over **all time steps** with a given `RNNCell`.\r\nSo you need `tf.keras.layers.StackedRNNCells`(which is a kind of `RNNCell`)  instead of `tf.keras.layers.RNN`. The following code should work:\r\n\r\n```\r\nnum_layers = 2\r\nnum_units = 128\r\nbatch_size = 20\r\nembedding_size = 50\r\n\r\ncells = [tf.keras.layers.LSTMCell(num_units) for _ in range(num_layers)]\r\nrnn = tf.keras.layers.StackedRNNCells(cells)\r\n\r\n# shape of x: [B, D]\r\nx = tf.Variable(tf.random.normal([batch_size, embedding_size]), dtype=tf.float32)\r\n# shape of state: (([B, H], [B, H]), ([B, H], [B, H]))\r\nstate = rnn.get_initial_state(x)\r\n\r\n# shape of output: [B, H]\r\n# shape of new_state: (([B, H], [B, H]), ([B, H], [B, H]))\r\noutput, new_state = rnn(x, state)\r\n\r\nprint(output)\r\nprint(new_state)\r\n\r\n```", "Thank you.", "@nicoosokhan Were you able to resolve this issue ?", "> @nicoosokhan Were you able to resolve this issue ?\r\n\r\nYes, by using @soloice description I have rewritten this part of the code.\r\nThe main consideration was migrating to Tensorflow 2 without changing the network.\r\n\r\n```\r\nclass MultiCellRNN(tf.keras.layers.Layer):\r\n\r\n    def __init__(self, num_layers, num_units, **kwargs):\r\n        super().__init__(**kwargs)\r\n\r\n        self.cells = [\r\n            tf.keras.layers.LSTMCell(\r\n                num_units,\r\n            )\r\n            for _ in range(num_layers)\r\n        ]\r\n\r\n        self.cell = tf.keras.layers.StackedRNNCells(\r\n            self.cells,\r\n        )\r\n\r\n    def call(self, x, state, **kwargs):\r\n\r\n        state = tf.split(state, [128, 128, 128, 128], 1)\r\n        x, state = self.cell(x, ([state[0], state[1]], [state[2], state[3]]))\r\n        state = tf.concat([state[0][0], state[0][1], state[1][0], state[1][1]], 1)\r\n\r\n        return x, state\r\n```\r\n\r\nI know that this is a dirty refactoring, but this was exactly what `tf.contrib.rnn.MultiRNNCell` old api do with `concatenated state` to have `tuple states`.\r\n\r\nI hope to refactor this hardcoded section as soon as possible.\r\n\r\nThanks for the community.", "Closing since its resolved. Thanks!"]}, {"number": 28215, "title": "The Nadam optimizer is non-deterministic due to a race condition related to inter_op_parallelism_threads", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- TensorFlow installed from (source or binary): Arch Linux repositories\r\n- TensorFlow version (use command below): 'unknown' 1.13.1\r\n- Python version: 3.7.3\r\n- GPU model and memory: None. I'm running on CPU.\r\n\r\n**Describe the current behavior**\r\n\r\nWhen `inter_op_parallelism_threads` is set to `1` the `iterations` variable in Nadam optimizer from `tensorflow/python/keras/optimizers.py` always starts at `1` when the optimizer is executed; when `inter_op_parallelism_threads` is higher than `1` then `iterations` sometimes starts as `0`. The higher the `inter_op_parallelism_threads` is set to the higher the chance that `iterations` will start at `0` instead of at `1`.\r\n\r\n(I know this because I `tf.Print`'d everything inside of the Nadam optimizer.)\r\n\r\nThis makes the training results non-deterministic since the Nadam optimizer depends on the current iteration number when calculating the weight updates.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe Nadam optimizer will be deterministic.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n#!/usr/bin/python\r\n\r\nimport tensorflow as tf;\r\nimport numpy as np;\r\n\r\nconstant = tf.keras.initializers.constant;\r\n\r\ninter_op_parallelism_threads = 1\r\n# inter_op_parallelism_threads = 20\r\n\r\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=inter_op_parallelism_threads)\r\ngraph = tf.get_default_graph()\r\nsession = tf.Session(graph=graph, config=session_conf)\r\ntf.keras.backend.set_session(session)\r\n\r\noptimizer = tf.keras.optimizers.Nadam(lr=1.0, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\r\n\r\nbias = constant(np.array([1.0]))\r\nweights = constant(np.array([1.0]))\r\nlayer = tf.keras.layers.Dense(1, bias_initializer=bias, kernel_initializer=weights, input_shape=(1,))\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layer)\r\nmodel.compile(optimizer=optimizer, metrics=[\"accuracy\"], loss=[\"mean_squared_error\"])\r\n\r\ninput = np.array([1.0])\r\noutput = np.array([1.0])\r\nmodel.train_on_batch(input, output)\r\n\r\nw = layer.get_weights()\r\nprint(w)\r\n```\r\n\r\nOutput when `inter_op_parallelism_threads` is set to 20 (so `iterations` starts at `0`):\r\n\r\n```\r\n[array([[-0.0564518]], dtype=float32), array([-0.0564518], dtype=float32)]\r\n```\r\n\r\nOutput when `inter_op_parallelism_threads` is set to 1 (so `iterations` starts at `1`):\r\n\r\n```\r\n[array([[-0.49368942]], dtype=float32), array([-0.49368942], dtype=float32)]\r\n```\r\n", "comments": ["@koute : I have tried to reproduce the issue on colab (tf1.13.1) but unfortunately I am able to get the same result in both the cases mentioned (inter_op_parallelism_threads = 1 or inter_op_parallelism_threads = 20). Please let us know if it is still an issue. Thanks!", "@achandraa Yes. I have no idea why it doesn't reproduce on colab; it does locally on my PC every time I run it. It's a race condition though, so I'm not surprised.\r\n\r\nIt does reproduce in a different way on colab though. Select \"GPU\" in \"Runtime\" -> \"Change runtime type\" and you'll always hit the case where `iterations` starts at `0` (so you'll get `-0.05648327`; it's not exactly the same as in my original reproduction most likely due to the fact that now it's running on the GPU instead of the CPU); if you select \"None\" then you'll always hit the case when `iterations` starts at `1` (so you'll get `-0.49368942` as result). I believe the root cause of this should be the same as my original issue.", "@koute : Tried in Colab (GPU runtime) as suggested but got the same result for cases where inter_op_parallelism_threads = 0 or inter_op_parallelism_threads = None which is [array([[-0.05648327]], dtype=float32), array([-0.05648327], dtype=float32)]. And this is on tensorflow version 1.13.1. Can you try and let us know whether you are getting different on Colab with same environment. Thanks!", "@achandraa Yes, you're getting the same results as I do. Now change the runtime to CPU, and you'll always get `-0.49368942`.\r\n\r\nI'll reiterate what I said - on Colab the original issue does not reproduce in the way as originally described, i.e. the parallelism settings do not matter, but it does reproduce in a different way. Here's a handy table:\r\n\r\n```\r\n| Runtime | initial value of iterations | Result      |\r\n| --------------------------------------------------- |\r\n| CPU     | 1                           | -0.49368942 |\r\n| GPU     | 0                           | -0.05648327 |\r\n| --------------------------------------------------- |\r\n```\r\n\r\nThe results on both runtimes should be the same. I believe the root cause of this problem should be the same as the root cause of my original problem.", "I got following output while running on different runtimes on Colab with version 1.13.1 :\r\n\r\nRuntime : CPU\r\nInitial value of iterations : 1\r\nResult : -0.0564518\r\n\r\nRuntime : GPU\r\nInitial value of iterations : 0\r\nResult : -0.05648327\r\n\r\nKindly let us know is this we are referring to? Thanks!", "So you're getting the same values on both the CPU and the GPU runtime? This is really strange. Because that's not what I am getting. Are you perhaps getting assigned a slightly different environment on Colab or something like that? (I'm not really too familiar with how exactly it works.)\r\n\r\nHere are the screenshots of my results:\r\n\r\nCPU results - https://imgur.com/eqhsNTn\r\nGPU results - https://imgur.com/dj2uxNg\r\n\r\nThis is on the default Colab environment in a fresh Python 3 notebook.", "After checking again and trying on default Colab environment in a fresh Python 3 notebook with TensorFlow version 1.13.1,  I was able to reproduce the issue with the outputs you have provided. Below  are the result : \r\n\r\nRuntime : CPU\r\nInitial value of iterations : 1\r\nResult : -0.49368942\r\n\r\nRuntime : GPU\r\nInitial value of iterations : 0\r\nResult : -0.05648327\r\n", "@koute \r\n\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28215\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28215\">No</a>\n"]}, {"number": 28214, "title": "Inspecting the Tensorflow 2.0 python package: unable to find or import modules", "body": "I'm analyzing the Tensorflow 2.0 Python package using `importlib` to loop through the symbols of the tensorflow package (see: https://github.com/galeone/rtf/blob/master/rtf/generators/base.py#L72 ).\r\n\r\nThere are certain modules that `importlib` is not able to load or find, e.g. the `tf.losses` `tf.keras.losses`, `tf.optmizers` (and all the module alias) etc. So far, recursively using `importlib.import_module` I get this structure\r\n\r\n```\r\n.\r\n\u251c\u2500\u2500 autograph\r\n\u251c\u2500\u2500 compat\r\n\u2502   \u251c\u2500\u2500 v1\r\n\u2502   \u2502   \u251c\u2500\u2500 autograph\r\n\u2502   \u2502   \u251c\u2500\u2500 config\r\n\u2502   \u2502   \u251c\u2500\u2500 data\r\n\u2502   \u2502   \u251c\u2500\u2500 distribute\r\n\u2502   \u2502   \u251c\u2500\u2500 estimator\r\n\u2502   \u2502   \u251c\u2500\u2500 io\r\n\u2502   \u2502   \u251c\u2500\u2500 keras\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 applications\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 datasets\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 mixed_precision\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 optimizers\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 preprocessing\r\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 wrappers\r\n\u2502   \u2502   \u251c\u2500\u2500 layers\r\n\u2502   \u2502   \u251c\u2500\u2500 lite\r\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 experimental\r\n\u2502   \u2502   \u251c\u2500\u2500 lookup\r\n\u2502   \u2502   \u251c\u2500\u2500 nn\r\n\u2502   \u2502   \u251c\u2500\u2500 random\r\n\u2502   \u2502   \u251c\u2500\u2500 saved_model\r\n\u2502   \u2502   \u251c\u2500\u2500 tpu\r\n\u2502   \u2502   \u251c\u2500\u2500 train\r\n\u2502   \u2502   \u2514\u2500\u2500 xla\r\n\u2502   \u2514\u2500\u2500 v2\r\n\u2502       \u251c\u2500\u2500 autograph\r\n\u2502       \u251c\u2500\u2500 config\r\n\u2502       \u251c\u2500\u2500 data\r\n\u2502       \u251c\u2500\u2500 distribute\r\n\u2502       \u251c\u2500\u2500 estimator\r\n\u2502       \u251c\u2500\u2500 io\r\n\u2502       \u251c\u2500\u2500 keras\r\n\u2502       \u2502   \u251c\u2500\u2500 applications\r\n\u2502       \u2502   \u251c\u2500\u2500 datasets\r\n\u2502       \u2502   \u251c\u2500\u2500 mixed_precision\r\n\u2502       \u2502   \u251c\u2500\u2500 optimizers\r\n\u2502       \u2502   \u251c\u2500\u2500 preprocessing\r\n\u2502       \u2502   \u2514\u2500\u2500 wrappers\r\n\u2502       \u251c\u2500\u2500 lookup\r\n\u2502       \u251c\u2500\u2500 random\r\n\u2502       \u251c\u2500\u2500 tpu\r\n\u2502       \u251c\u2500\u2500 train\r\n\u2502       \u2514\u2500\u2500 xla\r\n\u251c\u2500\u2500 config\r\n\u251c\u2500\u2500 data\r\n\u251c\u2500\u2500 distribute\r\n\u251c\u2500\u2500 estimator\r\n\u251c\u2500\u2500 io\r\n\u251c\u2500\u2500 keras\r\n\u2502   \u251c\u2500\u2500 applications\r\n\u2502   \u251c\u2500\u2500 datasets\r\n\u2502   \u251c\u2500\u2500 mixed_precision\r\n\u2502   \u251c\u2500\u2500 optimizers\r\n\u2502   \u251c\u2500\u2500 preprocessing\r\n\u2502   \u2514\u2500\u2500 wrappers\r\n\u251c\u2500\u2500 lite\r\n\u2502   \u251c\u2500\u2500 experimental\r\n\u2502   \u2502   \u2514\u2500\u2500 examples\r\n\u2502   \u2502       \u2514\u2500\u2500 lstm\r\n\u2502   \u251c\u2500\u2500 python\r\n\u2502   \u2502   \u2514\u2500\u2500 optimize\r\n\u2502   \u2514\u2500\u2500 toco\r\n\u251c\u2500\u2500 lookup\r\n\u251c\u2500\u2500 random\r\n\u251c\u2500\u2500 tools\r\n\u2502   \u2514\u2500\u2500 docs\r\n\u251c\u2500\u2500 tpu\r\n\u251c\u2500\u2500 train\r\n\u2514\u2500\u2500 xla\r\n```\r\n\r\nthat is incomplete.\r\n\r\nIs there a way to use the `importlib` module to get the complete structure of the tensorflow package and to successfully load the module?\r\n\r\nHere's a minimum reproducible example:\r\n\r\n## Install\r\n\r\n```\r\npip install --upgrade tf-nightly-2.0-preview\r\n```\r\n\r\n## Loading losses\r\n\r\n```python\r\nimport importlib \r\nimportlib.import_module(\"tensorflow.losses\")\r\n```\r\n\r\nProduces the error: \"ModuleNotFoundError: No module named 'tensorflow.losses'\"\r\n\r\nThis happens only in Tensorflow 2.0, if I install Tensorflow 1.14 I can inspect more or less every package without any failure (haven't tested in depth).\r\n\r\nDisclosure: this is a crosspost from https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/jibh60HPGA0\r\n\r\nGreetings", "comments": ["@galeone Request you to please refer the [link 2.0](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses) and use the built in loss functions.\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "Hi,\nThis is an \"other issue\", I opened it using the template for \"other issues\".\n\nThere is no built-in loss function, and it is not a StackOverflow question\nsince it is not about how to use Tensorflow.\n\nThe question is: why is impossible to import using Python importlib certain\nmodules of the Tensorflow package in Tensorflow 2.0?\n\nMaking the module inspectable and usable from Python is something that was\npossible in Tensorflow 1.14 but is no more possible in 2.0.\n\nI guess this is classifiable as a bug since it makes impossible to use the\nlibrary in this particular case.\n\n(I'm from my mobile, sorry for the typos and any other monstrosity)\n\nOn Mon, Apr 29, 2019, 17:23 muddham <notifications@github.com> wrote:\n\n> @galeone <https://github.com/galeone> Request you to please refer the link\n> 2.0 <https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses>\n> and use the built in loss functions.\n> This question is better asked on StackOverflow\n> <http://stackoverflow.com/questions/tagged/tensorflow> since it is not a\n> bug or feature request. There is also a larger community that reads\n> questions there.If you think we've misinterpreted a bug, please comment\n> again with a clear explanation, as well as all of the information requested\n> in the issue template\n> <https://github.com/tensorflow/tensorflow/issues/new/choose>. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28214#issuecomment-487621141>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACAJSDDW6EAZE65RZF2FSKTPS4HFTANCNFSM4HI4K4TQ>\n> .\n>\n", "Through investigation so far, it seems that this is fooled by the Lazy loaders we are using. Need to investigate more and see if we can fix it.", "Yes, I can confirm. I analyzed the source code a bit, and I found a lot of magic happening in a `v2/api.py` file (or something similar, I don't remember exactly), explicitly saying that lazy loaders are being used", "Adding @yifeif.\r\n@galeone could you share your use case with us, maybe then we can see where exactly the problem is. or how to work around it.", "The use case is the inspection of the tensorflow package (tensorflow==2.0.0beta1) using reflection.\r\n\r\nI need this because I'm working on a project that has the goal of emulating the same TensorFlow structure (packages, sub-packages, classes, and so on) and creating a new Python package that has the same structure. __[*]__\r\n\r\nIf you have some minute, you can have a look at the file: https://github.com/galeone/rtf/blob/master/rtf/generators/base.py\r\n\r\nIf the package is `tensorflow` I got the result I posted on the opening post. I can inspect many sub-modules, but most of them are impossible to reach and inspect (`tf.keras.losses`, `tf.losses`, `tf.optimizers`, ...).\r\n\r\n__[*]__ The goal of the project is to have a Python package, that has the same structure of the TensorFlow package, but that instead of locally executing the commands on the current machine, has every class/method/function reimplemented as a gRPC call, in order to remote execute TensorFlow", "One thing I can recommend is, our python API is actually documented here:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/api/golden\r\n\r\nWould the documented API be useful?", "Also, the tools to generate this API is also right here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/lib/python_object_to_proto_visitor.py\r\n\r\nCould you try this tool out to see if it works better for you?", "This folder ( https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/api/golden/v2 ) contains, perhaps, everything I need. Can you confirm that this is the TF 2 Python API completely described as protobuf? I mean: these files can be parsed and I can get the complete hierarchy of the tensorflow==2.0.0beta1 package I'm using?\r\n\r\nThe API generator, instead, look useful but I'm not sure on how to use it to get the complete hierarchy in the format I need. Searching in the repository I can only find a single usage in a test\r\n https://github.com/tensorflow/tensorflow/blob/a45a9d2ac8862b8f7162f6189ab567ed194c02b7/tensorflow/tools/api/tests/api_compatibility_test.py#L314\r\n", "I can confirm that for any given commit, these files will give you the full TF API. We have continuous tests, and presubmit tests that prevent these files and the TF api going out of sync.\r\n\r\nThe api symbols stored in these files are using this protocol buffer:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/lib/api_objects.proto#L37", "This is great and it will help me **a lot**. \r\n\r\nAlso from what I can see from the test I linked, using the visitor is not that difficult and it looks like I can use it to build `TFAPIObject` messages just buy looping over the files and post-process them\r\nhttps://github.com/tensorflow/tensorflow/blob/a45a9d2ac8862b8f7162f6189ab567ed194c02b7/tensorflow/tools/api/tests/api_compatibility_test.py#L341 \r\n\r\nright?", "As far as I know, you are correct.\r\n@annarev just in case something has changed since I last touched that code.", "Does something like the following work for your usecase @galeone?\r\ntf = importlib.import_module(\"tensorflow\")\r\ngetattr(tf, \"losses\")", "@yilei this approach seems to work!\r\n\r\nBut why is this happening? I mean: I can't load `\"tensorflow.losses\"` using `importlib.import_module` but I can load the losses package using `getattr`. Is this something related to the lazy loading of the modules?\r\n\r\nMoreover, having it working using `getattr` and not using `import_module` is an expected behavior?", "Hi,\r\n\r\nYes, lazy loading makes the modules only available via `getattr`\r\n\r\nYou can also try looking into `sys.modules`, even if the module is lazy loaded it should be as a key there (though probably not all of its submodules)", "@galeone \r\nPlease let us know if this is still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Saduf2019 no more an issue :+1: ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28214\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28214\">No</a>\n", "@galeone \r\nThank you for your update, glad to hear its not an issue anymore."]}, {"number": 28213, "title": "AttributeError: module 'tensorflow' has no attribute 'keras'", "body": "Hello,\r\n\r\nI have a question i can't compile while i have installed:\r\ntensorflow                2.0.0a0\r\nKeras                     2.2.4\r\n\r\nI got error:\r\nAttributeError: module 'tensorflow' has no attribute 'keras'\r\n\r\nmy Code:\r\nimport tensorflow as tf\r\nmnist = tf.keras.datasets.mnist\r\n\r\nI'm in Mac OS X 10.14\r\n\r\nSomthing i do wrong?\r\n", "comments": ["Hello,\r\n\r\ni found it the fix was. in Mac OS (cd ~/) there are a folder named tensorflow\r\ninside homefolder. you create a file named: test.py\r\n\r\nmy Code:\r\nimport tensorflow as tf\r\nmnist = tf.keras.datasets.mnist\r\n\r\nI'm in Mac OS X 10.14\r\n\r\nAnd it works now. i think you need to have the folder (cd ~/tensorflow)", "Hello,\r\n\r\nIf you change the file name to (tensorflow.py)\r\nit'll come with error:\r\nAttributeError: module 'tensorflow' has no attribute 'keras'", "Thanks for trying TF 2.0 alpha. Naming a script tensorflow.py messes up importing tf modules. Thus it is not a recommended naming convention.", "I am facing error AttributeError: module 'tensorflow' has no attribute 'keras' in windows,please tell what to do", "what is your python file name?", "Thanks for your reply to my query, but I got the solution to it, a file in\nthe same folder was named tensorflow by me.\n\nOn Sat, 7 Mar, 2020, 7:09 PM Frederik L. Frandsen, <notifications@github.com>\nwrote:\n\n> what is your python file name?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28213?email_source=notifications&email_token=AKDTZJ53RQZ4KCME45L37E3RGJFB7A5CNFSM4HI4F4Z2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEODZWAY#issuecomment-596089603>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKDTZJYY2WM6TJPYEISBFD3RGJFB7ANCNFSM4HI4F4ZQ>\n> .\n>\n"]}, {"number": 28212, "title": "[Java] Add eager operation builder", "body": "PR #3 of 5 of the Java eager environment epic.\r\n\r\nThis one adds a new implementation of `OperationBuilder` for eager operations. It replicates pretty much the `GraphOperationBuilder` but native calls are redirected to the TFE C API endpoints instead.\r\n\r\nCC: @sjamesr ", "comments": ["@angersson can you please help review this PR ?", "@rthadur I think @sjamesr is a better fit -- I don't know how TF Java works.", "> @rthadur I think @sjamesr is a better fit -- I don't know how TF Java works.\r\n\r\nSure , thank you @angersson ", "Thanks for the review @sjamesr , I just pushed my changes.", "Ok @sjamesr, I replaced the dynamic-size array allocation by a smart pointer, looks like that's what is being used elsewhere in the JNI code. Strange that my local GCC did not complain about it..."]}, {"number": 28211, "title": "Fatal error C1001 in tensorflow\\compiler\\xla\\literal.cc(1291)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: checkout from `master` branch\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?: conda 4.6.14\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): cl.exe C/C++ Optimizing Compiler Version 19.16.27030.1 for x64, Visual Studio Build Tools 2017\r\n- CUDA/cuDNN version: CUDA Toolkit V10.0.130, cuDNN 7.5.0\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n**Describe the problem**\r\nFatal error C1001: An internal error has occurred in the compiler in tensorflow\\compiler\\xla\\literal.cc(1291).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```bash\r\n(base) Stepii@STEPII D:\\Neural\\tensorflow\r\n$ python configure.py\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1 installed.\r\nPlease specify the location of python. [Default is D:\\Programs\\Anaconda3\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  D:\\Programs\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [D:\\Programs\\Anaconda3\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.0 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 6.1\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: /arch:AVX2\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n(base) Stepii@STEPII D:\\Neural\\tensorflow\r\n$ bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package -j 12\r\n\r\n[...]\r\n\r\nERROR: D:/neural/tensorflow/tensorflow/compiler/xla/BUILD:366:1: C++ compilation of rule '//tensorflow/compiler/xla:literal' failed (Exit 3): python.exe failed: error executing command\r\n  cd C:/users/stepii/_bazel_stepii/5mniti2w/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\\\MSBuild\\15.0\\bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=D:/Programs/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=D:/Programs/Anaconda3/lib/site-packages\r\n    SET TEMP=C:\\Users\\Stepii\\AppData\\Local\\Temp\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TF_NEED_TENSORRT=0\r\n    SET TMP=C:\\Users\\Stepii\\AppData\\Local\\Temp\r\n  D:/Programs/Anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN /arch:AVX2 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/_objs/literal/literal.o /c tensorflow/compiler/xla/literal.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nc:\\users\\stepii\\_bazel_stepii\\5mniti2w\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1291) : fatal error C1001: An internal error has occurred in the compiler.\r\n(compiler file 'd:\\agent\\_work\\1\\s\\src\\vctools\\compiler\\utc\\src\\p2\\main.c', line 187)\r\n To work around this problem, try simplifying or changing the program near the locations listed above.\r\nPlease choose the Technical Support command on the Visual C++\r\n Help menu, or open the Technical Support help file for more information\r\n  cl!InvokeCompilerPassW()+0x3e708\r\n\r\nc:\\users\\stepii\\_bazel_stepii\\5mniti2w\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1291) : fatal error C1001: An internal error has occurred in the compiler.\r\n(compiler file 'd:\\agent\\_work\\1\\s\\src\\vctools\\compiler\\utc\\src\\common\\error.c', line 835)\r\n To work around this problem, try simplifying or changing the program near the locations listed above.\r\nPlease choose the Technical Support command on the Visual C++\r\n Help menu, or open the Technical Support help file for more information\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1698.097s, Critical Path: 416.10s\r\nINFO: 3624 processes: 3624 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nSee [FullLog.txt](https://github.com/tensorflow/tensorflow/files/3123796/FullLog.txt).\r\n", "comments": ["@Stepiii You could try to install new version of TF 2.0 nightly [Similar issue](https://github.com/tensorflow/tensorflow/issues/24890) Also there is a common issue with Windows10 where path length is restricted by default. Please check this [resource](https://www.howtogeek.com/266621/how-to-make-windows-10-accept-file-paths-over-260-characters/) to remove that restriction and then install TF.Please let us know how it progresses. Thanks!", "@muddham I have long paths enabled but the error still appears. ", "@muddham Another thing I would like to point out is that when I run\r\n`bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package -j 12`\r\nagain, a different error appers on each run. See #28309", "@Stepiii As per the [link](https://tensorflow.google.cn/install/source_windows#tested_build_configurations),  Please use relevant bazel version, for TF1.13 it shows Bazel 0.15.0. Please try and let us know how it progresses. Thanks!", "@muddham Are you sure that I need Bazel 0.15.0?\r\n```bash\r\n(neural) Stepii@STEPII D:\\Neural\\tensorflow\r\n$ bazel version\r\nStarting local Bazel server and connecting to it...\r\n..............\r\nWARNING: while reading option defaults file 'd:\\neural\\tensorflow\\.bazelrc':\r\n  invalid command name 'try-import'.\r\nWARNING: while reading option defaults file 'd:\\neural\\tensorflow\\.bazelrc':\r\n  invalid command name 'try-import'.\r\nBuild label: 0.15.0\r\nBuild target: bazel-out/x64_windows-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 26 12:09:42 2018 (1530014982)\r\nBuild timestamp: 1530014982\r\nBuild timestamp as int: 1530014982\r\n\r\n(neural) Stepii@STEPII D:\\Neural\\tensorflow\r\n$ python configure.py\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.0 installed.\r\nPlease upgrade your bazel installation to version 0.24.1 or higher to build TensorFlow!\r\n```", "@Stepiii  As per the message the error is with respect to compiler can you please use Visual studio 2015 and check,Please refer the [link](https://www.tensorflow.org/install/source_windows)", "@muddham Sorry but the message says `upgrade your bazel installation to version 0.24.1 or higher to build TensorFlow!`. And actually there are no downloads for Visual Studio 2015", "@Stepiii I had the same problem, and running `bazel clean` then trying again solved the issue for me (using vs2019). \r\n\r\nNot sure if this is related, but one thing I did differently was to choose \"no\" for the `override eigen strong inline` option.", "@Stepiii Could you follow @rphsantos suggestion and let us know if it resolved your issue? Thanks!", "I googled this error message and the first hit suggests upgrading your msvc\nversion.\n\nhttps://developercommunity.visualstudio.com/content/problem/533781/internal-compiler-error-c1001-on-1916270301.html\n\n@gunan is it possible to detect during build time if someone is running\nwith an old version and make the build fail?\n\nOn Sat, May 18, 2019, 5:41 PM Vishnuvardhan Janapati <\nnotifications@github.com> wrote:\n\n> @Stepiii <https://github.com/Stepiii> Could you follow @rphsantos\n> <https://github.com/rphsantos> suggestion and let us know if it resolved\n> your issue? Thanks!\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28211?email_source=notifications&email_token=AABEZBZJM7GWNMXOVAJMG7LPWAIQJA5CNFSM4HI4AD3KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVWPXMQ#issuecomment-493681586>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABEZB6Y6O7MGPTLMWX2TBTPWAIQJANCNFSM4HI4AD3A>\n> .\n>\n", "@r4nt is this something that we can guard against during the build?", "I think this is a duplicate of #28309\r\ntl;dr we'll need to make XLA work on Windows; we're currently looking into upgrading the versions of VS we build with, after which that should be more tractable.", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@closing this issue as it is duplicate of https://github.com/tensorflow/tensorflow/issues/28309. Please let me know if I'm mistaken.Thanks!\r\n"]}, {"number": 28210, "title": "tf.image.per_image_standardization respects input dtype", "body": "Make it clear that it can operate on batches of image", "comments": ["@chrisyeh96 can you please check ubuntu sanity errors ?", "Whoops, turns out I went over the 80-chars-per-line limit. I also took this opportunity to make the `tf.image.per_image_standardization` respect the input image's dtype. Please review. Thank you!", "where should I write the unit tests?", "tensorflow/python/ops/image_ops_test.py\n\nOn Wed, May 1, 2019 at 4:30 PM Christopher Yeh <notifications@github.com>\nwrote:\n\n> where should I write the unit tests?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/28210#issuecomment-488487838>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHROKNFLRUQFRCYB7QRTPTIRXTANCNFSM4HI34UMQ>\n> .\n>\n\n\n-- \n - Alex\n", "@chrisyeh96 Did you get a chance to look on reviewer comments. Please let us know on the update. Thanks!", "@chrisyeh96 gentle ping to address reviewer comments. Thanks! ", "Just added unit tests! @alextp ", "@alextp It ran into linter errors, so I fixed those and rebased master as well (hence the force-push). If you can take another look, that would be great. Thanks!", "@alextp I don't think the Windows build failure is due to any of the changes I introduced. What does the pull-request process look like from here on out?", "@chrisyeh96 we'll try to submit until CI passes", "> \r\n> \r\n> @chrisyeh96 we'll try to submit until CI passes\r\n\r\n@alextp Do the internal CI builds automatically re-run every time new commits are merged into master? Or is there someone (perhaps yourself) manually requesting that the CI runs?", "@alextp are there any updates on this? Thanks!", "Still retrying... :-/\r\n", "Thanks @alextp! Really appreciate it!"]}, {"number": 28209, "title": "find_cuda: fix crash caused by unicode chars in ldconfig output", "body": "`find_cuda_config.py` crashes, if Unicode chars are found in `ldconfig` output:\r\n```\r\nTraceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 463, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 455, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 418, in find_cuda_config\r\n    _get_default_cuda_paths(cuda_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 159, in _get_default_cuda_paths\r\n    ] + _get_ld_config_paths()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 139, in _get_ld_config_paths\r\n    match = pattern.match(line.decode(\"ascii\"))\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 27: ordinal not in range(128)\r\nAsking for detailed CUDA configuration...\r\n```\r\nLocalized versions of `ldconfig` versions use Unicode chars, e.g.: `\u00bb/etc/ld.so.cache\u00ab`, causing the crash.\r\n\r\nThe fix simply skips lines with Unicode chars.", "comments": ["This line also fixed it for me, don't know if it is a (more?) generic one:\r\n`match = pattern.match(line.decode(sys.stdin.encoding))`", "Yes, that would fix the encoding generally. So I would be happy with this too.\r\nIt could potentially unleash Unicode-filled paths on the rest of the software, if someone would use those, not sure if that would be a problem.\r\nIn most cases it seems just the first line of the output of `ldconfig -p`, the header-line, that causes the problem, since it is the only part that is localized and thus can contain Unicode chars:\r\n```bash\r\n4403 Bibliotheken im Cache \u00bb/etc/ld.so.cache\u00ab gefunden\r\n        libzvbi.so.0 (libc6,x86-64) => /usr/lib/libzvbi.so.0\r\n....\r\n ```\r\nE.g. for German locale, the chars `\u00bb` and `\u00ab` in the header cause the issue.", "Yes, that was probably also the case in my case ;) Funnily only when calling ./configure from the command line, not when executing the code in ipython or spyder.", "@skye can you please review this? Thanks!"]}, {"number": 28208, "title": "Which API can implement tensor expansion in tensorflow \uff1f", "body": "\r\nIf  I have a tensor of (30,40,50), and I want to expand it out to the first order, then I get a second order tensor of (30,2000), and I don't know if tensorflow has an API that implements it.\r\nFor example\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\ndata1=tf.constant([[[2,5,7,8],[6,4,9,10],[14,16,86,54]],\r\n                 [[16,43,65,76],[43,65,7,24],[15,75,23,75]]])\r\n\r\ndata5=tf.reshape(data1,[3,8])\r\n\r\ndata2,data3,data4=tf.split(data1,3,1)\r\ndata6=tf.reshape(data2,[1,8])\r\ndata7=tf.reshape(data3,[1,8])\r\ndata8=tf.reshape(data4,[1,8])\r\ndata9=tf.concat([data6,data7,data8],0)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(data5))\r\n    print(sess.run(data))\r\n\r\ndata5\r\n [[ 2  5  7  8  6  4  9 10]\r\n [14 16 86 54 16 43 65 76]\r\n [43 65  7 24 15 75 23 75]]\r\n\r\ndata9\r\n[[ 2  5  7  8 16 43 65 76]\r\n [ 6  4  9 10 43 65  7 24]\r\n [14 16 86 54 15 75 23 75]]\r\n\r\n\r\nHow do I get data9 directly(Or maybe I got it wrong)\r\n\r\n", "comments": ["@goljx Request you to use the reference [link1](https://stackoverflow.com/questions/55878846/which-api-can-implement-tensor-expansion-in-tensorflow),[link TF1.13](https://www.tensorflow.org/api_docs/python/tf/keras/backend/expand_dims?hl=en), [link TF1.13](url),[link TF2.0](https://www.tensorflow.org/versions/r2.0/api_docs/cc/class/tensorflow/ops/expand-dims?hl=en).\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I think tf. Reshape just realized dimension to get data5, and I want to get is mathematical dimension to get data9.\r\nDgumo in stackoverflow suggested I can try data9 = tf.layers.flatten(tf.transpose(data1, 0, 2)). Actually I could get what I want from this.\r\nAnd latter I found I could get the right result by data9=tl.unfold(data1, mode=2) by  import tensorly as tl             \r\nand from tensorly.decomposition import parafac"]}, {"number": 28207, "title": "Where is F1_score located in tf.keras", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 1.13\r\nUbuntu 14.04 LTS\r\n- Are you willing to contribute it (Yes/No): YES\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nBasically there was an previous issue regarding multi-lable classifier functionalities like(micro,macro) are not present in F1_score calculation. So I tried to add these features but later wards Alextp asked me to add this features in tf.keras as tensorflow is now gong to depend on that for TF2.0, but I am not getting where F1_score is defined in tf.Keras?\r\n\r\n**Will this change the current api? How?**\r\nNO\r\n\r\n**Who will benefit with this feature?**\r\nAll those who wants multi-lable classifiers to be added to F1_score calculations\r\n**Any Other info.**\r\n", "comments": ["We do not have F1 score metric out of the box in tf.keras. If you think we should add it may be we can add to the AddOns repository (https://github.com/tensorflow/addons) and based on how widely it is being used we can move it to TensorFlow core.", "Tensorflow has f1_score defined here:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/metrics/python/metrics/classification.py\r\nWhats the problem if I contribute modifications there?", "Contrib module is deprecated in TF 2.0. AddOns repository exists for such use cases so this would be a good fit there."]}, {"number": 28206, "title": "Can't convert to Tensorflow Lite with operation Cast", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.12.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"predict_tflite.py\", line 740, in <module>\r\n    evaluatePlanes(args)\r\n  File \"predict_tflite.py\", line 95, in evaluatePlanes\r\n    predictions = getResults(options)\r\n  File \"predict_tflite.py\", line 289, in getResults\r\n    pred_dict = getPredictionCustom(options)\r\n  File \"predict_tflite.py\", line 538, in getPredictionCustom\r\n    tflite_model = converter.convert()\r\n  File \"/home/alexfu/PlaneNet/venv/local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 453, in convert\r\n    **converter_kwargs)\r\n  File \"/home/alexfu/PlaneNet/venv/local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 342, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/alexfu/PlaneNet/venv/local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\n2019-04-27 02:28:16.893332: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1517 operators, 2341 arrays (0 quantized)\r\n2019-04-27 02:28:16.938974: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1517 operators, 2341 arrays (0 quantized)\r\n2019-04-27 02:28:16.939290: F tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_unary.cc:164] Unsupported cast op input type\r\nAborted (core dumped)\r\n\r\nNone\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nhttps://mega.nz/#!sjpT2DiQ!Uo-6hxyldmtnPoKk3TTdUHKZADRGy6nIPlmAeVzJs_8\r\n(Actually using DeepLab in this model)\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@futuyao : Did you get chance to have a look on this [link](https://www.tensorflow.org/lite/guide/ops_compatibility). Let us know if that helps. Thanks!", "Cast is a supported operation. But in my model, I'm using operation Cast with input type Bool and output type float32. I'm not sure if casting from input type Bool is supported. Do you have more information on this?\r\n\r\nMore details here at line 234 at tensorflow/lite/toco/graph_transformations/resolve_constant_unary.cc:\r\n```\r\nif (unary_op->type == OperatorType::kCast) {\r\n    for (int i = 0; i < output_buffer_size; i++) {\r\n      float outval = 0.0f;\r\n      if (input_array.buffer->type == ArrayDataType::kFloat) {\r\n        outval = static_cast<float>(\r\n            input_array.GetBuffer<ArrayDataType::kFloat>().data[i]);\r\n      } else if (input_array.buffer->type == ArrayDataType::kUint8) {\r\n        outval = static_cast<float>(\r\n            input_array.GetBuffer<ArrayDataType::kUint8>().data[i]);\r\n      } else if (input_array.buffer->type == ArrayDataType::kInt32) {\r\n        outval = static_cast<float>(\r\n            input_array.GetBuffer<ArrayDataType::kInt32>().data[i]);\r\n      } else if (input_array.buffer->type == ArrayDataType::kInt64) {\r\n        outval = static_cast<float>(\r\n            input_array.GetBuffer<ArrayDataType::kInt64>().data[i]);\r\n      } else {\r\n        LOG(FATAL) << \"Unsupported cast op input type\";\r\n      }\r\n      output_float_data[i] = outval;\r\n    }\r\n```", "@futuyao  I think cast operation does not support bool input. ", "> @futuyao I think cast operation does not support bool input.\r\n\r\nI've made a small change to the converter code, and it should support bool after the fix. See #28315 .", "@futuyao Can you submit this PR against master? . Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "> > @futuyao I think cast operation does not support bool input.\r\n> \r\n> I've made a small change to the converter code, and it should support bool after the fix. See #28315 .\r\n\r\nI am facing the same issue while converting model to TFLite. can you please tell how you solved it? ", "@ksevta This issue is now fixed on master, try converting with RC 2 toco."]}, {"number": 28205, "title": "Optimized packet load in gemm_pack_rhs (for Packet16f and Packet8f)", "body": "These are the TensorFlow changes that use the recent Eigen update where\r\nwe had added the masked loads. These changes target loads in gemm_pack_rhs.\r\n\r\nWhen packet is split across 2 adjacent columns, we read it as two\r\n'partial' packets and then merge these into 1. Currently this only works\r\nfor Packet16f (AVX512) and Packet8f (AVX2). We plan to add this for\r\nother types of packets (such as Packet8d) also.\r\n\r\nThis optimization shows significant speedup in SpatialConvolution with\r\ncertain parameters. Some examples are below.\r\n\r\nBenchmark parameters are specified as:\r\nBatch size, Input dim, Depth, Num of filters, Filter dim\r\n\r\nSpeedup numbers are specified for number of threads 1, 2, 4, 8, 16.\r\n\r\nAVX512:\r\n\r\nParameters                  | Speedup (Num of threads: 1, 2, 4, 8, 16)\r\n----------------------------|------------------------------------------\r\n128,   24x24,  3, 64,   5x5 |2.18X, 2.13X, 1.73X, 1.64X, 1.66X\r\n128,   24x24,  1, 64,   8x8 |2.00X, 1.98X, 1.93X, 1.91X, 1.91X\r\n 32,   24x24,  3, 64,   5x5 |2.26X, 2.14X, 2.17X, 2.22X, 2.33X\r\n128,   24x24,  3, 64,   3x3 |1.51X, 1.45X, 1.45X, 1.67X, 1.57X\r\n 32,   14x14, 24, 64,   5x5 |1.21X, 1.19X, 1.16X, 1.70X, 1.17X\r\n128, 128x128,  3, 96, 11x11 |2.17X, 2.18X, 2.19X, 2.20X, 2.18X\r\n\r\nAVX2:\r\n\r\nParameters                  | Speedup (Num of threads: 1, 2, 4, 8, 16)\r\n----------------------------|------------------------------------------\r\n128,   24x24,  3, 64,   5x5 | 1.66X, 1.65X, 1.61X, 1.56X, 1.49X\r\n 32,   24x24,  3, 64,   5x5 | 1.71X, 1.63X, 1.77X, 1.58X, 1.68X\r\n128,   24x24,  1, 64,   5x5 | 1.44X, 1.40X, 1.38X, 1.37X, 1.33X\r\n128,   24x24,  3, 64,   3x3 | 1.68X, 1.63X, 1.58X, 1.56X, 1.62X\r\n128, 128x128,  3, 96, 11x11 | 1.36X, 1.36X, 1.37X, 1.37X, 1.37X\r\n\r\nIn the higher level benchmark cifar10, we observe a runtime improvement\r\nof around 6% for AVX512 on Intel Skylake server (8 cores).\r\n\r\nOn lower level PackRhs micro-benchmarks specified in\r\ntensorflow/core/kernels/eigen_spatial_convolutions_test.cc, we observe\r\nthe following runtime numbers:\r\n\r\nAVX512:\r\n\r\nParameters                                                     | Runtime without patch (ns) | Runtime with patch (ns) | Speedup\r\n---------------------------------------------------------------|----------------------------|-------------------------|---------\r\nBM_RHS_NAME(PackRhs, 128, 24, 24, 3, 64, 5, 5, 1, 1, 256, 56)  |  41350                     | 15073                   | 2.74X\r\nBM_RHS_NAME(PackRhs, 32, 64, 64, 32, 64, 5, 5, 1, 1, 256, 56)  |   7277                     |  7341                   | 0.99X\r\nBM_RHS_NAME(PackRhs, 32, 64, 64, 32, 64, 5, 5, 2, 2, 256, 56)  |   8675                     |  8681                   | 1.00X\r\nBM_RHS_NAME(PackRhs, 32, 64, 64, 30, 64, 5, 5, 1, 1, 256, 56)  |  24155                     | 16079                   | 1.50X\r\nBM_RHS_NAME(PackRhs, 32, 64, 64, 30, 64, 5, 5, 2, 2, 256, 56)  |  25052                     | 17152                   | 1.46X\r\nBM_RHS_NAME(PackRhs, 32, 256, 256, 4, 16, 8, 8, 1, 1, 256, 56) |  18269                     | 18345                   | 1.00X\r\nBM_RHS_NAME(PackRhs, 32, 256, 256, 4, 16, 8, 8, 2, 4, 256, 56) |  19468                     | 19872                   | 0.98X\r\nBM_RHS_NAME(PackRhs, 32, 64, 64, 4, 16, 3, 3, 1, 1, 36, 432)   | 156060                     | 42432                   | 3.68X\r\nBM_RHS_NAME(PackRhs, 32, 64, 64, 4, 16, 3, 3, 2, 2, 36, 432)   | 132701                     | 36944                   | 3.59X\r\n\r\nAVX2:\r\n\r\nParameters                                                     | Runtime without patch (ns) | Runtime with patch (ns) | Speedup\r\n---------------------------------------------------------------|----------------------------|-------------------------|---------\r\nBM_RHS_NAME(PackRhs, 128, 24, 24, 3, 64, 5, 5, 1, 1, 256, 56)  | 26233                      | 12393                   | 2.12X\r\nBM_RHS_NAME(PackRhs, 32, 64, 64, 32, 64, 5, 5, 1, 1, 256, 56)  |  6091                      |  6062                   | 1.00X\r\nBM_RHS_NAME(PackRhs, 32, 64, 64, 32, 64, 5, 5, 2, 2, 256, 56)  |  7427                      |  7408                   | 1.00X\r\nBM_RHS_NAME(PackRhs, 32, 64, 64, 30, 64, 5, 5, 1, 1, 256, 56)  | 23453                      | 20826                   | 1.13X\r\nBM_RHS_NAME(PackRhs, 32, 64, 64, 30, 64, 5, 5, 2, 2, 256, 56)  | 23167                      | 22091                   | 1.09X\r\nBM_RHS_NAME(PackRhs, 32, 256, 256, 4, 16, 8, 8, 1, 1, 256, 56) | 23422                      | 23682                   | 0.99X\r\nBM_RHS_NAME(PackRhs, 32, 256, 256, 4, 16, 8, 8, 2, 4, 256, 56) | 23165                      | 23663                   | 0.98X\r\nBM_RHS_NAME(PackRhs, 32, 64, 64, 4, 16, 3, 3, 1, 1, 36, 432)   | 72689                      | 44969                   | 1.62X\r\nBM_RHS_NAME(PackRhs, 32, 64, 64, 4, 16, 3, 3, 2, 2, 36, 432)   | 61732                      | 39779                   | 1.55X\r\n\r\nAll benchmarks on Intel Skylake server with 8 cores.", "comments": ["Hi Eugene. I have now made the following changes to the patch:\r\n\r\n1.  Removed `MaskType` from template parameter list for `OptimizedPacketLoadOverTwoColumns`. Instead, `unpacket_traits<PacketType>::mask_t` is used. This also removes the `unsigned long long` from the `enable_if` later.\r\n2. Minor refactoring of `loadPartialPacketStandard`, `loadPacketStandardFromTwoColumns`, `loadPacketStandardFromSingleColumn` and `loadPacketStandard` for (hopefully) improved readability. This includes:\r\n  a. Renamed variables to drop 'Part' suffix.\r\n  b. Reuse variable names as much as possible esp. where they \"mean\" the same for partial packet and packet.\r\n  c. Added some comments explaining the intended behavior of code blocks.\r\n"]}, {"number": 28204, "title": "Compute F1 score multilabel classifier #27171 #27446", "body": "Compute F1 score multilabel classifier #27171 ", "comments": ["@alextp I know you closed same PR for not affecting the code base but I literally want to work on this issue s can you please tell me the files where I can add these changes so that it can affect the files.", "@shashvatshahi1998 This is a one-file change which only affects private symbols in TF. It also has no tests and no documentation.\r\n\r\nSo I don't understand how this tries to fix the issue.", "@shashvatshahi1998  Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks!", "@shashvatshahi1998 gentle ping to check reviewer comments and let us know on the update. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28203, "title": "404 error on TensorFlow Lite model repository", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:\r\n- Doc Link:\r\n\r\n\r\n**Describe the documentation issue**\r\nThis link is unreachable\r\n```\r\nTable 1: Top-1 accuracy of floating point and fully quantized CNNs on Imagenet Validation dataset.\r\nOur pre-trained models are available in the TensorFlow Lite model repository. The code used to generate these models is available.\r\n```\r\n\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@1330154737 : Unfortunately did not get the link. Can you please provide the link to check the issue. Thanks! "]}, {"number": 28202, "title": "This is to workaround a problem with the following use pattern,", "body": "     op0 : VarHandleOp\r\n     op1 : Switch op0, pred\r\n     op2 : ReadVariableOp op0\r\n     op3 : ReadVariableOp op1\r\n\r\n  If op2 and op3 are clustered, the op0 and op1 are inputs,\r\n  and they are both considered as resources, and\r\n  op1 and op0 happen to point to the same resource, and\r\n  this will cause an execution error since the resource is\r\n  not allowed to be initialized (locked) twice.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28202) for more info**.\n\n<!-- need_sender_cla -->", "@xykong1958 Please sign CLA in order to proceed with next steps. Thank you !", "@xykong1958 gentle ping to sign CLA. Could you please resolve the conflicts? Thanks!", "@xykong1958  Did you get a chance to sign CLA and resolve the conflicts? Thanks!", "@xykong1958 Did you get a chance to sign CLA and resolve the conflicts? Please let us know on the update. Thanks!", "Sorry to say but we cannot proceed until CLA reflects YES. Could you please open a new PR(which may resolve the CLA issue) and try. Closing this."]}, {"number": 28201, "title": "Restore output shapes when loading V1 SavedModels.", "body": "The new `tf.saved_model.load_v2()` API currently ignores any shape information stored in the `SignatureDef`s of the SavedModel. For example, the following code snippet loads an object detection model from the [TensorFlow detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md):\r\n```python\r\nmodel_dir = \"./ssd_mobilenet_v1_coco_2018_01_28/saved_model\"\r\ntrackable = tf.saved_model.load_v2(model_dir)\r\nprint(trackable.signatures[\"serving_default\"].outputs)\r\n```\r\nThis code currently produces the following output:\r\n```\r\n[<tf.Tensor 'detection_boxes:0' shape=<unknown> dtype=float32>,\r\n<tf.Tensor 'detection_classes:0' shape=<unknown> dtype=float32>,\r\n<tf.Tensor 'detection_scores:0' shape=<unknown> dtype=float32>,\r\n<tf.Tensor 'num_detections:0' shape=<unknown> dtype=float32>]\r\n```\r\nNote the `shape=<unknown>` for each output.\r\n\r\nThis PR modifies the behavior of `tensorflow.python.saved_model.load_v1_in_v2.load()` such that the output shapes in the returned `ConcreteFunction` match the output shapes in the original SavedModel. After these changes, the code snippet above produces the following output:\r\n```\r\n[<tf.Tensor 'detection_boxes:0' shape=(?, 100, 4) dtype=float32>,\r\n <tf.Tensor 'detection_classes:0' shape=(?, 100) dtype=float32>,\r\n <tf.Tensor 'detection_scores:0' shape=(?, 100) dtype=float32>,\r\n <tf.Tensor 'num_detections:0' shape=(?,) dtype=float32>]\r\n```\r\nNote the additional shape information that wasn't there before.\r\n\r\nMost of the code modifications are in `WrappedFunction.prune()`, which I have extended to accept `TensorInfo` objects for fetches in addition to `Tensor` and `Operation` objects. I've included an automated regression test for the new functionality. I also added some documentation to `WrappedFunction.prune()`.\r\n", "comments": ["@frreiss can you please check build failures and resolve the conflict. Thanks!", "Conflicts resolved, starting a test build now.", "@frreiss could you please resolve the conflicts? Thanks!", "Conflicts resolved, rerunning regression tests now.", "@frreiss could you please resolve the conflicts? Thanks!", "Running the submit with some minor tweaks to resolve yet another merge conflict; you shouldn't need to do anything else. Sorry about all of those.", "Thanks, @allenlavoie ! For what it's worth, my own test build of the branch after my manual merge just completed without errors."]}]