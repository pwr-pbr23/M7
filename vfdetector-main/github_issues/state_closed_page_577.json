[{"number": 36375, "title": "AttributeError: 'Callback' object has no attribute 'validation_data'", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback\r\n\r\n## Description of issue (what needs changing):\r\n\r\nCurrently, the page https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback says `validation_data: Deprecated. Do not use.`, but if I attempt to access the attribute `validation_data` inside the callback I get the error `AttributeError: 'MyCustomCallbackClass' object has no attribute 'validation_data'.`. You should change the documentation to remove the attribute `validation_data`, which apparently was removed.\r\n\r\nSee also this issue https://github.com/tensorflow/tensorflow/issues/27318.", "comments": ["I would like to help with this issue. Is this just a case of having the documentation instead state that validation_data has been removed/ removing the line entirely?", "@joshz123 Have you seen https://github.com/tensorflow/tensorflow/pull/36414?", "@nbro no I had not, I guess that answers my question. Thanks!", "@nbro \r\nas there is pr to monitor this issue, pleas let us know if we can move this issue to closed status.", "@Saduf2019 If the PR is supposed to solve the problem and integrate the solution into the next TF release, yes, you can close this issue, otherwise, you should leave it opened.", "@nbro This was resolved already by this [fix](https://github.com/tensorflow/tensorflow/commit/1286b957c21eea2e956a51fcd758653ed7e7cb36). I cannot see `validation_data` attribute in the  [TF callback page](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback). \r\n\r\nCan you please verify once and close the issue if this was resolved for you. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I still have this issue today. Could I know if it has been solved?\r\n", "@ying-hao As this an old issue, can you please open a new issue with a simple standalone code to reproduce the issue? Thanks!"]}, {"number": 36374, "title": "regarding hosting of .h5 model using tfserving", "body": "hi, can I host .h5 model through tf serving, or it just supports .tfsavedformat models for hosting. please let me know if I can host it , and if you can suggest me an article for the support", "comments": ["please reply", "@divyag11 ,\r\nHi this issue looks like is related to tf-serving, can you please open new issue in [serving](https://github.com/tensorflow/serving/issues/new) repo?Thanks!"]}, {"number": 36373, "title": "AUCROC > 80% with sensitivity 1.0 and specificity 0.0?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla K80 11441MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTrue negatives and false negatives are 0, sensitivity computes to 1.0 and specificity to 0.0, but the AUCROC is giving 80%.\r\n\r\n**Describe the expected behavior**\r\nIsn't AUCROC used a single number to represent performance across the entire confusion matrix of true positives, true negatives, false positives and false negatives?\r\n\r\n**Code to reproduce the issue**\r\nThis code was used to parse the TensorBoard data into Pandas: https://gist.github.com/ptschandl/ef67bbaa93ec67aba2cab0a7af47700b\r\n\r\nWith the basic formula for both:\r\n```\r\n'sensitivity': np.divide(tp, np.add(tp, fn)),\r\n'specificity': np.divide(tn, np.add(tn, fp)),\r\n```\r\n\r\n**Other info / logs**\r\nMetrics included AUCROC, and all 4 metrics of the confusion matrix taken at threshold 50.\r\n<img width=\"918\" alt=\"Screen Shot 2020-01-29 at 9 26 16 pm\" src=\"https://user-images.githubusercontent.com/807580/73541346-4c9ebf80-4486-11ea-8e03-aba7c2209f9b.png\">", "comments": ["@SamuelMarks,\r\nI was unable to reproduce the issue from the link given above. Please find the Gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/faeada87cf68c37d640bac07ac5539da/36373.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Any updates regarding the complete reproducible code? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36372, "title": "More advanced ways of collecting metrics in tf.keras while training models", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Maybe\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, in `tf.keras`,  we can access the metrics by accessing the attribute `metrics` (a list) and `metrics_names` (list) of the model. Furthermore, when developing a callback, e.g. in the definition `def on_epoch_end(epoch, logs)`, `logs` is a dictionary whose keys are the names and whose values the corresponding values of the metrics, including the loss. \r\n\r\nIf you have a neural network with multiple outputs, all the metrics for each of these separate outputs are elements of `metrics` (or `metric_names`) and the dictionary `logs`. AFAIK, there's no direct way of getting e.g. the metrics for an output with name `\"outpu1\"`. This feature would be useful in the case we want to plot all (or a subset of) the metrics associated with one output. The model object could provide a method or attribute, `get_metrics_of_output(output_name)`, where the metrics are returned as an object (e.g. a dictionary) where the metrics associated with one output are collected in one separate list (or dictionary, etc) than the metrics associated with another output, so that `get_metrics_of_output(output_name)` returns the metrics of the output layer with name `output_name`. Furthermore, there should also be a way of differentiating between certain metrics associated with the **same** output. This could be determined by the way we define and pass the metrics to the `metrics` parameter of the `compile` method (e.g. as a dictionary). \r\n\r\nIn [this Stack Overflow question](https://stackoverflow.com/q/59996177/3924118), I am describing my current issue where this feature could be useful.\r\n\r\n**Will this change the current api? How?**\r\n\r\nProbably. You could just introduce a new method that returns a more advanced view of the metrics. `model.metrics` could still return just the flattened version of the metrics (as it is currently the case). However, `logs` (e.g. in `def on_epoch_end(epoch, logs)`) probably should change to return the more advanced view of the metrics. Or maybe there should be a third parameter that returns this advanced view of the metrics, e.g. `def on_epoch_end(epoch, logs, metrics)`.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone.\r\n\r\n**Any Other info.**\r\n", "comments": ["@nbro,\r\nCan you please take a look at [Tensorflow Model Analysis](https://www.tensorflow.org/tfx/model_analysis/get_started), which provides a facility to play around with **`Metrics`** of a **`Machine/Deep Learning Model`**, and see if it helps? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 36371, "title": "cannot import name 'resnet '", "body": "I installed tensorflow using the following command\r\n`conda install tensorflow-gpu`\r\n`conda install keras-gpu`\r\nthen\r\n`import tensorflow`\r\nI get the following error:\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow/__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/__init__.py\", line 83, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py\", line 26, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\r\n    from tensorflow.python.keras import applications\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/__init__.py\", line 64, in <module>\r\n    from tensorflow.python.keras.applications.resnet import ResNet50\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/resnet.py\", line 22, in <module>\r\n    from keras_applications import resnet\r\nImportError: cannot import name 'resnet'\r\n\r\n**System information**\r\n-ubuntu18.04\r\n\r\nanaconda env:\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main    defaults\r\n_tflow_select             2.1.0                       gpu    defaults\r\nabsl-py                   0.8.1                    py36_0    defaults\r\nastor                     0.8.0                    py36_0    defaults \r\nblas                      1.0                         mkl    defaults\r\nc-ares                    1.15.0            h7b6447c_1001    defaults\r\nca-certificates           2020.1.1                      0    defaults\r\ncertifi                   2019.11.28               py36_0    defaults                                               \r\ncudatoolkit               10.0.130                      0    defaults\r\ncudnn                     7.6.5                cuda10.0_0    defaults                                                    \r\ncupti                     10.0.130                      0    defaults\r\ngast                      0.2.2                    py36_0    defaults                                                  \r\ngoogle-pasta              0.1.8                      py_0    defaults\r\ngrpcio                    1.16.1           py36hf8bcb03_1    defaults                                            \r\nh5py                      2.10.0           py36h7918eee_0    defaults\r\nhdf5                      1.10.4               hb1b8bf9_0    defaults               \r\nintel-openmp              2019.4                      243    defaults\r\nkeras-applications        1.0.8                      py_0    defaults                                                           \r\nkeras-base                2.2.4                    py36_0    defaults\r\nkeras-gpu                 2.2.4                         0    defaults                                                                 \r\nkeras-preprocessing       1.1.0                      py_1    defaults\r\nld_impl_linux-64          2.33.1               h53a641e_7    defaults                                                                 \r\nlibedit                   3.1.20181209         hc058e9b_0    defaults\r\nlibffi                    3.2.1                hd88cf55_4    defaults                                                                              \r\nlibgcc-ng                 9.1.0                hdf63c60_0    defaults\r\nlibgfortran-ng            7.3.0                hdf63c60_0    defaults                                                                            \r\nlibprotobuf               3.11.2               hd408876_0    defaults\r\nlibstdcxx-ng              9.1.0                hdf63c60_0    defaults\r\nmarkdown                  3.1.1                    py36_0    defaults\r\nmkl                       2019.4                      243    defaults\r\nmkl-service               2.3.0            py36he904b0f_0    defaults\r\nmkl_fft                   1.0.15           py36ha843d7b_0    defaults\r\nmkl_random                1.1.0            py36hd6b4f25_0    defaults\r\nncurses                   6.1                  he6710b0_1    defaults\r\nnumpy                     1.18.1           py36h4f9e942_0    defaults\r\nnumpy-base                1.18.1           py36hde5b4d6_1    defaults\r\nopenssl                   1.1.1d               h7b6447c_3    defaults\r\nopt_einsum                3.1.0                      py_0    defaults\r\npip                       20.0.2                   py36_1    defaults\r\nprotobuf                  3.11.2           py36he6710b0_0    defaults\r\npython                    3.6.10               h0371630_0    defaults\r\npyyaml                    5.2              py36h7b6447c_0    defaults\r\nreadline                  7.0                  h7b6447c_5    defaults\r\nscipy                     1.3.2            py36h7c811a0_0    defaults\r\nsetuptools                45.1.0                   py36_0    defaults\r\nsix                       1.14.0                   py36_0    defaults\r\nsqlite                    3.30.1               h7b6447c_0    defaults\r\ntensorboard               2.0.0              pyhb38c66f_1    defaults\r\ntensorflow                2.0.0           gpu_py36h6b29c10_0    defaults\r\ntensorflow-base           2.0.0           gpu_py36h0ec5d1f_0    defaults\r\ntensorflow-estimator      2.0.0              pyh2649769_0    defaults\r\ntensorflow-gpu            2.0.0                h0d30ee6_0    defaults\r\ntermcolor                 1.1.0                    py36_1    defaults\r\ntk                        8.6.8                hbc83047_0    defaults\r\nwerkzeug                  0.16.1                     py_0    defaults\r\nwheel                     0.34.1                   py36_0    defaults\r\nwrapt                     1.11.2           py36h7b6447c_0    defaults\r\nxz                        5.2.4                h14c3975_4    defaults\r\nyaml                      0.1.7                had09818_2    defaults\r\nzlib                      1.2.11               h7b6447c_3    defaults\r\n(tfenv) nnir712@nnir712-Lenovo:~/temp$ python\r\nPython 3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 21:14:29)", "comments": ["I would try `conda remove keras-gpu`\r\n\r\nAs TensorFlow 2.0 comes with tf.keras, you don't want to use the keras conda package. (Which at version 2.2.4 only works with TensorFlow 1.X)", "> I would try `conda remove keras-gpu`\r\n> \r\n> As TensorFlow 2.0 comes with tf.keras, you don't want to use the keras conda package. (Which at version 2.2.4 only works with TensorFlow 1.X)\r\n\r\nI will use keras.then. use  `pip install keras==2.0.8` ?", "If using TensorFlow 2.0, you want to use tf.keras, all you need to install would be tensorflow-gpu.\r\n\r\nhttps://www.tensorflow.org/guide/keras", "> If using TensorFlow 2.0, you want to use tf.keras, all you need to install would be tensorflow-gpu.\r\n> \r\n> https://www.tensorflow.org/guide/keras\r\n\r\nok.\r\n I am learning tensorflow. But some problems seem to appear. I try to run the [code ](https://www.tensorflow.org/tutorials/images/cnn). But found the following error\r\n\r\n`Instructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\nTrain on 50000 samples, validate on 10000 samples\r\nEpoch 1/10\r\n2020-02-01 06:34:31.435605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there mu\r\nst be at least one NUMA node, so returning NUMA node zero\r\n2020-02-01 06:34:31.436650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.77GiB\r\n2020-02-01 06:34:31.436675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2020-02-01 06:34:31.784720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-01 06:34:31.784761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0\r\n2020-02-01 06:34:31.784770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N\r\n2020-02-01 06:34:31.785344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with\r\n10427 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n 2976/50000 [>.............................] - ETA: 42s - loss: nan - acc: 0.99562020-02-01 06:34:34.000984: E tensorflow/stream_executor/cuda/cuda_driver.cc:1110\r\n] could not synchronize on CUDA context: CUDA_ERROR_MISALIGNED_ADDRESS :: *** Begin stack trace ***\r\n        tensorflow::CurrentStackTrace()\r\n        perftools::gputools::cuda::CUDADriver::SynchronizeContext(perftools::gputools::cuda::CudaContext*)\r\n        perftools::gputools::StreamExecutor::SynchronizeAllActivity()\r\n        tensorflow::GPUUtil::SyncAll(tensorflow::Device*)\r\n        tensorflow::BaseGPUDevice::Sync()\r\n\r\n\r\n\r\n        Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\r\n        std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&\r\n)\r\n`\r\n\r\nI'm not sure where the problem is. Will my graphics card hardware be damaged?", "TensorFlow with not damage a graphic card. [The nvidia software is responsible for slowing the card down or shutting it off so as to not overheat.]\r\n\r\nI'm not sure about that error, but one thing to check is to make sure you have the latest NVIDIA driver installed. ", "> TensorFlow with not damage a graphic card. [The nvidia software is responsible for slowing the card down or shutting it off so as to not overheat.]\r\n> \r\n> I'm not sure about that error, but one thing to check is to make sure you have the latest NVIDIA driver installed.\r\n\r\nOk. thank you very much. I will reinstall the graphics driver, cuda and cudnn", "You only need to install the graphic driver. Because your using Conda, it will install cuda and cudnn as conda package (saving you work)   (For more info: https://stackoverflow.com/questions/59529804/nvidia-cudatoolkit-vs-conda-cudatoolkit)", "> You only need to install the graphic driver. Because your using Conda, it will install cuda and cudnn as conda package (saving you work) (For more info: https://stackoverflow.com/questions/59529804/nvidia-cudatoolkit-vs-conda-cudatoolkit)\r\n\r\nWhen I use conda install tensorflow-gpu\r\nconda is installed:\r\n  keras-applications pkgs/main/noarch::keras-applications-1.0.8-py_0                                                                                              \r\n  keras-preprocessi~ pkgs/main/noarch::keras-preprocessing-1.1.0-py_1    \r\n`import tensorflow`\r\nKeep getting errors\uff1aImportError: cannot import name 'resnet'\r\nthen\r\n` pip uninstall keras-applications` \r\n` pip uninstall keras-preprocessing` \r\n`import tensorflow`\r\nsuccess!\r\n\r\nBut now it's a problem with the [code](https://www.tensorflow.org/tutorials/images/cnn)\uff01\r\nerror\uff1a\r\n`         [[{{node sequential/conv2d/Conv2D}}]]\r\n   32/50000 [..............................] - ETA: 38:36Traceback (most recent call last):\r\n  File \"tfkeras_train_cifar.py\", line 65, in <module>\r\n    validation_data=(test_images, test_labels))\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError:  cuDNN launch failure : input shape([32,3,32,32]) filter shape([3,3,3,32])\r\n         [[node sequential/conv2d/Conv2D (defined at /home/nnir712/anaconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751)\r\n]] [Op:__inference_distributed_function_1055]\r\n\r\nFunction call stack:\r\ndistributed_function`\r\n\r\ncode:\r\n`from __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import datasets, layers, models\r\nfrom tensorflow.python.keras.datasets.cifar import load_batch\r\nfrom tensorflow.python.keras import backend as K\r\nimport os\r\nimport numpy as np\r\ndef load_data():\r\n  dirname = 'cifar-10-batches-py'\r\n  path = os.path.join('/media/nnir712/I/opendataset',dirname)\r\n\r\n  num_train_samples = 50000\r\n\r\n  x_train = np.empty((num_train_samples, 3, 32, 32), dtype='uint8')\r\n  y_train = np.empty((num_train_samples,), dtype='uint8')\r\n\r\n  for i in range(1, 6):\r\n    fpath = os.path.join(path, 'data_batch_' + str(i))\r\n    (x_train[(i - 1) * 10000:i * 10000, :, :, :],\r\n     y_train[(i - 1) * 10000:i * 10000]) = load_batch(fpath)\r\n\r\n  fpath = os.path.join(path, 'test_batch')\r\n  x_test, y_test = load_batch(fpath)\r\n\r\n  y_train = np.reshape(y_train, (len(y_train), 1))\r\n  y_test = np.reshape(y_test, (len(y_test), 1))\r\n\r\n  if K.image_data_format() == 'channels_last':\r\n    x_train = x_train.transpose(0, 2, 3, 1)\r\n    x_test = x_test.transpose(0, 2, 3, 1)\r\n\r\n  x_test = x_test.astype(x_train.dtype)\r\n  y_test = y_test.astype(y_train.dtype)\r\n\r\n  return (x_train, y_train), (x_test, y_test)\r\n  \r\n(train_images, train_labels), (test_images, test_labels) = load_data()\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\r\n               'dog', 'frog', 'horse', 'ship', 'truck']\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.summary()\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10, activation='softmax'))\r\nmodel.summary()\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nhistory = model.fit(train_images, train_labels, epochs=10, \r\n                    validation_data=(test_images, test_labels))\r\n                    \r\ntest_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\r\nprint(test_acc)`\r\n", "@zihaozhang9,\r\nI was able to run the above code without any issues. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/7ff98a538316fe22ec11cc92ea7e3276/36371.ipynb). Thanks!", "> @zihaozhang9,\r\n> I was able to run the above code without any issues. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/7ff98a538316fe22ec11cc92ea7e3276/36371.ipynb). Thanks!\r\nThank you very much for your continued attention!\r\nThe day before yesterday, I tried many versions of graphics driver, CUDA, cudnn and tensorflow.\r\nThen I discovered a magical phenomenon, when only 10 pictures are loaded in the graphics card, it can be trained normally. When loading 100 pictures, the video memory uses about 3G, and then reports an error.\r\nFrom this I suspect that it is not the version that is wrong, it should be the graphics card hardware failure.I will contact nvidia's technical engineer to repair the graphics card.\r\n", "@zihaozhang9,\r\nThank you for the update. Please feel free to close the issue if it is resolved. Thanks!", "It is a bit late but I've got the same problem and it is not implemented in your version\r\nhttps://github.com/tensorflow/tensorflow/issues/29040"]}, {"number": 36370, "title": "error converting to tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.14.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, EXP, EXPAND_DIMS, FILL, GATHER, GREATER, GREATER_EQUAL, LESS, LOGICAL_AND, LOGISTIC, MAXIMUM, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, SELECT, SHAPE, SLICE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TILE, TOPK_V2, TRANSPOSE, UNPACK, WHERE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Enter, Exit, LoopCond, Merge, NonMaxSuppressionV3, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\nTraceback (most recent call last):\r\n  File \"c:\\program files\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\program files\\python36\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Program Files\\Python36\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\program files\\python36\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\program files\\python36\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"c:\\program files\\python36\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\program files\\python36\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, EXP, EXPAND_DIMS, FILL, GATHER, GREATER, GREATER_EQUAL, LESS, LOGICAL_AND, LOGISTIC, MAXIMUM, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, SELECT, SHAPE, SLICE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TILE, TOPK_V2, TRANSPOSE, UNPACK, WHERE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Enter, Exit, LoopCond, Merge, NonMaxSuppressionV3, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\n\r\n", "comments": ["You may try and use the `target_spec.supported_ops` argument in the TensorFlow Lite converter.\r\nSee https://www.tensorflow.org/lite/guide/ops_select#converting_the_model", "Looks like you are using a model with Control Flow V1 - that is, nodes like Enter, Exit, etc. \r\nIn TFLite we mainly support Control Flow V2, since it is much easier to convert. [This](https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2) is how you enable it.\r\n\r\nStepping back for a minute, which model is this? Is this some object-detection model?", "@LeonTodoran Could you please let us know if the above comments help resolve the issue", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@LeonTodoran \r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 36369, "title": "Need for more flexible Loss Function", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 1.15\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSince, we know that sometimes in the training process, we need the **MAE and MSE type of behavior** simultaneously. We have this kind of loss function already - **Huber Loss**.  But in some cases, we don't need just one value, instead, we need the interval. We need the loss function which can output two values corresponding to lower and upper bounds of the interval. \r\nThis type of loss function is called **Quantile Huber Loss**, which is taken from [this paper](https://arxiv.org/pdf/1402.4624.pdf). The behavior is shown in below image.\r\n![73123322-db0dcf80-3fb4-11ea-852d-9dc672bd0906](https://user-images.githubusercontent.com/20843596/73537063-830c1880-444d-11ea-890c-3ed96aa98a51.png)\r\n\r\n\r\n**Will this change the current api? How?**\r\nNo, it will not change the current api. Instead, it will just add one loss function and will include it's tests.\r\n\r\n**Who will benefit with this feature?**\r\nThis will be very beneficial for the forecasting and time series. Especially, when the variation is very dynamic, then this loss function helps very much in comparison to existing losses in tf. \r\n", "comments": ["Please feel free to send a PR to the Tensorflow Addons repository for this Loss.", "cc @seanpmorgan for Addons\r\n\r\nhttps://github.com/tensorflow/addons"]}, {"number": 36368, "title": "You tried to call `count_params` on embedding, but the layer isn't built. You can build it manually via: `embedding.build(batch_input_shape)`.", "body": "when i run model.summary(), this issue occures.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10 64bit\r\n- TensorFlow installed from (source or binary):pip install tensorflow (cpu-version)\r\n- TensorFlow version (use command below):tensorflow 2.0\r\n- Python version:python 3.6\r\n\r\n**Code to reproduce the issue**\r\nimport tensorflow as tf\r\nfrom tagger.crf import crf_decode, crf_log_likelihood\r\n\r\nk = tf.keras\r\n\r\n\r\nclass TaggerModel(k.Model):\r\n    def __init__(self, config,  *args, **kwargs):\r\n        super(TaggerModel, self).__init__(args, kwargs)\r\n        self.embedding_layer = k.layers.Embedding(input_dim=config.vocab_size, output_dim=config.embedding_size)\r\n        forward_layer = k.layers.LSTM(units=config.num_units, return_sequences=True, activation=\"sigmoid\")\r\n        backward_layer = k.layers.LSTM(units=config.num_units, return_sequences=True, activation=\"sigmoid\",\r\n                                       go_backwards=True)\r\n        self.bidirectional_layer = k.layers.Bidirectional(\r\n            layer=forward_layer, backward_layer=backward_layer, merge_mode=\"concat\"\r\n        )\r\n        self.dropout_layer = k.layers.Dropout(rate=config.dropout)\r\n        self.dense_layer = k.layers.Dense(units=config.num_tags)\r\n        self.transition = tf.Variable(\r\n            initial_value=tf.random.truncated_normal(shape=[config.num_tags, config.num_tags]), name=\"transition\",\r\n            trainable=True)\r\n\r\n    @tf.function\r\n    def call(self, inputs, training=None, mask=None):\r\n        source, length = inputs\r\n\r\n        # embedding\r\n        embeddings = self.embedding_layer(source)\r\n\r\n        # bidirectional\r\n        out = self.bidirectional_layer(embeddings)\r\n        out = self.dropout_layer(out)\r\n        potentials = self.dense_layer(out)\r\n\r\n        # crf\r\n        logits, score = crf_decode(potentials=potentials, transition_params=self.transition, sequence_length=length)\r\n\r\n        return logits\r\n\r\nmodel = TaggerModel(config)\r\nmodel.compile(optimizer=config.optimizer, loss=tf.losses.binary_crossentropy, metrics={tf.metrics.Accuracy()})\r\nmodel.build(input_shape=((None, None), (None,)))\r\nmodel.summary()\r\n", "comments": ["@Goofy-G,\r\nI tried to reproduce the issue, but I am facing an error stating `ModuleNotFoundError: No module named 'tagger'`. You can find the Gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/c3b83278aed679baccf3a0818d6cc7f6/36368.ipynb).\r\n\r\nSeems like you are using custom python modules. In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here including the custom modules. Thanks!", "Hi\r\n  i use concreted config to replace my config, and then reproduce this issue\r\n\r\ncode:\r\n`\r\nimport tensorflow as tf\r\n\r\nk = tf.keras\r\n\r\n\r\nclass TaggerModel(k.Model):\r\n    def __init__(self, *args, **kwargs):\r\n        super(TaggerModel, self).__init__(args, kwargs)\r\n        # self.config = config\r\n        self.embedding_layer = k.layers.Embedding(input_dim=2300, output_dim=128)\r\n        forward_layer = k.layers.LSTM(units=128, return_sequences=True, activation=\"sigmoid\")\r\n        backward_layer = k.layers.LSTM(units=128, return_sequences=True, activation=\"sigmoid\",\r\n                                       go_backwards=True)\r\n        self.bidirectional_layer = k.layers.Bidirectional(\r\n            layer=forward_layer, backward_layer=backward_layer, merge_mode=\"concat\"\r\n        )\r\n        self.dropout_layer = k.layers.Dropout(rate=0.5)\r\n        self.dense_layer = k.layers.Dense(units=50)\r\n        # self.transition = tf.Variable(\r\n        #     initial_value=tf.random.truncated_normal(shape=[config.num_tags, config.num_tags]), name=\"transition\",\r\n        #     trainable=True)\r\n\r\n    @tf.function(input_signature=(tf.TensorSpec(shape=[None, ], dtype=tf.int64), tf.TensorSpec(shape=[], dtype=tf.int32)))\r\n    def call(self, inputs, training=None, mask=None):\r\n        source, length = inputs\r\n\r\n        # embedding\r\n        embeddings = self.embedding_layer(source)\r\n\r\n        # bidirectional\r\n        out = self.bidirectional_layer(embeddings)\r\n        out = self.dropout_layer(out)\r\n        potentials = self.dense_layer(out)\r\n\r\n        # # crf\r\n        # logits, score = crf_decode(potentials=potentials, transition_params=self.transition, sequence_length=length)\r\n        # output = tf.one_hot(logits, depth=self.config.num_tags, dtype=tf.float32)\r\n        output = potentials\r\n        return output\r\n\r\nmodel = TaggerModel()\r\nmodel.compile(optimizer=\"adam\", loss=tf.losses.sparse_categorical_crossentropy, metrics=[])\r\nmodel.summary()\r\n`\r\n", "@Goofy-G,\r\nI was able to run the above code without any issues. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/65f11e4f2af377bc024817606b50afba/36368_2.ipynb). Thanks!", "Hi\r\n   I am using tensorflow 2.0.0, not 2.1.0. And When I upgraded tensorflow to 2.1.0, it occured that I couldn't import tf. This is about another issue.\r\n  I run the completed code , \r\n`\r\nimport tensorflow as tf\r\n\r\nk = tf.keras\r\n\r\n\r\nclass TaggerModel(k.Model):\r\n    def init(self, *args, **kwargs):\r\n        super(TaggerModel, self).init(args, kwargs)\r\n        # self.config = config\r\n        self.embedding_layer = k.layers.Embedding(input_dim=2300, output_dim=128)\r\n        forward_layer = k.layers.LSTM(units=128, return_sequences=True, activation=\"sigmoid\")\r\n        backward_layer = k.layers.LSTM(units=128, return_sequences=True, activation=\"sigmoid\",\r\n                                       go_backwards=True)\r\n        self.bidirectional_layer = k.layers.Bidirectional(\r\n            layer=forward_layer, backward_layer=backward_layer, merge_mode=\"concat\"\r\n        )\r\n        self.dropout_layer = k.layers.Dropout(rate=0.5)\r\n        self.dense_layer = k.layers.Dense(units=50)\r\n        # self.transition = tf.Variable(\r\n        # initial_value=tf.random.truncated_normal(shape=[config.num_tags, config.num_tags]), name=\"transition\",\r\n        # trainable=True)\r\n\r\n    @tf.function(input_signature=(\r\n    tf.TensorSpec(shape=[None, None], dtype=tf.int64), tf.TensorSpec(shape=[None, ], dtype=tf.int32)))\r\n    def call(self, inputs, training=None, mask=None):\r\n        source, length = inputs\r\n\r\n        # embedding\r\n        embeddings = self.embedding_layer(source)\r\n\r\n        # bidirectional\r\n        out = self.bidirectional_layer(embeddings)\r\n        out = self.dropout_layer(out)\r\n        potentials = self.dense_layer(out)\r\n\r\n        # # crf\r\n        # logits, score = crf_decode(potentials=potentials, transition_params=self.transition, sequence_length=length)\r\n        # output = tf.one_hot(logits, depth=self.config.num_tags, dtype=tf.float32)\r\n        output = potentials\r\n        return output\r\n\r\nmodel = TaggerModel()\r\nmodel.compile(optimizer=\"adam\", loss=tf.losses.sparse_categorical_crossentropy, metrics=[])\r\ndataset = tf.data.Dataset.from_tensor_slices(tf.random.truncated_normal(shape=(20, 100)))\r\ndataset = dataset.map(lambda src: ((tf.cast(src, dtype=tf.int64), tf.size(src))))\r\nmodel.fit(x=dataset, validation_data=dataset)\r\nmodel.summary()\r\ntf.saved_model.save(model, \"mydir\")\r\n`\r\nand I got crash:\r\n\r\n> Traceback (most recent call last):\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 842, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2658, in bound_method_wrapper\r\n    return wrapped_fn(*args, **kwargs)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in converted code:\r\n\r\n    E:\\Python\\Work\\TestTensorflow\\TestTagger1\\tests\\common_test.py:26 call  *\r\n        source, length = inputs\r\n    D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:547 __iter__\r\n        self._disallow_iteration()\r\n    D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:540 _disallow_iteration\r\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\r\n    D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:518 _disallow_when_autograph_enabled\r\n        \" decorating it directly with @tf.function.\".format(task))\r\n\r\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Library\\Python\\lib\\unittest\\case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"D:\\Library\\Python\\lib\\unittest\\case.py\", line 605, in run\r\n    testMethod()\r\n  File \"E:\\Python\\Work\\TestTensorflow\\TestTagger1\\tests\\common_test.py\", line 52, in test_model\r\n    model.fit(x=dataset, validation_data=dataset)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 224, in fit\r\n    distribution_strategy=strategy)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 547, in _process_training_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 594, in _process_inputs\r\n    steps=steps)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2419, in _standardize_user_data\r\n    all_inputs, y_input, dict_inputs = self._build_model_with_inputs(x, y)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2622, in _build_model_with_inputs\r\n    self._set_inputs(cast_inputs)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2709, in _set_inputs\r\n    outputs = self(inputs, **kwargs)\r\n  File \"D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 854, in __call__\r\n    str(e) + '\\n\"\"\"')\r\nTypeError: You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass `dynamic=True` to the class constructor.\r\nEncountered error:\r\n\"\"\"\r\nin converted code:\r\n\r\n    E:\\Python\\Work\\TestTensorflow\\TestTagger1\\tests\\common_test.py:26 call  *\r\n        source, length = inputs\r\n    D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:547 __iter__\r\n        self._disallow_iteration()\r\n    D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:540 _disallow_iteration\r\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\r\n    D:\\Library\\VirtualEnv\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:518 _disallow_when_autograph_enabled\r\n        \" decorating it directly with @tf.function.\".format(task))\r\n\r\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n", "Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/ef609db8a8327eb0608d08156da9c6d0/36368_2-0.ipynb). Thanks!", "@Goofy-G Error is from `model.fit`. I have a question. Why the target tensor is empty as in `dataset = dataset.map(lambda src: ((tf.cast(src, dtype=tf.int64), tf.size(src))))`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/1cc461b8d8086e0a3f48c178542edfcb/36368_2-0.ipynb). Thanks!", "HI,\r\n   This is my mistake, I forget generating target data, the dataset should be `dataset = dataset.map(\r\n            lambda src: ((tf.cast(src, dtype=tf.int64), tf.size(src)), tf.cast(src, dtype=tf.int64)))`. However, it doesn't influence the following crash.", "@omalleyt12 \r\n\r\nThe error in the gist indicates that `inputs` is not a 2-element tuple as you'd expect, but instead a single Tensor. When you try to expand that, Python tries to iterate over that Tensor which gives you the error message. Not sure where that single Tensor is coming from, but you can see it by adding a print statement:\r\n\r\n```\r\n  def call(self, inputs, training=None, mask=None):\r\n        print(inputs)\r\n        ...\r\n  # Output: Tensor(\"inputs:0\", shape=(None, None), dtype=int64)\r\n```\r\n\r\n@Goofy-G Keras expects your dataset to be batched, so you should call `dataset = dataset.batch(1)` even if you work with single examples.\r\n\r\nBut even after I added the batching, it seems that the second tensor is still not being passed to the model, which I think is a bug.", "BTW, I couldn't see the original `count_oarams` in any of the error any more.", "Hi\r\n   Thanks for your reply. The original crash occured when I run` model.summary()` instead of `model.fit()`. Firstly, I used to build model with `keras.Model(input, output)`, there is no crash happen, and then I transfer it to `model.call()`.\r\n   Furthermore, I am failed to use `tf.saved_model.save()` to export serving model in tf 2.0. But I use `tf.keras.experimental.export_saved_model()` to export model, it's okay. What 's the deference about two.", "@Goofy-G I assume original issue was resolved. If you face any issues when saving model, please open a new issue. I think it will be easy for other users who are facing similar issue in saving model. Thanks!\r\n\r\nIf the original issue was resolved, close this issue and open a new issue on saving model. Thanks!", "@Goofy-G \r\n\r\nAny update on this issue please. Thanks!", "Hi\r\n   I got it, thanks bro."]}, {"number": 36367, "title": "Tensorflow 2.1.0 Error  :  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]", "body": "**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\nOS Platform and Distribution (Windows): Windows 10 Home, 10.0.18362 Build 18362\r\nTensorFlow installed from : PIP\r\nTensorFlow version: v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\nPython version: 3.7.6\r\nCUDA/cuDNN version: Cuda 10.1, cuDNN 7.6.5\r\nGPU model and memory: RTX 2070 8GB\"\r\n\r\n**Describe the problem:**\r\n\r\nI installed  tensorflow through pip,  when I try to run any of the notebooks downloaded from Tensorflow tutorials section, it gives ' Failed to get convolution error'. Though when I run some simple calculations on gpu it works fine, for example if I run the the following code no error is generated.\r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\ntf.debugging.set_log_device_placement(True)\r\n\r\n# Create some tensors\r\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\r\nc = tf.matmul(a, b)\r\n\r\nprint(c)\r\n```\r\nOutput:\r\nNum GPUs Available:  1\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\n\r\nWhen I run any  complex algorithm or any pre trained model  \"failed to get convolution error ....\" is generated. \r\n\r\n**Info:**\r\n\r\nTracing\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-19-1756cf577d92> in <module>\r\n      1 start = time()\r\n----> 2 dream_img = run_deep_dream_simple(img=original_img, steps=100, step_size=0.01)\r\n      3 print(time()-start)\r\n\r\n<ipython-input-17-193f882b4182> in run_deep_dream_simple(img, steps, step_size)\r\n     14     step += run_steps\r\n     15 \r\n---> 16     loss, img = deepdream(img, run_steps, tf.constant(step_size))\r\n     17 \r\n     18     display.clear_output(wait=True)\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    636               *args, **kwds)\r\n    637       # If we did not create any variables the trace we have is good enough.\r\n--> 638       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n    639 \r\n    640     def fn_with_cond(*inner_args, **inner_kwds):\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1609          if isinstance(t, (ops.Tensor,\r\n   1610                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1611         self.captured_inputs)\r\n   1612 \r\n   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node while/body/_1/model/conv2d/Conv2D}}]] [Op:__inference___call___12702]\r\n\r\nFunction call stack:\r\n__call__", "comments": []}, {"number": 36366, "title": "Discrepancy in training step accuracy with Tensorflow 2.1", "body": "tag:Bug/Performance Issue</em>\r\n\r\n**Describe the current behavior**\r\n1. I followed each and every step from the tensorflow documentation but still there is discrepancy coming in training step accuracy \r\n2. Training and validation accuracy is far too different though same dataset is used for training and validation\r\n\r\n**Describe the expected behavior**\r\n1. Training accuracy should not come that different from the validation accuracy\r\n2. model.fit and model.evaluate accuracy should be nearly same \r\n**Code to reproduce the issue**\r\nI have attached the link to my google colab notebook: \r\n[Google Colab Link](https://colab.research.google.com/drive/1Ir8LhIrYB-UexygQKIFuNwRkmb6yJh-w)\r\n<img width=\"1095\" alt=\"Screenshot 2020-01-31 at 3 48 50 PM\" src=\"https://user-images.githubusercontent.com/11526652/73531945-931dfb00-4441-11ea-8401-0cca6ea79142.png\">\r\n", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.research.google.com/gist/amahendrakar/5ac084bdfd001085dba47ef5f1aa1210/copy-of-36366.ipynb). Thanks!", "I am also facing this issue with both tensorflow 2.0 and 2.1. This is pretty weird. I am not sure if I am missing anything as it looks like a big issue/bug to have if it is true. Can you please help with this asap?", "Cannot find proper documentation for the same and facing the same issue. ", "@ymodak @amahendrakar  can u please help with this?", "Please take a look at tensorflow official example using same flower dataset https://colab.sandbox.google.com/github/tensorflow/examples/blob/master/community/en/flowers_tf_lite.ipynb\r\nThanks!\r\n", "@ymodak I was able to solve this issue by adding layers argument in the keras model as suggested in this [issue](](https://github.com/keras-team/keras/pull/9965#issuecomment-549126009) link\r\n\r\n`tf.keras.applications.mobilenet.MobileNet(input_shape=[224, 224, 3], layers = tf.keras.layers)}`\r\n\r\nThis issue has been affecting many and is documented very well in this keras [issue thread](https://github.com/keras-team/keras/pull/9965) and [blog](http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/)\r\n\r\nPlease fix this issue and update the existing tensorflow2 documentation with this fix asap", "The next 2.2 release candidate will include this commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/410852dbd24899e22f0020f9fdc9757f527dda55\r\n(This fixes a bug where some keras code was accidentally referring to the wrong versions internally)\r\n\r\nIf you previously had to put `layers=tf.keras.layers`, this should fix your issue.", "> The next 2.2 release candidate will include this commit:\r\n> [410852d](https://github.com/tensorflow/tensorflow/commit/410852dbd24899e22f0020f9fdc9757f527dda55)\r\n> (This fixes a bug where some keras code was accidentally referring to the wrong versions internally)\r\n> \r\n> If you previously had to put `layers=tf.keras.layers`, this should fix your issue.\r\n\r\nI tried adding `layers=tf.keras.layers`  but still it is not working for me.\r\n", "This is fixed now and available with latest tf-nightly version as well as release candidate rc-3. Thanks!", "> I am also facing this issue with both tensorflow 2.0 and 2.1. This is pretty weird. I am not sure if I am missing anything as it looks like a big issue/bug to have if it is true. Can you please help with this asap?\r\n\r\nI also faced the same issue"]}, {"number": 36365, "title": "Gradient Doc Changed for v2", "body": "Fixes #35628.\r\nI have raised PR #36206 which has modified **gradients()** function but at that time I forgot to modify **gradients_v2()** which is the latest function used in 2.1. So, this PR modifies gradient_v2().\r\n\r\n@alextp, Please review this PR too.", "comments": ["Changes have been merged into master by commit 97b900a. So closing the PR."]}, {"number": 36364, "title": "embedding_lookup_sparse divide by zero fixed", "body": "Fixes #36359. @Saduf2019 , @gowthamkpr and @yongtang, Please Review this one.", "comments": ["@alextp, Test case checking function is added. Please review PR.", "@alextp, Test cases are added. Please review it", "cc @mihaimaruseac ", "@ashutosh1919 Can you please check build failures? Thanks!", "@gbaned & @mihaimaruseac , Resolved ci error", "Tests fail internally\r\n\r\n```\r\n    tf_embedding_sum = embedding_sum.numpy()\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```", "@mihaimaruseac , I think it gives error because in TF-2.x, eager execution is enabled by default. Is there any suggestion from your side to deal with this type of issue?", "All code should be eager first and working on 2.x behavior.", "@mihaimaruseac , Please run ci, I hope it should work now.", "Still failing\r\n\r\n```\r\n    tf_embedding_sum = ops.convert_to_tensor(embedding_sum).numpy()\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```", "@mihaimaruseac , It is clearly mentioned [here](https://www.tensorflow.org/guide/eager#setup_and_basic_usage) that eager execution is compatible with `numpy()` function and can be applied on `tf.Tensor`. I am not able to catch the error. Can you please give me more info about the error message?", "It's probably an error caused by the internal integration. Let me dive into it and I will be back here with more details.", "@mihaimaruseac , any updates on this PR?", "Unfortunately I didn't yet get time to fully fix it. I'll dedicate some more time tomorrow if nothing else pops up and preempts from this", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@ashutosh1919  Any update on this PR? Please. Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36364) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "@mihaimaruseac , @rthadur can you please approve it? Want to see if it passes the tests now", "@mihaimaruseac , @rthadur - I have ran the bazel tests as well as sanity tests. Both are passing for the code which I changed. But the internal tests are failing here. I think the problem is with something else. Can you please look?\r\n\r\n```\r\n10,336 / 10,998] 8 actions running\r\nTarget //tensorflow/python/kernel_tests:embedding_ops_test up-to-date:\r\n  bazel-bin/tensorflow/python/kernel_tests/embedding_ops_test\r\nINFO: Elapsed time: 15494.530s, Critical Path: 2387.59s\r\nINFO: 7668 processes: 213 internal, 7455 local.\r\nINFO: Build completed successfully, 7668 total actions\r\n//tensorflow/python/kernel_tests:embedding_ops_test                      PASSED in 67.4s\r\n  Stats over 20 runs: max = 67.4s, min = 9.5s, avg = 26.4s, dev = 19.4s\r\n```\r\n\r\n"]}, {"number": 36363, "title": "Different behavior tf.keras and Keras for `stateful=True` ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): All platforms (tested on Ubuntu 18.04 and macOS)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\nThe original Keras API specifies that, when an LSTM is set `stateful=True`, it's batch size *must* be known beforehand (by specifying `batch_shape`). The same is true for `tf.keras`, but it adds another hidden requirement that was not there in the original Keras: tf.keras requires that the full input shape (including batch size) is known. If one of the dimensions is `None`, it emits the \"If a RNN is stateful, it needs to know its batch size.\" error.\r\n\r\n**Describe the expected behavior**\r\nAs with the Keras API, it should be allowed to have `None` dimensions besides the `batch_size`.\r\n\r\n**Code to reproduce the issue**\r\nKeras:\r\n\r\n```\r\nfrom keras.models import Model\r\nfrom keras.layers import Input, LSTM, Reshape\r\n\r\ndef model():\r\n    input_layer = Input(batch_shape=(1, None))\r\n    reshape_layer = Reshape((1, 100))(input_layer)\r\n    lstm_layer = LSTM(units=100, stateful=True)(reshape_layer)\r\n    return Model(inputs=input_layer, outputs=lstm_layer)\r\n\r\nmodel = model()\r\n\r\n# Code runs perfectly fine.\r\n```\r\n\r\ntf.keras:\r\n\r\n```\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, LSTM, Reshape\r\n\r\ndef model():\r\n    input_layer = Input(batch_shape=(1, None))\r\n    reshape_layer = Reshape((1, 100))(input_layer)\r\n    lstm_layer = LSTM(units=100, stateful=True)(reshape_layer)\r\n    return Model(inputs=input_layer, outputs=lstm_layer)\r\n\r\nmodel = model()\r\n\r\n\"\"\"\r\nValueError: If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \r\n- If using a Sequential model, specify the batch size by passing a `batch_input_shape` argument to your first layer.\r\n- If using the functional API, specify the batch size by passing a `batch_shape` argument to your Input layer.\r\n\"\"\"\r\n\r\n```\r\n\r\nThere are some very legitimate use cases for allowing non-batch dimensions to be unknown! This change in functionality prevents me from migrating a (variable) multi-stream CNN model from Keras to tf.keras.", "comments": ["Works as intended with [Keras](https://colab.sandbox.google.com/gist/amahendrakar/c4643b7f631003b9747ace2b26e417ee/36363_keras.ipynb), was able to reproduce the error with [tf.Keras](https://colab.sandbox.google.com/gist/amahendrakar/47bf7a1917dabc817d805b659090c3c6/36363_tf-keras.ipynb). Please find the attached Gist. Thanks!", "@gerwin3 I think shape information is missing. As a result shape inference is not working and throwing this error. A workaround is to simply use `set_shape` as shown below.\r\n\r\n```\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, LSTM, Reshape\r\n\r\ndef model():\r\n    input_layer = Input(batch_shape=(1, None))\r\n    reshape_layer = Reshape((1, 100))(input_layer)\r\n    reshape_layer.set_shape((1,1,100))\r\n    lstm_layer = LSTM(units=100, stateful=True)(reshape_layer)\r\n    \r\n    return Model(inputs=input_layer, outputs=lstm_layer)\r\n\r\nmodel = model()\r\n```", "@gerwin3 Can you please check my last comment?  Thanks!", "@jvishnuvardhan \r\n\r\nYour solution gets rid of the error message, but does not fix the underlying problem. Old Keras allowed for non-batch dimensions to be unknown (by setting them to `None`), even when using an RNN. The work-around you provided wouldn't really work if, for example, you have a variable dimension that can change in size depending on the input. (Or maybe it would if you were to invoke `set_shape` with each iteration?)\r\n\r\nI'm not sure if that feature of Keras was actually documented, but the current implementation of `tf.keras` is definitely incompatible with the original behavior of Keras.", "Thanks for reporting the issue. \r\n\r\nI think the root cause is from the reshape layer in the code provided. If you check the shape of the reshape layer output, it is (None, 1, 100), where the batch dim is None, and it cause the LSTM layer to fail the validation.\r\n\r\nPlease check the docstring of the reshape layer in https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape", "Btw, I don't think there is any change wrt to the reshape layer. The original keras docstring also gives the example where the batch dim is None. Could u print out the reshape_layer.shape for both example and see what's the value there?", "Humm, I think somehow the input_shape got by the lstm layer in keras is [1, 1, 100], which is not same as the shape of the reshape layer output. It might be some subtle diff in the keras sequential model that cause this difference. I need to check more details. ", "I think the root cause is that we don't call layer.comput_output_shape anymore for the default model building, and only call it for dynamic model. This cause the result of the reshape layer to lose the static shape information (batch dim), and cause the LSTM layer to error out. \r\n\r\nWill send a fix very soon.", "@gerwin3\r\n\r\nDoes the commit 40218a7 \"Set static shape information for reshape layer.\" fix the bug ? \r\nBecause on my side, I still have the same problem you described after having integrated the fix.", "The issue persists. I tested on `tf-nightly 2.2.0.dev20200418`.", "@gerwin3 ok, so it confirms the problem.\r\n\r\n@tensorflow-copybara, @qlzh727, is the commit 40218a7 \"Set static shape information for reshape layer.\" supposed to fix the issue ?", "Sorry, the issue is not fixed yet since my change kept getting rollback because of internal breakages. I will try to submit the fix soon.", "Hi @qlzh727 \r\nDo you know if you will have time to fix the issue soon ?\r\nThat would be really great ;-)", "This should now be fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36363\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36363\">No</a>\n"]}, {"number": 36362, "title": "load_weights fails for custom models", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): Tensorflow 2.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CPU \r\n- GPU model and memory:  CPU with 256GB RAM\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI created a simple custom model. Training runs fine and saving to hdf5 is also fine. When leading it back. I get the error\r\nYou are trying to load a weight file containing 3 layers into a model with 0 layers\r\n\r\n**Describe the expected behavior**\r\nThe weights need to be loaded correctly\r\n\r\n**Code to reproduce the issue**\r\n\r\nimport tensorflow as tf\r\nfrom random import seed\r\nfrom random import random\r\n\r\nseed(1)\r\nclass SqDataset:\r\n    def __init__(self):\r\n        pass\r\n    def generate(self):\r\n        for i in range(1000):\r\n            value = random()\r\n            value = value * 2.0 - 1.0\r\n            yield ({'x':value}, value*value)\r\n\r\ndat = SqDataset()\r\n\r\ndatGen = tf.data.Dataset.from_generator(dat.generate, ({'x':tf.float32}, tf.float32), ({'x':tf.TensorShape([])}, tf.TensorShape([])))\r\ndatGen = datGen.batch(10)\r\n\r\nclass SquareWave(tf.keras.Model):\r\n    def __init__(self):\r\n        super(SquareWave, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(5, activation=tf.nn.tanh, name='sqdense1', input_shape=(1,))\r\n        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.tanh, name='sqdense2')\r\n        self.dense3 = tf.keras.layers.Dense(1, activation=tf.nn.tanh, name='sqdense3')\r\n        \r\n\r\n    def call(self, inputs, training=False):\r\n        ips = inputs['x']\r\n        o = self.dense1(ips)\r\n        o = self.dense2(o)\r\n        o = self.dense3(o)\r\n        return o\r\n\r\n\r\n\r\nmodelName = 'weights.h5'\r\nif modelName != None:\r\n    sqMode = SquareWave()\r\n    sqMode.compile(optimizer=tf.keras.optimizers.RMSprop(), \r\n                   loss=tf.keras.losses.MeanSquaredError())\r\n    sqMode.load_weights(modelName) #fails always\r\n    \r\nelse:\r\n    sqMode = SquareWave()\r\n    sqMode.compile(optimizer=tf.keras.optimizers.RMSprop(), \r\n                   loss=tf.keras.losses.MeanSquaredError())\r\nsqMode.fit(datGen, workers=0, epochs=100, use_multiprocessing=False)       \r\n\r\nsqMode.save_weights('weights.h5', save_format='h5')\r\n\r\n**Other info / logs**\r\nThe set of trainable_variables and trainable_weights is empty during the loading of model because the build methods are not called on the layers. This results in an error", "comments": ["Just an update: SavedModel format works", "@suth1807,\r\nCan you please share the Code with proper indentation and using Code Formatting Option,  **<>** shown above, or please execute the Code in the Google Colab and Save it as Github Gist (File -> Save a Copy as Github Gist), and please share that Link with us. Thanks!", "The link is below\r\nhttps://gist.github.com/suth1807/9f2e0ab73046f7b85aa86959458bc4cd#file-untitled0-ipynb", "The code can be executed with modelName set to None to enable saving of the weights\r\n", "Could reproduce the error with Tensorflow Version 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/1b3447627937fd6759f7a1dd235b5fde/36362.ipynb). Thanks!", "@suth1807 There is a difference between subclass model and other keras model types(Sequential and Functional). Due to those differences listed [here](https://www.tensorflow.org/guide/keras/save_and_serialize#saving_subclassed_models), saving and loading of subclass models is different from other Keras models.\r\n\r\nTo restore a subclass model,  (1) you will need access to the code that created the model object and create the model again, (2) compile the model in order to restore the optimizer state and any stateful metrics, and (3) call it on some data before calling load_weights. \r\n\r\nYou have followed steps (1) and (2) correctly and missed calling the compiled model on some data before loading weights using `load_weights`.\r\n\r\nWhen I updated your code with the missing part of step(3), then everything worked as expected.\r\n\r\nI am closing as it was resolved. Please feel free to reopen if the issue persists again. Thanks!\r\n\r\n[Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/2acfb877e7ba068f00c0294649524068/36362.ipynb) is the gist for y/our reference. Thanks again for the creating issue with good details.", "Thanks for the explanation! I had used this approach of making a dummy call but thought of it as a work-around. The input_shapes for the first layer is passed as argument so all the required information is there, isn't it?", "@suth1807 `input_shape` in the first layer is required to infer shapes in Sequential and Functional keras model APIs. For subclass models, we need to call it on some data to initialize weights and then load weights. Hope this helps. Thanks!"]}, {"number": 36361, "title": "tf.function using higher GPU memory than normal python function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: (3, 5, 2, 'final', 0)\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla V100-PCIE-16GB\r\n\r\n**Describe the current behavior**\r\nI am running a simple LSTM model inside a function decorated with tf.function but it's consuming much higher GPU memory (around 8700MB) as compared to a function which isn't decorated with tf.function (which consumes around 2600MB).\r\n\r\n**Describe the expected behavior**\r\nBoth methods should consume similar amount of GPU memory.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\ntf.debugging.set_log_device_placement(True)\r\n\r\n#GPU growth code\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n\r\n#---------------PARAMS------------\r\nbatch_size = 32\r\nmax_out_len = 200\r\nnum_hidden = 400\r\nnum_classes = 73\r\nmax_time_steps = 900\r\nnum_features = 240\r\n#---------------------------------\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n\r\n        self.forward_cell = [\r\n            tf.keras.layers.LSTMCell(num_hidden, kernel_initializer='glorot_uniform')]\r\n        self.state = tf.keras.layers.RNN(self.forward_cell, time_major=False, return_sequences=True)\r\n        self.dense = tf.keras.layers.Dense(num_classes, use_bias=True, kernel_initializer=tf.initializers.TruncatedNormal(mean=0, stddev=0.1))\r\n\r\n    def call(self, inputs):\r\n        x, seq_len = inputs\r\n        mask = tf.sequence_mask(seq_len, maxlen=max_time_steps)\r\n        state = self.state(inputs=x, mask=mask)\r\n        logits = self.dense(state)\r\n        return logits\r\n\r\nx = tf.random.normal(shape=(2, batch_size, max_time_steps, num_features))\r\nseq_len = tf.constant([[5]*batch_size,[7]*batch_size])\r\ny_i = [tf.constant(sum([[[i,0],[i,1],[i,2]] for i in range(batch_size)], [])),tf.constant(sum([[[i,0],[i,2],[i,4]] for i in range(batch_size)], []))]\r\ny_v = [tf.constant([1,2,4]*batch_size),tf.constant([5,1,2]*batch_size)]\r\nv = [tf.constant([1,0]*(batch_size//2)),tf.constant([0,1]*(batch_size//2))]\r\ndef fn():\r\n    for i in range(2):\r\n        yield x[i], seq_len[i], y_i[i], y_v[i], v[i]\r\n\r\ndef loss(logits, s, yi, yv):\r\n    y = tf.SparseTensor(yi, yv, tf.TensorShape([batch_size, max_out_len]))\r\n    return tf.cast(tf.sparse.reduce_sum(y, axis=-1), tf.float32) - tf.reduce_sum(logits, axis=[-1, -2])\r\n\r\n\r\nwith tf.device('/cpu:0'):\r\n    model = Model()\r\n    model.build([(batch_size, max_time_steps, num_features), (batch_size,)])\r\n    dataset = tf.data.Dataset.from_generator(fn, output_types=(tf.float32, tf.int32, tf.int64, tf.int32, tf.float32))\r\n\r\ndef run_eager(x, s, yi, yv, v):\r\n    with tf.device('/gpu:0'):\r\n        with tf.GradientTape() as tape:\r\n            logits = model([x, s])\r\n            losses = tf.reduce_sum(loss(logits, s, yi, yv))\r\n        grads = tape.gradient(losses, model.trainable_variables)\r\n\r\n    # ToDo: call optimizer.apply_gradients\r\n\r\n    return losses\r\n\r\nif len(sys.argv) > 1 and sys.argv[1] == 'func':\r\n    run = tf.function(run_eager)\r\nelse:\r\n    run = run_eager\r\n\r\nwith tf.device('/cpu:0'):\r\n    it = iter(dataset)\r\n    l = run(*next(it))\r\n\r\nprint(l)\r\n```\r\n", "comments": ["@abhigoyal2210 \r\nI have tried on colab with TF version 2.1 and i am not able to see GPU memory usage with and without decorating tf.function . Please find the gist [here](https://colab.research.google.com/gist/ravikyram/62470c7593576abb4ec738c53cc4a76d/untitled614.ipynb). Thanks!", "By the time !nvidia-smi runs on colab, the code is already executed completely. Below are screenshots of GPU usage monitored in parallel using nvtop (GPU ID: 0).\r\n\r\nWithout tf.function decoration:\r\n<img width=\"827\" alt=\"Screenshot 2020-01-31 at 4 00 47 PM\" src=\"https://user-images.githubusercontent.com/53578184/73532745-57843080-4443-11ea-9bf9-bd1da3c82798.png\">\r\n<img width=\"822\" alt=\"Screenshot 2020-01-31 at 4 00 59 PM\" src=\"https://user-images.githubusercontent.com/53578184/73532750-5a7f2100-4443-11ea-8317-c55e99adddd6.png\">\r\n\r\nWith tf.function decoration:\r\n<img width=\"823\" alt=\"Screenshot 2020-01-31 at 4 01 26 PM\" src=\"https://user-images.githubusercontent.com/53578184/73532763-61a62f00-4443-11ea-942e-145fb1c58a7a.png\">\r\n<img width=\"823\" alt=\"Screenshot 2020-01-31 at 4 01 36 PM\" src=\"https://user-images.githubusercontent.com/53578184/73532766-62d75c00-4443-11ea-98e9-070fcbe64ab0.png\">\r\n\r\nYellow curve in above screenshots show memory usage.", "Even on collab, after you run the code, you can see occupied GPU memory. Below are the screenshots.\r\n\r\nWithout tf.function decoration (2445MiB):\r\n<img width=\"722\" alt=\"Screenshot 2020-01-31 at 4 13 31 PM\" src=\"https://user-images.githubusercontent.com/53578184/73533423-c9a94500-4444-11ea-9ceb-55aa78ddffca.png\">\r\n\r\nWith tf.function decoration (8589MiB):\r\n<img width=\"706\" alt=\"Screenshot 2020-01-31 at 4 12 51 PM\" src=\"https://user-images.githubusercontent.com/53578184/73533440-db8ae800-4444-11ea-8eb8-acd5e900c2cb.png\">\r\n", "@kkimdev @sanjoy for more insight about the differences of peak memory use.\r\n\r\nThe difference is not entirely surprising because tf.function and Python function execute things in different ways.", "Yes, they do but I found 3.5 times increase surprising. Also, If stacked tf.keras.layers.RNN (with LSTMCell) is replaced by tf.keras.layers.LSTM, this memory blow-up doesn't happen. On the contrary lesser memory is consumed (which might not be surprising as LSTM() would use CuDNN kernel). See the screenshot below.\r\n\r\nGPU usage when tf.function is used and RNN(LSTMCell) as replaced by LSTM().\r\n<img width=\"827\" alt=\"Screenshot 2020-02-04 at 9 35 08 PM\" src=\"https://user-images.githubusercontent.com/53578184/73762451-3b132b80-4796-11ea-8ec3-7245a3a5ba74.png\">\r\n\r\nCode used for this:\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\ntf.debugging.set_log_device_placement(True)\r\n\r\n#GPU growth code\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n\r\n#---------------PARAMS------------\r\nbatch_size = 32\r\nmax_out_len = 200\r\nnum_hidden = 400\r\nnum_classes = 73\r\nmax_time_steps = 900\r\nnum_features = 240\r\n#---------------------------------\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n\r\n        # self.forward_cell = [\r\n        #     tf.keras.layers.LSTMCell(num_hidden, kernel_initializer='glorot_uniform')]\r\n        # self.state = tf.keras.layers.RNN(self.forward_cell, time_major=False, return_sequences=True)\r\n        self.state = tf.keras.layers.LSTM(num_hidden, kernel_initializer='glorot_uniform', time_major=False, return_sequences=True)\r\n        self.dense = tf.keras.layers.Dense(num_classes, use_bias=True, kernel_initializer=tf.initializers.TruncatedNormal(mean=0, stddev=0.1))\r\n\r\n    def call(self, inputs):\r\n        x, seq_len = inputs\r\n        mask = tf.sequence_mask(seq_len, maxlen=max_time_steps)\r\n        state = self.state(inputs=x, mask=mask)\r\n        logits = self.dense(state)\r\n        return logits\r\n\r\nx = tf.random.normal(shape=(2, batch_size, max_time_steps, num_features))\r\nseq_len = tf.constant([[5]*batch_size,[7]*batch_size])\r\ny_i = [tf.constant(sum([[[i,0],[i,1],[i,2]] for i in range(batch_size)], [])),tf.constant(sum([[[i,0],[i,2],[i,4]] for i in range(batch_size)], []))]\r\ny_v = [tf.constant([1,2,4]*batch_size),tf.constant([5,1,2]*batch_size)]\r\nv = [tf.constant([1,0]*(batch_size//2)),tf.constant([0,1]*(batch_size//2))]\r\ndef fn():\r\n    for i in range(2):\r\n        yield x[i], seq_len[i], y_i[i], y_v[i], v[i]\r\n\r\ndef loss(logits, s, yi, yv):\r\n    y = tf.SparseTensor(yi, yv, tf.TensorShape([batch_size, max_out_len]))\r\n    return tf.cast(tf.sparse.reduce_sum(y, axis=-1), tf.float32) - tf.reduce_sum(logits, axis=[-1, -2])\r\n\r\n\r\nwith tf.device('/cpu:0'):\r\n    model = Model()\r\n    model.build([(batch_size, max_time_steps, num_features), (batch_size,)])\r\n    dataset = tf.data.Dataset.from_generator(fn, output_types=(tf.float32, tf.int32, tf.int64, tf.int32, tf.float32))\r\n\r\n@tf.function\r\ndef run_eager(x, s, yi, yv, v):\r\n    with tf.device('/gpu:0'):\r\n        with tf.GradientTape() as tape:\r\n            logits = model([x, s])\r\n            losses = tf.reduce_sum(loss(logits, s, yi, yv))\r\n        grads = tape.gradient(losses, model.trainable_variables)\r\n\r\n    # ToDo: call optimizer.apply_gradients\r\n\r\n    return losses\r\n\r\nif len(sys.argv) > 1 and sys.argv[1] == 'func':\r\n    run = tf.function(run_eager)\r\nelse:\r\n    run = run_eager\r\n\r\nwith tf.device('/cpu:0'):\r\n    it = iter(dataset)\r\n    l = run(*next(it))\r\n\r\nprint(l)\r\n```\r\n", "Interesting find!\r\n\r\nCan you check forcing the TF to use a smaller amount of memory (using [`set_local_device_configuration`](https://www.tensorflow.org/api_docs/python/tf/config/set_logical_device_configuration)) runs fine (i.e. without OOM) with `tf.function`?\r\n\r\n`tf.function` uses the graph executor which (grossly simplifying) picks some execution order consistent with the data flow in the graph and this total order could be different enough from the eager execution order to explain the difference in memory usage.  However, the TF allocator has a [mechanism](https://github.com/tensorflow/tensorflow/blob/3fcf6424ae5fe0d2f062b222c2ba4467c0350c1e/tensorflow/core/common_runtime/bfc_allocator.cc#L213) to (again, oversimplifying) \"pick\" a different total order to avoid an OOM.", "Yes, it works fine when I set memory limit to 4500 or higher and gives OOM otherwise.", "@abhigoyal2210 \r\nCould you please let us know if this is still an issue on tf 2.7 as per the above update, please confirm if this is still an issue.\r\nYou may refer to this [link](https://github.com/tensorflow/tensorflow/blob/3fcf6424ae5fe0d2f062b222c2ba4467c0350c1e/tensorflow/core/common_runtime/bfc_allocator.cc#L213)and let us know.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 36360, "title": "Parallel Write TFRecords Shards", "body": "**System information**\r\n- TensorFlow version (you are using): TF2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\nI have a `tf.data.Dataset` that reads raw image and annotations files and shards them. However, using `tf.data.experimental.TFRecordWriter` writes sequentially and is currently very slow.\r\n\r\n```\r\ndataset = tf.data.Dataset.....\r\nfor i in range(num_shards):\r\n    write_path = ....\r\n    tfrecords_writer = tf.data.experimental.TFRecordWriter(\r\n        filename=write_path,\r\n        compression_type=compression_type\r\n    )\r\n    dataset_shard = dataset.shard(num_shards, shard_index)\r\n\r\n    start_write_time = time.time()\r\n```\r\n\r\nI would like to be able to write the shards in parallel to reduce the write time by opening up multiple processes that process each shard individually.\r\n\r\nI've tried using a `multiprocessing.Process` and starting a bunch of processes that way, but it doesn't seem to speed up writing.", "comments": ["@rodyt Did you mean when you call tfrecords_writer.write it generate the tfrecord sequentially ?\r\nCould you provide a piece of code and the running time as the baseline?", "Could you try to write each dataset shard into a different tfrecord file? I suspect it will be much faster.\r\nSince it seems each tfrecords_writer.write op only have  one background thread which works. So even you start much process which invoke the same tfrecords_writer.write op, there may only  one thread works at each time.", "Hi @Leslie-Fang,\r\n\r\nThanks for your response and advice.\r\n\r\nI am trying to write each shard into a different `tfrecord` file, as you mentioned. The `tf.data.Dataset` is reading from the file system, preprocessing, and sharding them at the end (it might be more efficient to put sharding at the beginning though).\r\n\r\nI try to start **x** separate threads using `multiprocess` and then evenly distribute the shards across the threads. Each shard is written to its own `tfrecords` file, and each thread ends up writing **y** shards into **y** different `tfrecords` file (individually). (e.g. We have 4 shards and 2 processes; Process 0 will write shards 0 & 2, while process 1 writes shards 1 & 3)\r\n\r\nThis is the code I am running:\r\n\r\n```\r\n# Get the number of shards each process will be responsible for writing\r\nnum_shards_per_thread = num_shards // num_processes\r\n\r\ncoord = tf.train.Coordinator()\r\nprocesses = []\r\n\r\n# Create the processes\r\nfor thread_index in range(num_processes):\r\n    params = (\r\n        dataset,  # tf.data.Dataset object\r\n        output_dir,  # str containing path to somewhere on filesystem\r\n        num_shards,  # The total number of shards\r\n        num_shards_per_thread,  # The number of shards each thread is responsible for\r\n        thread_index\r\n    )\r\n\r\n    thread_process = multiprocessing.Process(\r\n        target=save_shard,\r\n        args=params\r\n    )\r\n\r\n    thread_process.start()\r\n    processes.append(thread_process)\r\ncoord.join(processes)\r\n\r\n```\r\n\r\nInside `save_shard`, I execute the following code:\r\n\r\n```\r\n# This code is executed for each process\r\nfor shard in range(num_shards_per_thread):\r\n\r\n    # Get the current shard's index out of all shards that will be written\r\n    # Takes into account the shard number in the current thread\r\n    shard_index = shard + (num_shards_per_thread * thread_index)\r\n    # Write each shard to its own .tfrecords file\r\n    filename = \"my_tfrecord_file_%d.tfrecords\" % (shard_index)\r\n    write_path = os.path.join(output_dir, filename)\r\n    # Create the writer\r\n    tfrecords_writer = tf.data.experimental.TFRecordWriter(\r\n        filename=write_path\r\n    )\r\n\r\n    # Actually shard the dataset and pass in the calculated index from above\r\n    dataset_shard = dataset.shard(num_total_shards, shard_index)\r\n\r\n    # Do the actual writing\r\n    tfrecords_writer.write(dataset_shard)\r\n\r\n```\r\n\r\nI tried to borrow some code from: [this SO post](https://stackoverflow.com/questions/51504222/writing-tfrecord-with-multithreading-is-not-fast-as-expected)\r\n\r\nIt doesn't work as expected. It looks like it hangs there and does nothing. It appears that the `tfrecord` files are created, albeit empty. I left the code running for a while and after I came back, nothing had been written yet.\r\n\r\nI'm a bit of a newbie at multi-threading, so I could be trying to do it completely wrong.", "The tf.data team has a plan for making it possible to read and write sharded TFRecord files in parallel. The first step in that direction is the tf.data snapshot proposal https://github.com/tensorflow/community/pull/193/files, which does this under the hoods when generating the snapshot. This is expected to be available in TF 2.3. As a follow up, we plan to expose API to `save` and `load` the snapshot (supporting TFRecord as the serialization format).\r\n\r\nAs a side note, my suggestion would be not to mix `multiprocessing.Process` with tf.data code. tf.data code is multi-threaded and mixing multi-processing with multi-threading is not safe (because only the calling thread in a forked process is copied to the new process).\r\n\r\n", "I have the same problem, and there seems to be no cure at this moment.", "Have you tried using the `tf.data.experimental.save` API that allows you to write the contents of a dataset in parallel (and later load it using `tf.data.experimental.load`)?", "@jsimsa I am trying to understand how to make the` tf.data.experimental.save` work in parallel.  Do you have an example of this?  Thank you!", "The implementation of `save` does parallel I/O. You as a user do not need to do anything.", "@jsimsa thank you for your prompt answer!  I am trying to understand why I always get a single shard when I do the `save` operation.  If I try applying a custom shard function, like `shard_func=lambda x: x % 7` I get an error: `TypeError: <lambda>() takes 1 positional argument but 3 were given`. My tensor element structure is something like a tuple of tensors, a tensor, a tensor: \r\n```\r\n(\r\n(\r\nTensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\r\n)\r\nTensorSpec(shape=(None, 1), dtype=tf.float32, name=None), \r\nTensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\r\n)\r\n\r\n```", "If only one shard is being generated when you do not explicitly specify `shard_func`, it means that the TensorFlow runtime believes that there is only one CPU core on the machine that you are running on (as the number of CPU cores is used as the default number of shards [source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/experimental/io_ops.cc#L145-L148)).\r\n\r\nThe error you are seeing is because your `shard_func` needs to have a signature that matches the structure of the elements flowing through the input pipeline. So you will need the following:\r\n\r\n```\r\ndataset = ... # dataset definition\r\n\r\ndef shard_func(x, y, z):\r\n  shard_index = ... # computes index based on `x`, `y,` and `z`\r\n\r\ntf.data.experimental.save(..., shard_func)\r\n```", "@jsimsa thank you!  I have tried this, I am getting: `AttributeError: 'Tensor' object has no attribute 'numpy'` (TF 2.4.0)\r\n\r\n```\r\ndef shard_func(x, y, w, num_shards=10):\r\n    shard_index = w[0].numpy() % num_shards\r\n    return shard_index\r\n```\r\n\r\n`w[0]` because it is a batch.", "a fix for this error: `tf.dtypes.cast(w[0], tf.int64) % num_shards`", "For future reference, the `shard_func` argument will be traced and executed as graph computation (as per documentation). In other words, `w` will be a Tensor object, which does not have a `numpy()` method.", "@jsimsa thank you!\r\n\r\nI am still having some trouble with the save/load and shards.  I was able to use the shard function by using the features scores.  The shards came out very uneven.  \r\n\r\nAt the same time, I get this message on the GPU (not on the local integration test running on the CPU):\r\n\r\n`2021-01-21 16:46:58.641448: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"LoadDataset/_1\"\r\n`\r\n\r\nIs there a reference example somewhere where the save/load and sharding techniques are combined for the distributed dataset use case?\r\n\r\nThank you!\r\n", "I was able to solve my initial uneven sharding issue with this (from the snapshot documentation):\r\n\r\n```\r\ndataset = dataset.enumerate() #this produces the id (x) for each record\r\ntf.data.experimental.save(dataset, target_path, lambda x, y=x % num_shards)\r\n```\r\n\r\nThen, remove this `id` later:  `dataset = dataset.map(lambda x, y: y)`\r\n", "@jsimsa I ended up writing a small [reference](https://github.com/iprovalo/tf/blob/main/input_pipeline_stats_benchmark.py) app to integrate all the parts for saving and loading the dataset as well as training a small model.  I am using it for benchmarking and profiling different approaches.  \r\n\r\nI noticed a GPU underutilization for my app, it is unlikely related to the dataset pipeline, since it is completely preprocessed.  Here is [the issue - 46800](https://github.com/tensorflow/tensorflow/issues/46800). ", "```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nmatrixs = np.random.randn(10000000, 512, 512, 3)\r\n\r\ntfrecord_save_path = 'data.tfrecord'\r\n\r\nwith tf.io.TFRecordWriter(tfrecord_save_path) as tfwriter:\r\n    for matrix in matrixs:\r\n        example = tf.train.Example(features=tf.train.Features(\r\n            feature={\r\n                'matrix': tf.train.Feature(float_list=tf.train.FloatList(value=matrix.reshape(-1))),\r\n                'matrix_shape': tf.train.Feature(int64_list=tf.train.Int64List(value=matrix.shape)),\r\n            }\r\n        ))\r\n\r\n        tfwriter.write(example.SerializeToString())\r\n```\r\n\r\nWhy doesn't TFRecordWriter have built-in integrated multiprocess write?  Maybe all in one `data.tfrecord` more  convenient, instead of multiple file \"shards\".   @jsimsa ", "@SmileTM please create a separate issue for this as `tf.io.TFRecordWriter` is orthogonal to tf.data.\r\n\r\nFor tf.data users, the recommended mechanism for efficiently serializing a tf.data.Dataset is to use `tf.data.experimental.save()` whose output can be later consumed using `tf.data.experimental.load()`."]}, {"number": 36359, "title": "[Bug] embedding_lookup_sparse divide by zero when use mean combiner", "body": "I come across this issue with TF==1.14.\r\nI found this bug was fixed in this PR: https://github.com/tensorflow/tensorflow/pull/21757.\r\nHowever, when I checked the implementations in 1.13, 1.14 and 1.15 I found all of them implemented as following:\r\n\r\n```\r\nelif combiner == \"mean\":\r\n        embeddings = math_ops.segment_sum(embeddings, segment_ids)\r\n        weight_sum = math_ops.segment_sum(weights, segment_ids)\r\n        embeddings = math_ops.div(embeddings, weight_sum, name=name)\r\n```\r\n\r\nShould this be fixed with `math_ops. div_no_nan ` ?\r\n\r\n", "comments": ["Please note,I have found the same issue for code in TF 2.x as well.", "@wenmin-wu Please take a look at this [comment](https://github.com/tensorflow/tensorflow/pull/21757#issuecomment-416835941) i.e.,\r\nin the documentation of `tf.nn.embedding_lookup_sparse` its mentioned that **This op assumes that there is at least one id for each row in the dense tensor represented by sp_ids**\r\n\r\nSo, due to this, the change has not been merged. ", "@gowthamkpr Even though this is true, the sum of weights can also be zero. \r\nI come across this problem because I want to embed user actions on items. Some people don't have any action on items. So for these users, I can only set `sp_ids=[0]`, `sp_weights=[0]`. With the current implementation, it will divide by 0. ", "@wenmin-wu Can you please share a reproducible test case where the sum of weights are zero. Thanks!", "@gowthamkpr Just now I found this issue can be solved with `tf.nn.safe_embedding_lookup_sparse` (tested with 1.14.0, I think the same for the other versions).  Here're the code snippets for reproducing.\r\n\r\n```Python\r\nimport tensorflow as tf\r\n\r\nE = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])\r\nids = tf.SparseTensor([[0,0]], np.array([0], dtype='int64'), dense_shape=[1,1])\r\nweights = tf.SparseTensor([[0,0]], [0.0], dense_shape=[1,1])\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(tf.nn.embedding_lookup_sparse(E, ids, weights, combiner='mean')))\r\n# ouput: [[nan nan nan]]\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(tf.nn.safe_embedding_lookup_sparse(E, ids, weights, combiner='mean')))\r\n\r\n# output: [[0. 0. 0.]]\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36359\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36359\">No</a>\n"]}, {"number": 36358, "title": "TypeError: '>' not supported between instances of 'Nonetype' and 'float'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**  \r\n- OS Platform and Distribution: Windows 10 1909  \r\n- TensorFlow version: 2.1.0  \r\n- Python version: 3.6.10  \r\n- CUDA/cuDNN version: 10.1/7.6.5  \r\n- GPU model and memory: GTX 1660Ti 6GB/32GB  \r\n\r\n        import tensorflow as tf\r\n        ACCURACY_THRESHOLD = 0.95\r\n\r\n        class myCallback(tf.keras.callbacks.Callback):\r\n\t    def on_epoch_end(self, epoch, logs={}):\r\n\t        if(logs.get('acc') > ACCURACY_THRESHOLD):\r\n\t\t    print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(ACCURACY_THRESHOLD*100))\r\n\t\t    self.model.stop_training = True\r\n\r\n        callbacks = myCallback()\r\n\r\n        mnist = tf.keras.datasets.fashion_mnist\r\n        (x_train, y_train),(x_test, y_test) = mnist.load_data()\r\n        # Scale data\r\n        x_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n        model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n        ])\r\n\r\n        model.compile(optimizer='adam', \\\r\n\t\t\t\tloss='sparse_categorical_crossentropy', \\\r\n\t\t\t\tmetrics=['accuracy'])\r\n\r\n        model.fit(x_train, y_train, epochs=20, callbacks=[callbacks])  \r\n\r\nwhen I ran this algorithm I got error saying this:\r\n\r\n![1](https://user-images.githubusercontent.com/44919399/73509681-fbe78200-4405-11ea-9dde-c9fe07b1f9da.jpg)\r\n\r\nBriefly it says that logs.get('acc') is Nonetype and 0.99 is float so, '>' this cannot compare. How to solve it?\r\n", "comments": ["@Krishnarohith10,\r\nReplace `if(logs.get('acc') > ACCURACY_THRESHOLD):` with `if(logs.get('accuracy') > ACCURACY_THRESHOLD):` in your code and it should work.\r\n\r\nWas able to resolve the issue. Please find the Gist [here](https://colab.research.google.com/gist/amahendrakar/cc1d75ca9f89f5b910daef7846bcf068/36358.ipynb#scrollTo=1KcZMQr7fNZp&line=10&uniqifier=1). Thanks!", "@amahendrakar \r\nThank You. That Worked.", "I am still getting this error \"TypeError: '>' not supported between instances of 'NoneType' and 'float'\"", "> @amahendrakar\r\n> Thank You. That Worked.\r\n\r\n\r\n\r\n> I am still getting this error \"TypeError: '>' not supported between instances of 'NoneType' and 'float'\"\r\n\r\nNever mind, using 'acc' worked for me. Maybe I am using an older version", "Make sure you are using acc instead of accuracy. And otherwise, at least be consistent. Check your version of the tensorflow and use the corresponding term accordingly. It worked for me in that way. \r\n\r\n", "`if(logs.get('acc') > ACCURACY_THRESHOLD)` worked for me", "Weird if(log.get('acc') > ACCURACY_THRESHOLD) worked for me too", "if(logs.get('accuracy') > ACCURACY_THRESHOLD) work for me", "`if(logs.get('acc') > ACCURACY_THRESHOLD)` worked for me. very weird though", "if(logs.get('acc') > ACCURACY_THRESHOLD) didn't work for me. What should i do?", "if(logs.get('acc') > ACCURACY_THRESHOLD) didn't work for me. What should i do?", "mine was giving error for `log.get('accuracy')` but when I changes it to` logs.get('acc')`, it worked", "This depends on what name you use to store your accuracy variable. IF its stored as 'acc' use log.get('acc') else if you stored accuracy to \"accuracy\" use log.get('accuracy').  If you stored on some other variable, use it so. \r\nCheers m8 ", "mine was giving error for log.get('accuracy') but when I changes it to logs.get('acc'), it worked", "if(logs.get('acc') or if(logs.get('accuracy') \r\ndepend on what you stored     return history.epoch, history.history['acc'][-1]", "Below worked for ```tf 2.2.0```\r\n```py\r\n    class endCallback(tf.keras.callbacks.Callback):\r\n        def on_epoch_end(self, epoch, logs={}):\r\n            if(logs.get('acc')>0.998):\r\n                print(\"\\nReached 99.8% accuracy so cancelling training!\")\r\n                self.model.stop_training = True\r\n```\r\n", "I had the same problem... Just tried here: both 'acc' and 'accuracy' works... I depends on what metrics have you specified on the compile():\r\n\r\n`\r\nmodel.compile(loss='binary_crossentropy',\r\n               optimizer='adam',\r\n               metrics=['acc'])\r\n`  \r\nor \r\n`\r\nmodel.compile(loss='binary_crossentropy',\r\n               optimizer='adam',\r\n               metrics=['accuracy'])\r\n` ", "For Tensorflow version equal to `1.14.0`, `if(logs.get('acc') > ACCURACY_THRESHOLD)` is working. Replacing `accuracy` with `acc` in version `1.14.0` resulting in no error like `'>' not supported between instances of 'NoneType' and 'float'`. For me both the changes had to be done, changing only one of them resulted like the error mentioned above.", "> I had the same problem... Just tried here: both 'acc' and 'accuracy' works... I depends on what metrics have you specified on the compile():\r\n> \r\n> `model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])`\r\n> or\r\n> `model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])`\r\n\r\nI dont think that depends on metrics,\r\nI defined metrics['accuracy'] but logs.get('acc') worked for me.\r\nI guess it has to be tried and tested for which one works", "Yeah this seems to be a bug in google's Computer Vision example \u2192 [https://colab.research.google.com/github/lmoroney/mlday-tokyo/blob/master/Lab2-Computer-Vision.ipynb](https://colab.research.google.com/github/lmoroney/mlday-tokyo/blob/master/Lab2-Computer-Vision.ipynb)", "> > I had the same problem... Just tried here: both 'acc' and 'accuracy' works... I depends on what metrics have you specified on the compile():\r\n> > `model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])`\r\n> > or\r\n> > `model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])`\r\n> \r\n> I dont think that depends on metrics,\r\n> I defined metrics['accuracy'] but logs.get('acc') worked for me.\r\n> I guess it has to be tried and tested for which one works\r\n\r\nEven I thought like that and I tried many times and the only time I succeeded was putting `acc`. May be for higher versions, this might not occur!", "> Yeah this seems to be a bug in google's Computer Vision example \u2192 https://colab.research.google.com/github/lmoroney/mlday-tokyo/blob/master/Lab2-Computer-Vision.ipynb\r\n\r\nI haven't tried in colab, I used jupyter notebook!", "Try using try-except:-\r\n\r\nclass myCallback(tf.keras.callbacks.Callback):\r\n  def on_epoch_end(self, epoch, logs = {}):\r\n    try:\r\n      if(logs.get('acc')>ACCURACY_THRESHOLD):\r\n        print(\"\\nReached\")\r\n        self.model.stop_training = True\r\n    except:\r\n      if(logs.get('accuracy')>ACCURACY_THRESHOLD):\r\n        print(\"Reached!!!\")\r\n        self.model.stop_training = True", "I'm doing the coursera TensorFlow course. And one of it's Exercise this problem occurred. In my local machine I ran with 'accuracy' and it worked but in coursera jupyter notebook 'acc' worked. ", "Just print logs by using:   print(logs) inside on_epoch_end() method and see whether accuracy is stored as \"accuracy\" or \"acc\". Then evaluate accuracy accordingly.", "This is due to versioning. Just print(logs) inside on_epoch_end() method, you will get to know which one to choose.", "In Coursera \"Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\" course, use `logs.get('acc')` instead of `logs.get('accuracy')`, otherwise you'll get the error **Can't compile the student's code. '>' not supported between instances of 'NoneType' and 'float'**\r\n\r\nAlso, replace 'accuracy' by 'acc' in other parts of the code.\r\n\r\n", "> mine was giving error for `log.get('accuracy')` but when I changes it to` logs.get('acc')`, it worked\r\n\r\nMe too! Amazing...", "Yes it worked thank you", "If we print -> print(logs) we get an array showing the exact keys for my case it was as : \r\n{'loss': 0.25769969047009944, 'acc': 0.92646664} ,  \r\nBoth options worked for me as below: \r\n1. logs.get('acc') \r\n2. float(logs.get('acc'))\r\n", "for me changing to 'acc' worked : if(logs.get('acc')>0.99)"]}, {"number": 36356, "title": "[S3] Allow configuration of certificate files when accessing S3", "body": "Description of these from the [AWS CPP SDK's page](https://sdk.amazonaws.com/cpp/api/LATEST/struct_aws_1_1_client_1_1_client_configuration.html#a246583f705480c7e708393d6ebf76426): \r\n\r\n- caFile: If you certificate file is different from the default, you can tell clients that aren't using the default trust store where to find your ca file. \r\n- caPath: If your Certificate Authority path is different from the default, you can tell clients that aren't using the default trust store where to find your CA trust store. ", "comments": ["@mihaimaruseac This is a small PR, could you take a look at this too?"]}, {"number": 36355, "title": "[S3] Fix error due to clashing names of temporary files created by the S3 file system", "body": "- Previously the name of the temp file was controlled by the AWS SDK. But the SDK uses timestamp to create temporary files. We've run into cases where the temp files are created with the same name for two different files, and data gets corrupted.\r\n- Adds a static counter and a lock to generate a unique suffix for each temp file to resolve this. ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36355) for more info**.\n\n<!-- need_author_cla -->", "@rahul003 thank you for your contribution, please sign CLA.", "@ravikyram Yes, my coauthor is on it", "Closing this, as the upgrade of AWS SDK fixes this issue. https://github.com/tensorflow/tensorflow/pull/36478"]}, {"number": 36354, "title": "[S3] Allow configuring AWS SDK's logging level", "body": "- Allows configuring the AWS CPP SDK's logging level correctly through the env variable `AWS_LOG_LEVEL`. This helps suppress verbose logs and allows access to debugging logs in the case of issues.\r\n- The current behavior takes in the TF log level and passes that along to the SDK. That has a few limitations. The logging levels don't map directly between AWS SDK (trace/debug/info/warning/error/fatal/off) and TF (info/warn/error). By passing TF level to SDK, we can never enable Debug and Trace. \r\n- Setting the AWS SDK's log level to Fatal by default. This is because in the regular flow of TF operations, many safe errors are seen. These confuse users into thinking their program is failing. Setting this to FATAL suppresses them.\r\n- In addition to the above, this PR adds VLOG statements helpful for debugging S3 issues. ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36354) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36354) for more info**.\n\n<!-- ok -->", "@rahul003 Can you please resolve conflicts? Thanks!", "@gbaned Fixed the conflict, please take a look", "Both the test failures are from unrelated changes. I know one has been fixed on master. Rebased to address that. Other probably hasn't. I'm not sure. ", "Can you solve conflicts please?", "@mihaimaruseac Please review", "Removed the trailing whitespace", "Can you take a look at the build failures please? Probably resync to master and run them again?", "Failures seemed unrelated to the PR. Pulled from master\r\n"]}, {"number": 36353, "title": "[S3] Adding retries for S3 file system operations, and fix read bug", "body": "- Requests to S3 can sometimes fail due to transient errors. This PR attempts to make the S3 File system more resilient by retrying some operations.\r\n- Moved the retrying file system capability for Cloud out of the folder, and used that for S3FileSystem as well.\r\n- Identified some cases where requests fail due to authentication failures, and raised an explicit error asking users to check their AWS credentials.\r\n- Changed the error code for one case. When TF tries to delete a non empty folder, changed the error code to Unknown from FailedPrecondition. This error can be thrown as S3 is eventually consistent. Here's a detailed explanation for why this can happen.\r\nDeleteDir consists of 1) ListFiles in directory 2) Remove each file in directory 3) DeleteDirectory. Both the operations List and Remove are eventually consistent. So the result of these operations may not be available when the DeleteDirectory request is made. So that can fail. Hence we need to retry this operation. \r\n", "comments": ["Thank you very much for your contribution.\r\nI did move around some code here, but may not be very familiar with everything.\r\nI am adding @mihaimaruseac , who would be more familiar than me with the filesystems", "@rahul003 Can you please check mihaimaruseac's comments and keep us posted? Thanks!", "@mihaimaruseac Updated the PR as per your comments. Created couple of functions and reused them for all operations. Please review again", "I think you need to replace `cc_test` with `tf_cc_test`", "Thanks @MihailSalnikov , pushed a commit fixing the builds for the tests", "When does tensorflow-gardener pull in a PR? How does that work?", "After review on GitHub, code is imported by Copybara into Google, integrated with local changes there and then there is another round of reviews internally. Once that passes, Copybara will try to submit the code, running internal and external tests again. If all are green, `tensorflow-gardener` will merge the PR, otherwise someone needs to comment on the PR to report issues or manually patch the imported change to fix internal tests.\r\n\r\nSee also https://github.com/tensorflow/tensorflow/issues/36527#issuecomment-583529777", "There are some conflicts. Can you ping me again once they're solved so I can do another review?", "@mihaimaruseac Please review", "Is there something to do on my end wrt the import/copybara failure?", "No, I was able to manually import it. Probably a tool failure"]}, {"number": 36352, "title": "[S3] Improving transfer to S3 for checkpoints, saved models, etc", "body": "- Improves the performance of transfer to S3 (i.e. WriteFile method of S3FileSystem) by leveraging AWS SDK's [TransferManager](https://sdk.amazonaws.com/cpp/api/LATEST/class_aws_1_1_transfer_1_1_transfer_manager.html) which splits a file into parts and uploads these parts in parallel. Some performance numbers below.\r\n\r\nTime to upload 2GB:\r\nBefore: 103sec \r\nWith this PR: 9sec\r\nSpeedup: 11x\r\n\r\nTime to upload 4GB\r\nBefore: 116sec\r\nWith this PR: 20sec\r\nSpeedup: 6x\r\n\r\nTime to upload 5.5GB:\r\nBefore: `Crash: Entity too large`\r\nWith this PR: 28sec\r\n\r\n- Also adds this functionality of split and upload when copying a file from one s3 location to another. Even in this case, S3 requires to use the MultiPart API when file is larger than 5GB. Since this functionality is not in the AWS SDK, it has been implemented in this PR. File is split up into chunks of 50MB and uploaded. This chunk size can be controlled by the environment variable `S3_MULTIPART_PART_SIZE` which takes a string representing the bytes of a part.\r\n- Increases the default timeout for S3 connect and request as it frequently times out when uploading large files. This has been set to to 5min.\r\n- It also allows retrying the upload of only a failed part instead of the whole file.\r\n- Supports models and checkpoints larger than 5GB through the [MultiPartUpload API of S3](https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html) which we use when we split up the file as above. Without this API, (i.e. as in the current file system) crashes when we try to upload a file larger than 5GB)", "comments": ["Can you solve conflicts please? Apologies for missing the review before, it slipped through notifications.", "Fixed the conflicts, please take a look", "Let me come to this one around the weekend", "Sure", "Hmm, I still see `osstringstream`. Did you forget to push the next changes?", "Yeah I've been trying to build it locally, and it actually fails for me. Shouldn't have pulled from master maybe...\r\n\r\n```\r\n\r\ntensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc: In static member function 'static tensorflow::Status tensorflow::MklEagerOpRewrite::SetupNewOp(tensorflow::EagerOperation*, std::__cxx11::string, std::unique_ptr<tensorflow::EagerOperation>*)':\r\ntensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc:138:25: error: no match for 'operator!=' (operand types are 'absl::variant<tensorflow::Device*, tensorflow::CustomDevice*>' and 'std::nullptr_t')\r\n   if (orig_op->Device() != nullptr) {\r\n                         ^\r\n\r\n\r\n\r\ntensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc:139:47: error: no matching function for call to 'tensorflow::EagerOperation::SetDevice(absl::variant<tensorflow::Device*, tensorflow::CustomDevice*>)'\r\n     (*new_mkl_op)->SetDevice(orig_op->Device());\r\n```\r\n\r\nI guess we have to wait for this to be fixed on master...", "Never mind, only MKL build is broken. Tested locally now without MKL. "]}, {"number": 36351, "title": "[ROCm][XLA] Disable ResizeBilinearTest in image_ops_test", "body": "This CL disables `ResizeBilinearTest` as it runs out of memory in ROCm CI. I regressed locally and confirmed that it causes the whole test to fail, but can pass if running the single test case by itself.", "comments": ["@cheshire  Gentle ping"]}, {"number": 36350, "title": "Add NVTX Ranges", "body": "- Adds NVTX ranges around ops executed by eager and graph executors.\r\n- Useful for NVIDIA Nsight Systems or DLProf profiling.\r\n- All ranges are added to the 'tensorflow-core' domain to avoid\r\n  conflicts with potential user-defined ranges.\r\n- Can be disabled with TF_DISABLE_NVTX_RANGES environment variable.\r\n\r\nAttn @jbaiocchi and @qiuminxu ", "comments": ["I know very little about NVTX but with just a cursory look at the PR I'm a bit concerned about how it is injected in the code. It seems like we need a bit more of a general solution where tracing hooks can be added and then different tracers can be registered to receive notifications for critical events. This would avoid the need for environment variables and inflating dependancies.\r\n\r\nWe're trying to see if we can get someone more knowledgable internally to help with this PR.", "Hi, I was a developer on the [original legacy NVTX](http://developer.download.nvidia.com/NsightVisualStudio/2.2/Documentation/UserGuide/HTML/Content/NVTX_Library.htm) for push/pop named ranges and colors on Nexus/Nsight Visual Studio. While I like the idea of adding NVTX ranges in TF to enrich tools, I am wondering if you have some performance numbers for the change. i.e. TF original, NVTX disabled, NVTX enabled with few ranges, NVTX enabled with many ranges, NVTX enabled with many ranges and long string names.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36350) for more info**.\n\n<!-- need_author_consent -->", "@nluehr please sign CLA", "Both myself and @DEKHTIARJonathan have signed the CLA. The bot seems to be confused.", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36350) for more info**.\n\n<!-- ok -->", "Hi I have benchmarked Tensorflow with/without this PR, here are the results:\r\n\r\n### Commit 6b59e63f8124bb907714a68a6bdd01ef87685e19  - Upstream (aka. before merge)\r\n\r\n``` shell\r\n$ git clone https://github.com/tensorflow/benchmarks.git\r\n$ cd benchmarks/scripts/tf_cnn_benchmarks/\r\n$ python tf_cnn_benchmarks.py --data_format=NCHW --distortions=false --use_fp16=True --model=resnet50 --batch_size=128 --xla=true\r\n```\r\n\r\n```python\r\nTensorFlow:  2.1\r\nModel:       resnet50\r\nDataset:     imagenet (synthetic)\r\nMode:        training\r\nSingleSess:  False\r\nBatch size:  128 global\r\n             128 per device\r\nNum batches: 100\r\nNum epochs:  0.01\r\nDevices:     ['/gpu:0']\r\nNUMA bind:   False\r\nData format: NCHW\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\n==========\r\nGenerating training model\r\nDone warm up\r\nStep\tImg/sec\ttotal_loss\r\n1\timages/sec: 746.3 +/- 0.0 (jitter = 0.0)\t7.871\r\n10\timages/sec: 747.2 +/- 1.1 (jitter = 2.5)\t7.954\r\n20\timages/sec: 748.5 +/- 0.8 (jitter = 2.8)\t7.950\r\n30\timages/sec: 749.0 +/- 0.7 (jitter = 3.6)\t7.949\r\n40\timages/sec: 748.5 +/- 0.5 (jitter = 3.5)\t7.960\r\n50\timages/sec: 747.9 +/- 0.5 (jitter = 3.4)\t7.718\r\n60\timages/sec: 747.6 +/- 0.5 (jitter = 3.3)\t7.918\r\n70\timages/sec: 746.9 +/- 0.5 (jitter = 3.9)\t7.842\r\n80\timages/sec: 746.8 +/- 0.4 (jitter = 3.8)\t7.965\r\n90\timages/sec: 746.6 +/- 0.4 (jitter = 3.6)\t7.793\r\n100\timages/sec: 746.5 +/- 0.4 (jitter = 3.4)\t7.784\r\n----------------------------------------------------------------\r\ntotal images/sec: 745.90\r\n----------------------------------------------------------------\r\n```\r\n\r\n### This PR -- Without ` TF_DISABLE_NVTX_RANGES=1`\r\n``` shell\r\n$ git clone https://github.com/tensorflow/benchmarks.git\r\n$ cd benchmarks/scripts/tf_cnn_benchmarks/\r\n$ python tf_cnn_benchmarks.py --data_format=NCHW --distortions=false --use_fp16=True --model=resnet50 --batch_size=128 --xla=true\r\n```\r\n\r\n```python\r\nTensorFlow:  2.1\r\nModel:       resnet50\r\nDataset:     imagenet (synthetic)\r\nMode:        training\r\nSingleSess:  False\r\nBatch size:  128 global\r\n             128 per device\r\nNum batches: 100\r\nNum epochs:  0.01\r\nDevices:     ['/gpu:0']\r\nNUMA bind:   False\r\nData format: NCHW\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\n==========\r\nGenerating training model\r\nDone warm up\r\nStep\tImg/sec\ttotal_loss\r\n1\timages/sec: 742.9 +/- 0.0 (jitter = 0.0)\t7.876\r\n10\timages/sec: 744.2 +/- 0.8 (jitter = 1.2)\t7.956\r\n20\timages/sec: 744.0 +/- 0.6 (jitter = 2.1)\t7.952\r\n30\timages/sec: 744.4 +/- 0.5 (jitter = 2.6)\t7.946\r\n40\timages/sec: 743.8 +/- 0.5 (jitter = 2.4)\t7.962\r\n50\timages/sec: 743.6 +/- 0.5 (jitter = 2.4)\t7.717\r\n60\timages/sec: 743.3 +/- 0.4 (jitter = 2.8)\t7.918\r\n70\timages/sec: 743.0 +/- 0.4 (jitter = 2.7)\t7.847\r\n80\timages/sec: 742.8 +/- 0.4 (jitter = 2.8)\t7.969\r\n90\timages/sec: 742.9 +/- 0.4 (jitter = 3.0)\t7.807\r\n100\timages/sec: 742.8 +/- 0.4 (jitter = 2.7)\t7.767\r\n----------------------------------------------------------------\r\ntotal images/sec: 742.28\r\n----------------------------------------------------------------\r\n```\r\n\r\n### This PR -- With ` TF_DISABLE_NVTX_RANGES=1` (effectively equivalent as upstream)\r\n\r\n``` shell\r\n$ git clone https://github.com/tensorflow/benchmarks.git\r\n$ cd benchmarks/scripts/tf_cnn_benchmarks/\r\n$ TF_DISABLE_NVTX_RANGES=1 python tf_cnn_benchmarks.py --data_format=NCHW --distortions=false --use_fp16=True --model=resnet50 --batch_size=128 --xla=true\r\n```\r\n\r\n```python\r\nTensorFlow:  2.1\r\nModel:       resnet50\r\nDataset:     imagenet (synthetic)\r\nMode:        training\r\nSingleSess:  False\r\nBatch size:  128 global\r\n             128 per device\r\nNum batches: 100\r\nNum epochs:  0.01\r\nDevices:     ['/gpu:0']\r\nNUMA bind:   False\r\nData format: NCHW\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\n==========\r\nGenerating training model\r\nDone warm up\r\nStep\tImg/sec\ttotal_loss\r\n1\timages/sec: 745.1 +/- 0.0 (jitter = 0.0)\t7.873\r\n10\timages/sec: 747.2 +/- 1.6 (jitter = 2.8)\t7.953\r\n20\timages/sec: 745.7 +/- 1.0 (jitter = 3.4)\t7.950\r\n30\timages/sec: 745.9 +/- 0.8 (jitter = 2.6)\t7.942\r\n40\timages/sec: 746.0 +/- 0.6 (jitter = 3.0)\t7.958\r\n50\timages/sec: 745.6 +/- 0.5 (jitter = 2.5)\t7.712\r\n60\timages/sec: 745.2 +/- 0.5 (jitter = 2.4)\t7.921\r\n70\timages/sec: 745.2 +/- 0.4 (jitter = 2.2)\t7.841\r\n80\timages/sec: 745.1 +/- 0.4 (jitter = 2.1)\t7.970\r\n90\timages/sec: 744.9 +/- 0.4 (jitter = 2.1)\t7.788\r\n100\timages/sec: 744.6 +/- 0.3 (jitter = 2.4)\t7.777\r\n----------------------------------------------------------------\r\ntotal images/sec: 743.99\r\n----------------------------------------------------------------\r\n```\r\n\r\n----------------------------------------------\r\n\r\nEach of these runs have been executed on a single GPU:\r\n```shell\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro RTX 8000     Off  | 00000000:1A:00.0 Off |                  Off |\r\n| 33%   43C    P8    14W / 260W |      1MiB / 48601MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\nI only ran them once, however it's seems like the impact is negligible.\r\n```\r\n\r\n@nluehr CC", "> Hi I have benchmarked Tensorflow with/without this PR, here are the results:\r\n\r\nThere is a very small difference (0.3%).  Do we know why that happens?  Or is that just noise?  (I don't think we need to sink time into address the delta, I'm mainly wondering if this is expected.)", "+1 from me, although I would have also provided a pre-commit benchmark (i.e. without NVTX) to see the real delta. Since these are NV specific code in TF, I imagine that they will be refactored behind a trace abstraction in future as jaingaurav@ had suggested.", "@yisitu Not sure to understand, the first benchmark is public upstream. So without nvtx.  ", "Ah - I read \"commit\" as if having been committed. ", "@jbaiocchi can you review the changes w.r.t to your requests ?\r\nThanks", "> @jbaiocchi can you review the changes w.r.t to your requests ?\r\n> Thanks\r\n\r\nLooks much better. Thanks!", "@jbaiocchi seems like most points are addressed. How does it look to you ?", "Hi I have benchmarked Tensorflow with/without this PR, here are the results:\r\n\r\n### Upstream Master - Commit ee7642b (aka. before merge)\r\n\r\n``` shell\r\n$ git clone https://github.com/tensorflow/benchmarks.git\r\n$ cd benchmarks/scripts/tf_cnn_benchmarks/\r\n$ export TF_CPP_MIN_LOG_LEVEL=3\r\n$ python tf_cnn_benchmarks.py --data_format=NCHW --distortions=false --use_fp16=True --model=resnet50 --batch_size=128 --xla=true --num_warmup_batches=200 --num_batches=500\r\n```\r\n\r\n```python\r\nTensorFlow:  2.1\r\nModel:       resnet50\r\nDataset:     imagenet (synthetic)\r\nMode:        training\r\nSingleSess:  False\r\nBatch size:  128 global\r\n             128 per device\r\nNum batches: 500\r\nNum epochs:  0.05\r\nDevices:     ['/gpu:0']\r\nNUMA bind:   False\r\nData format: NCHW\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\n==========\r\nGenerating training model\r\n\r\nDone warm up\r\nStep\tImg/sec\ttotal_loss\r\n1\timages/sec: 738.2 +/- 0.0 (jitter = 0.0)\t7.958\r\n10\timages/sec: 739.8 +/- 0.7 (jitter = 1.9)\t7.800\r\n20\timages/sec: 739.9 +/- 0.5 (jitter = 1.5)\t7.809\r\n30\timages/sec: 739.8 +/- 0.4 (jitter = 2.2)\t7.700\r\n40\timages/sec: 739.8 +/- 0.4 (jitter = 2.1)\t7.777\r\n50\timages/sec: 739.8 +/- 0.3 (jitter = 2.0)\t7.792\r\n[...]\r\n450\timages/sec: 739.1 +/- 0.1 (jitter = 2.1)\t7.633\r\n460\timages/sec: 739.1 +/- 0.1 (jitter = 2.1)\t7.555\r\n470\timages/sec: 739.2 +/- 0.1 (jitter = 2.1)\t7.582\r\n480\timages/sec: 739.2 +/- 0.1 (jitter = 2.1)\t7.549\r\n490\timages/sec: 739.2 +/- 0.1 (jitter = 2.1)\t7.578\r\n500\timages/sec: 739.2 +/- 0.1 (jitter = 2.1)\t7.557\r\n----------------------------------------------------------------\r\ntotal images/sec: 738.66\r\n----------------------------------------------------------------\r\n```\r\n\r\n- RUN 1: 738.66\r\n- RUN 2: 739.57\r\n- RUN 3: 743.06\r\n- **Average:** 740.43\r\n\r\n### This PR - With NVTX Ranges disabled (default behavior equivalent as upstream behavior)\r\n``` shell\r\n$ git clone https://github.com/tensorflow/benchmarks.git\r\n$ cd benchmarks/scripts/tf_cnn_benchmarks/\r\n$ export TF_CPP_MIN_LOG_LEVEL=3\r\n$ python tf_cnn_benchmarks.py --data_format=NCHW --distortions=false --use_fp16=True --model=resnet50 --batch_size=128 --xla=true --num_warmup_batches=200 --num_batches=500 # NVTX Ranges are disabled by default\r\n```\r\n\r\n```python\r\nTensorFlow:  2.1\r\nModel:       resnet50\r\nDataset:     imagenet (synthetic)\r\nMode:        training\r\nSingleSess:  False\r\nBatch size:  128 global\r\n             128 per device\r\nNum batches: 500\r\nNum epochs:  0.05\r\nDevices:     ['/gpu:0']\r\nNUMA bind:   False\r\nData format: NCHW\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\n==========\r\nGenerating training model\r\n\r\nDone warm up\r\nStep\tImg/sec\ttotal_loss\r\nStep\tImg/sec\ttotal_loss\r\n1\timages/sec: 742.8 +/- 0.0 (jitter = 0.0)\t7.956\r\n10\timages/sec: 740.9 +/- 0.9 (jitter = 2.7)\t7.791\r\n20\timages/sec: 740.3 +/- 0.5 (jitter = 1.7)\t7.792\r\n30\timages/sec: 740.2 +/- 0.5 (jitter = 1.6)\t7.687\r\n40\timages/sec: 740.2 +/- 0.5 (jitter = 1.5)\t7.768\r\n50\timages/sec: 740.0 +/- 0.4 (jitter = 1.7)\t7.791\r\n[...]\r\n450\timages/sec: 739.2 +/- 0.1 (jitter = 2.4)\t7.632\r\n460\timages/sec: 739.1 +/- 0.1 (jitter = 2.4)\t7.542\r\n470\timages/sec: 739.1 +/- 0.1 (jitter = 2.4)\t7.597\r\n480\timages/sec: 739.2 +/- 0.1 (jitter = 2.5)\t7.548\r\n490\timages/sec: 739.2 +/- 0.1 (jitter = 2.4)\t7.600\r\n500\timages/sec: 739.2 +/- 0.1 (jitter = 2.4)\t7.556\r\n----------------------------------------------------------------\r\ntotal images/sec: 738.64\r\n----------------------------------------------------------------\r\n```\r\n\r\n- RUN 1: 738.64\r\n- RUN 2: 738.41\r\n- RUN 3: 742.98\r\n- **Average:** 740.01 **(0.06% slower than upstream)**\r\n\r\n### This PR -- With `TF_ENABLE_NVTX_RANGES=1`\r\n``` shell\r\n$ git clone https://github.com/tensorflow/benchmarks.git\r\n$ cd benchmarks/scripts/tf_cnn_benchmarks/\r\n$ export TF_CPP_MIN_LOG_LEVEL=3\r\n$ TF_ENABLE_NVTX_RANGES=1 python tf_cnn_benchmarks.py --data_format=NCHW --distortions=false --use_fp16=True --model=resnet50 --batch_size=128 --xla=true --num_warmup_batches=200 --num_batches=500\r\n```\r\n\r\n```python\r\nTensorFlow:  2.1\r\nModel:       resnet50\r\nDataset:     imagenet (synthetic)\r\nMode:        training\r\nSingleSess:  False\r\nBatch size:  128 global\r\n             128 per device\r\nNum batches: 500\r\nNum epochs:  0.05\r\nDevices:     ['/gpu:0']\r\nNUMA bind:   False\r\nData format: NCHW\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\n==========\r\nGenerating training model\r\n\r\nDone warm up\r\nStep\tImg/sec\ttotal_loss\r\n1\timages/sec: 739.2 +/- 0.0 (jitter = 0.0)\t7.969\r\n10\timages/sec: 741.0 +/- 0.5 (jitter = 1.2)\t7.801\r\n20\timages/sec: 741.8 +/- 0.4 (jitter = 2.0)\t7.795\r\n30\timages/sec: 741.2 +/- 0.4 (jitter = 2.4)\t7.689\r\n40\timages/sec: 740.8 +/- 0.3 (jitter = 1.9)\t7.775\r\n50\timages/sec: 740.9 +/- 0.3 (jitter = 2.3)\t7.796\r\n[...]\r\n450\timages/sec: 741.0 +/- 0.1 (jitter = 2.5)\t7.618\r\n460\timages/sec: 741.0 +/- 0.1 (jitter = 2.5)\t7.559\r\n470\timages/sec: 741.0 +/- 0.1 (jitter = 2.5)\t7.599\r\n480\timages/sec: 741.0 +/- 0.1 (jitter = 2.5)\t7.563\r\n490\timages/sec: 741.0 +/- 0.1 (jitter = 2.5)\t7.567\r\n500\timages/sec: 741.0 +/- 0.1 (jitter = 2.5)\t7.581\r\n----------------------------------------------------------------\r\ntotal images/sec: 740.48\r\n----------------------------------------------------------------\r\n```\r\n\r\n- RUN 1: 740.48\r\n- RUN 2: 740.33\r\n- RUN 3: 739.55\r\n- **Average:** 740.12 **(statically equivalent to when deactivated)**\r\n\r\n### This PR -- With `TF_ENABLE_NVTX_RANGES_DETAILED=1`\r\n\r\n``` shell\r\n$ git clone https://github.com/tensorflow/benchmarks.git\r\n$ cd benchmarks/scripts/tf_cnn_benchmarks/\r\n$ export TF_CPP_MIN_LOG_LEVEL=3\r\n$ TF_ENABLE_NVTX_RANGES_DETAILED=1 python tf_cnn_benchmarks.py --data_format=NCHW --distortions=false --use_fp16=True --model=resnet50 --batch_size=128 --xla=true --num_warmup_batches=200 --num_batches=500\r\n```\r\n\r\n```python\r\nTensorFlow:  2.1\r\nModel:       resnet50\r\nDataset:     imagenet (synthetic)\r\nMode:        training\r\nSingleSess:  False\r\nBatch size:  128 global\r\n             128 per device\r\nNum batches: 500\r\nNum epochs:  0.05\r\nDevices:     ['/gpu:0']\r\nNUMA bind:   False\r\nData format: NCHW\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\n==========\r\nGenerating training model\r\n\r\nDone warm up\r\nStep\tImg/sec\ttotal_loss\r\n1\timages/sec: 712.7 +/- 0.0 (jitter = 0.0)\t7.950\r\n10\timages/sec: 715.8 +/- 0.7 (jitter = 1.9)\t7.812\r\n20\timages/sec: 715.7 +/- 0.5 (jitter = 2.7)\t7.779\r\n30\timages/sec: 716.3 +/- 0.5 (jitter = 2.4)\t7.684\r\n40\timages/sec: 716.2 +/- 0.4 (jitter = 2.1)\t7.760\r\n50\timages/sec: 716.6 +/- 0.4 (jitter = 2.1)\t7.798\r\n[...]\r\n450\timages/sec: 717.4 +/- 0.2 (jitter = 2.7)\t7.617\r\n460\timages/sec: 717.4 +/- 0.2 (jitter = 2.8)\t7.532\r\n470\timages/sec: 717.4 +/- 0.1 (jitter = 2.7)\t7.579\r\n480\timages/sec: 717.4 +/- 0.1 (jitter = 2.8)\t7.557\r\n490\timages/sec: 717.4 +/- 0.1 (jitter = 2.7)\t7.585\r\n500\timages/sec: 717.4 +/- 0.1 (jitter = 2.8)\t7.560\r\n----------------------------------------------------------------\r\ntotal images/sec: 716.89\r\n----------------------------------------------------------------\r\n```\r\n\r\n- RUN 1: 716.89\r\n- RUN 2: 715.05\r\n- RUN 3: 716.59\r\n- **Average:** 716.18\r\n\r\n----------------------------------------------\r\n### Example output in NSYS\r\n\r\n``` shell\r\n$ git clone https://github.com/tensorflow/benchmarks.git\r\n$ cd benchmarks/scripts/tf_cnn_benchmarks/\r\n$ export TF_CPP_MIN_LOG_LEVEL=3\r\n$ TF_ENABLE_NVTX_RANGES=1 nsys profile \\\r\n  --delay=120 \\\r\n  --duration=30 \\\r\n  --sample=cpu \\\r\n  -t 'nvtx,cuda' \\\r\n  -o ./nvtx_tf_cnn_benchmark \\\r\n  python tf_cnn_benchmarks.py \\\r\n    --data_format=NCHW \\\r\n    --distortions=false \\\r\n    --use_fp16=True \\\r\n    --model=resnet50 \\\r\n    --batch_size=128 \\\r\n    --xla=true \\\r\n    --num_warmup_batches=200 \\\r\n    --num_batches=500\r\n```\r\n\r\n![Screenshot from 2020-02-19 16-29-43](https://user-images.githubusercontent.com/10923599/74889508-23ab7500-5336-11ea-9b2e-88dcc05ec380.png)\r\n\r\n----------------------------------------------\r\n### Benchmark Configuration\r\n\r\nEach of these runs have been executed on a single GPU:\r\n```shell\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro RTX 8000     Off  | 00000000:1A:00.0 Off |                  Off |\r\n| 33%   43C    P8    14W / 260W |      1MiB / 48601MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\n### To be Noted\r\n\r\nIn order to accurately measure runs, I had to fixed the GPU frequency to a slightly lower GPU clock to avoid thermal throtling that could have biased the results.\r\n\r\nIf you are facing such issues you could run the following commands to reproduce:\r\n\r\n```shell\r\n# Query the supported graphics clocks (GPU 0)\r\nnvidia-smi --query-supported-clocks=\"gr\" --format=csv -i 0\r\n\r\n# Query the supported memory clocks (GPU 0)\r\nnvidia-smi --query-supported-clocks=\"mem\" --format=csv -i 0\r\n\r\n# set your GPU max clocks: memory_clock,graphics_clocks (GPU 0) => Will be reset at next reboot (or can be manually reset)\r\n# I advised to run once to see which graphics and mem clocks are used by your GPU under normal situation (see last `watch ...` command)\r\n# And set something slightly lower to ensure you max out the allowed GPU freq during runtime.\r\nsudo nvidia-smi --applications-clocks=6501,1710 -i 0\r\n\r\n# Watching the current GPU clocks (GPU 0)\r\nwatch -n 1 nvidia-smi --query-gpu=clocks.gr,clocks.mem --format=csv -i 0\r\n```\r\n----------------------------------------------\r\n\r\n@nluehr @jbaiocchi @sanjoy @cliffwoolley  CC", "@gbaned any update ?", "> @gbaned any update ?\r\n\r\n@jbaiocchi I think this is waiting a review from you?", "@jbaiocchi any update ?", "@CLWNRDW @jbaiocchi @gbaned we should be good now ;)\r\nPlease review", "@nluehr Can you please resolve conflicts? Thanks!", "Will resolve merge conflicts once review is complete.", "@nluehr Sorry for the delay, we will get it reviewed fast once the conflicts are resolved.\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Any updates from @nluehr?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 36349, "title": "Keras Layer With Trainable=False Flag Creates Variables With Trainable=True", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nSystem Version: macOS 10.15.2 (19C57)\r\nKernel Version: Darwin 19.2.0\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): v1.15.0-rc3-22-g590d6eef7e 1.15.0\r\n- Python version: 3.7.0 \r\n\r\n**Describe the current behavior**\r\nWhen I create a Keras model using layerwise structure in tf.keras.Model, and set one or more of the layers to `trainable=False`, this results in a situation where the keras model believes those variables/parameters to be non-trainable (and reports them as such in `model.non_trainable_variables` and `model.summary`) but doesn't set the actual `trainable` flag on the created Tensorflow variables to be False. This can lead to unintuitive behavior if other systems are reasonably expecting `var_obj.trainable` to correspond with the model's actual trainable variables. \r\n\r\n**Describe the expected behavior**\r\nWhen a layer is set to `trainable=False`, Tensorflow variables associated with that layer also have the trainable flag set to False \r\n\r\n**Code to reproduce the issue**\r\n```import tensorflow as tf\r\ninputs = tf.keras.layers.Input(shape=(10,))\r\nhiddens = tf.keras.layers.Dense(15,  trainable=True, name=\"trainable_layer\")(inputs)\r\noutput = tf.keras.layers.Dense(5, trainable=False, name=\"nontrainable_layer\")(hiddens)\r\nmodel = tf.keras.Model(inputs, output)\r\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\r\nprint(f\"Model trainable variables: {model.trainable_variables}\")\r\nprint(f\"Model non-trainable variables: {model.non_trainable_variables}\")\r\nprint(f\"Trainable flags on model non-trainable variables: {[v.trainable for v in model.non_trainable_variables]}\")\r\n```\r\n ", "comments": ["@decodyng  Could you please confirm if the issue still persist in the nightly version [gist](https://colab.research.google.com/gist/Saduf2019/d9046bbe32eadb99b4319ee9d9c712aa/36349.ipynb) shared\r\n\r\n", "@Saduf2019 I'm not sure if you intended me to independently try to install and check for the bug on the nightly version, but in the linked notebook, it does appear that variables in the `non_trainable_variables` set are still having their `trainable` flag incorrectly set to True. ", "@decodyng I am not completely sure about flags but it is doing as expected. When you run `model.summary` you can see correct number of trainable and non-trainable parameters. I also trained on toy data set and showed that trainable params change after training and non-trainable stays as it was instantiated. \r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/e7636b468739e9aa276bcd8ee2643ce0/36349.ipynb). Thanks\r\n\r\nPlease verify once and feel free to close if the issue was resolved for you. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36349\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36349\">No</a>\n"]}, {"number": 36348, "title": "Keras model.train() should automatically run table initializers.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Centos 7, OS X 10.15\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15 and 2.1\r\n- Python version: 2.7 and 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWe have Keras layers that use HashTables (e.g. `tf.lookup.StaticHashTable`), and these tables use initializers such as `tf.lookup.KeyValueTensorInitializer`.  When we perform model training using `model.train()` in **non-eager mode**, it does not run these table initializers and hence causes the training to crash.  Currently, we work around the issue with an ugly hack like this, by saving a reference to the initializer and running it manually. \r\n```\r\n    if not tf.executing_eagerly():\r\n      tf1.keras.backend.get_session().run(self.table._init_op)\r\n```\r\n\r\n**Describe the expected behavior**\r\nCalling model.train() should initialize all initializers, including hash table initializers such as `tf.lookup.KeyValueTensorInitializer`.\r\n\r\n**Code to reproduce the issue**\r\nAny Keras layer using StaticHashTable would repro the problem. See\r\nhttps://gist.github.com/yzhuang/0744b487c7a5ab1b65a5b152a06cda7c#file-keraslayertableinitialization-ipynb\r\n\r\n**Suggestions**\r\nMy suggestion is to support HashTable initialization or publish documentation / guidance on how to use HashTable with Keras Layers.\r\n\r\nThanks! \ud83d\ude4f ", "comments": ["@yzhuang,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Hi @amahendrakar \r\n\r\nHere's a simple colab reproducing this issue on TF 1.15\r\nhttps://gist.github.com/yzhuang/0744b487c7a5ab1b65a5b152a06cda7c#file-keraslayertableinitialization-ipynb\r\n\r\nThis colab shows that the eager mode correctly initializes the table, but non-eager mode does not initialize the table, and Keras Model API does not provide an API for table init ops.", "Was able to reproduce the issue with TF 1.15. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/c41e632bf014b78b8747500ab6d561ba/36348.ipynb). Thanks!", "You can run tf.tables_initailizer, Keras historically doesn't have tables so it doesn't track that.\r\nMoving forward, since we don't have any immediate plans to release 1.16 or backporting this, I think this might be the only way.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36348\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36348\">No</a>\n", "Hi @tanzhenyu \r\n\r\nThis is not just affecting 1.15.  I have also tried 2.1, and the same colab can reproduce the issue in 2.1 as well by calling `tf.compat.v1.disable_eager_execution()`. \r\n\r\nAre there plans to fix this in 2.x, since it is not specific to 1.15.\r\n\r\n\"You can run tf.tables_initailizer\".  In 2.x, we also no longer have access to the underlying sessions, any suggestions on how to run the initializer in TF 2.1?  Thanks.", "> Hi @tanzhenyu\r\n> \r\n> This is not just affecting 1.15. I have also tried 2.1, and the same colab can reproduce the issue in 2.1 as well by calling `tf.compat.v1.disable_eager_execution()`.\r\n> \r\n> Are there plans to fix this in 2.x, since it is not specific to 1.15.\r\n> \r\n> \"You can run tf.tables_initailizer\". In 2.x, we also no longer have access to the underlying sessions, any suggestions on how to run the initializer in TF 2.1? Thanks.\r\n\r\nIf you use tf.compat.v1.disable_eager_execution(), everything runs in graph mode, so you need to use tf.compat.v1.Session() to initialize it."]}, {"number": 36347, "title": "Official release of tensorflow==1.15.2 does not include GPU support", "body": "**System information**\r\n- OS Platform and Distribution: Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): [binary](https://pypi.org/project/tensorflow/1.15.2/#files)\r\n- TensorFlow version: 1.15.2\r\n- Python version: python2, python3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n**Describe the problem**\r\n\r\nAs [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0) Tensorflow 1.15 contained GPU support by default. Tensorflow 1.15.2 no longer has GPU support.\r\n", "comments": ["Related:\r\n\r\n- There is no 1.15.2 version of tensorflow_cpu: https://pypi.org/project/tensorflow-cpu/#history\r\n- The Java version of 1.15.2 hasnt been released either: https://mvnrepository.com/artifact/org.tensorflow/libtensorflow_jni", "There has been an unfortunate change in our build scripts and 1.15.2 uses `tensorflow_gpu` for GPU and `tensorflow` for CPU.", "@mihaimaruseac This isnt a blocker, but it would nice if you can make an announcement about this at least. ", "@pavanky,\r\nIs this still an issue? Please please free to close it if if answers your query.", "@amahendrakar I can build the combined wheel if I build from source, but this is about the official release. \r\n\r\nI dont see the download page updated yet.", "For 1.15 what our scripts did was:\r\n\r\n1. produce `tensorflow_gpu` and `tensorflow` pip package\r\n2. upload `tensorflow_gpu` pip package as it is\r\n3. rename `tensorflow` pip package to `tensorflow_cpu` and upload it under `tensorflow_cpu`\r\n4. rename `tensorflow_gpu` as `tensorflow` and upload it under `tensorflow`\r\n\r\nBoth of these renames are more than just a `cp source.whl target.whl`, they also required changing the metadata in the wheel. But this process caused issues with integrity checks as filenames changed.\r\n\r\nAfter 1.15.0 was released (actually, just before 2.1.0 was released) we fixed all of those issues but the fix was not backported into `r1.15`.\r\n\r\nCurrently, one could think we could attempt to manually do the steps above. Except we cannot. Pypi doesn't allow us to reuse package names (for good reasons) so once we have uploaded `tensorflow_gpu==1.15.2` and `tensorflow==1.15.2` we cannot change them around.\r\n\r\nActually, it is better to not do this since the move above caused `tensorflow_estimator` to depend on `tensorflow_cpu_estimator` for example (due to a bug). So if we wanted to manually do the steps above then we would need to create 3 more patch releases with no actual changes (2 for tensorflow, 1 for estimator).\r\n\r\nWe will update documentation on download page though", "@mihaimaruseac updating the documentation sounds good. this can be closed at that point. ", "Documentation updated. Please let me know if there's something else to change.\r\n\r\nApologies again for not having a single pip anymore on 1.15", "~With all due respect I think **we should release a hotfix ASAP** (say 1.15.3, 1.15.2.1 or something) to provide GPU support. Also, we could remove the 1.15.2 binary from pypi (as we did with 1.15.1). I believe it is obviously an unintended behavior and therefore a bug. I don't think it makes a sense to have a very unconsistent package scheme in the same version series.~  UPDATED: Please see the comments below.\r\n\r\nDue to the promise of v1.15, some people (definitely in certain cases) might have a dependency specification like `tensorflow>=1.15` rather than `tensorflow-gpu>=1.15` in the hope that GPU is enabled. But, in such cases we might end up installing a package **without GPU** (which I did).", "I carefully went through the documentation and the history again, and now I think it also makes a sense to revert the package consolidation behavior for 1.15.x series (which also makes a sense as this is kinda breaking change around v2.0). Given that [the documentation](https://www.tensorflow.org/install/pip) has been already updated, we should keep this one.\r\n\r\nOnly remaining issues would be legacy package specification in some cases, or outdated [announcements](https://groups.google.com/a/tensorflow.org/g/developers/c/iRCt5m4qUz0).\r\n\r\nUPDATE 1: https://www.tensorflow.org/install/gpu still says 1.15 has a consolidated package (which needs to be updated). \r\n\r\nThanks!", "Hi, I also got confused and used the wrong package (tensorflow==1.15.2 without GPU support), because I read this section in the install documentation:\r\nhttps://www.tensorflow.org/install/gpu#older_versions_of_tensorflow\r\n\r\n> For the 1.15 release, CPU and GPU support are included in a single package:\r\n> `pip install --pre \"tensorflow==1.15.*\"`\r\n\r\nPlease add a remark to this section, that it does not apply to the version 1.15.2 (and upwards). It's quite confusing if you explicitly state that the 1.15.x versions do NOT use a separate \"-gpu\" package, when in fact some of them do.\r\n", "Should be handled soon (with a 24hours delay due to the upgrade process). Apologies for missing that page", "@mihaimaruseac  Which cuda version is compatible with tensorflow-gpu==1.15.2 in windows?", "It should be the same CUDA version as 2.0.", "@mihaimaruseac I have tested that it works with cuda 10.0"]}, {"number": 36346, "title": "FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])", "body": "I am receiving this future warning message after compiling my code. I am following this gentleman's video https://www.youtube.com/watch?v=6g4O5UOH304 online. I don't know what kind of complications it might give me further along as we go through the video. Is this something to worry about?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nPlease,provide simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@AlexBritoOfficial \r\n\r\nAny update on this issue please. Thanks!", "Sorry @ravikyram I have been busy with some other work, but I got it to work. ", "@AlexBritoOfficial \r\n\r\nPlease, let us know is this still an issue.Please, close this thread if it is already resolved. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36345, "title": "How can I repeat the generator?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7.0\r\n\r\nThis is my custom generator code as follows:\r\n\r\n\r\n\r\n\r\n```import tensorflow as tf\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\nclass DataGenerator(tf.keras.utils.Sequence):\r\n    def __init__(self, df, batch_size = 32, target_size = (112, 112), shuffle = True):\r\n        self.len_df = len(df)\r\n        self.batch_size = batch_size\r\n        self.target_size = target_size\r\n        self.shuffle = shuffle\r\n        self.class_col = ['black', 'blue', 'brown', 'green', 'red', 'white', \r\n             'dress', 'shirt', 'pants', 'shorts', 'shoes']\r\n        self.generator = ImageDataGenerator(rescale = 1./255)\r\n        self.df_generator = self.generator.flow_from_dataframe(dataframe=df, \r\n                                                          directory='',\r\n                                                            x_col = 'image',\r\n                                                            y_col = self.class_col,\r\n                                                            target_size = self.target_size,\r\n                                                            color_mode='rgb',\r\n                                                            class_mode='other',\r\n                                                            batch_size=self.batch_size,\r\n                                                            seed=42)\r\n        self.colors_df = df['color']\r\n        self.on_epoch_end()\r\n      \r\n    def __len__(self):\r\n        return int(np.floor(self.len_df) / self.batch_size)\r\n    \r\n    def on_epoch_end(self):\r\n        self.indexes = np.arange(self.len_df)\r\n        if self.shuffle:\r\n            np.random.shuffle(self.indexes)\r\n        \r\n    def __getitem__(self, index):\r\n        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\r\n        colors = self.__data_generation(indexes)\r\n        \r\n        images, labels = self.df_generator.__getitem__(index)\r\n        \r\n        # return multi-input and output\r\n        return [images, colors], labels\r\n    \r\n    def __data_generation(self, indexes):\r\n        colors = self.colors_df[indexes]\r\n        \r\n        return colors\r\n```\r\n\r\nBut, it doesn't repeat in fit() with this warning message!\r\n![image](https://user-images.githubusercontent.com/33315343/73481608-836dca80-43df-11ea-8e5a-00e01e82cd87.png)\r\n", "comments": ["After removing the 'steps_per_epoch' and 'validation_steps', it works.\r\n\r\nBut there is problem the whole data is not use.\r\n\r\nMy train data num: 5578\r\nAfter removing arguments, it uses only 5568.\r\n\r\nHow can I solve this problem?\r\nIs there something wrong with my custom code?", "In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "It has been 36 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "+ mistake: close --> reopen\r\n\r\nSorry, Thank you for your check this issue.\r\n\r\nThe full code is as follows:\r\n[full_code with data](https://drive.google.com/open?id=1Q7zSYAZP8cr-1Qi51PD5gjjXl2VZYzSk)\r\n\r\nI haven't solved this problem yet.\r\nUnlike before, I am ready to answer quickly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.", "I will try it.\r\n\r\nThank you and close this issue."]}]