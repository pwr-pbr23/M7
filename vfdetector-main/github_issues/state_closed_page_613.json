[{"number": 35269, "title": "fix model.compile docstring to match Keras", "body": "`loss_weights` should be either a list or a dict, not a tensor. This is how the docstring is in the Keras documentation: https://keras.io/models/model/", "comments": []}, {"number": 35268, "title": "KeyError bug fixed", "body": "Using `dict.keys()` to iterate over dictionary keys is better than `dict.__iter__`. The former is the common interface for `dict` whereas the latter is not very clear.", "comments": ["Please review @alextp @gbaned ", "Yes, I should have added it along with the PR. Will do ", "Closing this since it is a duplicate of PR#34138."]}, {"number": 35267, "title": "Training fails when a multi-output Keras model has one output without a loss function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see minimal example.\r\n- OS Platform and Distribution: Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  N/A\r\n- TensorFlow installed from (source or binary): binary (specifically, `tensorflow/tensorflow:nightly-py3` Docker image)\r\n- TensorFlow version (use command below): 2.1.0-dev20191216\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nA multi-output Keras model compiled so that one output doesn't have a loss function raises an exception when calling `.fit`.\r\n\r\n**Describe the expected behavior**\r\n\r\nTraining should minimise the losses defined for the other output(s).\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\ninput_a = keras.layers.Input(shape=(10,), name=\"input_a\")\r\ninput_b = keras.layers.Input(shape=(20,), name=\"input_b\")\r\noutput_a = keras.layers.Dense(1, name=\"output_a\")(input_a)\r\noutput_b = keras.layers.Dense(1, name=\"output_b\")(input_b)\r\nmodel = keras.Model(inputs=[input_a, input_b], outputs=[output_a, output_b])\r\nmodel.compile(optimizer=\"sgd\", loss={\"output_a\": None, \"output_b\": \"mse\"})\r\n\r\nn = 128\r\ninput_a = np.ones((n, 10))\r\ninput_b = np.ones((n, 20))\r\noutput_a = np.ones((n, 1))\r\noutput_b = np.ones((n, 1))\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(\r\n    ((input_a, input_b), (output_a, output_b))\r\n).batch(64)\r\n\r\nmodel.fit(dataset)\r\n```\r\n\r\nRaises:\r\n\r\n```\r\nValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), for inputs ['output_b'] but instead got the following list of 2 arrays: [<tf.Tensor 'args_2:0' shape=(None, 1) dtype=float64>, <tf.Tensor 'args_3:0' shape=(None, 1) dtype=float64>]...\r\n```", "comments": ["Was able to reproduce the issue with TF version 2.1.0-rc1. Please find the [Gist](https://colab.sandbox.google.com/gist/amahendrakar/2c1f1df27ea0beffcf85ce142a1d21ce/35267.ipynb) here. Thanks!", "You need to define two dictionaries since you have two outputs. Perhaps you can try defining a dummy loss function for one of the inputs.", "Yes, that's what I did in the end:\r\n\r\n```\r\nimport tensorflow.keras.backend as K\r\n\r\ndef null_loss(y_true, y_pred):\r\n    return K.zeros_like(y_true)\r\n```\r\n\r\nPerhaps a more descriptive error message is neccesary to guide users in the right direction? It took me a while to figure out what was going on.", "@ymodak There is a bug in the `with backend.name_scope('loss'):` block of [in training_eager.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager.py#L145).  The block: ```loss_fns = [\r\n        loss_fn for loss_fn in model.loss_functions if loss_fn is not None\r\n    ]``` **excludes** `None` losses from `loss_fns`.  But then the enumerated index `i` in ```for i, loss_fn in enumerate(loss_fns):\r\n      weights = sample_weights[i] if sample_weights else None\r\n      mask = masks[i]\r\n      with backend.name_scope(model.output_names[i] + '_loss'):``` then no longer matches positions of the **included** losses in `model.output_names[...]`.  E.g., if the first loss is `None` and excluded from `loss_fns`, and `loss_fns[0] is model.loss_functions[1]`, then when `i == 0`, the second loss callable gets paired with the name of the first loss.  ", "@tomwphillips this is by design. You do not need to feed target data for the output for which there is no loss function during training. You can pass a dictionary with just the other output like `{'output_b': ...}`.\r\n\r\nTarget data is used for computing loss so in this case it is not required. ", "@pavithrasv I think the following modification of @tomwphillips 's example more clearly shows there is in fact a bug in training_eager.py, please see the comments in code:\r\n```\r\nimport tensorflow as tf\r\n\r\n# a model that forks into two independent heads 'one' and 'two'\r\ninputs = tf.keras.layers.Input(shape=(1,))\r\none = tf.keras.layers.Dense(units=1, name='one')(inputs)\r\ntwo = tf.keras.layers.Dense(units=1, name='two')(inputs)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=dict(one=one, two=two))\r\n\r\nlosses = dict(one=None, two='mse')  # 'one' loss is None, training should not affect weights of 'one'\r\n\r\nmodel.compile(loss=losses, optimizer='adam')\r\n\r\nx = tf.data.Dataset.from_tensor_slices([0., 1.])\r\ny = tf.data.Dataset.from_tensor_slices([1., 2.])\r\nyy = tf.data.Dataset.zip(dict(one=y, two=y))  # targets on both heads, but ONLY 'two' has loss\r\ndataset = tf.data.Dataset.zip((x, yy)).batch(1).repeat()\r\n\r\nmodel.fit(dataset, steps_per_epoch=1000, epochs=10)  # should NOT fit 'one' weights, only 'two' weights\r\n\r\n# printout shows that, unexpectedly, 'one' weights WERE fitted, 'two' weights WERE NOT fitted\r\nfor var in model.trainable_weights: \r\n    print(f'{var.name}: {var.numpy()}')\r\n```\r\nThe incorrect behavior in this example depends on the lexicographic ordering of loss names.  The fit trains on an incorrect loss when a None loss precedes non-None loss.  This is the case when 'one' is None and 'one' < 'two'.  Replacing 'one' with 'z_one', so that 'z_one' > 'two', results in a correct training of 'two', since now the None loss is the last loss.  ", "@mmilosav you are right. ", "Duplicate of #36044", "@tomwphillips Closing this issue as it was the expected behavior. Please feel free to re-open the issue if you have any concerns still. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35267\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35267\">No</a>\n"]}, {"number": 35266, "title": "KeyError bug fixed", "body": "`dict.keys()` is a common interface for `dict` but `__iter__` doesn't have any clear definitions", "comments": []}, {"number": 35265, "title": "R2.1 Release Notes: Move section on tf.debugging.enable_check_numerics to own subsection", "body": "", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35265) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35265) for more info**.\n\n<!-- ok -->"]}, {"number": 35264, "title": "ran out of memory trying to allocate", "body": "It will take up more than 30gb of memory, happening in tensorflow, tensorflow-gpu, tf-nightly\r\n\r\nCode:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\r\n\r\nx_train = x_train.reshape((-1,28,28,1))\r\nx_test = x_test.reshape((-1,28,28,1))\r\n\r\ny_train = keras.utils.to_categorical(y_train, 10)\r\ny_test = keras.utils.to_categorical(y_test, 10)\r\n\r\ninput_shape=(224, 224, 3)\r\ninputs=tf.keras.layers.Input(shape=input_shape)\r\n\r\nx = tf.keras.layers.Flatten()(inputs)\r\noutputs = tf.keras.layers.Dense(10, activation='softmax')(x)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel.compile(optimizer=keras.optimizers.Adam(),\r\n             loss=\"categorical_crossentropy\",\r\n              metrics=['accuracy'])\r\n\r\nx_train=tf.image.resize(x_train,input_shape[:2])\r\nx_train=tf.image.grayscale_to_rgb(x_train)\r\n\r\nx_train=x_train[:128]\r\ny_train=y_train[:128]\r\nmodel.fit(x=x_train,y=y_train,batch_size=1)\r\n```\r\n\r\n\r\n`2019-12-19 22:41:47.467474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-12-19 22:41:52.813348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-12-19 22:41:52.851093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.755\r\npciBusID: 0000:41:00.0\r\n2019-12-19 22:41:52.851257: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-19 22:41:52.851712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-19 22:41:53.319561: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-12-19 22:41:53.323650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.755\r\npciBusID: 0000:41:00.0\r\n2019-12-19 22:41:53.323795: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-19 22:41:53.324400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-19 22:41:53.989669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-19 22:41:53.989780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-12-19 22:41:53.989838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-12-19 22:41:53.990709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9530 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5)\r\n2019-12-19 22:41:54.080141: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 12042240000 exceeds 10% of system memory.\r\n2019-12-19 22:42:15.504057: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 11.21GiB (rounded to 12042240000).  Current allocation summary follows.\r\n2019-12-19 22:42:15.504241: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): \tTotal Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 48B client-requested in use in bin.\r\n2019-12-19 22:42:15.504381: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.504525: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.3KiB allocated for chunks. 1.3KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2019-12-19 22:42:15.504675: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.504821: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.504964: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.505108: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.505252: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.505421: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.505631: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.505849: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.506074: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 1022.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.506468: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.506957: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.507273: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 1. 5.74MiB allocated for chunks. 5.74MiB in use in bin. 5.74MiB client-requested in use in bin.\r\n2019-12-19 22:42:15.507582: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): \tTotal Chunks: 3, Chunks in use: 0. 26.26MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.507965: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.508284: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.508723: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.520336: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.520521: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-12-19 22:42:15.520749: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 11.21GiB was 256.00MiB, Chunk State: \r\n2019-12-19 22:42:15.521083: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 1048576\r\n2019-12-19 22:42:15.521203: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 000000020FC00000 next 1 of size 1280\r\n2019-12-19 22:42:15.521394: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 000000020FC00500 next 4 of size 256\r\n2019-12-19 22:42:15.521591: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 000000020FC00600 next 7 of size 256\r\n2019-12-19 22:42:15.521787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 000000020FC00700 next 8 of size 256\r\n2019-12-19 22:42:15.521961: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 000000020FC00800 next 18446744073709551615 of size 1046528\r\n2019-12-19 22:42:15.522163: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 8388608\r\n2019-12-19 22:42:15.522353: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 000000020FE00000 next 18446744073709551615 of size 8388608\r\n2019-12-19 22:42:15.522590: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 8388608\r\n2019-12-19 22:42:15.522763: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0000000210600000 next 18446744073709551615 of size 8388608\r\n2019-12-19 22:42:15.523051: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 16777216\r\n2019-12-19 22:42:15.523236: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0000000210E00000 next 6 of size 6021120\r\n2019-12-19 22:42:15.523468: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 00000002113BE000 next 18446744073709551615 of size 10756096\r\n2019-12-19 22:42:15.523719: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: \r\n2019-12-19 22:42:15.523923: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 256 totalling 768B\r\n2019-12-19 22:42:15.524051: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.3KiB\r\n2019-12-19 22:42:15.524201: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 6021120 totalling 5.74MiB\r\n2019-12-19 22:42:15.524454: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 5.74MiB\r\n2019-12-19 22:42:15.524644: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 34603008 memory_limit_: 9993660007 available bytes: 9959056999 curr_region_allocation_bytes_: 33554432\r\n2019-12-19 22:42:15.524974: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: \r\nLimit:                  9993660007\r\nInUse:                     6023168\r\nMaxInUse:                 22799872\r\nNumAllocs:                      20\r\nMaxAllocSize:              8388608\r\n\r\n2019-12-19 22:42:15.525331: W tensorflow/core/common_runtime/bfc_allocator.cc:424] *__________________________________________________******************_______________________________\r\n`", "comments": ["You may try to  use [limit gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) parameter by putting following snippet on top of your code.\r\nIf using TF 2.X\r\n```python\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n```\r\nFor TF 1.X\r\n```python\r\nimport tensorflow as tf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n```", "> You may try to use [limit gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) parameter by putting following snippet on top of your code.\r\n> If using TF 2.X\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> gpus = tf.config.experimental.list_physical_devices('GPU')\r\n> tf.config.experimental.set_memory_growth(gpus[0], True)\r\n> ```\r\n> \r\n> For TF 1.X\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> sess = tf.Session(config=config)\r\n> ```\r\n\r\nThanks, I tried \r\n`tf.config.experimental.set_memory_growth (tf.config.experimental.list_physical_devices ('GPU') [0], True)`\r\nbut it still gives an error, and the same error is reported in versions other than GPU", "When I use the cpu version, it will not crash, but it will fill up all my memory(more than 30gb) and get stuck", "Sorry, it's because x_train = tf.image.resize (x_train, input_shape [: 2])\r\nx_train = tf.image.grayscale_to_rgb (x_train) takes up too much memory, thank you very much", "I'm having the same error. How did you solve it?", "> I'm having the same error. How did you solve it?\r\n\r\nReduce the number of samples in memory.", "Umm Can you show me a code?", "> You may try to use [limit gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) parameter by putting following snippet on top of your code.\r\n> If using TF 2.X\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> gpus = tf.config.experimental.list_physical_devices('GPU')\r\n> tf.config.experimental.set_memory_growth(gpus[0], True)\r\n> ```\r\n> \r\n> For TF 1.X\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> sess = tf.Session(config=config)\r\n> ```\r\n\r\nno it doesnt rezolve\r\n", "I'm using Tensorflow 2.4.x My notebook has a NVIDIA GForce 920M (2GB RAM) and I tried to use set_memory_growth, but it doesn't worked. And I tried to limit memory to 1GB, also doesn't worked. So I limited memory utilization to 1.5GB and it worked.\r\n\r\n```\r\ndef limitgpu(maxmem):\r\n\tgpus = tf.config.list_physical_devices('GPU')\r\n\tif gpus:\r\n\t\t# Restrict TensorFlow to only allocate a fraction of GPU memory\r\n\t\ttry:\r\n\t\t\tfor gpu in gpus:\r\n\t\t\t\ttf.config.experimental.set_virtual_device_configuration(gpu,\r\n\t\t\t\t\t\t[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=maxmem)])\r\n\t\texcept RuntimeError as e:\r\n\t\t\t# Virtual devices must be set before GPUs have been initialized\r\n\t\t\tprint(e)\r\n\r\n\r\n# 1.5GB\r\nlimitgpu(1024+512) \r\n\r\n```"]}, {"number": 35263, "title": "[XLALite] Print the name that should be used to enable it.", "body": "This make it easier to find which operation. For example, the nodename can be AvgPool2d, while the TF operation is AvgPool.", "comments": []}, {"number": 35262, "title": "Add self-contained hexagon delegate lib", "body": "Adds self-contained target for experimental hexagon delegate.\r\nIt is mostly the same as GPU delegate, so that C++ user could build it it in a similar fashion: single self-contained target\r\n\r\nHope you don't mind it.", "comments": ["Hi @DoumanAsh \r\nThanks for sending this.\r\nCan you first explain what is the goal from having this self contained shared library ?\r\n\r\nlibtensorflowlite_hexagon_jni.so exposes both C and Java APIs\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/delegates/hexagon/java/BUILD#L12\r\n\r\nThanks", "@karimnosseir We use tensorflow lite C/C++ API only  so we don't really need Java APIs.\r\nThis PR only serves a purpose for consumers of C API only.\r\nWhile I do not mind Java orientation of tensorflow, I believe C and C++ consumers should be equally respected by havin respective targets for both main library and delegates (and there are for both tflite and gpu delegate for example)", "Just a side note: I was able successfully to use this target in our code base (NDK based).", "Hi,\r\n\r\nSorry for the late reply as i was out of office.\r\n\r\nWe totally support C/C++ users as Java users. The goal is to provide one shared library which produces C and Java APIs (as long as it is not hurting the size significantly).\r\nEven our AARs now includes the header files needed\r\nsee the GPU for example https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/BUILD#L47\r\n\r\nHope our goal is clearer now, and thanks for your change.\r\n\r\nRegarding your change,\r\nYou're exporting a lot of symbols which will create a big shared library, this library needs trimming.\r\nSee my comment on the change.\r\n\r\nI will not close the Pull request, will leave it to answer any questions you have and you can close it.\r\n\r\nThanks again", "@karimnosseir Sorry I'm not sure I follow you here.\r\nYou're basically suggesting to make another JNI library out of this target, while it includes only C/C++ code\r\n\r\nWith `-fvisibility=hidden` it only exports C public API (marked with `TFL_CAPI_EXPORT`)\r\nMy goal was only to create GPU library like https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/BUILD#L144\r\n\r\nSo just to clarify: this change is not needed?\r\nWe can always patch BUILD ourself to add this target ofc", "What i am saying is that your change is already part of the libtensorflowlite_hexagon_jni.so \r\nYou can use libtensorflowlite_hexagon_jni.so instead of this change.\r\n\r\nThanks"]}, {"number": 35261, "title": "Add legalization for missing ops from LHLO to LinAlg.", "body": "", "comments": ["@dfki-ehna Can you please resolve conflicts? Thanks!", "@dfki-ehna  Can you please resolve conflicts? Thanks!", "@gbaned The problem is on the MLIR side. We are working on it.", "@pifon2a Any update on this PR, please. Thanks!", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 35260, "title": "gfile read causes core dump", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, (running in docker)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Docker image tensorflow/tensorflow:nightly-gpu-py3\r\n- **TensorFlow version (use command below)**: 2.1.0-dev20191106\r\n- **Python version**: 3.6.8\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: \r\nsource_ref is link to image in s3\r\nimage_data = tf.io.gfile.GFile(source_ref, 'rb').read()\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nWhen the read is executing the core gets dumped\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nimport tensorflow as tf\r\n\r\nsource_ref is link to image in s3\r\nimage_data = tf.io.gfile.GFile(source_ref, 'rb').read()\r\n\r\nfree(): invalid pointer\r\nAborted (core dumped)\r\n", "comments": ["@dikvangenuchten Can you please reproduce this issue in google colab and share the gist. Thanks!", "Please provide answers to the template question so we can reproduce.", "Does the error only happen on s3?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@mihaimaruseac Can we close this issue?", "I think so. Closing due to no responses from issue opener."]}, {"number": 35259, "title": "Why list, dictionary and tuple are taken value vice versa when declare it in one-line?", "body": "I am feeling very strange when I am working on list, dictionary and tuple in python.\r\n\r\nWhen we declare multiple variables in one line like a = b = c = 0 it behaves like separate variables.\r\n\r\nIf we are update value any of the above variables then it will not affect another variable like below.\r\n\r\na = 10\r\nb = 11\r\nc = 13\r\nBut, this thing is not applying with list, tuple and dictionary. If we declare blank list like below.\r\n\r\na = b = c = []\r\n\r\nNow, I am appending the value of only a list.\r\n\r\na.append('Testing')\r\nNow, b and c is automatically assigned that value.\r\n\r\nThis thing also happens with dictionary and tuple also.\r\n\r\nCan anyone please help to solve this issue. Please don't suggest to declare it in a different line.", "comments": ["@Mayurjotaniya3, When we declare multiple variables as a list, tuple and dictionary, all the variables shows unique number representing the identity of an object. Unique identity number can be identified using `id` function . \r\na=b=c=[]   # a,b,c will have same `id` number.\r\n\r\nFor more read https://stackoverflow.com/questions/16348815/python-assigning-multiple-variables-to-same-value-list-behavior.\r\n\r\nClosing this issue since its not related to Tensorflow. Please feel free to reopen if its Tensorflow issue by providing all the information asked in Template. Thanks!"]}, {"number": 35258, "title": "install issue", "body": "I downloaded the win_amd64.whl for python 3.6 in Windows, because when i write \r\n\r\n`pip install tensorflow`\r\n\r\nit says it could not find a version that satisfies the requirement from version none\r\nSo i downloaded it manually and when i install it with\r\n\r\n`pip install tensorflow-2.0.0-cp36-cp36m-win_amd64.whl`\r\n\r\nIt says that it's not compatible with my platform\r\nWhat should i do?", "comments": ["`pip debug --verbose`. Most likely your python version is on 32 bits, and we don't provide wheels for that platform. See #32315\r\n\r\nPlease fill in issue template too.", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to no activity. Please reopen filling in the template if it is still an issue."]}, {"number": 35257, "title": "tensorflow 2.0 tf.linalg.normalize yields nan", "body": "### tf.linalg.normalize(np.zeros([10, 4]), ord=1, axis=-1) yields nan as below\r\n\r\n(<tf.Tensor: id=58, shape=(10, 4), dtype=float64, numpy=\r\n array([[nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan],\r\n        [nan, nan, nan, nan]])>,\r\n <tf.Tensor: id=57, shape=(10, 1), dtype=float64, numpy=\r\n array([[0.],\r\n        [0.],\r\n        [0.],\r\n        [0.],\r\n        [0.],\r\n        [0.],\r\n        [0.],\r\n        [0.],\r\n        [0.],\r\n        [0.]])>)\r\n\r\n\r\n* I know this is caused by divide by 0, so in the future tensorflow should make this operation more numerically stable. ", "comments": ["\r\nI have tried on colab with TF version 2.0 ,2.1.0-rc1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/050a8b84e6e584ca6567697b674e8230/untitled490.ipynb). Thanks!", "This seems to be working as intended. How would you normalize 0? The NaN tell you that you tried something that is not well defined, exactly as intended.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35257\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35257\">No</a>\n", "@hyperji You can use `tf.math.l2_normalize` instead. ( https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize )"]}, {"number": 35256, "title": "Add support of hadoop archive", "body": "Fix PR https://github.com/tensorflow/tensorflow/pull/35094 that has been reverted\r\n\r\nThe fix is now I use the TF macro to raise error status in case of error of the new Split method. It should fix the compilation error.", "comments": []}, {"number": 35255, "title": "Fix typos conv2d, conv2d_v2 docstrings", "body": "Very minor typos in conv2d, con2d_v2 docstring. \r\n\r\nCurrently:\r\n\r\n  Must have `strides[0] = strides[3] = 1`.  For the most common case of the same\r\n  horizontal and **vertices** strides, `strides = [1, stride, stride, 1]`.\r\n\r\nProposed fix:\r\n\r\n  Must have `strides[0] = strides[3] = 1`.  For the most common case of the same\r\n  horizontal and **vertical** strides, `strides = [1, stride, stride, 1]`.\r\n", "comments": []}, {"number": 35254, "title": "module 'tensorflow._api.v1.keras.layers' has no attribute 'DenseFeatures'", "body": "I want to use feature column as input to my customed model. I do as the answer \r\n of the question suggested,   \r\nhttps://stackoverflow.com/questions/54375298/how-to-use-tensorflow-feature-columns-as-input-to-a-keras-model\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns=feature_columns)\r\n\r\nbut i got the error as below:\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-23-db9a4807151e> in <module>\r\n----> 1 feature_layer = tf.keras.layers.DenseFeatures(feature_columns=feature_columns)\r\n      2 # feature_layer_outputs = feature_layer(feature_layer_inputs)\r\n\r\nAttributeError: module 'tensorflow._api.v1.keras.layers' has no attribute 'DenseFeatures'\r\n\r\n\r\nI use tensorflow 1.14 and keras version 2.2.4\r\nDoes anyone could tell me how to fix it, thanks very much!", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "One thing you can try is using tf.compat.v1.keras.layers.DenseFeatures if your project requires TF 1.14. If not, consider upgrading to TensorFlow 2.0", "> In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\nmy code snippet is basically like this:\r\n\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom tensorflow import feature_column\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\ndataframe.head()\r\n\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\nprint(len(train), 'train examples')\r\nprint(len(val), 'validation examples')\r\nprint(len(test), 'test examples')\r\n\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\nbatch_size = 5 # A small batch sized is used for demonstration purposes\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nage = feature_column.numeric_column(\"age\")\r\nfeature_columns = []\r\nfeature_layer_inputs = {}\r\n\r\n\r\nfor header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n  feature_columns.append(feature_column.numeric_column(header))\r\n  feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)\r\n\r\nage_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\nfeature_columns.append(age_buckets)\r\n\r\n\r\nthal = feature_column.categorical_column_with_vocabulary_list(\r\n      'thal', ['fixed', 'normal', 'reversible'])\r\nthal_one_hot = feature_column.indicator_column(thal)\r\nfeature_columns.append(thal_one_hot)\r\nfeature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)\r\n\r\nbatch_size = 32\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)", "> One thing you can try is using tf.compat.v1.keras.layers.DenseFeatures if your project requires TF 1.14. If not, consider upgrading to TensorFlow 2.0\r\n\r\nYes, my project requires TF1.14, I tried tf.compat.v1.keras.layers.DenseFeatures but it did not work.  I will try TensorFlow 2.0. ", "@WakerKQ,\r\nI was able to run the code with TF version 1.14, without any errors. Please check for the attached [Gist here](https://colab.sandbox.google.com/gist/amahendrakar/94f228a23ba7881553acd24dfd52e2f8/35254.ipynb). Thanks! ", "> @WakerKQ,\r\n> I was able to run the code with TF version 1.14, without any errors. Please check for the attached [Gist here](https://colab.sandbox.google.com/gist/amahendrakar/94f228a23ba7881553acd24dfd52e2f8/35254.ipynb). Thanks!\r\n\r\nThanks for your helpful answers, both should work.   I upgraded to TF2.0 and solved the problem.", "Closing issue as it is resolved."]}, {"number": 35253, "title": "tensorflow build fails", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform Windows 10 Pro\r\n- desktop computer:\r\n- TensorFlow installed from source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: installed via command line / git clone etc.\r\n- Bazel version (if compiling from source): 1.1.0\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2017 Redistributables\r\n- CUDA/cuDNN version: --- (ROCm selected)\r\n- GPU model and memory:  AMD FX-8800P R7\r\n\r\nBuild fails, see listing below.\r\n\r\nC:\\Users\\Bludorf\\tensorflow>python ./configure.py\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: Waiting for server process to terminate (waited 5 seconds, waiting at most 60)\r\nWARNING: Waiting for server process to terminate (waited 10 seconds, waiting at most 60)\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 1.1.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\Bludorf\\AppData\\Local\\Programs\\Python\\Python37\\python.exe]: \r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\Bludorf\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\Bludorf\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: y\r\nROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\nC:\\Users\\Bludorf\\tensorflow>bazel build --config=v2 //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Writing tracer profile to 'C:/users/bludorf/_bazel_bludorf/jkbqqwso/command.profile.gz'\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/Bludorf/AppData/Local/Programs/Python/Python37/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\bludorf\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from c:\\users\\bludorf\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Bludorf/AppData/Local/Programs/Python/Python37/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Bludorf/AppData/Local/Programs/Python/Python37/lib/site-packages --python_path=C:/Users/Bludorf/AppData/Local/Programs/Python/Python37/python.exe --config=xla --config=rocm --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\bludorf\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\users\\bludorf\\tensorflow\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:xla in file c:\\users\\bludorf\\tensorflow\\.tf_configure.bazelrc: --define with_xla_support=true\r\nINFO: Found applicable config definition build:rocm in file c:\\users\\bludorf\\tensorflow\\.bazelrc: --crosstool_top=@local_config_rocm//crosstool:toolchain --define=using_rocm=true --define=using_rocm_hipcc=true --action_env TF_NEED_ROCM=1\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\bludorf\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:windows in file c:\\users\\bludorf\\tensorflow\\.bazelrc: --copt=/w --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --incompatible_windows_native_test_wrapper --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\bludorf\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - C:/users/bludorf/tensorflow/WORKSPACE:37:1\r\nERROR: An error occurred during the fetch of repository 'io_bazel_rules_docker':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 177\r\n                _clone_or_update(ctx)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 36, in _clone_or_update\r\n                git_repo(ctx, directory)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 91, in git_repo\r\n                _update(ctx, git_repo)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 101, in _update\r\n                init(ctx, git_repo)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 115, in init\r\n                _error(ctx.name, cl, st.stderr)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 181, in _error\r\n                fail(<1 more arguments>)\r\nerror running 'git init C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker' while working with @io_bazel_rules_docker:\r\njava.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(\"git\" init C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker): Das System kann die angegebene Datei nicht finden.\r\n (error: 2)\r\nERROR: no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 177\r\n                _clone_or_update(ctx)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 36, in _clone_or_update\r\n                git_repo(ctx, directory)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 91, in git_repo\r\n                _update(ctx, git_repo)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 101, in _update\r\n                init(ctx, git_repo)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 115, in init\r\n                _error(ctx.name, cl, st.stderr)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 181, in _error\r\n                fail(<1 more arguments>)\r\nerror running 'git init C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker' while working with @io_bazel_rules_docker:\r\njava.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(\"git\" init C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker): Das System kann die angegebene Datei nicht finden.\r\n (error: 2)\r\nERROR: no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 177\r\n                _clone_or_update(ctx)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 36, in _clone_or_update\r\n                git_repo(ctx, directory)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 91, in git_repo\r\n                _update(ctx, git_repo)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 101, in _update\r\n                init(ctx, git_repo)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 115, in init\r\n                _error(ctx.name, cl, st.stderr)\r\n        File \"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 181, in _error\r\n                fail(<1 more arguments>)\r\nerror running 'git init C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker' while working with @io_bazel_rules_docker:\r\njava.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(\"git\" init C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker): Das System kann die angegebene Datei nicht finden.\r\n (error: 2)\r\nINFO: Elapsed time: 7.563s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\n", "comments": ["I could verify that this problem happens in version bazel 1.0.0 and higher. Lower versions are not allowed to build tensorflow:\r\n\r\n-------> bazel 0.29.1\r\n\r\nC:\\Users\\Bludorf\\tensorflow>bazel version\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nBuild label: 0.29.1\r\nBuild target: bazel-out/x64_windows-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Sep 10 13:45:30 2019 (1568123130)\r\nBuild timestamp: 1568123130\r\nBuild timestamp as int: 1568123130\r\n\r\nC:\\Users\\Bludorf\\tensorflow>python ./configure.py\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: Waiting for server process to terminate (waited 5 seconds, waiting at most 60)\r\nWARNING: Waiting for server process to terminate (waited 10 seconds, waiting at most 60)\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.29.1 installed.\r\nPlease upgrade your bazel installation to version 1.0.0 or higher to build TensorFlow!\r\n\r\nC:\\Users\\Bludorf\\tensorflow>\r\n\r\n-------> bazel 1.0.0\r\n\r\nC:\\Users\\Bludorf\\tensorflow>bazel version\r\nStarting local Bazel server and connecting to it...\r\nServer crashed during startup. Now printing c:\\users\\bludorf\\_bazel_bludorf\\jkbqqwso\\server\\jvm.out\r\nI/O Error: C:/users/bludorf/_bazel_bludorf/jkbqqwso/server/server_info.rawproto.tmp -> C:/users/bludorf/_bazel_bludorf/jkbqqwso/server/server_info.rawproto (Permission denied)\r\n\r\nC:\\Users\\Bludorf\\tensorflow>", "By now, this situation is without solution, since newer bazel versions than 1.1.0 are not accepted to build tensorflow.\r\nDid anyone ever build tensorflow on Windows successfully? It doesn't seem so.\r\nBy the way, on Ubuntu tensorflow doesn't build from source either, from other (but similar) reasons. So it looks like all amd gpu's that need opengl are unable to be used with tensorflow.", "I was able to build TensorFlow 2.0 with CUDA support successfully on Windows 10.\r\nI needed following to be added at the top of my workspace file in order to build successfully:\r\n```\r\nhttp_archive(\r\nname = \"io_bazel_rules_docker\",\r\nsha256 = \"aed1c249d4ec8f703edddf35cbe9dfaca0b5f5ea6e4cd9e83e99f3b0d1136c3d\",\r\nstrip_prefix = \"rules_docker-0.7.0\",\r\nurls = [\"https://github.com/bazelbuild/rules_docker/archive/v0.7.0.tar.gz\"],\r\n)\r\n```\r\nThe fix was suggested by discussion in another issue [here](https://github.com/tensorflow/tensorflow/issues/27576#issuecomment-504703397). I'm using Windows 10, Python 3.7 in Anaconda virtual environment, Visual Studio 2017, Bazel 0.28.0", "I can confirm that the solution given by @ashutoshbsathe does indeed work under Windows. Of course it is preferable to update rules_docker to the latest version possible (I'm currently building using 0.12.0). For your convenience, this is the updated rule:\r\n\r\n```\r\nhttp_archive(\r\n    name = \"io_bazel_rules_docker\",\r\n    sha256 = \"413bb1ec0895a8d3249a01edf24b82fd06af3c8633c9fb833a0cb1d4b234d46d\",\r\n    strip_prefix = \"rules_docker-0.12.0\",\r\n    urls = [\"https://github.com/bazelbuild/rules_docker/archive/v0.12.0.tar.gz\"],\r\n)\r\n```", "@FranzBl,\r\nIs this still an issue?\r\n\r\nCould you please follow the official [build form source guide](https://www.tensorflow.org/install/source_windows) and check if you are facing the same issue with TF v2.4 as well? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35253\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35253\">No</a>\n"]}, {"number": 35252, "title": "Issue - Blas GEMM launch failed", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\n```\r\nimport math\r\nimport requests\r\nimport datetime\r\nfrom config import config\r\nfrom db_analysis import LotteryDatabase\r\nimport pandas as pd\r\n\r\nfrom sklearn import model_selection\r\n\r\nfrom sklearn.metrics import (accuracy_score,\r\n                             recall_score,\r\n                             precision_score,\r\n                             f1_score,\r\n                             roc_curve,\r\n                             auc, roc_auc_score,\r\n                             confusion_matrix)\r\n\r\nfrom hyperopt import STATUS_OK, Trials, tpe, hp, fmin, space_eval\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers.core import Dense, Dropout, Activation\r\n\r\nspace = {'choice': hp.choice('num_layers',\r\n                                         [{'layers': 'two', },\r\n                                          {'layers': 'three',\r\n                                           'units3': hp.choice('units3', range(64, 256)),\r\n                                           'dropout3': hp.uniform('dropout3', .25, .75)}\r\n                                          ]),\r\n                     'units1': hp.choice('units1', range(64, 256)),\r\n                     'units2': hp.choice('units2', range(64, 256)),\r\n\r\n                     'dropout1': hp.uniform('dropout1', .25, .75),\r\n                     'dropout2': hp.uniform('dropout2', .25, .75),\r\n\r\n                     'batch_size': hp.choice('batch_size', range(5, 10)),\r\n\r\n                     'nb_epochs': 100,\r\n                     'optimizer': hp.choice('optimizer', ['adadelta', 'adam', 'rmsprop']),\r\n                     'activation': 'relu'\r\n                     }\r\n\r\nclass TestTF:\r\n\r\n    def __init__(self):\r\n\r\n        self.ldb = LotteryDatabase(config['database']) - connect to sqlite database\r\n\r\n        self.x = None\r\n        self.y = None\r\n\r\n        self.x_train = None\r\n        self.x_validation = None\r\n        self.y_train = None\r\n        self.y_validation = None\r\n\r\n    def main_tf(self):\r\n\r\n        dataset = pd.concat(self.generate_df_pieces(self.ldb.conn, 100000, offset=0, ids=5000)) - pull data from sqlite database in chunks total of 1.2 mln samples\r\n        array = dataset.values\r\n\r\n        self.x = array[:, :150]\r\n        self.y = array[:, 150]\r\n\r\n        self.x_train, self.x_validation, self.y_train, self.y_validation = model_selection.train_test_split(\r\n            self.x, self.y, test_size=0.2, random_state=42)\r\n\r\n        bayes_trials = Trials()\r\n\r\n        best = fmin(fn=self.keras_objective, space=space, algo=tpe.suggest, max_evals=50, trials=bayes_trials)\r\n\r\n        print(best)\r\n\r\n        for bt in bayes_trials:\r\n            print(bt['result']['loss'])\r\n            print(bt['result']['params']\r\n\r\n    def keras_objective(self, params):\r\n\r\n        model = Sequential()\r\n        model.add(Dense(output_dim=params['units1'], input_dim=int(self.x_train.shape[1])))\r\n        model.add(Activation(params['activation']))\r\n        model.add(Dropout(params['dropout1']))\r\n\r\n        model.add(Dense(output_dim=params['units2'], init=\"glorot_uniform\"))\r\n        model.add(Activation(params['activation']))\r\n        model.add(Dropout(params['dropout2']))\r\n\r\n        if params['choice']['layers'] == 'three':\r\n            model.add(Dense(output_dim=params['choice']['units3'], init=\"glorot_uniform\"))\r\n            model.add(Activation(params['activation']))\r\n            model.add(Dropout(params['choice']['dropout3']))\r\n\r\n        model.add(Dense(1))\r\n        model.add(Activation('sigmoid'))\r\n        model.compile(loss='binary_crossentropy', optimizer=params['optimizer'])\r\n\r\n        model.fit(self.x_train, self.y_train, nb_epoch=params['nb_epochs'], batch_size=params['batch_size'], verbose=0)\r\n\r\n        pred_auc = model.predict_proba(self.x_validation, batch_size=10, verbose=0)\r\n        acc = roc_auc_score(self.y_validation, pred_auc)\r\n        # print('AUC:', acc)\r\n        # sys.stdout.flush()\r\n\r\n        return {'loss': -acc, 'params': params}\r\n\r\n\r\nif __name__ == '__main__':\r\n    TF = TestTF()\r\n    TF.main_tf()\r\n```\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7 Ultimate 64bit\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version:\r\n\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Wed_Oct_23_19:32:27_Pacific_Daylight_Time_2019\r\nCuda compilation tools, release 10.2, V10.2.89\r\n\r\n- GPU model and memory:\r\n\r\nGTX 1050Ti 4GB\r\n\r\nGPU Engine Specs\r\nCUDA Cores\r\n768\r\nGraphics Clock (MHz)\r\n1290\r\nProcessor Clock (MHz)\r\n1392\r\nGraphics Performance\r\nhigh-6747\r\nMemory Specs\r\nMemory Clock\r\n7 Gbps\r\nStandard Memory Config\r\n4 GB\r\nMemory Interface\r\nGDDR5\r\nMemory Interface Width\r\n128-bit\r\nMemory Bandwidth (GB/sec)\r\n112\r\nFeature Support\r\nSupported Technologies\r\nCUDA, 3D Vision, PhysX, NVIDIA G-SYNC\u2122, Ansel\r\nThermal and Power Specs\r\nMaximum GPU Temperature (in C)\r\n97\r\nMaximum Graphics Card Power (W)\r\n75\r\nMinimum System Power Requirement (W)\r\n300\r\n\r\n**Describe the current behavior**\r\n\r\nTesting keras with hyperopt on my model (total shape approx. [1200000, 150]\r\n\r\nReceving error below (nvidia-smi below error code):\r\n\r\nWhat I have tried so far:\r\n-installing different CUDAA/cuDNN/tenserflow configurations\r\n-updating packages\r\n-checking for additional nvidia-smi processes\r\n- batch_size manimulation range(1, 64)\r\n\r\n```\r\nE:/GitHub Repositories/MYOPM/test.py:139: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=126, units=114)`\r\n  model.add(Dense(output_dim=params['units1'], input_dim=int(self.x_train.shape[1])))\r\n\r\n2019-12-19 06:49:34.323001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-12-19 06:49:34.474001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392\r\npciBusID: 0000:01:00.0\r\n2019-12-19 06:49:34.474001: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-19 06:49:34.503001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-19 06:49:34.516001: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-12-19 06:49:34.539001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392\r\npciBusID: 0000:01:00.0\r\n2019-12-19 06:49:34.539001: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-19 06:49:34.546001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-19 06:49:46.336401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-19 06:49:46.336401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-12-19 06:49:46.337401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-12-19 06:49:46.404401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3374 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:Large dropout rate: 0.592226 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\r\nE:/Programowanie/GitHub Repositories/MYOPM/test.py:143: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=88, kernel_initializer=\"glorot_uniform\")`\r\n  model.add(Dense(output_dim=params['units2'], init=\"glorot_uniform\"))\r\n\r\nE:/GitHub Repositories/MYOPM/test.py:156: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\r\n  model.fit(self.x_train, self.y_train, nb_epoch=params['nb_epochs'], batch_size=params['batch_size'], verbose=0)\r\n\r\n2019-12-19 06:49:49.410401: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found\r\n2019-12-19 06:49:49.410401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR\r\n2019-12-19 06:49:49.515401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR\r\n2019-12-19 06:49:49.516401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR\r\n2019-12-19 06:49:49.577401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR\r\n2019-12-19 06:49:49.578401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR\r\n2019-12-19 06:49:49.586401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR\r\n2019-12-19 06:49:49.586401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR\r\n2019-12-19 06:49:49.587401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR\r\n2019-12-19 06:49:49.666401: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR\r\n2019-12-19 06:49:49.667401: W tensorflow/stream_executor/stream.cc:1919] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n2019-12-19 06:49:49.667401: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Blas GEMM launch failed : a.shape=(9, 126), b.shape=(126, 114), m=9, n=114, k=126\r\n\t [[{{node dense_1/MatMul}}]]\r\nTraceback (most recent call last):\r\n  0%|          | 0/50 [00:15<?, ?it/s, best loss: ?]\r\n  File \"E:/GitHub Repositories/MYOPM/test.py\", line 168, in <module>\r\n    TF.main_tf()\r\n  File \"E:/GitHub Repositories/MYOPM/test.py\", line 104, in main_tf\r\n    best = fmin(fn=self.keras_objective, space=space, algo=tpe.suggest, max_evals=50, trials=bayes_trials)\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\hyperopt\\fmin.py\", line 403, in fmin\r\n    show_progressbar=show_progressbar,\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\hyperopt\\base.py\", line 651, in fmin\r\n    show_progressbar=show_progressbar)\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\hyperopt\\fmin.py\", line 422, in fmin\r\n    rval.exhaust()\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\hyperopt\\fmin.py\", line 276, in exhaust\r\n    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\hyperopt\\fmin.py\", line 241, in run\r\n    self.serial_evaluate()\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\hyperopt\\fmin.py\", line 141, in serial_evaluate\r\n    result = self.domain.evaluate(spec, ctrl)\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\hyperopt\\base.py\", line 856, in evaluate\r\n    rval = self.fn(pyll_rval)\r\n  File \"E:/Programowanie/GitHub Repositories/MYOPM/test.py\", line 156, in keras_objective\r\n    model.fit(self.x_train, self.y_train, nb_epoch=params['nb_epochs'], batch_size=params['batch_size'], verbose=0)\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 1239, in fit\r\n    validation_freq=validation_freq)\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\keras\\engine\\training_arrays.py\", line 196, in fit_loop\r\n    outs = fit_function(ins_batch)\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 3740, in __call__\r\n    outputs = self._graph_fn(*converted_inputs)\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1081, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1121, in _call_impl\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"D:\\Programs\\Python\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError:  Blas GEMM launch failed : a.shape=(9, 126), b.shape=(126, 114), m=9, n=114, k=126\r\n\t [[node dense_1/MatMul (defined at D:\\Programs\\Python\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_1201]\r\n\r\nFunction call stack:\r\nkeras_scratch_graph\r\n\r\nThu Dec 19 06:53:46 2019\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 441.22       Driver Version: 441.22       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 105... WDDM  | 00000000:01:00.0  On |                  N/A |\r\n| 30%   30C    P8    N/A /  75W |    262MiB /  4096MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0       520    C+G   C:\\Windows\\system32\\Dwm.exe                N/A      |\r\n|    0      4016    C+G   ...6)\\Google\\Chrome\\Application\\chrome.exe N/A      |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```\r\n\r\n**Other info / logs**\r\nUsing hyperopt setup for keras NN from https://github.com/keras-team/keras/issues/1591\r\n", "comments": ["As stack trace says `Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found` it indicates that it fails to find `cuda` library.\r\nYou need to install cuda 10.0 for gpu support.\r\nSee https://www.tensorflow.org/install/gpu#software_requirements\r\nLater add cuda path to environment variable see https://www.tensorflow.org/install/gpu#windows_setup", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@MrDominikku ,\r\nAny update on the issue?Thanks!", "Apologies for late notice, @ymodak suggestion resolved issue. Thanks \ud83d\ude42"]}, {"number": 35251, "title": "LSTMCell initialization issue ( get_initial_state )", "body": "Hi,\r\nWhen I am using get_initial_state to initialize, I am getting error below . \r\nCould you please suggest me how to initialize LSTMCell cell? \r\n\r\nlstm_cell = tf.keras.layers.LSTM(units=128)\r\nlstm_cell = tf.nn.RNNCellDropoutWrapper(   lstm_cell,    output_keep_prob=self.dropout_keep_prob)\r\nself._initial_state = **lstm_cell.get_initial_state(128, tf.float32)**\r\n\r\n**ValueError**: slice index 0 of dimension 0 out of bounds. for strided_slice (op: StridedSlice) with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.", "comments": ["@niranjan8129 ,\r\nCan you please provide the code to reproduce the error reported?Kindly mention TF version being used.\r\nMeanwhile also refer to this [link](https://stackoverflow.com/questions/50385447/how-to-deal-with-keras-erroslice-index-0-of-dimension-0-out-of-bounds) and let us know if it helps solving your issue,Thanks!", "@oanush \r\n\r\nI am working on Tensorflow 2.0. Here is the colab reproduce the error . there is no zero_state in lSTM cell on the TF2.0 version. They added new syntax **get_initial_state**.\r\n\r\n**self._initial_state = lstm_cell.get_initial_state(128, tf.float32)** \r\n\r\nhttps://colab.sandbox.google.com/gist/ravikyram/304d34ec814dbbf13ed63b5a23416ab1/untitled458.ipynb\r\n\r\nOR\r\n\r\nyou can go through the below steps.\r\n\r\n!git clone https://github.com/niranjan8129/classification_tensorflow\r\n\r\n!pip install tensorflow-gpu==2.0.0-beta1\r\nfrom google.colab import drive\r\ndirpath=drive.mount('/classification_tensorflow')\r\n\r\n!chmod 777 classification_tensorflow\r\n!python ./classification_tensorflow/train.py ./classification_tensorflow/data/train.csv.zip ./classification_tensorflow/training_config.json\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 35250, "title": "Converting to TFLite: Invalid quantization params for op MAXIMUM at index 4 in subgraph 0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.15.0-gpu\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nimport os\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\n\r\ndataset = []\r\ndirectory_images = './data_test'\r\ndirectory_saved = './models'\r\nfor img in os.listdir(directory_images):\r\n\tdata = Image.open(os.path.join(directory_images,img)))\r\n\tdata = np.asarray(data, dtype=np.float32)[np.newaxis, :]\r\n\tdataset.append(data)\r\n\r\ndef representative_dataset_gen():\r\n    for input_value in dataset:\r\n        yield [input_value]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(directory_saved + \"/saved\")\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\ntflite_model = converter.convert()\r\n\r\nname = directory_saved + \"/tflite_model\"\r\nopen(name + \".tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"saved2lite.py\", line 34, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 993, in convert\r\n    inference_output_type)\r\n  File \"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 239, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 78, in calibrate_and_quantize\r\n    np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n  File \"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 115, in QuantizeModel\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\nRuntimeError: Invalid quantization params for op MAXIMUM at index 4 in subgraph 0\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://www.dropbox.com/s/tj96fsm6t6rq8ye/model-r100-arcface-ms1m-refine-v2.zip?dl=0\r\n```\r\n\r\n**Failure details**\r\nConversion fails.\r\n\r\n\r\n**Any other info / logs**\r\nFull log:\r\n```\r\n2019-12-19 11:49:10.789835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-12-19 11:49:10.810583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:10.811068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392\r\npciBusID: 0000:01:00.0\r\n2019-12-19 11:49:10.811245: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-19 11:49:10.812226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-19 11:49:10.813094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-12-19 11:49:10.813351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-12-19 11:49:10.814559: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-12-19 11:49:10.815596: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-12-19 11:49:10.817880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-19 11:49:10.818002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:10.818618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:10.819072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-19 11:49:10.819465: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-12-19 11:49:10.843162: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\r\n2019-12-19 11:49:10.843971: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55724cd4c080 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-12-19 11:49:10.844031: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-12-19 11:49:10.926515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:10.927095: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55724cdae1a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2019-12-19 11:49:10.927112: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n2019-12-19 11:49:10.927282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:10.927748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392\r\npciBusID: 0000:01:00.0\r\n2019-12-19 11:49:10.927774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-19 11:49:10.927782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-19 11:49:10.927790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-12-19 11:49:10.927797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-12-19 11:49:10.927804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-12-19 11:49:10.927811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-12-19 11:49:10.927819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-19 11:49:10.927857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:10.928283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:10.928683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-19 11:49:10.928703: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-19 11:49:10.929442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-19 11:49:10.929452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-12-19 11:49:10.929458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-12-19 11:49:10.929674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:10.930285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:10.930708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2934 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:From /home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\n2019-12-19 11:49:13.849255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:13.849669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392\r\npciBusID: 0000:01:00.0\r\n2019-12-19 11:49:13.849711: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-19 11:49:13.849721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-19 11:49:13.849729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-12-19 11:49:13.849737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-12-19 11:49:13.849744: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-12-19 11:49:13.849752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-12-19 11:49:13.849759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-19 11:49:13.849796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:13.850140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:13.850456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-19 11:49:13.850478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-19 11:49:13.850484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-12-19 11:49:13.850488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-12-19 11:49:13.850595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:13.850985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:13.851332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2934 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-12-19 11:49:16.749682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:16.750076: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-12-19 11:49:16.750135: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-12-19 11:49:16.750521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:16.750902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392\r\npciBusID: 0000:01:00.0\r\n2019-12-19 11:49:16.750927: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-19 11:49:16.750936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-19 11:49:16.750945: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-12-19 11:49:16.750952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-12-19 11:49:16.750959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-12-19 11:49:16.750966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-12-19 11:49:16.750974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-19 11:49:16.751007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:16.751348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:16.751663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-19 11:49:16.751682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-19 11:49:16.751688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-12-19 11:49:16.751692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-12-19 11:49:16.751831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:16.752173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:16.752495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2934 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-12-19 11:49:17.182263: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2019-12-19 11:49:17.182289: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2019-12-19 11:49:17.182737: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\nWARNING:tensorflow:From /home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nWARNING:tensorflow:From /home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\n2019-12-19 11:49:18.713005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:18.713562: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-12-19 11:49:18.713618: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-12-19 11:49:18.714027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:18.714386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392\r\npciBusID: 0000:01:00.0\r\n2019-12-19 11:49:18.714424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-19 11:49:18.714433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-19 11:49:18.714441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-12-19 11:49:18.714448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-12-19 11:49:18.714455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-12-19 11:49:18.714462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-12-19 11:49:18.714470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-19 11:49:18.714501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:18.714882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:18.715214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-19 11:49:18.715235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-19 11:49:18.715242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-12-19 11:49:18.715261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-12-19 11:49:18.715411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:18.715760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-19 11:49:18.716119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2934 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-12-19 11:49:20.750755: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2019-12-19 11:49:20.750831: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 1230 nodes (-2206), 1329 edges (-2360), time = 1332.40198ms.\r\n2019-12-19 11:49:20.750850: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 1230 nodes (0), 1329 edges (0), time = 370.552ms.\r\nTraceback (most recent call last):\r\n  File \"saved2lite.py\", line 34, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 993, in convert\r\n    inference_output_type)\r\n  File \"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 239, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 78, in calibrate_and_quantize\r\n    np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n  File \"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 115, in QuantizeModel\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\nRuntimeError: Invalid quantization params for op MAXIMUM at index 4 in subgraph 0\r\n```", "comments": ["@miaout17 Could you please take a look?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35250\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35250\">No</a>\n"]}, {"number": 35249, "title": "Wrong Accuracy value for Training data in tutorial...", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/keras/classification/\r\n\r\n## Description of issue (what needs changing):\r\n\r\nUnder \"Train the model\" in \"Build the model\", the accuracy of the model on training data after 10 epochs is 0.91(91%) while it is mentioned as 0.88(88%).\r\n\r\n### Clear description\r\n\r\nSince, it is already mentioned in the tutorials that the model overfits the training data, thus the accuracy on training data should be more than that on testing data(88.3%).\r\n### Submit a pull request?\r\n\r\nIf this issue is alright, I'll be glad the submit a PR right away...\r\nThanks for the help!\r\n", "comments": ["Perhaps we may update the statement to `This model reaches an accuracy of about 0.91 (or 91%) on the training data` in the `Train the model` cell.", "I'm sorry but I didn't get you clearly, Are you saying that we should add \r\n`model.fit(train_images, train_labels, epochs=10)`\r\nand\r\n`As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.91 (or 91%) on the training data.`\r\nto the cell `Train the model` above??", "Looks like we have 89% accuracy for 5 epochs while for 10 epochs, accuracy is 91%\r\nSo, i think instead changing the epochs, changing accuracy is fine...\r\nThis is what I've done in this PR: https://github.com/tensorflow/docs/pull/1311\r\nCan someone plz check it out.", "Closing this issue since the associated PR has been merged. Thanks!"]}, {"number": 35240, "title": "Google Code-In Task", "body": "", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/35240\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n You'll be able to see Jupyter notebook diff and discuss changes. Powered by <a href='https://www.reviewnb.com'>ReviewNB</a>.", "Is this PR supposed to be against `tensorflow:0.6.0`?"]}, {"number": 35238, "title": "[ROCm][r2.1] eigen patch needed for HIP header changes", "body": "This patch is needed to fix the ROCm TensorFlow build for changes currently in HIP master.  Same patch as #34770, cherry-picked.", "comments": []}, {"number": 35237, "title": "Fix typo in TFLiteConverter.from_concrete_function error message", "body": "", "comments": []}, {"number": 35236, "title": "Update TFLu (micro) to use AmbiqSuite SDK Release 2.2.0 for Apollo3", "body": "We can update to the latest SDK release from Ambiq to stay current. This will make it easier to build/maintain additional functionality in the future if needed.", "comments": ["@gbaned Should I be concerned about the Linux GPU, Windows Bazel, and Windows Bazel GPU checks failing? If you can advise me on next steps to land this PR I would appreciate it much. \r\n\r\nThanks!", "> @gbaned Should I be concerned about the Linux GPU, Windows Bazel, and Windows Bazel GPU checks failing? If you can advise me on next steps to land this PR I would appreciate it much.\r\n> \r\n> Thanks!\r\n\r\n@oclyke Sorry for the delay, it is waiting for an internal approval. We will let you know if anything requires from you for this PR. Thank you!"]}, {"number": 35235, "title": "[r2.1 Cherrypick] Pass experimental_relax_shapes to instance methods decorated with `tf.function` and Relax shapes for Keras _on_batch functions.", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35235) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35235) for more info**.\n\n<!-- cla_yes -->", "@googlebot I consent.", "I know it is not needed, but I wanted to enjoy doing it anyway ;-)"]}, {"number": 35234, "title": "Revert \"[r2.1 cherry-pick] Fix pip package API generation\"", "body": "Reverts tensorflow/tensorflow#34822\r\n\r\nUnfortunately having #34822 in in a pip package causes segfaults on `python -c \"import scipy; import tensorflow\"`.", "comments": []}, {"number": 35233, "title": "Move TF-TensorRT util functions", "body": "This PR moves some utility functions into the compiler/tf2tensorrt/tensorrt/convert/utils.{h,cc} files. The functions are not modified.\r\n\r\nRationale: the moved functions will be used in an upcoming PR from other files, this way we can access the declarations without circular dependency between the headers.", "comments": []}, {"number": 35232, "title": "fixes 35136", "body": "fixes #35136", "comments": []}, {"number": 35231, "title": "[r1.15 Cherrypick] Fix hip-clang build", "body": "Remove new line before comparing string.", "comments": ["@mihaimaruseac can you please help merge this PR, thank you.", "It is not on master. It will be merged later, after builds on release branches also pass. (that is, after #33982)"]}]