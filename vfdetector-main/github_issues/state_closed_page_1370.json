[{"number": 11969, "title": "tensorflow_gpu-1.3.0rc1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.", "body": "I am trying to install TF 1.3.0rc1 in CentOS 7.3 with Nvidia GPUs for python3.6. When I installed with python2.7, it was fine.\r\n\r\n`sudo pip3.6 install -U http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0rc1-cp27-none-linux_x86_64.whl`\r\n\r\nI get this error with python3.6\r\n\r\nMy pip3.6 has a slight difference with sudo\r\n\r\nWithout sudo, when I run pip3.6 --version, I get\r\n\r\n> pip 9.0.1 from /usr/local/lib/python3.6/site-packages (python 3.6)\r\n\r\nWith sudo I get\r\n\r\n> pip 9.0.1 from /usr/lib/python3.6/site-packages (python 3.6)\r\n\r\nAny workaround for this?", "comments": []}, {"number": 11968, "title": "[OpenCL] Fix allocator destruction race condition (#136)", "body": "* [OpenCL] Changes SYCL Interface construction\r\n\r\nUses C++11 static initialisation to provide singleton instance, rather\r\nthan a mutex and pointer.\r\n\r\n* [OpenCL] Adds const to SYCL Interface methods\r\n\r\n* [OpenCL] Fix allocator destruction race condition\r\n\r\nA Tensor's allocator must outlive it, however there is no easy way to\r\ndetermine whether an Allocator has any Tensors still alive, and so we\r\ncannot know when it is safe to destroy an allocator. The CPU allocator\r\ngets round this by being deleted, so we adopt this convention here.\r\n\r\n* [OpenCL] Reformats SYCL code\r\n\r\n* [OpenCL] Fixes SYCL comments\r\n\r\n* [OpenCL] Tidies SYCL device description\r\n\r\nAdds check for whether the QueueInterface pointer is valid, as this\r\nmay not always be the case.\r\n\r\n* [OpenCL] Adds nullptr checking to SYCL allocator\r\n\r\n* [OpenCL] Adds const specifier to SYCL Interface", "comments": ["Can one of the admins verify this patch?", "@lukeiwanski, thanks for your PR! By analyzing the history of the files in this pull request, we identified @benoitsteiner, @martinwicke and @tensorflower-gardener to be potential reviewers.", "Thanks for the feedback we will apply it ASAP", "@tensorflow-jenkins test this please"]}, {"number": 11967, "title": "Training_ops_test assertion failures on Ubuntu 16.04", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  'v1.2.1-0-gb4957ff', '1.2.1'\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: No GPU\r\n- **GPU model and memory**: No GPU\r\n- **Exact command to reproduce**: bazel test -c opt //tensorflow/python:training_ops_test\r\n\r\n### The problem\r\nThe `//tensorflow/python:training_ops_test` is failing on s390x with AssertionError in `_testTypesForFtrl` method. I have added test log below.\r\n\r\nA single value differs as below in the result:\r\n```\r\nlhs =  [-2.9473114]\r\nrhs =  [-2.94730377]\r\n```\r\n\r\n@gunan, I saw your suggestion in [issue](https://github.com/tensorflow/tensorflow/issues/8945#issuecomment-291763267), that the test tolerance bounds are too tight.\r\n\r\nSo I tried increasing tolerance to `4e-06 ` as below and test passes.\r\n\r\n```\r\n- self.assertAllClose(linear_update, linear.eval())\r\n+ self.assertAllClose(linear_update, linear.eval(), rtol=4e-06, atol=4e-06)\r\n```  \r\nIs this tolerance level acceptable?    \r\n\r\n   \r\n    \r\n    \r\n\r\n### Source code / logs   \r\n```\r\nAssertionError:\r\nNot equal to tolerance rtol=1e-06, atol=1e-06\r\nNone\r\n(mismatch 1.0%)\r\n x: array([  1.020000e+02,   1.038411e+02,   1.050863e+02,   1.055917e+02,\r\n         1.053070e+02,   1.042043e+02,   1.022649e+02,   9.947507e+01,\r\n         9.582399e+01,   9.130299e+01,   8.590485e+01,   7.962347e+01,...\r\n y: array([  1.020000e+02,   1.038411e+02,   1.050863e+02,   1.055917e+02,\r\n         1.053070e+02,   1.042043e+02,   1.022649e+02,   9.947507e+01,\r\n         9.582399e+01,   9.130299e+01,   8.590485e+01,   7.962347e+01,...\r\n\r\n----------------------------------------------------------------------\r\n\r\nRan 9 tests in 4.704s\r\n\r\nFAILED (failures=1)\r\nnot close where =  (array([19]),)\r\nnot close lhs =  [-2.9473114]\r\nnot close rhs =  [-2.94730377]\r\nnot close dif =  [  7.62939453e-06]\r\nnot close tol =  [  3.94730387e-06]\r\ndtype = float32, shape = (100,)\r\n```", "comments": ["Which test specifically is failing?  If it's one where the final values are the result of multiple math ops, then 1e-6 is not a very high tolerance.", "@poxvoculi , [testApplyFtrl](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/python/training/training_ops_test.py#L131) is failing at [this](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/python/training/training_ops_test.py#L119).\r\n\r\nWithout change in tolerance, the test passes if I run in `debug` mode: \r\n`bazel test -c dbg //tensorflow/python:training_ops_test\r\n`\r\nHowever it fails with `opt` or `fastbuild` compilation modes:\r\n```\r\nbazel test -c opt //tensorflow/python:training_ops_test\r\nbazel test //tensorflow/python:training_ops_test\r\n```\r\n", "Internal bug opened to raise the error tolerance at this point.", "@poxvoculi, Thank you for you support.\r\n\r\nOn similar lines I found a test failure in `//tensorflow/python/kernel_tests:summary_image_op_test`.\r\nHere the error tolerance increase needed is even lower ( i.e. 2e-06) at [assert](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/summary_image_op_test.py#L83).\r\n\r\nAlso test passes when run in debug mode without tolerance increase.\r\n\r\nAdding the log below:\r\n\r\n```\r\nF..\r\n======================================================================\r\nFAIL: testImageSummary (__main__.SummaryImageOpTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/summary_image_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/summary_image_op_test.py\", line 83, in testImageSummary\r\n    self.assertAllClose(image, adjusted[0])\r\n  File \"/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/summary_image_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 634, in assertAllClose\r\n    self._assertArrayLikeAllClose(a, b, rtol=rtol, atol=atol)\r\n  File \"/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/summary_image_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 604, in _assertArrayLikeAllClose\r\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol, err_msg=msg)\r\n  File \"/usr/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 1391, in assert_allclose\r\n    verbose=verbose, header=header)\r\n  File \"/usr/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 733, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=1e-06, atol=1e-06\r\nNone\r\n(mismatch 2.85714285714%)\r\n x: array([[[221],\r\n        [102],\r\n        [129],...\r\n y: array([[[ 221.],\r\n        [ 102.],\r\n        [ 129.],...\r\n\r\n----------------------------------------------------------------------\r\nRan 3 tests in 0.069s\r\n\r\nFAILED (failures=1)\r\nnot close where =  (array([3]), array([6]), array([0]))\r\nnot close lhs =  [0]\r\nnot close rhs =  [ 1.]\r\nnot close dif =  [ 1.]\r\nnot close tol =  [  1.99999999e-06]\r\ndtype = uint8, shape = (5, 7, 1)\r\n\r\n```\r\n\r\n", "Thanks, I've added that one to the change.", "Will be fixed in upcoming release."]}, {"number": 11966, "title": "[XLA] Add a scheme for disabling tests based on type support by backends", "body": "This change allows CPP tests to be marked as requiring certain type support.  As it stands, I have updated only the scalar math test with the annotations.  I will do the rest once this change is debated.\r\n\r\nThe plugin config has something like this added to the 'copts' option:\r\n\r\n```\r\n    \"copts\": [\r\n      \"-DXLA_TEST_DISABLE_F64\",\r\n    ],\r\n```", "comments": ["Can one of the admins verify this patch?", "after discussion on the xla dev forum, cancelling this."]}, {"number": 11965, "title": "Moving average and moving variance in Batchnorm aren't updated", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**:  3.5.3\r\n- **Bazel version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: 8/5.1\r\n- **GPU model and memory**: GeForce 1080\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI'm using the slim wrapper, which in turn returns an instance of BatchNormalization from layers/normalisation.py. All paramers are set to default, except for scale which is set to True (i.e. adding the gamma scaler). After training, when looking the at the learned parameters, I notice that all the moving means in the network are still 0 while all the moving variances are 1, i.e. they weren't updated. \r\n\r\nBoth variables don't show up in tf.trainable_variables() which might explain the lack of updates. However, since these are not actually learned but rather calculated, I'm not sure whether they would be updated by the optimiser.\r\n", "comments": ["I can't edit my orginal message, so I'll just add a comment.\r\nI tried running the test function with `is_training=False` (but with the exact same checkpoint as before). The accuracy went from ~98% to roughly 12%.\r\n\r\nMy theory here is that the batchnorm layer is keeping the mean and variance variables in a different place than it's telling the collection", "You probably forgot this which is written in the document of batch_norm:\r\n```\r\n  Note: when training, the moving_mean and moving_variance need to be updated.\r\n  By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they\r\n  need to be added as a dependency to the `train_op`. For example:\r\n\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    with tf.control_dependencies(update_ops):\r\n      train_op = optimizer.minimize(loss)\r\n\r\n  One can set updates_collections=None to force the updates in place, but that\r\n  can have a speed penalty, especially in distributed settings.\r\n```", "Ok, this was indeed the problem. Many thanks.\r\n\r\nDo I need to concatenate this collection with any other one for normal training? Why does it make sense to have it like this?\r\n\r\nAre the statistics (mean and var.) also updated without the optimiser settings?", "Other layers don't have similar caveats AFAIK.\r\n\r\nIt makes sense because the moving averages are not updated by gradient descent, therefore there must be another way to update it.", "Many thanks for the help and the info.\r\nYet, I fear I'll have to ask for a re-open as the problem only seems to be half solved.\r\nThe mean and variance are properly saved and loaded now (why does it saved the variance btw, when the std is required?). However, when evaluating the model with `is_training=False` the accuracies are still around 35% while the same script with `is_training=True` has around 97% accuracy.\r\n\r\nI checked if all the weights and parameters are properly loaded and everything seems to be in place", "Same as \r\n#1122 \r\n\r\nhttps://stackoverflow.com/questions/42770757/tensorflow-batch-norm-does-not-work-properly-when-testing-is-training-false\r\n\r\nhttps://stackoverflow.com/questions/39353503/tensorflow-tf-slim-model-with-is-training-true-and-false?rq=1\r\n\r\nhttps://stackoverflow.com/questions/44211371/tensorflow-batch-norm-breaks-network-when-is-training-false?rq=1\r\n\r\nI'm currently training again with a lower decay to confirm #1122 and will update tomorrow.\r\n\r\nUpdate - the lower decay rate (0.9) and `updates_collections=None`  seemed to do the work.", "I am experiencing the same issue. My validation accuracy on CIFAR 10 is lower with batchnorm than without. I have added `tf.GraphKeys.UPDATE_OPS` to the optimizer, and set `is_training=False` during validation. I'm on tensorflow 1.3.\r\n\r\nWhy is a decay rate required for batch_norm to work? Is there a bug with the batch_norm implementation?", "Make sure that you collect with tf.GraphKeys.UPDATE_OPS with the right name scope:\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS,name_scope)", "change decay=0.999 to 0.9 works fine for me.", "@ppwwyyxx . I read the https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/layers/normalization.py code, to see the implementation of tf.layers.batch_normalization(). \r\nBut in this code I could not find any control dependency adding for moving mean and variance. There is no line of code which puts the moving average or variance in tf.GraphKeys.UPDATE_OPS collection. ", "https://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/python/layers/normalization.py#L416-L417", "Can you please refer to function add_update?", "I think its `add_update` method is inherited from `base.Layer`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/python/layers/base.py#L237"]}, {"number": 11964, "title": "Fail to restore the model after upgraded to version 1.2.1", "body": "I trained my model in tensorflow version 1.1.0, and test or restore it successfully. But after I upgraded tensorflow to version 1.2.1,  I fail to restore the same model  with the same code!\r\n\r\n### Error Log \r\n```\r\nrestore from ckpt: ./Models/dl_fast/TextRecognize/model-105000\r\n2017-08-02 15:18:09.050018: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key bidirectional_rnn/bw/lstm_cell/bias not found in checkpoint\r\n2017-08-02 15:18:09.050057: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key bidirectional_rnn/fw/lstm_cell/kernel not found in checkpoint\r\n2017-08-02 15:18:09.050751: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key bidirectional_rnn/fw/lstm_cell/bias not found in checkpoint\r\n2017-08-02 15:18:09.050753: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key bidirectional_rnn/bw/lstm_cell/kernel not found in checkpoint\r\nTraceback (most recent call last):\r\n  File \"dl_fast_recognizer.py\", line 151, in <module>\r\n    text_recognizer_model=text_recognizer_model\r\n  File \"dl_fast_recognizer.py\", line 37, in __init__\r\n    self.text_recognizer = Predictor('dl_fast.yml', text_recognizer_model)\r\n  File \"../Recognition/predictor.py\", line 45, in __init__\r\n    saver.restore(self.sess, ckpt)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1548, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key bidirectional_rnn/bw/lstm_cell/bias not found in checkpoint\r\n\t [[Node: save/RestoreV2_6 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_6/tensor_names, save/RestoreV2_6/shape_and_slices)]]\r\n\r\nCaused by op u'save/RestoreV2_6', defined at:\r\n  File \"dl_fast_recognizer.py\", line 151, in <module>\r\n    text_recognizer_model=text_recognizer_model\r\n  File \"dl_fast_recognizer.py\", line 37, in __init__\r\n    self.text_recognizer = Predictor('dl_fast.yml', text_recognizer_model)\r\n  File \"./Recognition/predictor.py\", line 43, in __init__\r\n    saver = tf.train.Saver(tf.global_variables(),max_to_keep=100)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1139, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1170, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 691, in build\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 640, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): Key bidirectional_rnn/bw/lstm_cell/bias not found in checkpoint\r\n\t [[Node: save/RestoreV2_6 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_6/tensor_names, save/RestoreV2_6/shape_and_slices)]]\r\n```\r\n\r\n###  Restore Code\r\n```\r\n        with self.sess.as_default():\r\n            with self.model.graph.as_default(), tf.device('/cpu:0'):\r\n                self.sess.run(tf.global_variables_initializer())\r\n\r\n                ckpt = tf.train.latest_checkpoint(model_path)\r\n                print('restore from ckpt: {}'.format(ckpt))\r\n                saver = tf.train.Saver(tf.global_variables(),max_to_keep=100)\r\n                if ckpt:\r\n                    saver.restore(self.sess, ckpt)\r\n                    print('restored from ckpt: {}'.format(ckpt))\r\n                else:\r\n                    print('cannot restore')\r\n```\r\n\r\nIs it a bug of tensorflow or my code? How to fix it?", "comments": ["Sorry you ran into this.\r\nI believe the RNN code was moved from experimental (`tf.contrib`) to the main repository in 1.2.0. Quoting from the [release notes](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md) of 1.2.0:\r\n\r\nRNNCells' variable names have been renamed for consistency with Keras layers. Specifically, the previous variable names \"weights\" and \"biases\" have been changed to \"kernel\" and \"bias\", respectively. This may cause backward incompatibility with regard to your old checkpoints containing such RNN cells, in which case you can use the tool [checkpoint_convert script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/tools/checkpoint_convert.py) to convert the variable names in your old checkpoints.\r\n\r\nDo give that a spin and let us know if that works out.", "It works. @asimshankar Thank you very much!"]}, {"number": 11963, "title": "Infiniband with tensorflow", "body": "-----------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\ncommit 3bee923c9\r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n- **CUDA/cuDNN version**:\r\ncuda 8.0/cudnn 5.1.5\r\n- **GPU model and memory**:\r\nTitan Xp\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI tried to use Infiniband with tensorflow using ` server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index, protocol='grpc+verbs')`.\r\nBut the session is not hanged.\r\nI used S1, and S2, the cluster is configured with the Infiniband ip address. Is there something that I missed?\r\n\r\n\r\nibdev2netdev \r\nS1\r\nmlx5_0 port 1 ==> ib0 (Down)\r\nmlx5_1 port 1 ==> ib1 (Up)\r\nS2\r\nmlx5_0 port 1 ==> ib0 (Up)\r\nmlx5_1 port 1 ==> ib1 (Up)\r\n\r\n### Source code / logs", "comments": ["This a [known issue](https://github.com/tensorflow/tensorflow/issues/9926#issuecomment-302259929) that when the first IB port is down, the verbs server wouldn't run. You could manually change it to the second port [here](https://github.com/tensorflow/tensorflow/blob/314008aceb061675eb30daf7e0cae303ebfe8723/tensorflow/contrib/verbs/rdma.cc#L74).\r\n\r\nOr you could wait a little bit more time to try my GDR patch (#11392) when it is merged :)", "Sorry for only addressing this now, did you try changing the environment variable **RDMA_DEVICE**?", "I solved it now but I don't remember what setting I changed.  Anyway, I'll close the issue. "]}, {"number": 11962, "title": "Some gpu tests are failing on Windows in Bazel build", "body": "They are marked as `no_windows_gpu` in https://github.com/tensorflow/tensorflow/pull/11901, but we need to investigate why they are now failing.\r\n```\r\n//py_test_dir/tensorflow/python/kernel_tests:metrics_test               TIMEOUT in 1 out of 3 in 6541.4s\r\n//py_test_dir/tensorflow/python/kernel_tests:norm_op_test               TIMEOUT in 20 out of 20 in 6379.0s\r\n//py_test_dir/tensorflow/python:math_grad_test                          TIMEOUT in 566.2s\r\n//py_test_dir/tensorflow/python:math_ops_test                           TIMEOUT in 512.5s\r\n//py_test_dir/tensorflow/python:special_math_ops_test                   TIMEOUT in 88.8s\r\n//py_test_dir/tensorflow/python/kernel_tests:cholesky_op_test            FAILED in 5 out of 5 in 409.8s\r\n//py_test_dir/tensorflow/python/kernel_tests:linalg_ops_test             FAILED in 559.0s\r\n//py_test_dir/tensorflow/python/kernel_tests:reduction_ops_test          FAILED in 4 out of 4 in 455.1s\r\n//py_test_dir/tensorflow/python:session_clusterspec_prop_test            FAILED in 226.5s\r\n//py_test_dir/tensorflow/python:session_list_devices_test                FAILED in 182.7s\r\n```", "comments": ["@gunan do you have any comments on this?", "I think this is just a tracking bug.\r\nThe issues were known before, so the tests were disabled, but as time goes by we need to resolve all these issues. @meteorcloudy are the failures due to TF code, or certain dependencies we are missing.\r\nNote that we also disable a whole bunch of tests in our cmake build:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_tests.cmake#L189", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It looks like we should have removed the \"awaiting tensorflower\" label back in August, after @gunan commented. I'll close; @meteorcloudy please reopen if you've got an answer for @gunan's question and want to pursue."]}, {"number": 11961, "title": "timeout breaks FIFOQueue", "body": "### System information\r\n-  Linux Ubuntu 14.04\r\n-  TensorFlow installed from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.1-cp27-none-linux_x86_64.whl\r\n-  Tensorflow-gpu 1.2.1\r\n-  Python version 2.7\r\n-  CUDA 8.0 /cuDNN 5.1.10\r\n-  GPU - Nvidia GTX 1080\r\n\r\n### Describe the problem\r\nI use thread to add my training data to a FIFO Queue. There is always an thread error occurring in the process of my training.\r\n\r\n### Source code / logs\r\nHere is my source code relevant to the error:\r\n```js\r\nwith tf.Session() as sess:\r\n    self.q = tf.FIFOQueue(self.queue_capacity, [tf.float32, tf.float32, train_label_dtype], shapes=[(self.train_batch_size, self.im_height, self.im_width, self.num_tensor_channels), (self.train_batch_size, self.pose_dim), (self.train_batch_size,)])\r\n    self.enqueue_op = self.q.enqueue([self.train_data_batch, self.train_poses_batch, self.train_labels_batch])\r\n    self.train_labels_node = tf.placeholder(train_label_dtype, (self.train_batch_size,))\r\n    self.input_im_node, self.input_pose_node, self.train_labels_node = self.q.dequeue()\r\n\r\n    self.queue_thread = threading.Thread(target=self._load_and_enqueue)\r\n    self.queue_thread.start()\r\n\r\n    for step in training_range:\r\n    \tself._check_dead_queue()\r\n    \t\"\"\"run optimization\"\"\"\r\n    \t_, l, lr, predictions, batch_labels, output, train_images, conv1_1W, conv1_1b, pose_node = self.sess.run(\r\n    \t\t[optimizer, loss, learning_rate, train_predictions, self.train_labels_node, self.train_net_output, self.input_im_node, self.weights.conv1_1W, self.weights.conv1_1b, self.input_pose_node], options=GeneralConstants.timeout_option)\r\n\r\ndef _load_and_enqueue(self):\r\n        \"\"\" Loads and Enqueues a batch of images for training \"\"\"\r\n\r\n        train_data = np.zeros(\r\n            [self.train_batch_size, self.im_height, self.im_width, self.num_tensor_channels]).astype(np.float32)\r\n        train_poses = np.zeros([self.train_batch_size, self.pose_dim]).astype(np.float32)\r\n        label_data = np.zeros(self.train_batch_size).astype(self.numpy_dtype)\r\n\r\n        while not self.term_event.is_set():\r\n            time.sleep(self.cfg['queue_sleep'])\r\n\r\n            # Omit the code for geeting data\r\n\r\n            # send data to queue\r\n            if not self.term_event.is_set():\r\n                try:\r\n                    self.sess.run(self.enqueue_op, feed_dict={self.train_data_batch: train_data,\r\n                                                              self.train_poses_batch: train_poses,\r\n                                                              self.train_labels_batch: label_data})\r\n                except:\r\n                    pass\r\n        del train_data\r\n        del train_poses\r\n        del label_data\r\n        self.dead_event.set()\r\n        logging.info('Queue Thread Exiting')\r\n        self.queue_thread_exited = True\r\n\r\n```\r\nWhen I train the model for some time, there will be an error:\r\n```js\r\nINFO:root:Step 41817 (epoch 3.35), 0.1 s\r\nINFO:root:Minibatch loss: 1.256, learning rate: 0.000857\r\nINFO:root:Minibatch error: 26.562%\r\nINFO:root:Step 41818 (epoch 3.35), 0.1 s\r\nINFO:root:Minibatch loss: 1.790, learning rate: 0.000857\r\nINFO:root:Minibatch error: 59.375%\r\nINFO:root:Step 41819 (epoch 3.35), 0.1 s\r\nINFO:root:Minibatch loss: 0.868, learning rate: 0.000857\r\nINFO:root:Minibatch error: 3.125%\r\n2017-08-02 07:26:54.769345: W tensorflow/core/kernels/queue_base.cc:302] _0_data_queue/fifo_queue: Skipping cancelled dequeue attempt with queue not closed\r\n2017-08-02 07:26:54.769537: W tensorflow/core/framework/op_kernel.cc:1158] Cancelled: Dequeue operation was cancelled\r\n\t [[Node: data_queue/fifo_queue_Dequeue = QueueDequeueV2[component_types=[DT_FLOAT, DT_FLOAT, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](data_queue/fifo_queue)]]\r\n2017-08-02 07:26:54.769608: W tensorflow/core/framework/op_kernel.cc:1158] Cancelled: Dequeue operation was cancelled\r\n\t [[Node: data_queue/fifo_queue_Dequeue = QueueDequeueV2[component_types=[DT_FLOAT, DT_FLOAT, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](data_queue/fifo_queue)]]\r\n2017-08-02 07:26:54.769715: W tensorflow/core/framework/op_kernel.cc:1158] Cancelled: Dequeue operation was cancelled\r\n\t [[Node: data_queue/fifo_queue_Dequeue = QueueDequeueV2[component_types=[DT_FLOAT, DT_FLOAT, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](data_queue/fifo_queue)]]\r\n2017-08-02 07:26:54.769747: W tensorflow/core/framework/op_kernel.cc:1158] Cancelled: Dequeue operation was cancelled\r\n\t [[Node: data_queue/fifo_queue_Dequeue = QueueDequeueV2[component_types=[DT_FLOAT, DT_FLOAT, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](data_queue/fifo_queue)]]\r\nTraceback (most recent call last):\r\n  File \"training.py\", line 269, in <module>\r\n    sgdOptimizer.optimize()\r\n  File \"/home/liuquande/Desktop/SenceTime/gqcnn/test/gqcnn/sgd_optimizer.py\", line 254, in optimize\r\n    [optimizer, loss, learning_rate, train_predictions, self.train_labels_node, self.train_net_output, self.input_im_node, self.weights.conv1_1W, self.weights.conv1_1b, self.input_pose_node], options=GeneralConstants.timeout_option)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.DeadlineExceededError: Timed out waiting for notification\r\nINFO:root:Queue Thread Exiting\r\nINFO:rospy.core:signal_shutdown [atexit]\r\n```", "comments": ["This doesn't sounds like a bug in TensorFlow (the error you see is signaling that you have not enqueued enough elements onto the queue)"]}, {"number": 11960, "title": "Added an \"UpdateInput\" method to the Python C API", "body": "This is the only current limitation I see for allowing the implementation of gradients through the C API for while loops. @skye this relates to our discussion in #10089 and could provide a temporary solution until we have a more elegant solution. Since it is in the `python_api.h` interface, similar to `AddControlInput`, I hope accepting this doesn't cause any issues.", "comments": ["@eaplatanios, thanks for your PR! By analyzing the history of the files in this pull request, we identified @skye and @itsmeolivia to be potential reviewers.", "Can one of the admins verify this patch?", "I also added a \"ClearControlInputs\" function which turns out to be necessary to implement the \"_RemoveExternalControlEdges\" method of control flow contexts in the Python API.", "Hi Anthony, I'd prefer not to add the ClearControlInputs function, and have you wait for the C API implementation. (We actually need the UpdateInput function to support parts of the Python API.) Have you actually started implementing while loop backprop in Scala? I believe even with these functions it'll be tricky without having the C API provide something like WhileContext (although maybe possible, if you traverse the new while loop and reconstruct the state needed).", "@skye I have actually started implementing it by mirroring what's done in the Python API. I am creating and managing control flow contexts in the Scala side, similar to how it's done on the Python side. However, that requires the ClearControlInputs function since that's used in Python. Once I finish the implementation and have something working, I can look into whether I can remove the calls to ClearControlInputs. This is unclear at this point though.\r\n\r\nOne problem I have in my mind that is also not resolved in the Python API is what happens when you deserialize a graph from a MetaGraphDef file. This is generally handled very loosely in the Python API (e.g., with respect to prevent feeding and fetching, and contexts). I have done some work to improve the Scala implementation such that after loading a graph from a file, appending ops to that graph and manipulating it mimicks the behavior you'd get had you created it in the Scala side in the first place. However, with respect to control flow contexts, I don't yet have a clear picture at to when they come in and out and I can also not see them being serialized with the graph in the Python side. That's something to think about I guess, but I'll work on it after I have a basic implementation working for while loops.\r\n\r\nAlso, note that I am implementing conditionals (i.e., tf.cond) in the same way as done for the Python API. That is, using a CondContext.", "Ah I see. Unfortunately we really do not want every language binding to reimplement this code (it's extremely easy to introduce bugs, and forces us to allow dangerous graph modifications like creating cycles, as this patch does). It looks like you're familiar with C/++; would you be willing to help me with the C API while loop implementation? I'm working on getting the major gradient pieces in, but there are still many TODOs to resolve in the forward-pass loop implementation. I can help you get started with some of these if you like, and once I get the basic gradient code checked in we can possibly divide up that work as well. What do you think?\r\n\r\nRe: loading MetaGraphDefs, I too have noticed that loading while loops doesn't really work if you wanna continue graph construction. I haven't thought about this much either, since my immediate goal is to reimplement the current Python behavior. I agree it would be good to properly handle this though.", "@skye I would be more than willing to help with C API while loop implementation, as I do agree that repetition of this code in other language APIs is not a good idea. :)\r\n\r\nI have a couple of questions though to get started with this:\r\n1. Why do we also have a CondContext in the Python API? If we do not have that, how would we handle nested conds? In general, I would like to be clear on the functionality related to cond, before we go into while loops.\r\n2. Intuitively, it seems to me that we would know the control flow context of every op, at creation time, since they would be part of a cond, for example. So, wouldn't it better to make the control flow context of each op immutable and serialize it together with the op? We can perform the serialization efficiently by storing all control flow contexts of a graph, alongside the graph and then storing an ID for each op's control flow context.\r\n3. Are redundant cycles currently being introduced in the Python API implementation?\r\n\r\nI'm not sure if my questions make sense the way I phrased them, but I could explain further if needed. Probably the most helpful thing would be if you gave me an overview of what needs to be done as you said.\r\n\r\nIn any case, it would also be good to think of potential ways for other language APIs to provide gradient implementations that are not yet supported in the C API, so that we can have some plan in mind for how to integrate well with them. We could follow a similar approach to how while loops are defined and create a new graph for each gradient op that we then merge into the main target graph, although I'm not sure how efficient that would be.", "Great, thanks Anthony! And apologies for not suggesting we collaborate earlier, I didn't mean to send you down a rabbit hole. I'll answer your questions shortly, but do you mind shooting me an email in the meantime? That'll probably be better than communicating in this PR :)", "@skye what's the status of this?", "We don't actually need this patch, closing."]}, {"number": 11959, "title": "Fix tensordot with list of ints as axes", "body": "This fix tries to fix the tensordot issue raised in 11950 where\r\n```\r\nTypeError: object of type 'int' has no len()\r\n```\r\nis thrown out when the axes is a list of ints:\r\n```\r\nwith tf.Session() as sess:\r\n    sess.run(\r\n         tf.tensordot(tf.ones((3,3), dtype=tf.float32), tf.constant([2, 3, 1], dtype=tf.float32)[None, None], axes=[1, 2])\r\n    )\r\n```\r\n\r\nNote: In numpy it is possible to specify \r\n```\r\nnp.tensordot(np.ones((3,3)), np.array([2, 3, 1])[None, None], axes=[1, 2])\r\n```\r\n\r\nThis fix fixes #11950.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @aselle to be potential reviewers.", "@rmlarsen Thanks for the review. The PR has been updated. Please take a look.", "@tensorflow-jenkins test this please"]}, {"number": 11958, "title": "[Bug?]Session Hang during training with 'mnist_replica.py' and learning rate as a placeholder", "body": "### System information\r\n OS: Mac OS \r\n TF: 1.2.0, pip install\r\n Python: 2.7.9\r\nalso tested on RHEL with TF 1.1 and Python 2.7.13, the problem still exists. I don't think it is system specific.\r\n\r\n\r\n### Describe the problem\r\nI am training the 'mnist-replica.py' provided on github. link is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py) \r\nand set up two workers and one parameter server on my laptop.\r\nI made only one modification to the code:\r\n use learning_rate as a placeholder rather than a fixed value and feed 0.01 to it each step.\r\nWhen I train it asynchronously, it is ok to go, but the session hangs in .run() after one or two steps when I train it in synchronous.\r\n\r\nCan anyone figure out the reason of this?\r\n\r\n### Source code / logs\r\nthe source code is \r\n[here](https://drive.google.com/open?id=0Bw4fA7bI0IScTVlsT3FidS0tMXM). just provide it for ease. It is almost the same as the original one.", "comments": ["I am not sure is this the right way to do?"]}, {"number": 11957, "title": "cudnn failed with GTX960", "body": "I am trying to use conv2d (for cnn) with GPU, and there are one GTX 960 and Quadro 600 on my computer. I set up the `CUDA_VISIBLE_DEVICES=0` to use GTX 960, but I get\r\n```\r\n$ ./mnist-train /home/qinka/ out.p10 1000 +RTS -N\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 960\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.1775\r\npciBusID 0000:28:00.0\r\nTotal memory: 1.95GiB\r\nFree memory: 1.91GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:28:00.0)\r\nW tensorflow/core/framework/op_kernel.cc:993] Unimplemented: Conv2DBackprop for GPU is not currently supported without cudnn\r\n\t [[Node: Conv2DBackpropFilter_152 = Conv2DBackpropFilter[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0_0/_141, Shape_151/_185, Reshape_146)]]\r\nW tensorflow/core/framework/op_kernel.cc:993] Unimplemented: Conv2DBackprop for GPU is not currently supported without cudnn\r\n\t [[Node: Conv2DBackpropFilter_152 = Conv2DBackpropFilter[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0_0/_141, Shape_151/_185, Reshape_146)]]\r\nW tensorflow/core/framework/op_kernel.cc:993] Unimplemented: Conv2DBackprop for GPU is not currently supported without cudnn\r\n\t [[Node: Conv2DBackpropFilter_152 = Conv2DBackpropFilter[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0_0/_141, Shape_151/_185, Reshape_146)]]\r\nW tensorflow/core/framework/op_kernel.cc:993] Unimplemented: Conv2DBackprop for GPU is not currently supported without cudnn\r\n\t [[Node: Conv2DBackpropFilter_152 = Conv2DBackpropFilter[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0_0/_141, Shape_151/_185, Reshape_146)]]\r\nmnist-train: TensorFlowException TF_UNIMPLEMENTED \"Conv2DBackprop for GPU is not currently supported without cudnn\\n\\t [[Node: Conv2DBackpropFilter_152 = Conv2DBackpropFilter[T=DT_FLOAT, data_format=\\\"NHWC\\\", padding=\\\"VALID\\\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=false, _device=\\\"/job:localhost/replica:0/task:0/gpu:0\\\"](_recv_Placeholder_0_0/_141, Shape_151/_185, Reshape_146)]]\"\r\n```\r\n\r\n\r\n### System information\r\n- I Just using Haskell's binding, but I don't think that is key to this problem, and the libtensorflow.so is the gpu one.\r\n- system\r\n```\r\nuname -a\r\nLinux ETVP-z400 4.10.0-26-generic #30-Ubuntu SMP Tue Jun 27 09:30:12 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nlsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 17.04\r\nRelease:\t17.04\r\nCodename:\tzesty\r\n```\r\n- Using libtensorflow.so download from official site\r\n- tensorflow 1.0\r\n- Using Haskell's binding\r\n- CUDA-8.0 & cuDNN-5.1\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n```\r\n- gpu:\r\n```\r\n$ nvidia-smi\r\nWed Aug  2 09:06:00 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro 600          Off  | 0000:0F:00.0     Off |                  N/A |\r\n| 34%   52C    P0    N/A /  N/A |      0MiB /   963MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 960     Off  | 0000:28:00.0     Off |                  N/A |\r\n|  0%   35C    P0    24W / 120W |      0MiB /  1996MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n", "comments": ["(I forgot to set up `use_cudnn_on_gpu` to true....) "]}, {"number": 11956, "title": "Feature Request: Correlation Layer ", "body": "There is a surging interest in Geometric Computer Vision and a large number of recent papers leveraging an operation(with small variations) dubbed \"Correlation\" layer. There is a CUDA kernel for this operation in the FlowNet paper's author's fork of Caffe. Is there plan on including it in Tensorflow? I couldn't locate a relevant issue anywhere and this is why I am raising this issue. ", "comments": ["@benoitsteiner do you have any comment on this?", "Maybe this could be contributions welcome? First step could be implemented this as a user_op which could exist in a separate repo", "+1 for this request.\r\n\r\nIf anyone's interested there's a working (albeit slightly crude) implementation using Keras over Tensorflow here:\r\n\r\nhttps://github.com/WeidiXie/New_Layers-Keras-Tensorflow/blob/master/src/normalized_correlation_layer.py", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Just FYI: There are also two implementations of correlation layer on Github:\r\n1. https://github.com/sampepose/flownet2-tf/tree/master/src/ops/correlation\r\n2. https://github.com/jgorgenucsd/corr_tf\r\n\r\nUpdate:\r\nAnother one here: https://github.com/simonmeister/UnFlow/blob/8e74f2b33138ab72d775bf1c3a9256105677834e/ops/correlation_op.cu.cc", "+1,\r\n\r\nIs it possible to implement this using current `conv3d` layer? The doc for it says:\r\n\"Our Conv3D implements a form of cross-correlation.\", but I was not able to make it actually work for correlating 2 images.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Sounds like a good potential contribution!", "+1", "ditto, would be very useful.", "Thanks for the helpful links to user-level solutions! I'll start implementing this later in July, and given my free time it will likely take me till around September to complete.", "@zboldyga , did you get anywhere? This would be very useful. Something a little better engineered, documented and generic than some of the efforts already out there would be wonderful. ", "@evolu8 eek! I totally lost track of this ticket, thanks for the reminder... I read the paper today as a refresher and the correlation layer seems straightforward. Will be in the weeds and make some commits shortly  \ud83d\ude04\r\n\r\nEdit: Looks like PatWie has already implemented this in: https://github.com/tensorflow/tensorflow/pull/21392 . The TF team is working on merging it into addons.", "Brilliant. Good luck!\n\nOn Thu, Jan 3, 2019, 12:41 AM Zach Boldyga <notifications@github.com wrote:\n\n> @evolu8 <https://github.com/evolu8> eek! I totally lost track of this\n> ticket, thanks for the reminder... I read the paper today as a refresher\n> and the correlation layer seems straightforward. Will be in the weeds and\n> make some commits shortly \ud83d\ude04\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11956#issuecomment-451007371>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABvlRZjw82pDKDRk5rP8dR-JJo_qGzoRks5u_TWrgaJpZM4Oqehe>\n> .\n>\n", "This can be closed as it was added in https://github.com/tensorflow/addons/pull/207\r\n\r\ncc @facaiy ", "Well done"]}, {"number": 11955, "title": "Incorrect tfBinaryURL for installing with Anaconda on Linux", "body": "There is incorrect tfBinaryURL at tensorflow.org.\r\n\r\nIn case of Installing with Anaconda in Linux, the example shows the following command for Python 2.7.\r\n(tensorflow)$ pip install --ignore-installed --upgrade \\\r\n https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp34-cp34m-linux_x86_64.whl\r\nBut this is for Python 3.4 so correct command is below.\r\n(tensorflow)$ pip install --ignore-installed --upgrade \\\r\n https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp27-none-linux_x86_64.whl\r\n", "comments": ["Thanks for pointing this out.", "So, we already have fixed this in the source, as you can see here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_linux.md#InstallingAnaconda\r\n\r\nIt just hasn't propagated out to be the root version of the documentation on tensorflow.org yet, but that should happen fairly soon."]}, {"number": 11953, "title": "Is `tf.matrix_inverse()` supported for TF v.1.2?", "body": "I notice that the Tensorlfow computation op `tf.matrix_inverse()` was not supported running on GPU in the previous versions, and I wonder if it's supported for Tensorflow v.1.2 now. If it's still not, can we make a feature request for it? Thanks!\r\n", "comments": ["@RuofanKong it's supported in head, benchmark numbers are in https://github.com/tensorflow/tensorflow/pull/12116/commits/e57e11b74b983d663f612d40facd97b83189bbac"]}, {"number": 11952, "title": "Make plugin_data optional instead of repeated", "body": "Every summary op writes data for a single plugin to process. Hence, each\r\nSummaryMetadata proto should have a single PluginData optional field\r\n(instead of a repeated one). This removes much complexity from\r\nTensorBoard logic that loops over the plugin data. It also simplifies\r\nthe SQL schema - it can now enforce a one-to-one relationship between\r\nsummary op and plugin.", "comments": ["Can one of the admins verify this patch?", "@chihuahua, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @tensorflower-gardener and @keveman to be potential reviewers.", "Mr. Jenkins: test this please.", "The included changes look good to me\u2014are there really no changes to API goldens?", "@wchargin, same question: `PASS: //tensorflow/tools/api/tests:api_compatibility_test`", "Mr. Jenkins, test this please.", "Mr. Jenkins: test this please.", "Does anyone know why the tests don't run? Thank you!", "@tensorflow-jenkins test this please", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please. Cmake build hung."]}, {"number": 11951, "title": "Make plugin_data optional and (not repeated)", "body": "Make plugin_data optional and (not repeated)\u00a0\u2026Every summary op writes data for a single plugin to process. Hence, each SummaryMetadata proto should have a single PluginData optional field (instead of a repeated one). This removes much complexity from TensorBoard logic that loops over the plugin data. It also simplifies the SQL schema - it can now enforce a one-to-one relationship between summary op and plugin.\r\n\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@chihuahua, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @tensorflower-gardener and @keveman to be potential reviewers.", "I am closing this specific PR. Please review https://github.com/tensorflow/tensorflow/pull/11952 instead."]}, {"number": 11950, "title": "Issues with tensordot and axes is list of ints", "body": "```\r\n== cat /etc/issue ===============================================\r\nLinux node014-jupyter-20170708-132328 4.4.0-1022-aws #31-Ubuntu SMP Tue Jun 27 11:27:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux node014-jupyter-20170708-132328 4.4.0-1022-aws #31-Ubuntu SMP Tue Jun 27 11:27:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.2.0)\r\ntensorflow-gpu (1.2.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\r\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue Aug  1 17:13:34 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n| N/A   59C    P0    67W / 149W |      0MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n```\r\n### Describe the problem\r\n`np.tensordot` works for a given set of settings but `tf.tensordot` does not.\r\n\r\n### Source code / logs\r\nThis works:\r\n```python\r\nnp.tensordot(np.ones((3,3)), np.array([2, 3, 1])[None, None], axes=[1, 2])\r\n```\r\nbut this does not:\r\n```python\r\nwith tf.Session() as sess:\r\n    sess.run(\r\n         tf.tensordot(tf.ones((3,3), dtype=tf.float32), tf.constant([2, 3, 1], dtype=tf.float32)[None, None], axes=[1, 2])\r\n    )\r\n```\r\nI get:\r\n```py3tb\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-e93670defe29> in <module>()\r\n      2 with tf.Session() as sess:\r\n      3     sess.run(\r\n----> 4         tf.tensordot(tf.ones((3,3), dtype=tf.float32), tf.constant([2, 3, 1], dtype=tf.float32)[None, None], axes=[1, 2])\r\n      5     )\r\n\r\n~/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py in tensordot(a, b, axes, name)\r\n   2415     a = ops.convert_to_tensor(a, name=\"a\")\r\n   2416     b = ops.convert_to_tensor(b, name=\"b\")\r\n-> 2417     a_axes, b_axes = _tensordot_axes(a, axes)\r\n   2418     a_reshape, a_free_dims, a_free_dims_static = _tensordot_reshape(a, a_axes)\r\n   2419     b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(b, b_axes,\r\n\r\n~/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py in _tensordot_axes(a, axes)\r\n   2403       a_axes = axes[0]\r\n   2404       b_axes = axes[1]\r\n-> 2405       if len(a_axes) != len(b_axes):\r\n   2406         raise ValueError(\r\n   2407             \"Different number of contraction axes 'a' and 'b', %s != %s.\",\r\n\r\nTypeError: object of type 'int' has no len()\r\n```", "comments": ["The case of `axes=[1, 2]` was not covered in `_tensordot_axes()`, only `axes=[[1], [2]]` had been covered.\r\n\r\nCreated a PR #11959 to fix this issue."]}, {"number": 11949, "title": "Fix \"depsets cannot contain mutable items\" error when using Bazel 0.5.3", "body": "Allows CUDA builds with more recent versions of Bazel. Fixes https://github.com/tensorflow/tensorflow/issues/11871. Perhaps we should also file a Bazel bug, although it would be nice to have the workaround in the meantime.", "comments": ["We know that building with bazel 0.5.3 doesn't work -- bazel knows as well. Thanks for the fix though!", "Thanks Allen! cmake failure is an infra issue, and this shouldn't affect cmake anyway. Merging.", "@av8ramit is this on the 1.3 branch? If not, can you cherry-pick it? It's build only so no need for another RC because of this. ", "@martinwicke @av8ramit I've created #12248 where I cherry picked the fix and added a pull request to r1.3 as detailed above. I needed the change anyway. :-)", "Thanks @ahundt ! I'll merge your change into the 1.3 branch."]}, {"number": 11948, "title": "Memory leak in Java API when using GPU", "body": "### System information\r\n- **Custom code**: https://github.com/riklopfer/TensorflowJavaGpuMemoryTest\r\n- **OS**: CentOS 7\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: n/a\r\n- **Python version**: n/a\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GeForce GTX 1080\r\n- **Exact command to reproduce**: see https://github.com/riklopfer/TensorflowJavaGpuMemoryTest\r\n\r\n### Describe the problem\r\nMain memory on the machine is continuously consumed when running on the GPU. Memory consumption hovers around 600M when running on the CPU.\r\n\r\n### Source code / logs\r\nsee: https://github.com/riklopfer/TensorflowJavaGpuMemoryTest", "comments": ["@asimshankar could you please take a look at this.", "Sample log output fwiw, \r\n\r\n```\r\n2017-08-29 14:30:27.963729: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-29 14:30:27.963779: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-29 14:30:27.963788: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-29 14:30:27.963795: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-29 14:30:27.963802: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-29 14:30:29.569904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\n2017-08-29 14:30:29.569957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 \r\n2017-08-29 14:30:29.569965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y \r\n2017-08-29 14:30:29.569981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\n```", "I've added [valgrind](http://valgrind.org/info/tools.html#memcheck) output to the test repository: https://github.com/riklopfer/TensorflowJavaGpuMemoryTest/blob/master/valgrind.out\r\n\r\nI'm not really familiar with this tool, but it seems like it would be useful. The summary makes me think that there definitely is a leak somewhere\r\n\r\n```\r\n==28997== LEAK SUMMARY:\r\n==28997==    definitely lost: 257,022 bytes in 1,028 blocks\r\n==28997==    indirectly lost: 6,840 bytes in 15 blocks\r\n==28997==      possibly lost: 61,716,234 bytes in 14,975 blocks\r\n==28997==    still reachable: 397,427,506 bytes in 261,680 blocks\r\n==28997==                       of which reachable via heuristic:\r\n==28997==                         stdstring          : 2,034,837 bytes in 43,856 blocks\r\n==28997==                         newarray           : 22,536 bytes in 1 blocks\r\n==28997==         suppressed: 0 bytes in 0 blocks\r\n==28997== Reachable blocks (those to which a pointer was found) are not shown.\r\n==28997== To see them, rerun with: --leak-check=full --show-leak-kinds=all\r\n==28997== \r\n==28997== For counts of detected and suppressed errors, rerun with: -v\r\n==28997== ERROR SUMMARY: 1011246 errors from 460 contexts (suppressed: 0 from 0)\r\n```", "@riklopfer : Thanks very much for getting that information across. Unfortunately not a lot struck out to me.\r\n\r\nI did see 32 bytes of leaks from graph construction, which I will fix, but that happens once - not in a loop so won't explain the increasing usage over time.\r\n\r\n", "@asimshankar thanks for the fixes. Were you able to reproduce the issue of ever-increasing memory consumption? Any idea what the next steps might be? ", "Updating CUDA and Nvidia drivers seems to have greatly mitigated the problem for me. I added [updated valgrind output](https://github.com/riklopfer/TensorflowJavaGpuMemoryTest/blob/master/updated-valgrind.out) to the test repo. ", "Thanks for the update @riklopfer \r\nSampling the latest output, I'm not sure if there are false positives or actual leaks (e.g., many leaks are reported in `CreateJavaVM`, which IIUC has nothing to do with TensorFlow, it's just JVM initialization.\r\n\r\nWhen you say \"greatly mitigated\", are you still seeing a monotonic increase in memory usage over time, or does it stabilize?", "@asimshankar I no longer see monotonic increase in memory consumption when running my the small test  in the linked repo. However, when I run a longer, more complicated graph on the GPU, it is killed by the OOM killer. I wasn't able to get a valgrind dump for that process. When I have time, I will try increasing the complexity of the test graph until it shows the problem again (or not).", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Running with 1.4.0, I still see a slow, monotonic increase in memory consumption. I haven't had a chance to attempt to minimally reproduce the issue. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Closing since the original issue has been fixed. Please file another ticket with a repro if you can. Thanks!"]}, {"number": 11947, "title": "No module named 'tensorflow.contrib.keras.datasets'", "body": "In the documentation, in order to import Keras dataset, you have to\r\n\r\n```python\r\nimport tensorflow.contrib.keras.datasets\r\n``` \r\nBut instead, it only works when\r\n\r\n```python\r\nimport tensorflow.contrib.keras.python.keras.datasets\r\n``` \r\nI think this problem would occur not just for datasets but anything else in Keras.\r\n", "comments": ["I'm not able to find the import statement you cite.  Could you tell me where it is?\r\n", "https://www.tensorflow.org/api_docs/python/tf/contrib/keras/datasets/mnist/load_data", "I'm afraid I still don't see it.  Are you talking about the module names, in the left column?\r\n\r\n", "I might be doing something wrong, but since the documentation suggests 'tf.contrib.keras.datasets.mnist' as the module name, I assumed that I import it by \r\n```python\r\nimport tensorflow.contrib.keras.datasets\r\n```\r\nbut that didn't work.", "It appears that the module names in the documentation are frequently not the same as the include paths. \r\n This appears to be intentional.  The documentation often gives a specific path where a particular module is defined.", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (25/25 (100%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from o8.sgmail.github.com ([167.89.101.199]:49132)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1dcxkT-0001C4-Fv\n\tfor mazecreator@mazecreator.com; Wed, 02 Aug 2017 12:48:27 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:in-reply-to:references:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=f5+n68l/4GpsLN1qcMXq1bJCPAc=; b=f3xdCKBBPrrDXpaw\n\tdb0Ed+gmwWgEzLEmEghLlUCGZs2tfcmMrar/9z9UYG7jaDhJayySKdZedUrnbRFB\n\t156Lt8XEMwtlQ1LAJzeY/b829sFc029rqq14g/KKAsQt1PEhQIW/QPYf7kogH0Wh\n\taqYj1/XSIQB4n9+P4f4oBoMjO88=\nReceived: by filter1183p1mdw1.sendgrid.net with SMTP id filter1183p1mdw1-4011-598212CD-4\n        2017-08-02 17:58:37.141982529 +0000 UTC\nReceived: from github-smtp2b-ext-cp1-prd.iad.github.net (github-smtp2b-ext-cp1-prd.iad.github.net [192.30.253.17])\n\tby ismtpd0003p1iad1.sendgrid.net (SG) with ESMTP id QSYq8lRUQ2-QET19N8RkSA\n\tfor <mazecreator@mazecreator.com>; Wed, 02 Aug 2017 17:58:37.094 +0000 (UTC)\nDate: Wed, 02 Aug 2017 17:58:37 +0000 (UTC)\nFrom: Paul Tucker <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/issues/11947/319749254@github.com>\nIn-Reply-To: <tensorflow/tensorflow/issues/11947@github.com>\nReferences: <tensorflow/tensorflow/issues/11947@github.com>\nSubject: Re: [tensorflow/tensorflow] No module named\n 'tensorflow.contrib.keras.datasets' (#11947)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_598212cd930_43193f937dc4dc2c2913b3\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: poxvoculi\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a055f8e1fbcaad06280825e8fc7a1dda1f112b3b9e92cf000000011599d4cc92a169ce0ebb50fd@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoCQ2EEnFvCfSMVi4FKqOFJFjQ5R0ks5sULjMgaJpZM4OqGqK>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1JqwUDOF6o+ABtgutVnJidtUDbxKNJzfSU5jNRY\n u1bTaGsrzn+h97saTmOZs0se1Htoe96xeR//QMyWPMinDVWIVEFFLiHMBHxoDQFPYQ/mOiWCPIRVWW\n Wif7vSHd6glW8UisOuImMP8FZUzGfn1iedAtlR3Tqyqcl8LuVLFa6sLnNdCY1SXg7nP1vPUWrc5oRi\n 4=\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_598212cd930_43193f937dc4dc2c2913b3\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\nIt appears that the module names in the documentation are frequently not the same as the include paths. \n This appears to be intentional.  The documentation often gives a specific path where a particular module is defined.\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/issues/11947#issuecomment-319749254\n----==_mimepart_598212cd930_43193f937dc4dc2c2913b3\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: quoted-printable\n\n<p>It appears that the module names in the documentation are frequently not=\n the same as the include paths.<br>\nThis appears to be intentional.  The documentation often gives a specific p=\nath where a particular module is defined.</p>\n\n<p style=3D\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mda=\nsh;<br />You are receiving this because you are subscribed to this thread.<=\nbr />Reply to this email directly, <a href=3D\"https://github.com/tensorflow=\n/tensorflow/issues/11947#issuecomment-319749254\">view it on GitHub</a>, or =\n<a href=3D\"https://github.com/notifications/unsubscribe-auth/ARjzoHSrXLxy29=\n1Y8CosjV6FMu_0Dfphks5sULjMgaJpZM4OqGqK\">mute the thread</a>.<img alt=3D\"\" h=\neight=3D\"1\" src=3D\"https://github.com/notifications/beacon/ARjzoGdEnEJLA0Nb=\nRZoEMxp92vyyiVyCks5sULjMgaJpZM4OqGqK.gif\" width=3D\"1\" /></p>\n<div itemscope itemtype=3D\"http://schema.org/EmailMessage\">\n<div itemprop=3D\"action\" itemscope itemtype=3D\"http://schema.org/ViewAction=\n\">\n  <link itemprop=3D\"url\" href=3D\"https://github.com/tensorflow/tensorflow/i=\nssues/11947#issuecomment-319749254\"></link>\n  <meta itemprop=3D\"name\" content=3D\"View Issue\"></meta>\n</div>\n<meta itemprop=3D\"description\" content=3D\"View this Issue on GitHub\"></meta>\n</div>\n\n<script type=3D\"application/json\" data-scope=3D\"inboxmarkup\">{\"api_version\"=\n:\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"Gi=\ntHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"ten=\nsorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https:=\n//cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95f=\nc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com=\n/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":=\n{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}}=\n,\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@poxvoculi in #11947: I=\nt appears that the module names in the documentation are frequently not the=\n same as the include paths. \\r\\n This appears to be intentional.  The docum=\nentation often gives a specific path where a particular module is defined.\"=\n}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/tensorflow/tenso=\nrflow/issues/11947#issuecomment-319749254\"}}}</script>=\n\n----==_mimepart_598212cd930_43193f937dc4dc2c2913b3--\n"]}, {"number": 11946, "title": "Branch 163848365", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @meheffernan, @hawkinsp and @blakehechtman to be potential reviewers."]}, {"number": 11945, "title": "Fix #11803: Force CUPTI to flush GPU activity buffers when stopping tracing.", "body": "As described #11803, stopping GPU tracing, GPU events are not flushed. Then we cannot get GPU activities in performance statistics.\r\n\r\nCalling  ActivityFlushAll() with flag CUPTI_ACTIVITY_FLAG_NONE causes this problem, so this commit changes the flag to CUPTI_ACTIVITY_FLAG_FLUSH_FORCED.\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11944, "title": "Feature Request: Video Decoder", "body": "A video decoder would make it much easier to work with video. For instance, something like tf.image.decode_jpeg, but for mp4 videos (or some other format perhaps).", "comments": ["@shlens do you have any comments on this?", "We are generally reluctant to add more dependencies to tensorflow on external libraries. In particular, many external open-source video decoding libraries (e.g. ffmpeg) tend to be quite large and bring in many dependencies. In addition, decoding a video on the fly can be computationally expensive and may detract from the training speed.\r\n\r\nThat said, one can always build a stand-alone script that imports arbitrary libraries and provides a one-time conversion for individual video frames to tf.Example's that may be processed by TensorFlow with ease.\r\n", "FFmpeg has already been integrated.\r\nhttps://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=ffmpeg&type=\r\n\r\nDuplicate of https://github.com/tensorflow/tensorflow/issues/6265.", "@shlens  If I want to do on-line video classification. An integrated video infrastructure will speed up the inference time according to my profiling on my application."]}, {"number": 11943, "title": "Consoles freezes while reading an image. ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1\r\n- **Python version**:  3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:51:32)  GCC 4.4.7 \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 5.1\r\n- **GPU model and memory**: GTX-1050 4GB\r\n\r\n\r\n### Describe the problem\r\n\r\nConsoles freezes while reading an image. \r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\nimage_filename = \"/home/kaiyin/PycharmProjects/tensorflow-for-machine-intelligence/images/chapter-05-object-recognition-and-classification/working-with-images/test-input-image.jpg\"\r\nfilename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(image_filename))\r\nimage_reader = tf.WholeFileReader()\r\n_, image_file = image_reader.read(filename_queue)\r\nimage = tf.image.decode_jpeg(image_file)\r\nsess = tf.InteractiveSession()\r\nsess.run(image)\r\n```\r\n\r\nAlso tried the non-interactive session:\r\n\r\n```python\r\nimport tensorflow as tf\r\nwith tf.Session() as sess:\r\n    image_filename = \"/home/kaiyin/PycharmProjects/tensorflow-for-machine-intelligence/images/chapter-05-object-recognition-and-classification/working-with-images/test-input-image.jpg\"\r\n    filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(image_filename))\r\n    image_reader = tf.WholeFileReader()\r\n    _, image_file = image_reader.read(filename_queue)\r\n    image = tf.image.decode_jpeg(image_file)\r\n    sess.run(image)\r\n```\r\n\r\nError:\r\n\r\n```\r\n2017-08-01 17:39:36.997011: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 17:39:36.997023: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 17:39:36.997026: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 17:39:36.997028: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 17:39:36.997031: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 17:39:37.078504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-08-01 17:39:37.078697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: GeForce GTX 1050\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.493\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 1.84GiB\r\n2017-08-01 17:39:37.078705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-08-01 17:39:37.078708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-08-01 17:39:37.078715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)\r\n```\r\n\r\nAfter showing the message above, the console is not responsive any more. \r\n", "comments": ["This might be better suited to stackoverflow (you didn't start queue runners)", "@kindlychung  @yaroslavvb \r\nHi, i met the same issue in Win7x64, when i read an image, \r\n\r\n```python\r\nimport tensorflow as tf\r\nimage_filename = \"./1.jpg\"\r\nfilename_queue = tf.train.string_input_producer(\r\n    tf.train.match_filenames_once(image_filename)\r\n)\r\nimage_reader = tf.WholeFileReader()\r\n_, image_file = image_reader.read(filename_queue)\r\nimage = tf.image.decode_jpeg(image_file)\r\nsess=tf.InteractiveSession()\r\nprint(sess.run(image))\r\n\r\n```\r\nThe console show the log:\r\n```log\r\n>py imgload.py\r\nTensor(\"ReaderReadV2:1\", shape=(), dtype=string)\r\n2018-05-18 10:03:39.221544: I C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windo\r\nws\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports in\r\nstructions that this TensorFlow binary was not compiled to use: AVX2\r\n```\r\n\r\nThe console has no responsive any more.", "@kindlychung\r\nit seems the code can resloved the issue, but i donnot know why.\r\nso the code work well.\r\n```\r\nimport tensorflow as tf\r\nimage_filename = \"./1.jpg\"\r\n\r\n# i donnot know why i need change this code here????????\r\nfilename_queue = tf.train.string_input_producer([image_filename])\r\n\r\nimage_reader = tf.WholeFileReader()\r\n_, image_file = image_reader.read(filename_queue)\r\nimage = tf.image.decode_jpeg(image_file)\r\nsess = tf.InteractiveSession()\r\n\r\nsess.run(tf.global_variables_initializer())\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(coord=coord)\r\n\r\nprint(sess.run(image))\r\n\r\ncoord.request_stop()\r\ncoord.join(threads)\r\n```\r\nYou can get the result:\r\n```\r\n>py imgload.py\r\n[[[198 135 205]\r\n  [200 130 190]\r\n  [219 131 171]\r\n  [243 131 143]\r\n  [253 124 119]]\r\n\r\n [[ 35   0 102]\r\n  [ 51   0  97]\r\n  [ 63   0  59]\r\n  [233 129 162]\r\n  [252 127 131]]\r\n\r\n [[ 14   0 161]\r\n  [ 26   0 142]\r\n  [ 49   0 100]\r\n  [223 134 188]\r\n  [247 131 144]]]\r\n```\r\n", "Closing as this is resolved, feel free to reopen if problem persists."]}, {"number": 11942, "title": "Got `ValueError: Both labels and logits must be provided` while both labels and logits have been provided. ", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1\r\n- **Python version**:  3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:51:32)  GCC 4.4.7 \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 5.1\r\n- **GPU model and memory**: GTX-1050 4GB\r\n\r\n\r\n### Describe the problem\r\n\r\nGot `ValueError: Both labels and logits must be provided` while both labels and logits have been provided. \r\n\r\n### Source code / logs\r\n\r\n```python\r\n# Softmax example in TF using the classical Iris dataset\r\n# Download iris.data from https://archive.ics.uci.edu/ml/datasets/Iris\r\n\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nimport os\r\n\r\ndef combine_inputs(X):\r\n    res =  tf.matmul(X, W) + b\r\n    res = tf.identity(res, name=\"linear_out\")\r\n\r\n\r\ndef inference(X):\r\n    return tf.nn.softmax(combine_inputs(X), \"softmax_out\")\r\n\r\n\r\ndef loss(X, Y):\r\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=combine_inputs(X), name=\"softmax_entropy\"), name=\"loss\")\r\n\r\n\r\ndef read_csv(batch_size, file_name, record_defaults):\r\n    filename_queue = tf.train.string_input_producer([os.path.dirname(__file__) + \"/\" + file_name])\r\n    reader = tf.TextLineReader(skip_header_lines=1)\r\n    key, value = reader.read(filename_queue)\r\n    # decode_csv will convert a Tensor from type string (the text line) in\r\n    # a tuple of tensor columns with the specified defaults, which also\r\n    # sets the data type for each column\r\n    decoded = tf.decode_csv(value, record_defaults=record_defaults)\r\n    # batch actually reads the file and loads \"batch_size\" rows in a single tensor\r\n    return tf.train.shuffle_batch(decoded,\r\n                                  batch_size=batch_size,\r\n                                  capacity=batch_size * 50,\r\n                                  min_after_dequeue=batch_size)\r\n\r\n\r\ndef inputs():\r\n    sepal_length, sepal_width, petal_length, petal_width, label =\\\r\n        read_csv(100, \"iris.csv\", [[0.0], [0.0], [0.0], [0.0], [\"\"]])\r\n    # convert class names to a 0 based class index.\r\n    label_number = tf.to_int32(tf.argmax(tf.to_int32(tf.stack([\r\n        tf.equal(label, [\"Iris-setosa\"]),\r\n        tf.equal(label, [\"Iris-versicolor\"]),\r\n        tf.equal(label, [\"Iris-virginica\"])\r\n    ])), 0))\r\n    # Pack all the features that we care about in a single matrix;\r\n    # We then transpose to have a matrix with one example per row and one feature per column.\r\n    features = tf.transpose(tf.stack([sepal_length, sepal_width, petal_length, petal_width]))\r\n    return features, label_number\r\n\r\n\r\ndef train(total_loss):\r\n    learning_rate = 0.01\r\n    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\r\n\r\n\r\ndef evaluate(sess, X, Y):\r\n    predicted = tf.cast(tf.arg_max(inference(X), 1), tf.int32)\r\n    print(sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))\r\n\r\n\r\n# Launch the graph in a session, setup boilerplate\r\nwith tf.Graph().as_default():\r\n    with tf.Session() as sess:\r\n        # this time weights form a matrix, not a column vector, one \"weight vector\" per class.\r\n        W = tf.Variable(tf.zeros([4, 3]), name=\"weights\")\r\n        # so do the biases, one per class.\r\n        b = tf.Variable(tf.zeros([3], name=\"bias\"))\r\n        tf.global_variables_initializer().run()\r\n        X, Y = inputs()\r\n        total_loss = loss(X, Y)\r\n        train_op = train(total_loss)\r\n        summary_out = os.path.join(os.path.dirname(__file__), \"tf_summary\")\r\n        import shutil\r\n        shutil.rmtree(summary_out)\r\n        writer = tf.summary.FileWriter(summary_out, graph=sess.graph)\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n        # actual training loop\r\n        training_steps = 1000\r\n        for step in range(training_steps):\r\n            sess.run([train_op])\r\n            # for debugging and learning purposes, see how the loss gets decremented thru training steps\r\n            if step % 10 == 0:\r\n                print(\"loss: \", sess.run([total_loss]))\r\n        evaluate(sess, X, Y)\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n        writer.close()\r\n```\r\n\r\nError:\r\n\r\n```\r\n/home/kaiyin/miniconda3/envs/tf/bin/python3.6 /home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py\r\n2017-08-01 15:12:57.469597: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 15:12:57.469610: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 15:12:57.469613: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 15:12:57.469616: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 15:12:57.469618: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 15:12:57.554651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-08-01 15:12:57.554839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: GeForce GTX 1050\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.493\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 1.97GiB\r\n2017-08-01 15:12:57.554848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-08-01 15:12:57.554852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-08-01 15:12:57.554859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)\r\nTraceback (most recent call last):\r\n  File \"/home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py\", line 69, in <module>\r\n    total_loss = loss(X, Y)\r\n  File \"/home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py\", line 17, in loss\r\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(None, Y, combine_inputs(X)), name=\"loss\")\r\n  File \"/home/kaiyin/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1555, in softmax_cross_entropy_with_logits\r\n    labels, logits)\r\n  File \"/home/kaiyin/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1512, in _ensure_xent_args\r\n    raise ValueError(\"Both labels and logits must be provided.\")\r\nValueError: Both labels and logits must be provided.\r\n```\r\n", "comments": ["The combine_inputs function does not return any value.\r\n\r\nThis sort of question is better asked on stackoverflow.", "TMKB You could answer the question"]}, {"number": 11941, "title": "Where can I find \"gen_checkpoint_ops\"", "body": "_**When I was debuging inception v4, there has a message:**_\r\n\"22 from tensorflow.contrib.framework.python.ops import gen_checkpoint_ops\r\n     23 from tensorflow.contrib.util import loader\r\n     24 from tensorflow.python.framework import dtypes\r\n\r\nImportError: cannot import name gen_checkpoint_ops\r\n\".\r\n_**But I can't find a module or package named \"gen_checkpoint_ops\".**_\r\n_**I don't know how to resolve this problem.**_ \r\n#  Help!\r\n", "comments": ["This is probably better for stackoverflow or tensorflow/models repo. I'm guessing you need to use a different version of TF"]}, {"number": 11940, "title": "dynamic_stitch op gpu version", "body": "Issue([#7251](https://github.com/tensorflow/tensorflow/issues/7251))\r\nHave implemented the **GPU** version **dynamic_stitch**. all tested passed locally.\r\nuse the [script](https://gist.github.com/nolanliou/b63b64bf32f1a52d7bf0cd775bbbc52b) do a quick benchmark.\r\n\r\n**Env**:\r\nBased TF Version: r1.3\r\ncuda: 8.0\r\ncudnn: 5.1\r\nGPU: Tesla M40\r\n\r\n**Result**:\r\nCPU Version: Time 1000: **40.180424**s\r\nGPU Version: Time 1000: **5.095576**s\r\n\r\nThe thought reference @MycChiu code, thanks for great jobs.  \r\nThanks @girving for precious suggestions. ", "comments": ["Can one of the admins verify this patch?", "@nolanliou, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @josh11b and @andrewharp to be potential reviewers.", "@ekelsen fixed it. thanks.", "@ekelsen Is there someone to call Jenkins to test this PR?", "I don't think I can do it either", "Jenkins test this please", "Ping @benoitsteiner ?", "Is there someone to test and merge this PR?", "@tensorflow-jenkins test this please", "@nolanliou I'm sorry this was not tested earlier. I'm oncall this week and will make sure we get it tested etc.", "https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/6212/consoleFull\r\n\r\n@nolanliou It looks like you are missing a registration:\r\n\r\n2017-08-08 20:13:55.937575: I tensorflow/core/kernels/logging_ops.cc:79] Hello[3]\r\n.2017-08-08 20:13:56.003997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1042] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:0a.0)\r\n2017-08-08 20:13:56.089320: E tensorflow/core/common_runtime/executor.cc:643] Executor failed to create kernel. Not found: No registered 'DynamicStitch' OpKernel for GPU devices compatible with node y_shape = DynamicStitch[N=2, T=DT_INT32](stitch_idx0, i, x_shape, stitch_val1)\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_STRING]\r\n  device='CPU'; T in [DT_BOOL]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n\r\n\t [[Node: y_shape = DynamicStitch[N=2, T=DT_INT32](stitch_idx0, i, x_shape, stitch_val1)]]", "@rmlarsen Should we support the DT_INT32 for gpu version? I checked some Ops and there are no DT_INT32 registration. \r\nThere is an explanation [no-gpu-kernel-for-an-int32-variable-op](https://stackoverflow.com/questions/37439299/no-gpu-kernel-for-an-int32-variable-op) about this question.  ", "Maybe it's necessary, I have updated the code.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@rmlarsen could you help to check what's going on the MacOS CPU Tests? I cannot see the log, thanks."]}, {"number": 11939, "title": "Disable denormal_test on ppc64le platform", "body": "Please find the relevant discussion - https://github.com/tensorflow/tensorflow/issues/11902\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/core/platform/denormal.cc#L43-L73\r\n_MM_GET_FLUSH_ZERO_MODE\r\n_MM_GET_DENORMALS_ZERO_MODE\r\n_MM_SET_FLUSH_ZERO_MODE\r\n_MM_SET_DENORMALS_ZERO_MODE\r\n\r\nSome embedded machines perform slight better in this mode, but POWER servers do not. So for POWER you do not need this and should simply disable it.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Looks like a Jenkins issue, retest this please.", "@gunan Checks failed due to unrelated error, please have a look ", "Jenkins, test this please.", "@tensorflow-jenkins test this please", "MacOS test unrelated."]}]