[{"number": 10377, "title": "tf.contrib.data.Dataset fails to batch elements that are tuples", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.2.0-rc0-312-g0b72359', '1.2.0-rc0')\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: TITAN X (Pascal) / 12186MiB\r\n\r\n### Issue\r\n\r\nWith the new `tf.contrib.data.Dataset` API, it seems not possible to batch the outputs of a Dataset instance whose elements are tuples.  Code demonstrating problem:\r\n\r\n```\r\nds = tf.contrib.data.Dataset.range(100).map(lambda x: (x, 2*x))\r\nbatched_ds = ds.batch(4)\r\nbatched_iter = batched_ds.make_one_shot_iterator()\r\nnxt = batched_iter.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    val = sess.run(nxt)\r\n    print(type(val))  # => <type 'tuple'>\r\n    print(len(val))  # => 2\r\n```\r\nIt seems that `sess.run(nxt)` should provide an object having 4 elements.", "comments": ["`val` is a tuple of batches, in this case, `print(val)` yields:\r\n\r\n```python\r\n(array([0, 1, 2, 3]), array([0, 2, 4, 6]))\r\n```\r\n\r\n@mrry for comments on how batch is intended to and documented to deal with tuples.", "On second thought, I think this is the expected behavior -- seemed unintuitive though because the batching is done by grouping elements of a series of tuples, rather than grouping the series of tuples where each tuple is a row in the output 'batch tensor'."]}, {"number": 10376, "title": "Lack support of qint32 in tf.nn.tanh", "body": "According to the [doc](https://www.tensorflow.org/versions/master/api_docs/python/tf/tanh), the `tanh` operation supports floating point inputs as well as fixed point inputs of type qint32. However, in the latest master, a `TypeError` raised when running following code:\r\n```\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\nx = tf.constant([1.,2.,3.], dtype=tf.float32)\r\n\r\nfrom tensorflow.python.ops.gen_array_ops import quantize_v2\r\nx_quant = quantize_v2(x, min_range=0., max_range=4., T=tf.qint32)\r\ny_quant = tf.nn.tanh(x_quant[0])\r\n```\r\nThe complete error message is \r\n```\r\nTypeError: Value passed to parameter 'x' has DataType qint32 not in list of allowed values: float16, float32, float64, complex64, complex128\r\n```\r\n\r\nAccording to the backend function `_tanh` in `gen_math_ops.py`:\r\n```\r\ndef _tanh(x, name=None):\r\n  r\"\"\"Computes hyperbolic tangent of `x` element-wise.\r\n\r\n  Args:\r\n    x: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `complex64`, `complex128`.\r\n    name: A name for the operation (optional).\r\n```\r\nIt shows that it doesn't support qint32.\r\n", "comments": ["@dr4b The Python docstring for `tanh` and `sigmoid` says they support various types, e.g. qint32, but fail when those types are passed.\r\n\r\nCC: @girving ", "We gladly accept pull requests on the docs!\r\n\r\n(or I can go change the docstring.  What is the actual list of possible types?  We currently seem to say \"float, double, int32, complex64, int64, or qint32\" which is quite a bit off from \"float16, float32, float64, complex64, complex128\" in the error message OR the backend \"`half`, `float32`, `float64`, `complex64`, `complex128`\".)", "This is just a Python doc error: the C++ op registrations use a reasonable set of types (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc#L193) but the Python wrapper does not (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py#L2061)."]}, {"number": 10375, "title": "[Feature] sparse_tensor_dense_matmul with two SparseTensor", "body": "Would it be possible to implement matrix multiplication between two `SparseTensor`? \r\n\r\nCurrently, it's possible to multiply two dense `Tensor` with many zeros using the `X_is_sparse` parameter of `tf.matmul`, and a `SparseTensor` with a `Tensor` using `tf.sparse_tensor_dense_matmul`. \r\n\r\nHowever, with some datasets it is impossible to store dense matrices in memory and it would be great if we could have the option of fully sparse multiplication.\r\n\r\nThanks,\r\nDaniele\r\n", "comments": ["@ebrevdo @rmlarsen : Do you know if anyone is planning to add this?", "Another thing I would like to add is that it would be awesome if there would be an option to **return a `SparseTensor`** instead of a `Tensor`, I am doing normalization of a `SparseTensor` with `tf.sparse_tensor_dense_matmul` but this results in a dense `Tensor` with the same shape as the `SparseTensor`, which makes the whole `SparseTensor` obsolete. ", "We are looking into sparse-sparse matmuls but won't have anything available\nin the near future.  If we do, it won't be\ncalled sparse_tensor_dense_matmul.\n\nOn Jun 27, 2017 4:09 AM, \"wbwvos\" <notifications@github.com> wrote:\n\n> Another thing I would like to add is that it would be awesome if there\n> would be an option to *return a SparseTensor* instead of a Tensor, I am\n> doing normalization of a SparseTensor with tf.sparse_tensor_dense_matmul\n> but this results in a dense Tensor with the same shape as the SparseTensor,\n> which makes the whole SparseTensor obsolete.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10375#issuecomment-311326800>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6yX-MdU4x4BaO7qPB0GNi-Eba-Yks5sIOLcgaJpZM4NtKgf>\n> .\n>\n", "Sparse matrix multiply another sparse matrix. The implementation should be simpler than \"sparse X dense\". It is so strange that the former does not exist in TF but later one exists. ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "+1 on this request. Any update on a timeline?\r\n  ", "+1 Here here!", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@ebrevdo any updates on sparse-sparse matmul support?  Should this be \"contributions welcome\"?", "+1 ", "+1", "Yes; at this point I'd say this is a \"contributions welcome\" since no one\nis actively working on this.\n\nOn Fri, Feb 16, 2018 at 11:26 AM, Pierre Lison <notifications@github.com>\nwrote:\n\n> +1\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10375#issuecomment-366334441>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0Sr3_4EMQo1ascqkB3KDVOPdn18ks5tVdZtgaJpZM4NtKgf>\n> .\n>\n", "+1", "+1", "+1", "+1", "+1 ", "+1", "+1", "Hi, I'm just checking back after a few months. \r\nI am starting to look into developing this on my own, if anybody has any suggestions they are more than welcome (maybe @ebrevdo can help). I'm a total C++ noob but I'll try to edit some existing ops in order to reuse boilerplate code. \r\n\r\nIs [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc) a good starting point (sparse_tensor_dense_matmul_op.cc/cu.cc/h)?\r\nI was thinking that maybe all that's needed is to convert the sparse inputs to a format supported by cuSPARSE and let it handle the multiplication. Would that be a possible approach? \r\n\r\nThanks\r\n", "We do have a sparse matrix TF library that uses cuSparse; and it does have\nthis functionality.  We hope to move it into tf.linalg.experimental\nsometime this year.\n\nOn Thu, May 2, 2019 at 2:24 AM Daniele Grattarola <notifications@github.com>\nwrote:\n\n> Hi, I'm just checking back after a few months.\n> I am starting to look into developing this on my own, if anybody has any\n> suggestions they are more than welcome (maybe @ebrevdo\n> <https://github.com/ebrevdo> can help). I'm a total C++ noob but I'll try\n> to edit some existing ops in order to reuse boilerplate code.\n>\n> Is this\n> <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc>\n> a good starting point (sparse_tensor_dense_matmul_op.cc/cu.cc/h)?\n> I was thinking that maybe all that's needed is to convert the sparse\n> inputs to a format supported by cuSPARSE and let it handle the\n> multiplication. Would that be a possible approach?\n>\n> Thanks\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10375#issuecomment-488607403>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AANWFG5DV2TMBLLTIDDCN5DPTKXLFANCNFSM4DNUVAPQ>\n> .\n>\n", "class GraphConvolutionSparse(Layer):\r\n\t\"\"\"Graph convolution layer for sparse inputs.\"\"\"\r\n\r\n\tdef __init__(self, input_dim, output_dim, adj, features_nonzero, dropout=0., act=tf.nn.relu, **kwargs):\r\n\t\tsuper(GraphConvolutionSparse, self).__init__(**kwargs)\r\n\t\twith tf.variable_scope(self.name + '_vars'):\r\n\t\t\tself.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name=\"weights\")\r\n\t\tself.dropout = dropout\r\n\t\tself.adj = adj\r\n\t\tself.act = act\r\n\t\tself.issparse = True\r\n\t\tself.features_nonzero = features_nonzero\r\n\r\n\tdef _call(self, inputs):\r\n\t\tx = inputs\r\n\t\tx = dropout_sparse(x, 1 - self.dropout, self.features_nonzero)\r\n\t\tx = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])  # X W\r\n\t\tx = tf.sparse_tensor_dense_matmul(self.adj, x)  # A X W\r\n\t\toutputs = self.act(x)\r\n\t\treturn outputs\r\n\r\nI am working on GCN (Graph Convolutional Network). My graph data has more than 1 million nodes. When I run this model, I meet a memory explosion which will kill my process. But once I reduce the size of the graph, it works well. So my question is: is there any solution to the large-scale matrix multiplication? I am stuck here, please help me! Thanks!", "> class GraphConvolutionSparse(Layer):\r\n> \"\"\"Graph convolution layer for sparse inputs.\"\"\"\r\n> \r\n> ```\r\n> def __init__(self, input_dim, output_dim, adj, features_nonzero, dropout=0., act=tf.nn.relu, **kwargs):\r\n> \tsuper(GraphConvolutionSparse, self).__init__(**kwargs)\r\n> \twith tf.variable_scope(self.name + '_vars'):\r\n> \t\tself.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name=\"weights\")\r\n> \tself.dropout = dropout\r\n> \tself.adj = adj\r\n> \tself.act = act\r\n> \tself.issparse = True\r\n> \tself.features_nonzero = features_nonzero\r\n> \r\n> def _call(self, inputs):\r\n> \tx = inputs\r\n> \tx = dropout_sparse(x, 1 - self.dropout, self.features_nonzero)\r\n> \tx = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])  # X W\r\n> \tx = tf.sparse_tensor_dense_matmul(self.adj, x)  # A X W\r\n> \toutputs = self.act(x)\r\n> \treturn outputs\r\n> ```\r\n> \r\n> I am working on GCN (Graph Convolutional Network). My graph data has more than 1 million nodes. When I run this model, I meet a memory explosion which will kill my process. But once I reduce the size of the graph, it works well. So my question is: is there any solution to the large-scale matrix multiplication? I am stuck here, please help me! Thanks!\r\n\r\nPS: the X inputs in the code is super large.", "@chaoyanghe Hmm, facing the exact same problem on GCN. ", "+1. I'm facing same problem on GCN . Although the memory of my GPU is 24gb, it's still not enough to apply a dense tensor of adjacent matrix to execute calculation.", "@danielegrattarola Is there any progress on this? I also need this functionality and would be willing to assist you in developing it. I also have some prior experience with developing new tf-ops. Let me know if I can assist you", "@chpohl I gave up after @ebrevdo told me that they already have the functionality implemented, but it never got released.\nUnfortunately, TF2 and the new Keras broke most practical uses of sparse matrices, so I don't think the the tf devs have any interest in supporting the feature moving forward.\n\nIf you want to implement sparse-sparse matmul for some older version of TF drop me an email, maybe we can look at it together. \n", "It's available in tf2 nightlies under the tf.linalg.experimental module\n\nOn Sat, Nov 2, 2019, 4:34 PM Daniele Grattarola <notifications@github.com>\nwrote:\n\n> @chpohl <https://github.com/chpohl> I gave up after @ebrevdo\n> <https://github.com/ebrevdo> told me that they already have the\n> functionality implemented, but it never got released.\n> Unfortunately, TF2 and the new Keras broke most practical uses of sparse\n> matrices, so I don't think the the tf devs have any interest in supporting\n> the feature moving forward.\n>\n> If you want to implement sparse-sparse matmul for some older version of TF\n> drop me an email, maybe we can look at it together.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10375?email_source=notifications&email_token=AANWFG42DJNBCOFX556DYPDQRYFAHA5CNFSM4DNUVAP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEC5HJYY#issuecomment-549090531>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFGYSLXJE2FJ6RI4DT23QRYFAHANCNFSM4DNUVAPQ>\n> .\n>\n", "Thanks @ebrevdo! I'll close the issue.", "I just install tf-nightly-gpu and cannot find a tf.linalg.experimental module. Do you mean the functionality under tensorflow.python.ops.linalg.sparse ? When I read it correctly, the matmul there supports both sparse inputs. "]}, {"number": 10374, "title": "Weird bugs after successful running until reboot computer", "body": "- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nr0.9\r\n- **Bazel version (if compiling from source)**:\r\n0.2.3\r\n- **CUDA/cuDNN version**:\r\nv4\r\n- **GPU model and memory**:\r\nk80\r\n\r\nDue to need debug some old Tensorflow code, i need to install TF0.9 version, after i installed, it could run code, everything goes okey. But after rebooting computer, run tensorflow code once, it can not run again, it trapped in tf.Session() funtion.\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10373, "title": "Limit the number of images in a summary [feature request]", "body": "When working on dense prediction tasks it is very convenient to show the predictions of the network in TensorBoard to evaluate them qualitatively during training. Even if TensorBoard shows only a subset of these images, all the summaries that have been saved since the beginning of times are kept on disk. The occupied space grows with the size of the dataset, the sampling frequency and the number of experiments, often resulting in a huge waste of space. When the disk space is limited, this can heavily limit the number of experiments logs one can run simultaneously and/or keep stored.\r\n\r\nIt would be great to have a way to either limit the number of images saved on disk or to remove some of them (e.g., for old experiments). Here is what I suggest:\r\n\r\n- Add an argument to `tf.Summary.Image` to define how many images should be kept on disk (e.g., `buffer_size` or `steps_to_retain`). Also add an extra argument (e.g., `retain_strategy`) to select the strategy to define which images should be kept (e.g., keep latest, sample uniformly, ..).\r\n- Add a function to remove some of the stored images programmatically.\r\n\r\nThis feature request is partially related to https://github.com/tensorflow/tensorflow/issues/5039\r\nAlso related to this SO thread: https://stackoverflow.com/questions/41543954/remove-image-outputs-from-tensorboard/41690170\r\n", "comments": ["I don't believe we have any immediate plans for working on this, so I'm going to mark this as Contributions Welcome, with the expectation that this might be updated as things towards #5039 happen.\r\n\r\nIf anyone does want to assist with this, it may make sense to discuss some design here before investing too much time in implementation.\r\n\r\nThanks!", "Hey guys, I have been working on TF for about 3 months now and would like to make a contribution. I can spare about 30 minutes every day. Please let me know what is the qualifying criteria for contribution. \r\n\r\nOf the suggested approaches, I think the first approach works better, since there are future plans to display more than the current subset of images contained in the tensor, as per the SO thread.  \r\nInstead of adding new API parameters - can the max_output parameter be used to determine the summary to be stored on the disk ? Going by the documentation, max_outputs defines the max number of batch elements to generate images for.  ", "@fvisin The summary doesn't really have any control or influence over how things are retained on disk. Basically, everything written to disk stays there forever until you manually remove it. \r\n\r\nThere's some work on the TensorBoard side to switch TB over to using a sql database on the backend, at which point it would be feasible to just delete the old event files directly because a subset of their data will have already been ingested into TensorBoard.\r\n\r\nIf you want to write code for sweeping and downsampling old event files, you're welcome to, but I think it would be better to wait and see how things play out with converting the TensorBoard backend to sql. Pinging @jart so she can chime in on this; also closing as infeasible.\r\n\r\n@pbanavara, that's awesome that you want to contribute. I don't think this one would be the best one to implement. There's another TensorBoard issue [here](https://github.com/tensorflow/tensorboard/issues/30) which a number of people thumbs-up'd and I think would be a good candidate for an external pull request.", "Thanks @dandelionmane , I look forward to an sql backend for Tensorboard. Is there a PR or an issue I can subscribe to, to follow the advancement? I couldn't find any.", "I would take a look here https://github.com/tensorflow/tensorboard/issues/92"]}, {"number": 10372, "title": "Enable figures in the tfprof README.md", "body": "I don't know if excluding the figures from the tfprof README.md was intentional or not and I could not find any reasoning why they are excluded. \r\n\r\nIf there is a reason, then ignore the PR.", "comments": ["Can one of the admins verify this patch?", "LGTM. It's a syntax difference between internal/external doc.", "Jenkins, test this please"]}, {"number": 10371, "title": "//tensorflow/tools/test:cast_op_benchmark  and //tensorflow/tools/test:rnn_op_benchmark tests are failing with KeyError: 'l2_cache_size' on ppc64le", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n      Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n     Installed from source (v1.0.1)\r\n- **TensorFlow version (use command below)**:\r\n     ('v1.0.1-0-ge895d5c-dirty', '1.0.1')\r\n- **Bazel version (if compiling from source)**:\r\n     0.4.4-2017-05-26 (@80a07b5)\r\n- **CUDA/cuDNN version**:\r\n     CUDA = 8.0 and cuDNN = 5.1\r\n- **GPU model and memory**:\r\n     GPU 0: Tesla P100-SXM2-16GB\r\n     GPU 1: Tesla P100-SXM2-16GB\r\n- **Exact command to reproduce**:\r\n    bazel test --config=opt --config=cuda //tensorflow/tools/test:cast_op_benchmark \r\n                                                           or\r\n    bazel test --config=opt --config=cuda //tensorflow/tools/test:rnn_op_benchmark\r\n\r\n### Describe the problem\r\nBoth tests are passing on x86, however getting failure on ppc64le with following errors :\r\n\r\n### Source code / logs\r\n```\r\n$   bazel test --config=opt --config=cuda //tensorflow/tools/test:rnn_op_benchmark\r\n\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n..\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.008s\r\n\r\nOK\r\nTraceback (most recent call last):\r\n  File \"/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/tools/test/run_and_gather_logs.py\", line 99, in <module>\r\n    app.run()\r\n  File \"/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/tools/test/run_and_gather_logs.py\", line 78, in main\r\n    test_args)\r\n  File \"/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/tools/test/run_and_gather_logs_lib.py\", line 145, in run_and_gather_logs\r\n    log_files=log_files), mangled_test_name)\r\n  File \"/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/tools/test/run_and_gather_logs_lib.py\", line 76, in process_test_logs\r\n    system_info_lib.gather_machine_configuration())\r\n  File \"/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/tools/test/system_info_lib.py\", line 44, in gather_machine_configuration\r\n    config.cpu_info.CopyFrom(gather_cpu_info())\r\n  File \"/home/amit/.cache/bazel/_bazel_amit/24cb277fcedebf1cf5a3d7b8e713f578/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/tools/test/rnn_op_benchmark.runfiles/org_tensorflow/tensorflow/tools/test/system_info_lib.py\", line 98, in gather_cpu_info\r\n    l2_cache_size = re.match(r'(\\d+)', str(info['l2_cache_size']))\r\nKeyError: 'l2_cache_size'\r\n\r\n```\r\n\r\nAny comments/suggestions ?", "comments": ["Seems like `l2_cache_size` does not exist in the CPU info returned on your platform.\r\n\r\nCould you paste the output of the following (you may need to `pip install py-cpuinfo`):\r\n```python\r\nimport cpuinfo\r\nprint(cpuinfo.get_cpu_info())\r\n```\r\n\r\nThat will help determine how we can make the test get L2 cache information.\r\n(Which would ultimately end up as a patch to https://github.com/tensorflow/tensorflow/blob/557c539/tensorflow/tools/test/system_info_lib.py#L99)\r\n\r\n", "Hi @asimshankar,  getting following output \r\n\r\n`{'count': 160, 'hz_advertised': '4.0230 GHz', 'bits': 64, 'brand': 'POWER8NVL (raw), altivec supported', 'cpuinfo_version': (3, 2, 0), 'flags': ['dabr', 'dabrx', 'dsisr', 'fpu', 'lp', 'mmu', 'pp', 'rislb', 'run', 'slb', 'sprg3', 'ugr_in_dscr'], 'raw_arch_string': 'ppc64le', 'hz_actual_raw': (4023000000, 0), 'hz_actual': '4.0230 GHz', 'arch': 'PPC_64', 'hz_advertised_raw': (4023000000, 0)}\r\n`\r\n\r\nThanks!", "Alright, it seems that `l2_cache_size` is not something that is returned by `get_cpu_info()` on your platform. I'll send out a change to make `system_info_lib.py` resilient to this case.", "Hi @asimshankar ,\r\n\r\nDid you get chance to look into this ?\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.0.1/tensorflow/tools/test/system_info_lib.py#L93\r\n\r\nAs per my point of view -\r\nIt should not just assume that the l2_cache_size will be there:\r\n\r\n```\r\n  # Gather the rest\r\n  info = cpuinfo.get_cpu_info()\r\n  cpu_info.cpu_info = info['brand']\r\n  cpu_info.num_cores = info['count']\r\n  cpu_info.mhz_per_cpu = info['hz_advertised_raw'][0] / 1.0e6\r\n  l2_cache_size = re.match(r'(\\d+)', str(info['l2_cache_size']))\r\n  if l2_cache_size:\r\n    # If a value is returned, it's in KB\r\n   cpu_info.cache_size['L2'] = int(l2_cache_size.group(0)) * 1024\r\n```\r\n\r\nIt could be changed to something like:\r\n\r\n```\r\n  # Gather the rest\r\n  info = cpuinfo.get_cpu_info()\r\n  cpu_info.cpu_info = info.get('brand', None)\r\n  cpu_info.num_cores = info.get('count', None)\r\n  cpu_info.mhz_per_cpu = info.get('hz_advertised_raw', (0, 0))[0] / 1.0e6\r\n  if 'l2_cache_size' in info:\r\n      l2_cache_size = re.match(r'(\\d+)', str(info['l2_cache_size'])) \r\n      cpu_info.cache_size['L2'] = int(l2_cache_size.group(0)) * 1024\r\n\r\n```\r\nI am not sure whether it is correct or not, please guide me to make changes in system_info_lib.py file to support this case.", "@sandipmgiri : I did submit a change internally to fix this, similar to yours. We push our internal changes to Github typically once a day so PR #10440 should resolve this."]}, {"number": 10370, "title": "Bug: ProtoBuf tokenizer crashes when loading single_image_random_dot_stereograms OP", "body": "### System information\r\n- No custom code\r\n- Ubuntu 17.04 (also confirmed on 16.04)\r\n- TensorFlow installed from source\r\n- TensorFlow version: v1.2.0-rc0-486-g95d90ab2e 1.2.0-rc1\r\n- Bazel version: 0.5.0\r\n- Python version: 3.5.3 (also confirmed on 2.7.12):\r\n- Not tested with GPU support:\r\n\r\nReproduction\r\n------------------\r\n\r\n    import tensorflow as tf\r\n    regressor = tf.contrib.learn.LinearRegressor(feature_columns=[])\r\n\r\nAlternative:\r\n\r\n    import keras\r\n\r\nReference: [Stackoverflow](https://stackoverflow.com/questions/44291072/google-protobuf-text-format-parseerror-when-instantiating-a-tensorflow-model-wit)\r\n\r\nManifestation of the error\r\n---------------------------------\r\n\r\nThe first method to reproduce should cause an assert due to the empty `feature_columns`.\r\nInstead, the protobuf tokenizer crashes with:\r\n\r\n> Traceback (most recent call last):\r\n>  File \"<stdin>\", line 1, in <module>\r\n>  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n>    module = self._load()\r\n>  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n>    module = importlib.import_module(self.__name__)\r\n>  File \"/usr/lib/python3.5/importlib/__init__.py\", line 126, in import_module\r\n>    return _bootstrap._gcd_import(name[level:], package, level)\r\n>  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n>  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n>  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n>  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\r\n>  File \"<frozen importlib._bootstrap_external>\", line 673, in exec_module\r\n>  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n>  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py\", line 35, in <module>\r\n>    from tensorflow.contrib import image\r\n>  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/image/__init__.py\", line 40, in <module>\r\n>    from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms\r\n>  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py\", line 26, in <module>\r\n>    \"_single_image_random_dot_stereograms.so\"))\r\n>  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/util/loader.py\", line 55, in load_op_library\r\n>    ret = load_library.load_op_library(path)\r\n>  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/load_library.py\", line 84, in load_op_library\r\n>    exec(wrappers, module.__dict__)\r\n>  File \"<string>\", line 248, in <module>\r\n>  File \"<string>\", line 114, in _InitOpDefLibrary\r\n>  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py\", line 481, in Merge\r\n>    descriptor_pool=descriptor_pool)\r\n>  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py\", line 535, in MergeLines\r\n>    return parser.MergeLines(lines, message)\r\n>  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py\", line 568, in MergeLines\r\n>    self._ParseOrMerge(lines, message)\r\n>  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py\", line 583, in _ParseOrMerge\r\n>    self._MergeField(tokenizer, message)\r\n>  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py\", line 684, in _MergeField\r\n>    merger(tokenizer, message, field)\r\n>  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py\", line 773, in _MergeMessageField\r\n>    self._MergeField(tokenizer, sub_message)\r\n>  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py\", line 684, in _MergeField\r\n>    merger(tokenizer, message, field)\r\n>  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py\", line 773, in _MergeMessageField\r\n>    self._MergeField(tokenizer, sub_message)\r\n>  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py\", line 684, in _MergeField\r\n>    merger(tokenizer, message, field)\r\n>  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py\", line 773, in _MergeMessageField\r\n>    self._MergeField(tokenizer, sub_message)\r\n>  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py\", line 652, in _MergeField\r\n>    (message_descriptor.full_name, name))\r\n>google.protobuf.text_format.ParseError: 48:12 : Message type \"tensorflow.AttrValue\" has no field named \"5\".\r\n\r\nWhat causes this exception?\r\n--------------------------------------\r\n\r\nThe problem is the information extracted from the `_single_image_random_dot_stereograms.so` library file from `/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/image/python/ops/`\r\nThis file contains encoded information passed to protobuf.\r\n\r\nThe problem occurs when a line with a `float` default is parsed.\r\n\r\nIn this case, it is the sequence\r\n\r\n    eye_separation: float = 2.5\r\n\r\nat offset *0xa3b4* in `emphasized _single_image_random_dot_stereograms.so`\r\n\r\nSomehow, the parser replaces decimal points with commas. In the end, this is created:\r\n\r\n    attr {\\n'\r\n      name: \"eye_separation\"\\n'\r\n      type: \"float\"\\n'\r\n      default_value {\\n'\r\n        f: 2,5\\n'\r\n      }\\n'\r\n    }\\n'\r\n\r\nThe tokenizer (at `google/protobuf/text_format.py`) gets confused by the `,` in the default value and thinks that `5` is a separate field.\r\n\r\nRoot of the error\r\n---------------------\r\n\r\nThe error occurs during the execution of [load_library.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/load_library.py#L72).\r\n\r\n    op_list_str = py_tf.TF_GetOpList(lib_handle)\r\n    op_list = op_def_pb2.OpList()\r\n    op_list.ParseFromString(compat.as_bytes(op_list_str))\r\n    wrappers = py_tf.GetPythonWrappers(op_list_str)\r\n\r\n`op_list` contains the correct default value of `2.5`, whereas `wrappers`, the wrapped list returned from [python_op_gen.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/python_op_gen.cc#L762) contains a `2,5`.\r\n\r\nThis is what `GetPythonWrappers` does:\r\n\r\n    string GetPythonWrappers(const char* op_list_buf, size_t op_list_len) {\r\n      string op_list_str(op_list_buf, op_list_len);\r\n      OpList ops;\r\n      ops.ParseFromString(op_list_str);\r\n      return GetPythonOps(ops, {}, false);\r\n    }\r\n\r\nThe appended files include the contents of the `op_list` and the `wrappers` variable:\r\n[op_list.txt](https://github.com/tensorflow/tensorflow/files/1044775/op_list.txt)\r\n[wrapper.txt](https://github.com/tensorflow/tensorflow/files/1044777/wrapper.txt)\r\n\r\n", "comments": ["@josh11b has been doing some work on these pbtxt op definitions. Any idea why periods would get replaced with commas?", "I'm going to have to defer to @cwhipkey for this one.", "my guess is that a localization setting on the machine is set to use comma as the decimal separator.  We are probably using a locale-specific function to re-serialize the floating point value.\r\n\r\nIs that possibly what is happening on your machine?  If so, I can track down which proto-text serialization is doing this and try to fix it there.", "I think that is probably the cause - appending uses AppendNumeric in proto_text_util.h, which calls StrCat() on the value.  This ends up calling FloatToBuffer, which looks like it uses locale-specific snprintf for printing.", "@cwhipkey\r\nI can confirm that changing the decimal separator in my locale settings resolved the problem.", "Let me know if you think there is something that I can correct in  _single_image_random_dot_stereograms but it seems like this is outside of that Op at the moment.", "I got the same problem with import tensorflow.contrib.keras as K resulting in identical error message as being reported here. \r\nSystem and related details:\r\nLinux Mint 18.2; TF r1.2; Python 3.6\r\n\r\nAnd indeed changing the decimal separator made everything work as it supposed.\r\n1/ edit locale file located /usr/share/i18n/locales/\r\n ```\r\nLC_NUMERIC\r\ndecimal_point   \"<U002E>\"\r\nthousands_sep   \"<U002C>\"\r\ngrouping        3;3\r\nEND LC_NUMERIC\r\n```\r\n2/ sudo locale-gen\r\n3/ reboot computer and check:\r\n`locale -k LC_NUMERIC`\r\n\r\n", "Changing assignee for triaging.", "This seems like a real bug, we should be using a numeric serialization that will round trip independent of the locale settings.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@MaximilianKoestler I had a PR #22044 which might have fixed the issue you encounter. The PR has been merged into the master. I think this issue could be closed, but feel free to reopen if the issue still exists."]}, {"number": 10369, "title": "Deadlock in MapDataset", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n\r\n- **TensorFlow installed from (source or binary)**: source\r\n\r\n- **TensorFlow version (use command below)**:\r\n95d90ab2e0994127ffc42b80e16a3f532895cf6d\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.5.0\r\n\r\n- **CUDA/cuDNN version**:\r\nNone\r\n\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\npython map_dataset_op_test.py\r\n\r\n\r\n### Describe the problem\r\nThe process hangs forever\r\n\r\n### Source code / logs\r\nBelow is from map_dataset_op_test.py:\r\n\r\n\t\"\"\"Tests for the experimental input pipeline ops.\"\"\"\r\n\tfrom __future__ import absolute_import\r\n\tfrom __future__ import division\r\n\tfrom __future__ import print_function\r\n\r\n\timport numpy as np\r\n\r\n\tfrom tensorflow.contrib.data.python.ops import dataset_ops\r\n\tfrom tensorflow.python.framework import constant_op\r\n\tfrom tensorflow.python.framework import dtypes\r\n\tfrom tensorflow.python.framework import errors\r\n\tfrom tensorflow.python.ops import array_ops\r\n\tfrom tensorflow.python.ops import data_flow_ops\r\n\tfrom tensorflow.python.ops import lookup_ops\r\n\tfrom tensorflow.python.ops import math_ops\r\n\tfrom tensorflow.python.ops import random_ops\r\n\tfrom tensorflow.python.ops import string_ops\r\n\tfrom tensorflow.python.ops import variable_scope\r\n\tfrom tensorflow.python.platform import test\r\n\r\n\r\n\tclass MapDatasetTest(test.TestCase):\r\n\r\n\t  def _buildParallelMapDataset(self, components, count, num_threads,\r\n\t\t\t\t\t\t\t\t   output_buffer_size):\r\n\t\tdef _map_fn(x, y, z):\r\n\t\t  return math_ops.square(x), math_ops.square(y), math_ops.square(z)\r\n\t\treturn (dataset_ops.Dataset.from_tensor_slices(components).map(\r\n\t\t\t_map_fn, num_threads=num_threads, output_buffer_size=output_buffer_size)\r\n\t\t\t\t.repeat(count))\r\n\r\n\t  def testParallelMapDataset(self):\r\n\t\t\"\"\"Test an dataset that maps a TF function across its input elements.\"\"\"\r\n\t\t# The pipeline is TensorSliceDataset -> ParallelMapDataset(square_3) ->\r\n\t\t# RepeatDataset(count).\r\n\t\tcomponents = [np.arange(7),\r\n\t\t\t\t\t  np.array([[1, 2, 3]]) * np.arange(7)[:, np.newaxis],\r\n\t\t\t\t\t  np.array(37.0) * np.arange(7)]\r\n\t\tcount = array_ops.placeholder(dtypes.int64, shape=[])\r\n\t\tnum_threads = array_ops.placeholder(dtypes.int32, shape=[])\r\n\t\toutput_buffer_size = array_ops.placeholder(dtypes.int64, shape=[])\r\n\r\n\t\tdataset = self._buildParallelMapDataset(components, count, num_threads,\r\n\t\t\t\t\t\t\t\t\t\t\t\toutput_buffer_size)\r\n\t\titerator = dataset.make_initializable_iterator()\r\n\t\tinit_op = iterator.initializer\r\n\t\tget_next = iterator.get_next()\r\n\r\n\t\tself.assertEqual([c.shape[1:] for c in components],\r\n\t\t\t\t\t\t [t.shape for t in get_next])\r\n\r\n\t\twith self.test_session() as sess:\r\n\t\t  def do_test(num_threads_val, output_buffer_size_val):\r\n\t\t\t# Test single-threaded access to the iterator.\r\n\t\t\tsess.run(init_op, feed_dict={\r\n\t\t\t\tcount: 14,\r\n\t\t\t\tnum_threads: num_threads_val,\r\n\t\t\t\toutput_buffer_size: output_buffer_size_val})\r\n\t\t\tfor _ in range(14):\r\n\t\t\t  for i in range(7):\r\n\t\t\t\tresult = sess.run(get_next)\r\n\t\t\t\tfor component, result_component in zip(components, result):\r\n\t\t\t\t  self.assertAllEqual(component[i]**2, result_component)\r\n\t\t\twith self.assertRaises(errors.OutOfRangeError):\r\n\t\t\t  sess.run(get_next)\r\n\r\n\t\t\t# Test multi-threaded access to the same iterator.\r\n\t\t\tsess.run(init_op, feed_dict={\r\n\t\t\t\tcount: 18,\r\n\t\t\t\tnum_threads: num_threads_val,\r\n\t\t\t\toutput_buffer_size: output_buffer_size_val})\r\n\t\t\tresults = []\r\n\t\t\tdef iterator_thread():\r\n\t\t\t  while True:\r\n\t\t\t\ttry:\r\n\t\t\t\t  results.append(sess.run(get_next))\r\n\t\t\t\texcept errors.OutOfRangeError:\r\n\t\t\t\t  return\r\n\t\t\tthreads = [self.checkedThread(target=iterator_thread) for _ in range(8)]\r\n\t\t\tfor t in threads:\r\n\t\t\t  t.start()\r\n\t\t\tfor t in threads:\r\n\t\t\t  t.join()\r\n\r\n\t\t\t# `results` will contain the same elements components**2\r\n\t\t\t# repeated 18 times, but in a non-deterministic order. Sort the\r\n\t\t\t# results, and assert that each element of components**2 is\r\n\t\t\t# produced 18 times.\r\n\t\t\tresults.sort(key=lambda x: x[0])\r\n\t\t\tfor i in range(7):\r\n\t\t\t  for j in range(18):\r\n\t\t\t\tfor component, result_component in zip(components,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t   results[i * 18 + j]):\r\n\t\t\t\t  self.assertAllEqual(component[i]**2, result_component)\r\n\r\n\t\t  for num_threads_val, output_buffer_size_val in [\r\n\t\t\t  (1, 1), (1, 2), (2, 2), (2, 4), (8, 8), (8, 16)]:\r\n\t\t\tdo_test(num_threads_val, output_buffer_size_val)\r\n\r\n\tif __name__ == \"__main__\":\r\n\t  test.main()\r\n\r\n[bt.txt](https://github.com/tensorflow/tensorflow/files/1044353/s.txt)\r\n\r\n\t\r\n", "comments": ["\t(gdb) thr 14\r\n\t(gdb) f2\r\n\t(gdb) p *mutex\r\n\t$2 = {__data = {__lock = 2, __count = 0, __owner = 28288, __nusers = 1, __kind = 0, __spins = 0, __elision = 0,\r\n\t\t__list = {__prev = 0x0, __next = 0x0}},\r\n\r\nThread 12 is waiting on a mutex, which is owned by LWP 28288. (Thread 12),  Thread 12 is waiting the output_buffer_ to be non-empty. \r\n\r\n\r\n", "@snnn : Could you reduce your sampler to something simpler? (Among other things, seems like you're running multiple groups of threads, if you can reduce it to the smallest example that demonstrates the failure, that will be helpful)\r\n\r\nCC @mrry ", "This is definitely a real bug, which I suspect arises because you have 8 or fewer cores on the machine where you're running the test. The issue is that the current implementation of the `IteratorGetNext` op is a synchronous `OpKernel` but it can block an inter-op threadpool thread, and the unblocking action may require the use of another inter-op threadpool thread. The default threadpool size is the number of cores in your machine.\r\n\r\nI'm working on a fix, but there are two short-term workarounds:\r\n* Increase the size of the inter-op threadpool when you create the session using `tf.ConfigProto`. Setting it to (maximum number of concurrent `get_next()` ops) + 1 (i.e. 9 in this case) should address the deadlock. \r\n* Reduce the number of concurrent `get_next()` calls to (number of cores) - 1.\r\n\r\nThe true fix will involve rewriting `IteratorGetNext` as an `AsyncOpKernel`, which I'm working on now...."]}, {"number": 10368, "title": "Fixes of AutoParallel bug", "body": "This PR fixes the bug that AutoParallel sets variables as don't_replicate_nodes, but not for variable initializers and snapshot nodes. This can break variable collections in meta graph because variable initializers and snapshot nodes could be replicated with different names.\r\n\r\nThis issue is already discussed in #9906.", "comments": ["Can one of the admins verify this patch?", "Benoit, are you reviewing this?", "Thanks Martin. I'm taking a look.", "@sj6077, Thanks for the contribution! I'm not sure how updating initializers and snapshot nodes inside C++ auto_parallel.cc would help, as the C++ code doesn't return the modified metagraph back. Currently they have to be on the python side, for example, like [this](https://gist.github.com/zhangyaobit/08c0b776e4b7a4f786714d46e723ee8a).\r\n\r\nWe do plan to come up with a better solution that avoids importing and exporting graph and thus directly uses session.run with ConfigProto to specify the optimizers (currently this is not possible because metagraph is not passed to the optimizer, that's why the ptb code has to use the tf_optimizer.OptimizeGraph interface). \r\n\r\nHowever, once we solved the problem of passing metagraph to the optimizer, we don't need to use import/export or tf_optimizer any more and I think it may make sense to update initializers and snapshot nodes inside C++ auto_parallel.cc and we can revisit this issue.\r\n\r\nI don't know about your use case. Could you provide some skeleton code/usage pattern to show how this change would help? Thanks.", "```\r\nclass UpdateCollection(object):\r\n  \"\"\"Update collection info in MetaGraphDef for AutoParallel optimizer.\"\"\"\r\n\r\n  def __init__(self, metagraph, model):\r\n    self._metagraph = metagraph\r\n    self.replicate_states(model.initial_state_name)\r\n    self.replicate_states(model.final_state_name)\r\n    self.update_snapshot_name(\"variables\")\r\n    self.update_snapshot_name(\"trainable_variables\")\r\n\r\n  def update_snapshot_name(self, var_coll_name):\r\n    var_list = self._metagraph.collection_def[var_coll_name]\r\n    for i, value in enumerate(var_list.bytes_list.value):\r\n      var_def = variable_pb2.VariableDef()\r\n      var_def.ParseFromString(value)\r\n      # Somehow node Model/global_step/read doesn't have any fanout and seems to\r\n      # be only used for snapshot; this is different from all other variables.\r\n      if var_def.snapshot_name != \"Model/global_step/read:0\":\r\n        var_def.snapshot_name = with_autoparallel_prefix(\r\n            0, var_def.snapshot_name)\r\n      value = var_def.SerializeToString()\r\n      var_list.bytes_list.value[i] = value\r\n\r\n  def replicate_states(self, state_coll_name):\r\n    state_list = self._metagraph.collection_def[state_coll_name]\r\n    num_states = len(state_list.node_list.value)\r\n    for replica_id in range(1, FLAGS.num_gpus):\r\n      for i in range(num_states):\r\n        state_list.node_list.value.append(state_list.node_list.value[i])\r\n    for replica_id in range(FLAGS.num_gpus):\r\n      for i in range(num_states):\r\n        index = replica_id * num_states + i\r\n        state_list.node_list.value[index] = with_autoparallel_prefix(\r\n            replica_id, state_list.node_list.value[index])\r\n```\r\nAccording to the example that you mentioned, there is `update_snapshot_name` for metagraph collection. This is because variable, variable_snapshot, variable_initializer are all together defined for a variable, but snapshot and initializer are differently managed in auto_parallel. With this PR, the update_snapshot_name could be removed.", "@zhangyaobit I addressed your comments, thank you for the review.", "Jenkins, test this please.", "@benoitsteiner I addressed your comments, I didn't know about the usage of snapshot nodes.", "Jenkins, please test this.", "Jenkins, test this please."]}, {"number": 10367, "title": "Error building TensorFlow from source", "body": "### System information\r\n\r\n- No custom code.\r\n- Ubuntu 16.04.\r\n- Trying to compile Tensorflow r1.1 from source.\r\n- Bazel 0.5.0\r\n- CUDA 8.0, cuDNN 5\r\n- K80 GPU (12GB memory)\r\n\r\n### Describe the problem\r\n\r\nI'm trying to build TensorFlow from source, with CUDA support. My CUDA and cuDNN are installed successfully, as I can run a TensorFlow program fine (with pre-built TensorFlow GPU image).\r\n\r\nThe command I used was:\r\n\r\n```sh\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-mavx --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both\r\n```\r\n\r\nI specified CUDA 8.0 and cuDNN 5 explicitly in `./configure` before running the above. The error is the following:\r\n\r\n```sh\r\nERROR: /home/danqing/.cache/bazel/_bazel_danqing/f5e3ebf5f39671e6e38e6235913f00e0/external/protobuf/BUILD:241:1: C++ compilation of rule '@protobuf//:js_embed' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /home/danqing/.cache/bazel/_bazel_danqing/f5e3ebf5f39671e6e38e6235913f00e0/execroot/tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64 \\\r\n    PATH='/home/danqing/.pyenv/plugins/pyenv-virtualenv/shims:/home/danqing/.pyenv/shims:~/.pyenv/bin:/home/danqing/bin:/home/danqing/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda/bin' \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -g0 -MD -MF bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.d '-frandom-seed=bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o' -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c external/protobuf/src/google/protobuf/compiler/js/embed.cc -o bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2.\r\npython: can't open file 'external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc': [Errno 2] No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nHowever, the file actually exists:\r\n\r\n```sh\r\nsudo find / -name 'crosstool_wrapper_driver_is_not_gcc'\r\n/home/danqing/.cache/bazel/_bazel_danqing/f5e3ebf5f39671e6e38e6235913f00e0/external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\r\n```\r\n\r\n(Also exists in `bazel-tensorflow/external/local_config_cuda/crosstool`)\r\n\r\nThanks!\r\n", "comments": ["Sorry you're running into this. Some questions:\r\n\r\n- Could you include the output of your run of the `configure` script?\r\n- Does the build work with a simpler setup (CPU-only, no-CUDA, reduced custom flags etc.)?", "I got it working.\r\n\r\nI installed conda and used the Python there, and rebuilt TensorFlow with success.\r\n\r\nThanks!", "If anyone is having this issue, I solved this issue by making sure that shell always points to system python binary. \r\nSome python utility tools such like pyenv causes confusion to bazel (dont' know how) and causes this particular error. "]}, {"number": 10366, "title": "Fixes issue #10258", "body": "On CUDA versions previous to 8.0, only __shared__ variables could be declared as static in the device code.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please", "Jenkins, test this please", "https://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/5266/\r\nCurrent failure seems to be a flake.\r\nmerging."]}, {"number": 10365, "title": "Fixed typos in documentation & READMEs", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 10364, "title": "worker.pb.h file not found", "body": "I am trying to find the file worker.ph.h file which is included in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc\r\n\r\nCan I know if need to run any command like below one to generate the file\r\n**bazel build tensorflow/cc:cc_ops** as indicated in https://github.com/tensorflow/tensorflow/issues/3017 to generate the respective ops files\r\n", "comments": ["pb.h files are generated from protocol buffers.\r\nBazel creates these once you have the dependencies set up correctly.\r\nIn this case, `\"//tensorflow/core:worker_proto_cc\",` dependency is the correct proto dependency we use.", "Yeah when you bazel build it, you can find it by fishing around in the bazel-bin/ or bazel-genfiles/ directory.", "I ran the command \"//tensorflow/core:worker_proto_cc\" but it did not generate worker.pb.h file\r\n\r\nIs there any command to build all core files at a time, instead of building\r\neach such files as when I build tensorflow using bazel, the bazel-genfiles folder is empty\r\n\r\nOn Jun 2, 2017 5:46 AM, \"Justine Tunney\" <notifications@github.com> wrote:\r\n\r\n> Yeah when you bazel build it, you can find it by fishing around in the\r\n> bazel-bin/ or bazel-genfiles/ directory.\r\n>\r\n> \u2014\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/10364#issuecomment-305654551>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AJjYUL_EpBQmVMRk_vnZB65jsQxY0vN0ks5r_1RAgaJpZM4NspUI>\r\n> .\r\n>\r\n", "`bazel build //tensorflow/core/...`"]}, {"number": 10363, "title": "TensorBoard: Graph Visualization failed Runtime statistics", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 8 (jessie)\r\n- **TensorFlow installed from (source or binary)**: using virtualenv, pip install\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 7.5, cuDNN 4.0\r\n- **GPU model and memory**: GeForce GTX TITAN X 12GB\r\n- **Exact command to reproduce**: Run this https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py scripts.\r\n\r\n### Describe the problem\r\nI am following the tutorials in https://www.tensorflow.org/get_started/graph_viz, downloading and running the mnist_with_summaries.py scripts.\r\n\r\nAll the Tensorboard visualization works, except the Runtime statistics. In the GRAPHS tab, the Session runs shows (0) items available. And it says 'None'. ", "comments": ["That is a duplicate of https://github.com/tensorflow/tensorflow/issues/10317, albeit with a different configuration.", "Closing in favor of the other issue. Thanks for the report."]}, {"number": 10362, "title": "dataset.output_shapes returns demension(none) after batching", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.2.0-rc0\r\n- **CUDA/cuDNN version**:8.0\r\n- **GPU model and memory**:GTX 1080 8G\r\n\r\n### Describe the problem\r\nI'm using the Dataset API for input pipelines in tensorflow r1.2\r\nI build my own dataset and batch it with batch size = 128 and then input it into RNN.\r\nbut the dataset.output_shape returns dimension(none) in the first dimension, so the RNN raises a error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"untitled1.py\", line 188, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/harold/anaconda2/envs/tensorflow_py2.7/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"untitled1.py\", line 121, in main\r\n    run_training()\r\n  File \"untitled1.py\", line 57, in run_training\r\n    is_training=True)\r\n  File \"/home/harold/huawei/ConvLSTM/ConvLSTM.py\", line 216, in inference\r\n    initial_state=initial_state)\r\n  File \"/home/harold/anaconda2/envs/tensorflow_py2.7/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 566, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/home/harold/anaconda2/envs/tensorflow_py2.7/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 636, in _dynamic_rnn_loop\r\n    \"Input size (depth of inputs) must be accessible via shape inference,\"\r\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.\r\n```\r\nI think this error is caused by the shape of input, the first dimension should be batch size but not none.\r\n\r\n### Source code / logs\r\n```\r\norigin_dataset = Dataset.BetweenS_Dataset(FLAGS.data_path)\r\ntrain_dataset = origin_dataset.train_dataset\r\ntest_dataset = origin_dataset.test_dataset\r\nshuffle_train_dataset = train_dataset.shuffle(buffer_size=10000)\r\nshuffle_batch_train_dataset = shuffle_train_dataset.batch(128)\r\nbatch_test_dataset = test_dataset.batch(FLAGS.batch_size)\r\n\r\niterator = tf.contrib.data.Iterator.from_structure(\r\n                           shuffle_batch_train_dataset.output_types,\r\n                            shuffle_batch_train_dataset.output_shapes)\r\n(images, labels) = iterator.get_next()\r\n\r\ntraining_init_op = iterator.make_initializer(shuffle_batch_train_dataset)\r\ntest_init_op = iterator.make_initializer(batch_test_dataset)\r\n\r\nprint(shuffle_batch_test_dataset.output_shapes)\r\n```\r\nI print output_shapes and it gives:\r\n`(TensorShape([Dimension(None), Dimension(36), Dimension(100)]), TensorShape([Dimension(None)]))`\r\n\r\nI suppose that it should be 128, because I have batched dataset:\r\n`(TensorShape([Dimension(128), Dimension(36), Dimension(100)]), TensorShape([Dimension(128)]))`", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10361, "title": "Excuse me,I know this is a basic problem, but I don't know how to solve, time is tight, trouble to give some guidance, how to find the lost files, such as libprotobuf - lite. a, libprotobuf. a?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.\r\n\r\nThough, based on the title, it seems that this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Excuse me, the Header Search Paths set up relative path, so packaged into the SDK when others use path also want this configuration?\r\n\r\n![067d6de4-98e5-43f8-a59d-16ab194903d5](https://cloud.githubusercontent.com/assets/8908244/26709012/2f1eb5b8-4782-11e7-8bb5-da972190107f.png)\r\n"]}, {"number": 10360, "title": "How to compile a 32-bit shared library with bazel on a 64-bit machine ?", "body": "", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10359, "title": "Branch 155393864", "body": "Pushing internal commits", "comments": ["@tensorflow-jenkins Test this, please.", "@tensorflow-jenkins Test this, please.", "@tfboyd @gunan Any idea why user \"unknown\" keeps aborting the tests?\r\n\r\n```\r\nBuild was aborted\r\nAborted by unknown\r\n[Set GitHub commit status (universal)] PENDING on repos [] (sha:8a095d9) with context:tensorflow-pull-requests-cpu-python3\r\nUnable to get pull request builder trigger!!\r\nSetting status of b0f739da665b0491920667a7d1e2bcc2fd721f0d to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/4716/ and message: 'FAILURE\r\n '\r\nUsing context: Linux CPU Tests (Python 3)\r\nFinished: ABORTED\r\n```\r\n\r\nAnd from the ci.tensorflow.org test:\r\n```\r\nFirst time build. Skipping changelog.\r\n    >> Job status: [tensorflow-pull-requests-sanity] the 'build only if scm changes' feature is disabled.\r\n```", "@tensorflow-jenkins Test this, please.", "@tensorflow-jenkins Test this, please.", "@gunan Ok to ignore the //tensorflow/python/kernel_tests:stage_op_test  timeout?", "@tensorflow-jenkins Test this, please.", "Yes, I will take care of that timeout separately."]}, {"number": 10358, "title": "Branch 155393864", "body": "", "comments": []}, {"number": 10357, "title": "TensorFlow build: protobuf/pyext/_message.so' failed", "body": "I'm trying to build TensorFlow with Bazel on Windows.\r\n\r\nBut i'm getting this error:\r\n\r\n    ERROR: C:/users/spenh/appdata/local/temp/_bazel_spen/icnq02mb/external/protobuf/BUILD:623:1: \r\n    C++ compilation of rule '@protobuf//:python/google/protobuf/pyext/_message.so' failed: \r\n    msvc_cl.bat failed: error executing command \r\n    external/local_config_cc/wrapper/bin/msvc_cl.bat /DOS_WINDOWS=OS_WINDOWS /DCOMPILER_MSVC /DNOGDI /DNOMINMAX /DPRAGMA_SUPPORTED /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS ... (remaining 42 argument(s) skipped): \r\n    com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2.\r\n    ...\r\n    C:\\users\\spenh\\appdata\\local\\temp\\_bazel_spen\\icnq02mb\\execroot\\org_tensorflow\\external\\protobuf\\python\\google\\protobuf\\pyext\\repeated_scalar_container.cc(307): fatal error C1057: Unerwartetes Dateiende bei der Erweiterung eines Makros\r\n    C:\\users\\spenh\\appdata\\local\\temp\\_bazel_spen\\icnq02mb\\execroot\\org_tensorflow\\external\\protobuf\\python\\google\\protobuf\\pyext\\repeated_scalar_container.cc(307): fatal error C1057: Unerwartetes Dateiende bei der Erweiterung eines Makros\r\n        \r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n\r\n\r\nAny idea what i can try to solve this? Or what information i could provide you to solve this?\r\n\r\n - Bazel: git/master (30.05.2017)\r\n - TensorFlow: git/master (30.05.2017) \r\n - Python 3.6.1 x64\r\n - Visual Studio 2017 v15.0.26430.6 x64\r\n - Cuda: 8.0\r\n - Java JDK 1.8.0_60\r\n - Windows 10\r\n - Nvidia 1080\r\n\r\nBuild command:\r\n\r\n    bazel build -c opt --config=win-cuda --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=-w --host_copt=-w tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\nStackoverflow link: https://stackoverflow.com/questions/44264801/tensorflow-bazel-build-protobuf-pyext-message-so-failed", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\n(As per https://www.tensorflow.org/install/install_sources - building from source on Windows isn't supported yet, in that we don't have the bandwidth to debug various configurations. You might want to try the CMake build https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake)\r\n", "@asimshankar \r\nWhy is this not a bug? How do you know?", "@Spenhouet :  You snippet suggest that it is failing to build the protocol buffer library (not TensorFlow code) when using a particular version of MSVC.\r\n\r\nWhat I meant to convey in the previous comment is that this is not a configuration that we have the bandwidth to support or debug. Figuring this out would require some experience/expertise with bazel-on-windows-building-protobuf-using-MSVC. You're likely to get more help on this from StackOverflow where there is a larger community.\r\n\r\nWe try to keep the github issues focused on bugs (on supported platforms) or feature requests.\r\n\r\nHope that makes sense. Thanks!\r\n", "@asimshankar I can't find my self to resolve this issue. I thought about trying CMake but on the GitHub page it says \r\n\r\n> It is not possible to load a custom Op library.\r\n\r\nWhat i want to do, and why i think i need to build tensorflow in the first place, is to add a new optimization algorithm. So if i understand correct i don't even need to try to build with CMake because it wouldn't allow me to add any optimizer?", "@Spenhouet Well your not alone atleast. I've built tensorflow HEAD, r1.1 and r1.0 with Visual Studio 2015 and 2017. Also tried with Bazel 0.4.5, 0.5.0, 0.5.1 and all of them fail. Tried it on Windows 7 and Windows 10.", "Me too.\r\n\r\nC:\\tmp\\_bazel_chasun\\5dtkiy-n\\execroot\\tensorflow\\external\\protobuf\\python\\google\\protobuf\\pyext\\repeated_scalar_container.cc(307): fatal error C1057: unexpected end of file in macro expansion\r\n\r\n\r\nHi @Spenhouet , the document is outdated.  You can load a custom op library dll or a filesystem dll on Windows. It's doable. "]}, {"number": 10356, "title": "Python 3.6 support on windows.", "body": "Fixes #6999 ", "comments": []}, {"number": 10355, "title": "confusing warning of rank of input Tensor should be the same as output_rank ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.2.0-rc0 \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8.0\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\ncod (mostly from [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/input_fn/boston.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/input_fn/boston.py))\r\n```\r\nfrom tensorflow.contrib.learn.python.learn.learn_io import pandas_io\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport pdb\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\nCOLUMNS = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\", \"age\",\r\n           \"dis\", \"tax\", \"ptratio\", \"medv\"]\r\nFEATURES = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\",\r\n            \"age\", \"dis\", \"tax\", \"ptratio\"]\r\nLABEL = \"medv\"\r\n\r\ntraining_set = pd.read_csv(\"boston_train.csv\", skipinitialspace=True,\r\n                           skiprows=1, names=COLUMNS)\r\n# test_set = pd.read_csv(\"boston_test.csv\", skipinitialspace=True,\r\n#                        skiprows=1, names=COLUMNS)\r\n# prediction_set = pd.read_csv(\"boston_predict.csv\", skipinitialspace=True,\r\n#                              skiprows=1, names=COLUMNS)\r\n\r\nfeature_cols = [tf.contrib.layers.real_valued_column(k) for k in FEATURES]\r\n\r\nregressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\r\n                                          hidden_units=[10, 10],\r\n                                          model_dir=\"/tmp/boston_model\")\r\n\r\npd_input_fn = pandas_io.pandas_input_fn(training_set[FEATURES], y=training_set[LABEL],\r\n                                        batch_size=128,target_column='medv')\r\n\r\nregressor.fit(input_fn=pd_input_fn, steps=5000)\r\n```\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. \r\n\r\nI saw a stakoervlow question [here](https://stackoverflow.com/questions/41273182/tensorflow-0-12-tutorials-produce-warning-rank-of-input-tensor-should-be-the-s), but I don't think that applies to me. \r\n\r\nI have the following confusing warnings.  As a result, I am not sure that everything is right. \r\n```\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWA\r\n```\r\n\r\n### Source code / logs\r\nFull output:\r\n\r\n```\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5a3710eba8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 1.0\r\n}\r\n, '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/boston_model'}\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From /home/jiqiang/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Restoring parameters from /tmp/boston_model/model.ckpt-16\r\nINFO:tensorflow:Saving checkpoints for 17 into /tmp/boston_model/model.ckpt.\r\nINFO:tensorflow:loss = 115.183, step = 17\r\nINFO:tensorflow:Saving checkpoints for 20 into /tmp/boston_model/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 161.23.\r\n\r\n```\r\n\r\n", "comments": ["@maverickg : Thanks for the report. The good news is that you can ignore the WARNING message, it is advisory - I don't think there is anything wrong with your setup.\r\n\r\n@ispirmustafa : Would you mind taking a look? I think this is coming from https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/layers/python/layers/feature_column.py#L1582 - suggesting that something in feature_column.py needs to change to address the concerns expressed by the warning log?\r\n", "We graduated FeatureColumns from contrib. We fixed this warning in the new ones.\r\nYou can use tf.feature_column.*."]}, {"number": 10354, "title": "Fixes and improvements to cmake windows build.", "body": "", "comments": []}, {"number": 10353, "title": "Fixed formatting in Linux install guide", "body": "Formatting issues were introduced in PR #8825, commit f30918b3694afe844990cbddc82e27e023d88856 ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 10351, "title": "Improve docs for selective registration headers", "body": "progressing #10299 @andrewharp ", "comments": ["Can one of the admins verify this patch?", "Regarding why there are two macros, some of it is historical:\r\n- originally, on mobile platforms, a reduced set of types were supported (see register_types.h, for example).\r\n- then we added selective registration support via ops_to_register.h\r\n- but then, when using selective registration, you might as well include all kernel types, since unused kernels are stripped.  So SUPPORT_SELECTIVE_REGISTRATION was added.\r\n\r\nProbably we could make SELECTIVE_REGISTRATION imply SUPPORT_SELECTIVE_REGISTRATION...\r\n\r\nWhen building the binary in pieces (like bazel would), it may be useful to have SUPPORT_SELECTIVE_REGISTRATION available as a standalone option - so that the framework+core kernels can all be built with that macro, without requiring passing SELECTIVE_REGISTRATION+including a header.  Although that can probably be eliminated as well, and just use SELECTIVE_REGISTRATION there.", "Jenkins, test this please", "Jenkins, test this please.", "Is SUPPORT_SELECTIVE_REGISTRATION supposed to make all ops available in the build, or just all the types needed to support all ops?  When using SELECTIVE_REGISTRATION and a model that includes ops not built by default, I still have to modify tensorflow/core/kernels/BUILD to add files under android_extended_ops_group1.  I would have expected SUPPORT_SELECTIVE_REGISTRATION to eliminate the need to edit any BUILD files in order to include an op such as Sin or Cos.", "@nimatter Did you create and include `ops_to_register.h`?", "@Androbin yes, I created and included `ops_to_register.h`.  For my procedure, please see my response [here](https://stackoverflow.com/questions/51105847/how-to-include-contrib-op-in-tensorflow-bazel-build/52170008#52170008) on StackOverflow.  I was searching github one more time while considering submitting an issue.", "AFAIK, you need to pass both `-DSELECTIVE_REGISTRATION` and `-DSUPPORT_SELECTIVE_REGISTRATION` (for historical reasons).", "I tried using both `-DSELECTIVE_REGISTRATION` and `-DSUPPORT_SELECTIVE_REGISTRATION` and removing edits in android_extended_ops_group1 (\"cwise_op_asin.cc\") and it builds but I get the following on Android:\r\n\r\njava.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Asin' with these attrs\r\n\r\nThis goes away when I add cwise_op_asin.cc to android_extended_ops_group1.", "Would you mind sharing (a minimal version of) your graph in `.pbtxt` format (via Pastebin)?\r\nWhat transformations do you apply to the graph? (Freezing, Quantizing, Stripping)\r\nWhere do you use `tf.asin`? (sometimes there are issues towards the end of the graph, or with certain ops like `tf.case` or `tf.identity`)"]}, {"number": 10350, "title": "Build Android demo, ambiguous python reference", "body": "Hi, while I was building the android demo with the command `bazel build -c opt //tensorflow/examples/android:tensorflow_demo --verbose_failures`,  it throw the error: \r\n\r\n```\r\nERROR: /home/damian/ai/tensorflow/tensorflow/examples/android/BUILD:63:1: Extracting Java resources from deploy jar for split Java resource apk failed: resource_extractor failed: error executing command \r\n  (cd /home/damian/.cache/bazel/_bazel_damian/80fc8f2224411108cd5fffb32690db73/execroot/tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/bazel_tools/tools/android/resource_extractor bazel-out/local-opt/bin/tensorflow/examples/android/tensorflow_demo_deploy.jar bazel-out/local-opt/bin/tensorflow/examples/android/_dx/tensorflow_demo/extracted_tensorflow_demo_deploy.jar): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n  File \"/home/damian/.cache/bazel/_bazel_damian/80fc8f2224411108cd5fffb32690db73/execroot/tensorflow/bazel-out/host/bin/external/bazel_tools/tools/android/resource_extractor.runfiles/org_tensorflow/../bazel_tools/tools/android/resource_extractor.py\", line 98\r\n    print USAGE\r\n              ^\r\nSyntaxError: Missing parentheses in call to print\r\n\r\n```\r\n\r\nI debuged the error and the cause is in the file `bazel-out/host/bin/external/bazel_tools/tools/android/resource_extractor` around the line 35:  \r\n\r\n```\r\nPYTHON_BINARY = 'python'\r\nif IsWindows() and not PYTHON_BINARY.endswith('.exe'):\r\n  PYTHON_BINARY = PYTHON_BINARY + '.exe'\r\n\r\n# Find a file in a given search path.\r\ndef SearchPath(name):\r\n  search_path = os.getenv('PATH', os.defpath).split(os.pathsep)\r\n  for directory in search_path:\r\n    if directory == '': continue\r\n    path = os.path.join(directory, name)\r\n    if os.path.isfile(path) and os.access(path, os.X_OK):\r\n      return path\r\n  return None\r\n\r\ndef IsRunningFromZip():\r\n  return False\r\n\r\n# Find the real Python binary if it's not a normal absolute path\r\ndef FindPythonBinary():\r\n  if PYTHON_BINARY.startswith('//'):\r\n    # Case 1: Path is a label. Not supported yet.\r\n    raise AssertionError(\r\n      'Bazel does not support execution of Python interpreters via labels yet')\r\n  elif PYTHON_BINARY.startswith('/'):\r\n    # Case 2: Absolute path.\r\n    return PYTHON_BINARY\r\n  elif '/' in PYTHON_BINARY:\r\n    # Case 3: Path is relative to current working directory.\r\n    return os.path.join(os.getcwd(), PYTHON_BINARY)\r\n  else:\r\n    # Case 4: Path has to be looked up in the search path.\r\n    return SearchPath(PYTHON_BINARY)\r\n\r\n```\r\n\r\nthe script  need and assumes the default /usr/bin/python is linked to python 2.x wich in my case is not,\r\nMy current work around is a simple `ln -sf /usr/bin/python2 /usr/bin/python`.\r\n\r\nI would like to patch but I dont have idea where is the file `bazel-out/host/bin/external/bazel_tools/tools/android/resource_extractor` because is an autogen script, I tried look for it in the bazel repo but I can't find it ", "comments": ["@petewarden, who would be good to take a look?", "@osdamv The simplest fix would seem to be to install Python 2.X if possible.\r\n\r\nClosing however as this doesn't appear to be an actual issue with TensorFlow. You would probably be better off making a Bazel GitHub issue or asking on StackOverflow.", "Hello! I meet the same problem as you. Have you solved the problem? Can you tell me what i should do? Hope your answer as soon as possible. Thank you very much!"]}, {"number": 10349, "title": "updated TensorFlow official example: cifar10", "body": "I found a problem in the official example: cifar10\r\n\r\nIt not working in the newest TensorFlow because of API change.\r\n\r\nBut it's an excellent tutorials for newcomer.\r\n\r\nSo i updated it and hope more people could see it.\r\n\r\nExcept API changes,I didn't do any other change.\r\n\r\nI hope this tutorials can add to official example again.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Please check this~", "The files are all in a cfair10/ directory, instead of cifar10/, can you check?", "Sorry~\r\nnow moved", "Can you update it in the models repo instead?\r\nhttps://github.com/tensorflow/models/tree/master/tutorials/image/cifar10"]}, {"number": 10348, "title": "Android Detect App does not draw overlay correctly.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: [No] Further troubleshooting shows a minor change may cause this.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android 7.1\r\n- **TensorFlow installed from (source or binary)**: N/A (Nightly apk)\r\n- **TensorFlow version (use command below)**: 1.1,1.2\r\n- **Phone Hardware**: Pixel C Tablet\r\n- **Exact command to reproduce**: Use the Detect App and move your target off centre\r\n\r\n### Describe the problem\r\nDue to how the Detect act appears to process it's imagery and camera fragments etc it means that the overlay that's drawn drifts from the target when off centre. (Will try to post pics when I can.)\r\n\r\nPart of this issue appears to be the fact that camera preview shown in the app is oddly cropped and squished from what the app actually sees (from debug view). Because of this, when the overlays are drawn they do not match the camera feed below. (Noticeable when your target object is towards the edge of the input)\r\n\r\n### Source code / logs\r\nTo reproduce, install the app, load up an image of a human on google images and move your camera so it's on the edge. It doesn't line up with the camera preview. Moving this from side to side helps show the problem.\r\n\r\nI have spent many hours at work trying to understand why this is. Unfortunately I find the source code very difficult to work with and understand how it at all works. I've had to start again but I came across this bug.\r\n\r\nThanks", "comments": ["@jubjamie Some screenshots would be very helpful as I don't have a device on hand to verify this with. It's possible this is a bug in how those devices report the camera or display orientation and the code mixes up width and height, or something of that nature.\r\n\r\nWhen you turn on debug visualizations (using the volume keys), what do you see in the thumbnail view? It should be rendering static detections there.", "@andrewharp Curiously i'm not getting the same behaviour from my 6P today. However the Pixel C is giving me trouble. It looks like it's to do with how the camera feed is displayed as it is being cropped and squished but the overlay goes on as if it hasn't it.\r\nUsing the debug view is fine and shows that the camera preview feed is being cropped as I can see much more in the debug view than the main view.\r\n\r\nI also have a feeling that this cropping may happen because I have to force the rotation on the Pixel C as it reports the rotation to be 3 deg. I have to manually set sensorOrientation to 90 for it to capture images the correct way up.\r\nScreenshots:\r\n1) The target centred.\r\n![screenshot_20170601-102936](https://cloud.githubusercontent.com/assets/25011496/26674284/de942f76-46b7-11e7-8e90-2eaa6cd583a5.png)\r\n2) The target off-centre. See how the overlay \"drifts\". It looks like it's because the image preview camera feed has been cropped and stretched past the screen.\r\n![screenshot_20170601-102952](https://cloud.githubusercontent.com/assets/25011496/26674287/e2561566-46b7-11e7-8703-026001e8367f.png)\r\n3) With debug on this time.\r\n![screenshot_20170601-103004](https://cloud.githubusercontent.com/assets/25011496/26674291/e56d9116-46b7-11e7-882b-6b7ac044cdb2.png)\r\n4) Notice how in the debug window we see that there is some grass to the left of the target. However in our preview window we see that the image is cut of right up to the target. However, we can see that the yellow overlay looks likes it's in the right position if we were given the debug view.\r\n![screenshot_20170601-103018](https://cloud.githubusercontent.com/assets/25011496/26674297/eaaa2d10-46b7-11e7-9374-01502d8a7755.png)\r\n\r\nI believe that there is a problem with how the camera view is created and that it's partly to do with the screenOrientation. For some reason the Pixel C will report a rotation of 3 (in the debug view) and have the image rotated by 90 (or 87) degrees when using the default app. The only solution I have found to get the app to \"see\" pictures in the correct orientation is to change sensorOrientation to this:\r\n\r\n`sensorOrientation = 90;`\r\n\r\nInstead of the default found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L159).\r\n\r\nWhile this appears to be partly caused by the hardware (we have tried on two different Pixel C's) there is also some kind of bug with how the camera feed is drawn. I'm really struggling to understand how it's done in the code so hopefully you can help.\r\n\r\nThanks", "Assigning to @andrewharp, to remove from triage queue.", "@jubjamie Thanks for the details. Sounds like a bug in the device's camera implementation. 3 is obviously an invalid value for [SENSOR_ORIENTATION](https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#SENSOR_ORIENTATION), so maybe it makes sense to default to the most common setting of 90 when the value % 90 != 0?\r\n\r\nHowever given the undesired squashing and stretching of the image it might be best just to force the pixel C to use the camera 1 API. See #8736 for a partially implemented PR -- if you patch that in, does it resolve the issue?", "@andrewharp Not sure i'll get a chance to look at using camera 1 today but reading through that PR it looks like it is incomplete? Do you have any idea if it's at a usable point yet? I'm not a big java person so after going through the commit I couldn't work out if it was suitable yet. Perhaps a ping for @BrianOn99 or @osdamv ?? \r\nOne concern being that from the comments it appears to remove the red debug boxes to show the results and without that i'm unable to continue troubleshooting this issue properly.", "At the moment the stylized demo and classifier demo are working with the api 1, I am working with the detector right now ", "@jubjamie: Is this issue still occurring after #8736?", "tbh I have no idea. I'm not going to have a chance to look at this now. I think that the fix doesn't apply to the detector app which is the one causing the issue. I've seen a similar issue on here somewhere but I can't remember where. Unless something has changed then I don't think it fixed the issue. To replicate, use a Pixel C and rotate the app to landscape and try to use.", "I took a closer look and my initial interpretation was a bit off; I was thinking that `sensorOrientation` referred to camera orientation only when in fact it's a computed variable of both sensor and display rotation relative to device.\r\n\r\nThe \"invalid\" 3 value was actually coming from the `getWindowManager().getDefaultDisplay().getRotation()` call which oddly enough returns an integer constant 1-3 rather than the rotation as one might expect. In this case the actual rotation is encoded in the variable name: e.g. android.view.Surface.ROTATION_180 = 2.\r\n\r\nAccounting for that and doing a bit of variable swapping for height/width coordinate scaling fixed the issue and now the detector demo works in both landscape and portrait mode (as soon as #14236 goes in)."]}, {"number": 10347, "title": "[OpenCL] Cleans reshape.cc", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}]