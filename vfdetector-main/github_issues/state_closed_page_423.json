[{"number": 41201, "title": "[-Wsign-compare] warning fixes batch 14", "body": "@mihaimaruseac ", "comments": ["@mihaimaruseac ", "@mihaimaruseac  `int64` changed to `int64_t.", "Closing as it has been handled by new PRs."]}, {"number": 41200, "title": "Usage model's intermediate layer output in custom loss causes OOM", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2 ('tf.version.GIT_VERSION, tf.version.VERSION' output: v2.2.0-rc4-8-g2b96f3662b 2.2.0)\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nUsage model's intermediate layer output in custom loss function causes OOM.\r\nI found the issue working on some custom loss function, but report the issue using much simplified toy example (the code is below). The code is based on this example https://www.tensorflow.org/guide/eager#variables_and_optimizers (conceptually I only added `+ x` in loss function, details are below)\r\nIt crashes with error (full script's output is attached):\r\n`tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[2000,2000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RealDiv]`\r\n\r\n**Describe the expected behavior**\r\nThe below code runs ok if in the line...\r\n`  error = model(inputs) - targets + x  # dummy operation just to use intermediate layer in loss  `\r\n...you remove + x like this\r\n`  error = model(inputs) - targets `\r\nSo it feels like the root cause is usage intermediate tensor in my loss function. I need to reference intermediate model layer output in custom loss function and being able to train model.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ninput = tf.keras.Input(shape=(1,))\r\nx = tf.keras.layers.Dense(1)(input)\r\n\r\nmodel = tf.keras.Model(inputs=input, outputs=x)\r\n\r\n# A toy dataset of points around 3 * x + 2\r\nNUM_EXAMPLES = 2000\r\ntraining_inputs = tf.random.normal([NUM_EXAMPLES])\r\nnoise = tf.random.normal([NUM_EXAMPLES])\r\ntraining_outputs = training_inputs * 3 + 2 + noise\r\n\r\n\r\ndef loss(model, inputs, targets):\r\n  error = model(inputs) - targets + x  # dummy operation just to use intermediate layer in loss (but conceptually it's case from a real project as many useful losses need reference some intermediate model's layers outputs)\r\n  return tf.reduce_mean(tf.square(error))\r\n\r\n\r\ndef grad(model, inputs, targets):\r\n  with tf.GradientTape() as tape:\r\n    loss_value = loss(model, inputs, targets)\r\n  return tape.gradient(loss_value, model.trainable_variables)\r\n\r\n\r\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\r\n\r\nsteps = 300\r\nfor i in range(steps):\r\n  grads = grad(model, training_inputs, training_outputs)\r\n  optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n```\r\n\r\n**Other info / logs** \r\n[scriptoutput.txt](https://github.com/tensorflow/tensorflow/files/4891146/scriptoutput.txt)", "comments": ["@RomanGirin,\r\nPlease try [limiting GPU](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) memory growth and check if you are still facing the same issue. Thanks!", "@amahendrakar  thank you for the reply!\r\nFirst of all, do you agree that it's a bug or am I creating the loss function violating some TF's concepts? if I create the loss in wrong way conceptually, please, let me know how to fix it.\r\n\r\nLimiting GPU memory usage didn't help - the same error but much earlier during the model's training.", "I'm experiencing a similar issue with a custom center loss implementation, as I explain in [this stackoverflow question](https://stackoverflow.com/questions/63099178/adding-custom-loss-to-keras-model-causes-oom-error).", "Any updates on this?", "@RomanGirin Tried reproducing this issue but ran into an other error. Please take a look at it [here](https://colab.research.google.com/gist/gowthamkpr/7d7724873335ec5afc03395293910c54/untitled.ipynb)", "@gowthamkpr  Thank you for the reply! I got the same error running the code on tf-nightly-gpu on colab. \r\n\r\nI also debug the code locally on tf-nightly-gpu. In `loss_value` variable I have \r\n`KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_mean/Mean:0', description=\"created by layer 'tf.math.reduce_mean'\")`\r\nI got the exception on the same line as it noted on your stacktrace. In deed, 'KerasTensor' object has no attribute '_id' :) Don't know how to fix it.\r\n\r\n**Error is different but it caused by usage of \"x\" in the loss function (the same \"+ x\" part marked above). Without \"+ x\" the code runs on TF 2.2. and tf-nightly-gpu, but I need to use intermediate layer in loss function in a similar way as it is in this simplified code.**\r\n\r\nStill have no clue how to get the code running neither on TF 2.2 (getting OOM error reported above) nor on tf-nightly-gpu. \r\nThe issue remains. Any help is appreciated!\r\n", "@RomanGirin The shape of model(inputs) and  targets is not compatible without adding the intermediate layer. I think this is not the correct implementation of loss function. I think this is not a bug, this is an implementation problem.", "@gowthamkpr  yes, you are right! thank you a lot!!! I've added slicing in corresponding parts to get correct shapes and now it works on TF version 2.3. \r\n\r\nTwo remaining issues are:\r\nthe first, on  tf-nightly-gpu I still get the same error as you mentioned https://github.com/tensorflow/tensorflow/issues/41200#issuecomment-673055970\r\n\r\nand the second, in corrected script which learns the toy network without errors now (thanks to you) I added accuracy validation logic and again facing an error (on TF 2.3) if I use intermediate layers output: \r\n\r\n`Traceback (most recent call last):\r\n  File \"C:/PyProjects/TF_Issue/main.py\", line 53, in <module>\r\n    hits += tf.reduce_sum(tf.where((output - target_batch + x_slice) < 0.01, 1, 0)).numpy()\r\nAttributeError: 'Tensor' object has no attribute 'numpy'`\r\n\r\nHere is the script (I marked the line which causes the error).  Without usage of intermediate layer it works. \r\nPlease, note these two lines in the script below:\r\n```\r\n  hits += tf.reduce_sum(tf.where((output - target_batch + x_slice) < 0.01, 1, 0)).numpy() # this line causes the error\r\n  # hits += tf.reduce_sum(tf.where(output - target_batch < 0.01, 1, 0)).numpy() # if x_slice is omitted script works fine\r\n```\r\ncomment the first one and uncomment the second to get the script working.\r\nI checked tensors shapes, of course. \r\n\r\nHere is the script itself:\r\n```\r\nimport tensorflow as tf\r\n\r\nbatch_size = 128\r\n\r\ninput = tf.keras.Input(shape=(None, 1))\r\nx = tf.keras.layers.Dense(1)(input)\r\noutput = tf.keras.layers.Dense(1)(x)\r\n\r\nmodel = tf.keras.Model(inputs=input, outputs=output)\r\n\r\n# A toy dataset of points around 3 * x + 2\r\nNUM_EXAMPLES = 2000\r\ninputs = tf.random.normal([NUM_EXAMPLES])\r\nnoise = tf.random.normal([NUM_EXAMPLES])\r\noutputs = inputs * 3 + 2 + noise\r\n\r\ntraining_inputs = tf.reshape(inputs[:1500], (1500, 1))\r\ntraining_outputs = tf.reshape(outputs[:1500], (1500, 1))\r\ntraining_inputs = tf.data.Dataset.from_tensor_slices(training_inputs).batch(batch_size)\r\ntraining_outputs = tf.data.Dataset.from_tensor_slices(training_outputs).batch(batch_size)\r\ntest_inputs = tf.reshape(inputs[1500:], (500, 1))\r\ntest_outputs = tf.reshape(outputs[1500:], (500, 1))\r\ntest_inputs = tf.data.Dataset.from_tensor_slices(test_inputs).batch(batch_size)\r\ntest_outputs = tf.data.Dataset.from_tensor_slices(test_outputs).batch(batch_size)\r\n\r\n\r\ndef loss(model, inputs, targets):\r\n  outputs = model(inputs)\r\n  output = outputs[:, 0]  # take the first output (in general model can have several outputs)\r\n  global x\r\n  x_slice = x[:, 0]\r\n  error = output - targets + x_slice\r\n  return tf.reduce_mean(tf.square(error))\r\n\r\n\r\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\r\nepoch = 3\r\nfor i in range(epoch):\r\n  for input_batch, target_batch in zip(training_inputs, training_outputs):\r\n    with tf.GradientTape() as tape:\r\n      loss_value = loss(model, input_batch, target_batch)\r\n      grads = tape.gradient(loss_value, model.trainable_variables)\r\n      optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n  print('epoch #:', i)\r\n\r\n\r\nhits = 0\r\ntotal = 0\r\nfor input_batch, target_batch in zip(test_inputs, test_outputs):\r\n  outputs = model(input_batch)\r\n  output = outputs[:, 0]  # take the first output (in general model can have several outputs)\r\n  x_slice = x[:, 0]\r\n  hits += tf.reduce_sum(tf.where((output - target_batch + x_slice) < 0.01, 1, 0)).numpy() # this line causes the error\r\n  # hits += tf.reduce_sum(tf.where(output - target_batch < 0.01, 1, 0)).numpy() # if x_slice is omitted script works fine\r\n  total += input_batch.shape[0]\r\n\r\nprint(hits)\r\nprint('Accuracy: ', hits/total)\r\n```\r\n\r\nAny ideas how to fix the issues?\r\n\r\n", "@RomanGirin Please create a new issue as the first issue has been resolved. Thank you! ", "@gowthamkpr \r\nhttps://github.com/tensorflow/tensorflow/issues/43293\r\nok, done!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41200\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41200\">No</a>\n", "In case it'll be useful for someone else. In further digging  I found the key misconcept in the example above: **usage of intermediate layer in eager calcs requires to create model which outputs the layer's output. You cannot just use in eager calculations SymbolicTensor (which is created when model defined with Keras Functional API).**\r\nI use the term 'SymbolicTensor' as it explained in https://blog.tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html and discussed here https://stackoverflow.com/questions/59707065/what-are-symbolic-tensors-in-tensorflow-and-keras\r\n\r\nSo reconsidered (and hopefully fixed) script is:\r\n```\r\n\r\nimport tensorflow as tf\r\n\r\nbatch_size = 128\r\n\r\ninput = tf.keras.Input(shape=(1,))\r\nx = tf.keras.layers.Dense(1)(input)\r\noutput = tf.keras.layers.Dense(1)(x)\r\n\r\n# usage of intermediate layer in eager calcs requires to create model\r\n# which outputs the layer's output\r\n# you cannot just use in eager calculations SymbolicTensor (which is created when model\r\n# defined with Keras Functional API)\r\nintermediate_model = tf.keras.Model(inputs=input, outputs=x)\r\n\r\nmodel = tf.keras.Model(inputs=input, outputs=output)\r\n\r\n# A toy dataset of points around 3 * x + 2\r\ninputs = tf.random.normal([2000])\r\nnoise = tf.random.normal([2000])\r\noutputs = inputs * 3 + 2 + noise\r\n\r\ntraining_inputs = tf.reshape(inputs[:1500], (1500, 1))\r\ntraining_outputs = tf.reshape(outputs[:1500], (1500, 1))\r\ntraining_inputs = tf.data.Dataset.from_tensor_slices(training_inputs).batch(batch_size)\r\ntraining_outputs = tf.data.Dataset.from_tensor_slices(training_outputs).batch(batch_size)\r\ntest_inputs = tf.reshape(inputs[1500:], (500, 1))\r\ntest_outputs = tf.reshape(outputs[1500:], (500, 1))\r\ntest_inputs = tf.data.Dataset.from_tensor_slices(test_inputs).batch(batch_size)\r\ntest_outputs = tf.data.Dataset.from_tensor_slices(test_outputs).batch(batch_size)\r\n\r\n\r\ndef loss(model, inputs, targets):\r\n  outputs = model(inputs)\r\n  x = intermediate_model(inputs)  # note that shared with the 'main' model layer are used in forward pass twice which is 'recorded' with tf.GradientTape()\r\n  error = outputs - targets + x\r\n  return tf.reduce_mean(tf.square(error))\r\n\r\n\r\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\r\nepoch = 300\r\nfor i in range(epoch):\r\n  for input_batch, target_batch in zip(training_inputs, training_outputs):\r\n    with tf.GradientTape() as tape:\r\n      loss_value = loss(model, input_batch, target_batch)\r\n      grads = tape.gradient(loss_value, model.trainable_variables)\r\n      optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n  print('epoch #:', i)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n"]}, {"number": 41199, "title": "Fix  categorical_crossentropy docstring", "body": "This PR fixes the docstring of tf.keras.backend.categorical_crossentropy method.", "comments": ["@mihaimaruseac \r\n\r\nSince `from_logits=False`,  the tensor  `b` expects to the result of a softmax. \r\n\r\nThis modification ensures that `tf.reduce_sum(b, axis=1)`  generates ` array([1., 1., 1.]`.\r\n\r\n\r\n\r\n"]}, {"number": 41198, "title": "[-Wsign-compare] warning fixes batch 13", "body": "@mihaimaruseac ", "comments": ["@mihaimaruseac ", "Closing as it has been handled by new PRs."]}, {"number": 41197, "title": "Make tf.errors subclasses of relevant Python builtin exceptions", "body": "Whe porting some code from Python's builtin file I/O to `tf.io.gfile` I had to adapt a lot of error handling which was checking against the [builtin Python exceptions](https://docs.python.org/3.8/library/exceptions.html#exception-hierarchy).\r\n\r\nThis PR makes the following `tf.errors` subclasses of their respective builtin Python analogue so that users don't need to modify `isinstance` checks during error handling when switching between Python's file I/O and `tf.io.gfile`:\r\n- `tf.errors.NotFoundError` subclasses [`FileNotFoundError`](https://docs.python.org/3.8/library/exceptions.html#FileNotFoundError)\r\n- `tf.errors.AlreadyExistsError` subclasses [`FileExistsError`](https://docs.python.org/3.8/library/exceptions.html#FileExistsError)\r\n- `tf.errors.PermissionDeniedError` subclasses [`PermissionError`](https://docs.python.org/3.8/library/exceptions.html#PermissionError)\r\n- `tf.errors.UnimplementedError` subclasses [`NotImplementedError`](https://docs.python.org/3.8/library/exceptions.html#NotImplementedError)\r\n\r\nTogether with #41173 this should improve the usability of `tf.io.gfile` as a replacement for Python's file I/O.\r\n\r\nOne could also think of making [`tf.errors.OutOfRangeError`](https://www.tensorflow.org/api_docs/python/tf/errors/OutOfRangeError) a subclass of [`StopIteration`](https://docs.python.org/3.8/library/exceptions.html#StopIteration) but I am not sure if there are some subtle differences between the meaning of those two errors.", "comments": ["I had to update the API goldens since the change in inheritance would fail CI otherwise.", "We need to not subclass the two exceptions that are used outside of file scope due to the fact that we use the TF ones in more places", "> We need to not subclass the two exceptions that are used outside of file scope due to the fact that we use the TF ones in more places\r\n\r\nHmm in that case I am not sure if this PR is actually an overall improvement (the motivation for this PR was mainly the `AlreadyExistsError` and `NotFoundError` which are mainly used as filesystem errors in TF).\r\nI would argue that having some errors subclass the related builtin exceptions and some errors not, is more confusing to users than not subclassing any of them.\r\n", "Yes, I think it will be more confusing to do so.\r\n\r\nUnfortunately our C++ exception hierarchy does not fit 100% to the standard Python one and it would be a lot of work to migrate.", "@lgeiger  Can you please resolve conflicts? Thanks!", "@lgeiger Can you please resolve conflicts? Thanks!", "It seems there are thousands of internal tests failing. Looking into it, probably some import is missing. If that is not the case, I'll be back here", "The failing test are caused by running internal tests with Python2, since internally we still need to support some of these while they move forward.\r\n\r\nAre these errors different in py2?", "> Are these errors different in py2?\r\n\r\nIt looks like `PermissionError` doesn't exist in Python 2.\r\n\r\n> The failing test are caused by running internal tests with Python2, since internally we still need to support some of these while they move forward.\r\n\r\nIs there a timeline for when Python 3 only code can be submitted to the TensorFlow code base?\r\n\r\nSince we cannot really change `NotFoundError` and `AlreadyExistsError` anyway, I won't spend time making this PR Python 2 compatible. It also looks like it is accumulating a lot of merge conflicts in the release notes.\r\nI am happy to resubmit once TensorFlow is Python 3 only.", "Apologies for the wasted time/effort.\r\n\r\nUnfortunately, we don't have a public guarantee on when Python 2 support can be fully dropped. External PRs must only be Python3 and code to TF must be only Python3 even if internally. But if an internal test that uses Py2 breaks we either need to wait for that test to fix in sync with the change or delay/reject the change. Sorry for all the issues this causes", "> Apologies for the wasted time/effort.\r\n\r\nThat's alright! @mihaimaruseac you are very responsive on GitHub so not much effort was wasted here. Not all PRs can be accepted that's normal (even if it's due to internal reasons).\r\nI have other open PRs where where I'm waiting a few months to get a review, that's much more frustrating, than a PR that doesn't get accepted :)\r\n\r\n> Unfortunately, we don't have a public guarantee on when Python 2 support can be fully dropped. External PRs must only be Python3 and code to TF must be only Python3 even if internally. But if an internal test that uses Py2 breaks we either need to wait for that test to fix in sync with the change or delay/reject the change. Sorry for all the issues this causes\r\n\r\nI see, thanks for the explanation.", "Apologies for the stalled PRs. Can you ping me on them to take a look, as possible?\r\n\r\nThank you"]}, {"number": 41196, "title": "Issue with CUDA and cuDNN", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): stable-baselines library\r\n- TensorFlow version: 1.15.3 also TensorFlow-gpu 1.15.0\r\n- Python version: 3.7.7\r\n- Installed using virtualenv? pip? conda?: Created conda environment using PyCharm\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA release 10.0, V10.0.130 / cuDNN 7.4.2\r\n- GPU model and memory: NVIDIA GeForce GTX 1050\r\n\r\nI am trying to train models with images as observations. I am using A2C and PPO2 with the Pendulum-v1 environment from OpenAI Gym. I am also using stable-baselines, a reinforcement learning library. Here is the complete code that I am using:\r\n\r\n```\r\nimport gym\r\nimport numpy as np\r\nfrom gym.envs.classic_control import PendulumEnv\r\nfrom stable_baselines.common.env_checker import check_env\r\nfrom stable_baselines.sac.policies import CnnPolicy\r\nfrom stable_baselines import PPO2\r\n\r\nfrom skimage import data, color\r\nfrom skimage.transform import rescale, resize, downscale_local_mean\r\n\r\nfrom gym import Wrapper, spaces\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\n\r\nclass RGBArrayAsObservationWrapper(Wrapper):\r\n    \"\"\"\r\n    Use env.render(rgb_array) as observation\r\n    rather than the observation environment provides\r\n    \"\"\"\r\n\r\n    def __init__(self, env):\r\n        # TODO this might not work before environment has been reset\r\n        super(RGBArrayAsObservationWrapper, self).__init__(env)\r\n        self.reset()\r\n        dummy_obs = env.render('rgb_array')\r\n        dummy_obs_resized = resize(dummy_obs, (dummy_obs.shape[0] // 10, dummy_obs.shape[1] // 10),\r\n                                   anti_aliasing=True)\r\n        # Update observation space\r\n        # TODO assign correct low and high\r\n        self.observation_space = spaces.Box(low=0, high=255, shape=dummy_obs_resized.shape,\r\n                                            dtype=dummy_obs_resized.dtype)\r\n\r\n    def reset(self, **kwargs):\r\n        obs = self.env.reset(**kwargs)\r\n        obs = self.env.render(\"rgb_array\")\r\n        obs = resize(obs, (obs.shape[0] // 10, obs.shape[1] // 10),\r\n                     anti_aliasing=True)\r\n        return obs\r\n\r\n    def step(self, action):\r\n        obs, reward, done, info = self.env.step(action)\r\n        obs = self.env.render(\"rgb_array\")\r\n        obs = resize(obs, (obs.shape[0] // 10, obs.shape[1] // 10),\r\n                     anti_aliasing=True)\r\n        return obs, reward, done, info\r\n\r\n\r\n# tensorboard --logdir=PPO2_IMG_PENDULUM:C:\\Users\\meric\\OneDrive\\Masa\u00fcst\u00fc\\TUM\\Thesis\\Pycharm\\pioneer\\ppo2_pendulum_tensorboard --host localhost\r\n\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\nTEST_COUNT = 100\r\n\r\npendulum_env = PendulumEnv()\r\npendulum_env = RGBArrayAsObservationWrapper(pendulum_env)\r\ncheck_env(pendulum_env, warn=True)\r\n\r\nmodel = PPO2(\"CnnPolicy\", pendulum_env, verbose=1, tensorboard_log=\"./ppo2_pendulum_tensorboard/\")\r\nmodel.learn(total_timesteps=100_000, log_interval=10)\r\nmodel.save(\"ppo2_pendulum\")\r\n\r\nsum_rewards = 0\r\ndone = False\r\nobs = pendulum_env.reset()\r\nfor i in range(TEST_COUNT):\r\n    while not done:\r\n        action, _states = model.predict(obs)\r\n        obs, rewards, done, info = pendulum_env.step(action)\r\n        pendulum_env.render()\r\n        sum_rewards += rewards\r\n\r\n    pendulum_env.reset()\r\n    done = False\r\n\r\nprint(sum_rewards / TEST_COUNT)\r\n\r\n```\r\nAnd here is the complete output in terminal.\r\n```\r\n2020-07-08 13:35:50.450192: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\n2020-07-08 13:35:53.202978: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-07-08 13:35:53.206120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-07-08 13:35:53.236974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\r\npciBusID: 0000:01:00.0\r\n2020-07-08 13:35:53.237468: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2020-07-08 13:35:53.242817: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2020-07-08 13:35:53.246183: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll\r\n2020-07-08 13:35:53.247490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll\r\n2020-07-08 13:35:53.252276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll\r\n2020-07-08 13:35:53.255771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll\r\n2020-07-08 13:35:53.268419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-08 13:35:53.268904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-07-08 13:35:53.903957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-08 13:35:53.904131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2020-07-08 13:35:53.904234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2020-07-08 13:35:53.904599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3001 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nC:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\env_checker.py:25: UserWarning: It seems that your observation is an image but the `dtype` of your observation_space is not `np.uint8`. If your observation is not an image, we recommend you to flatten the observation to have only a 1D vector\r\n  warnings.warn(\"It seems that your observation is an image but the `dtype` \"\r\nC:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\env_checker.py:210: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines.readthedocs.io/en/master/guide/rl_tips.html\r\n  warnings.warn(\"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\r\nWrapping the env in a DummyVecEnv.\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2020-07-08 13:35:55.069303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\r\npciBusID: 0000:01:00.0\r\n2020-07-08 13:35:55.069545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2020-07-08 13:35:55.069705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2020-07-08 13:35:55.069866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll\r\n2020-07-08 13:35:55.070064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll\r\n2020-07-08 13:35:55.070230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll\r\n2020-07-08 13:35:55.070391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll\r\n2020-07-08 13:35:55.070552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-08 13:35:55.070737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-07-08 13:35:55.070901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-08 13:35:55.071070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2020-07-08 13:35:55.071172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2020-07-08 13:35:55.071344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3001 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\tf_layers.py:103: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\base_class.py:1169: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\r\n\r\n2020-07-08 13:35:55.970675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2020-07-08 13:35:56.208807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-08 13:35:57.076059: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.2 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n2020-07-08 13:35:57.079198: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.2 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1350, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1443, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node model/c1/Conv2D}}]]\r\n\t [[output/strided_slice_1/_9]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node model/c1/Conv2D}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/meric/OneDrive/Masa\u00fcst\u00fc/TUM/Thesis/Pycharm/pioneer/pendulum_image_PPO2.py\", line 65, in <module>\r\n    model.learn(total_timesteps=100_000, log_interval=10)\r\n  File \"C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\", line 336, in learn\r\n    rollout = self.runner.run(callback)\r\n  File \"C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\runners.py\", line 48, in run\r\n    return self._run()\r\n  File \"C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\", line 472, in _run\r\n    actions, values, self.states, neglogpacs = self.model.step(self.obs, self.states, self.dones)\r\n  File \"C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\policies.py\", line 576, in step\r\n    {self.obs_ph: obs})\r\n  File \"C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node model/c1/Conv2D (defined at \\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\r\n\t [[output/strided_slice_1/_9]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node model/c1/Conv2D (defined at \\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nOriginal stack trace for 'model/c1/Conv2D':\r\n  File \"/Users/meric/OneDrive/Masa\u00fcst\u00fc/TUM/Thesis/Pycharm/pioneer/pendulum_image_PPO2.py\", line 64, in <module>\r\n    model = PPO2(\"CnnPolicy\", pendulum_env, verbose=1, tensorboard_log=\"./ppo2_pendulum_tensorboard/\")\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\", line 97, in __init__\r\n    self.setup_model()\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\", line 131, in setup_model\r\n    n_batch_step, reuse=False, **self.policy_kwargs)\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\policies.py\", line 602, in __init__\r\n    feature_extraction=\"cnn\", **_kwargs)\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\policies.py\", line 559, in __init__\r\n    pi_latent = vf_latent = cnn_extractor(self.processed_obs, **kwargs)\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\policies.py\", line 25, in nature_cnn\r\n    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=8, stride=4, init_scale=np.sqrt(2), **kwargs))\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\stable_baselines\\common\\tf_layers.py\", line 107, in conv\r\n    return bias + tf.nn.conv2d(input_tensor, weight, strides=strides, padding=pad, data_format=data_format)\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\", line 2010, in conv2d\r\n    name=name)\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\", line 1071, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"\\Users\\meric\\Anaconda3\\envs\\pioneer\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n\r\n\r\n My code was running smoothly before I tried putting cufft64_100.dll, curand64_100.dll, cusolver64_100.dll, cusparse64_100.dll, cudnn64_7.dll in my environment library in the anaconda3 folder.  My goal is to use my GPU during training so it is faster. Any help would be appreciated. I am also okay with deleting my anaconda environment and uninstalling all the CUDA and cuDNN files and starting from scratch if someone could help me on how to do this correctly. I need tensorflow 1.15 any other version does not work for me.\r\n", "comments": ["@meric-sakarya \r\nPlease share complete code with all dependencies as i ran your code and face different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/978ce4607380ac1ad5a145de0fcdf365/untitled269.ipynb).\r\nWith respect to the error faced, refer to below issues:\r\n#37725 #34355 #24828 [link](https://github.com/tensorflow/tensorflow/issues/36025#issuecomment-630375877) [link1](https://github.com/tensorflow/tensorflow/issues/25160#issuecomment-647104234)\r\n\r\nAlso is there any particular reason to be using 1.x when there are new versions of tf.", "I am using 1.15 because stable-baselines supports no newer version. You should install stable baselines using this command maybe that is causing your error. `pip install git+https://github.com/hill-a/stable-baselines\r\n`\r\nAlso i am using tensorflow-gpu==1.15 and you are only installing regular tensorflow. This is the whole code that I am using. You also should install sci-kit image with `pip install scikit-image`\r\nI have already taken a look into other similar issues they did not help me.", "This generally happens when CUDA and CUDNN versions are not compatible with each other.\r\nTry changing your CUDNN version 7.6.5 for CUDA version 10.0.130.\r\n\r\nYou should update your OS, GPU driver, Tensorflow, CUDA, CUDNN to recent versions for better support.", "@meric-sakarya\r\nPlease update as per above comment.\r\nI ran the code with the dependencies shared and face a different error, please find [gist here](https://colab.research.google.com/gist/Saduf2019/96408d7c9c833f79f3d0138e560b7125/untitled271.ipynb)", "I updated CUDA to 10.1 and CUDNN to 7.6.5 I also downloaded some files from an earlier version of CUDA(10.0) and added them to my NVIDIA GPU Computing Toolkit\\bin folder\r\n(link for download:https://drive.google.com/file/d/1GNx7RYty9wq7TJ8UC0A6Pw_nYGQ1qmep/view). \r\nMy code does not give any errors anymore. I also added the said bin folder to my PATH. My current problem is the slow training of my model. Unfortunately making these adjustments did not speed up the process. My GPU usage is around %5 only. Any suggestions on that?", "@meric-sakarya  Please roll back to cuda 10.0 since you are using TF 1.15\r\nTF 1.15 pre built binary does not support cuda 10.1 \r\nFurther we should try limiting your gpu memory growth and also reduce `batch_size` to avoid the error you are facing.\r\n```python\r\nimport tensorflow as tf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n # Rest of your code\r\n```", "Look like you have removed Path or cudart runtime so tf skips GPU(maybe).\r\nYou can debug Cuda GPU from https://github.com/dailylifesking/CUDA_not_working/blob/master/cuda_not_found_in_tensorflow.ipynb", "> Look like you have removed Path or cudart runtime so tf skips GPU(maybe).\r\n> You can debug Cuda GPU from https://github.com/dailylifesking/CUDA_not_working/blob/master/cuda_not_found_in_tensorflow.ipynb\r\n\r\nHow can I debug with this?\r\n\r\n", "@meric-sakarya Does limiting gpu memory help in your case?", "I could not debug using the given link. I rolled back to CUDA 10.0 and limited my GPU usage with \r\n\r\n`import tensorflow as tf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)`\r\n\r\nNone of these worked.", "Any help?", "Just to clarify, your cuda and cudnn version mismatch issue is solved and now you are not seeing maximum gpu utilization is that correct?\r\nIf that is the case can you try [manual device placement](https://www.tensorflow.org/guide/gpu#manual_device_placement)?\r\n```python\r\nimport tensorflow\r\ntf.debugging.set_log_device_placement(True)\r\n# Place tensors on the GPU\r\nwith tf.device('/GPU:0'):\r\n  # Your code\r\n```", "Yes I am definitely not seeing max gpu utilization. I tried the code nothing changed.", "> Yes I am definitely not seeing max gpu utilization. I tried the code nothing changed.\r\n\r\nCan you try using the [TensorBoard profiler](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) to see if something sticks out?", "# TensorFlow Profiler: Profile model performance\r\n\r\n<table class=\"tfo-notebook-buttons\" align=\"left\">\r\n  <td>\r\n    <a target=\"_blank\" href=\"https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\r\n  </td>\r\n  <td>\r\n    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_profiling_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\r\n  </td>\r\n  <td>\r\n    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorboard/blob/master/docs/tensorboard_profiling_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\r\n  </td>\r\n</table>\r\n\r\n## Overview\r\nMachine learning algorithms are typically computationally expensive. It is thus vital to quantify the performance of your machine learning application to ensure that you are running the most optimized version of your model. Use the TensorFlow Profiler to profile the execution of your TensorFlow code. \r\n\r\n\r\n## Setup\r\n\r\nfrom datetime import datetime\r\nfrom packaging import version\r\n\r\nimport os\r\n\r\nThe TensorFlow Profiler requires the latest versions of TensorFlow and TensorBoard (`>=2.2`).\r\n\r\n\r\n!pip install -U tensorboard_plugin_profile\r\n\r\nimport tensorflow as tf\r\n\r\nprint(\"TensorFlow version: \", tf.__version__)\r\n\r\nConfirm that TensorFlow can access the GPU.\r\n\r\ndevice_name = tf.test.gpu_device_name()\r\nif not device_name:\r\n  raise SystemError('GPU device not found')\r\nprint('Found GPU at: {}'.format(device_name))\r\n\r\n## Train an image classification model with TensorBoard callbacks\r\n\r\nIn this tutorial, you explore the capabilities of the TensorFlow Profiler by capturing the performance profile obtained by training a model to classify images in the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). \r\n\r\nUse TensorFlow datasets to import the training data and split it into training and test sets. \r\n\r\nimport tensorflow_datasets as tfds\r\ntfds.disable_progress_bar()\r\n\r\n(ds_train, ds_test), ds_info = tfds.load(\r\n    'mnist',\r\n    split=['train', 'test'],\r\n    shuffle_files=True,\r\n    as_supervised=True,\r\n    with_info=True,\r\n)\r\n\r\nPreprocess the training and test data by normalizing pixel values to be between 0 and 1.\r\n\r\ndef normalize_img(image, label):\r\n  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\r\n  return tf.cast(image, tf.float32) / 255., label\r\n\r\nds_train = ds_train.map(normalize_img)\r\nds_train = ds_train.batch(128)\r\n\r\nds_test = ds_test.map(normalize_img)\r\nds_test = ds_test.batch(128)\r\n\r\nCreate the image classification model using Keras.\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\r\n  tf.keras.layers.Dense(128,activation='relu'),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\nmodel.compile(\r\n    loss='sparse_categorical_crossentropy',\r\n    optimizer=tf.keras.optimizers.Adam(0.001),\r\n    metrics=['accuracy']\r\n)\r\n\r\nCreate a TensorBoard callback to capture performance profiles and call it while training the model.\r\n\r\n# Create a TensorBoard callback\r\nlogs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n\r\ntboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\r\n                                                 histogram_freq = 1,\r\n                                                 profile_batch = '500,520')\r\n\r\nmodel.fit(ds_train,\r\n          epochs=2,\r\n          validation_data=ds_test,\r\n          callbacks = [tboard_callback])\r\n\r\n## Use the TensorFlow Profiler to profile model training performance\r\n\r\nThe TensorFlow Profiler is embedded within TensorBoard. Load TensorBoard using Colab magic and launch it. View the performance profiles by navigating to the **Profile** tab. \r\n\r\n# Load the TensorBoard notebook extension.\r\n%load_ext tensorboard\r\n\r\nThe performance profile for this model is similar to the image below.\r\n\r\n# Launch TensorBoard and navigate to the Profile tab to view performance profile\r\n%tensorboard --logdir=logs\r\n\r\n<img class=\"tfo-display-only-on-site\" src=\"https://github.com/tensorflow/tensorboard/blob/master/docs/images/profiler_overview_page_bad_ip.png?raw=1\"/>\r\n\r\nThe **Profile** tab opens the Overview page which shows you a high-level summary of your model performance. Looking at the Step-time Graph on the right, you can see that the model is highly input bound (i.e., it spends a lot of time in the data input piepline). The Overview page also gives you recommendations on potential next steps you can follow to optimize your model performance. \r\n\r\nTo understand where the performance bottleneck occurs in the input pipeline, select the **Trace Viewer** from the **Tools** dropdown on the left. The Trace Viewer shows you a timeline of the different events that occured on the CPU and the GPU during the profiling period. \r\n\r\nThe Trace Viewer shows multiple event groups on the vertical axis. Each event group has multiple horizontal tracks, filled with trace events. The track is an event timeline for events executed on a thread or a GPU stream. Individual events are the colored, rectangular blocks on the timeline tracks. Time moves from left to right. Navigate the trace events by using the keyboard shortcuts `W` (zoom in), `S` (zoom out), `A` (scroll left), and `D` (scroll right).\r\n\r\nA single rectangle represents a trace event. Select the mouse cursor icon in the floating tool bar (or use the keyboard shortcut `1`) and click the trace event to analyze it. This will display information about the event, such as its start time and duration.\r\n\r\nIn addition to clicking, you can drag the mouse to select a group of trace events. This will give you a list of all the events in that area along with an event summary. Use the `M` key to measure the time duration of the selected events.\r\n\r\nTrace events are collected from:\r\n\r\n*   **CPU:** CPU events are displayed  under an event group named `/host:CPU`. Each track represents a thread on CPU. CPU events include input pipeline events, GPU operation (op) scheduling events, CPU op execution events etc.\r\n*   **GPU:** GPU events are displayed under event groups prefixed by `/device:GPU:`. Each event group represents one stream on the GPU. \r\n\r\n## Debug performance bottlenecks\r\n\r\nUse the Trace Viewer to locate the performance bottlenecks in your input pipeline. The image below is a snapshot of the performance profile. \r\n\r\n![profiler_trace_viewer_bad_ip](https://github.com/tensorflow/tensorboard/blob/master/docs/images/profiler_trace_viewer_bad_ip.png?raw=1)\r\n\r\nLooking at the event traces, you can see that the GPU is inactive while the `tf_data_iterator_get_next` op is running on the CPU. This op is responsible for processing the input data and sending it to the GPU for training. As a general rule of thumb, it is a good idea to always keep the device (GPU/TPU) active.\r\n\r\nUse the `tf.data` API to optimize the input pipeline. In this case, let's cache the training dataset and prefetch the data to ensure that there is always data available for the GPU to process. See [here](https://www.tensorflow.org/guide/data_performance) for more details on using `tf.data` to optimize your input pipelines. \r\n\r\n\r\n(ds_train, ds_test), ds_info = tfds.load(\r\n    'mnist',\r\n    split=['train', 'test'],\r\n    shuffle_files=True,\r\n    as_supervised=True,\r\n    with_info=True,\r\n)\r\n\r\nds_train = ds_train.map(normalize_img)\r\nds_train = ds_train.batch(128)\r\nds_train = ds_train.cache()\r\nds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\nds_test = ds_test.map(normalize_img)\r\nds_test = ds_test.batch(128)\r\nds_test = ds_test.cache()\r\nds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\nTrain the model again and capture the performance profile by reusing the callback from before.\r\n\r\nmodel.fit(ds_train,\r\n          epochs=2,\r\n          validation_data=ds_test,\r\n          callbacks = [tboard_callback])\r\n\r\nRe-launch TensorBoard and open the **Profile** tab to observe the performance profile for the updated input pipeline.\r\n\r\nThe performance profile for the model with the optimized input pipeline is similar to the image below.\r\n\r\n%tensorboard --logdir=logs\r\n\r\n<img class=\"tfo-display-only-on-site\" src=\"https://github.com/tensorflow/tensorboard/blob/master/docs/images/profiler_overview_page_good_ip.png?raw=1\"/>\r\n\r\nFrom the Overview page, you can see that the Average Step time has reduced as has the Input Step time. The Step-time Graph also indicates that the model is no longer highly input bound. Open the Trace Viewer to examine the trace events with the optimized input pipeline.\r\n\r\n![profiler_trace_viewer_good_ip](https://github.com/tensorflow/tensorboard/blob/master/docs/images/profiler_trace_viewer_good_ip.png?raw=1)\r\n\r\nThe Trace Viewer shows that the `tf_data_iterator_get_next` op executes much faster. The GPU therefore gets a steady stream of data to perform training and achieves much better utilization through model training.\r\n\r\n## Summary\r\n\r\nUse the TensorFlow Profiler to profile and debug model training performance. Read the [Profiler guide](https://www.tensorflow.org/guide/profiler) and watch the [Performance profiling in TF 2](https://www.youtube.com/watch?v=pXHAQIhhMhI) talk from the TensorFlow Dev Summit 2020 to learn more about the TensorFlow Profiler.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41196\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41196\">No</a>\n"]}, {"number": 41195, "title": "module 'tensorflow.keras.layers' has no attribute 'Conv1DTranspose'", "body": "Hello,\r\n\r\nI am trying to import tf.keras.layers.conv1DTranspose in colab and this creates an error message.\r\nI have followed this fix: https://github.com/tensorflow/tensorflow/issues/40937\r\n\r\nHowever, this makes tensorflow probability import to fail:\r\n----> 3 import tensorflow_probability as tfp\r\n      4 import numpy as np\r\n      5 from sklearn import preprocessing\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/experimental/auto_batching/frontend.py in <module>()\r\n     43 from tensorflow.python.autograph.converters import return_statements\r\n     44 from tensorflow.python.autograph.core import converter\r\n---> 45 from tensorflow.python.autograph.core import naming\r\n     46 from tensorflow.python.autograph.pyct import anno\r\n     47 from tensorflow.python.autograph.pyct import inspect_utils\r\n\r\nImportError: cannot import name 'naming'\r\n\r\nThanks for your help!\r\n\r\nMarc", "comments": ["I would like to work on this if no one is working on it ", "@mhuertascompany \r\n\r\nPlease, let us know which tensorflow version you are using?\r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@mhuertascompany \r\n\r\nI have tried in colab with TF version 2.2 and i am not seeing any issue while import naming.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/7e795f67892441fb926515aac30c309e/untitled96.ipynb).Thanks!", "Thanks. The problem comes when you try to import tensorflow-probability with 2.3-rc0 as explained here:\r\nhttps://github.com/tensorflow/tensorflow/issues/40937\r\n\r\nI need to upgrade to 2.3-rc0 to be able to use keras.layers.Conv1DTranspose as explained here: https://github.com/tensorflow/tensorflow/issues/40937\r\n\r\nHowever, then, the import of tf-probability fails.\r\nSee here:\r\nhttps://colab.research.google.com/drive/1DC7FkM7m7BNlajmAfz_xTZO7EnIrgMRm?usp=sharing", "@mhuertascompany \r\n\r\nI am able to replicate the issue with TF 2.3-rc1 anf nightly versions.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/f18fe304e86da3b20a48218ad20c8ca9/untitled103.ipynb).This issue seems to be duplicate #40584 .Thanks!", "@mhuertascompany \r\n\r\nAny update on this issue please. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mhuertascompany \r\n\r\nI have tried in colab with TF nightly version(`2.4.0-dev20200730`) and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/f558ae06a47fb650d1e0e6d22511d8f5/untitled208.ipynb).Please, verify once and close the issue.Thanks!", "Thanks!"]}, {"number": 41194, "title": "Building a custom delegate as shared library", "body": "## Description of the system/problem\r\nMy research group is currently working on a custom delegate to accelerate neural networks using FPGAs.\r\nWe wanted to build a custom delegate for the inference of the neural networks using the tensorflow lite interpreter.\r\nTo achieve this we wanted to build a shared library which can be loaded by \"tflite.load_delegate\" - using the the tflite_runtime in python.\r\n\r\nI created the delegate using c++, with help of the official tensorflow lite delegate [guide](https://www.tensorflow.org/lite/performance/delegates) and the source code of the [nnapi delegate](https://github.com/tensorflow/tensorflow/blob/v2.1.1/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc).\r\nFor compiling and linking I used CMake and built a static library of tensorflow lite using the [build scripts](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/make). I made sure, to use the same git repository version of tensorflow, as the newest runtime wheel build did (tflite_runtime-2.1.0.post1) and which I am using.\r\n\r\nWriting and compiling a small inference example in c++ using my delegate and the compiled static library works as expected and without errors. If I want to use my delegate as shared library in combination with the tflite_runtime, it comes to weird behaviors:  \r\nThe tensors seem not to be allocated directly. Data access / Dimension array access are leading to segmentation faults. Allocation Types and Data Types containing really large numbers instead of enum indices.\r\n\r\nI suppose there is something wrong in the way I compile my shared library. I will append the source code I used.\r\nAny help is appreciated!\r\n\r\n## Source Code\r\nDelegate source code, with minimal example. Requires compiled tensorflow lite static library:\r\n~~[delegate.zip]~~\r\n\r\nPython minimal example using tflite_runtime-2.1.0.post1 and numpy:\r\n[python_example.zip](https://github.com/tensorflow/tensorflow/files/4890286/python_example.zip)\r\n\r\nUsed tensorflow git version: `0.6.0-76902-gd855adfc5a` (output of `git describe`)\r\n\r\nExpected output from delegate:\r\n```\r\nOutputIndex: 10 \r\nUsing outputIndex: 10\r\nAllocation type is: 2 kTfLiteArenaRw\r\ntensor type: 9 INT8\r\ndims pointer: 0x557c461b2d20 is null: 0\r\ndims: [1 75 75 64  ]\r\n```\r\n\r\nGiven output using delegate shared library:\r\n```\r\nOutputIndex: 10 \r\nUsing outputIndex: 10\r\nAllocation type is: 33539632 UNKNOWN!\r\ntensor type: 1016998161 Unknown type\r\ndims pointer: 0x15f900 is null: 0\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n\r\n", "comments": ["I managed to get this working:  \r\nI was building the static library with the false git repo version (not the one mentioned above).\r\nUsing the version `0.6.0-76902-gd855adfc5a` fixed the problem.\r\nBefore the static library could be build I had to manually fix the Makefile regarding the commit in issue #36184.\r\n\r\nIs there a way to contribute to the docs, so other people trying to build shared library delegate may have a example how to build it?", "Hi @blacktoby I am trying to create custom delegate and shared library too. I'm new to delegates. Can you tell me the steps to build the delegate.zip i.e. I need to link it with libtensorflow-lite.so?\r\nAlso when I try to execute your python example I get error\r\nOSError: ./libastericsdelegate.so: cannot open shared object file: No such file or directory\r\n(But the library is present)\r\nDid you face this issue? ", "Well, I think you've tried to use my delegate. Which I wouldn't recommend, since it was only to rebuild my issue.\r\nAs I said in my post, I have built a static library with the Makefiles located in [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/make).\r\nWith the static library created, you can build your own delegate and link this static library to use all the TF-Lite functionality provided.\r\n\r\nI didn't face the issue. Maybe you have just set the wrong path? The error is clear."]}, {"number": 41193, "title": "DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3", "body": "Deprecation warnings as in the subject:\r\n```\r\nhome/bjourne/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:546\r\n  /home/bjourne/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\r\n    class IteratorBase(collections.Iterator, trackable.Trackable,\r\n\r\n/home/bjourne/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:106\r\n  /home/bjourne/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:106: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\r\n    class DatasetV2(collections.Iterable, tracking_base.Trackable,\r\n```\r\nThese warnings should be very easy to fix using conditional imports. tf version 2.4.0-dev20200708", "comments": ["@bjourne \r\nPlease share the stand alone indented code for us to replicate this issue, also please share the tf version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41193\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41193\">No</a>\n"]}, {"number": 41192, "title": "Add Rename, Copy, Delete recursively", "body": "In this PR, I add 3 function for `gcs` and some comments about the behavior of `gcs_filesystem`. `google-cloud-cpp` is missing a feature which allows us to mimic the current implementation so I am asking the maintainer team. Here is the link https://github.com/googleapis/google-cloud-cpp/issues/4482 . \r\n\r\nIn the meantime, I will work on `s3`. I will send you PR for `gcs` too but they will work as a cloud filesystem rather than local filesystem ( When the feature is ready, I will send you some PRs to mimic the current implementation )\r\n\r\nAnd how are the bugs in  `modular_filesystem_test` going ?", "comments": ["Sorry, I missed your question about the test suite. I will probably not get to it this week due to a very urgent high priority task unfortunately.", "No need to worry. Thank you."]}, {"number": 41191, "title": "C++ Program crashed while running tensorflow 2.0.0 with cuda 10.0 on Tegra Tx2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): jetpack 4.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):2.0.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):0.26.1\r\n- GCC/Compiler version (if compiling from source):7.5\r\n- CUDA/cuDNN version:10.0/7.6.3\r\n- GPU model and memory: tegra tx2\r\n\r\n\r\n**Describe the current behavior**\r\nwhen I try running sample application it is crashing. With error logs as \"CUDA runtime implicit initialization on GPU:0 failed. Status: unknown error\"\r\n\r\n**Describe the expected behavior**\r\nit should start session\r\n\r\n**Standalone code to reproduce the issue**\r\nSample Code  **example.cpp**\r\n```\r\n#include <tensorflow/core/platform/env.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include <iostream>\r\nusing namespace std;\r\nusing namespace tensorflow;\r\n\r\nint main()\r\n{\r\n    Session* session;\r\n    Status status = NewSession(SessionOptions(), &session);\r\n    if (!status.ok()) {\r\n        cout << status.ToString() << \"\\n\";\r\n    }\r\n    session->Close();\r\n    cout << \"Session successfully created.\\n\";\r\n}\r\n\r\n```\r\n\r\nCompile with the following command\r\n```\r\ng++ -c -std=c++11 -Wall -Wno-variadic-macros -g -O3 -fPIC -pedantic -I/usr/local/bin example.cpp -o example.o\r\ng++ -MM -std=c++11 -Wall -Wno-variadic-macros -g -O3 -fPIC -pedantic -I/usr/local/bin example.cpp > example.d\r\ng++ -o example example.o -std=c++11 -Wall -Wno-variadic-macros -g -O3 -fPIC -pedantic -L/usr/local/lib -ltensorflow_cc -lprotobuf\r\n\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n$ ./example \r\n2020-07-08 15:01:34.122381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-08 15:01:34.127413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero\r\n2020-07-08 15:01:34.127566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3\r\npciBusID: 0000:00:00.0\r\n2020-07-08 15:01:34.127608: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-07-08 15:01:34.127693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero\r\n2020-07-08 15:01:34.127815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero\r\n2020-07-08 15:01:34.127879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\nInternal: CUDA runtime implicit initialization on GPU:0 failed. Status: unknown error\r\nSegmentation fault (core dumped)\r\n\r\n```", "comments": ["@gowthamkpr  \r\n\r\nI was trying to search this error on google. I found at some place to set CUDA_VISIBLE_DEVICES env variable. I tried the program as follows and now I am facing another error.\r\n\r\n_*setenv(\"CUDA_VISIBLE_DEVICES\", \"\", 0);*_\r\n\r\nUpdate Sample Code\r\n```\r\n#include <tensorflow/core/platform/env.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include <iostream>\r\nusing namespace std;\r\nusing namespace tensorflow;\r\n\r\nint main()\r\n{\r\n    setenv(\"CUDA_VISIBLE_DEVICES\", \"\", 0);    // <--- Here the line added\r\n    Session* session;\r\n    Status status = NewSession(SessionOptions(), &session);\r\n    if (!status.ok()) {\r\n        cout << status.ToString() << \"\\n\";\r\n    }\r\n    session->Close();\r\n    cout << \"Session successfully created.\\n\";\r\n}\r\n```\r\nError logs are as follows.\r\n\r\n```\r\n$ ./example\r\n2020-07-10 15:49:20.957008: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so\r\n.1                                                                                        \r\n2020-07-10 15:49:20.961536: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-cap\r\nable device is detected                                           \r\n2020-07-10 15:49:20.961633: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this\r\n host (civilmapstx2-desktop): /proc/driver/nvidia/version does not exist\r\nSession successfully created.                                  \r\n```", "+@chsigg, who might know more about tegra tx2 on arm64.", "Thanks @chsigg @timshen91 !  Please let me know if you need any more info. ", "if I set environment variable CUDA_VISIBLE_DEVICES, GPU is not getting detected. If I don;t set it is not getting initialized. Behavior is it freezes for few seconds and then crashes when CUDA_VISIBLE_DEVICES is not set. \r\n\r\n```\r\nInternal: CUDA runtime implicit initialization on GPU:0 failed. Status: unknown error\r\nSegmentation fault (core dumped)\r\n```", "It looks like you are compiling for sm_30, but running on sm_62. This requires JIT compiling the CUDA kernels from sm_30 PTX to sm_62 SASS at startup. There are lots of kernels and that might be too much for Tx2. Can you try to compile for sm_62?", "Thanks @chsigg ... You mean I should compile tensorflow with sm_62? \r\n\r\nBecause the application I am compiling with gcc like as follows. (removed CMakeLists.txt in original post)\r\n\r\n```\r\ng++ -c -std=c++11 -Wall -Wno-variadic-macros -g -O3 -fPIC -pedantic -I/usr/local/bin example.cpp -o example.o\r\ng++ -MM -std=c++11 -Wall -Wno-variadic-macros -g -O3 -fPIC -pedantic -I/usr/local/bin example.cpp > example.d\r\ng++ -o example example.o -std=c++11 -Wall -Wno-variadic-macros -g -O3 -fPIC -pedantic -L/usr/local/lib -ltensorflow_cc -lprotobuf\r\n```", "Hey  @chsigg ,\r\n\r\nBelow is some observations with monolithic options\r\nAbove logs are with tensorflow compiled with monolithic option as I want to use opencv in my app.\r\n```\r\nbazel build --config=opt --config=v2 --config=monolithic --config=cuda --verbose_failures //tensorflow:libtensorflow_cc.so\r\n```\r\n\r\nI recompiled the tensorflow without monolithic options and the logs are as follows.\r\n```\r\nbazel build --config=opt --config=cuda --verbose_failures //tensorflow:libtensorflow_cc.so\r\n```\r\n\r\nNew Logs\r\n```\r\n# ./example\r\n2020-07-14 14:56:03.696872: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-14 14:56:03.718559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero\r\n2020-07-14 14:56:03.718732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.02\r\npciBusID: 0000:00:00.0\r\n2020-07-14 14:56:03.719530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-07-14 14:56:03.722110: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-07-14 14:56:03.724413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-07-14 14:56:03.725281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-07-14 14:56:03.728799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-07-14 14:56:03.732678: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-07-14 14:56:03.742980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-14 14:56:03.743217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero\r\n2020-07-14 14:56:03.743420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero\r\n2020-07-14 14:56:03.743554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-07-14 14:56:03.743667: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-07-14 14:56:03.823764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-14 14:56:03.823829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2020-07-14 14:56:03.823854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2020-07-14 14:56:03.824132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero\r\n2020-07-14 14:56:03.824434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero\r\n2020-07-14 14:56:03.824760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero\r\n2020-07-14 14:56:03.824943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 374 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)\r\n\r\n2020-07-14 15:04:10.368231: F tensorflow/stream_executor/cuda/cuda_driver.cc:175] Check failed: err == cudaSuccess || err == cudaErrorInvalidValue Unexpected CUDA error: unknown error\r\n```", "Hi Harendra, can you copy the content of your .tf_configure.bazelrc file?\r\nAlso, what commit of TF are you using? The error location ([cuda_driver.cc:175](https://github.com/tensorflow/tensorflow/blob/6d73eb4c3f8664ba5bbc12028c3ece32d433d23f/tensorflow/stream_executor/cuda/cuda_driver.cc#L175)) doesn't line up with master.", "Hi @chsigg ,\r\n\r\nPlease find the content of .tf_configure.bazelrc \r\n\r\n```\r\nbuild --host_force_python=PY2\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=tensorrt\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"3.5,7.0\"\r\nbuild --action_env LD_LIBRARY_PATH=\":/usr/lib/aarch64-linux-gnu:/usr/lib/aarch64-linux-gnu/tegra:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/lib\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\nNote: This is file content when I removed monolithic option(compiled on 14th july), However I want to use it with with monolithic option (old logs).  but I don't have that old file. (Not sure if it gets changed with different options.)\r\n\r\nI am using tensorflow tag 2.0.0\r\n\r\n```\r\n# clone tensorflow 2.0.0\r\ngit clone https://github.com/tensorflow/tensorflow.git && cd tensorflow && git checkout tags/v2.0.0\r\n```", "@sanjoy @chsigg \r\n\r\nwaiting for your reply.", "Hi Harendra,\r\n\r\nThanks for the info. \r\n\r\nTegras are [different](https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/) than normal CUDA environments wrt UVA. I'm still surprised that a call to \r\n[cudaPointerGetAttributes](https://github.com/tensorflow/tensorflow/blob/64c3d382cadf7bbe8e7e99884bede8284ff67f56/tensorflow/stream_executor/cuda/cuda_driver.cc#L175) fails, not something earlier, like cudaHostRegister.\r\n\r\nThis might be a little tricky to debug over GitHub, but let's give it a try. \r\n\r\nCan you run your program through nvprof (--print-api-trace) and cuda-memcheck (--report-api-errors all) to see if that gives any extra information?\r\nAlso printing the pointer value might be helpful (`<< \"Unexpected CUDA error: \" << cudaGetErrorString(err) << \" ptr=\" << ptr` on [line 176](https://github.com/tensorflow/tensorflow/blob/64c3d382cadf7bbe8e7e99884bede8284ff67f56/tensorflow/stream_executor/cuda/cuda_driver.cc#L176)).\r\n\r\nI don't think this will change anything, but you should use TF_CUDA_COMPUTE_CAPABILITIES=6.2 if you run on Jetson TX2 only.\r\n\r\n", "@harendra247  Any update on this?  Please go ahead and close the issue if you don't have any further queries.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41191\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41191\">No</a>\n", "Closing the issue as I could compile tf 2.3.1 on tx2 using jetpack 4.5."]}, {"number": 41190, "title": "_0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed", "body": "I am trying to start the training mask rcnn of tensorpack:\r\n```\r\npython train.py --config DATA.BASEDIR=/home/n/car/car MODE_FPN=True \"DATA.VAL=('balloon_val',)\"  \"DATA.TRAIN=('balloon_train',)\" TRAIN.BASE_LR=1e-3 TRAIN.EVAL_PERIOD=0 \"TRAIN.LR_SCHEDULE=[1000]\" \"PREPROC.TRAIN_SHORT_EDGE_SIZE=[600,1200]\" TRAIN.CHECKPOINT_PERIOD=1 DATA.NUM_WORKERS=1 --load COCO-MaskRCNN-R50FPN2x.npz --logdir log_full\r\n```\r\nAnd exactly after epoch 20 Iam getting:\r\n```\r\n2020-07-08 10:21:42.621628: W tensorflow/core/kernels/queue_base.cc:277] _0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed\r\n```\r\nEnvironment:\r\n```\r\nsys.platform          linux\r\nPython                3.6.10 |Anaconda, Inc.| (default, May  8 2020, 02:54:21) [GCC 7.3.0]\r\nTensorpack            v0.10.1-0-g8f831349\r\nNumpy                 1.19.0\r\nTensorFlow            1.13.1/b'v1.13.1-0-g6612da8951'\r\nTF Compiler Version   4.8.5\r\nTF CUDA support       True\r\nTF MKL support        False\r\nNvidia Driver         /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.440.33.01\r\nCUDA                  /usr/local/cuda-10.0/lib64/libcudart.so.10.0.130\r\nCUDNN                 /usr/local/cuda-10.0/lib64/libcudnn.so.7.5.1\r\nNCCL\r\nCUDA_VISIBLE_DEVICES  Unspecified\r\nGPU 0,1,2,3,4,5,6,7   Tesla K80\r\nFree RAM              475.14/480.07 GB\r\nCPU Count             32\r\ncv2                   4.2.0\r\nmsgpack               1.0.0\r\npython-prctl          False\r\n```\r\nI have tried changing:\r\n```\r\n MODE_FPN=True/False\r\n```\r\nHowever with \"False\" Im getting worse results.\r\n\r\nHere is the [link](https://paste.ofcode.org/ZsreU9FeP4EbJKqsGmK3nm)\r\n\r\nWhat is happening ? And how to fix it ?", "comments": ["@Adblu,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.\r\n\r\nAlso, please update TensorFlow to v1.15 or v2.2 and check if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41190\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41190\">No</a>\n"]}, {"number": 41188, "title": "DLL", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41188\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41188\">No</a>\n", "Nothing filled in the template"]}, {"number": 41187, "title": " module 'tensorflow' has no attribute 'compat' in 2.2.0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: \r\n\r\nAfter installing tensorflow 2.2.0 and CUDA 10.1 by using pip but when I am importing tensorflow, I get this error - module 'tensorflow' has no attribute 'compat' . ", "comments": ["What is the output of `pip list`?", "\r\n@mihaimaruseac \r\nCheck This:\r\n![Screenshot (81)](https://user-images.githubusercontent.com/46136132/86943286-dfddce80-c163-11ea-8af7-902540bf1e25.png)\r\n\r\n![Screenshot (82)](https://user-images.githubusercontent.com/46136132/86943227-cfc5ef00-c163-11ea-8d27-2307ea2c1c80.png)\r\n\r\n\r\n", "@sejal129,\r\nComments from similar issues [#1](https://github.com/tensorflow/tensorflow/issues/40422#issuecomment-651677723), [#2](https://github.com/tensorflow/tensorflow/issues/40422#issuecomment-651677723), [#3](https://github.com/tensorflow/tensorflow/issues/37525#issuecomment-598480859), [#4](https://github.com/tensorflow/tensorflow/issues/40819#issuecomment-650028666) say installing `tensorflow-estimator` fixes the error. \r\n\r\nCould you please try uninstalling TensorFlow completely and check if a fresh install works? Thanks!", "> @amahendrakar \r\n> Comments from similar issues [#1](https://github.com/tensorflow/tensorflow/issues/40422#issuecomment-651677723), [#2](https://github.com/tensorflow/tensorflow/issues/40422#issuecomment-651677723), [#3](https://github.com/tensorflow/tensorflow/issues/37525#issuecomment-598480859), [#4](https://github.com/tensorflow/tensorflow/issues/40819#issuecomment-650028666) say installing `tensorflow-estimator` fixes the error.\r\n> \r\n> Could you please try uninstalling TensorFlow completely and check if a fresh install works? Thanks!\r\n\r\nYaa. I have tried uninstalling and then fresh installing multiple times.", "@sejal129,\r\nPlease run the below commands in that particular order and check if it works.\r\n\r\n\r\n`pip uninstall tensorflow-gpu-estimator`\r\n`pip uninstall tensorflow-gpu`\r\n`pip uninstall tensorflow-estimator`\r\n`pip uninstall tensorflow`\r\n`pip install tensorflow`\r\n\r\n\r\nAlso, try installing TensorFlow in a virtual environment and check if you are facing the same error. Thanks!", "@sejal129 please always post the textual output, not an image. The text is easier to search (so others can also identify if they have similar issues), visible even for people who cannot see the images (eye reasons, data reasons, getting the notification via email instead of from GitHub, etc.), and easier to select, copy and paste in responses.\r\n\r\nIn this case, you have both the gpu and the non-gpu versions of the code. Recommendation is to uninstall all TF ecosystem packages and install `tensorflow` (it should be the same as `tensorflow_gpu`) after that. Doing so in a virtual environment is even better", "> @sejal129 please always post the textual output, not an image. The text is easier to search (so others can also identify if they have similar issues), visible even for people who cannot see the images (eye reasons, data reasons, getting the notification via email instead of from GitHub, etc.), and easier to select, copy and paste in responses.\r\n> \r\n> In this case, you have both the gpu and the non-gpu versions of the code. Recommendation is to uninstall all TF ecosystem packages and install `tensorflow` (it should be the same as `tensorflow_gpu`) after that. Doing so in a virtual environment is even better\r\n\r\nOkay. I will do that", "> @sejal129,\r\n> Please run the below commands in that particular order and check if it works.\r\n> \r\n> `pip uninstall tensorflow-gpu-estimator`\r\n> `pip uninstall tensorflow-gpu`\r\n> `pip uninstall tensorflow-estimator`\r\n> `pip uninstall tensorflow`\r\n> `pip install tensorflow`\r\n> \r\n> Also, try installing TensorFlow in a virtual environment and check if you are facing the same error. Thanks!\r\n\r\nThanks. I will try this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41187\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41187\">No</a>\n"]}, {"number": 41186, "title": "ModuleNotFoundError: No module named 'tensorflow_core.keras'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10, anaconda\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope\r\n- TensorFlow installed from (source or binary): Anaconda Navigator\r\n- TensorFlow version (use command below):  tensorflow 2.0\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI wrote the code below, and I came up with an error.(error report is attached under the code)\r\n------------------------------------------------------------------------------------------------------------------------------------------\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nnp.random.seed(3)\r\ntf.random.set_seed(3)\r\n\r\n'''\r\ndataset = np.loadtxt(\"C:\\\\Users\\\\tongt\\\\PycharmProjects\\\\kaggle\\\\titanic\\\\train_sample1.csv\", delimiter= ',')\r\nY = dataset[:, 0]\r\nX = dataset[:, 1:6+1]\r\n'''\r\ndf= pd.read_csv(\"C:\\\\Users\\\\tongt\\\\PycharmProjects\\\\kaggle\\\\train_sample1.csv\")\r\n\r\ndataset = df.values\r\nX = dataset[:, 2:6+1]\r\nY = dataset[:, 1]\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(12, input_dim=6, activation='relu'))\r\nmodel.add(Dense(8, activation='relu'))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accracy'])\r\n\r\nmodel.fit(X, Y, epochs = 200, batch_size = 10)\r\n--------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nC:\\Users\\tongt\\anaconda3\\envs\\ML\\python.exe C:/Users/tongt/PycharmProjects/kaggle/code.py\r\nTraceback (most recent call last):\r\n  File \"C:/Users/tongt/PycharmProjects/kaggle/code.py\", line 1, in <module>\r\n    from tensorflow.keras.models import Sequential\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 75, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\framework\\framework_lib.py\", line 25, in <module>\r\n    from tensorflow.python.framework.ops import Graph\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 58, in <module>\r\n    from tensorflow.python.platform import app\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 23, in <module>\r\n    from absl.app import run as _run\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\site-packages\\absl\\app.py\", line 35, in <module>\r\n    import pdb\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\pdb.py\", line 76, in <module>\r\n    import code\r\n  File \"C:\\Users\\tongt\\PycharmProjects\\kaggle\\code.py\", line 1, in <module>\r\n    from tensorflow.keras.models import Sequential\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\tongt\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named 'tensorflow_core.keras'\r\n\r\n----------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nI already searched google and tensorflow document to fix this problem, but I cannot handle this one... \r\nSo I tried to reinstall anaconda and pycharm. But it didn't work..\r\n\r\n\r\n**Describe the expected behavior**\r\ndd\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@tongtiger \r\nPlease refer to these issues with similar error and let us know if it helps:\r\n\r\n#34607 #32768 #37144 #36441 #32768  [link](https://stackoverflow.com/questions/59253581/flask-keras-webservice-modulenotfounderror-no-module-named-tensorflow-core-ker) [link1](https://www.kaggle.com/c/digit-recognizer/discussion/69597) [link2](https://www.edureka.co/community/67388/modulenotfounderror-no-module-named-keras).\r\n", "> @tongtiger \n> \n> Please refer to these issues with similar error and let us know if it helps:\n> \n> \n> \n> #34607 #32768 #37144 #36441 #32768  [link](https://stackoverflow.com/questions/59253581/flask-keras-webservice-modulenotfounderror-no-module-named-tensorflow-core-ker) [link1](https://www.kaggle.com/c/digit-recognizer/discussion/69597) [link2](https://www.edureka.co/community/67388/modulenotfounderror-no-module-named-keras).\n> \n> \n\nActually, I have already read that but I couldn\u2019t solve the problem... Do I need to add CUDA to path even if I don\u2019t use Tensorfow GPU?", "@tongtiger Please try it using the latest version of tensorflow i.e., 2.3.0 and see if the issue still persists. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41186\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41186\">No</a>\n"]}, {"number": 41185, "title": "How to gather the last X indices embeddings before every grouping of False in a boolean mask for every row in a batch", "body": "Urgent help needed in doing the below operation. Mask for input is given, its shape is [batch size, no of timesteps]. From this, I need to collect X number of embeddings of shape [batch size, timestep index, embedding size] such that they are grouped just before each grouping of False.\r\n\r\nsay mask of a batch size of 1 is T,T,T,F,F,F,F,|T,T,F,F,F,F,F and X=2 (by '|', I assumed a break which indicates a row split length=7), then should get list of concatenated embeddings given by indices (1,2) (8, 9). In the case when X is greater than number of Trues, should return the embeddings from index of first True till index which is X away from it.\r\n\r\nShould be able to replicate the above when batch size is variable without having to do individually for each batch as my batch size is pretty high. Output should be [ [ (1,2) , (.,.),.. for other batches at first row split (0:7) ] , [ (8,9) , (.,.) for other batches at second row split (7:14)], ..]\r\n\r\nAny help will be highly appreciated.\r\n", "comments": ["@vyshnavigutta369 \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41185\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41185\">No</a>\n"]}, {"number": 41184, "title": "C++ Program crashed while running tensorflow 2.0.0 with cuda 10.0 on NVIDIA Tegra Tx2", "body": "I am trying to build tensorflow2.0.0 on tx2. below are the details.\r\n\r\n**System information**\r\n- OS Platform and Distribution: jetpack 4.3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.5\r\n- CUDA/cuDNN version: 10.0/7.6.3\r\n- GPU model and memory: Tegra TX2\r\n\r\n**Describe the current behavior**\r\nwhen I try running sample application it is crashing. With error logs as \"CUDA runtime implicit initialization on GPU:0 failed. Status: unknown error\"\r\n\r\n**Describe the expected behavior**\r\nit should start a session.\r\n\r\n**Standalone code to reproduce the issue**\r\nSample Code \r\n```\r\n#include <tensorflow/core/platform/env.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include <iostream>\r\nusing namespace std;\r\nusing namespace tensorflow;\r\n\r\nint main()\r\n{\r\n    Session* session;\r\n    Status status = NewSession(SessionOptions(), &session);\r\n    if (!status.ok()) {\r\n        cout << status.ToString() << \"\\n\";\r\n    }\r\n    session->Close();\r\n    cout << \"Session successfully created.\\n\";\r\n}\r\n\r\n```\r\n\r\nCMakeLists.txt\r\n```\r\ncmake_minimum_required(VERSION 3.10)\r\n\r\nproject(test_hello)\r\n\r\nset(TENSORFLOW_LIBRARIES tensorflow_cc protobuf)\r\nadd_executable(example example.cpp)\r\nset(CUDA_NVCC_FLAGS\r\n        ${CUDA_NVCC_FLAGS};\r\n        -O3 -gencode arch=compute_30,code=sm_30;\r\n        --std=c++11\r\n        )\r\nset(CMAKE_CXX_STANDARD 11)\r\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\r\n\r\n# Link the Tensorflow library.\r\n# TensorFlow headers\r\ninclude_directories(\"/usr/local/include/\")\r\ninclude_directories(\"/usr/local/include/tensorflow/\")\r\ninclude_directories(\"/usr/local/include/third-party/\")\r\ntarget_link_libraries(example \"/usr/local/lib/libtensorflow_cc.so\")\r\n\r\n# You may also link cuda if it is available.\r\nfind_package(CUDA)\r\nif(CUDA_FOUND)\r\n  target_link_libraries(example ${CUDA_LIBRARIES})\r\nendif()\r\n\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n$ ./example \r\n2020-07-08 15:01:34.122381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-08 15:01:34.127413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero\r\n2020-07-08 15:01:34.127566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3\r\npciBusID: 0000:00:00.0\r\n2020-07-08 15:01:34.127608: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-07-08 15:01:34.127693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero\r\n2020-07-08 15:01:34.127815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:973] ARM64 does not support NUMA - returning NUMA node zero\r\n2020-07-08 15:01:34.127879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\nInternal: CUDA runtime implicit initialization on GPU:0 failed. Status: unknown error\r\nSegmentation fault (core dumped)\r\n\r\n```\r\n\r\nI am very new to tensorflow ...not sure what is going on. Appreciate the help.\r\n\r\nNOTE: I even tried by installing tensorflow-1.15.0 and running the same program. Same crash happens.", "comments": ["@harendra247,\r\nCould you please update TensorFlow to v2.2 and check if you are facing the same issue?\r\n\r\nAlso, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu) and check if you are running the compatible CUDA and cuDNN versions. Thanks! ", "@amahendrakar Thank you for replying. \r\nYes I am running compatible CUDA and cuDNN versions (10.0 and 7.6.3). This is coming by defaults in the jetpack 4.3. , I started this activity with tensorflow-2.2.0 but it was not even compiling so I downgraded to 2.0.0.  ", "@amahendrakar \r\n\r\nI was trying to search this error on google. I found at some place to set CUDA_VISIBLE_DEVICES env variable. I tried the program as follows and now I am facing another error.\r\n\r\n_*setenv(\"CUDA_VISIBLE_DEVICES\", \"\", 0);*_\r\n\r\nUpdate Sample Code\r\n```\r\n#include <tensorflow/core/platform/env.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include <iostream>\r\nusing namespace std;\r\nusing namespace tensorflow;\r\n\r\nint main()\r\n{\r\n    setenv(\"CUDA_VISIBLE_DEVICES\", \"\", 0);    // <--- Here the line added\r\n    Session* session;\r\n    Status status = NewSession(SessionOptions(), &session);\r\n    if (!status.ok()) {\r\n        cout << status.ToString() << \"\\n\";\r\n    }\r\n    session->Close();\r\n    cout << \"Session successfully created.\\n\";\r\n}\r\n```\r\nError logs are as follows.\r\n\r\n```\r\n$ ./example\r\n2020-07-10 15:49:20.957008: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so\r\n.1                                                                                        \r\n2020-07-10 15:49:20.961536: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-cap\r\nable device is detected                                           \r\n2020-07-10 15:49:20.961633: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this\r\n host (civilmapstx2-desktop): /proc/driver/nvidia/version does not exist\r\nSession successfully created.                                  \r\n```", "The error with the `CUDA_VISIBLE_DEVICES` change is a red herring: that disables GPUs and thus the program is failing.\r\n\r\nRegarding your original problem, it is tricky to narrow down `cudaErrorUnknown`.  Can you cat `.tf_configure.bazelrc` and paste it here?  Maybe we'll be able to spot a red flag in there.", "@sanjoy,\r\n\r\nThis is duplicate of #41191. \r\n\r\n\r\nI have pasted .tf_configure.bazelrc in the issue #41191. Can you please comment on that issue. \r\n\r\nI am closing it. I'll continue discussing #41191 ."]}, {"number": 41183, "title": "model.fit hangs when using matplotlib in the same script.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOS Catalina 10.15.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nAnaconda 3.6\r\n- TensorFlow version (use command below):\r\nunknown 2.0.0\r\n- Python version:\r\nPython 3.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nRadeon Pro 560X / Intel UHD Graphics 630\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen displaying a plot, then running model.fit, the following error is produced:\r\n```bash\r\nTrain on 60000 samples\r\nEpoch 1/5\r\nOMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.\r\nOMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.\r\n```\r\nI have discovered two workarounds:\r\n-  One workaround is described in this github issue # https://github.com/dmlc/xgboost/issues/1715\r\n```python\r\nimport os\r\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\r\n```\r\n- The other is to remove any references to matplot lib code. I have included an example in my sample code below.\r\n\r\n**Describe the expected behavior**\r\nI would expect that leveraging matplotlib code and tensorflow code in the same script to work without any work aorunds.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nif __name__ == '__main__':\r\n    import tensorflow as tf\r\n    import matplotlib.pyplot as plt\r\n\r\n    mnist = tf.keras.datasets.mnist\r\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\n    # This pyplot block of code contributes to the error. Without it, model.train functions fine.\r\n    # Begin Block\r\n    def display_image(position):\r\n        image = x_train[position].squeeze()\r\n        plt.title('Example %d. Label: %d' % (position, y_train[position]))\r\n        plt.imshow(image, cmap='gray')\r\n    display_image(0)\r\n    # End block\r\n\r\n    x_train = x_train.reshape(len(x_train), 28, 28, 1)\r\n    x_test = x_test.reshape(len(x_test), 28, 28, 1)\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Conv2D(filters=6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.AveragePooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(units=10, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n    ])\r\n\r\n    model.compile(optimizer='adam',\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n\r\n    \"\"\"\r\n    # This will throw the error\r\n    OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.\r\n    OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.\r\n    \"\"\"\r\n    model.fit(x_train, y_train, epochs=5, verbose=1)\r\n\r\n    # test accuracy\r\n    model.evaluate(x_test, y_test)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThis error doesnt seem to be reproducible in google collab.", "comments": ["@dddiaz Sorry for the late response. I ran your code in `TF2.3` and `tf-nightly` and I don't see the code hangs when matplotlib in the same script. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/fcca315efc54b535ec23fa5380c2ca2a/untitled22.ipynb).\r\n\r\nWhen I ran this code in GPU it took ~25 sec and in CPU it took ~110 sec. \r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41183\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41183\">No</a>\n"]}, {"number": 41182, "title": "Bad input tensor parameters in model", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):FreeRTOS\r\n- TensorFlow installed from (source or binary): TensorFlow Lite\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):ESP32 with 2MB external flash\r\nhttps://www.banggood.com/LILYGO-TTGO-T-Camera-Plus-ESP32-DOWDQ6-8MB-SPRAM-OV2640-Camera-Module-1_3-Inch-Display-With-WiFi-bluetooth-Board-p-1426498.html?ID=566073&cur_warehouse=CN\r\n\r\n**Describe the problem**\r\nFollowed the steps in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech#deploy-to-esp32\r\n\r\nGot the following error:\r\n\r\nBad input tensor parameters in model\r\nGuru Meditation Error: Core  1 panic'ed (LoadProhibited). Exception was unhandled.\r\n\r\nCore  1 register dump:\r\nPC      : 0x400d389b  PS      : 0x00060730  A0      : 0x800d35cf  A1      : 0x3ffc2080  \r\n\r\n0x400d389b: FeatureProvider::PopulateFeatureData(tflite::ErrorReporter*, int, int, int*) at /Users/velsaran/project/sound-ai/micro_speech/esp-idf/build/../main/feature_provider.cc:37\r\n\r\nA2      : 0x00000000  A3      : 0x3ffb1200  A4      : 0x00000000  A5      : 0x00000000  \r\nA6      : 0x3ffc20c4  A7      : 0x00000004  A8      : 0x800f1610  A9      : 0x3ffc1f20  \r\nA10     : 0x00000000  A11     : 0x3f402bac  A12     : 0x3ffc2080  A13     : 0x3ffc2060  \r\nA14     : 0x00000008  A15     : 0x3ffb56a0  SAR     : 0x00000001  EXCCAUSE: 0x0000001c  \r\nEXCVADDR: 0x00000000  LBEG    : 0x400014fd  LEND    : 0x4000150d  LCOUNT  : 0xffffffff  \r\n\r\nBacktrace:0x400d3898:0x3ffc2080 0x400d35cc:0x3ffc20c0 0x400d2fe2:0x3ffc20f0\r\n0x400d3898: FeatureProvider::PopulateFeatureData(tflite::ErrorReporter*, int, int, int*) at /Users/velsaran/project/sound-ai/micro_speech/esp-idf/build/../main/feature_provider.cc:36\r\n\r\n0x400d35cc: loop at /Users/velsaran/project/sound-ai/micro_speech/esp-idf/build/../main/main_functions.cc:132\r\n\r\n0x400d2fe2: tf_main(int, char**) at /Users/velsaran/project/sound-ai/micro_speech/esp-idf/build/../main/esp/main.cc:29 (discriminator 1)\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["Any help is appreciated. Is the tensor model having issue? or I need to change the GPIO for the MIC?", "I have faced the same issue. The hello_world example also crashes when accessing the input tensor.", "It's possible this is related to https://github.com/tensorflow/tensorflow/issues/41900. As a temporary solution, try reverting to TF Micro on r2.2, or reverting [this change](https://github.com/tensorflow/tensorflow/commit/fbf407383c93774d10bd7c45cd66788a070b0e07) locally.", "This issue should have been fixed with [this PR](https://github.com/tensorflow/tensorflow/pull/42653), Please check. If the issue is fixed, you may close it.\r\nThank you", "I have tested it and it works. This build configuration should be documented somewhere when tf-micro is statically linked.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41182\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41182\">No</a>\n"]}, {"number": 41180, "title": "index error found in tf.keras.backend.dot function ", "body": "<em>Sorry for bothering. an error both found in tf1.15</em>\r\n\r\n**System information**\r\n- OS: Windows 10 1904\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow 1.15 and tensorflow 2.1\r\n- Python version: python 3.6 for tensorflow 1.15, python 3.6 for tensorflow 2.1\r\n- CUDA/cuDNN version:10\r\n- GPU model and memory: Nvidia rtx 2080ti 11g\r\n\r\n**Describe the current behavior**\r\nHere is the source code of tensorflow.keras.backend.dot:\r\n`\r\n@keras_export('keras.backend.dot')\r\ndef dot(x, y):\r\n  \"\"\"Multiplies 2 tensors (and/or variables) and returns a *tensor*.\r\n\r\n  When attempting to multiply a nD tensor\r\n  with a nD tensor, it reproduces the Theano behavior.\r\n  (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)\r\n\r\n  Arguments:\r\n      x: Tensor or variable.\r\n      y: Tensor or variable.\r\n\r\n  Returns:\r\n      A tensor, dot product of `x` and `y`.\r\n  \"\"\"\r\n   if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):\r\n        x_shape = []\r\n        for i, s in zip(int_shape(x), tf.unstack(tf.shape(x))):\r\n            if i is not None:\r\n                x_shape.append(i)\r\n            else:\r\n                x_shape.append(s)\r\n        x_shape = tuple(x_shape)\r\n        y_shape = []\r\n        for i, s in zip(int_shape(y), tf.unstack(tf.shape(y))):\r\n            if i is not None:\r\n                y_shape.append(i)\r\n            else:\r\n                y_shape.append(s)\r\n        y_shape = tuple(y_shape)\r\n        y_permute_dim = list(range(ndim(y)))\r\n        y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim\r\n        xt = tf.reshape(x, [-1, x_shape[-1]])\r\n        yt = tf.reshape(tf.transpose(y, perm=y_permute_dim), [y_shape[-2], -1])\r\n        return tf.reshape(tf.matmul(xt, yt),\r\n                          x_shape[:-1] + y_shape[:-2] + y_shape[-1:])\r\n    if is_sparse(x):\r\n        out = tf.sparse.sparse_dense_matmul(x, y)\r\n    else:\r\n        out = tf.matmul(x, y)\r\n    return out\r\n\r\n`\r\nThe issue is that I have a tensor x, which shape is (?,?,128) and a tensor y with shape (128,) . Thus , satisfy the \r\nif judgement clause, and variant **y_permute_dim will be [0]**.  Next, an index out of range error would be raised\r\nfrom the code `y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim`.\r\nIt is obvious, for there is only one element in y_permute_dim.\r\nThis error was caught both in tensorflow_backend.py(tensorflow1.15), and source code is just the same in backend.py(tensorflow 2.1)\r\n\r\nLooking forward for your reply.\r\n\r\n**Other info / logs** I\r\n  File \"d:\\Anaconda3\\envs\\tf1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1365, in dot\r\n    y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim\r\nIndexError: pop index out of range", "comments": ["@sataliulan,\r\nIn order to expedite the trouble-shooting process, could you please provide a sample code to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41180\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41180\">No</a>\n"]}, {"number": 41179, "title": "pypi packages built against different CUDA versions needed", "body": "\r\n\r\n\r\n**System information** Arch Linux\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nDifferent Linux distributions ships with different CUDA & cudnn version, it would help researchers save a lot of time to build pip package if you can provide pypi package built against different CUDA version, just as what pytorch has done:\r\n![pytorch](https://user-images.githubusercontent.com/36614441/86866464-8f616500-c0c0-11ea-8aa0-881ce6d65032.png)\r\nThank you!\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n**Who will benefit with this feature?**\r\nPeople without plenty of resources.\r\n**Any Other info.**\r\nNo.", "comments": ["This question is above my level. +Gunan to confirm.\r\n\r\nMy understanding is that the support matrix for TensorFlow is already huge and unmanageable, and we can't add new dimensions to it.\r\n\r\nI think we're just going to close this.", "Mark has a great point. In addition:\r\npypi is a very limited resource. If you look at the above image carefully, you will see that most pytorch packages are not available on pypi. They are served out of \"download.pytorch.org\". \r\nSo, we will need to create a separate pip package mirror we own to do this. But then we have the problem of expanded support matrix.\r\n\r\nWhile in the future this might happen, at the moment this is not a priority for TF.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41179\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41179\">No</a>\n", "Hope you can build a separate pip package mirror soon!", "@fridex @sub-mod @harshad16"]}, {"number": 41178, "title": "Migrate python implementation of gelu from addons", "body": "As per https://github.com/tensorflow/community/pull/252.\r\n\r\nOld PR for C++ kernels: https://github.com/tensorflow/tensorflow/pull/33945\r\n\r\ncc @seanpmorgan @alextp ", "comments": ["Thanks @WindQAQ! \r\n\r\nJust wanted to add a note that per discussion there is an upcoming RFC for adding custom op kernels alongside python composite ops. Once that is made public a future PR can add those since they've been requested in the migration RFC.\r\n\r\n@alextp Is there a need for an advanced activation layer? What is the criteria for those?\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.2/tensorflow/python/keras/layers/advanced_activations.py\r\n\r\n", "Hi @karmel, do we also need to put `GELU` into [advanced_activations](https://github.com/tensorflow/tensorflow/blob/r2.2/tensorflow/python/keras/layers/advanced_activations.py) in order for `Layer` subclass? Thank you!", "When the C++ kernels are added it might be useful to have approximate=False by default.\r\nhttps://github.com/pytorch/pytorch/issues/39853#issuecomment-658806898\r\nIt doesn't seem like the approximate version adds any speed over an optimized exact version.", "> for @tensorflow/api-owners\r\n> \r\n> @WindQAQ - it is fine to skip making a layer in advanced_activations.py for now.\r\n> \r\n> If, however, as the comment from @hendrycks suggests, we want approximate=False to be the default in the future, we should make that the default now, as we will not be able to change from True to False in the future.\r\n\r\nGot it. Let alone the speed, two versions of gelu (approximate or not) produce different values. In my opinion, because the existing BERT-like models use the approximation version, if we change it to non-approximation by default, it will somehow loss some precision in the pretrained models.\r\n\r\nhttps://colab.research.google.com/drive/1UK47XgQgAWzvG4PwCk1jNI8VAIm0AQEL?usp=sharing", "> it will somehow loss some precision in the pretrained models.\r\n\r\nI think [BERT](https://github.com/huggingface/transformers/blob/82601f4c1a5c3edb680593bdd9b54abd5846cfa7/src/transformers/activations.py#L16), RoBERTa, and many (but not all) recent models have used the exact version. I also believe the exact version is more numerically stable. It should be noted that PyTorch doesn't even offer the approximate version and only has the exact version. As an author of the GELU paper, I think it would be good if we all started using the exact version since the approximations do not seem to provide speed anymore.", "Hi @karmel , I change the default value :-) Please review again. Thank you!", "> > it will somehow loss some precision in the pretrained models.\r\n> \r\n> I think [BERT](https://github.com/huggingface/transformers/blob/82601f4c1a5c3edb680593bdd9b54abd5846cfa7/src/transformers/activations.py#L16), RoBERTa, and many recent models have used the exact version. I also believe the exact version is more numerically stable. It should be noted that PyTorch doesn't even offer the approximate version and only has the exact version. As an author of the GELU paper, I think it would be good if we all started using the exact version since the approximations do not seem to provide speed anymore.\r\n\r\nWe have been used the approximate versions for many applications and many follow-up papers are using the approximate version, which at least performs well on TPU.", "> We have been used the approximate versions for many applications and many follow-up papers are using the approximate version, which at least performs well on TPU.\r\n\r\nThe current commit keeps the approximate form as an option.", "Hi @alextp @karmel, any suggestion on default value of the argument `approximate`? Thank you.", "approximate=False should be the default\n\nOn Mon, Jul 20, 2020 at 9:07 AM Tzu-Wei Sung <notifications@github.com>\nwrote:\n\n> Hi @alextp <https://github.com/alextp> @karmel <https://github.com/karmel>,\n> any suggestion on default value of the argument approximate? Thank you.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/41178#issuecomment-661134839>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRO3JQLVVVIHCSES5P3R4RTSLANCNFSM4OUA24CA>\n> .\n>\n\n\n-- \n - Alex\n", "@WindQAQ @seanpmorgan @penpornk \r\n\r\nI am confused if the RFC for C++ kernel implementation is created? Any updates? I understand the Python wrapper function is available now and merged in the master\r\n\r\nHere is the discussion link:\r\nhttps://github.com/tensorflow/tensorflow/pull/41178#issuecomment-655231498", "Waiting for an upcoming RFC on being able to easily support custom-ops with python op fallbacks. cc @alextp to see if there is any update on that?", "@seanpmorgan Sorry for the delay! @allenlavoie told me custom device support could be used for this: https://github.com/tensorflow/tensorflow/blob/84967b39fa98d27f5984648f9ec47a159206cfda/tensorflow/c/eager/c_api_experimental.h#L477-L517\r\n\r\nIn the long run (i.e., not now), @allenlavoie also has an [RFC on front-end op handlers](https://github.com/tensorflow/community/pull/275) that could help as well.", "One challenge with the custom device approach is that they're currently mutually exclusive with physical devices (an op is either on a physical device or a custom device). So if we're thinking of something that filters through every eager operation and is turned on globally then the custom device approach seems challenging for anything other than prototyping.\r\n\r\nThe op handler RFC proposes an updated version of that mechanism which isn't mutually exclusive with physical devices, and there I could see a global composite op lowering handler working. But even once that's implemented, it being useful is blocked on Python migrating to the eager/graph agnostic C API (see \"risks\" in the proposal). I don't expect it to be implemented quickly.", "Talked to Alex and he actually meant `tf.composite` by @liufengdb, so here it is, as another alternative:\r\n[Code](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/tfr)\r\n[Examples](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/tfr/examples)\r\n\r\n@liufengdb plans to add some README and documentation soon. "]}, {"number": 41177, "title": "Convert ParameterizedBenchmark to tf.test.Benchmark.", "body": "~~Add PerfZeroBenchmark into benchmarks folder and use PerfZeroBenchmark as base class. It can be run both on Bazel tests and PerfZero script.~~\r\nWe decide to use tf.test.Benchmark for our benchmark.", "comments": ["@xingyu-long Can you please resolve conflicts? Thanks!", "> @xingyu-long Can you please resolve conflicts? Thanks!\r\n\r\nSince host and I decided to hold this PR for now, but I will talk to her about this.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Already update in another [PR](https://github.com/tensorflow/tensorflow/pull/41456)"]}, {"number": 41176, "title": "Keras timeseriesgenerator: predict multiple data points in one step?", "body": "I have meteorological data that looks like this:\r\n\r\n```\r\nDateIdx               winddir   windspeed   hum         press       temp\r\n2017-04-17 00:00:00   0.369397  0.155039    0.386792    0.196721    0.238889\r\n2017-04-17 00:15:00   0.363214  0.147287    0.429245    0.196721    0.233333\r\n2017-04-17 00:30:00   0.357032  0.139535    0.471698    0.196721    0.227778\r\n2017-04-17 00:45:00   0.323029  0.127907    0.429245    0.204918    0.219444\r\n2017-04-17 01:00:00   0.347759  0.116279    0.386792    0.213115    0.211111\r\n2017-04-17 01:15:00   0.346213  0.127907    0.476415    0.204918    0.169444\r\n2017-04-17 01:30:00   0.259660  0.139535    0.566038    0.196721    0.127778\r\n2017-04-17 01:45:00   0.205564  0.073643    0.523585    0.172131    0.091667\r\n2017-04-17 02:00:00   0.157650  0.007752    0.481132    0.147541    0.055556\r\n2017-04-17 02:15:00   0.122101  0.003876    0.476415    0.122951    0.091667\r\n```\r\n\r\nMy aim: to use the keras timeseriesgenerator (`from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator`) to predict multiple data points (multiple rows) at once, e.g. not to do\r\n\r\n```\r\n[input X]                  | [predictions y]\r\n[dp1, dp2, dp3, dp4, dp5]  | [dp6]\r\n[dp2, dp3, dp4, dp5, dp6]  | [dp7]\r\n[dp3, dp4, dp5, dp6, dp7]  | [dp8]\r\n                          ...\r\n```\r\nbut to do\r\n\r\n```\r\n[input X]                  | [predictions y]\r\n[dp1, dp2, dp3, dp4, dp5]  | [dp6, dp7, dp8]\r\n[dp2, dp3, dp4, dp5, dp6]  | [dp7, dp8, dp9]\r\n[dp3, dp4, dp5, dp6, dp7]  | [dp8, dp9, dp10]\r\n                          ...\r\n```\r\nI can achieve the top kind of predictions with\r\n\r\n```\r\ngenerator = TimeseriesGenerator(\r\n    X,\r\n    X,\r\n    length=5,\r\n    sampling_rate=1,\r\n    stride=1,\r\n    start_index=0,\r\n    end_index=None,\r\n    shuffle=False,\r\n    reverse=False,\r\n    batch_size=1,\r\n)\r\n```\r\n, but I haven't figured out how I can tweak the generator options for the second kind of predictions.\r\n\r\nIs there an easy way to achieve the desired prediction window of 3 data points with the timeseriesgenerator? If not, can you suggest me some code to bin my predictions y to achieve the task? Tnx", "comments": ["@Nestak2,\r\nPlease take a look at the \"Multivariate Time Series Example\" from [this blog](https://machinelearningmastery.com/how-to-use-the-timeseriesgenerator-for-time-series-forecasting-in-keras/) and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41175, "title": "Sign compare warning fixes batch 12", "body": "in resolution of [-Wsign-compare] warnings with associated ids:\r\n[\r\n234, 245, 248, 250, 251, 258, 262, \r\n267, 270, 271, 273, 277, 281, 282, 284, \r\n293, 294, 296, 298, 310, 316, 319, 325, \r\n327, 332, 333, 335, 338, 342, 349, 350, \r\n353, 376, 428, 438, 439, 441, 443, 444,\r\n446, 450, 451, 459, 462, \r\n]\r\n\r\n", "comments": ["Closing as it has been handled by new PRs."]}, {"number": 41174, "title": "setting model.stop_training=True in custom callbacks does not stop training in v2.2.0 (works in v2.1.0 though)", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Ubuntu 18.04** \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.6.9\r\n\r\n\r\n**Describe the current behavior**\r\nSetting `self.model.stop_training=True` in `on_train_batch_end` does **NOT** stop training in v2.2.0.  It **does** work correctly in v2.1.0, though.\r\n\r\n**Describe the expected behavior**\r\nSetting `self.model.stop_training=True` in `on_train_batch_end` should correctly stop training.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nPlease see [this Google Colab link](https://colab.research.google.com/drive/1cDOIJTiVR0wFTdsTi0RaE-MVY71dYpD-) to reproduce the problem.\r\n\r\nThe code to reproduce is also here:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n# Define the Keras model to add callbacks to\r\ndef get_model():\r\n    model = keras.Sequential()\r\n    model.add(keras.layers.Dense(1, input_dim=784))\r\n    model.compile(\r\n        optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\r\n        loss=\"mean_squared_error\",\r\n        metrics=[\"mean_absolute_error\"],\r\n    )\r\n    return model\r\n\r\n# Load example MNIST data and pre-process it\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nx_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\r\nx_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\r\n\r\n# Limit the data to 1000 samples\r\nx_train = x_train[:1000]\r\ny_train = y_train[:1000]\r\nx_test = x_test[:1000]\r\ny_test = y_test[:1000]\r\n\r\nclass CustomCallback(keras.callbacks.Callback):\r\n    def on_train_batch_end(self, batch, logs=None):\r\n        keys = list(logs.keys())\r\n        print('value of model.stop_training: %s' % (self.model.stop_training))\r\n        if batch == 1:\r\n            print('stop training on batch %s' % (batch))\r\n            self.model.stop_training = True\r\n            return\r\n\r\nmodel = get_model()\r\nmodel.fit(\r\n    x_train,\r\n    y_train,\r\n    batch_size=128,\r\n    epochs=1,\r\n    verbose=0,\r\n    validation_split=0.5,\r\n    callbacks=[CustomCallback()],\r\n)\r\n\r\n```\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/0f297f0e2e8693608f0d99841d6f234b/41174-2-2.ipynb#scrollTo=PhCGba4zUtw9) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/766a327a49acf9a300c75aefb400639e/41174-tf-nightly.ipynb). Works as intended with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/3f500ee38278657627c54c35402feda7/41174-2-1.ipynb). Please find the attached gist. Thanks!", "@amaiya we realized there was a regression between 2.1 and 2.2, and we're actively fixing this. We plan to release the fix in 2.4. In the meantime, would stopping at epoch boundaries work for you?", "@rchao Thanks for including a fix in 2.4.  \r\n\r\nThe `stop_training` flag is being used in a [learning-rate-finder](https://github.com/amaiya/ktrain) that estimates an optimal learning rate by gradually increasing the learning rate and stopping training when the loss increases and diverges (similar to the fastai/PyTorch implementation).  It is important to stop at the batch boundary rather than epoch boundary in such an application.\r\n\r\nGiven this and some other v2.2 issues, I'll try to pin to v2.1 until 2.4.0 is released.\r\n\r\nThanks.", "@amaiya thanks for the updates! Sorry for the inconvenience.", "Is there a recommended work-around for those of us that can't downgrade to 2.1 and can't rely on stopping at epoch boundaries? I am using a custom callback to stop and exit training gracefully if an ephemeral machine goes away don't always have time to wait until the epoch ends. Thanks!", "Setting self.model.stop_training=True in on_train_batch_end does NOT stop training in v2.2.0.  \r\n@rchao  \r\nI suggest to add `train_steps` setting in keras `model.fit`.    Now , many models `stop flag` are `train step` rather than  `epoch` .  \r\nFor example: Bert\r\n\r\n\r\n\r\n\r\n", "I report `stop_training` flag has no effect within `on_train_batch_end` in v2.3.0 too.\r\n\r\nAs a workaround, if you can extrapolate/deduce what will the batch number be when you need to interrupt your training, then you can achieve the same result by setting the `steps_per_epoch` par in `.fit` accordingly.", "@amaiya,\r\nLooks like the issue is resolved with the latest TF-nightly. I was able to run the code without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f777e386d587d44b74b71f7690365547/41174-tf-nightly.ipynb). Thanks!", "@amahendrakar Very nice - thanks a lot.", "@amaiya,\r\nThank you for the update. Marking this issue as closed, as it is resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41174\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41174\">No</a>\n"]}, {"number": 41173, "title": "Support path-like objects in FileIO and GFile", "body": "[Path-like objects](https://docs.python.org/3/glossary.html#term-path-like-object) as used in [`pathlib`](https://docs.python.org/3.8/library/pathlib.html?highlight=pathlib#module-pathlib) are the recommended way to interact with file paths in Python. This PR changes `FileIO` to allow for path-like objects which would otherwise raise an exception when passed to `compat.as_bytes`.\r\n\r\n`FileIO` and `GFile` emulate the API of [Python's builtin `open`](https://docs.python.org/3.8/library/functions.html#open) and the [`os` module](https://docs.python.org/3/library/os.html) which both correctly handle path-like objects. This possibility will be expected by users of `FileIO` and should make switching between `open` and `GFile` easier.\r\n\r\nCloses https://github.com/tensorflow/tensorflow/issues/37357#issuecomment-604738669  @mihaimaruseac", "comments": ["`__fspath__` is [introduced in Python 3.6](https://docs.python.org/3/library/os.html#os.PathLike) so the following snippet does not work with Python 3.5. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/575632d3ebb1742bb17bfa0f1bde69b8b7f8c619/tensorflow/python/util/compat.py#L201-L202\r\n\r\nNot sure how to fix this, but https://github.com/iterative/dvc/issues/2903 might help.\r\n\r\n```\r\n======================================================================\r\nERROR: testCopypathlib (__main__.FileIoTest)\r\ntestCopypathlib (__main__.FileIoTest)\r\ntestCopypathlib(<function <lambda> at 0x7fa1e14c2268>)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/buildfarm/default/operations/20fdebd6-4aa7-4d36-99a6-8b6237935860/bazel-out/k8-opt/bin/tensorflow/python/file_io_test.runfiles/absl_py/absl/testing/parameterized.py\", line 265, in bound_param_test\r\n    test_method(self, *testcase_params)\r\n  File \"/buildfarm/default/operations/20fdebd6-4aa7-4d36-99a6-8b6237935860/bazel-out/k8-opt/bin/tensorflow/python/file_io_test.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io_test.py\", line 200, in testCopy\r\n    file_io.FileIO(file_path, mode=\"w\").write(\"testing\")\r\n  File \"/buildfarm/default/operations/20fdebd6-4aa7-4d36-99a6-8b6237935860/bazel-out/k8-opt/bin/tensorflow/python/file_io_test.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", line 102, in write\r\n    self._prewrite_check()\r\n  File \"/buildfarm/default/operations/20fdebd6-4aa7-4d36-99a6-8b6237935860/bazel-out/k8-opt/bin/tensorflow/python/file_io_test.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", line 88, in _prewrite_check\r\n    compat.path_to_bytes(self.__name), compat.as_bytes(self.__mode))\r\n  File \"/buildfarm/default/operations/20fdebd6-4aa7-4d36-99a6-8b6237935860/bazel-out/k8-opt/bin/tensorflow/python/file_io_test.runfiles/org_tensorflow/tensorflow/python/util/compat.py\", line 201, in path_to_bytes\r\n    return as_bytes(path)\r\n  File \"/buildfarm/default/operations/20fdebd6-4aa7-4d36-99a6-8b6237935860/bazel-out/k8-opt/bin/tensorflow/python/file_io_test.runfiles/org_tensorflow/tensorflow/python/util/compat.py\", line 87, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got PosixPath('/buildfarm/default/operations/20fdebd6-4aa7-4d36-99a6-8b6237935860/_tmp/02b4a152c1ace2a8f4a1d68629e95e627s69wyg2/tmp6x6tvldm/base_dir/temp_file')\r\n```", "@byronyi It looks like this is a problem with [`tf.compat.path_to_str`](https://www.tensorflow.org/api_docs/python/tf/compat/path_to_str) since it doesn't correctly handle `pathlib.Path` in Python 3.5 as it only started implementing the `PathLike` interface in 3.6.\r\n\r\nI could make it compatible with earlier versions by specifically checking for `pathlib.Path` in Python > 3.4 but it would add a bit of complexity. I don't think that users still on Python 3.5 are actually using `pathlib.Path` in practice since it only became widely supported by the standard library in 3.6.\r\n\r\nSo I'd be in favour of just adapting the unittest to support 3.5 which would fix the failure and if people are actually running into this in practice I am happy to provide a proper fix for older Python versions. @byronyi How do you feel about that?"]}, {"number": 41172, "title": "Updated estimator version", "body": "", "comments": []}, {"number": 41171, "title": "mlagents-learn is not responding: Please help me, am new on learn ml agents in unity", "body": "(base) D:\\PROGRAMMING\\UNITY\\AI\\ml-agents-release_3>mlagent-learn\r\n'mlagent-learn' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n\r\n(base) D:\\PROGRAMMING\\UNITY\\AI\\ml-agents-release_3>conda activate unity_rl\r\n\r\n(unity_rl) D:\\PROGRAMMING\\UNITY\\AI\\ml-agents-release_3>mlagents-learn\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\Scripts\\mlagents-learn-script.py\", line 33, in <module>\r\n    sys.exit(load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')())\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\Scripts\\mlagents-learn-script.py\", line 25, in importlib_load_entry_point\r\n    return next(matches).load()\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\site-packages\\importlib_metadata\\__init__.py\", line 105, in load\r\n    module = import_module(match.group('module'))\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"d:\\programming\\unity\\ai\\ml-agents-release_3\\ml-agents\\mlagents\\trainers\\learn.py\", line 12, in <module>\r\n    from mlagents import tf_utils\r\n  File \"d:\\programming\\unity\\ai\\ml-agents-release_3\\ml-agents\\mlagents\\tf_utils\\__init__.py\", line 1, in <module>\r\n    from mlagents.tf_utils.tf import tf as tf  # noqa\r\n  File \"d:\\programming\\unity\\ai\\ml-agents-release_3\\ml-agents\\mlagents\\tf_utils\\tf.py\", line 3, in <module>\r\n    import tensorflow as tf  # noqa I201\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Kumah\\.conda\\envs\\unity_rl\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n(unity_rl) D:\\PROGRAMMING\\UNITY\\AI\\ml-agents-release_3>mlagent-learn\r\n'mlagent-learn' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n\r\n(unity_rl) D:\\PROGRAMMING\\UNITY\\AI\\ml-agents-release_3>", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@Kumah1 \r\nPlease check [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) thread from a similar issue and let us know if it helps. \r\n\r\nPlease verify the below for this commonly occurring issue: \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the latest [microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "It's because my CPU doesn't support avx.\r\nIs there a way to train ml agents on my machine without avx?.\r\n\r\n Thank you very much", "@Kumah1 \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/40978#issuecomment-652454678) to resolve your issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41171\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41171\">No</a>\n"]}, {"number": 41170, "title": "Shape issue for `tensordot` with nested `vectorized_map` and graph exectued function", "body": "I have some issues with shapes for nested vectorized function call, I'm including the simplest code here:\r\n```python\r\nimport tensorflow as tf\r\nx = tf.ones((100, 10, 5))\r\n@tf.function()\r\ndef tdot(x):\r\n    res = tf.tensordot(x, x, 1)\r\n    return res\r\ndef func(x):\r\n    def func_inner(x_):\r\n        return tf.vectorized_map(tdot, x_)\r\n    return tf.vectorized_map(func_inner, x)\r\n\r\n@tf.function\r\ndef test():\r\n    res = func(x)\r\n    print(res.shape) # shape should be (100, 10) instead of (None, 10)\n\ntest()\r\n```\r\nIsn't it expected for the first dimension to be defined in the resulting tensor (see the comment). Or I am missing something in the docs?", "comments": ["@rrkarim \r\nPlease, let us know which TF version you are using? Also, in the given code you have defined the functions and you are not calling.On calling the test function i got the output(`<tensorflow.python.eager.def_function.Function at 0x7fd6a4a9e630>`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d9b21ed97733e50fab2d1ff4127afa38/untitled95.ipynb).\r\nPlease, help us with the reproducible code.Thanks!", "@ravikyram edited the code. As in the google colab, the bug exists in the stable version of tf.", "@rrkarim \r\n\r\nIf you run in eager mode instead of using autograph you will not face this issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/388c03f523202c42f04dab218f85e24c/untitled101.ipynb).Thanks!", "@ravikyram If we change tensordot to other tf op, we dont have the issue.", "I am able to reproduce the issue in colab with TF versions 2.2,2.3-rc1,nightly versions.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/33086d474185f4df68d2cfbb43dea014/untitled100.ipynb).If i run the code in eager mode you will not see any issue.Thanks!", "@ravikyram If also run with no vectorization, we don't see any issues. @jvishnuvardhan ", "@rrkarim @tf.function builds a callable graph of the function that it decorates. Using nested @tf.function is not suggestable. When I removed @tf.function decorating `def test():`, you can print the shape as expected. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/c013d0fccace2c99f967d9ce98261640/untitled100.ipynb).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "@jvishnuvardhan I don't get why it is not suggestable, can you please expand on that? And how it is connected to `vectorized_map` being used? `tf.map_fn` works alright. The thing is that the code that I'm working with depends on the nested `tf.function` decoration.", "@jvishnuvardhan Can you please recommend the easiest way of debugging pfor?", "TF Shape inference and constant propagation are not perfect.  PFor (underlying tf.vectorized_map) does an explicit set_shape to make up for this.  Please see:\r\nhttps://github.com/tensorflow/tensorflow/blob/8b87c1a09bf156ca9a42d9f72fad07da62100318/tensorflow/python/ops/parallel_for/pfor.py#L1564\r\n\r\nIt looks like the constant_value call there is not able to extract that value. Note that the value is passed in by vectorized_map here:\r\nhttps://github.com/tensorflow/tensorflow/blob/8b87c1a09bf156ca9a42d9f72fad07da62100318/tensorflow/python/ops/parallel_for/control_flow_ops.py#L428\r\n\r\nCan you check where the shape values are first becoming unknown in the call chain ? ", "@agarwal-ashish thanks, I'm debugging the issue right now. The thing I observe is that assigning `x = tf.ones((100, 10, 5, 5))` works as expected.", "@rrkarim Is this still an issue? Please close the issue if this was already resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing, thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41170\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41170\">No</a>\n"]}]