[{"number": 45276, "title": "[XLA] Relaxing requirements for CloneWithNewOperands to preserve sharding info", "body": "Changing conditions for HloInstruction::CloneWithNewOperands to preserve sharding information.\r\nSharding information is currently disscarded if the destination tuple shape is different.\r\n\r\nThis change makes is so the the shapes at the leaves can be different and sharding information will still be copied.\r\nSharding information is only discarded if the tuple tree structure is different.", "comments": ["@Alfie-Edwards  Can you please check @blakehechtman's comments and keep us posted ? Thanks!", "For tiled sharding, I think it needs to check if the ranks are still the same at that leaf node. You can make CompatibleKind() more restrictive, or add extra checks in SetupDerivedInstruction() for leaves with tiled sharding.", "@Alfie-Edwards  Can you please check @ukoxyz's comments and keep us posted ? Thanks!", "> For tiled sharding, I think it needs to check if the ranks are still the same at that leaf node. You can make CompatibleKind() more restrictive, or add extra checks in SetupDerivedInstruction() for leaves with tiled sharding.\r\n\r\nSorry for the delay. I've made CompatibleKind more restrictive.", "The new change looks good to me.", "@ukoxyz are you able to approve the PR?", "I don't see an option to approve it. Maybe I'm not in the author list? Feel free to approve it if you can.", "Is this CI error something I need to address? I can't see the details of the error."]}, {"number": 45275, "title": "[XLA] Fix a dangling reference bug that is triggered in Ubuntu 20.04 with GCC 9.3", "body": "The Span returned point to an initializer list that is removed. This isn't allowed.\r\n\r\nThis is triggered in Ubuntu 20.03 that have GCC 9.3. If I install and use gcc 8 in the same container, the error doesn't show up.\r\n\r\n@sanjoy @nluehr ", "comments": ["LGTM, but can you please use a more specific subject :) maybe \"Fix a dangling reference bug\"?", "Edited. I also added a little bit more information in the description.", "@nouiz  Can you please check @sanjoy's comments and keep us posted ? Thanks!", "I amended the commit. I had understood that @sanjoy would do the fix internally.", "Any update? This bug is in the TF2.4 release. Should it be backported?", "I'm not surprised. I just too a chance by asking."]}, {"number": 45274, "title": "Initial step for TensorBoard Callback", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAdd ```initial_train_step``` and ```initial_test_step``` as ```__init__``` arguments.\r\nCurrently, TensorBoard callback starts writing metrics from step 0 for both train and test, but it breaks the graphs if model.fit() was used in tandem with loaded checkpoint. *STEP* horizontal axis does not provide graph consistency, *RELATIVE* and *WALL* horizontal axis provides metrics consistency, but horizontal axis has huge value gaps in it. Each subsequent run starts writing from step 0\r\n\r\n![image](https://user-images.githubusercontent.com/42458049/100635373-81e39a80-3341-11eb-8082-13c2d966e468.png)\r\n*STEP horizontal axis*\r\n\r\n![image](https://user-images.githubusercontent.com/42458049/100635480-a0499600-3341-11eb-85c8-3834de3759b5.png)\r\n*RELATIVE horizontal axis, first small gap is from testing after epoch*\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nPeople who use model.fit() with checkpoints\r\n", "comments": ["This is a Keras callback bug which is owned by Keras team.", "That was an accident", "@BananaLoaf \r\nLooks like you used the model.fit() twice in your example, then you save the log to a same file. In fact, nobody knows except yourself that the two training are continuous and consistent. Sometimes it is possible to train the same model in different directions after a checkpoint.\r\nSo in your case all you need is the initial_epoch parameter of the model.fit().\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\r\nThat's my opinion and I hope it helps you.", "Hi @BananaLoaf !\r\nWe are checking to see whether you still need help in this issue . Did you check with above [suggestion ](https://github.com/tensorflow/tensorflow/issues/45274#issuecomment-740495910) yet? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45273, "title": "DLL load failed while importing _pywrap_tensorflow_internal", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@mayankmehra2606 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45273\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45273\">No</a>\n"]}, {"number": 45272, "title": "Namerror when loading model", "body": "OS: Ubuntu 18.04\r\nTensorflow version : 2.2.0\r\n\r\n\r\nI am trying to load a model saved in h5 format for a model with a custom layer but getting a Namerror.\r\n\r\nThis is the implementation of the custom layer (Dropout) :\r\n```\r\nclass myDropout(keras.layers.Dropout):\r\n   \"\"\"Applies Dropout to the input.\r\n     Dropout consists in randomly setting\r\n     a fraction `rate` of input units to 0 at each update during training time,\r\n     which helps prevent overfitting.\r\n     # Arguments\r\n         rate: float between 0 and 1. Fraction of the input units to drop.\r\n         noise_shape: 1D integer tensor representing the shape of the\r\n             binary dropout mask that will be multiplied with the input.\r\n             For instance, if your inputs have shape\r\n             `(batch_size, timesteps, features)` and\r\n             you want the dropout mask to be the same for all timesteps,\r\n             you can use `noise_shape=(batch_size, 1, features)`.\r\n         seed: A Python integer to use as random seed.\r\n     # References\r\n         - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\r\n            http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\r\n     \"\"\"\r\n     def __init__(self, rate, training=True, noise_shape=None, seed=None, name=None, **kwargs):\r\n         super(myDropout, self).__init__(rate, noise_shape=None, seed=None,name = name,**kwargs)\r\n         self.training = training\r\n\r\n     def get_config(self):\r\n         config = super(myDropout, self).get_config()\r\n         config.update({\"rate\": self.rate})\r\n         # return config\r\n         return {\"output\": config, \"name\": self.name}\r\n\r\n\r\n     def call(self, inputs, training=None):\r\n         if 0. < self.rate < 1.:\r\n             noise_shape = self._get_noise_shape(inputs)\r\n\r\n             def dropped_inputs():\r\n                 return K.dropout(inputs, self.rate, noise_shape,\r\n                                  seed=self.seed)\r\n             if not training:\r\n                 return K.in_train_phase(dropped_inputs, inputs, training=self.training)\r\n             return K.in_train_phase(dropped_inputs, inputs, training=training)\r\n         return inputs)\r\n```\r\nand the command I use to load the model is:\r\n\r\nloaded_model = tf.keras.models.load_model(path, custom_objects={'myDropout': myDropout})\r\n\r\nand the error message is:\r\n\r\nNameError: name 'myDropout' is not defined\r\n\r\n", "comments": ["@quartermaine,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45272\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45272\">No</a>\n"]}, {"number": 45271, "title": "Use pretrained weight for another smaller model", "body": "Is it possible to possible to use a pretrained weight of a large model (for example ResNet151) and cut a part out of it to fit a custom, rather small model?\r\nIf it is the case, how can it be managed?\r\nThanks for any ideas!\r\n", "comments": ["@BogoK \r\nCould you please explain what is the issue faced", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45270, "title": "torch.nn.functional.grid_smaple tensorflow2.0 how to recurrence?", "body": "torch.nn.functional.grid_smaple tensorflow2.0 how to recurrence?", "comments": ["@kgju \r\n\r\nCan you please refer this [SO link](https://stackoverflow.com/questions/52888146/what-is-the-equivalent-of-torch-nn-functional-grid-sample-in-tensorflow-numpy/52896615#52896615) and see if it helps you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45269, "title": "tf.saved_model.save does not work using uint8 Input-Layer", "body": "System information:\r\n- Testted on colab.research.google.com\r\n- Tensorflow 2.3.1\r\n\r\nThe following minimal Model cannot be saved using `tf.saved_model.save`:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.layers.experimental.preprocessing import Rescaling\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.utils import Sequence\r\nfilters = (40, 60, 80)\r\np = dict(padding=\"same\",\r\n          activation=\"selu\",\r\n          kernel_size=(3, 3),\r\n          kernel_initializer=\"lecun_normal\",\r\n          bias_initializer=\"zeros\")\r\nALPHABET = \"asdf4711\"\r\ni = layers.Input((None, None, 3), name='input', dtype='uint8')\r\nx = layers.Permute((2, 1, 3))(i)\r\nx = Rescaling(1. / 255.)(x)\r\nx = layers.LayerNormalization(axis=1, center=False, scale=False)(x)\r\nx = layers.Conv2D(filters[0], **p)(x)\r\nx = layers.Conv2D(filters[0], **p)(x)\r\nx = layers.Conv2D(filters[1], strides=(2, 3), **p)(x)\r\nx = layers.Conv2D(filters[1], **p)(x)\r\nx = layers.Conv2D(filters[1], **p)(x)\r\nx = layers.Conv2D(filters[2], strides=(2, 3), **p)(x)\r\nx = layers.Conv2D(filters[2], **p)(x)\r\nx = layers.Conv2D(filters[2], **p)(x)\r\n\r\nx = layers.Lambda(lambda t: tf.math.reduce_max(t, axis=2))(x)\r\nx = layers.Bidirectional(layers.GRU(x.shape[-1], return_sequences=True), 'sum')(x)\r\nx = layers.Bidirectional(layers.GRU(x.shape[-1], return_sequences=True), 'sum')(x)\r\nx = layers.Conv1D(len(ALPHABET) + 1, 1, activation=\"softmax\")(x)\r\nm = Model(inputs=i, outputs=x)\r\n```\r\nInstead of saving the model, i get the following error:\r\n`\r\nValueError: Python inputs incompatible with input_signature: inputs ((<tf.Tensor 'input:0' shape=(None, None, None, 3) dtype=uint8>,)), input_signature ((TensorSpec(shape=(None, None, None, None), dtype=tf.float32, name=None),))`\r\n\r\nWhen I change the type of the input layer to `'float32'`,  the export works fine. However, when deployed in tensorflow-serving and calling it via grpc, using uint8 instead of float32 significantly reduces network io in this use case.\r\n\r\n", "comments": ["The following change has fixed the issue:\r\n```\r\n i = layers.Input((None, None, 3), name='input', dtype='uint8')\r\n x = layers.Lambda(K.cast_to_floatx)(i)\r\n```\r\nMy expectation is that `Rescaling ` does the type-conversion.\r\n", "> The following change has fixed the issue:\r\n\r\n@Hhessling,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "I think this should be fixed in `tensorflow.keras.layers.experimental.preprocessing.Rescaling`. However, since this Layer is considered as being experimental, I close this issue. "]}, {"number": 45268, "title": "What is the difference between Keras Attention and \u201cScaled dot product attention\u201d as in the TF Transformer tutorial", "body": "I am going through the TF Transformer tutorial: https://www.tensorflow.org/tutorials/text/transformer#scaled_dot_product_attention\r\n\r\nAnd I was comparing the output from their defined Attention to the one from Keras (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention there you can also see the code).\r\n\r\nTesting on basic examples, they produce slightly different results.\r\n\r\nFor example, given the following input x:\r\n\r\n```\r\nIn [311]: x\r\nOut[311]: \r\n<tf.Tensor: shape=(1, 2, 4), dtype=float32, numpy=\r\narray([[[0.3020829 , 0.38190305, 0.48654556, 0.6720122 ],\r\n        [0.04044473, 0.04240108, 0.19330955, 0.01818049]]], dtype=float32)>\r\n```\r\n\r\nWhich was generated as\r\n\r\n`x = tf.random.uniform((1, 2, 4))`\r\n\r\nif I run this through the tutorial attention function, I get:\r\n\r\n```\r\nIn [312]: scaled_dot_product_attention(x, x, x, None)\r\nOut[312]: \r\n(<tf.Tensor: shape=(1, 2, 4), dtype=float32, numpy=\r\n array([[[0.19679338, 0.24527925, 0.36854032, 0.40889454],\r\n         [0.17432278, 0.21612138, 0.34335595, 0.35274068]]], dtype=float32)>\r\n```\r\n\r\nBut if I use keras attention function, everything default, I get:\r\n\r\n```\r\nIn [309]: model.predict([x,x,x])\r\nOut[309]: \r\narray([[[0.22044972, 0.27597573, 0.39505363, 0.46801156],\r\n        [0.17737839, 0.22008634, 0.34678057, 0.3603766 ]]], dtype=float32)\r\n```\r\n\r\nwhere model is just\r\n\r\n`model = tf.keras.models.Model(inputs=[query, value, key], outputs=tf.keras.layers.Attention()([value,value,value]))`\r\n\r\nAs you can see, the values are very similar, but not exactly the same.\r\n\r\nI wonder where this difference is coming from. Checking through the keras github I don't see them doing a scaling factor, such as in the tutorial:\r\n\r\n```\r\n  # scale matmul_qk\r\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\r\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\r\n```\r\n\r\nbut if I define \"use_scale\" in keras attention, it doesnt make a difference:\r\n\r\n```\r\nIn [313]: model = tf.keras.models.Model(inputs=[query, value, key], outputs=tf.keras.layers.Attention(use_scale=True)([value,value,value]))\r\nIn [315]: model.predict([x,x,x])\r\nOut[315]: \r\narray([[[0.22044972, 0.27597573, 0.39505363, 0.46801156],\r\n        [0.17737839, 0.22008634, 0.34678057, 0.3603766 ]]], dtype=float32)\r\n```\r\n\r\nI would really appreciate if someone could tell me what's the difference between keras attention and the attention from the tutorial. Thanks\r\n", "comments": ["I would recommend checking out the new MultiHeadAttention layer which should match the transformer paper.\r\nThe Attention layer in keras has a learnable scale variable while the transformer paper uses 1/sqrt(query_dim).", "@thephet Is this resolved for you? \r\n\r\nPlease close the issue if this was resolved for you. thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45268\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45268\">No</a>\n"]}, {"number": 45267, "title": "ERROR: Could not find a version that satisfies the requirement tensorflow==1.2.1 ? pip cannot find tensroflow 1 versions", "body": "I cannot install any tensorflow 1 version in my virtual env.. it is required by a project that I'm trying to run..\r\n\r\n\r\n    pip install tensorflow==1.2.1\r\n    ERROR: Could not find a version that satisfies the requirement tensorflow==1.2.1 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3)\r\n    ERROR: No matching distribution found for tensorflow==1.2.1\r\n\r\nTried things suggested here :\r\n\r\nhttps://stackoverflow.com/questions/42317075/tensorflow-r1-0-could-not-a-find-a-version-that-satisfies-the-requirement-tens\r\n\r\n\r\nmy python version is not old : 3.8.5\r\n\r\nmy pip version is OK : 20.0.2\r\n\r\n\r\ni still CANNOT install tensorflow 1.2.1 ...\r\n\r\n\r\nwhat i gotta do.. I am using ubuntu 20", "comments": ["There is no build for python 3.8 https://pypi.org/project/tensorflow/1.2.1/#files", "@LusKrew\r\n\r\nTensorflow 1.2 is very old. Please, upgrade to T 2.3 and make sure you have all the requiremnts as per the tested build configurations from [here](https://www.tensorflow.org/install/source#cpu).Thanks!\r\n", "@LusKrew ^^", "If you want old versions, you should use the same version of Python that was in use at the time. PyPi page for the package lists which python versions are available.\r\n\r\nClosing since this is not an issue with TF code and it refers to TF versions that are long out of the support window.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45267\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45267\">No</a>\n"]}, {"number": 45266, "title": "tf.gather behavior depends on dtype", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): sort of\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n```\r\na = tf.ones((2, 3), dtype=tf.int32)\r\nb = tf.constant([0, 1])\r\nprint(  tf.gather(a, b, batch_dims=-1).shape )\r\n```\r\nreturns a shape of (2, ) whereas\r\n\r\n```\r\na = tf.ones((2, 3), dtype=tf.int64)\r\nb = tf.constant([0, 1])\r\nprint(  tf.gather(a, b, batch_dims=-1).shape )\r\n```\r\nreturns a shape of (2, 2).\r\n\r\n**Describe the expected behavior**\r\nI'm not 100% which should be the expected behavior of tf.gather, but it seems like it should not depend on the dtype of a.\r\n\r\n**Standalone code to reproduce the issue**\r\nSee above\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\u2013\r\n", "comments": ["@FordUniver,\r\nI did not observe any difference on running the code with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/a23572d7bc9e921b2a9ad02b1f82287c/45266-2-2.ipynb#scrollTo=ilg8TJQUKorm) and [TF v2.3](https://colab.research.google.com/gist/amahendrakar/f0893034cd88e71c350ed8b6c3d4d647/45266.ipynb). With both the versions the shape is `(2, 2)`. Please check the attached gist for reference. \r\n\r\nAlso, could you please try running the code in a virtual environment with TF v2.3 and check if you are facing the same issue? Thanks!", "After testing a bit, it seems like this only occurred for me in one particular, active kernel running in a particular conda environment and that the behavior changed after I restarted the kernel. I'm not sure what caused it in the first place, since the code snippet I posted reloads TensorFlow and defines all its own variables, so it shouldn't just have been caused by me being silly and accidentally reusing some otherwise defined b or having overwritten anything. This self-contained snippet still produced the error when just pasting and running it into its own jupyter notebook cell prior to restarting that particular kernel, but I guess that's not much to go on and the issue can be closed."]}, {"number": 45265, "title": "Add missing arch-specific dependencies to tf_to_kernel", "body": "The tf_to_kernel target is missing some dependencies which shows on e.g. POWER by undefined reference errors\r\nFixes #45104", "comments": ["@mihaimaruseac can you please help with pulling this PR in ?", "I'm worried that so many builds are failing. Can we first rebase this on master and trigger a new test run first?", "@mihaimaruseac rebased without any changes on current master."]}, {"number": 45264, "title": "[CherryPick:r2.4] Revert renaming of tools to exec_tools", "body": "Reverts part of f827c023906e7d30f0e5f2992b111ab34153310a as that causes trouble due to action_env variables not passed through to any dependent build which breaks builds using TF_SYSTEM_LIBS\r\n\r\nCherry-pick of #43156, similar PR #45124 already merged but this one was missed.", "comments": []}, {"number": 45263, "title": "libcusolver.so.10? ", "body": "```\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.74GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-11-30 17:36:59.367782: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-30 17:36:59.369653: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-30 17:36:59.369702: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-30 17:36:59.370388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-30 17:36:59.370548: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-30 17:36:59.370610: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-11-30 17:36:59.371080: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-30 17:36:59.371185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-30 17:36:59.371200: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-11-30 17:36:59.475119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-30 17:36:59.475153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0 \r\n2020-11-30 17:36:59.475162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N \r\n```\r\n\r\nbut we should have\r\n```\r\nlibcusolverMg.so.11 (libc6,x86-64) => /opt/cuda/lib64/libcusolverMg.so.11\r\nlibcusolverMg.so (libc6,x86-64) => /opt/cuda/lib64/libcusolverMg.so\r\nlibcusolver.so.11 (libc6,x86-64) => /opt/cuda/lib64/libcusolver.so.11\r\nlibcusolver.so (libc6,x86-64) => /opt/cuda/lib64/libcusolver.so\r\n```\r\n\r\n", "comments": ["@SchrodingerZhu \r\nYou will need to add cuda, cudnn, cupti to the environment path.\r\nSee https://www.tensorflow.org/install/gpu#linux_setup", "But this should already in my ld conf.\r\n```\r\n\ud83d\udd59[ 13:55:52 ] \u03bb cat /etc/ld.so.conf.d/*.conf\r\n/opt/cuda/lib64\r\n/opt/cuda/nvvm/lib64\r\n/opt/cuda/extras/CUPTI/lib64\r\n/usr/lib/libfakeroot\r\n/opt/intel/mkl/lib/intel64\r\n/opt/intel/mkl/lib/intel64_lin\r\n/opt/intel/compilers_and_libraries_2020.2.254/linux/compiler/lib/intel64_lin\r\n/usr/lib32\r\n/usr/lib/octave/5.2.0\r\n/usr/lib/openmpi\r\n```\r\nMoreover,\r\n```\r\n\ud83d\udd59[ 13:57:31 ] \u03bb env LD_LIBRARY_PATH=/opt/cuda/lib64/ python3\r\nPython 3.8.6 (default, Nov 28 2020, 13:43:41) \r\n[GCC 10.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-12-01 13:58:03.056049: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n>>> tf.test.\r\ntf.test.Benchmark(                  tf.test.create_local_cluster(       tf.test.is_built_with_xla(\r\ntf.test.TestCase(                   tf.test.gpu_device_name(            tf.test.is_gpu_available(\r\ntf.test.assert_equal_graph_def(     tf.test.is_built_with_cuda(         tf.test.main(\r\ntf.test.benchmark_config(           tf.test.is_built_with_gpu_support(  \r\ntf.test.compute_gradient(           tf.test.is_built_with_rocm(         \r\n>>> tf.test.gpu_device_name()\r\n2020-12-01 13:58:15.539168: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-01 13:58:15.541897: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-01 13:58:15.557986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-12-01 13:58:15.638215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 13:58:15.638675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: \r\npciBusID: 0000:0a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.74GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-12-01 13:58:15.638691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-01 13:58:15.673486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-12-01 13:58:15.673575: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-12-01 13:58:15.683915: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-12-01 13:58:15.707195: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-12-01 13:58:15.707289: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/cuda/lib64/\r\n2020-12-01 13:58:15.723204: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-12-01 13:58:15.723901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-12-01 13:58:15.723916: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-12-01 13:58:15.797186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-01 13:58:15.797211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0 \r\n2020-12-01 13:58:15.797218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N \r\n''\"\r\n```\r\n\r\n--------\r\nSome new findings:\r\n\r\nI am using Arch Linux. I found that the official package delivered in arch's repo (version `2.3.0`) is compatible with `CUDA 11` and `Python 3.9` and using that the device name is detected correctly.\r\n\r\nHowever, in my `pipenv`, I used `Python 3.8` and `tf-nightly(-gpu)` installed from `pypi` but it just failed with the above output.", "In my global env with the versions delivered in Arch Official:\r\n```\r\nPython 3.9.0 (default, Oct  7 2020, 23:09:01) \r\n[GCC 10.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-12-01 14:02:30.312027: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n>>> tf.test.gpu_device_name()\r\n2020-12-01 14:02:34.584965: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-01 14:02:34.603043: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 4000199999 Hz\r\n2020-12-01 14:02:34.604630: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564b2fe03990 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-12-01 14:02:34.604659: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-12-01 14:02:34.606204: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-12-01 14:02:34.693722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 14:02:34.694203: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564b2fd48890 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-12-01 14:02:34.694214: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-12-01 14:02:34.703648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 14:02:34.704054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:0a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.74GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-12-01 14:02:34.704083: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-01 14:02:34.705639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-12-01 14:02:34.706268: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-12-01 14:02:34.706423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-12-01 14:02:34.740611: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-12-01 14:02:34.741244: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-12-01 14:02:34.741347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-12-01 14:02:34.741437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 14:02:34.741892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 14:02:34.742279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-12-01 14:02:34.742299: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-01 14:02:36.464604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-01 14:02:36.464638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-12-01 14:02:36.464644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-12-01 14:02:36.464835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 14:02:36.465293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-01 14:02:36.465795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 9728 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:0a:00.0, compute capability: 7.5)\r\n'/device:GPU:0'\r\n```", "@SchrodingerZhu Looks like there is mix of `CUDA8.0`, `CUDA10.0` and `CUDA11.0`.\r\n\r\n```\r\nSuccessfully opened dynamic library libcudart.so.11.0\r\n2020....  Successfully opened dynamic library libcublas.so.11\r\n2020..... Successfully opened dynamic library libcufft.so.10\r\n2020..... Successfully opened dynamic library libcurand.so.10\r\n2020..... Successfully opened dynamic library libcusolver.so.11\r\n2020..... Successfully opened dynamic library libcusparse.so.11\r\n2020.... Successfully opened dynamic library libcudnn.so.8\r\n```\r\n\r\nCan you please confirm whether you removed old versions completely before install `CUDA11.0`? Thanks!", "ops. that maybe the problem. but I am completely rely on the Arch Repo:\r\n```\r\ncommunity/cuda 11.1.1-1 [installed]\r\ncommunity/cudnn 8.0.5.39-1 [installed]\r\n```\r\nSure enough, there is some libraries ends with `.10`:\r\n```\r\n/opt/cuda/nsight_systems/target-linux-armv8/libcupti-sbsa.so.10.2\r\n/opt/cuda/nsight_systems/target-linux-armv8/libcupti-tegra.so.10.0\r\n/opt/cuda/nsight_systems/target-linux-armv8/libcupti-tegra.so.10.1\r\n/opt/cuda/nsight_systems/target-linux-armv8/libcupti-tegra.so.10.2\r\n/opt/cuda/nsight_systems/target-linux-x64/libcupti.so.10.0\r\n/opt/cuda/nsight_systems/target-linux-x64/libcupti.so.10.1\r\n/opt/cuda/nsight_systems/target-linux-x64/libcupti.so.10.2\r\n/opt/cuda/targets/x86_64-linux/lib/libcufft.so.10\r\n/opt/cuda/targets/x86_64-linux/lib/libcufft.so.10.3.0.105\r\n/opt/cuda/targets/x86_64-linux/lib/libcufftw.so.10\r\n/opt/cuda/targets/x86_64-linux/lib/libcufftw.so.10.3.0.105\r\n/opt/cuda/targets/x86_64-linux/lib/libcurand.so.10\r\n/opt/cuda/targets/x86_64-linux/lib/libcurand.so.10.2.2.105\r\n```\r\nMaybe it is the OS problem?", "@SchrodingerZhu Can you please try to uninstall, restart the system, reinstall and check whether the issue was resolved or not. thanks!", "It looks like similar to #44312. Even I removed all CUDA 10.1 and 10.2 packages and install 11.1, I got same error.\r\n\r\nI fix this issue by installing `libcusolver10` using `apt` in Ubuntu 20.04, but not sure this is right solution", "I'm also facing this issue, however I doubt I have much to bring to the discussion as I also run ArchLinux and hence the same environment as @SchrodingerZhu. Just \"bumping\" to show interest.", "> ops. that maybe the problem. but I am completely rely on the Arch Repo:\r\n> \r\n> ```\r\n> community/cuda 11.1.1-1 [installed]\r\n> community/cudnn 8.0.5.39-1 [installed]\r\n> ```\r\n> \r\n> Sure enough, there is some libraries ends with `.10`:\r\n> \r\n> ```\r\n> /opt/cuda/nsight_systems/target-linux-armv8/libcupti-sbsa.so.10.2\r\n> /opt/cuda/nsight_systems/target-linux-armv8/libcupti-tegra.so.10.0\r\n> /opt/cuda/nsight_systems/target-linux-armv8/libcupti-tegra.so.10.1\r\n> /opt/cuda/nsight_systems/target-linux-armv8/libcupti-tegra.so.10.2\r\n> /opt/cuda/nsight_systems/target-linux-x64/libcupti.so.10.0\r\n> /opt/cuda/nsight_systems/target-linux-x64/libcupti.so.10.1\r\n> /opt/cuda/nsight_systems/target-linux-x64/libcupti.so.10.2\r\n> /opt/cuda/targets/x86_64-linux/lib/libcufft.so.10\r\n> /opt/cuda/targets/x86_64-linux/lib/libcufft.so.10.3.0.105\r\n> /opt/cuda/targets/x86_64-linux/lib/libcufftw.so.10\r\n> /opt/cuda/targets/x86_64-linux/lib/libcufftw.so.10.3.0.105\r\n> /opt/cuda/targets/x86_64-linux/lib/libcurand.so.10\r\n> /opt/cuda/targets/x86_64-linux/lib/libcurand.so.10.2.2.105\r\n> ```\r\n> \r\n> Maybe it is the OS problem?\r\n\r\nJust as a FYI, those files are a part of the current `cuda` package, hence they are at least not 'leftover' files from a previous installation (however it might be wrong to include them anyways?).\r\n\r\n```\r\n>>> pacman -Ql cuda | rg '\\.so\\.10'\r\ncuda /opt/cuda/nsight_systems/target-linux-armv8/libcupti-sbsa.so.10.2\r\ncuda /opt/cuda/nsight_systems/target-linux-armv8/libcupti-tegra.so.10.0\r\ncuda /opt/cuda/nsight_systems/target-linux-armv8/libcupti-tegra.so.10.1\r\ncuda /opt/cuda/nsight_systems/target-linux-armv8/libcupti-tegra.so.10.2\r\ncuda /opt/cuda/nsight_systems/target-linux-x64/libcupti.so.10.0\r\ncuda /opt/cuda/nsight_systems/target-linux-x64/libcupti.so.10.1\r\ncuda /opt/cuda/nsight_systems/target-linux-x64/libcupti.so.10.2\r\ncuda /opt/cuda/targets/x86_64-linux/lib/libcufft.so.10\r\ncuda /opt/cuda/targets/x86_64-linux/lib/libcufft.so.10.4.0.72\r\ncuda /opt/cuda/targets/x86_64-linux/lib/libcufftw.so.10\r\ncuda /opt/cuda/targets/x86_64-linux/lib/libcufftw.so.10.4.0.72\r\ncuda /opt/cuda/targets/x86_64-linux/lib/libcurand.so.10\r\ncuda /opt/cuda/targets/x86_64-linux/lib/libcurand.so.10.2.3.68\r\n```", "I can write confidently to solve this issue.\r\nThe issue has two focuses, with the first one being **CUDA components versions**, and the second being **libcusolver.so.10 error**.\r\n\r\n**CUDA components versions**\r\nThe is some asynchronous relationship between CUDA versions and their components versions.\r\nIf you use TensorFlow 2.4, you probably use CUDA 11.0. Check the CUDA 11.0 release document. You can find the following list.\r\n\r\n> This release of the toolkit includes the following updates:\r\n> \r\n> CUDA Math libraries toolchain uses C++11 features, and a C++11-compatible standard library is required on the host.\r\n> cuBLAS 11.0.0\r\n> cuFFT 10.1.3\r\n> cuRAND 10.2.0\r\n> cuSPARSE 11.0.0\r\n> cuSOLVER 10.4.0\r\n> NPP 11.0.0\r\n> nvJPEG 11.0.0\r\n\r\nThe above list concludes that CUDA 11.0 does contain some components with version 10.\r\nBTW, from CUDA 11.1, you will find cusolver.so version above 11.\r\n\r\n**libcusolver.so.10 error**\r\nThis is an error caused by the environment variable **LD_LIBRARY_PATH**.\r\nIf you use a terminal, you have probably already set the LD_LIBRARY_PATH, with /usr/local/cuda-11.0/lib64 in it.\r\nIf you use PyCharm or other IDEs, you need to check environment variables as well.\r\nWith environment variables being just the default one: PYTHONUNBUFFERED=1, you will get:\r\n`W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory`\r\nWith environment variables being **LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64:/usr/local/lib**, you will get:\r\n`I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10`\r\n", "@xidchen 's assessment is correct.", "@xidchen is indeed correct.\r\n", "So, I think this issue can be closed. #44312 also.\r\n\r\n", "sorry for the delay. I just returned and tested. I think the current discussion is already enough for solving the issue.", "Closing as [requested](https://github.com/tensorflow/tensorflow/issues/45263#issuecomment-751933452) Please reopen if it's a mistake", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45263\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45263\">No</a>\n", "> It looks like similar to #44312. Even I removed all CUDA 10.1 and 10.2 packages and install 11.1, I got same error.\r\n> \r\n> I fix this issue by installing `libcusolver10` using `apt` in Ubuntu 20.04, but not sure this is right solution\r\n\r\nI cannot install libcusolver10 through apt, any idea?\r\n", "For me, I actually had `libcusolver.so.11` available because I am using CUDA 11.1.  For TensorFlow 2.4.1 (as of this date), it still refers to `libcusolver.so.10`.  I solved this by making a symbolic link in my `/usr/local/cuda-11.1/lib64` directory so that version 10 points to version 11:\r\n\r\n```\r\nsudo ln -s /usr/local/cuda-11.1/lib64/libcusolver.so.11 /usr/local/cuda-11.1/lib64/libcusolver.so.10\r\n```", "> For me, I actually had `libcusolver.so.11` available because I am using CUDA 11.1. For TensorFlow 2.4.1 (as of this date), it still refers to `libcusolver.so.10`. I solved this by making a symbolic link in my `/usr/local/cuda-11.1/lib64` directory so that version 10 points to version 11:\r\n> \r\n> ```\r\n> sudo ln -s /usr/local/cuda-11.1/lib64/libcusolver.so.11 /usr/local/cuda-11.1/lib64/libcusolver.so.10\r\n> ```\r\n\r\nI also have the same issue with CUDA v11.1. I tried TensorFlow v2.5,0-rc1 and it's working just fine. \r\nI believe that the problem is that TensorFlow v2.4.1 links only to CUDA 11.0", "@alealv that's good to know! ", "I also had a similar experience to @alealv \r\n\r\nAfter figuring out nvidia driver issues between Arch and Manjaro's preferred nvidia drivers, explicitly upgrading to 2.5.0 rc2 worked for me after making sure to uninstall any previous tensorflow or tensorflow-gpu installations. "]}, {"number": 45262, "title": "make xtensa downloader generic to support variants", "body": "Change the download script so that we can support multiple xtensa target variants. The functionality is identical to https://github.com/tensorflow/tensorflow/pull/44845 but the script is ready for additional download variants.\r\n\r\nTested the following commands:\r\n\r\n1. Incorrect params to the download script\r\n    ```\r\n    $ tensorflow/lite/micro/tools/make/ext_libs/xtensa_download.sh /tmp blah\r\n    ```\r\n    Gave the expected error:\r\n    ```\r\n    Attempting to download an unsupported xtensa variant: blah\r\n    ```\r\n\r\n1. Correct invocation for hifi4 \r\n    ```\r\n    $ tensorflow/lite/micro/tools/make/ext_libs/xtensa_download.sh /tmp hifi4\r\n    ```\r\n    Downloaded the library to `/tmp/xa_nnlib_hifi4`.\r\n\r\n1. Using the makefile\r\n    ```\r\n    rm -rf tensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/\r\n    make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa XTENSA_CORE=blah XTENSA_BASE=blah XTENSA_TOOLS_VERSION=blah TARGET_ARCH=hifi4 clean\r\n    ```\r\n    Downloaded the library and printed out the sources list.\r\n\r\n1. Using the makefile after already downloading the library\r\n    ```\r\n    make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa XTENSA_CORE=blah XTENSA_BASE=blah XTENSA_TOOLS_VERSION=blah TARGET_ARCH=hifi4 clean\r\n    ```\r\n    Correctly determined that the downloaded file already exists (`tensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4 already exists, skipping the download.`) and prints out the sources list on the terminal.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45262) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\r\n\r\nRegards\r\npravin\r\n\r\n\r\nFrom: google-cla[bot] <notifications@github.com>\r\nSent: 30 November 2020 12:25 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Pravin Gangadhar Karandikar <kpraving@cadence.com>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] make xtensa downloader generic to support variants (#45262)\r\n\r\nEXTERNAL MAIL\r\n\r\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n\r\n\ud83d\udcdd Please visit https://cla.developers.google.com/<https://urldefense.com/v3/__https:/cla.developers.google.com/__;!!EHscmS1ygiU1lA!Rmgr56B8Pf89QLSAzxSHoaZ3q7I4jCA5vW6hMh2CtieOkYo7YWtSdR-DNHvbSkKN$> to sign.\r\n\r\nOnce you've signed (or fixed any issues), please reply here with @googlebot I signed it! and we'll verify it.\r\n\r\n________________________________\r\nWhat to do if you already signed the CLA\r\nIndividual signers\r\n\r\n  *   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data<https://urldefense.com/v3/__https:/cla.developers.google.com/clas__;!!EHscmS1ygiU1lA!Rmgr56B8Pf89QLSAzxSHoaZ3q7I4jCA5vW6hMh2CtieOkYo7YWtSdR-DNLrPPUnI$> and verify that your email is set on your git commits<https://urldefense.com/v3/__https:/help.github.com/articles/setting-your-email-in-git/__;!!EHscmS1ygiU1lA!Rmgr56B8Pf89QLSAzxSHoaZ3q7I4jCA5vW6hMh2CtieOkYo7YWtSdR-DNPVzlJCr$>.\r\n\r\nCorporate signers\r\n\r\n  *   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot<https://urldefense.com/v3/__http:/go/cla*troubleshoot__;Iw!!EHscmS1ygiU1lA!Rmgr56B8Pf89QLSAzxSHoaZ3q7I4jCA5vW6hMh2CtieOkYo7YWtSdR-DNMQhosVe$> (Public version<https://urldefense.com/v3/__https:/opensource.google/docs/cla/*troubleshoot__;Iw!!EHscmS1ygiU1lA!Rmgr56B8Pf89QLSAzxSHoaZ3q7I4jCA5vW6hMh2CtieOkYo7YWtSdR-DNGkciDLB$>).\r\n  *   The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data<https://urldefense.com/v3/__https:/cla.developers.google.com/clas__;!!EHscmS1ygiU1lA!Rmgr56B8Pf89QLSAzxSHoaZ3q7I4jCA5vW6hMh2CtieOkYo7YWtSdR-DNLrPPUnI$> and verify that your email is set on your git commits<https://urldefense.com/v3/__https:/help.github.com/articles/setting-your-email-in-git/__;!!EHscmS1ygiU1lA!Rmgr56B8Pf89QLSAzxSHoaZ3q7I4jCA5vW6hMh2CtieOkYo7YWtSdR-DNPVzlJCr$>.\r\n  *   The email used to register you as an authorized contributor must also be attached to your GitHub account<https://urldefense.com/v3/__https:/github.com/settings/emails__;!!EHscmS1ygiU1lA!Rmgr56B8Pf89QLSAzxSHoaZ3q7I4jCA5vW6hMh2CtieOkYo7YWtSdR-DNG7PEsSg$>.\r\n\r\n\u2139\ufe0f Googlers: Go here<https://urldefense.com/v3/__https:/goto.google.com/prinfo/https*3A*2F*2Fgithub.com*2Ftensorflow*2Ftensorflow*2Fpull*2F45262__;JSUlJSUlJQ!!EHscmS1ygiU1lA!Rmgr56B8Pf89QLSAzxSHoaZ3q7I4jCA5vW6hMh2CtieOkYo7YWtSdR-DNEYY3rfY$> for more info.\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://urldefense.com/v3/__https:/github.com/tensorflow/tensorflow/pull/45262*issuecomment-735590635__;Iw!!EHscmS1ygiU1lA!Rmgr56B8Pf89QLSAzxSHoaZ3q7I4jCA5vW6hMh2CtieOkYo7YWtSdR-DNPnM6rQW$>, or unsubscribe<https://urldefense.com/v3/__https:/github.com/notifications/unsubscribe-auth/AROTIS2QCB6JTPN2GS373CTSSM6S7ANCNFSM4UHEWM7Q__;!!EHscmS1ygiU1lA!Rmgr56B8Pf89QLSAzxSHoaZ3q7I4jCA5vW6hMh2CtieOkYo7YWtSdR-DNNG9MQkv$>.\r\n", "Thanks @kpraving. I have pushed a commit that makes the following changes:\r\n\r\n1. moves the URL, MD5 etc to the bash script from the Makefile so that we can run the download script more easily.\r\n1. Reduced the number of parameters to the script at the cost of duplicating the downloaded folder name (`xa_nnlib_hifi4`) across the bash script and the makefile.\r\n1. having the downloaded zip folder in /tmp be also named according to the variant being downloaded.\r\n\r\nIf you can review and comment here that the changes look good to you then we can get this merged.\r\n\r\nNote that since I directly pushed to your branch, you should `git pull` before making any further changes locally.", "@advaitjain, We added TARGET_ARCH dependency in both files. We could just keep dependency in 'xtensa.inc' and keep 'xtensa_download.sh' generic. But this works as well. Changes look good to me. Thanks.", "> @advaitjain, We added TARGET_ARCH dependency in both files. We could just keep dependency in 'xtensa.inc' and keep 'xtensa_download.sh' generic. But this works as well. Changes look good to me. Thanks.\r\n\r\nThanks. Let's go with this for now. I have a preference for being able to run the script easily independent of the makefile (even though it means more duplication)."]}, {"number": 45261, "title": "ReduceLROnPlateau forced casting on integer learning rates", "body": "**Issue in `ReduceLROnPlateau`**\r\n- `ReduceLROnPlateau` relies of the function: `Keras.set_value`\r\n- When using `ReduceLROnPlateau`, this ensures the reduced LR is of the same type as input LR\r\n- This is an issue if the learning rate is specified as an integer. I.e. different behaviour for LR = 1 and LR = 1.0\r\n\r\n**Describe the current behavior**\r\nExample 1: Initial learning rate 1, factor 0.5. First reduction should drop to 0.5.\r\nExample 2:Initial learning rate 5, factor 0.5. First reduction should drop to 2.5.\r\n\r\n**Describe the expected behavior**\r\nExample 1: First reduction drops to 0.\r\nExample 2: First reduction drops to 2.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\r\n\r\nn = 100\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Dense(n, activation='linear')\r\n\r\nmodel.compile(optimizer = keras.optimizers.Adam(lr=1), loss =  ['mse'])\r\n\r\nx = np.random.rand(2, n)\r\ny = np.random.rand(2, n)\r\n\r\nreduce_lr = ReduceLROnPlateau(monitor='loss', min_lr=1e-6, patience = 1, factor = 0.1, verbose = 1)\r\n\r\nhistory = model.fit(x, y, epochs = 10, callbacks = [reduce_lr,], verbose = 0)\r\n\r\nhistory.history['lr']\r\n```\r\n**Outputs**\r\nEpoch 00002: ReduceLROnPlateau reducing learning rate to 0.1.\r\n\r\n\r\n[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\r\n", "comments": ["I have tried in colab with TF version 2.4, nightly version(`2.5.0-dev20201129`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/6eeb6765af29ffb7cd0e3519be0473d4/untitled550.ipynb).Thanks!", "@alipwong Thanks for the clear report. I can reproduce the issue. However, the `learning_rate` is supposed to be a floating value as mentioned in the [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam). Once we change learning rate from int to float, everything works as expected. [Here](https://colab.research.google.com/gist/jvishnuvardhan/bf1a3247a19ab898ded29bb8975a7efe/untitled550.ipynb) is the gist for our reference.\r\n\r\nI am not sure whether it is intentional or not. I see that `k.set_value` used in other callbacks also. we will look into it. Thanks!\r\n\r\n@omalleyt12 Any comments on the usage of `k.set_value`? Is this intended behavior? Thanks!", "@alipwong  Could you please respond t @jvishnuvardhan comment.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45261\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45261\">No</a>\n"]}, {"number": 45260, "title": "[Language Fix] Change BlackBox to ClosedBox", "body": "Fixing a borderline-racist term in the Tensorflow repository.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45260) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\r\n"]}, {"number": 45259, "title": "SeparableConv documention missing argument constraint", "body": "The documentation of the SeparableConv layers does not mention that the strides must be equal length.\r\n\r\nTrying to use different lengths produces an InvalidArgumentError: Current implementation only supports equal length strides in the row and column dimensions.", "comments": ["@rschumi0 \r\n\r\nCan you please refer the documentation link you are referring to. Thanks!", "It concerns both these layers:\r\n[https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv1D)\r\n[https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv2D)", "@rschumi0 A PR raised to correct this documentation. Will closed the issue when the PR gets merged. thanks!", "@rschumi0 I am closing this issue as the related PR https://github.com/tensorflow/tensorflow/pull/48250 got merged. Thanks!"]}, {"number": 45258, "title": "Unable to install CUDA 11.1 and cuDNN 8.05 for tensorflow 2.3", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.3\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1 and 8.05\r\n- GPU model and memory: GeForce GTX 1650 4GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nUnable to detect GPU\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nInstalled tensorflow 2.3\r\nInstalled CUDA 11.1\r\nDownloaded cuDNN 8.05\r\nadded files from cuDNN to the respective folders in CUDA directory\r\nadded CUDA to the path\r\nimported tensorflow-gpu in python shows error that gpu not found\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["TensorFlow 2.3 requires CUDA 10.1. CUDA 11.0 becomes supported with TensorFlow 2.4.\r\n\r\nWith Win10 you can easily install CUDA 10.1 in addition to your current CUDA 11.1 simply by installing CUDA 10.1 on top of current setup but make sure to **not** install any drivers by doing it (CUDA 10.1 is compatible with the latest drivers).\r\n\r\nAfter that make sure your CUDA-related system environment variables point to the 10.1 version.\r\n\r\nWould this fix the error?", "@anirudhakulkarni \r\nPlease refer to this comment on [cuda 11.1](https://github.com/tensorflow/tensorflow/issues/44159#issuecomment-735542190), could you please use nightly and let us know.", "@ahtik  thanks for the response. This should work I guess. But I wanted to know if I could use CUDA 11.1\r\n@Saduf2019 thanks for the response. I tried the nightly:\r\nit said:\r\ntf-nightly-gpu 2.5.0.dev20201128 requires numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\r\nand uninstalled my previous numpy 1.19.4 and installed 1.18.5\r\nBut still when I tried to check if its working by:\r\n`import tensorflow as tf`\r\n`print(tf.test.is_gpu_available())`\r\nit throws:\r\nWARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2020-11-30 19:19:10.581054: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-30 19:19:10.593149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-11-30 19:19:11.267760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-11-30 19:19:11.268049: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-30 19:19:11.752064: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-30 19:19:11.752305: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-30 19:19:12.020271: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-30 19:19:12.055871: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-11-30 19:19:12.058268: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-11-30 19:19:12.266522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-11-30 19:19:12.289685: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-30 19:19:12.289912: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-11-30 19:19:13.228085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-30 19:19:13.228375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0\r\n2020-11-30 19:19:13.229647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N\r\n2020-11-30 19:19:13.232971: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nFalse", "@anirudhakulkarni \r\nPlease confirm if you have used cuda 11 and not 11.1", "Its 11.1. I mentioned it 11.1 everywhere", "@anirudhakulkarni Your nightly build with Cuda 11.1 \"mostly\" works (the only error is cusolver not being found).\r\n\r\nIn our system the following works, feel free to try this setup and let me know:\r\n1.  I have updated to CUDA 11.1.1 (it's an in-place update for the existing 11.1). Reiterating - whenever installing a new or older CUDA, *always* choose Advanced mode to exclude any driver installation.\r\n2. Latest cudnn has been downloaded to `C:\\cudnn-8.0.5\\11.1\\`\r\nThe following works (env paths are for our own internal use for being able to easily switch between CUDA installations):\r\n```\r\n\\python38\\python.exe -m venv \\p\\venv-tf-n\r\n\\p\\venv-tf-n\\Scripts\\activate\r\npython -m pip install -U pip\r\npip install tf-nightly-gpu==2.5.0.dev20201130 numpy==1.19.3\r\n\r\nSET CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\r\nSET PATH=C:\\cudnn-8.0.5\\11.1\\bin;%CUDA_PATH%\\bin;%CUDA_PATH%\\libnvvp;%PATH%\r\npython -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])));print(tf.config.list_physical_devices('GPU'))\"\r\n```", "thank you @ahtik for help.\r\nJust to confirm :\r\n1. `\\python38\\python.exe -m venv \\p\\venv-tf-n`\r\n`\\p\\venv-tf-n\\Scripts\\activate` \r\nthese 2 lines didn't work on my system so I didn't create virtual environment.\r\n2. I downloaded the cudnn again. It was named \"cudnn-11.1-windows-x64-v8.0.5.39\" so to follow your instructions I copied it to C drive and renamed it as cudnn-8.0.5. Inside the downloaded folder, there was a folder by name cuda and I renamed it as 11.1 so as to match with the path you mentioned. So output of ls command is\r\n`C:\\cudnn-8.0.5\\11.1>ls`\r\n`NVIDIA_SLA_cuDNN_Support.txt  bin  include  lib`\r\nResult:\r\nSame didn't work.\r\n2020-12-02 12:29:29.506366: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-02 12:29:31.895115: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-02 12:29:31.896303: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-02 12:29:31.918460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-12-02 12:29:31.918715: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-02 12:29:31.927910: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-02 12:29:31.928266: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-02 12:29:31.935820: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-02 12:29:31.938315: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-02 12:29:31.939781: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-12-02 12:29:31.963246: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-02 12:29:31.964109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-02 12:29:31.964623: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-12-02 12:29:31.965561: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-02 12:29:31.969817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-02 12:29:31.969998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]\r\n2020-12-02 12:29:31.970462: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\ntf.Tensor(990.16626, shape=(), dtype=float32)\r\n[]\r\n\r\nAfter this I reinstalled CUDA 11.1 (without drivers. I already have NVIDIA studio driver 457.30 ) and restarted system but still no luck. same error message", "@anirudhakulkarni Oh, I just noticed that my PATH was already \"littered\" with the CUDA 11.0 paths, so the CUDA 11.1 took precedence but cusolver64_10.dll was still taken from the CUDA 11.0 dir.\r\n\r\nThis explains why it first worked for me. You're also saving yourself time by additionally installing CUDA 11.0 and making sure it is being used. Having CUDA 11.0 should make it possible to use both `tensorflow==2.4.0rc3` and nightly. For the latest \"stable\" release `tensorflow==2.3.1` you'd need CUDA 10.1.\r\n\r\nMy cudnn instruction might have been too involved, could be easier to just extract the contents to the CUDA dir as instructed in docs. I just prefer to have a bit more control over deployment and need multiple cuda&cudnn combinations.\r\n\r\nIf  you install CUDA 11.0 then does it work with `tensorflow==2.4.0rc3` and/or `tf-nightly-gpu==2.5.0.dev20201201`?\r\n\r\nThe choice of \"primary\"/active CUDA is only taken from the Windows env variables.\r\n\r\nFor CUDA 11.0 I'm using `cudnn-8.0.4.30`.\r\n\r\nPS. My venv commands were expecting python to be installed in c:\\python38\\ dir.\r\n\r\nAdding my console output for reference.\r\n```\r\n\\p\\venv-tf-n\\Scripts\\activate\r\n\r\n(venv-tf-n) C:\\Users\\ak>SET CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\r\n\r\n(venv-tf-n) C:\\Users\\ak>SET PATH=C:\\cudnn-8.0.5\\11.1\\bin;%CUDA_PATH%\\bin;%CUDA_PATH%\\libnvvp;%PATH%\r\n\r\n(venv-tf-n) C:\\Users\\ak>python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])));print(tf.config.list_physical_devices('GPU'))\"\r\n2020-12-02 10:17:27.383440: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-02 10:17:30.824838: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-02 10:17:30.826042: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-02 10:17:30.856819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 SUPER computeCapability: 7.5\r\ncoreClock: 1.8GHz coreCount: 40 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-02 10:17:30.856915: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-02 10:17:31.243185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-02 10:17:31.243282: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-02 10:17:31.465292: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-02 10:17:31.492316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-02 10:17:31.716494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-02 10:17:31.891929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-02 10:17:32.928875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-02 10:17:32.929293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\r\n2020-12-02 10:17:32.931569: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-02 10:17:32.932795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 SUPER computeCapability: 7.5\r\ncoreClock: 1.8GHz coreCount: 40 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-02 10:17:32.932929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-02 10:17:32.933032: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-02 10:17:32.933130: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-02 10:17:32.933226: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-02 10:17:32.933298: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-02 10:17:32.933356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-02 10:17:32.933409: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-02 10:17:32.933471: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-02 10:17:32.933554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\r\n2020-12-02 10:17:33.866419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-02 10:17:33.866513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0\r\n2020-12-02 10:17:33.866770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N\r\n2020-12-02 10:17:33.868306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6611 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-12-02 10:17:33.869560: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\ntf.Tensor(914.61194, shape=(), dtype=float32)\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n```", "@anirudhakulkarni\r\n Please update, is this still an issue", " Sorry, @ahtik  @Saduf2019  for the delayed response.\r\nNo, it didn't work\u2014same error.\r\nThis time I am attaching some relevant info too.\r\nThe output of the pip list:\r\n~~~\r\nPackage                  Version\r\n------------------------ -------------------\r\nabsl-py                  0.10.0\r\nargcomplete              1.10.0\r\nastroid                  2.4.1\r\nastunparse               1.6.3\r\natomicwrites             1.4.0\r\nattrs                    19.3.0\r\nbackcall                 0.1.0\r\nbeautifulsoup4           4.8.0\r\nbleach                   3.1.5\r\ncachetools               4.1.1\r\ncertifi                  2020.4.5.1\r\ncffi                     1.14.3\r\nchardet                  3.0.4\r\ncolorama                 0.4.3\r\ncryptography             3.1.1\r\ncssselect                1.1.0\r\ncycler                   0.10.0\r\nCython                   0.29.21\r\ndataclasses              0.6\r\ndecorator                4.4.2\r\ndefusedxml               0.6.0\r\ndill                     0.3.2\r\ndistro                   1.5.0\r\ndlx                      1.0.4\r\ndocplex                  2.15.194\r\ndocx2txt                 0.8\r\nEasyProcess              0.3\r\nEbookLib                 0.17.1\r\nentrypoint2              0.2.1\r\nentrypoints              0.3\r\nextract-msg              0.23.1\r\nfastdtw                  0.3.4\r\nfastjsonschema           2.14.5\r\nflatbuffers              1.12\r\nfuture                   0.18.2\r\ngast                     0.3.3\r\ngoogle-auth              1.21.3\r\ngoogle-auth-oauthlib     0.4.1\r\ngoogle-pasta             0.2.0\r\ngrpcio                   1.32.0\r\nh5py                     2.10.0\r\nidna                     2.9\r\nimageio                  2.8.0\r\nIMAPClient               2.1.0\r\ninflection               0.5.1\r\nipykernel                5.3.0\r\nipython                  7.15.0\r\nipython-genutils         0.2.0\r\nisort                    4.3.21\r\njavaobj-py3              0.4.1\r\njedi                     0.17.0\r\nJinja2                   2.11.2\r\njoblib                   0.16.0\r\njson5                    0.9.5\r\njsonschema               3.2.0\r\njupyter-client           6.1.3\r\njupyter-core             4.6.3\r\njupyterlab               2.1.4\r\njupyterlab-server        1.1.5\r\nKeras-Preprocessing      1.1.2\r\nkeyboard                 0.13.5\r\nkiwisolver               1.2.0\r\nlazy-object-proxy        1.4.3\r\nlxml                     4.5.2\r\nMarkdown                 3.2.2\r\nMarkupSafe               1.1.1\r\nmatplotlib               3.2.1\r\nmccabe                   0.6.1\r\nmistune                  0.8.4\r\nmore-itertools           8.4.0\r\nmosspy                   1.0.8\r\nMouseInfo                0.1.3\r\nmpmath                   1.1.0\r\nmss                      6.0.0\r\nmultitasking             0.0.9\r\nnbconvert                5.6.1\r\nnbformat                 5.0.6\r\nnest-asyncio             1.4.0\r\nnetworkx                 2.5\r\nnotebook                 6.0.3\r\nntlm-auth                1.5.0\r\nnumpy                    1.19.3\r\noauthlib                 3.1.0\r\nolefile                  0.46\r\nopencv-python            4.4.0.42\r\nopt-einsum               3.3.0\r\npackaging                20.4\r\npandas                   1.0.4\r\npandocfilters            1.4.2\r\nparsel                   1.6.0\r\nparso                    0.7.0\r\npdfminer                 20191125\r\npdfminer.six             20181108\r\npickleshare              0.7.5\r\nPillow                   7.1.2\r\npip                      20.3.1\r\npluggy                   0.13.1\r\nply                      3.11\r\nprometheus-client        0.8.0\r\nprompt-toolkit           3.0.5\r\nprotobuf                 3.13.0\r\npsutil                   5.7.2\r\npy                       1.8.2\r\npyasn1                   0.4.8\r\npyasn1-modules           0.2.8\r\nPyAutoGUI                0.9.50\r\npybind11                 2.5.0\r\npycparser                2.20\r\npycryptodome             3.9.8\r\nPyGetWindow              0.0.8\r\nPygments                 2.6.1\r\npylint                   2.5.2\r\nPyMsgBox                 1.0.8\r\npyparsing                2.4.7\r\nPyPDF2                   1.26.0\r\npyperclip                1.8.0\r\nPyRect                   0.1.4\r\npyrsistent               0.16.0\r\npyscreenshot             2.2\r\nPyScreeze                0.1.26\r\npytest                   5.4.3\r\npython-constraint        1.4.0\r\npython-dateutil          2.8.1\r\npython-pptx              0.6.18\r\npytube                   9.6.0\r\npytube3                  9.6.4\r\nPyTweening               1.0.3\r\npytz                     2020.1\r\npywin32                  227\r\npywinpty                 0.5.7\r\nPyYAML                   5.3.1\r\npyzmq                    19.0.1\r\nqiskit                   0.21.0\r\nqiskit-aer               0.6.1\r\nqiskit-aqua              0.7.5\r\nqiskit-ibmq-provider     0.9.0\r\nqiskit-ignis             0.4.0\r\nqiskit-terra             0.15.2\r\nqsharp                   0.12.2007.2031\r\nQuandl                   3.5.2\r\nrequests                 2.23.0\r\nrequests-ntlm            1.1.0\r\nrequests-oauthlib        1.3.0\r\nretworkx                 0.5.0\r\nrsa                      4.6\r\nscikit-learn             0.23.2\r\nscipy                    1.4.1\r\nselenium                 3.141.0\r\nSend2Trash               1.5.0\r\nsetuptools               47.1.0\r\nsix                      1.15.0\r\nsklearn                  0.0\r\nsortedcontainers         2.2.2\r\nsoupsieve                2.0.1\r\nSpeechRecognition        3.8.1\r\nsympy                    1.6.2\r\ntabula                   1.0.5\r\ntabula-py                2.2.0\r\ntb-nightly               2.5.0a20201128\r\ntensorboard              2.4.0\r\ntensorboard-plugin-wit   1.7.0\r\ntensorboardX             2.1\r\ntensorflow               2.4.0rc3\r\ntensorflow-estimator     2.4.0rc0\r\ntensorflow-gpu           2.3.1\r\ntensorflow-gpu-estimator 2.3.0\r\ntermcolor                1.1.0\r\nterminado                0.8.3\r\ntestpath                 0.4.4\r\ntextract                 1.6.3\r\ntf-estimator-nightly     2.4.0.dev2020102301\r\ntf-nightly-gpu           2.5.0.dev20201130\r\nthreadpoolctl            2.1.0\r\ntoml                     0.10.1\r\ntorch                    1.7.0+cpu\r\ntorchaudio               0.7.0\r\ntorchvision              0.8.1+cpu\r\ntornado                  6.0.4\r\ntraitlets                4.3.3\r\ntyping-extensions        3.7.4.3\r\ntzlocal                  1.5.1\r\nurllib3                  1.25.9\r\nw3lib                    1.22.0\r\nwcwidth                  0.2.3\r\nwebencodings             0.5.1\r\nwebsockets               8.1\r\nWerkzeug                 1.0.1\r\nwheel                    0.35.1\r\nwrapt                    1.12.1\r\nxlrd                     1.2.0\r\nXlsxWriter               1.3.6\r\nyfinance                 0.1.54\r\n~~~\r\nEnvironment variables:\r\n![image](https://user-images.githubusercontent.com/29736196/101255888-2d775b00-3745-11eb-89a4-6a6b7483f821.png)\r\n\r\nFinal output for ```python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])));print(tf.config.list_physical_devices('GPU'))\"``` :\r\n~~~\r\nC:\\Users\\91930>python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])));print(tf.config.list_physical_devices('GPU'))\"\r\n2020-12-05 21:54:10.995836: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-05 21:54:13.347310: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-05 21:54:13.348033: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-05 21:54:14.457474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-12-05 21:54:14.457650: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-05 21:54:14.464722: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-05 21:54:14.464862: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-05 21:54:14.468456: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-05 21:54:14.469702: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-05 21:54:14.470886: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-12-05 21:54:14.474321: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-05 21:54:14.496440: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-05 21:54:14.496592: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-12-05 21:54:14.497192: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-05 21:54:14.497932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-05 21:54:14.498055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]\r\n2020-12-05 21:54:14.499112: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\ntf.Tensor(22.821045, shape=(), dtype=float32)\r\n[]\r\n~~~\r\n\r\nAlso to make sure I installed Cudnn correctly, the directory structure for relevant folders is:\r\n~~~\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin>ls\r\nbin2c.exe              cudnn_adv_infer64_8.dll  curand64_10.dll      nppig64_11.dll   nvjpeg64_11.dll\r\ncompute-sanitizer.bat  cudnn_adv_train64_8.dll  cusolver64_11.dll    nppim64_11.dll   nvlink.exe\r\ncrt                    cudnn_cnn_infer64_8.dll  cusolverMg64_11.dll  nppist64_11.dll  nvprof.exe\r\ncublas64_11.dll        cudnn_cnn_train64_8.dll  cusparse64_11.dll    nppisu64_11.dll  nvprune.exe\r\ncublasLt64_11.dll      cudnn_ops_infer64_8.dll  fatbinary.exe        nppitc64_11.dll  nvrtc-builtins64_111.dll\r\ncuda-memcheck.exe      cudnn_ops_train64_8.dll  nppc64_11.dll        npps64_11.dll    nvrtc64_111_0.dll\r\ncudafe++.exe           cufft64_10.dll           nppial64_11.dll      nvblas64_11.dll  nvvp.bat\r\ncudart32_110.dll       cufftw64_10.dll          nppicc64_11.dll      nvcc.exe         ptxas.exe\r\ncudart64_110.dll       cuinj64_111.dll          nppidei64_11.dll     nvcc.profile\r\ncudnn64_8.dll          cuobjdump.exe            nppif64_11.dll       nvdisasm.exe\r\n~~~\r\n~~~\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1>ls\r\nCUDA_Toolkit_Release_Notes.txt  EULA.txt  bin                extras   lib      nvml  src\r\nDOCS                            README    compute-sanitizer  include  libnvvp  nvvm  tools\r\n~~~\r\n~~~\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\include>ls\r\nCL                           cudnn_version.h                           nppi_filtering_functions.h\r\nbuiltin_types.h              cufft.h                                   nppi_geometry_transforms.h\r\nchannel_descriptor.h         cufftXt.h                                 nppi_linear_transforms.h\r\ncommon_functions.h           cufftw.h                                  nppi_morphological_operations.h\r\ncooperative_groups           curand.h                                  nppi_statistics_functions.h\r\ncooperative_groups.h         curand_discrete.h                         nppi_support_functions.h\r\ncrt                          curand_discrete2.h                        nppi_threshold_and_compare_operations.h\r\ncuComplex.h                  curand_globals.h                          npps.h\r\ncub                          curand_kernel.h                           npps_arithmetic_and_logical_operations.h\r\ncublas.h                     curand_lognormal.h                        npps_conversion_functions.h\r\ncublasLt.h                   curand_mrg32k3a.h                         npps_filtering_functions.h\r\ncublasXt.h                   curand_mtgp32.h                           npps_initialization.h\r\ncublas_api.h                 curand_mtgp32_host.h                      npps_statistics_functions.h\r\ncublas_v2.h                  curand_mtgp32_kernel.h                    npps_support_functions.h\r\ncuda                         curand_mtgp32dc_p_11213.h                 nvPTXCompiler.h\r\ncuda.h                       curand_normal.h                           nvblas.h\r\ncudaD3D10.h                  curand_normal_static.h                    nvfunctional\r\ncudaD3D11.h                  curand_philox4x32_x.h                     nvjpeg.h\r\ncudaD3D9.h                   curand_poisson.h                          nvml.h\r\ncudaGL.h                     curand_precalc.h                          nvrtc.h\r\ncudaProfiler.h               curand_uniform.h                          nvtx3\r\ncuda_awbarrier.h             cusolverDn.h                              sm_20_atomic_functions.h\r\ncuda_awbarrier_helpers.h     cusolverMg.h                              sm_20_atomic_functions.hpp\r\ncuda_awbarrier_primitives.h  cusolverRf.h                              sm_20_intrinsics.h\r\ncuda_bf16.h                  cusolverSp.h                              sm_20_intrinsics.hpp\r\ncuda_bf16.hpp                cusolverSp_LOWLEVEL_PREVIEW.h             sm_30_intrinsics.h\r\ncuda_d3d10_interop.h         cusolver_common.h                         sm_30_intrinsics.hpp\r\ncuda_d3d11_interop.h         cusparse.h                                sm_32_atomic_functions.h\r\ncuda_d3d9_interop.h          cusparse_v2.h                             sm_32_atomic_functions.hpp\r\ncuda_device_runtime_api.h    device_atomic_functions.h                 sm_32_intrinsics.h\r\ncuda_egl_interop.h           device_atomic_functions.hpp               sm_32_intrinsics.hpp\r\ncuda_fp16.h                  device_double_functions.h                 sm_35_atomic_functions.h\r\ncuda_fp16.hpp                device_functions.h                        sm_35_intrinsics.h\r\ncuda_gl_interop.h            device_launch_parameters.h                sm_60_atomic_functions.h\r\ncuda_occupancy.h             device_types.h                            sm_60_atomic_functions.hpp\r\ncuda_pipeline.h              driver_functions.h                        sm_61_intrinsics.h\r\ncuda_pipeline_helpers.h      driver_types.h                            sm_61_intrinsics.hpp\r\ncuda_pipeline_primitives.h   fatBinaryCtl.h                            surface_functions.h\r\ncuda_profiler_api.h          fatbinary.h                               surface_functions.hpp\r\ncuda_runtime.h               fatbinary_section.h                       surface_indirect_functions.h\r\ncuda_runtime_api.h           host_config.h                             surface_indirect_functions.hpp\r\ncuda_surface_types.h         host_defines.h                            surface_types.h\r\ncuda_texture_types.h         library_types.h                           texture_fetch_functions.h\r\ncudalibxt.h                  math_constants.h                          texture_fetch_functions.hpp\r\ncudart_platform.h            math_functions.h                          texture_indirect_functions.h\r\ncudnn.h                      mma.h                                     texture_indirect_functions.hpp\r\ncudnn_adv_infer.h            npp.h                                     texture_types.h\r\ncudnn_adv_train.h            nppcore.h                                 thrust\r\ncudnn_backend.h              nppdefs.h                                 vector_functions.h\r\ncudnn_cnn_infer.h            nppi.h                                    vector_functions.hpp\r\ncudnn_cnn_train.h            nppi_arithmetic_and_logical_operations.h  vector_types.h\r\ncudnn_ops_infer.h            nppi_color_conversion.h\r\ncudnn_ops_train.h            nppi_data_exchange_and_initialization.h\r\n~~~\r\n~~~\r\n\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\lib\\x64>ls\r\nOpenCL.lib         cudnn_adv_infer.lib      cudnn_ops_infer64_8.lib  nppc.lib     nppitc.lib\r\ncublas.lib         cudnn_adv_infer64_8.lib  cudnn_ops_train.lib      nppial.lib   npps.lib\r\ncublasLt.lib       cudnn_adv_train.lib      cudnn_ops_train64_8.lib  nppicc.lib   nvblas.lib\r\ncuda.lib           cudnn_adv_train64_8.lib  cufft.lib                nppidei.lib  nvjpeg.lib\r\ncudadevrt.lib      cudnn_cnn_infer.lib      cufftw.lib               nppif.lib    nvml.lib\r\ncudart.lib         cudnn_cnn_infer64_8.lib  curand.lib               nppig.lib    nvptxcompiler_static.lib\r\ncudart_static.lib  cudnn_cnn_train.lib      cusolver.lib             nppim.lib    nvrtc.lib\r\ncudnn.lib          cudnn_cnn_train64_8.lib  cusolverMg.lib           nppist.lib\r\ncudnn64_8.lib      cudnn_ops_infer.lib      cusparse.lib             nppisu.lib\r\n\r\n~~~\r\n\r\nThis is all I have. I am still unsure where did I go wrong. Any suggestion will be constructive.\r\n\r\n\r\nPS:\r\nAs I was searching for the missing file \"cusolver64_10.dll\", I found \"cusolver64_11.dll\" in the bin folder of CUDA.\r\nI renamed it to \"cusolver64_10.dll\" and guess what, it worked!\r\n\r\n~~~\r\nC:\\Users\\91930>python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])));print(tf.config.list_physical_devices('GPU'))\"\r\n2020-12-05 22:20:36.833043: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-05 22:20:39.159405: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-05 22:20:39.160248: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-05 22:20:40.253360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-12-05 22:20:40.253636: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-05 22:20:40.262822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-05 22:20:40.263003: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-05 22:20:40.268415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-05 22:20:40.270505: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-05 22:20:40.329207: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-05 22:20:40.333279: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-05 22:20:40.334136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-05 22:20:40.335561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\r\n2020-12-05 22:20:40.336268: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-05 22:20:40.337557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-12-05 22:20:40.337739: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-05 22:20:40.339407: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-05 22:20:40.339984: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-05 22:20:40.340509: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-05 22:20:40.341164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-05 22:20:40.341741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-05 22:20:40.342833: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-05 22:20:40.346122: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-05 22:20:40.346766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\r\n2020-12-05 22:20:41.330007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-05 22:20:41.330099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0\r\n2020-12-05 22:20:41.330795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N\r\n2020-12-05 22:20:41.333302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2903 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-12-05 22:20:41.335887: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\ntf.Tensor(-902.86865, shape=(), dtype=float32)\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n~~~\r\nI am unsure if I should go this way or it will lead to some errors in the future.\r\n", "@anirudhakulkarni As said, it is not recommended to keep going with CUDA 11.1, even after renaming `cusolver64_11` to `cusolver64_10` unless there is no possibility to install CUDA 11.0 in your Windows installation :)\r\n\r\n`cusolver64_10.dll` is part of CUDA 11.0 and CUDA 10.1 installation, `cusolver64_11.dll` is part of CUDA 11.1.\r\nUsing a wrong version `.dll` simply by renaming it asks for more confusing errors than the one you had at the startup :)\r\n\r\nPlease consider installing CUDA 11.0, which is supported and would work with `tensorflow==2.4.0rc3`, `tensorflow==2.4.0rc4` and for example the latest `tf-nightly-gpu==2.5.0.dev20201206`.", "@anirudhakulkarni \r\nAs already informed repeatedly please CUDA 11.0 and not 11.1. ", "Thanks for resolving the issue. It worked with CUDA 11.0.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45258\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45258\">No</a>\n"]}, {"number": 45257, "title": "module 'tensorflow._api.v2.data' has no attribute 'dataset'", "body": "Code written:\r\nbatch_size = 32\r\ndataset = tf.data.dataset.from_tensor_slices(x_train).shuffle(1000)\r\ndataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\r\n\r\nError:\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-fba98a470002> in <module>\r\n      1 batch_size = 32\r\n----> 2 dataset = tf.data.dataset.from_tensor_slices(x_train).shuffle(1000)\r\n      3 dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\r\n\r\nAttributeError: module 'tensorflow._api.v2.data' has no attribute 'dataset'\r\n\r\nTensorflow version: 2.3.1\r\n\r\nSolved\r\n\r\nThe code should be:\r\ndataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(1000)", "comments": ["@sjaddya \r\n\r\nPlease use `tf.data.Dataset.from_tensor_slices`.Please, refer this [tutorial](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Please, close this thread if your issue was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45257\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45257\">No</a>\n"]}, {"number": 45256, "title": "Barrier in implementing demo: SHAPE -> STRIDED_SLICE impossible?", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 4d2124d19daffa088aca8c2485b56c8f1717ec0e\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32\r\n\r\n**Describe the problem**\r\n\r\nAfter training/converting a Keras MobileNetV1 model, running inference on the ESP32 produces this error:\r\n```\r\nType INT32 (2) not supported.\r\nProcessing Node STRIDED_SLICE (number 34) failed to invoke with status 1\r\nNode STRIDED_SLICE (number 34) failed to invoke with status 1\r\n```\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nSteps are based on the [person detector](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection/training_a_model.md\r\n) tutorial. \r\n\r\nI build the model with keras: \r\n```model = tf.keras.applications.mobilenet.MobileNet(weights=None, input_shape=(96, 96, 1), alpha = 0.25, classes=6)```\r\n\r\nThen successfully convert:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"...\")\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # removing these lines does not fix the error\r\nconverter.inference_input_type = tf.int8 # removing these lines does not fix the error\r\nconverter.inference_output_type = tf.int8 # removing these lines does not fix the error\r\ntflite_model = converter.convert()\r\nopen(\"model_out.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nAnd finally run:\r\n```\r\nxxd -i model_out.tflite > custom_model_data.cc\r\n```\r\n\r\n\r\nI am running a slightly modified version of the person_detection demo project. Some additional layers need to be registered compared to the demo code (not sure why):\r\n```\r\nstatic tflite::MicroMutableOpResolver<10> micro_op_resolver(error_reporter);\r\n  micro_op_resolver.AddAveragePool2D();\r\n  micro_op_resolver.AddConv2D();\r\n  micro_op_resolver.AddDepthwiseConv2D();\r\n  micro_op_resolver.AddPad();\r\n  micro_op_resolver.AddMean();\r\n  micro_op_resolver.AddShape();\r\n  micro_op_resolver.AddStridedSlice();\r\n  micro_op_resolver.AddPack();\r\n  micro_op_resolver.AddReshape();\r\n  micro_op_resolver.AddSoftmax();\r\n```\r\n\r\nAny inference attempts result in the original error I posted above. I believe that this is the inevitable result of a SHAPE layer layer (which outputs an int32 type), followed by a STRIDED_SLICE layer (which doesn't support int32 as input). \r\n\r\nIs there any way to get around this? Or alternatively, how did the authors of the tutorial successfully produce a mobilenetv1 model that required such few operations to be registered? I am taking the same quantization steps, and believe the model is the same.\r\n\r\nThanks!\r\n\r\n", "comments": ["@advaitjain Any suggestions for getting feature-parity with demo results? It looks like you're working in this area. I'm debating re-writing my project to use TF Slim for the small chance that it will somehow fix this. Can't find many other differences vs. the demo training example code.", "Update: I believe networks produced with ```tf.keras.applications.MobileNet```, specifically while using a non-default ```classes``` parameter value, are not supported with TF Micro. Not sure what TF Slim is doing for binary image classification but it must be different.", "Kind of a similar issue occured to me too. While both my TF saved model and TF Lite model work when running in the PC, when running it on ESP32 I get the same error. I noticed that there is an issue with the Reshape Layer, being converted into Shape -> Strided Slice -> Pack -> Reshape in TF Lite (used Netron to visualize it), and because of the Pack Layer outputting an int32, TF Lite Micro outputs an error. The described situation occured while using TF 2.3.1. \r\n\r\nThe problem was solved when running an older TF version (2.2.1). Now the Reshape Layer is converted into just a TF Lite Reshape Layer (no extras) and the model works fine in ESP32.", "> Kind of a similar issue occured to me too. While both my TF saved model and TF Lite model work when running in the PC, when running it on ESP32 I get the same error. I noticed that there is an issue with the Reshape Layer, being converted into Shape -> Strided Slice -> Pack -> Reshape in TF Lite (used Netron to visualize it), and because of the Pack Layer outputting an int32, TF Lite Micro outputs an error. The described situation occured while using TF 2.3.1.\r\n> \r\n> The problem was solved when running an older TF version (2.2.1). Now the Reshape Layer is converted into just a TF Lite Reshape Layer (no extras) and the model works fine in ESP32.\r\n\r\nIt seems that `int8` support for input and output is added in tf2.3, so unfortunately `converter.inference_input_type = tf.int8` doesn't work in tf2.2.", "Anyone is working on this issue? I can reproduce the problem with tf 2.4 using new converter. \r\nI found a workaround by removing Reshape layer all together - I had it right after Input and it turns out tf micro doesn't need the input to be explicitly reshaped, e.g.\r\n\r\n```\r\n  int8_t x_quantized[72];\r\n  float x[72] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0};        \r\n\r\n  for (byte i = 0; i < 72; i = i + 1) {\r\n        input->data.int8[i] = x[i] / input->params.scale + input->params.zero_point;\r\n  }\r\n```\r\n\r\nthe above code works with the following network (input shape = batch_size, 24, 3)\r\n\r\n```\r\ninput = Input(shape=(window_size, 3, ))\r\nx = Conv1D(8, 3, activation='relu', padding='same')(input)\r\nx = AveragePooling1D(2)(x)\r\nx = Conv1D(16, 3, activation='relu', padding='same')(x)\r\nx = AveragePooling1D(2)(x)\r\nx = Conv1D(32, 3, activation='relu', padding='same')(x)\r\nx = AveragePooling1D(2)(x)\r\nx = Conv1D(32, 3, activation='relu', padding='same')(x)\r\nx = Conv1D(64, 3, activation='relu', padding='same')(x)\r\n#x = GlobalAveragePooling1D()(x)\r\nx = Dropout(0.25)(x)\r\nx = Flatten()(x)\r\ntype_out = Dense(len(labels), activation=\"softmax\", name=\"type\")(x)\r\nprecip_out = Dense(1, activation=\"sigmoid\", name=\"precip\")(x)\r\n```", "Here is how i adapted Keras MobileNetV1 model to work in tensorflow micro \r\n\r\n```\r\nfrom tensorflow.keras.applications import MobileNet\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Dense, Softmax, Dropout\r\n\r\nmodel = MobileNet(include_top = False, weights=None, input_shape=(96, 96, 1), pooling='avg', alpha=0.25, dropout=0.001 )\r\ntop_layer = Dropout(0.001)(model.output)\r\ntop_layer = Dense(2)(top_layer)\r\ntop_layer = Softmax()(top_layer)\r\nmodel = Model(model.input, top_layer)\r\n```\r\n\r\nit still does not have the exact same behavior in training as the default model, but performs fine.", "I was looking for a solution to this problem and found this.\r\nhttps://stackoverflow.com/questions/64850356/error-with-strided-slice-in-tensorflow-lite/66223851#66223851\r\nIt appears that editing tensorflow\\lite\\micro\\kernels\\strided_slice.cc to have an extra case that assumes int32 can be treated as int8 can work in at least some cases. (It worked for me; I have trained my model with tf 2.4.2 and built tf-micro with tf 2.4.2 source)\r\n```\r\ncase kTfLiteInt32:\r\n  reference_ops::StridedSlice(op_params,\r\n  tflite::micro::GetTensorShape(input),\r\n  tflite::micro::GetTensorData<int8_t>(input),\r\n  tflite::micro::GetTensorShape(output),\r\n  tflite::micro::GetTensorData<int8_t>(output));\r\n break;\r\n```\r\n\r\nIf someone has any ideas on why this works / ideas for a proper fix I would like to learn more about this.\r\nI am going to attach the modified strided_slice.cc if anyone would like to try and replace theirs with it.\r\n[strided_slice.zip](https://github.com/tensorflow/tensorflow/files/6845050/strided_slice.zip)\r\n\r\n@EliZucker it does appear this this issue is related to a bad shape layer, here is an image of the netron output for my model that had this issue. \r\n![image](https://user-images.githubusercontent.com/10223929/126254577-ed233a0b-c0e3-46e6-9858-3d2a6e62d4cf.png)\r\n\r\n\r\nAlso credit to @ocihangir for figuring this out in the first place!", "@EliZucker \r\nCould you please let us know if this is still an issue in latest stable TF v2.6.0 ?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45256\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45256\">No</a>\n", "I have to say that this problem stil exist in the latest version of tensorflow (colab 2.7.0)\r\n\r\nIt prevents me from generating a working model on the Arduino 33 ble.\r\n\r\nThis is a serious problem because we cannot even run this [official demo](https://www.tensorflow.org/lite/performance/post_training_integer_quant)", "> I have to say that this problem stil exist in the latest version of tensorflow (colab 2.7.0)\r\n> \r\n> It prevents me from generating a working model on the Arduino 33 ble.\r\n> \r\n> This is a serious problem because we cannot even run this [official demo](https://www.tensorflow.org/lite/performance/post_training_integer_quant)\r\n\r\nI can confirm this is still not fixed in tensorflow 2.8.0. @[Saduf2019](https://github.com/Saduf2019) Can you please re-open this issue, it has been there for over a year now and I still need to avoid reshape layers...", "I was looking at the source code for tensorflow micro and believe a fix may have been implemented. However, in order to receive the fix you would have build the library from source instead of downloading it through the arduino package manager.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45256\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45256\">No</a>\n", "Can you please try the instructions from https://github.com/tensorflow/tflite-micro-arduino-examples and let us know if that helps."]}, {"number": 45254, "title": "model.save fails to create directories on Windows", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 2004\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: cudart64_101.dll\r\n- GPU model and memory: GeForce GTX 1070\r\n\r\n\r\n**Steps to reproduce**\r\nFirst make sure the current directory doesn't have `saved_model` folder.\r\n\r\n![dopus_fJLqMA2WiE](https://user-images.githubusercontent.com/614159/100533710-53cb6100-3242-11eb-992a-b78539462db1.png)\r\n\r\nThen run the following code\r\n\r\n```\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nprint(tf.version.VERSION)\r\n\r\n\r\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\r\n\r\ntrain_labels = train_labels[:1000]\r\ntest_labels = test_labels[:1000]\r\n\r\ntrain_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\r\ntest_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0\r\n\r\ndef create_model():\r\n\tmodel = tf.keras.models.Sequential([\r\n\t\tkeras.layers.Dense(512, activation='relu', input_shape=(train_images.shape[1],)),\r\n\t\tkeras.layers.Dropout(0.2),\r\n\t\tkeras.layers.Dense(10)\r\n\t])\r\n\r\n\tmodel.compile(optimizer='adam',\r\n\t\t\t\t  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n\t\t\t\t  metrics=['accuracy'])\r\n\r\n\treturn model\r\n\r\n\r\nmodel = create_model()\r\n\r\nmodel.summary()\r\n\r\n# Create and train a new model instance.\r\nmodel = create_model()\r\nmodel.fit(train_images, train_labels, epochs=5)\r\n\r\n# Save the entire model as a SavedModel.\r\nmodel.save('saved_model/my_model')\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nmodel.save throws exception:\r\n> Traceback (most recent call last):\r\n>   File \"D:/renaming/neural network/tensorflow/6.save_and_load.py\", line 124, in <module>\r\n>     model.save('saved_model/my_model')\r\n>   File \"C:\\ProgramData\\Miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 1008, in save\r\n>     signatures, options)\r\n>   File \"C:\\ProgramData\\Miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\", line 115, in save_model\r\n>     signatures, options)\r\n>   File \"C:\\ProgramData\\Miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save.py\", line 78, in save\r\n>     save_lib.save(model, filepath, signatures, options)\r\n>   File \"C:\\ProgramData\\Miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\", line 915, in save\r\n>     utils_impl.get_or_create_variables_dir(export_dir)\r\n>   File \"C:\\ProgramData\\Miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\utils_impl.py\", line 214, in get_or_create_variables_dir\r\n>     file_io.recursive_create_dir(variables_dir)\r\n>   File \"C:\\ProgramData\\Miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\", line 440, in recursive_create_dir\r\n>     recursive_create_dir_v2(dirname)\r\n>   File \"C:\\ProgramData\\Miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\", line 455, in recursive_create_dir_v2\r\n>     pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(path))\r\n> tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: saved_model/my_model\\variables; No such file or directory\r\n\r\nFolder saved_model is created, but it contains nothing.\r\n\r\n![dopus_K6JLvfsHQ7](https://user-images.githubusercontent.com/614159/100533801-54b0c280-3243-11eb-8600-0dc81f526fc6.png)\r\n\r\n**Describe the expected behavior**\r\n\r\nIf model.save requires the presence of the path, the method should fail in  the first place, without creating the `saved_model` folder\r\n\r\nIf the method is able to create folders, why doesn't it create `my_model` subfolder as well?", "comments": ["Change `model.save('saved_model/my_model')` to `model.save('saved_model\\\\my_model')`\r\nAnd it should work.", "@gqqnbig \r\n\r\nPlease, create a folder `saved_model` before saving the model so `my_model` will be saved in that directory. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/dd43d94373a9ff4ba712729d4b85a5be/untitled548.ipynb).Please, verify once and close the issue. \r\nPlease, refer [tutorial](https://www.tensorflow.org/tutorials/keras/save_and_load#savedmodel_format)\r\nThanks!"]}, {"number": 45253, "title": "Eager Gradients C api is C++, along with a few others", "body": "A few files in `tensorflow/c/eager` expose C++ APIs instead of C.  [gradients.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/gradients.h), [tape.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/tape.h), and the `abstract_.h` files (i.e.  [abstract_context.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/abstract_context.h))  do this, and possibly more.  There are no C apis exposed for most of these, either, just the C++.   Has the design changed or is this just an oversight?\r\n\r\ncc @karllessard", "comments": ["These are WIP C++ impls for a new set of APIs for eager execution, tf.function and tf.GradientTape. We will soon be publishing RFCs for these. The C APIs will come soon as well once the implementation is settled. "]}, {"number": 45252, "title": "\"error: 'tf.Conv2D' op is neither a custom op nor a flex op\" when converting a basic MNIST network", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary from pip3 install tensorflow\r\n- TensorFlow version (or github SHA if from source): 2.3.1 and tf-nightly 2.5.0-dev20201128\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nprint(tf.__version__)\r\n\r\nmodel = keras.models.load_model(\"mnist.h5\")\r\n# tf.saved_model.save(model, \"new_mnist\")\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n# converter = tf.lite.TFLiteConverter.from_saved_model(\"mnist.h5\")\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nnum_calibration_steps = 10\r\n\r\ndef representative_dataset_gen():\r\n  for _ in range(num_calibration_steps):\r\n    # Get sample input data as a numpy array in a method of your choosing.\r\n    yield [input]\r\n    \r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\ntflite_quant_model = converter.convert()\r\n\r\nopen(\"quantized.tflite\", \"wb\").write(tflite_quant_model)\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.   (Full log is attached at the end)\r\ncx872@T460p:~/Desktop/temp$ python3 quantize.py \r\n2020-11-28 15:28:02.571351: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-11-28 15:28:02.571375: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-11-28 15:28:03.736827: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-11-28 15:28:03.736848: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-11-28 15:28:03.736865: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (T460p): /proc/driver/nvidia/version does not exist\r\n2020-11-28 15:28:03.737083: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-28 15:28:03.762679: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2599990000 Hz\r\n2020-11-28 15:28:03.763178: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4878980 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-11-28 15:28:03.763205: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From /home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-11-28 15:28:03.973359: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From /home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-11-28 15:28:04.369274: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-11-28 15:28:04.369473: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-11-28 15:28:04.374144: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-11-28 15:28:04.374171: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2020-11-28 15:28:04.374181: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-11-28 15:28:04.398758: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-11-28 15:28:04.398813: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\nloc(callsite(\"sequential/conv2d/Conv2D\"(\"/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\":1073:0) at callsite(\"/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\":1167:0 at callsite(\"/home/cx872/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\":804:0 at \"quantize.py\":23:0)))): error: 'tf.Conv2D' op is neither a custom op nor a flex op\r\nloc(callsite(\"sequential/conv2d_1/Conv2D\"(\"/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\":1073:0) at callsite(\"/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\":1167:0 at callsite(\"/home/cx872/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\":804:0 at \"quantize.py\":23:0)))): error: 'tf.Conv2D' op is neither a custom op nor a flex op\r\nloc(callsite(\"sequential/conv2d_2/Conv2D\"(\"/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\":1073:0) at callsite(\"/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\":1167:0 at callsite(\"/home/cx872/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\":804:0 at \"quantize.py\":23:0)))): error: 'tf.Conv2D' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.Conv2D {data_format = \"NCHW\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\r\nTraceback (most recent call last):\r\n  File \"/home/cx872/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\", line 196, in toco_convert_protos\r\n    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,\r\n  File \"/home/cx872/.local/lib/python3.8/site-packages/tensorflow/lite/python/wrap_toco.py\", line 32, in wrapped_toco_convert\r\n    return _pywrap_toco_api.TocoConvert(\r\nException: /home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:1073:0: error: 'tf.Conv2D' op is neither a custom op nor a flex op\r\n/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:1167:0: note: called from\r\n/home/cx872/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py:804:0: note: called from\r\nquantize.py:23:0: note: called from\r\n/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:1073:0: note: see current operation: %0 = \"tf.Conv2D\"(%arg0, %cst) {data_format = \"NCHW\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<?x1x28x28xf32>, tensor<3x3x1x4xf32>) -> tensor<?x4x26x26xf32>\r\n/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:1073:0: error: 'tf.Conv2D' op is neither a custom op nor a flex op\r\n/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:1167:0: note: called from\r\n/home/cx872/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py:804:0: note: called from\r\nquantize.py:23:0: note: called from\r\n/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:1073:0: note: see current operation: %2 = \"tf.Conv2D\"(%1, %cst_0) {data_format = \"NCHW\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<?x4x26x26xf32>, tensor<3x3x4x4xf32>) -> tensor<?x4x24x24xf32>\r\n/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:1073:0: error: 'tf.Conv2D' op is neither a custom op nor a flex op\r\n/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:1167:0: note: called from\r\n/home/cx872/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py:804:0: note: called from\r\nquantize.py:23:0: note: called from\r\n/home/cx872/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:1073:0: note: see current operation: %4 = \"tf.Conv2D\"(%3, %cst_1) {data_format = \"NCHW\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<?x4x24x24xf32>, tensor<3x3x4x4xf32>) -> tensor<?x4x22x22xf32>\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.Conv2D {data_format = \"NCHW\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\r\n<unknown>:0: note: see current operation: \"func\"() ( {\r\n^bb0(%arg0: tensor<?x1x28x28xf32>):  // no predecessors\r\n  %cst = \"std.constant\"() {value = dense<[[[[0.0649565831, -0.0746282786, 0.162961558, 0.0248589013]], [[-0.200302526, -0.296956897, -0.191993952, 0.220391616]], [[-0.239433467, 0.189716026, -0.311748624, 0.0287476946]]], [[[0.161856413, 0.14310281, 0.234080926, 0.183344632]], [[-0.0514137745, -0.209035754, -0.103884049, 0.158034727]], [[0.193658769, -0.0461433418, 0.327664047, -0.0481641293]]], [[[0.0117518231, 3.49453388E-4, -0.232930154, -0.24110195]], [[0.134880558, 0.145225272, 0.0606381223, 0.157407314]], [[-0.0305803325, -0.0580686741, -0.210291281, -1.786370e-01]]]]> : tensor<3x3x1x4xf32>} : () -> tensor<3x3x1x4xf32>\r\n  %cst_0 = \"std.constant\"() {value = dense<\"0x246BA8BCE77820BE3ED7853ED4B554BD4EC87D3E204B0D3D6F2A883E3EE48C3D16D285BEF12B14BD519BEABDEF5E0DBD338DBCBDE90A0E3E999006BE4312993D9BFC22BDB9C0BD3D29AD97BEF97BE53D66BB98BD88E1783E8A50393EBBA648BE7B81A13CB9D46CBEB74C5B3E93CAAEBD04965DBE4FBEAD3D8279103DC15460BD2E503A3C867E10BE45275ABEEA3303BE1AA21A3E8AB858BE91E1D93DE4B185BCA5ED8DBEBB99A83D623B6DBDDB1A7B3C4A93653DDFC1013E75D25CBE934A6CBC6E48133E83671EBE5BF1013D6DCF8CBD844862BEE79AA13EC8806BBE06A623BE02164D3B2A95E43DD7E6843C49857D3EF202B2BD7873B7BE068EB2BD06E264BDBB6B243DEA4AA5BD28B1803EDF99AE3DAD55A7BDA1BC17BEDC55933EC1C3CDBD55DFF23DBCD6DDBD3412E6BDA3A471BDB4EAAFBD06B582BDC60CB0BE1FDEB43D5CCF4BBC2300523E7E86803E8969ED3D718C25BE6094373E555D6A3E2AE034BD7E7E2A3D92E3CABD188D95BECB4FCF3C0CEE2EBE91CAA9BE085C8F3ED8B81BBEBE85913E95A4B4BD5937E4BD284C333E4C1E5BBDAD9A7ABC3D2425BD6B0DE33D2E251DBE49AC923E785F903DD0E247BD4639033E626897BEECE708BC6D803A3ED3F9093E700C48BE938BC2BDB65061BDFEC42C3E53EB353BA75E0F3E39749E3C3C3F9E3D61CD243EB8688ABE0724C73D818E9BBC1366A5BEB0D903BEA71BB13B918C323C49E4353E6FABF33D871B743EDDEF0BBCDA56E9BC1629603D92B76A3EC6CEFABD2092EEBDB0629DBC1AE468BEED54E13DFEEE2CBDCAB957BE37C0FFBC\"> : tensor<3x3x4x4xf32>} : () -> tensor<3x3x4x4xf32>\r\n  %cst_1 = \"std.constant\"() {value = dense<\"0x6C8307BED9C6833D1EF6B9BE24D236BE24CCC8BD350DB33D1CD3A53DC392D2BD6B81AA3DF53DBCBD19550E3DC69422BC1F9686BE743FF1BC581808BE723E15BD5917E9BDB12A683C0FEEAFBEC192C4BBA881F3BCB8FF7FBE74FA063E8FFBE7BD5727AD3C012EDFBE82FB303D7E64923D391B49BE569BA7BD6C4FB1BDCC09BE3C4488993D078E03BFA8A5F43C1F65BFBDBE8C263E438998BD551AF53D6E4FBABD398069BE90D9863DA99E03BF22FE0D3C8984E9BD921D54BE1FE1343EAF04363EF79618BD3975C9BD6FE32EBE099BA2BEF717C3BD77AF16BE0810E4BD9FBDE5BCB3BDB1BDB24AF23A19009BBE4EC58D3E0AD125BECADB273E33D91F3E56C489BDD87DB73D48D55BBEA7089DBE13B1C4BC50990E3D75A0173EA137A83EF18756BEFCA212BE504710BE9968C8BE5E858ABEE50F4EBD08EA0DBDE8F803BE4D9017BEE151E5BC80E61E3E5E19BCBD77EF1CBE1DA7B13DC209A8BD72FDDFBD71A0D7BD4C14103ECEE162BE4B05783DBEF02A3D8B49C1BC460EA4BC80C977BE09F09CBD62754C3DF12585BD4708243EC9C5543EB1952FBEE2E7B2BECE570A3EDDE9DE3D28C5AA3D8B329B3D7C8462BCE647E43D5FE10B3E5F47DC3D2DA6773DDFE493BE1111793C85ADF0BD56A055BC6D6D913D50A694BC3B4702BE4698373D02B983BE7DEDD9BC9F4012BEC26F103D53000F3E51B704BEFAE88F3D9136783C4683EEBD26141E3E23F0013DF5C1E0BD91A232BDA4EC14BCDBFA8ABEC0E506BE50223BBEF810323E559D983C9E8F573EB53FB3BE5C156A3D2D64BC3D631959BD046EB13D\"> : tensor<3x3x4x4xf32>} : () -> tensor<3x3x4x4xf32>\r\n  %cst_2 = \"std.constant\"() {value = dense<[0.0515903942, 0.674950719, 1.961430e-03, -0.249269262, 0.172605708, -0.0643907487, 0.0460773893, 0.245695099, -0.507323146, -0.116961144]> : tensor<10xf32>} : () -> tensor<10xf32>\r\n  %cst_3 = \"std.constant\"() {value = dense<[-1, 1936]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_4 = \"std.constant\"() {value = dense<\"0x4BA6EFBC1D3EE5BCC63F83BC0A188BBD0E055C3D1BC2853B9C074EBCF42E07BD2F21193DB9AC443D842D453D27C931BC3EDF7E3D58647F3D33F419BDF336A63D6511E63C8A4E593D01ACBB3C3607B93D2C5B2BBD52099F3C6844D93D334FEBBCD3A71D3DF321333CF847A43C5CC9863CC5B8FC3CE8B9653CD0006BBC0BA2D03CA7814A3CDCACAB3C91B9873DFCBD93BC394EC23DB9AC02BDCD5F703BAA1EEBBCE1F35A3D40ACAF3D2EDF993DA0AF223D27E1A0BD18978E3DC88BCF3B9269B73D397B443DF363E63C134BB83C624E9BBC493C92BD8EA6E3BC230A43BD08C75EBC2EBF87BD5FB7903C25E0D93C1431D33D96E6823D3F07E83D73E0A43BB59BED3D32C0D43D10AAB73DAB01DDBC39D1EB3D717AA4BB3002893DBC2E5D3C461C913B966E8ABD9F85DDBC99668D3D0A152B3DE5E0093D6629A43D3F39FBBC6045763D03C3723DB816AE3CAD5E65BDD2AC60BBBAE709BDFD1E31BB4AA3B33DB502793D97D63E3ECC9A143D20088E3D7ABBB5BDFF0D02BD865AF93C35EEC53C473F36BDBA79693DA2FC773B1241D33C4E49BC3C859896BDA1D8E7BCCD9B153C1EE20EBCB502FC3CE7C3CABD76413D3B9E936B3DCC64E9BCF5A9EC3D096D963CF535E83D9945CFBCC7DB06BED9F0D1BD033F08BE3EFCA3BD05044CBD288E693D574BE6BD56C076BDA7B9ACBD483831BBF3BA74BD2676D4BD1DAC053CAF08263C40A5A53C20CFA83C7BC00CBD22F5933D84BF173EE2A18A3C6735BE3B6E9982BD81E6623B929544BE1F53E93BF6FAEC3CA16CEE3DB10E04BE8358783D316910BE23873EBEA1D524BE50CF70BE62AD02BE32FC32BECB5101BC7B0F42BDBFD8F93C11883C3D907278BC8805F\r\n\r\n\r\nThe rest is similar hex values like the above.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\nThis is a basic MNIST model in keras format. Model summary is listed below.\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d (Conv2D)              (None, 4, 26, 26)         36        \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 4, 24, 24)         144       \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 4, 22, 22)         144       \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 1936)              0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 10)                19370     \r\n=================================================================\r\nTotal params: 19,694\r\nTrainable params: 19,694\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n\r\nhttps://drive.google.com/file/d/12HM4mRhYnN-CZuqk1zEwRWkIyQr0RTUC/view?usp=sharing\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n\r\nThe converter prompts `tf.Conv2D` is not supported. But I think this op is written in your allowlist for sure. \r\n\r\n\r\n**Any other info / logs**\r\n[error with tf 2.3.1.log](https://github.com/tensorflow/tensorflow/files/5611223/error.with.tf.2.3.1.log)\r\n\r\nRunning on tf-nightly build 2.5.0-dev20201128 throws an error as well, but the error message is different. I tried this in Google Colab.\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-ec82a6e4977e> in <module>()\r\n     23 converter.inference_input_type = tf.int8  # or tf.uint8\r\n     24 converter.inference_output_type = tf.int8  # or tf.uint8\r\n---> 25 tflite_quant_model = converter.convert()\r\n     26 \r\n     27 open(\"quantized.tflite\", \"wb\").write(tflite_quant_model)\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    893 \r\n    894     return super(TFLiteKerasModelConverterV2,\r\n--> 895                  self).convert(graph_def, input_tensors, output_tensors)\r\n    896 \r\n    897 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n    651     calibrate_and_quantize, flags = quant_mode.quantizer_flags()\r\n    652     if calibrate_and_quantize:\r\n--> 653       result = self._calibrate_quantize_model(result, **flags)\r\n    654 \r\n    655     flags_modify_model_io_type = quant_mode.flags_modify_model_io_type(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type, activations_type, allow_float)\r\n    479       return calibrate_quantize.calibrate_and_quantize(\r\n    480           self.representative_dataset.input_gen, inference_input_type,\r\n--> 481           inference_output_type, allow_float, activations_type)\r\n    482 \r\n    483   def _is_unknown_shapes_allowed(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float, activations_type, resize_input)\r\n     95         initialized = True\r\n     96         if resize_input:\r\n---> 97           self._calibrator.Prepare([list(s.shape) for s in sample])\r\n     98         else:\r\n     99           self._calibrator.Prepare()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py in <listcomp>(.0)\r\n     95         initialized = True\r\n     96         if resize_input:\r\n---> 97           self._calibrator.Prepare([list(s.shape) for s in sample])\r\n     98         else:\r\n     99           self._calibrator.Prepare()\r\n\r\nAttributeError: 'function' object has no attribute 'shape'\r\n```\r\n", "comments": ["@CPA872 \r\n\r\nPlease, share mnist.h5 file. It helps me to reproduce the issue.\r\n\r\nPlease, grant me the access to below link\r\n\r\nhttps://drive.google.com/file/d/12HM4mRhYnN-CZuqk1zEwRWkIyQr0RTUC/view?usp=sharing\r\n\r\nThanks!", "Sorry I forgot to change the access settings.\r\nhttps://drive.google.com/file/d/12HM4mRhYnN-CZuqk1zEwRWkIyQr0RTUC/view?usp=sharing\r\nIt should work now.", "This issue is resolved for me. I did not add `converter.target_spec.supported_types = [tf.int8]` to specify my target quantization.\r\n\r\nMaybe you can update this tutorial page since it makes people think int8 quantization is enabled by default without the above line. By following the example code on this page, one would reproduce the error I had described in this issue. \r\n\r\nhttps://www.tensorflow.org/lite/performance/post_training_quantization#integer_only\r\n\r\nThanks!", "@CPA872 Can you please share a gist. I tried your code and still see same error even after adding `converter.target_spec.supported_types = [tf.int8]`. Can you please check my [gist here](https://colab.research.google.com/gist/jvishnuvardhan/3ff1900656a695bea20c17c43d1f6d79/untitled.ipynb) and let me know what I am missing.\r\n\r\nOnce I can find a root-cause, I can update the doc. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing due to lack of recent activity. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45252\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45252\">No</a>\n"]}, {"number": 45251, "title": "[tflite] [gpu] Add DHWC4 no-copy support", "body": "See discussion in #44336 . The GPU delegate [README](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/gpu#tips-and-tricks) currently promotes the possibility to send input in DHWC4 format to avoid an useless buffer copy. In practice, this does not work, at least for SSBOs, and that is, because:\r\n\r\n- The `DefaultTensorTie` in `api2.cc` does not register the external SSBO to the object manager, so that the runtime in `runtime.cc` is not able to find the tensor object.\r\n- The `Runtime` in `runtime.cc` retrieves the buffers and creates binding functions for them at initialization time. This means that this op is done during `InferenceBuilder::Builld`, which is **before** we can attempt to provide the real object through `InferenceRunner::SetInputObject`, for example.\r\n\r\nThis PR introduces two changes to address these issues:\r\n- The `Runtime` now acknowledges the fact that external objects are not immutable (which is clear looking at the fact that InferenceRunner has setters for them). So, **only for external objects**, they are now retrieved and validated at binding time, not initialization time. This way inference invocations can pick up the latest buffer available.\r\n- The `DefaultTensorTie`, which is the component that pipes the external user-defined `TensorObject` to the internal one, recognizes the case where we want to have a DHWC4 no-copy piping, and correctly registers the buffer to the ObjectManager. The tie also registers a placeholder object at initialization time, for the runtime to initialize properly.\r\n\r\nUsing DHWC4 input gave me a 25% performance boost in my tests on a super simple model (take image, output the mean).", "comments": ["@natario1  Can you please resolve conflicts? Thanks!", "@gbaned could you please give some high-level opinion first? I haven't looked at conflicts in depth, but recent commits are actually doing something very similar: \r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/bcd5dd0148ebdbbd93875049598b02b73bae8e30#diff-1ad703f1ca05896beb3921399d6486250abdad7e5ca027cd62568052ff5a177f\r\n\r\nSeems to be what I have suggested in #44336 (which got no answer) and tried to implement here. So if the team is already working on this, we're in good hands and we can close the PR. Thanks!", "Thank you, @natario1 for bringing this issue to our attention. The [internal fix](https://github.com/tensorflow/tensorflow/commit/bcd5dd0148ebdbbd93875049598b02b73bae8e30), which you already mentioned, was ready by the time you sent your PR and it just took some time to submit. We appreciate your proactive interest to solving this problem. On our end we verified no-copy DHWC4 inputs work correctly now, hope things got fixed on your end too. Please, keep sending us bugs in the future, if you spot one, your finding was very helpful!", "@ekaterinaignasheva I'll test when it's released, thanks. One issue that I'm seeing with my implementation (which is very similar to yours) is that no-copy DHWC4 actually slows execution down when OpenCL is available. Compare third and fourth plots in #46486 for example... I wonder if the CL delegate has some issues similar to the GL issues that we fixed here. \r\n\r\nOr maybe CL does not use 4 channels internally and prefers a properly sized SSBO (e.g. single channel if model is single channel). In which case I think we need an API to know which backend the delegate is using (GL/CL), so that we can properly size our buffers for efficiency.", "As per confirmation from @ekaterinaignasheva, closing this PR since problem was solved."]}, {"number": 45250, "title": "build from master branch with --config=v1 can not output tensorflow 1.x version", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master ( git clone from source)\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 11.1, 8.0.5\r\n- GPU model and memory: RTX 3090\r\n\r\n\r\n**Describe the problem**\r\nI tried to build tensorflow 1.x from source code with param`--config=v1`. But the output is always `tensorflow-2.5.0-xxx`.\r\nSo I wonder how to build tensorflow 1.x version from master branch of the source code? Is the `--cofing=v1` param not supposed to build tensorflow 1.x version? \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`bazel build --config=v1 --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["You need to use the v1 branch, e.g. `git checkout r1.15`.", "> You need to use the v1 branch, e.g. `git checkout r1.15`.\r\n\r\nBut the officail documents say `on master branch`. That means there has error on the officail documents?", "See related [issue ](https://github.com/tensorflow/tensorflow/issues/40272#issuecomment-644270769)", "> issue\r\n\r\nThanks! By the way, the official documents is a little vague and should be updated.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45250\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45250\">No</a>\n"]}, {"number": 45248, "title": "Fail to find the dnn implementation while using recurrent layers", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04 running in WSL2\r\n- TensorFlow version): 2.3.1\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: CUDA 10.1/ cuDNN 7.6.5.32\r\n- GPU model and memory: RTX 2060 6GB\r\n\r\n**Current behavior**\r\nI want to train a model containing Keras LSTM layers, however the following error occurs:  \r\n\r\n**Jupyter output:**\r\n`UnknownError:    Fail to find the dnn implementation.\r\n\t [[{{node CudnnRNN}}]]\r\n\t [[sequential_2/lstm_1/PartitionedCall]] [Op:__inference_train_function_5270]`\r\n\r\n**Console output:**\r\n`OP_REQUIRES failed at cudnn_rnn_ops.cc:1510 : Unknown: Fail to find the dnn implementation.`\r\n\r\n\r\n**Expected behavior**\r\nI expect the code to run since I am able to run Conv2D layers wich are properly accelerated by the GPU.  \r\nI have already tried multiple things such as using different Tensorflow/Cuda/cuDNN versions.\r\nI also tried to enable the memory growth as described in #36508 but it did not work either.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nThe environment was set up by following the installation instructions (without installing the nvidia driver inside the VM as mentioned in the nvidia documentation ): https://www.tensorflow.org/install/gpu#install_cuda_with_apt\r\n\r\nI was able to reproduce this issue by running the RNN tutorial available on the online Tensorflow documentation : https://www.tensorflow.org/guide/keras/rnn\r\n\r\nI would appreciate any help to solve this issue.\r\n", "comments": ["I finally found the way to solve the problem by reverting my Nvidia driver installed on Windows from **465 to 460**, as mentioned in the note in the nvidia documentation : https://docs.nvidia.com/cuda/wsl-user-guide/index.html\r\n\r\nSince using Windows Subsystem  for Linux is becoming more and more common, why not adding  a section in the documentation to set up Tensorflow with CUDA inside WSL ?", "We are currently discussing moving towards Windows GPU support only via WSL.", "I continue without solving this issue...\r\nI have tried all that you have mentioned but it continues the same problem\r\nmy OS is:\r\n\r\nUbuntu 18.04\r\nCUDA 10.0\r\nTensorflow 2.0\r\nNvidia-driver 460 (Although I have tried with 450 and it also does not work)\r\ngeForce RTX2060\r\nPython 3.7\r\n\r\nI have tried to compile with CUDA 10.1 and TF 2.1 but I continue without solving it. It starts to be a little frustrating\r\n\r\nThis is what I obtain after fitting:\r\n\r\nEpoch 1/50\r\n2021-01-25 18:59:34.964218: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-01-25 18:59:35.096029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n128/2156 [>.............................] - ETA: 15sWARNING:tensorflow:Can save best model only with val_loss available, skipping.\r\n\r\n.2021-01-25 18:59:35.364099: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2021-01-25 18:59:35.364136: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1510 : Unknown: Fail to find the dnn implementation.\r\n2021-01-25 18:59:35.364158: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Fail to find the dnn implementation.\r\n[[{{node CudnnRNN}}]]\r\n2021-01-25 18:59:35.364356: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: {{function_node __forward_cudnn_lstm_with_fallback_2517_specialized_for_sequential_lstm_StatefulPartitionedCall_at___inference_distributed_function_3196}} {{function_node __forward_cudnn_lstm_with_fallback_2517_specialized_for_sequential_lstm_StatefulPartitionedCall_at___inference_distributed_function_3196}} Fail to find the dnn implementation.\r\n[[{{node CudnnRNN}}]]\r\n[[sequential/lstm/StatefulPartitionedCall]]\r\n\r\nAll testings of the cuDnn and Cuda works well.", "> \r\n> \r\n> I continue without solving this issue...\r\n> I have tried all that you have mentioned but it continues the same problem\r\n> my OS is:\r\n> \r\n> Ubuntu 18.04\r\n> CUDA 10.0\r\n> Tensorflow 2.0\r\n> Nvidia-driver 460 (Although I have tried with 450 and it also does not work)\r\n> geForce RTX2060\r\n> Python 3.7\r\n> \r\n> I have tried to compile with CUDA 10.1 and TF 2.1 but I continue without solving it. It starts to be a little frustrating\r\n> \r\n> This is what I obtain after fitting:\r\n> \r\n> Epoch 1/50\r\n> 2021-01-25 18:59:34.964218: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2021-01-25 18:59:35.096029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 128/2156 [>.............................] - ETA: 15sWARNING:tensorflow:Can save best model only with val_loss available, skipping.\r\n> \r\n> .2021-01-25 18:59:35.364099: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> 2021-01-25 18:59:35.364136: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1510 : Unknown: Fail to find the dnn implementation.\r\n> 2021-01-25 18:59:35.364158: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Fail to find the dnn implementation.\r\n> [[{{node CudnnRNN}}]]\r\n> 2021-01-25 18:59:35.364356: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: {{function_node __forward_cudnn_lstm_with_fallback_2517_specialized_for_sequential_lstm_StatefulPartitionedCall_at___inference_distributed_function_3196}} {{function_node __forward_cudnn_lstm_with_fallback_2517_specialized_for_sequential_lstm_StatefulPartitionedCall_at___inference_distributed_function_3196}} Fail to find the dnn implementation.\r\n> [[{{node CudnnRNN}}]]\r\n> [[sequential/lstm/StatefulPartitionedCall]]\r\n> \r\n> All testings of the cuDnn and Cuda works well.\r\n\r\nDid u find any solutions?\r\nI have same system configurations and facing the same issue while running my dl model", "@iamMOY,\r\n\r\nCan you take a look at this [link](https://www.tensorflow.org/install/source#tested_build_configurations) to know about tested configurations and please update to latest stable version i.e `TF 2.6.0` and create a new issue if you face any. Thanks!", "@Avditvs,\r\n\r\nAs the problem has been fixed after you have downgraded the NVIDIA driver. Can you confirm if we are good to close this issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45248\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45248\">No</a>\n"]}, {"number": 45247, "title": "Unable to provide constant input tensors to keras functional API for TF2.0+", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0\r\n\r\n**Describe the current behavior**\r\nUnder eager execution, Keras cannot use an existing tensor as a constant input. It returns the following error:\r\n\r\n> ValueError: You should not pass an EagerTensor to `Input`. For example, instead of creating an InputLayer, you should instantiate your model and directly call it on your input.\r\n\r\n**Describe the expected behavior**\r\nKeras should be able to wrap an optional existing tensor  into the\u00a0Input\u00a0layer, using `tf.keras.Input(tensor=existing_tensor)`\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\nconst_np = np.random.rand(2,10,3)\r\nconst_tf = tf.convert_to_tensor(const_np, dtype=tf.float32)\r\nconst_k = tf.keras.Input(tensor=const_tf)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-ba2f0cbae65f> in <module>()\r\n      4 const_np = np.random.rand(2,10,3)\r\n      5 const_tf = tf.convert_to_tensor(const_np, dtype=tf.float32)\r\n----> 6 const_k = tf.keras.Input(tensor=const_tf)\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_layer.py in Input(shape, batch_size, name, dtype, sparse, tensor, ragged, **kwargs)\r\n    309     input_layer_config.update(\r\n    310         {'batch_size': batch_size, 'input_shape': shape})\r\n--> 311   input_layer = InputLayer(**input_layer_config)\r\n    312 \r\n    313   # Return tensor including `_keras_history`.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_layer.py in __init__(self, input_shape, batch_size, dtype, input_tensor, sparse, name, ragged, **kwargs)\r\n    171           raise_eager_tensor_error = True\r\n    172       if raise_eager_tensor_error:\r\n--> 173         raise ValueError('You should not pass an EagerTensor to `Input`. '\r\n    174                          'For example, instead of creating an '\r\n    175                          'InputLayer, you should instantiate your model and '\r\n\r\nValueError: You should not pass an EagerTensor to `Input`. For example, instead of creating an InputLayer, you should instantiate your model and directly call it on your input.\r\n```\r\n", "comments": ["@fkong7 \r\n\r\nI have tried in TF nightly version(`2.5.0-dev20201129`) and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/adfc8c2a8c763d3ae1a974a2c9ef325d/untitled553.ipynb).Please, veriy once and close the issue.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45247\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45247\">No</a>\n"]}, {"number": 45246, "title": "Fixed error in the example of image_gradients documentation", "body": "image_gradients returns (dy, dx) rather than (dx, dy)", "comments": ["Please fix in master branch", "> Please fix in master branch\r\n\r\n@mihaimaruseac Can you explain what I am expected to do?\r\n", "You need to submit the PR against the `master` branch, not `r2.1`. We don't update release branches except for patch releases and the patch releases are only issued for major bugs / security issues. Whenever we take a cherry-pick to the release branches, the source must exist in the `master` branch already.", "Thanks for the explanation, @mihaimaruseac \r\nNew pull request: #45461 "]}, {"number": 45245, "title": "Build for FMA AVX always shows cpu compatibility warnings", "body": "**System information**\r\n- OS: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: commit `fab3f85`:\r\n- Python version: Python 3.6.9\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): bazel 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: Cuda 11.1 cuDNN 8.0.5\r\n- GPU model and memory: 7.5, 3.8Gb\r\n\r\n**Describe the problem**\r\nI have compiled tensorflow from source many times attempting to add support for FMA, AVX and AVX 2 and no matter how many times or different ways I try to add support for these instruction sets I always get the message: \r\n```\r\nYour CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nHere is my bazelrc\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild --config=xla\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda-11.1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"7.5\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/x86_64-linux-gnu-gcc-7\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --copt=-mavx\r\nbuild:opt --copt=-mavx2\r\nbuild:opt --copt=-mfma\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_env=LD_LIBRARY_PATH\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n<details>\r\n<pre>\r\njosh@cryptex00005:~/tensorflow$ bazel build --config=v2 --config=monolithic //tensorflow/tools/lib_package:libtensorflow\r\nWARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=265\r\nINFO: Reading rc options for 'build' from /home/josh/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/josh/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/josh/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/josh/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/josh/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/josh/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file /home/josh/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/josh/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:v2 in file /home/josh/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file /home/josh/tensorflow/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:linux in file /home/josh/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/josh/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /home/josh/.cache/bazel/_bazel_josh/bbb24127b68cff8e4647a298dd3f5a01/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:5: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nINFO: Analyzed target //tensorflow/tools/lib_package:libtensorflow (233 packages loaded, 22913 targets configured).\r\n</pre>\r\n</details>\r\n\r\nHere is the resulting build: https://s3-us-west-2.amazonaws.com/bin.cryptexlabs.com/tensorflow/cuda-11.1_cudnn8.0.5_75_fma-avx-avx2.tar.gz", "comments": ["I found that my issue is that @tensorflow/tfjs is hard coded to use version 1.15 while I was compiling for tensorflow v2.x. So I have to checkout a 1.5 tag in this repo and then compile for cuda 10.0 and cudnn7.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45245\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45245\">No</a>\n", "Here is the build result for anyone running into this issue in the future https://s3-us-west-2.amazonaws.com/bin.cryptexlabs.com/tensorflow/tensorflow-1.15.4%2Bcuda-10.0%2Bcuda-capabilities-7.5%2BcuDNN-7.6.5%2Bfma%2Bavx%2Bavx2.tar.gz"]}]