[{"number": 24842, "title": "Fix TF_AddGradients to work within while loop frame", "body": "Adds a control edge from the pivot node to each newly created constant node in AddGradient. The pivot is chosen to be one of the input nodes to TF_AddGradients. This will ensure that each newly created node within the gradient subgraph is in the same (while loop) frame as the inputs.", "comments": ["See issue here for in-depth explanation: https://github.com/tensorflow/tensorflow/issues/24837\r\n\r\n@samdow @melissagrueter", "Nagging Reviewer @skye: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Closing this PR because addGradients will work correctly with [these changes](https://github.com/tensorflow/tensorflow/pull/25949), which avoids building while loops explicitly with control flow ops. "]}, {"number": 24841, "title": "Fix support for newer bazel versions - Issue 23673 closed with incorrect solution.", "body": "I believe https://github.com/tensorflow/tensorflow/issues/23673 was closed incorrectly.\r\n\r\nThe problem is newer versions of bazel are not reading the bazel.rc that provides:\r\n```\r\ntools/bazel.rc:build --define=grpc_no_ares=true\r\n```\r\n\r\nThe solution can not be to install ares as this option is supposed to disable the need for it.\r\n\r\nPlease fix the build process to support newer versions of bazel (in my case, using 0.19.0)", "comments": ["This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24841\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24841\">No</a>\n"]}, {"number": 24840, "title": "Issue/24450: allocate smaller slice length for 32-bit arch", "body": "This patch addresses issue [24450](https://github.com/tensorflow/tensorflow/issues/24450).\r\n\r\nSigned-off-by: Saurabh Deoras <sdeoras@gmail.com>", "comments": []}, {"number": 24839, "title": "importing tf-nightly-gpu error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Wheel\r\n- TensorFlow version: tf-nightly-gpu 1.13.0.dev20181225\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip on a clean conda env\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: rtx 2070 8GB GDDR6\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nCreated a new conda env, installed python 3.6.8 on it. Pip installed the windows wheel after clearing cache. Went into python in the console, imported tensorflow and got the following error.\r\n\r\n**Any other info / logs**\r\nPython 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 18:50:55) [MSC v.1915 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["Same here. +1\r\n\r\nEdit: Maybe -1 actually.", "I don't know if the nightly version of Tensorflow 1.13 supports CUDA10.0.However, the stable version of Tensorflow 1.13 installed via PIP does not support CUDA10.0.", "> I don't know if the nightly version of Tensorflow 1.13 supports CUDA10.0.However, the stable version of Tensorflow 1.13 installed via PIP does not support CUDA10.0.\r\n\r\nFrom what I've seen in this post, 1.13 should support once released. https://github.com/tensorflow/tensorflow/issues/22706", "Same error faced with same config and CUDA 10", "We're still working on CUDA10 for some platforms. You can probably wait it out, shouldn't take much longer. \r\n\r\n@av8ramit FYI.", "We are still working on adding CUDA 10 for Windows and the Ubuntu Python3.7 build for tf-nightly-gpu. Sorry for the delay and thank you for your patience. The official 1.13 will support CUDA 10.", "@av8ramit, so until cuda 10.0 is supported, it will not import correctly?", "Unfortunately, I believe until CUDA 10 is supported you'll see this import error with CUDA 10 out of the box", "@av8ramit is there a tentative timeframe for the release of 1.13?", "@martinwicke @av8ramit Any updates?", "The rc0 will be out probably next week, and the official release 3 weeks after that.", "@av8ramit the rc0 will support cuda 10, correct? ", "Yes we are adding CUDA 10 support to 1.13 rc0.", "Thank you for the information. Should be all questions answered for everyone with this issue."]}, {"number": 24838, "title": "importing tf-nightly-gpu error", "body": "os: windows 10\r\nAnaconda clean env\r\npip 10.4\r\npython 3.6.8\r\n\r\nI installed the windows python 3.6 wheel https://pypi.org/project/tf-nightly-gpu/1.13.0.dev20181225/#files and installed it in a clean environment. Importing Tensorflow yields me this error:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": []}, {"number": 24837, "title": "[TF Java, Control Flow] Constant nodes created via TF_AddGradients are not in the correct frame", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWe are using TF Java's `graph.addGradients` which calls upon c_api.h's `TF_AddGradients`. Currently the function works fine when called in the outermost frame (frame '')). \r\n\r\nHowever, we would like to call TF Java's `graph.addGradients` on nodes that are in the same while loop frame. This gives us the error:\r\n```\r\n[error] Exception in thread \"main\" java.lang.IllegalArgumentException: {{node Gradients/Multiply_1}} has inputs from different frames. The input {{node Gradients/Div_2}} is in frame ''. The input {{node Sub_1}} is in frame 'While'.\r\n```\r\nThe issue here is that specifically constant nodes created for the gradient subgraph via `TF_AddGradients`  are not in the correct frame. (The error mentions Multiply and Div nodes because the incorrect frame of constant nodes propagates to those nodes.) Unlike other nodes (Multiply, Sub, Div...), constant nodes do *not* take inputs from other nodes and therefore will always be in the outermost frame (frame '') unless a control input is specified.\r\n\r\n**Will this change the current api? How?**\r\nAdding a control edge from a node (let's call this node a pivot) within the while loop frame (frame 'While') to each constant node created by TF_AddGradients to forces the constant node into the while loop frame and our graph will run without error. \r\n\r\nSupport for this can be added within the `TF_AddGradients` function.\r\n\r\nThere is already a loop in the function that does a sanity check on each node. Some simple logic can be added to add a control edge from each newly created constant node to a pivot.\r\n\r\nThe pivot can be chosen as one of the given inputs to `TF_AddGradients`. This way we ensure that all nodes in the subgraph are in the same frame as the inputs. (Inputs to `TF_AddGradients` must be in the same frame.)\r\n\r\n**Who will benefit with this feature?**\r\nThose who want greater usability/flexibility in addGradients will benefit as this will allow addGradients to be called in frames other than the outermost.\r\n\r\n**Any Other info.**\r\n\r\nHere is a hack-y solution that assumes the pivot is the node created immediately before `TF_AddGradients` is called (rather than using one of the inputs as the pivot).\r\n\r\nIn `c_api.cc/TF_AddGradients`:\r\n```\r\n...\r\n    // Assumes the pivot is the node created immediately before the gradient nodes.\r\n    // TODO: Use input as pivot.\r\n    Node* pivot = g->graph.FindNodeId(first_new_node_id - 1);\r\n\r\n    for (int i = first_new_node_id; i < g->graph.num_node_ids(); ++i) {\r\n      Node* n = g->graph.FindNodeId(i);\r\n      if (n == nullptr) continue;\r\n\r\n      // Add control edge from each newly created constant node to the pivot to force\r\n      // the nodes to be in the same frame as pivot.\r\n      if (n->IsConstant()) {\r\n        g->graph.AddControlEdge(pivot, n);\r\n      }\r\n...\r\n```\r\n", "comments": ["@melissagrueter @samdow", "PR here: https://github.com/tensorflow/tensorflow/pull/24842", "PR #25949"]}, {"number": 24836, "title": "Bazel build failed building Android demo", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 2.7.12\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 0.19.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nRunning the following command:\r\n\r\n```\r\nbazel build --cxxopt='--std=c++11' -c opt //tensorflow/examples/android:tensorflow_demo --verbose_failures\r\n```\r\n\r\ncauses the following build errors:\r\n```\r\nINFO: From ProtoCompile tensorflow/core/example/example.pb.cc:\r\nbazel-out/android-armeabi-v7a-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/android-armeabi-v7a-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nERROR: /home/glorenz/.cache/bazel/_bazel_glorenz/701fde1b3ff5c4a449c0c2cab4570a97/external/com_google_absl/absl/base/BUILD.bazel:29:1: C++ compilation of rule '@com_google_absl//absl/base:spinlock_wait' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/glorenz/.cache/bazel/_bazel_glorenz/701fde1b3ff5c4a449c0c2cab4570a97/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=28.0.3 \\\r\n    ANDROID_NDK_API_LEVEL=14 \\\r\n    ANDROID_NDK_HOME=/home/glorenz/Android/Sdk/ndk-bundle \\\r\n    ANDROID_SDK_API_LEVEL=23 \\\r\n    ANDROID_SDK_HOME=/home/glorenz/Android/Sdk \\\r\n    PATH=/home/glorenz/Workspace/cat-local/cat_test_1/bin:/home/glorenz/bin:/home/glorenz/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/glorenz/.dotnet/tools:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/lib/python2.7/dist-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -fno-integrated-as -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/_objs/spinlock_wait/spinlock_wait.d '-frandom-seed=bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/_objs/spinlock_wait/spinlock_wait.o' -iquote external/com_google_absl -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/com_google_absl -iquote bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/bazel_tools -iquote bazel-out/android-armeabi-v7a-opt/bin/external/bazel_tools '--std=c++11' -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -Wno-sign-compare '--sysroot=external/androidndk/ndk/platforms/android-14/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c external/com_google_absl/absl/base/internal/spinlock_wait.cc -o bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/_objs/spinlock_wait/spinlock_wait.o)\r\nIn file included from external/com_google_absl/absl/base/internal/spinlock_wait.cc:27:\r\nIn file included from external/com_google_absl/absl/base/internal/spinlock_linux.inc:17:\r\nexternal/androidndk/ndk/platforms/android-14/arch-arm/usr/include/linux/futex.h:28:21: error: field has incomplete type 'struct robust_list'\r\n struct robust_list __user *next;\r\n                    ^\r\nexternal/androidndk/ndk/platforms/android-14/arch-arm/usr/include/linux/futex.h:27:8: note: definition of 'robust_list' is not complete until the closing '}'\r\nstruct robust_list {\r\n       ^\r\nexternal/androidndk/ndk/platforms/android-14/arch-arm/usr/include/linux/futex.h:28:27: error: expected ';' at end of declaration list\r\n struct robust_list __user *next;\r\n                          ^\r\nexternal/androidndk/ndk/platforms/android-14/arch-arm/usr/include/linux/futex.h:37:27: error: expected ';' at end of declaration list\r\n struct robust_list __user *list_op_pending;\r\n                          ^\r\n3 errors generated.\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\nINFO: Elapsed time: 176.016s, Critical Path: 25.68s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 340 processes: 328 local, 12 worker.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nThis happens when using the most recent commit (with the hashcode `3c4b35297e4b44baaf4cc7ed5b099c586ffa049b`).\r\nHowever, the build suceeds if the cloned repository is hard reset to the commit with the following hashcode:\r\n`30ebb9b712b2008525944deb47dee8b653f522a3`.\r\nThis also happened with bazel 0.21.0, which is the latest version to date.\r\n\r\n**Full Log**\r\n[stack.txt](https://github.com/tensorflow/tensorflow/files/2746756/stack.txt)\r\n", "comments": ["@tferrerITBA Your issue looks similar to the following [1](https://github.com/tensorflow/tensorflow/issues/23673), [2](https://github.com/tensorflow/tensorflow/issues/24841), and [3](https://github.com/tensorflow/tensorflow/issues/24905). Could you follow the solutions in those posts and let us know how it progresses? Thanks!", "@jvishnuvardhan the issues mentioned above are different (I did try their solutions, however). In [1](https://github.com/tensorflow/tensorflow/issues/23673) and [2](https://github.com/tensorflow/tensorflow/issues/24841), one of the solutions is to install `libc-ares-dev`, which I did (even though my `.bazelrc` file contains the line `build --define=grpc_no_ares=true`). Nothing changed. The other solution was to add the line `import tools/bazel.rc` but there is no such file in the `master` branch of tensorflow (the one I work on). Instead, I noticed most of the contents of another user's `bazel.rc` are already included in `.bazelrc`. And the bazel version is unlikely to be the problem, as both 0.19.x and 0.21.0 failed to build.\r\nI tried everything in another Ubuntu 16.04 installation but it didn't help. It may be useful to know that `which gcc` returns `/usr/bin/gcc`.", "Dont think im the correct person to assign this to. Not hugely familiar with the android build. Maybe petewarden@", "I have exactly the same issue.", "I found the solution for the building. Follow this: https://github.com/tensorflow/tensorflow/issues/20192", "@tferrerITBA \r\nClosing this out since I understand it to be resolved, but please let me know if I'm mistaken.\r\n", "> I found the solution for the building. Follow this: #20192\r\n\r\nPerfect! Changing the line `build --action_env ANDROID_NDK_API_LEVEL=\"21\"` in the file `.tf_configure.bazelrc` worked like a charm. Thanks!", "sorry, my friend, i am not familiar with this bazel and Android, where is the .tf_configure.bazelrc file?? i do not find it. plz. thank you very much", "@tferrerITBA plz ", "@NorwayLobster It most likely depends on how you configured tensorflow in the first place. In the `.bazelrc` file you can see the line `try-import %workspace%/.tf_configure.bazelrc`. I think you are asked to set your workspace's directory after running `./configure`. It defaults to tensorflow's root folder."]}, {"number": 24835, "title": "Windows 10/Anaconda3: Missing File on install", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Installed from pip\r\n- TensorFlow version: 2 Preview\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n**Describe the problem**\r\nI created a virtual environment and tried to install the preview that Martin Wicke suggested on twitter. It failed with a missing file.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nCommands from Anaconda3 prompt:\r\nconda create -n tf_daily python=3.6\r\nactivate tf_daily\r\npip install tf-nightly-2.0-preview\r\n\r\nError Message:\r\n(tf_daily) C:\\Users\\nurl_>pip install tf-nightly-2.0-preview\r\nCollecting tf-nightly-2.0-preview\r\n  Using cached https://files.pythonhosted.org/packages/a6/d9/0499db98422207eb3d7643ee3b8152dd503a85dc2f958f77834aa0a3fcde/tf_nightly_2.0_preview-1.13.0.dev20190108-cp36-cp36m-win_amd64.whl\r\nCould not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\nurl_\\\\AppData\\\\Local\\\\Temp\\\\pip-install-r8cg1kry\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-1.13.0.dev20190108.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'\r\n\r\n", "comments": ["Anna, do you know what could be wrong here?", "This is likely due to path length limits on Windows. There should be a way to disable this limit on Windows 10.\r\n\r\n@ehennis can you try following instructions here:\r\nhttps://mspoweruser.com/ntfs-260-character-windows-10/", "I didn't have that option in the group policy section but I was able to update using the registry (https://www.itprotoday.com/windows-10/enable-long-file-name-support-windows-10). After a reboot I was able to install.\r\n\r\nSomewhat related, I have a similar issue with Windows limits with a jupyter notebook using a virtual environment. The shortcut path is too long and it crashes. Go Windows.", "1. Hit the Windows key, type `gpedit.msc` and press Enter.\r\n2. Navigate to `Local Computer Policy > Computer Configuration > Administrative Templates > System > Filesystem`.\r\n3. Double click the `Enable Win32 long paths` option and enable it.\r\n\r\nref: https://superuser.com/questions/1119883/windows-10-enable-ntfs-long-paths-policy-option-missing", "@wangzhe258369 I did have the setting at that location. I was down in the NTFS section looking earlier.", "How about Mac?", "HowToGeek has a good tutorial about directly editing the registry to disable the character limit. This can be used on Windows devices that are not running the Windows Pro or Enterprise editions, unlike the Group Policy mentioned above. I followed those instructions, and that allowed Tensorflow to finish installing on my device:\r\n\r\n1. Run `regedit` by typing it in the search bar or through Powershell or command prompt.\r\n2. Under `HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem` is a registry key called `LongPathsEnabled`. Set that key to 1. This disables the 260 character path limit.\r\n\r\nhttps://www.howtogeek.com/266621/how-to-make-windows-10-accept-file-paths-over-260-characters/", "> HowToGeek has a good tutorial about directly editing the registry to disable the character limit. This can be used on Windows devices that are not running the Windows Pro or Enterprise editions, unlike the Group Policy mentioned above. I followed those instructions, and that allowed Tensorflow to finish installing on my device:\r\n> \r\n> 1. Run `regedit` by typing it in the search bar or through Powershell or command prompt.\r\n> 2. Under `HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem` is a registry key called `LongPathsEnabled`. Set that key to 1. This disables the 260 character path limit.\r\n> \r\n> https://www.howtogeek.com/266621/how-to-make-windows-10-accept-file-paths-over-260-characters/\r\n\r\nThis was helpful! Thank you!"]}, {"number": 24834, "title": "tf-nightly-2.0-preview failed to install on my Windows 7/64", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7/64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["pip install tf-nightly-2.0-preview does not install on my Windows 7/64\r\n\r\n\r\nCollecting tf-nightly-2.0-preview\r\n  Using cached https://files.pythonhosted.org/packages/a6/d9/0499db98422207eb3d7643ee3b8152dd503a85dc2f958f77834aa0a3fcde/tf_nightly_2.0_pre\r\nview-1.13.0.dev20190108-cp36-cp36m-win_amd64.whl\r\nCould not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\brm738\\\\AppData\\\\Local\\\\Temp\\\\pip-in\r\nstall-ku5u1ik5\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-1.13.0.dev20190108.data/purelib/tensorflow/include/tensorflow/include/externa\r\nl/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'", "Can you use please setup a virtual environment and try again?\r\n>virtualenv -p python3 venv-tf-nightly-2.0     \r\n>source venv-tf-nightly-2.0/bin/activate\r\n>pip3 install tf-nightly-2.0-preview\r\n", "Thank you ymodak.\r\nI installed the vitualenv via pip install and followed your instruction adopting it to the Windows, (yours is for Linux).\r\nMy python is mapped to python3.\r\n**virtualenv -p python venv-tf-nightly-2.0\r\nvenv-tf-nightly-2.0\\Scripts\\activate\r\npip3 install tf-nightly-2.0-preview**\r\n\r\nStill get the same environment error:\r\n\r\n**(venv-tf-nightly-2.0) C:\\Users\\brm738>pip3 install tf-nightly-2.0-preview**\r\nCollecting tf-nightly-2.0-preview\r\n  Using cached https://files.pythonhosted.org/packages/5d/ac/238d28669c12d5cac587863ac16fd022b7692049c906be6c4986e64dfb87/tf_nightly_2.0_preview-1.13.0.dev20190110-cp36-cp36m-win_amd64.whl\r\n**Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\brm738\\\\AppData\\\\Local\\\\Temp\\\\pip-install-yrga1ect\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-1.13.0.dev20190110.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupp orted/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'**\r\n\r\n", "Thanks for the information. This issue is likely due to limitation of path length on windows. See a duplicate issue #24835 ", "Not possible to get rid of path length limitation on Windows 7. I have to upgrade to Windows 10", "I will close this issue since upgrading your OS can help fix it. Please feel free to reopen if have any further problems. Thanks!", "For those who can't upgrade their OS (company policies, ...) and still want to access nightly previews (issue has been solved in alpha version `pip install tensorflow==2.0.0-alpha0`):\r\n- Download the latest wheel matching their configuration from pypi:\r\n-- GPU version here; https://pypi.org/project/tf-nightly-gpu-2.0-preview/#history \r\n-- CPU version here: https://pypi.org/project/tf-nightly-2.0-preview/#history\r\n- Open the whl with a zip application (like 7-zip)\r\n- Edit the 2 folders names at root level of the archive to remove both the \"_preview\" and \"dev2019XXXX.\" string in their names\r\n- Extract the RECORD file in the root subfolder ending in .dist-info\r\n- Edit the extracted RECORD file and remove all \"_preview\" and \"dev2019XXXX.\" strings\r\n- Put the modified RECORD file back into the archive\r\n- Install the wheel with pip using command `pip install <path_to_modified.whl>`\r\n\r\n", "@IkabongoCS thanks for the update and the workaround. After following your instructions, however, I keep running into a directory error:\r\n\r\n```\r\nC:\\Users\\asus>pip install \"C:\\Users\\asus\\Downloads\\tf_nightly_gpu_2.0_preview-2.0.0.dev20190327-cp36-cp36m-win_amd64.whl\"\r\nProcessing c:\\users\\asus\\downloads\\tf_nightly_gpu_2.0_preview-2.0.0.dev20190327-cp36-cp36m-win_amd64.whl\r\nRequirement already satisfied: protobuf>=3.6.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (3.6.1)\r\nRequirement already satisfied: six>=1.10.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (1.10.0)\r\nRequirement already satisfied: google-pasta>=0.1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (0.1.4)\r\nRequirement already satisfied: numpy<2.0,>=1.14.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (1.15.4)\r\nRequirement already satisfied: tb-nightly<1.15.0a0,>=1.14.0a0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (1.14.0a20190301)\r\nRequirement already satisfied: keras-applications>=1.0.6 in c:\\users\\asus\\appdata\\roaming\\python\\python36\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (1.0.6)\r\nRequirement already satisfied: termcolor>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (1.1.0)\r\nCollecting tensorflow-estimator-2.0-preview (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327)\r\n  Downloading https://files.pythonhosted.org/packages/ed/d2/7d9931a45b1085c5f74132fd6178982fc8481ee10e2016a7973696da0c92/tensorflow_estimator_2.0_preview-1.14.0.dev2019040100-py2.py3-none-any.whl (352kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 358kB 1.2MB/s\r\nRequirement already satisfied: grpcio>=1.8.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (1.16.1)\r\nRequirement already satisfied: astor>=0.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (0.7.1)\r\nRequirement already satisfied: absl-py>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (0.7.1)\r\nRequirement already satisfied: gast>=0.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (0.2.2)\r\nRequirement already satisfied: wheel>=0.26 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (0.29.0)\r\nRequirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\asus\\appdata\\roaming\\python\\python36\\site-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (1.0.5)\r\nRequirement already satisfied: setuptools in c:\\users\\asus\\appdata\\roaming\\python\\python36\\site-packages (from protobuf>=3.6.1->tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (39.1.0)\r\nRequirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (3.0.1)\r\nRequirement already satisfied: werkzeug>=0.11.15 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (0.12.2)\r\nRequirement already satisfied: h5py in c:\\users\\asus\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tf-nightly-gpu-2.0-preview==2.0.0.dev20190327) (2.8.0)\r\nInstalling collected packages: tensorflow-estimator-2.0-preview, tf-nightly-gpu-2.0-preview\r\nException:\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\asus\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 179, in main\r\n    status = self.run(options, args)\r\n  File \"c:\\users\\asus\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 393, in run\r\n    use_user_site=options.use_user_site,\r\n  File \"c:\\users\\asus\\anaconda3\\lib\\site-packages\\pip\\_internal\\req\\__init__.py\", line 57, in install_given_reqs\r\n    **kwargs\r\n  File \"c:\\users\\asus\\anaconda3\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 913, in install\r\n    use_user_site=use_user_site, pycompile=pycompile,\r\n  File \"c:\\users\\asus\\anaconda3\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 445, in move_wheel_files\r\n    warn_script_location=warn_script_location,\r\n  File \"c:\\users\\asus\\anaconda3\\lib\\site-packages\\pip\\_internal\\wheel.py\", line 428, in move_wheel_files\r\n    assert info_dir, \"%s .dist-info directory not found\" % req\r\nAssertionError: tf-nightly-gpu-2.0-preview==2.0.0.dev20190327 .dist-info directory not found\r\n\r\nC:\\Users\\asus>pip install \"C:\\Users\\asus\\Downloads\\tf_nightly_gpu_2.0-cp36-cp36m-win_amd64.whl\"\r\ntf_nightly_gpu_2.0-cp36-cp36m-win_amd64.whl is not a valid wheel filename.\r\n\r\nC:\\Users\\asus>pip install \"C:\\Users\\asus\\Downloads\\tf_nightly_gpu_2.0-cp36-cp36m-win_amd64.whl\"\r\ntf_nightly_gpu_2.0-cp36-cp36m-win_amd64.whl is not a valid wheel filename.\r\n\r\nC:\\Users\\asus>cd Downloads\r\n\r\nC:\\Users\\asus\\Downloads>pip install tf_nightly_gpu_2.0-cp36-cp36m-win_amd64.whl\r\ntf_nightly_gpu_2.0-cp36-cp36m-win_amd64.whl is not a valid wheel filename.\r\n\r\nC:\\Users\\asus\\Downloads>pip install tf_nightly_gpu_2.0.whl6\r\nCollecting tf_nightly_gpu_2.0.whl6\r\n  Could not find a version that satisfies the requirement tf_nightly_gpu_2.0.whl6 (from versions: )\r\nNo matching distribution found for tf_nightly_gpu_2.0.whl6\r\n\r\nC:\\Users\\asus\\Downloads>pip install tf_nightly_gpu_2.0.whl\r\ntf_nightly_gpu_2.0.whl is not a valid wheel filename.\r\n```", "Hi @dorian821 , to fix that issue is that you may not rename the .dist-info inside the wheel and also you may not rename the wheel filename. So the places to remove \"_preview\" and \"dev2019XXXX\" is the folder (not .dist-info) and the RECORD file. I am in Windows 7 as well btw.", "Hi @aaron-maslim, :+1: thanks a lot! it installed just fine!", "Great to hear that!", "> Hi @dorian821 , to fix that issue is that you may not rename the .dist-info inside the wheel and also you may not rename the wheel filename. So the places to remove \"_preview\" and \"dev2019XXXX\" is the folder (not .dist-info) and the RECORD file. I am in Windows 7 as well btw.\r\n\r\nHi, I did as instructed, \r\ni only renamed the folders\r\nand then edited the RECORD file without changing the file name..\r\ni'm still getting the error that the file name is still too long\r\n\r\n\r\n```\r\nCould not install packages due to an EnvironmentError: [WinError 206] The filename or extension is too long: 'C:\\\\Users\\\\-----------------\\\\AppData\\\\Local\\\\Temp\\\\pip-install-s_wy_wb_\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-2.0.0.dev20190411-cp36-cp36m-win_amd64/tf_nightly_2.0-2.0.0.data/purelib/tensorflow/include/external/absl_py/absl/third_party/unittest3_backport/\r\n```\r\n\r\nany other suggestions?\r\nthanks", "@RadEdje Hi, I resolved this problem by _only_ renaming the _data_ folder and the file paths in the RECORD file. That is, I left the dist-info folder as it is, unchanged. \r\n\r\nIn your response, you mention that you renamed the folder**s**, but you should only rename one folder, the data folder. ", "@RadEdje I would suggest also renaming the wheel file to make it shorter (only remove `\"_preview\" and \"dev2019XXXX.\"`), without removing other essential information ([https://stackoverflow.com/questions/33213430/whl-is-not-a-valid-wheel-filename-storing-debug-log-for-failure-in-c](url)).", "> @RadEdje Hi, I resolved this problem by _only_ renaming the _data_ folder and the file paths in the RECORD file. That is, I left the dist-info folder as it is, unchanged.\r\n> \r\n> In your response, you mention that you renamed the folder**s**, but you should only rename one folder, the data folder.\r\n\r\nHi. thanks for the suggestion.\r\ni tried just renaming the data folder.\r\nsame thing.\r\nthe file path name is still too long.\r\n\r\n\r\n```\r\nCould not install packages due to an EnvironmentError: [WinError 206] The filename or extension is too long: 'C:\\\\Users\\\\------------------------\\\\AppData\\\\Local\\\\Temp\\\\pip-install-v3dk57w_\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-2.0.0.dev20190411-cp36-cp36m-win_amd64/tf_nightly_2.0-2.0.0.data/purelib/tensorflow/include/external/absl_py/absl/third_party/unittest3_backport/'\r\n```\r\n\r\nThanks for the help.\r\nis there anything else i can remove to make the filepath shorter?\r\nI'm on windows 8.1.\r\npython 3.6\r\nusing 7zip\r\nand Microsoft VS code to replace/remove all the instances of _preview and the dev numbers in the RECORD file.\r\n\r\n\r\nI noticed something....\r\nwhen i made the file name short enough it was spewing out a different error;\r\n\r\n```\r\nCould not install packages due to an EnvironmentError: [Errno 2] No such file or\r\ndirectory: 'C:\\\\Users\\\\-----------\\\\AppData\\\\Local\\\\Temp\\\\pip-install-257lmry1\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-2.0.0.dev20190411-cp36-cp36m-win_amd64/tf_nightly_2.data/purelib/tensorflow/include/external/com_google_absl/absl/container/internal/hashtablez_sampler.h'\r\n```\r\n\r\n\r\nno such file exists.\r\ni checked the file name structure.\r\nseems there is an inconsistency in the slash and back slash. the first ones use a windows style folder path name structure while the second half uses a bash style.\r\n\r\nAfter this. i tried 2 things.\r\n\r\ni replaced all the \"/\" with a the opposite slash in the RECORD file.\r\nthat did not work.\r\n\r\nI then tried running the script both in BASH and in windows CMD.\r\n\r\nstill the formatting is\r\n\r\n```\r\nC:\\\\Users\\\\-----------\\\\AppData\\\\Local\\\\Temp\\\\pip-install-257lmry1\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-2.0.0.dev20190411-cp36-cp36m-win_amd64/tf_nightly_2.data/purelib/tensorflow/include/external/com_google_absl/absl/container/internal/hashtablez_sampler.h\r\n```\r\n\r\nNotice the inconsistency in the slashes and back slashes. i think this is where the problem lies.\r\n\r\nany suggestions?\r\n\r\nthanks again.\r\n", "for clarity, what exactly have you renamed?\r\n\r\nAlso, since you're on windows 8, you should be able to extend the file path limit to 30k+ by changing the relevant key in the registry as discussed in the thread here: \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/24835\r\n\r\n", "> for clarity, what exactly have you renamed?\r\n> \r\n> Also, since you're on windows 8, you should be able to extend the file path limit to 30k+ by changing the relevant key in the registry as discussed in the thread here:\r\n> \r\n> #24835\r\n\r\nOh... I thought that was only for windows 10.\r\nI'll back read.\r\n\r\nanyway here are the renamed folder (singular now as you suggested i only renamed the data folder):\r\n\r\n```\r\ntf_nightly_2.0-2.0.0.data\r\n```\r\n\r\nUsing VS code I replaced/removed all of the instances of \r\n\"_preview\" and \"dev2019XXXX.\" strings in the RECORD file but I did not change the file name.\r\n\r\nwhen the\r\nfile path name is too long error is no longer seen. A new error shows up.\r\nThat's the error that says the \r\nthat a file does not exist.\r\nI think it's in the formatting...\r\n\r\nnote how the slashes change from\r\n\r\nslash `/` to back slash `\\\\`\r\n\r\njust my two cents.\r\n\r\nAnyway i'll try to change the registry in windows 8.\r\n\r\n\r\n\r\n\r\n\r\n", "I've just tested renaming (removed -preview and .devXXXX) the wheel file, entries in RECORD, the data folder and the dist-info folder, installation works for me.", "ok, great. thanks for clarifying. I had the same thought regarding the slash, but it won't be an issue once the wheel is set up properly. \r\n\r\nit looks like you might have grabbed a trailing period in the RECORD file, \r\n\r\nthe paths should go from:\r\n\r\ntf_nightly_2.0_preview-2.0.0.dev20190401.data/purelib/\r\n\r\nto \r\n\r\ntf_nightly_gpu_2.0.data/purelib/\r\n\r\nwith a period separating 2.0 and data\r\n\r\nit looks like you took \"dev2019XXXX.\" and so may have \"tf_nightly_gpu_2.0data/purelib/\", which is incorrect\r\n\r\n\r\ncheck to make sure that this is the case. ", "> ok, great. thanks for clarifying. I had the same thought regarding the slash, but it won't be an issue once the wheel is set up properly.\r\n> \r\n> it looks like you might have grabbed a trailing period in the RECORD file,\r\n> \r\n> the paths should go from:\r\n> \r\n> tf_nightly_2.0_preview-2.0.0.dev20190401.data/purelib/\r\n> \r\n> to\r\n> \r\n> tf_nightly_gpu_2.0.data/purelib/\r\n> \r\n> with a period separating 2.0 and data\r\n> \r\n> it looks like you took \"dev2019XXXX.\" and so may have \"tf_nightly_gpu_2.0data/purelib/\", which is incorrect\r\n> \r\n> check to make sure that this is the case.\r\n\r\n\r\nHi did as you asked, \r\n\r\nunfortuantely same error. the file does not exist:\r\n\r\n\r\n```\r\nCould not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\----------------------\\\\AppData\\\\Local\\\\Temp\\\\pip-install-eluview2\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-2.0.0.dev20190411-cp36-cp36m-win_amd64/tf_nightly_2.0.data/purelib/tensorflow/include/external/com_google_absl/absl/base/internal/low_level_scheduling.h'\r\n\r\n```\r\n\r\nthis is the untouched name of the whl file\r\n```\r\ntf_nightly_2.0_preview-2.0.0.dev20190411-cp36-cp36m-win_amd64.whl\r\n```\r\n\r\nhere is the name of the DATA folder\r\n```\r\ntf_nightly_2.0.data\r\n```\r\n\r\nhere is a sample of a string line in the RECORD file (RECORD FILE name also untouched)\r\nthis is also the file that's allegedly missing in the error log\r\n\r\n```\r\ntf_nightly_2.0.data/purelib/tensorflow/include/external/com_google_absl/absl/base/internal/low_level_scheduling.h,sha256=5x8MD_0rHwmOE3L-IeK-B4bbMU0yJtCNHHWzpOB3Zt0,3985\r\n\r\n```\r\n\r\nAny suggestions?\r\n\r\nthanks again for your patience and help.\r\n", "@RadEdje  It looks like you removed too many things:\r\n- DATA folder, I have : tf_nightly_2.0-2.0.0.data\r\n- RECORD sample, I have: tf_nightly_2.0-2.0.0.data/purelib/tensorflow/__init__.py,sha256=4cVbFxbZBU5CbfDK9gA9BVyoEcDwwrs93aZrX0pMASE,18489", "\r\n\r\n@RadEdje It looks like you removed too many things:\r\n\r\nDATA folder, I have : tf_nightly_2.0-2.0.0.data\r\nRECORD sample, I have: tf_nightly_2.0-2.0.0.data/purelib/tensorflow/init.py,sha256=4cVbFxbZBU5CbfDK9gA9BVyoEcDwwrs93aZrX0pMASE,18489\r\n\r\n\r\nHi, that was my original format.\r\nThat did not work either.\r\n:-(", "just wanted to update.\r\ni gave up on windows 8.1\r\ndual booted with ubuntu.\r\nEverything installed and worked flawlessly without a hitch on the linux machine just with the pip install commands. Never found out what I was doing wrong on windows though. "]}, {"number": 24833, "title": "Triplet loss semi hard function does not work", "body": "The triplet loss semi hard function does not work in the line where have an assertion which checking variable dimension", "comments": ["@zhasulan Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? \r\n\r\nAlong with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "You can make it work by removing the dimension assert in the function, Another issue is that this function often returns nan values that are messing with the training, I have created another function that works with tensorflow eager execution, it still needs optimization, I will create a pull request.", "@thebeancounter Thanks for the workaround and also for working on the solution. I will close this issue. Please feel free to open If you see the issue again. Thanks!"]}, {"number": 24832, "title": "Could NOT find CUDA: Found unsuitable version \"9.2\"  ON WINDOWS SOURCE BUILD WITH CMAKE", "body": "\r\n**System information**\r\n- Windows 10\r\n- TensorFlow installed from source: r1.8\r\n- TensorFlow version: r1.8\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): not use bazel\r\n- GCC/Compiler version (if compiling from source): cmake 3.13.0-rc1\r\n- CUDA/cuDNN version: 9.2/7.1.4\r\n- GPU model and memory: 2G\r\n\r\nwhen i try to build rensorflow on windows,there is the error:\r\n\r\nD:\\tf3\\tensorflow-r1.8\\tensorflow\\contrib\\cmake\\build>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/tf3/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=C:/Python36/python.exe -DPYTHON_LIBRARIES=C:/Python36/python36.dll -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\"\r\n-- Building for: Visual Studio 15 2017\r\n-- The C compiler identification is MSVC 19.16.27026.1\r\n-- The CXX compiler identification is MSVC 19.16.27026.1\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed\r\n-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED\r\n-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED - Success\r\nCMake Error at C:/cmake-3.13.0-rc1-win64-x64/share/cmake-3.13/Modules/FindPackageHandleStandardArgs.cmake:137 (message):\r\n  **Could NOT find CUDA: Found unsuitable version \"9.2\", but required is exact\r\n  version \"9.0\" (found C:/Program Files/NVIDIA GPU Computing\r\n  Toolkit/CUDA/v9.2)**\r\nCall Stack (most recent call first):\r\n  C:/cmake-3.13.0-rc1-win64-x64/share/cmake-3.13/Modules/FindPackageHandleStandardArgs.cmake:376 (_FPHSA_FAILURE_MESSAGE)\r\n  C:/cmake-3.13.0-rc1-win64-x64/share/cmake-3.13/Modules/FindCUDA.cmake:1099 (find_package_handle_standard_args)\r\n  CMakeLists.txt:310 (find_package)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"D:/tf3/tensorflow-r1.8/tensorflow/contrib/cmake/build/CMakeFiles/CMakeOutput.log\".\r\nSee also \"D:/tf3/tensorflow-r1.8/tensorflow/contrib/cmake/build/CMakeFiles/CMakeError.log\".\r\n\r\n**and then, I change the CUDA version to 9.0,like this: (changed virtual path)**\r\n\r\nD:\\tf3\\test\\tensorflow-r1.8\\tensorflow\\contrib\\cmake\\build>\"C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64\\vcvars64.bat\"\r\nD:\\tf3\\test\\tensorflow-r1.8\\tensorflow\\contrib\\cmake\\build>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/tf3/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=C:/Python36/python.exe -DPYTHON_LIBRARIES=C:/Python36/libs/python36.lib -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\"\r\n-- Building for: Visual Studio 15 2017\r\n-- The C compiler identification is MSVC 19.16.27026.1\r\n-- The CXX compiler identification is MSVC 19.16.27026.1\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed\r\n-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED\r\n-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED - Success\r\n-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0 (found suitable exact version \"9.0\")\r\nCMake Error at CMakeLists.txt:322 (message):\r\n  Selected compiler (or version) is not supported for CUDA\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"D:/tf3/test/tensorflow-r1.8/tensorflow/contrib/cmake/build/CMakeFiles/CMakeOutput.log\".\r\nSee also \"D:/tf3/test/tensorflow-r1.8/tensorflow/contrib/cmake/build/CMakeFiles/CMakeError.log\".\r\n\r\nso how can i to do it correctly?,please help....\r\n", "comments": []}, {"number": 24831, "title": "Build a Convolutional Neural Network using Estimators", "body": "Hello\r\n\r\nI'm learning CNN with mnist from this [Documentation](https://www.tensorflow.org/tutorials/estimators/cnn). I followed the tutorial step by step. After I finished the code and run it, the error comes: \r\n\r\n**ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for 'sparse_softmax_cross_entropy_loss/remove_squeezable_dimensions/Squeeze' (op: 'Squeeze') with input shapes: [100,10].**\r\n\r\nBelow is the code I think is related code:\r\n\r\n`loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)`\r\n\r\nThe labels are from the mnist dataset:\r\n\r\n`train_labels = np.asarray(mnist.train.labels, dtype=np.int32)`\r\n\r\n     train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": train_data},\r\n        y=train_labels,\r\n        batch_size=100,\r\n        num_epochs=None,  \r\n        shuffle=True)     \r\n    mnist_classifier.train(\r\n        input_fn=train_input_fn,\r\n        steps=20000,\r\n        hooks=[logging_hook])`\r\n\r\n\r\n\r\n\r\n", "comments": ["the reason is I import local dataset:`mnist = input_data.read_data_sets('MNIST_data',  one_hot=True)`\r\n\r\nit should be `mnist = input_data.read_data_sets('MNIST_data')`"]}, {"number": 24830, "title": "TfLiteCameraDemo performance of NNAPI on Honor10 View (Kirin 970)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO.\r\nI used the TF code from the TFLiteCameraDemo example. Only modified the line 47 in interpreter.cc where I set UseNNAPI(true);\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Honor 10 View (with Kirin970 and NPU)\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): tf: 1.5.0\r\n- Python version: 2.7.15rc1\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): GCC 7.3.0\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\nI compiled the demo app twice using two different bazel commands:\r\n\r\ncase 1. bazel build -c opt --cxxopt='--std=c++11' \\\r\n  //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo\r\n\r\ncase 2. bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n\r\n**Describe the current behavior**\r\nI was switching in the app between CPU and NNAPI and checked the execution time.\r\nIn case 1 the CPU was faster then NNAPI\r\nIn case 2 both CPU and NNAPI gave approximately the same execution time. No real difference in was seen. \r\n\r\n**Describe the expected behavior**\r\nExpected behavior is that the NNAPI time should be much shorter than CPU\r\n\r\n\r\n", "comments": ["My Honor 10 View has Android 9.0.", "By default, the TfLiteCameraDemo uses quantized model, but, as far as I know, Kirin 970's neural network accelerator is for floating point. Not sure if the Honor 10 View has NNAPI driver or not. Assuming it does, it's for floating point models only I think.\r\n\r\nIf it does have NNAPI driver, switch to a floating point model then you can see the difference you expect. ", "**No difference.** In both cases CPU and NNAPI, the timings are similar - between 300ms and 400ms - when set to mobilenet V1 floating point model. \r\n**Does it mean: NO driver available?**  \r\nSetting the model to quantize makes the inference faster for both CPU and floating point case. In both cases I got between 50ms and 150ms. \r\n**Is setting UseNNAPI(true) needed to activate the driver?**\r\nI set it to **true** before compiling with bazel but I did not see any timing difference with setting to **false**. \r\n**How to fix it?** \r\n**Where can I check if Kirin supports the NNAPI?**", "According to this source:\r\n**https://www.xda-developers.com/honor-view-10-mini-review/**\r\nKirin 970 should have the driver and NNAPI support:\r\n\r\n_Beyond the dedicated Kirin AI API, the NPU can be accessed through Android\u2019s Neural Networks API (NNAPI), and it supports many popular machine learning platforms such as Facebook\u2019s Caffe2 and Google\u2019s TensorFlow and TensorFlow Lite. In other words, apps with AI-powered features can make the most of the View 10\u2019s hardware._", "As far as I know, there is no standard way to check if there are NN HAL drivers and if they are configured properly. Enable NNAPI debugging may be helpful:\r\n```\r\n> adb shell setprop debug.nn.vlog 1\r\n```\r\nthen you can check message with `adb logcat`", "thank you.\r\nI followed your advice. Attached is the logcat log.\r\nHere are some filtered lines. You can see a possible problem:\r\n**Device ipuadaptor can't do operation CONV_2D**\r\n\r\n[honor10view-nnapi-logcat.txt](https://github.com/tensorflow/tensorflow/files/2749187/honor10view-nnapi-logcat.txt)\r\n\r\n\r\n01-11 10:50:09.006 26033 28849 I TfLiteCameraDemo: Changing model to mobilenet v1 float device NNAPI\r\n01-11 10:50:09.009 26033 28849 D TfLiteCameraDemo: Created a Tensorflow Lite Image Classifier.\r\n01-11 10:50:09.031 26033 28849 D TfLiteCameraDemo: Timecost to put values into ByteBuffer: 11\r\n01-11 10:50:09.031 26033 28849 I ModelBuilder: setOperandValue for operand 0 size 3456\r\n01-11 10:50:09.031 26033 28849 I ModelBuilder: Saving large value\r\n01-11 10:50:09.031 26033 28849 I ModelBuilder: setOperandValue for operand 1 size 18432\r\n01-11 10:50:09.031 26033 28849 I ModelBuilder: Saving large value\r\n01-11 10:50:09.031 26033 28849 I ModelBuilder: setOperandValue for operand 2 size 1048576\r\n01-11 10:50:09.031 26033 28849 I ModelBuilder: Saving large value\r\n..\r\n..\r\n..\r\n01-11 10:50:09.065   740 13436 I aiserver: IPUNNAdaptor getSupportedOperations_1_1\r\n01-11 10:50:09.065   740 13436 I aiserver: AiModelMngrService getSupportedOperations_1_1\r\n01-11 10:50:09.066   740 13436 E aiserver: AiModelMngrServic::getSupportedOperations_1_1 not relaxed Model.\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: Device ipuadaptor can't do operation CONV_2D\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 1\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: Device ipuadaptor can't do operation DEPTHWISE_CONV_2D\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(DEPTHWISE_CONV_2D) = 1\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: Device ipuadaptor can't do operation CONV_2D\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 1\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: Device ipuadaptor can't do operation DEPTHWISE_CONV_2D\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(DEPTHWISE_CONV_2D) = 1\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: Device ipuadaptor can't do operation CONV_2D\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 1\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: Device ipuadaptor can't do operation DEPTHWISE_CONV_2D\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(DEPTHWISE_CONV_2D) = 1\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: Device ipuadaptor can't do operation CONV_2D\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 1\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: Device ipuadaptor can't do operation DEPTHWISE_CONV_2D\r\n01-11 10:50:09.066 26033 28849 I ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(DEPTHWISE_CONV_2D) = 1\r\n..\r\n..\r\n01-11 10:50:09.243   719 28864 W Misc____: [288a640_64W] saveQRSeamlessInfo, getQRSeamlessMap fail!\r\n01-11 10:50:09.245 26033 26033 D HwAppInnerBoostImpl: asyncReportData com.example.android.tflitecamerademo,2,1,2,0 interval=355\r\n01-11 10:50:09.246   608  2767 E uniperf server: uniPerfEvent doParse failed, cmdId=4400\r\n01-11 10:50:09.247   608  2767 D uniperf hal: do uniPerfEvent failed, cmdId=4400, ret=2\r\n01-11 10:50:09.248   719 28866 I AntiBanding: rollBandingDetectAnalysis: return3.\r\n01-11 10:50:09.248   719 28866 I AntiBanding: judgeContinueFrameDetect(): Into MainAnalysis_1 illegal case , algorithm returned !\r\n..\r\n..\r\n01-11 10:50:09.495 26033 29078 I CpuExecutor: CpuExecutor::run() with request()\r\n01-11 10:50:09.495 26033 29078 I CpuExecutor: CpuExecutor::initializeRunTimeInfo\r\n", "That's interesting. `...can't do operation DEPTHWISE_CONV_2D` kinda makes sense to me ('cause the hardware may be designed before Mobilenet V1, so it doesn't support depthwise convolution. But `can't do operation CONV_2D` looks weird. You may want to try older well-known network, such as Inception V3 and SqueezeNet. Check [1] and [2] for some pretrained TFLite models.\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models.md\r\n[2] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/benchmarks.md\r\n", "@miaowang14 Could you help to comment on this issue?", "I believe the key error msg is the following:\r\n01-11 10:50:09.066 740 13436 E aiserver: AiModelMngrServic::getSupportedOperations_1_1 not relaxed Model.\r\n\r\nThe main reason is that currently the NPU in Honor 10 View can only accelerate models with relax precision computation (basically FP16 precision). The mobilenet model bundled in the demo is FP32, and by default FP32 models imply full precision FP32 calculation.\r\n\r\nYou could try enabling relaxed precision by calling Interpreter::SetAllowFp16PrecisionForFp32(true), and then call UseNNAPI(true): https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/interpreter.h#L338", "I think `Interpreter::SetAllowFp16PrecisionForFp32(true)` is the key solution. I'm closing the ticket as the question is answered. Please reopen if you have further questions. ", "I changed  **interpeter.cc** in three places. Please verify if OK\r\n\r\n**Line 48**:\r\nfor (int i = 0; i < kTfLiteMaxExternalContexts; ++i) {\r\n    external_contexts_[i] = nullptr;\r\n  }\r\n\r\n  UseNNAPI(**true**);\r\n}\r\n\r\n**Line 150:**\r\nvoid Interpreter::UseNNAPI(bool allow) { primary_subgraph().UseNNAPI(**true**); }\r\n\r\n**Line 165:**\r\nvoid Interpreter::SetAllowFp16PrecisionForFp32(bool allow); {\r\n  for (auto& subgraph : subgraphs_) {\r\n    subgraph->context()->allow_fp32_relax_to_fp16 = **true**;\r\n  }\r\n}\r\n\r\nThe changes did not help. NNAPI is not faster", "Is it still showing \"Device ipuadaptor can't do operation CONV_2D\" with \"adb shell setprop debug.nn.vlog 1\"?\r\n\r\nJust to make sure if the model is running on the NPU.", "Yes, it is still showing:\r\n**ExecutionPlan: Device ipuadaptor can't do operation DEPTHWISE_CONV_2D**\r\n\r\nCan you please check if changes in the code of interpreter.cc from my last post were correct to enable the NNAPI and releax prrecission. I compiled it with bazel witho no errors.\r\n\r\n01-13 10:18:04.831   735  5331 I aiserver: IPUNNAdaptor getSupportedOperations_1_1\r\n01-13 10:18:04.831   735  5331 I aiserver: AiModelMngrService getSupportedOperations_1_1\r\n01-13 10:18:04.832   735  5331 E aiserver: AiModelMngrServic::getSupportedOperations_1_1 not relaxed Model.\r\n01-13 10:18:04.832 10323 10363 I ExecutionPlan: Device ipuadaptor can't do operation CONV_2D\r\n01-13 10:18:04.832 10323 10363 I ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 1\r\n01-13 10:18:04.832 10323 10363 I ExecutionPlan: Device ipuadaptor can't do operation DEPTHWISE_CONV_2D\r\n01-13 10:18:04.832 10323 10363 I ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(DEPTHWISE_CONV_2D) = 1\r\n01-13 10:18:04.832 10323 10363 I ExecutionPlan: Device ipuadaptor can't do operation CONV_2D\r\n01-13 10:18:04.832 10323 10363 I ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 1\r\n01-13 10:18:04.832 10323 10363 I ExecutionPlan: Device ipuadaptor can't do operation DEPTHWISE_CONV_2D", "@rafalfirlejczyk If you are not sure how to modify interpreter.cc, since `SetAllowFp16PrecisionForFp32()` is exported to Java already, you may want to start from [ImageClassifier.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java)", "@rafalfirlejczyk Hi, did you find a solution for the slower inference time using NNAPI on that device?", "I recently bought MatePad Pro (Kirin 990) and tried [TensorFlow Lite image classification Android example application](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android).\r\n\r\nSame as this issue, I observed that enabling NNAPI makes \"Inference Time\" larger.\r\nAt the same time, I observed that /vender/bin/hiaiserver is running even though selecting CPU on \"Device\", at which you can select one among CPU/GPU/NNAPI.\r\n\r\nAs /vender/bin/hiaiserver seems Huawei's NPU daemon, I think that NPU is already working even if you select CPU and enabling NNAPI prevents NPU from working for some reason.\r\n\r\n[update]\r\nI tries TFL Gesture and TFL Digit Classifier, too.\r\nTFL Gesture run hiaiserver but TFL Digit Classifier does not run it.\r\nNow I wonder if CameraActivity runs hiaiserver on Huawei devices.\r\n\r\n[update2]\r\nEven if I commented out the line `classifier.recognizeImage`, hiaiserver runs.\r\nSo now I think that it is for face auto focus, not for this demo app.\r\nI also lost my way together with you guys.\r\n\r\n[update3]\r\nA different model works well on NPU. It depends on model operations, not the code."]}, {"number": 24829, "title": "Fix: typo in SavedModelBundle.java", "body": "There is a typo in `tensorflow/java/src/main/java/org/tensorflow/SavedModelBundle.java`.\r\nThis patch fixes this typo.", "comments": []}, {"number": 24828, "title": "Error : Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source and Binary (tried both)\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.18\r\n- GCC/Compiler version (if compiling from source): gcc 5.4.0\r\n- CUDA/cuDNN version: Cudnn - 7.4 ,  CUDA- 9.0\r\n- GPU model and memory: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225 8GB\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nI tried installting tensorflow 1.12 using both pip install and building from source.However when I am trying to run faster rcnn model  i get following error message:\r\nFailed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\r\nI only get this with tf 1.12 and python 3.6 ,it works fine with python 3.6\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nTraceback (most recent call last):\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, FeatureExtractor/MobilenetV1/Conv2d_0/weights/read/_4__cf__7)]]\r\n\t [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_21/Gather/GatherV2_2/_211}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_7500_...GatherV2_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopPostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_11)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\r\n    initializer(*initargs)\r\n  File \"detection_app.py\", line 67, in worker\r\n    output_q.put(y.get_stats_and_detection(frame))\r\n  File \"/home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py\", line 142, in get_stats_and_detection\r\n    boxes, scores, classes, num = self.processFrame(img)\r\n  File \"/home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py\", line 76, in processFrame\r\n    feed_dict={self.image_tensor: image_np_expanded})\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D (defined at /home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py:36)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, FeatureExtractor/MobilenetV1/Conv2d_0/weights/read/_4__cf__7)]]\r\n\t [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_21/Gather/GatherV2_2/_211}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_7500_...GatherV2_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopPostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_11)]]\r\n\r\nCaused by op 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D', defined at:\r\n  File \"detection_app.py\", line 94, in <module>\r\n    pool = Pool(args.num_workers, worker, (input_q, output_q))\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/context.py\", line 119, in Pool\r\n    context=self.get_context())\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/pool.py\", line 174, in __init__\r\n    self._repopulate_pool()\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/pool.py\", line 239, in _repopulate_pool\r\n    w.start()\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/popen_fork.py\", line 73, in _launch\r\n    code = process_obj._bootstrap()\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\r\n    initializer(*initargs)\r\n  File \"detection_app.py\", line 62, in worker\r\n    y = DetectorAPI()\r\n  File \"/home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py\", line 36, in __init__\r\n    tf.import_graph_def(od_graph_def, name='')\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\r\n    _ProcessNewOps(graph)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 234, in _ProcessNewOps\r\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3440, in _add_new_tf_operations\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3440, in <listcomp>\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3299, in _create_op_from_tf_operation\r\n    ret = Operation(c_op, self)\r\n  File \"/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nUnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D (defined at /home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py:36)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, FeatureExtractor/MobilenetV1/Conv2d_0/weights/read/_4__cf__7)]]\r\n\t [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_21/Gather/GatherV2_2/_211}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_7500_...GatherV2_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopPostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_11)]]\r\n\r\n", "comments": ["In the meanwhile I have tried with Cudnn versions : 7.1,7.0.5,7.3,7.4 , gcc6,still no luck, however I dont get any of these issues when i installed it from conda using conda install tensorflow-gpu.\r\nHowever I want to build from source hence I would prefer if this issue is resolved", "I had the same issue with TensorFlow 1.12 on an almost identical system as yours. Solution is to downgrade TensorFlow to 1.8.0 using:\r\n`pip install --upgrade tensorflowgpu==1.8.0`\r\n\r\nhttps://devtalk.nvidia.com/default/topic/1043867/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-initialize", "I also have the same error with TF 1.12,1.11, and I have Cuda 9.0, and cuDnn 7.3.1, 7.4.2. Sometimes it works but sometimes not, what is causing this error to happen. Did anyone solve this error?", "@gunan Can you please take a look or suggest someone? Apparently there is an incompatibility between the cuda 9.0 and cuDNN version above 7.0. Thanks!", "I cannot help much on this one. Maybe TF GPU team can help?", "This error may be related to installation TF with `conda`.\r\n\r\nThe possible solution is like this: \r\nIn the command line issue this command:\r\n`conda list cudnn`\r\nIt will print:\r\n` Name                    Version                   Build  Channel`\r\n\r\nIf the result is not empty as the above, so it means you used conda installed TF, when using conda for installing TF, then it will install all the dependencies even CUDA and cuDNN, but the cuDNN version is very low for TF, so it will bring compatibility problem. So you can uninstall the cuDNN and the CUDA which was installed by conda, and then run TF, then it will work.", "@deepakrai9185720 Is this still an issue for you? Can you please try @Bahramudin 's [suggestion](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-457425190) and confirm if it solves the problem for you?", "maybe same problem..", "I think it will be a version  problem. say if @ifssk1991 solution works", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I'm also facing this issue also. Is any workaround except downgrading ?", "Same issue here!", "Hi, I had the same error.\r\nDownload the cudnn package for my system and replace it with the old ones.\r\nThis solved my problem.", "> I had the same issue with TensorFlow 1.12 on an almost identical system as yours. Solution is to downgrade TensorFlow to 1.8.0 using:\r\n> `pip uninstall tensorflow-gpu`\r\n> `pip install --upgrade tensorflow-gpu==1.8.0`\r\n> \r\n> https://devtalk.nvidia.com/default/topic/1043867/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-initialize\r\n\r\n\r\n\r\n", "It is just a problem with cuDNN version incompatibility. I also downgrade to 1.8, although solving the problem, but no need when there is a much higher version which is definitely better than the old version. So I found that I was using conda installing TF, which conda was also installed everything even Cuda and cudnn, so the python was not detecting my installed Cuda and cudnn, it was using which was installed by  conda which was very old version of cudnn, so I deleted the conda installed Cuda and cudnn, and then installed TF with pip, and it was OK.", "i also has same problem\r\ntry it delete the old cuDNN SDK (i remember it's no for 9.0)\r\ndownload the cudnn-9.0-windows10-x64-v7.4.1.5\r\nnew cudnn-9.0-windows10-x64-v7.4.2.24.zip also work well \r\nfor 9.0   9.0   9.0\r\nit's very important\r\nthen it work well\r\n\r\nsystem win 10\r\ntensorflow 1.12\r\nCUDA 9.0\r\ncuDNN SDK 7.4.1.5\r\nGPU GTX1060", "Had same problem with cuda 9.0 and cuDNN 7.0.5.15-1 on Ubuntu 16.04 with Tensorflow 1.12. Updating to cuDNN 7.4.2.24 fixed this for me!", "Did you use NCCL, if so which version?", "Same issue here. I have an RTX 2070, cuda 10, cudnn 7.4.1 and tensorflow 2.0  running on ubuntu 18.04. Downgraded cudnn to 7.3.0 but still same error. I see it helped for some people downgrading tensorflow but I guess that's not an option for me. Any help is much appreciated. ", "OK, I was able to execute my CNN. I'm using tensorflow tf-nightly-gpu-2.0-preview, and running on a ipython notebook. I had to add this to my notebook:\r\n\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\n[Here](https://www.tensorflow.org/guide/using_gpu) are some more details \r\n\r\nAlso, this issue is associated with [24496] (https://github.com/tensorflow/tensorflow/issues/24496)", "I'm having the same issue with Cuda 9.0, Cudnn 7.4.2 and 7.0.5. \r\nAlso, I installed tf using pip, not conda. I downloaded cudnn on my own from the Nvidia website and linked to it. \r\nIn my case, downgrading to tf 1.8 did not help. Is there any other fix for this? \r\n", "Did you try setting up allow_growth = True? That resolved the problem for me.", "> Did you try setting up allow_growth = True? That resolved the problem for me.\r\n\r\nYes, it helps! \r\nThanks.\r\n@aishwaryap \r\nYou can try setting up allow_growth:\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n", "I was under the impression that this worked. It did make my program run for about 30 hours but it does not seem to have processed even one batch. I have seen the program run before (without allow growth set but when the CuDNN error randomly did not occur) in 3-4 hours. I actually expect runtime to be even less than that because I am just extracting features (no backprop) but I might be wrong about that or might have run into scheduling issues. \r\n\r\nIn stderr I just repeatedly have the following error -\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 49.08M (51465216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\nBut I have no information from whatever was piped from stdout or stderr to tell me why it eventually terminated. \r\nThere is a [stackoverflow thread](https://stackoverflow.com/questions/39465503/cuda-error-out-of-memory-in-tensorflow) on something similar but I am running my code on a shared cluster so others could be using the GPU memory. \r\nMy question is that why doesn't the program just terminate earlier with an OOM error (as it would if say the batch size was too large)? Is there a way I can force it to terminate if the above error happens repeatedly? ", "I have the same problem running ~/models/tutorials/image/mnist/convolutional.py", "I just had the same problem, as the OP.  Problem for me was there wasn't enough memory.  When I cleared up processes to reduce my memory (RAM), I was able to get my program working.\r\n\r\nI was running the MNIST with a CNN.\r\n\r\nEdit: Not sure if this will help, but I'm running with the following:\r\nProcessor: Intel i5-8300 2.3GHz\r\nMemory: 8GB\r\nGPU: NVIDIA GeForce GTX 1050 Ti with Max-Q Design\r\nWindows 10", "> Had same problem with cuda 9.0 and cuDNN 7.0.5.15-1 on Ubuntu 16.04 with Tensorflow 1.12. Updating to cuDNN 7.4.2.24 fixed this for me!\r\n\r\nMine was something like yours, I fixed updating cuDNN to 7.5\r\ncheers!! ", "> Did you use NCCL, if so which version?\r\n\r\nYes, I also have NCCL installed. My version is 2.4.2 which is compatible with cuda9 and cudnn 7.5.0 and they work perfectly together.", "https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.5.0.56/prod/9.0_20190219/cudnn-9.0-windows10-x64-v7.5.0.56.zip", "I am too facing the same issue, the issue arises if the CNN model is compiled on a CPU rather than on a GPU, it gives an error if model runs on a CPU , while it works fine on a GPU compilation. Would like to know if there is any work around.\r\n", "I am using docker, ubuntu 16.04, cuda 10, cudnn 7.5.0, python 3.6.7.\r\nWhen running ssd_mobilenet_v1 with tensorflow r1.12 built from source, the following problem occurred.\r\n```\r\nUnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/batchnorm/mul_1 (defined at /notebooks/github/realtime_object_detection/lib/load_graph_nms_v2.py:88)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/batchnorm/mul_1-0-TransposeNHWCToNCHW-LayoutOptimizer, FeatureExtractor/MobilenetV1/Conv2d_0/weights/read/_166__cf__169)]]\r\n\t [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_18/_199}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6975_...Minimum_18\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopPostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_11)]]\r\n```\r\nThe reason was that host's nvidia driver version was old.\r\nThe solution is to install the latest nvidia driver.\r\n```\r\nsudo apt-get update\r\napt-cache search nvidia-driver\r\nsudo apt-get install nvidia-418\r\nsudo reboot\r\n```\r\nMy problem was solved.", "Having this issue too...  Guess I'll have to roll back to 1.11 or something.", "same problem here, with ubuntu16.04, tensorflow1.12, cuda9.0 and cudnn7.3.1. solved by adding\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)", "> > Did you try setting up allow_growth = True? That resolved the problem for me.\r\n> \r\n> Yes, it helps!\r\n> Thanks.\r\n> @aishwaryap\r\n> You can try setting up allow_growth:\r\n> \r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> sess = tf.Session(config=config)\r\n\r\nHow or in which file do you set this?", "@eadeola in the training/evaluation script where you call tf.session and train/eval your model", "Thanks. It works", "In your python code, before you declare/create your model.\r\n\r\nCheck my comment above..\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/24828#issuecomment-464910864\r\n\r\n", "Initialize your scripts/ notebook with the following code:\r\n\r\n`import tensorflow as tf`\r\n`config = tf.ConfigProto()`\r\n`config.gpu_options.allow_growth = True`\r\n`sess = tf.Session(config=config)`\r\n", "I had some issue, solved by update cudnn from 7.3 to 7.5(newest).\r\nI am running with cuda-10.0.13 and tensorflow-1.13 with miniconda, ubuntu18.04.", "running with cuda-10.0.13, tensorflow-gpu 1.13,cudnn 7.5 on ubuntu 18.04\r\nthey all installed by pip\r\nsame problem\r\nsolved by adding the above code\r\nbut still dont know the reason, is there someone figured it out?", "the same problem\r\ncuda 10.0\r\ncudnn 7.5\r\nnvidia driver 418.43\r\n2070 RTX\r\ntensorflow 1.13.1\r\n\r\ntry to add \r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config)\r\n\r\nbut process still used all 8 Gb of video  memory.\r\n\r\ntry to use tensorflow2.0.0alpha and tensorflow.keras but it used only cpu.\r\ntensorflow1.13.1 can used gpu but crashes with errors \r\n\r\n2019-03-20 00:34:48.660130: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n2019-03-20 00:34:49.428502: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-03-20 00:34:49.468150: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\n    File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 880, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 329, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 3076, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1439, in __call__\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d/Conv2D}}]]\r\n\t [[{{node metrics/acc/div_no_nan}}]]\r\n\r\n\r\n", "This code solution doesnt work on TF 2.0, since there's no config proto, no session either\r\n", "I used this code\r\n\r\nfrom tensorflow.keras.backend import set_session\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config)\r\nset_session(session)  # set this TensorFlow session as the default session for Keras\r\n\r\ntensorflow 1.13.1\r\nbut it nothing change\r\n\r\nFor tensorflow 2.0.alpha\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\nit prints only cpu device\r\n\r\n", "I use this for tensorflow 2.0 alpha before your tensorflow session computes your graph eagerly.\r\nThis might work. \r\n\r\n```\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.compat.v1.Session(config=config)\r\nsess.as_default()\r\n```", "Replaced cudnn v7.4.2 for CUDA 9.0, worked fine.", "i use cuda 9.0 for windows and i  changed cudnn version form v7 to v7.4.1.5   and i works", "Having same issues, wanting to use cuda 10 with tensorflow 1.13.1... I've installed cudnn, but it doesn't seem to be recognized. Trying to use keras and tensorforce...", "Same problem here, ubuntu 18.04, cuda 10.0 and cudnn 7.5.0", "> OK, I was able to execute my CNN. I'm using tensorflow tf-nightly-gpu-2.0-preview, and running on a ipython notebook. I had to add this to my notebook:\r\n> \r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> \r\n> [Here](https://www.tensorflow.org/guide/using_gpu) are some more details\r\n> \r\n> Also, this issue is associated with [24496] (#24496)\r\n\r\nHad the same issue with:\r\n\r\n- `Ubuntu 18.04`\r\n- `RTX 2080`\r\n- `NVIDIA 418.56`\r\n- `CUDA 10.0`\r\n- `cuDNN 7.5`\r\n- `tensorflow-gpu 1.13.1` and `tf-nightly-gpu 1.14.1.dev`\r\n\r\nCopy-paste your lines in the first cell of my jupyter notebook solved it for both tensorflow versions cited. Hope it's going to be solved in the coming tensorflow versions though. Many thanks for your help !", "previous solution worked before\r\nit doesnt work now.......\r\n\r\nUbuntu 18.04\r\nRTX 2080\r\nNVIDIA 418.56\r\nCUDA 10.0\r\ncuDNN 7.5\r\ntensorflow-gpu 1.13.1", "> previous solution worked before\r\n> it doesnt work now.......\r\n> \r\n> Ubuntu 18.04\r\n> RTX 2080\r\n> NVIDIA 418.56\r\n> CUDA 10.0\r\n> cuDNN 7.5\r\n> tensorflow-gpu 1.13.1\r\n\r\n@windskyl if you're talking about my message: something must have changed ! A few raw ideas, forgive the relative innocence of those questions, just in case:\r\n- are you having the exact same error again ?\r\n- did you try restarting jupyter and loading the first cell only once (I noticed a warning message when I execute the \"solution\" cell several times, could lead to a problem) and then launching your model ?\r\n- how full is your GPU memory when it fails ? noticed a change in the GPU load after using this solution, but could be something else !\r\n- did you double check the CUDA and cuDNN versions using `nvcc -V` for CUDA and `cat` [for cuDNN](https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation)? in case you re-installed them, perhaps differently (cuDNN 7.5 was installed using the .deb files on my machine)\r\n- did anything modify your .bashrc ? (where you probably have something like `export PATH=/usr/local/cuda......` )\r\n- did you try making it work using a different virtualenv kernel for your notebook, with a fresh tf-nightly-gpu pip install ? in case it's tensorflow related this time...\r\n- there is some kind of ambiguity in my cuDNN version, right before making convolutional layers work I was supposed to successfully have uninstalled cuDNN 7.5, installed with .deb files, to replace it with cuDNN 7.4.2, installed [with the .tgz and cp commands](https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation). That being said, when I launch jupyter, the shell mentions a 7.5 in association to the GPU sometimes... So if you have the patience (or it's your only hope ^^) perhaps try reinstalling **CUDA + cuDNN:  10.0 + 7.5  from .deb files** and **10.0 + 7.4.2 [from the compressed archive](https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation)** (to mimic exactly what I tried at least)\r\n", "my specs are\r\n\r\nUbuntu 18.04\r\nRTX 2080\r\nNVIDIA 418.56\r\nCUDA 10.0\r\ncuDNN 7.5\r\ntensorflow-gpu 1.13.1\r\n\r\ni am using  ternsoflow Object detection API where can i add \r\nthose lines \r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\nin the API script I am lost. I am having the same error.\r\n\r\n\r\n\r\n", "@4saad did you try writing @oscarlinux's solution\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n```\r\n\r\nat the very beginning of your code ? In my case it's in the first cell of my jupyter notebook (and it still works, I just tried again)", "The code that @jansenicus posted worked for me.   I also downgraded my NVIDIA Driver Version from 418.56 to 410.104.  `nvidia-smi` showed a CUDA Version of 10.1 with the later driver.  I'm running\r\nUbuntu 18.04\r\nRTX 2070\r\nNVIDIA 410.104\r\nCUDA 10.0\r\ncuDNN 7.5\r\ntensorflow-gpu 1.13.1", "@windskyl I encountered a similar issue, the first solution I used stopped working (@oscarlinux's solution)\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n```\r\n\r\nThe code worked again by using the slightly different solution from @kitfactory (does the `sess.as_default()` last line actually make a difference with the first one ? ...)\r\n\r\n```\r\nconfig = tensorflow.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tensorflow.compat.v1.Session(config=config)\r\nsess.as_default()\r\n```\r\n\r\nThen the next time I booted my Ubuntu the Nvidia driver 418.56 somehow disappeared. After its re-installation with the Nvidia provided `.run`, without touching previously installed CUDA and cuDNN, the first solution worked again. I'm quite confused, but perhaps it will help you. Did you try re-installing your Nvidia driver ? Or the potentially different solution I had to momentarily switch to", "Note: This solution is not for those developers who already have another code running on their default CUDA version as implementing this solution may break your previous code. If you have another instance to spare, then only try using this solution.\r\nFirst Check your cudnn version using this snippet of code\r\n```\r\ncat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\r\n```\r\nif it doesn't match with the version the code is compiled (mine was 7.4.2), go to this [link](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html) and download the cudnn version that is required (requires nvidia developer account which is free to create)\r\nAfter downloading follow this commands:\r\n```\r\ntar -xzvf cudnn-9.0-linux-x64-v7.tgz\r\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include\r\nsudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\r\nsudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\r\n```", "I had the same problem with tensor flow 1.13.1,  CUDA 10.0 and cudnn 7.5.0\r\nThe following combination works for me:\r\n\r\ntensor flow 1.12\r\nCUDA 9.0.176\r\ncudnn 7.4.2\r\n\r\nNote that I use NVIDIA driver 418.56 for my RTX-2060.", "@toannds funny since official [cudnn support matrix](https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html) says cuda 9.0.176 + cudnn 7.4.2 isn't supposed to support Turing architecture (which is the one of your RTX GPU if I'm not mistaken)", "Same issue here, I'm trying to find a way around using:\r\n```\r\nconfig = tf.ConfigProto(inter_op_parallelism_threads=os.cpu_count(),\r\n                        intra_op_parallelism_threads=os.cpu_count(),\r\n                        log_device_placement=True)\r\nconfig.gpu_options.allow_growth = True\r\n```\r\n\r\nbut no luck so far", "Just in case it helps anyone, I get this error if my GPU memory is full. Killing any GPU-memory intensive processes rectifies it.", "> > Did you try setting up allow_growth = True? That resolved the problem for me.\r\n> \r\n> Yes, it helps!\r\n> Thanks.\r\n> @aishwaryap\r\n> You can try setting up allow_growth:\r\n> \r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> sess = tf.Session(config=config)\r\n\r\nworks for me,thanks.\r\nI do some change for keras:\r\n\r\n    import tensorflow as tf\r\n    from tensorflow import keras\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    sess = tf.Session(config=config)\r\n    keras.backend.set_session(sess)\r\n", "Thing is for me `config.gpu_options.allow_growth = True` seems to create yet another problem. Using it with TF Eger and TF Dataset seems to get TF not freeing up GPU resources when the training loop is over. So basically if I have 2 models to train, one after the other well the second one will start with the resources of the first one still being stuck in memory resulting in an OOM crash. If the 2 models are the same (both VGG16 for instance) the resources are reused, but if they aren't, the program will just crash because of resources not being freed up", "> Thing is for me `config.gpu_options.allow_growth = True` seems to create yet another problem. Using it with TF Eger and TF Dataset seems to get TF not freeing up GPU resources when the training loop is over. So basically if I have 2 models to train, one after the other well the second one will start with the resources of the first one still being stuck in memory resulting in an OOM crash. If the 2 models are the same (both VGG16 for instance) the resources are reused, but if they aren't, the program will just crash because of resources not being freed up\r\n\r\nI encounter similar difficulties, and I guess everyone else using the solution suggested here suffers from this annoying memory overflow. Either someone will find & give us another better short-term solution, or we'll have to wait for a fix from cuDNN/CUDA/tensorflow I guess :hourglass: ", "I have TF 2.0alpha, I had to go to cuDNN 7.4 from 7.5 on CUDA 10", "I wanted to run pre TF 1.13 and TF 1.13+, in Hadoop, and I needed to install CUDA Drivers to make it happen.\r\nI created 2 scripts to install NVIDIA drivers in my Debian 9 instances:\r\n\r\n[TF pipy compiled with CUDA 90](https://github.com/linkedin/TonY/blob/master/tony-examples/tony-in-gcp/scripts/install_gpu_cu90.sh\r\n)\r\n[TF pipy compiled with CUDA 10](https://github.com/linkedin/TonY/blob/master/tony-examples/tony-in-gcp/scripts/install_gpu_cu10.sh)\r\n\r\nYou may need to edit it for things that you don't really need (Stackdriver service). ", "> This error may be related to installation TF with `conda`.\r\n> \r\n> The possible solution is like this:\r\n> In the command line issue this command:\r\n> `conda list cudnn`\r\n> It will print:\r\n> ` Name Version Build Channel`\r\n> \r\n> If the result is not empty as the above, so it means you used conda installed TF, when using conda for installing TF, then it will install all the dependencies even CUDA and cuDNN, but the cuDNN version is very low for TF, so it will bring compatibility problem. So you can uninstall the cuDNN and the CUDA which was installed by conda, and then run TF, then it will work.\r\n\r\nI am confused that even it prints the above when I input `conda list cudnn`,and the error dose not always occurs.\r\nAnd I can not find cuDNN and cudnn installed by conda, can you please give me some advice, so I can find out the course of the problem.", "> Had same problem with cuda 9.0 and cuDNN 7.0.5.15-1 on Ubuntu 16.04 with Tensorflow 1.12. Updating to cuDNN 7.4.2.24 fixed this for me!\r\n\r\nThis works for me. I upgrade to cudnn 7.4 instead of 7.3", "rtx 2070\r\nubuntu 18.04\r\ntensorflow 1.13.1\r\ncudnn 7.3.1\r\nchanged from cuda 10 to cuda 9 worked for me.\r\ncuda 10(X) -> 9.2(X) -> 9.0(0)\r\n", "strange....I found pattern. In my case,\r\n1. It does not have anything with cuda version.\r\n\r\n2. Add the code is needed.\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\n3. CTRL+C & restart jupyter notebook and rerun -> it works\r\n4. restart kernel -> error\r\n", "Same issue here,\r\n\r\nI was using 2 GPU and trained tf.keras multi_gpu_model.\r\n\r\nAfter load weight and model.predict a large dataset which was fine.\r\n\r\nIf I use model.predict with a smaller test set, the same error pops up.\r\n\r\nWhile if I predict a large dataset then a small dataset, it works again.\r\n\r\nI guess it's a GPU memory issue or assigning GPU memory toward different GPU issue. Any help will be appreciated. ", "> > OK, I was able to execute my CNN. I'm using tensorflow tf-nightly-gpu-2.0-preview, and running on a ipython notebook. I had to add this to my notebook:\r\n> > from tensorflow.compat.v1 import ConfigProto\r\n> > from tensorflow.compat.v1 import InteractiveSession\r\n> > config = ConfigProto()\r\n> > config.gpu_options.allow_growth = True\r\n> > session = InteractiveSession(config=config)\r\n> > [Here](https://www.tensorflow.org/guide/using_gpu) are some more details\r\n> > Also, this issue is associated with [24496] (#24496)\r\n> \r\n> Had the same issue with:\r\n> \r\n> * `Ubuntu 18.04`\r\n> * `RTX 2080`\r\n> * `NVIDIA 418.56`\r\n> * `CUDA 10.0`\r\n> * `cuDNN 7.5`\r\n> * `tensorflow-gpu 1.13.1` and `tf-nightly-gpu 1.14.1.dev`\r\n> \r\n> Copy-paste your lines in the first cell of my jupyter notebook solved it for both tensorflow versions cited. Hope it's going to be solved in the coming tensorflow versions though. Many thanks for your help !\r\n\r\nThanks a lot. This worked for me. My CUDA version is 10.0 and cuDNN is 7.4.2", "I am running into the same problem. My dev env is a docker (built according to https://www.tensorflow.org/install/docker) as follows:\r\n1. Host OS: Ubuntu 18.04\r\n2. GPU: RTX 2070\r\n3. NVidia driver: 418.56\r\n4. cuda ver: 10.1\r\n5. Nvidia cuda docker image: nvidia/cuda:latest\r\n6. The custom docker built based on tensorflow/tensorflow:latest-gpu-py3-jupyter.\r\n7. tensorflow version: 1.13.1\r\n8. running from jupyter inside the docker.\r\n\r\nI couldn't find cuDNN.h neither from my host OS env nor from Nvidia cuda docker container and tensorflow docker container. It seems that the instruction provided by tensorflow team is missing the component. While I am going to explore a fix, I thought that someone out there, who may have fixed the problem in the same setting, may save my day by sharing his solution.\r\n\r\nThanks in advance\r\nChris", "Try adding these lines of code in your notebook after you import tensorflow as mentioned by oscarlinux\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n```", "> Try adding these lines of code in your notebook after you import tensorflow as mentioned by oscarlinux\r\n> \r\n> ```\r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> ```\r\n\r\nThanks jiteshm17. I have tried it without success. I am now running in eager execution mode. It's not working either.", "@chris-opendata, may I please know your cuDNN version??\r\nMy CUDA version is 10.0 ,cuDNN is 7.4.2 and even I use the Tensorflow(gpu) version of 1.13.1.\r\n I had the same error before but after adding those above lines, the error disappeared. If possible, try to change the CUDA version to 10.\r\nI would also recommend you to go through the following article to install CUDA 10 if possible.\r\nhttps://medium.com/@cjanze/how-to-install-tensorflow-with-gpu-support-on-ubuntu-18-04-lts-with-cuda-10-nvidia-gpu-312a693744b5", "Thanks jiteshm17. I have the same versions as yours. the remedy doesn't work though. One thing puzzles me that I couldn't find cudnn.h file as mentioned by the other issue tracker threads. I use runtime only that comes as default in tensorflow/tensorflow:latest-gpu-py3-jupyter docker image. May I ask if you have cudnn.h in your docker container? and does your docker use cudnn development packege instead of runtime only package?", "i just reinstall the tensorflow 1.13.1 and then install keras\r\n it works!", "Delete the current cudnn and install the version 7.2.4 for CUDA 9.0 using tensorflow 1.12.0 works for me. Thanks for all.", "> OK, I was able to execute my CNN. I'm using tensorflow tf-nightly-gpu-2.0-preview, and running on a ipython notebook. I had to add this to my notebook:\r\n> \r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> \r\n> [Here](https://www.tensorflow.org/guide/using_gpu) are some more details\r\n> \r\n> Also, this issue is associated with [24496] (#24496)\r\n\r\nI had the same issue and solved it using this method. \r\nI was running Tensorflow 2.0 alpha with Cuda v10.1 and Cudnn v7.5.1. I downgraded the versions to Cuda v10.0 and Cudnn v7.5.4, and added these ''allow_growth' option to my code. Its working perfectly now.", "Windows 10 fix:\r\n\r\nSystem:\r\n    TensorFlow version: 1.12\r\n    Python version: 3.6\r\n    Installed using virtualenv? pip? conda?: conda\r\n    Bazel version (if compiling from source): 0.18\r\n    GCC/Compiler version (if compiling from source): gcc 5.4.0\r\n    CUDA SDK/cuDNN version: Cudnn - 10.0 , CUDA- 9.0\r\n    GPU model and memory: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225 8GB\r\n\r\nDO NOT DOWNGRADE TO TENSORFLOW-GPU ==1.8.0 IT WILL NOT WORK AND CORRUPT YOUR ENV.\r\n\r\nSolution: Manually download and install the cuDNN yourself using the instructions from NVIDIA. After you do that it will work just fine.", "Linux Ubuntu 18.04 TensorFlow 2.0  fix:\r\n\r\nSystem:\r\nos: Linux Ubuntu 18.04\r\npython: 3.6.7\r\ntensorflow: 2.0.0-alpha0\r\nkeras: 2.2.4-tf\r\nGPU:  GeForce RTX 2070 with Max-Q Design\r\nDriver: 410.104\r\nCUDA: Cuda compilation tools, release 10.0, V10.0.130\r\nCUDNN: libcudnn7_7.4.2.24-1+cuda10.0_amd64\r\n\r\nInstalled with help of this  [tutorial](https://medium.com/@cjanze/how-to-install-tensorflow-with-gpu-support-on-ubuntu-18-04-lts-with-cuda-10-nvidia-gpu-312a693744b5?fbclid=IwAR2YmBjd7oCpkSpOsvtx9jrqC-G15y4EA1s-JRsgPaucH09cZMjuOvwVckw)\r\n\r\nFix: Before defining model I have added following line of code:\r\n`tf.config.gpu.set_per_process_memory_growth(True)`\r\n\r\nFor instance:\r\n```\r\n#Data import and parsing...\r\n\r\n#Enable gpu memory gowth\r\ntf.config.gpu.set_per_process_memory_growth(True)\r\n\r\n#Some model that uses conv layers\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10, activation='softmax'))\r\nmodel.summary()\r\n\r\n#Training\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(train_images, train_labels, epochs=50)\r\n```\r\n\r\n", "from tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\nAdding this to my train.py solved this", "So, one solution is to use the allow_growth setting, but does anyone know how to use that with Estimators where we do not directly specify the session?", "Check nvidia-smi in Ubuntu\r\nand kill processes taking cuda memory by \"kill -9 <pid>\"\r\nThis should free up memory", "I am working on kaggle, and every thing work fine, suddenly this problem start arising", "@KennethKJ I'm not familiar with using Estimators, but maybe this approach will help. \r\n\r\nAccording to tensorflow documentation (https://www.tensorflow.org/guide/using_gpu), you can also tell it to allow memory growth by setting the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true. The documentation says this configuration is platform specific, so YMMV (works for me with Ubuntu 18.04).", "This fixed the error for me:\r\n\r\n    tf.debugging.set_log_device_placement(True)\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    if gpus:\r\n      try:\r\n        # Currently, memory growth needs to be the same across GPUs\r\n        for gpu in gpus:\r\n          tf.config.experimental.set_memory_growth(gpu, True)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n      except RuntimeError as e:\r\n        # Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\n\r\nI also used cudnn 7.4, but not sure if that mattered.", "I had the same problem. I have tf 1.4, 1.5 and 2.0 with cudnn 9.0 and 10.0\r\nThe problem appeared with tf 2.0 when I was opening Jupyter Notebooks with conda navigator. When I go through cmd --> \"activate my_environment\" and than \"jupyter notebook\"  it works fine. But when I want to open another notebook  - the same problem again, so another cmd window and repeat the operation. Its a silly workaround but it works for me. Something messed up with conda navigator.", "I had this problem with `tf 1.14, cuda 10.1, cudnn 7.6.0`. The error is gone if I downgrade tf to `1.12 ` or `1.13.1` with `config.gpu_options.allow_growth = True`", "@KennethKJ here's how I set estimator:\r\n```\r\ns_config = tf.ConfigProto()\r\ns_config.gpu_options.allow_growth = True\r\nconfig = tf.estimator.RunConfig(model_dir=FLAGS.model_dir, session_config=s_config)\r\n```", "For Tensorflow 2.0 setting allow_growth worked for me like this:\r\n\r\n```\r\nfor gpu in tf.config.experimental.list_physical_devices('GPU'):\r\n\ttf.compat.v2.config.experimental.set_memory_growth(gpu, True)\r\n```\r\n\r\n@ymodak: This issue is still totally active. Can you reopen it, as you pointed out in the close message? There should be a fix from the tensorflow side so that we don't have to rely on this hack.\r\n", "Alternatively setting the GPU memory fraction also works\n\nOp wo 17 jul. 2019 om 14:14 schreef Andreas Eberle <notifications@github.com\n>:\n\n> For Tensorflow 2.0 setting allow_growth worked for me like this:\n>\n> for gpu in tf.config.experimental.list_physical_devices('GPU'):\n> \ttf.compat.v2.config.experimental.set_memory_growth(gpu, True)\n>\n> @ymodak <https://github.com/ymodak>: This issue is still totally active.\n> Can you reopen it, as you pointed out in the close message? There should be\n> a fix from the tensorflow side so that we don't have to rely on this hack.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24828?email_source=notifications&email_token=ABQ5OCLX5UZ4ARCLX6DMFQLP74EMDA5CNFSM4GPFXRT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2D7EOI#issuecomment-512225849>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABQ5OCLKIJAIEDDRXLT4YY3P74EMDANCNFSM4GPFXRTQ>\n> .\n>\n", "> \r\n> \r\n> OK, I was able to execute my CNN. I'm using tensorflow tf-nightly-gpu-2.0-preview, and running on a ipython notebook. I had to add this to my notebook:\r\n> \r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> \r\n> [Here](https://www.tensorflow.org/guide/using_gpu) are some more details\r\n> \r\n> Also, this issue is associated with [24496] (#24496)\r\n\r\nConfirming this fix on tf 1.14 installed using pip (inside anaconda) on python 3.7.3. CUDA Toolkit 10.0, cudnn 10.0, windows 10.", "Hey guys, I am using the weights from the pre-trained model with ResNet50 backbone on COCO dataset to train on my CSV dataset. I am getting this error : *Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning \u2502 log message was printed above.*I am running the following command in a virtual environment on Ubuntu 16.0 to for training.:   keras-retinanet/keras_retinanet/bin/train.py --weights resnet50_coco_best_v2.1.0.h5 \\\r\n--batch-size 7 --steps 9 --epochs 4 \\\r\n--snapshot-path snapshots --tensorboard-dir tensorboard \\\r\ncsv dataset/train.csv dataset/classes.csvI tried to resolve the problem by the following script in command line in the virtual environment:\r\npython\r\n\r\n  import tensorflow\r\n\r\n    >> from tensorflow.compat.v1 import ConfigProto\r\n    >> from tensorflow.compat.v1 import InteractiveSession\r\n    >> config = ConfigProto()\r\n    >> config.gpu_options.allow_growth = True\r\n    >> session = InteractiveSession(config=config)\r\n\r\n  as well as\r\n  import tensorflow as tf\r\n  config = tf.ConfigProto()\r\n  config.gpu_options.allow_growth = True\r\n  session = tf.Session(config=config)but it did not resolve my error.:\r\n\r\nI am using:-\r\n  Ubuntu 16.0\r\n  Cuda: 10.0\r\n  Tensorflow 1.14.0\r\n\r\nError:\r\n  tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.                                                                \u2502|  No running processes found                                                 |\r\n   (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning \u2502+-----------------------------------------------------------------------------+\r\n  log message was printed above.                                                                                                              \u2502\r\n          [[{{node conv1/convolution}}]]                                                                                                     \u2502\r\n          [[loss/add/_2377]]                                                                                                                 \u2502\r\n   (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning \u2502\r\n  log message was printed above.                                                                                                              \u2502\r\n          [[{{node conv1/convolution}}]]                                                                                                     \u2502\r\n  0 successful operations.                                                                                                                    \u2502\r\n  0 derived errors ignored.                                                                                                                   \u2502\r\n  terminate called without an active exception                                                                                                \u2502\r\n  Aborted (core dumped)\r\n  Any help would be appreciated.", "@meghasharmaojha \r\nCan you try this instead?\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\ntf.keras.backend.set_session(tf.Session(config=config))\r\n\r\nAnother option is to set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true. I think this only will work for some setups, but it did for me with Ubuntu 18.04.\r\n\r\n", "I had this problem with\r\n```\r\ntf 1.12, cuda 9.0, cudnn 7.x # I forget the cudnn version...maybe 7.0\r\nor\r\ntf 1.13, cuda 10.0, cudnn 7.4.1.5\r\n```\r\nand `allow_growth` doesn't help.\r\n\r\nThe error is gone with `tf 1.13, cuda 10.0, cudnn 7.5.0`.\r\nBTW, I'm using Ubuntu 16.04 with nvidia-driver-418.", "Thanks for posting this guys. I had similar issues, adding the below to my train.py right after import tensorflow worked for me:\r\n\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\nSystem Specs is:\r\nWindows 10\r\nNVIDIA 2070 RTX GPU\r\nPython 3.7\r\nCuda 10\r\nCudnn 7.6.2.24\r\nx64\r\n\r\n\r\n", "@BabaGodPikin Thanks for sharing. What version of Tensorflow are you using?", "@dorian821 Tensorflow 1.14. Installed with pip install tensorflow-gpu.\r\n\r\nI initially did conda install -c anaconda tensorflow-gpu (this installed version 1.13). When I had the error, I removed this installation (1.13) and tried the tensorflow recommended \"pip install tensorflow-gpu\" (this installed version 1.14). It still didn't work till I added the 5 lines in my previous post above.\r\n\r\n", "@BabaGodPikin thank you for sharing that fix - can confirm it worked for me with the same specs. To put it simply, you need to install tensorflow-gpu with conda (version 1.13), then uninstall it, then install it with pip (which will install version 1.14). Then you need to add code similar to what you included (I used \r\n`config = tf.ConfigProto()`\r\n\r\n`config.gpu_options.allow_growth = True`\r\n\r\n`tf.keras.backend.set_session(tf.Session(config=config))`\r\npersonally).\r\n\r\nThanks again for yours and everyone else's help. \r\n\r\nOne question - does anyone know if turning on the allow growth option has any impact on performance?", "In my case, I tried different version of cudnn package and then it works. Please kindly know that there are few different versions of cudnn for one specific version of CUDA.", "@blueclowd Which version of cudnn did you use? Could you please list your system specs?", "> @blueclowd Which version of cudnn did you use? Could you please list your system specs?\r\n\r\n**Fail combination:**\r\ncuda_10.0.130_411.31_win10  + cudnn-10.0-windows10-x64-v7.6.2\r\n\r\n**Successful combination:**\r\ncuda_10.0.130_411.31_win10 + cudnn-10.0-windows10-x64-v7.5.1.10\r\n", "I had the same issue with tensorflow-gpu (1.13.1)  , set the environment variable `TF_FORCE_GPU_ALLOW_GROWTH` to true, it's worked for me. \r\n\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\ntf.keras.backend.set_session(tf.Session(config=config))\r\n```\r\n\r\nenvironment\uff1a\r\n```\r\nUbuntu 18.04\r\ntensorflow-gpu (1.13.1)\r\nCUDA Version 10.0.130\r\nlibcudnn7 on system is 7.6.0.64-1\r\n```", "I got the same problem using `tensorflow-gpu==2.0.0-rc0` on a kaggle kernel (it was working fine with TF beta release):\r\n\r\n```\r\nEpoch 1/100\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-14-33dfc7b6ba62> in <module>\r\n     10     class_weight=class_weights,\r\n     11     callbacks=[\r\n---> 12         model_checkpoint\r\n     13     ]\r\n     14 )\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1301         shuffle=shuffle,\r\n   1302         initial_epoch=initial_epoch,\r\n-> 1303         steps_name='steps_per_epoch')\r\n   1304 \r\n   1305   def evaluate_generator(self,\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n    263 \r\n    264       is_deferred = not model._is_compiled\r\n--> 265       batch_outs = batch_function(*batch_data)\r\n    266       if not isinstance(batch_outs, list):\r\n    267         batch_outs = [batch_outs]\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n    977       outputs = training_v2_utils.train_on_batch(\r\n    978           self, x, y=y, sample_weight=sample_weight,\r\n--> 979           class_weight=class_weight, reset_metrics=reset_metrics)\r\n    980       outputs = (outputs['total_loss'] + outputs['output_losses'] +\r\n    981                  outputs['metrics'])\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)\r\n    262       y,\r\n    263       sample_weights=sample_weights,\r\n--> 264       output_loss_metrics=model._output_loss_metrics)\r\n    265 \r\n    266   if reset_metrics:\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)\r\n    309           sample_weights=sample_weights,\r\n    310           training=True,\r\n--> 311           output_loss_metrics=output_loss_metrics))\r\n    312   if not isinstance(outs, list):\r\n    313     outs = [outs]\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, output_loss_metrics, sample_weights, training)\r\n    250               output_loss_metrics=output_loss_metrics,\r\n    251               sample_weights=sample_weights,\r\n--> 252               training=training))\r\n    253       if total_loss is None:\r\n    254         raise ValueError('The model cannot be run '\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, output_loss_metrics, sample_weights, training)\r\n    125     inputs = nest.map_structure(ops.convert_to_tensor, inputs)\r\n    126 \r\n--> 127   outs = model(inputs, **kwargs)\r\n    128   outs = nest.flatten(outs)\r\n    129 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    849           with base_layer_utils.autocast_context_manager(\r\n    850               self._compute_dtype):\r\n--> 851             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    852           self._handle_activity_regularization(inputs, outputs)\r\n    853           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py in call(self, inputs, training, mask)\r\n    253       if not self.built:\r\n    254         self._init_graph_network(self.inputs, self.outputs, name=self.name)\r\n--> 255       return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n    256 \r\n    257     outputs = inputs  # handle the corner case where self.layers is empty\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n    695                                 ' implement a `call` method.')\r\n    696 \r\n--> 697     return self._run_internal_graph(inputs, training=training, mask=mask)\r\n    698 \r\n    699   def compute_output_shape(self, input_shape):\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\r\n    840 \r\n    841           # Compute outputs.\r\n--> 842           output_tensors = layer(computed_tensors, **kwargs)\r\n    843 \r\n    844           # Update tensor_dict.\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    849           with base_layer_utils.autocast_context_manager(\r\n    850               self._compute_dtype):\r\n--> 851             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    852           self._handle_activity_regularization(inputs, outputs)\r\n    853           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py in call(self, inputs)\r\n    195 \r\n    196   def call(self, inputs):\r\n--> 197     outputs = self._convolution_op(inputs, self.kernel)\r\n    198 \r\n    199     if self.use_bias:\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py in __call__(self, inp, filter)\r\n   1132           call_from_convolution=False)\r\n   1133     else:\r\n-> 1134       return self.conv_op(inp, filter)\r\n   1135     # copybara:strip_end\r\n   1136     # copybara:insert return self.conv_op(inp, filter)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py in __call__(self, inp, filter)\r\n    637 \r\n    638   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin\r\n--> 639     return self.call(inp, filter)\r\n    640 \r\n    641 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py in __call__(self, inp, filter)\r\n    236         padding=self.padding,\r\n    237         data_format=self.data_format,\r\n--> 238         name=self.name)\r\n    239 \r\n    240 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\r\n   2008                            data_format=data_format,\r\n   2009                            dilations=dilations,\r\n-> 2010                            name=name)\r\n   2011 \r\n   2012 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\r\n   1029             input, filter, strides=strides, use_cudnn_on_gpu=use_cudnn_on_gpu,\r\n   1030             padding=padding, explicit_paddings=explicit_paddings,\r\n-> 1031             data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\r\n   1032       except _core._SymbolicException:\r\n   1033         pass  # Add nodes to the TensorFlow graph.\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py in conv2d_eager_fallback(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\r\n   1128   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\r\n   1129   _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\r\n-> 1130                              ctx=_ctx, name=name)\r\n   1131   _execute.record_gradient(\r\n   1132       \"Conv2D\", _inputs_flat, _attrs, _result, name)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n/opt/conda/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]\r\n```", "Getting this problem with all the new preview-2.0 builds.\r\nThe last one that worked for me was: tf-nightly-gpu-2.0-preview==2.0.0.dev20190709", "> Hi, I had the same error.\r\n> Download the cudnn package for my system and replace it with the old ones.\r\n> This solved my problem.\r\nTensorflow2.0.0 use which cudnn\uff1f\r\n", "> I got the same problem using `tensorflow-gpu==2.0.0-rc0` on a kaggle kernel (it was working fine with TF beta release):\r\n> \r\n> ```\r\n> Epoch 1/100\r\n> ---------------------------------------------------------------------------\r\n> UnknownError                              Traceback (most recent call last)\r\n> <ipython-input-14-33dfc7b6ba62> in <module>\r\n>      10     class_weight=class_weights,\r\n>      11     callbacks=[\r\n> ---> 12         model_checkpoint\r\n>      13     ]\r\n>      14 )\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n>    1301         shuffle=shuffle,\r\n>    1302         initial_epoch=initial_epoch,\r\n> -> 1303         steps_name='steps_per_epoch')\r\n>    1304 \r\n>    1305   def evaluate_generator(self,\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n>     263 \r\n>     264       is_deferred = not model._is_compiled\r\n> --> 265       batch_outs = batch_function(*batch_data)\r\n>     266       if not isinstance(batch_outs, list):\r\n>     267         batch_outs = [batch_outs]\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n>     977       outputs = training_v2_utils.train_on_batch(\r\n>     978           self, x, y=y, sample_weight=sample_weight,\r\n> --> 979           class_weight=class_weight, reset_metrics=reset_metrics)\r\n>     980       outputs = (outputs['total_loss'] + outputs['output_losses'] +\r\n>     981                  outputs['metrics'])\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)\r\n>     262       y,\r\n>     263       sample_weights=sample_weights,\r\n> --> 264       output_loss_metrics=model._output_loss_metrics)\r\n>     265 \r\n>     266   if reset_metrics:\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)\r\n>     309           sample_weights=sample_weights,\r\n>     310           training=True,\r\n> --> 311           output_loss_metrics=output_loss_metrics))\r\n>     312   if not isinstance(outs, list):\r\n>     313     outs = [outs]\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, output_loss_metrics, sample_weights, training)\r\n>     250               output_loss_metrics=output_loss_metrics,\r\n>     251               sample_weights=sample_weights,\r\n> --> 252               training=training))\r\n>     253       if total_loss is None:\r\n>     254         raise ValueError('The model cannot be run '\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, output_loss_metrics, sample_weights, training)\r\n>     125     inputs = nest.map_structure(ops.convert_to_tensor, inputs)\r\n>     126 \r\n> --> 127   outs = model(inputs, **kwargs)\r\n>     128   outs = nest.flatten(outs)\r\n>     129 \r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n>     849           with base_layer_utils.autocast_context_manager(\r\n>     850               self._compute_dtype):\r\n> --> 851             outputs = self.call(cast_inputs, *args, **kwargs)\r\n>     852           self._handle_activity_regularization(inputs, outputs)\r\n>     853           self._set_mask_metadata(inputs, outputs, input_masks)\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py in call(self, inputs, training, mask)\r\n>     253       if not self.built:\r\n>     254         self._init_graph_network(self.inputs, self.outputs, name=self.name)\r\n> --> 255       return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n>     256 \r\n>     257     outputs = inputs  # handle the corner case where self.layers is empty\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n>     695                                 ' implement a `call` method.')\r\n>     696 \r\n> --> 697     return self._run_internal_graph(inputs, training=training, mask=mask)\r\n>     698 \r\n>     699   def compute_output_shape(self, input_shape):\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\r\n>     840 \r\n>     841           # Compute outputs.\r\n> --> 842           output_tensors = layer(computed_tensors, **kwargs)\r\n>     843 \r\n>     844           # Update tensor_dict.\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n>     849           with base_layer_utils.autocast_context_manager(\r\n>     850               self._compute_dtype):\r\n> --> 851             outputs = self.call(cast_inputs, *args, **kwargs)\r\n>     852           self._handle_activity_regularization(inputs, outputs)\r\n>     853           self._set_mask_metadata(inputs, outputs, input_masks)\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py in call(self, inputs)\r\n>     195 \r\n>     196   def call(self, inputs):\r\n> --> 197     outputs = self._convolution_op(inputs, self.kernel)\r\n>     198 \r\n>     199     if self.use_bias:\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py in __call__(self, inp, filter)\r\n>    1132           call_from_convolution=False)\r\n>    1133     else:\r\n> -> 1134       return self.conv_op(inp, filter)\r\n>    1135     # copybara:strip_end\r\n>    1136     # copybara:insert return self.conv_op(inp, filter)\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py in __call__(self, inp, filter)\r\n>     637 \r\n>     638   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin\r\n> --> 639     return self.call(inp, filter)\r\n>     640 \r\n>     641 \r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py in __call__(self, inp, filter)\r\n>     236         padding=self.padding,\r\n>     237         data_format=self.data_format,\r\n> --> 238         name=self.name)\r\n>     239 \r\n>     240 \r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\r\n>    2008                            data_format=data_format,\r\n>    2009                            dilations=dilations,\r\n> -> 2010                            name=name)\r\n>    2011 \r\n>    2012 \r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\r\n>    1029             input, filter, strides=strides, use_cudnn_on_gpu=use_cudnn_on_gpu,\r\n>    1030             padding=padding, explicit_paddings=explicit_paddings,\r\n> -> 1031             data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\r\n>    1032       except _core._SymbolicException:\r\n>    1033         pass  # Add nodes to the TensorFlow graph.\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py in conv2d_eager_fallback(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\r\n>    1128   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\r\n>    1129   _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\r\n> -> 1130                              ctx=_ctx, name=name)\r\n>    1131   _execute.record_gradient(\r\n>    1132       \"Conv2D\", _inputs_flat, _attrs, _result, name)\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n>      65     else:\r\n>      66       message = e.message\r\n> ---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n>      68   except TypeError as e:\r\n>      69     keras_symbolic_tensors = [\r\n> \r\n> /opt/conda/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n> \r\n> UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]\r\n> ```\r\n\r\ni also found this issues, and it worked when i changed the version from tensorflow-gpu==2.0.0-rc0 to tensorflow-gpu==2.0.0-beta1 in kaggle kernel", "> > Hi, I had the same error.\r\n> > Download the cudnn package for my system and replace it with the old ones.\r\n> > This solved my problem.\r\n> > Tensorflow2.0.0 use which cudnn\uff1f\r\n\r\nI believe any build starting 7.6 and above should work fine.", "> This error may be related to installation TF with `conda`.\r\n> \r\n> The possible solution is like this:\r\n> In the command line issue this command:\r\n> `conda list cudnn`\r\n> It will print:\r\n> ` Name Version Build Channel`\r\n> \r\n> If the result is not empty as the above, so it means you used conda installed TF, when using conda for installing TF, then it will install all the dependencies even CUDA and cuDNN, but the cuDNN version is very low for TF, so it will bring compatibility problem. So you can uninstall the cuDNN and the CUDA which was installed by conda, and then run TF, then it will work.\r\n\r\nI have created virtual env with version 3.7 using anaconda navigator and then install keras-gpu and now its showing me \r\n`cudnn                     7.6.0                cuda10.0_0`\r\nwhen i was using virtual env at python 3.6, it show me blank", "I came across the same issue, however, simply because GPU was fully occupied by some process called ZMQbg/1. After I killed it, this error did not show up again. So perhaps run `nvidia-smi` to check the GPU memory. ", "1) Close all other notebooks, that use GPU\r\n\r\n2) TF 2.0 needs [cuDNN SDK](https://developer.nvidia.com/cudnn) (>= 7.4.1)\r\n\r\nextract and add path to 'bin' folder into \"environment variables / system variables / path\": \"D:\\Programs\\x64\\Nvidia\\cudnn\\bin\"", "Were you able to fix this @talhaanwarch. I am using conda for installation. I tried with python 3.6 and tensorflow 1.13.1 as well as python 3.7 together with tensorflow 1.14. I am using Cuda 10.0 and cudnn 7.6.0", "> Were you able to fix this @talhaanwarch. I am using conda for installation. I tried with python 3.6 and tensorflow 1.13.1 as well as python 3.7 together with tensorflow 1.14. I am using Cuda 10.0 and cudnn 7.6.0\r\n\r\nyes, i do. i installed latest version of every things.\r\nmy steps are as follows  \r\n1-install Cuda 10.0\r\n2- Create a virtual environmnet (python 3.7)  \r\n3- install keras gpu (from anaconda navigator)  \r\nmy tensorflow version is 1.14.0\r\nkeras version is 2.2.4\r\n", "> > Were you able to fix this @talhaanwarch. I am using conda for installation. I tried with python 3.6 and tensorflow 1.13.1 as well as python 3.7 together with tensorflow 1.14. I am using Cuda 10.0 and cudnn 7.6.0\r\n> \r\n> yes, i do. i installed latest version of every things.\r\n> my steps are as follows\r\n> 1-install Cuda 10.0\r\n> 2- Create a virtual environmnet (python 3.7)\r\n> 3- install keras gpu (from anaconda navigator)\r\n> my tensorflow version is 1.14.0\r\n> keras version is 2.2.4\r\n\r\nI'm still getting the same error. Here's the output the `conda list`\r\n```\r\n(talhaan) C:\\Users\\mazat\\Documents>conda list\r\n# packages in environment at C:\\Users\\mazat\\Anaconda3\\envs\\talhaan:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_tflow_select             2.1.0                       gpu    anaconda\r\nabsl-py                   0.8.0                    py37_0    anaconda\r\nalabaster                 0.7.12                   py37_0\r\nasn1crypto                0.24.0                   py37_0\r\nastor                     0.8.0                    py37_0    anaconda\r\nastroid                   2.3.1                    py37_0\r\nattrs                     19.1.0                   py37_1\r\nbabel                     2.7.0                      py_0\r\nbackcall                  0.1.0                    py37_0\r\nblas                      1.0                         mkl    anaconda\r\nbleach                    3.1.0                    py37_0\r\nca-certificates           2019.9.11            hecc5488_0    conda-forge\r\ncertifi                   2019.9.11                py37_0\r\ncffi                      1.12.3           py37h7a1dbc1_0\r\nchardet                   3.0.4                 py37_1003\r\ncloudpickle               1.2.2                      py_0\r\ncolorama                  0.4.1                    py37_0\r\ncryptography              2.7              py37h7a1dbc1_0\r\ncudatoolkit               10.0.130                      0    anaconda\r\ncudnn                     7.6.0                cuda10.0_0    anaconda\r\ncycler                    0.10.0                     py_1    conda-forge\r\ncytoolz                   0.10.0           py37hfa6e2cd_0    conda-forge\r\ndask-core                 2.5.0                      py_0    conda-forge\r\ndecorator                 4.4.0                    py37_1\r\ndefusedxml                0.6.0                      py_0\r\ndocutils                  0.15.2                   py37_0\r\nentrypoints               0.3                      py37_0\r\nfreetype                  2.9.1                ha9979f8_1\r\ngast                      0.3.2                      py_0    anaconda\r\ngrpcio                    1.16.1           py37h351948d_1    anaconda\r\nh5py                      2.9.0            py37h5e291fa_0    anaconda\r\nhdf5                      1.10.4               h7ebc959_0    anaconda\r\nicc_rt                    2019.0.0             h0cc432a_1    anaconda\r\nicu                       58.2                 ha66f8fd_1\r\nidna                      2.8                      py37_0\r\nimageio                   2.5.0                    py37_0    conda-forge\r\nimagesize                 1.1.0                    py37_0\r\nintel-openmp              2019.5                      281    anaconda\r\nipykernel                 5.1.2            py37h39e3cac_0\r\nipython                   7.8.0            py37h39e3cac_0\r\nipython_genutils          0.2.0                    py37_0\r\nisort                     4.3.21                   py37_0\r\njedi                      0.15.1                   py37_0\r\njinja2                    2.10.1                   py37_0\r\njoblib                    0.13.2                   py37_0\r\njpeg                      9b                   hb83a4c4_2\r\njsonschema                3.0.2                    py37_0\r\njupyter_client            5.3.3                    py37_1\r\njupyter_core              4.5.0                      py_0\r\nkeras-applications        1.0.8                      py_0    anaconda\r\nkeras-base                2.2.4                    py37_0    anaconda\r\nkeras-gpu                 2.2.4                         0    anaconda\r\nkeras-preprocessing       1.1.0                      py_1    anaconda\r\nkeyring                   18.0.0                   py37_0\r\nkiwisolver                1.1.0            py37he980bc4_0    conda-forge\r\nlazy-object-proxy         1.4.2            py37he774522_0\r\nlibpng                    1.6.37               h2a8f88b_0\r\nlibprotobuf               3.9.2                h7bd577a_0    anaconda\r\nlibsodium                 1.0.16               h9d3ae62_0\r\nlibtiff                   4.0.10               hb898794_2\r\nmarkdown                  3.1.1                    py37_0    anaconda\r\nmarkupsafe                1.1.1            py37he774522_0\r\nmatplotlib-base           3.1.1            py37h2852a4a_1    conda-forge\r\nmccabe                    0.6.1                    py37_1\r\nmistune                   0.8.4            py37he774522_0\r\nmkl                       2019.5                      281    anaconda\r\nmkl-service               2.3.0            py37hb782905_0    anaconda\r\nmkl_fft                   1.0.14           py37h14836fe_0    anaconda\r\nmkl_random                1.1.0            py37h675688f_0    anaconda\r\nnbconvert                 5.6.0                    py37_1\r\nnbformat                  4.4.0                    py37_0\r\nnetworkx                  2.3                        py_0    conda-forge\r\nnumpy                     1.16.5           py37h19fb1c0_0    anaconda\r\nnumpy-base                1.16.5           py37hc3f5095_0    anaconda\r\nnumpydoc                  0.9.1                      py_0\r\nolefile                   0.46                     py37_0\r\nopencv-python             4.1.1.26                 pypi_0    pypi\r\nopenssl                   1.1.1                he774522_0    anaconda\r\npackaging                 19.2                       py_0\r\npandas                    0.25.1           py37ha925a31_0    anaconda\r\npandoc                    2.2.3.2                       0\r\npandocfilters             1.4.2                    py37_1\r\nparso                     0.5.1                      py_0\r\npickleshare               0.7.5                    py37_0\r\npillow                    6.1.0            py37hdc69c19_0\r\npip                       19.2.3                   py37_0    anaconda\r\nprompt_toolkit            2.0.9                    py37_0\r\nprotobuf                  3.9.2            py37h33f27b4_0    anaconda\r\npsutil                    5.6.3            py37he774522_0\r\npycodestyle               2.5.0                    py37_0\r\npycparser                 2.19                     py37_0\r\npyflakes                  2.1.1                    py37_0\r\npygments                  2.4.2                      py_0\r\npylint                    2.4.2                    py37_0\r\npyopenssl                 19.0.0                   py37_0\r\npyparsing                 2.4.2                      py_0\r\npyqt                      5.9.2            py37h6538335_2\r\npyreadline                2.1                      py37_1    anaconda\r\npyrsistent                0.15.4           py37he774522_0\r\npysocks                   1.7.1                    py37_0\r\npython                    3.7.4                h5263a28_0    anaconda\r\npython-dateutil           2.8.0                    py37_0\r\npytz                      2019.2                     py_0\r\npywavelets                1.0.3            py37h452e1ab_1    conda-forge\r\npywin32                   223              py37hfa6e2cd_1\r\npyyaml                    5.1.2            py37he774522_0    anaconda\r\npyzmq                     18.1.0           py37ha925a31_0\r\nqt                        5.9.7            vc14h73c81de_0\r\nqtawesome                 0.6.0                      py_0\r\nqtconsole                 4.5.5                      py_0\r\nqtpy                      1.9.0                      py_0\r\nrequests                  2.22.0                   py37_0\r\nrope                      0.14.0                     py_0\r\nscikit-image              0.15.0           py37he350917_2    conda-forge\r\nscikit-learn              0.21.3           py37h6288b17_0\r\nscipy                     1.3.1            py37h29ff71c_0    anaconda\r\nsetuptools                41.2.0                   py37_0    anaconda\r\nsip                       4.19.8           py37h6538335_0\r\nsix                       1.12.0                   py37_0    anaconda\r\nsnowballstemmer           1.9.1                      py_0\r\nsphinx                    2.2.0                      py_0\r\nsphinxcontrib-applehelp   1.0.1                      py_0\r\nsphinxcontrib-devhelp     1.0.1                      py_0\r\nsphinxcontrib-htmlhelp    1.0.2                      py_0\r\nsphinxcontrib-jsmath      1.0.1                      py_0\r\nsphinxcontrib-qthelp      1.0.2                      py_0\r\nsphinxcontrib-serializinghtml 1.1.3                      py_0\r\nspyder                    3.3.6                    py37_0\r\nspyder-kernels            0.5.2                    py37_0\r\nsqlite                    3.29.0               he774522_0    anaconda\r\ntensorboard               1.14.0           py37he3c9ec2_0    anaconda\r\ntensorflow                1.14.0          gpu_py37h5512b17_0    anaconda\r\ntensorflow-base           1.14.0          gpu_py37h55fc52a_0    anaconda\r\ntensorflow-estimator      1.14.0                     py_0    anaconda\r\ntensorflow-gpu            1.14.0               h0d30ee6_0    anaconda\r\ntermcolor                 1.1.0                    py37_1    anaconda\r\ntestpath                  0.4.2                    py37_0\r\ntk                        8.6.8                hfa6e2cd_0\r\ntoolz                     0.10.0                     py_0    conda-forge\r\ntornado                   6.0.3            py37he774522_0\r\ntraitlets                 4.3.2                    py37_0\r\nurllib3                   1.24.2                   py37_0\r\nvc                        14.1                 h0510ff6_4    anaconda\r\nvs2015_runtime            14.16.27012          hf0eaf9b_0    anaconda\r\nwcwidth                   0.1.7                    py37_0\r\nwebencodings              0.5.1                    py37_1\r\nwerkzeug                  0.16.0                     py_0    anaconda\r\nwheel                     0.33.6                   py37_0    anaconda\r\nwin_inet_pton             1.1.0                    py37_0\r\nwincertstore              0.2                      py37_0    anaconda\r\nwrapt                     1.11.2           py37he774522_0    anaconda\r\nxz                        5.2.4                h2fa13f4_4\r\nyaml                      0.1.7            vc14h4cb57cf_1  [vc14]  anaconda\r\nzeromq                    4.3.1                h33f27b4_3\r\nzlib                      1.2.11           vc14h1cdd9ab_1  [vc14]  anaconda\r\nzstd                      1.3.7                h508b16e_0\r\n\r\n```\r\nAnd the error:\r\n```\r\n\r\n  File \"<ipython-input-1-c77ea08f5c30>\", line 1, in <module>\r\n    runfile('C:/Users/mazat/Documents/Python/MVTools/player_detector/player_detector_testing.py', wdir='C:/Users/mazat/Documents/Python/MVTools/player_detector')\r\n\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/mazat/Documents/Python/MVTools/player_detector/player_detector_testing.py\", line 404, in <module>\r\n    player_detector_run()\r\n\r\n  File \"C:/Users/mazat/Documents/Python/MVTools/player_detector/player_detector_testing.py\", line 397, in player_detector_run\r\n    glavnaya(dropbox_folder,gamename,mvstatus)\r\n\r\n  File \"C:/Users/mazat/Documents/Python/MVTools/player_detector/player_detector_testing.py\", line 252, in glavnaya\r\n    __,box1,score = yolo_class.detect_images(im2[ii].astype('uint8'))\r\n\r\n  File \"C:\\Users\\mazat\\Documents\\Python\\MVTools\\player_detector\\yolo3\\yolo3.py\", line 181, in detect_images\r\n    K.learning_phase(): 0\r\n\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n\r\nUnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node conv2d_1/convolution (defined at C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3650) ]]\r\n\t [[concat_11/_2833]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node conv2d_1/convolution (defined at C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3650) ]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node conv2d_1/convolution:\r\n conv2d_1/kernel/read (defined at C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:402)\t\r\n input_1 (defined at C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517)\r\n\r\nInput Source operations connected to node conv2d_1/convolution:\r\n conv2d_1/kernel/read (defined at C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:402)\t\r\n input_1 (defined at C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517)\r\n\r\nOriginal stack trace for 'conv2d_1/convolution':\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\spyder_kernels\\console\\__main__.py\", line 11, in <module>\r\n    start.main()\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\spyder_kernels\\console\\start.py\", line 318, in main\r\n    kernel.start()\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\r\n    self.io_loop.start()\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\asyncio\\base_events.py\", line 534, in run_forever\r\n    self._run_once()\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\r\n    handle._run()\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\asyncio\\events.py\", line 88, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\r\n    ret = callback()\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\r\n    self.run()\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\r\n    yielded = self.gen.send(value)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\r\n    return runner(coro)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-c77ea08f5c30>\", line 1, in <module>\r\n    runfile('C:/Users/mazat/Documents/Python/MVTools/player_detector/player_detector_testing.py', wdir='C:/Users/mazat/Documents/Python/MVTools/player_detector')\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\r\n    execfile(filename, namespace)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n  File \"C:/Users/mazat/Documents/Python/MVTools/player_detector/player_detector_testing.py\", line 404, in <module>\r\n    player_detector_run()\r\n  File \"C:/Users/mazat/Documents/Python/MVTools/player_detector/player_detector_testing.py\", line 397, in player_detector_run\r\n    glavnaya(dropbox_folder,gamename,mvstatus)\r\n  File \"C:/Users/mazat/Documents/Python/MVTools/player_detector/player_detector_testing.py\", line 142, in glavnaya\r\n    yolo_class=YOLO(model_name,script_dir, res)\r\n  File \"C:\\Users\\mazat\\Documents\\Python\\MVTools\\player_detector\\yolo3\\yolo3.py\", line 39, in __init__\r\n    self.boxes, self.scores, self.classes = self.generate()\r\n  File \"C:\\Users\\mazat\\Documents\\Python\\MVTools\\player_detector\\yolo3\\yolo3.py\", line 68, in generate\r\n    if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\r\n  File \"C:\\Users\\mazat\\Documents\\Python\\MVTools\\player_detector\\yolo3\\model.py\", line 72, in yolo_body\r\n    darknet = Model(inputs, darknet_body(inputs))\r\n  File \"C:\\Users\\mazat\\Documents\\Python\\MVTools\\player_detector\\yolo3\\model.py\", line 48, in darknet_body\r\n    x = DarknetConv2D_BN_Leaky(32, (3,3))(x)\r\n  File \"C:\\Users\\mazat\\Documents\\Python\\MVTools\\player_detector\\yolo3\\utils.py\", line 16, in <lambda>\r\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\r\n  File \"C:\\Users\\mazat\\Documents\\Python\\MVTools\\player_detector\\yolo3\\utils.py\", line 16, in <lambda>\r\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 457, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 171, in call\r\n    dilation_rate=self.dilation_rate)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3650, in conv2d\r\n    data_format=tf_data_format)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 894, in convolution\r\n    name=name)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 971, in convolution_internal\r\n    name=name)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1071, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\mazat\\Anaconda3\\envs\\talhaan\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```", "@mazatov , have you tried adding the following to your code?\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\ntf.keras.backend.set_session(tf.Session(config=config))\r\n\r\nSimilarly, you can set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true. I think this only will work for some setups, but it did for me with Ubuntu 18.04.\r\n\r\n", "@synapse8, yeah I tried those solutions and same error. I've installed tensorflow on other machines with the same method and only have issues with this new laptop.", "I began experiencing the issue as well. My setup is: \r\nUbuntu 18.04\r\nCuda compilation tools, release 10.0, V10.0.130 (as shown by nvcc --version)\r\ncudnn version 7.4.1 (as shown in cat /usr/include/cudnn.h)\r\nNVIDIA driver 430.26 (as shown grep \"X Driver\" /var/log/Xorg.0.log)\r\nRTX 2070\r\n\r\nI followed the tensorflow.org directions for installing cuda (the apt way). Prior to the latest release (during 2.0 alpha), in a different setup with the same specs, the methods outlined above were sufficient to resolve the issue for me (both ConfigProto and tf.config.gpu method). However, I began having the issues again and using none of the methods outlined above helped me resolve it (including  tf.config.experimental.set_memory_growth(gpu, True)).\r\n\r\nHowever after completing the installation of cuda via the apt way, I ended up with a cudnn installation >7.4. Then, I downgraded both of the cudnn packages to 7.4.1 via apt. \r\n\r\nI am using the Anaconda distribution to manage my environments and using the pip tensorflow-gpu installation inside a python 3.6.9 environment. If of importance, I run Jupyter lab server in my (base) environment and discover the kernels via the nb_conda_kernels package. \r\n\r\nI will appreciate any help. \r\n\r\nThe device placement log and error stack log is below: \r\n2019-10-02 22:23:55.954612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-10-02 22:23:55.994185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-02 22:23:55.995181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:42:00.0\r\n2019-10-02 22:23:55.995371: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-02 22:23:55.996093: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-02 22:23:55.996936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-02 22:23:55.997132: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-02 22:23:55.998046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-02 22:23:55.998696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-02 22:23:56.000455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-02 22:23:56.000578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-02 22:23:56.001680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-02 22:23:56.002628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-02 22:23:56.003049: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-02 22:23:56.028972: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3493015000 Hz\r\n2019-10-02 22:23:56.030382: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d7b909ac90 executing computations on platform Host. Devices:\r\n2019-10-02 22:23:56.030406: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-10-02 22:23:56.138700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-02 22:23:56.139251: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d7b90fcf00 executing computations on platform CUDA. Devices:\r\n2019-10-02 22:23:56.139282: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5\r\n2019-10-02 22:23:56.139544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-02 22:23:56.140240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:42:00.0\r\n2019-10-02 22:23:56.140283: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-02 22:23:56.140297: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-02 22:23:56.140310: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-02 22:23:56.140323: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-02 22:23:56.140335: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-02 22:23:56.140347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-02 22:23:56.140361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-02 22:23:56.140425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-02 22:23:56.141143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-02 22:23:56.141778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-02 22:23:56.141816: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-02 22:23:56.142797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-02 22:23:56.142815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-10-02 22:23:56.142821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-10-02 22:23:56.142932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-02 22:23:56.143630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-02 22:23:56.144296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7176 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:42:00.0, compute capability: 7.5)\r\n>>> model.compile(optimizer='adam',\r\n...               loss='sparse_categorical_crossentropy',\r\n...               metrics=['accuracy'])\r\n>>> model.fit(train_images, train_labels, epochs=10)\r\nTrain on 60000 samples\r\nEpoch 1/10\r\n2019-10-02 22:24:21.763808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-02 22:24:21.955544: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-02 22:24:22.569016: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-10-02 22:24:22.571165: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-10-02 22:24:22.571239: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node sequential/conv2d/Conv2D}}]]\r\n   32/60000 [..............................] - ETA: 37:35Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/kaandonbekci/anaconda3/envs/brain-decoding/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/kaandonbekci/anaconda3/envs/brain-decoding/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"/home/kaandonbekci/anaconda3/envs/brain-decoding/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/kaandonbekci/anaconda3/envs/brain-decoding/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/kaandonbekci/anaconda3/envs/brain-decoding/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/kaandonbekci/anaconda3/envs/brain-decoding/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/kaandonbekci/anaconda3/envs/brain-decoding/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/kaandonbekci/anaconda3/envs/brain-decoding/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/kaandonbekci/anaconda3/envs/brain-decoding/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/home/kaandonbekci/anaconda3/envs/brain-decoding/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/home/kaandonbekci/anaconda3/envs/brain-decoding/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node sequential/conv2d/Conv2D (defined at /home/kaandonbekci/anaconda3/envs/brain-decoding/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_677]\r\n\r\nFunction call stack:\r\ndistributed_function", "> @synapse8, yeah I tried those solutions and same error. I've installed tensorflow on other machines with the same method and only have issues with this new laptop.\r\n\r\nWell, I take my words back for now. Somehow after multiple restarts and reinstalls, these lines of code started working. Honestly not sure what changed. Seems like same results from `conda list` but it seems to work now \ud83e\udd37\u200d\u2642 ", "@kdonbekci I resolved my error via upgrading libcudnn packages back to 7.6.2 and using the code snippet\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n    except RuntimeError as e:\r\n        print(e)", "Anyone know how to remove cudnn and cuda from the ones listed in \"conda list cudnn\"?", "@kdonbekci Thank you for sharing.\r\nDowngrading libcudnn solved the issue for me even without turning on memory growth.\r\n\r\n```\r\napt-get remove libcudnn7\r\napt-get install libcudnn7=7.6.2.24-1+cuda10.0\r\n```\r\n\r\n\r\nKeras with [tensorflow 1.13.2-gpu](https://hub.docker.com/r/tensorflow/tensorflow/) backend\r\nTesla P100\r\nDriver Version: 410.79\r\nCUDA Version: 10.0", "> @kdonbekci Thank you for sharing.\r\n> Downgrading libcudnn solved the issue for me even without turning on memory growth.\r\n> \r\n> ```\r\n> apt-get remove libcudnn7\r\n> apt-get install libcudnn7=7.6.2.24-1+cuda10.0\r\n> ```\r\n> \r\n> Keras with [tensorflow 1.13.2-gpu](https://hub.docker.com/r/tensorflow/tensorflow/) backend\r\n> Tesla P100\r\n> Driver Version: 410.79\r\n> CUDA Version: 10.0\r\n\r\nWhere do I copy the line of code? I've just started to experience the same issue.", "@jamesgreenxxvii you can execute each line individually in your terminal.\r\nHope this helps.\r\n\r\nBest\r\nFabian", "Like @meanderingmoose0 and @blueclowd above, **I saw this when using the wrong version of cudnn.**  However the version recommended by @bluecloud did not work.  It turns out that if you scroll through the console messages, you'll find one that looks like this:\r\n\r\n```\r\n2019-10-22 23:09:31.460459: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.1 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version.\r\n```\r\n\r\nThis tells you what exact version you need to download.", "I have the same problem. Windows 10, 2080Ti, the latest versions of CUDA and cuDNN installed yesterday from NVIDIA site (both 10.1), so I don't think I need to downgrade them.\r\nDo I need to install a cuDNN driver for Anaconda? Or a CUDA driver? I've used pip install tensorflow-gpu, and have TF 2.0 now. Not sure it have installed any additional packages.\r\nSorry for the stupid questions, I'm very new, and I really can't launch anything :(", "I previously installed tf-nightly-gpu as the super user. I fixed the 'failed to get convolution algorithm' problem by re-installing tf-nightly-gpu with my normal user.\r\n\r\ntf-nightly-gpu-2.1.0\r\nlibcudnn7 (7.6.4.38)\r\ncuda 10.0\r\nUbuntu 19.10\r\npython3.7", "I got the same issue (TF2.0). It was working before so I got suspicious. I checked `nvidia-smi` and I realized the memory was full, even though I had stopped all my notebooks. I killed the process and it worked perfectly fine once again.", "I got this same error running the `models/research/deeplab/deeplab_demo.ipynb` notebook on my local machine. The error also coincided with my GPU memory filling completely. \r\n\r\nOn TF2.0, passing in `gpu_options.allow_growth = True` when creating the `Session` fixed the issue.\r\n\r\nHere's the exact change I made. I took this line:\r\n\r\n```\r\nself.sess = tf.Session(graph=self.graph)\r\n```\r\n\r\nAnd modified it along with adding two additional lines:\r\n\r\n```\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nself.sess = tf.compat.v1.Session(graph=self.graph, config=config)\r\n```\r\n(Since I'm using TensorFlow 2.0, I needed to add the `tf.compat.v1.Session` for compatibility)\r\n\r\nNow I can successfully run inference with the xception and mobilenetv2 models, which use about 8.1 and 7.7 GB of GPU memory, respectively (out of my card's 8.4 GB).\r\n\r\ntensorflow-gpu-2.0.0\r\nUbuntu 18.04\r\ncuda 10.0\r\ncudnn 7.6\r\nnvidia driver 430\r\nRTX 2070 Super graphics card", "Just for remind. I use tf2+cuda10, have the same question. But I found there still a process in the target GPU, so I kill it and solved.", "Same issue here\r\n\r\nWorks with these lines added\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.compat.v1.Session(graph=self.graph, config=config)", "> Just for remind. I use tf2+cuda10, have the same question. But I found there still a process in the target GPU, so I kill it and solved.\r\n\r\nCan you be specific? How did you solve it with tf2.0 and cuda10.0?", "@sri9s see my post above for an example that works. ", "I'm using an RTX 2080ti and on checking nvdia-smi it shows dedicated gpu memory is almost fully occupied by some apps running on my win10.\r\nHow do I disable windows apps from using my gpu memory and only use it only for deep learning?", "> @kdonbekci Thank you for sharing.\r\n> Downgrading libcudnn solved the issue for me even without turning on memory growth.\r\n> \r\n> ```\r\n> apt-get remove libcudnn7\r\n> apt-get install libcudnn7=7.6.2.24-1+cuda10.0\r\n> ```\r\n> \r\n> Keras with [tensorflow 1.13.2-gpu](https://hub.docker.com/r/tensorflow/tensorflow/) backend\r\n> Tesla P100\r\n> Driver Version: 410.79\r\n> CUDA Version: 10.0\r\n\r\nThanks that did the trick with TensorFlow 2.0.0 (GPU). Just a quick suggestion, run this to prevent updating it with `apt-get upgrade`:\r\n```\r\napt-mark hold libcudnn7=7.6.2.24-1+cuda10.0\r\n```", "Same issue here\r\n\r\ndownloaded cuDNN https://developer.nvidia.com/rdp/cudnn-download, and installed it(follow the link https://askubuntu.com/questions/767269/how-can-i-install-cudnn-on-ubuntu-16-04), the problem was sloved.", "**Same error i got , The Reason of getting this error is due to the mismatch of the version of the cudaa/cudnn with your tensorflow version there are two methods to solve this:**\r\n\r\n1.  Either you Downgrade your Tensorflow Version \r\n`pip install --upgrade tensorflowgpu==1.8.0`\r\n\r\n2.  Or You can follow the steps at [Here](https://www.tensorflow.org/install/gpu).\r\n\r\n        tip:  Choose your ubuntu version and follow the steps.:-)", "> @kdonbekci Thank you for sharing.\r\n> Downgrading libcudnn solved the issue for me even without turning on memory growth.\r\n> \r\n> ```\r\n> apt-get remove libcudnn7\r\n> apt-get install libcudnn7=7.6.2.24-1+cuda10.0\r\n> ```\r\n> \r\n> Keras with [tensorflow 1.13.2-gpu](https://hub.docker.com/r/tensorflow/tensorflow/) backend\r\n> Tesla P100\r\n> Driver Version: 410.79\r\n> CUDA Version: 10.0\r\n\r\nThat's a good catch!!!\r\nI follow the TF gpu guideline and missed to \"remove\" the libcudnn7 before I install the 7.6.2.24 and now it all works for me:\r\nUbuntu 16\r\ncuda10-0\r\npython3.7.5\r\nTeala P40\r\nTF2.0", "Hi all. I am facing the same issue here:\r\n\r\ntensorflow-gpu-2.0.0\r\nUbuntu 18.04\r\ncudatoolkit 10.0\r\ncudnn 7.6.4\r\ncuda 10.0\r\nNvidia driver 430\r\nRTX 2080\r\n\r\nDoes anyone know how to downgraded cudnn 7.6.4 to 7.4.1 version? or how do I just maintain the cudnn 7.6.4 version and use another way to solve it? Anyone? Thanks.", "> Hi all. I am facing the same issue here:\r\n> \r\n> tensorflow-gpu-2.0.0\r\n> Ubuntu 18.04\r\n> cudatoolkit 10.0\r\n> cudnn 7.6.4\r\n> cuda 10.0\r\n> Nvidia driver 430\r\n> RTX 2080\r\n> \r\n> Does anyone know how to downgraded cudnn 7.6.4 to 7.4.1 version? or how do I just maintain the cudnn 7.6.4 version and use another way to solve it? Anyone? Thanks.\r\n\r\nFor TF2.0, I think it's the 'out of memory' problem. By default, TF use all your GPU memory then triger this issue. Therefore, you could initial your code with: \r\n\r\n`physical_devices = tf.config.experimental.list_physical_devices('GPU')`\r\n`assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"`\r\n`tf.config.experimental.set_memory_growth(physical_devices[0], True)`\r\n\r\nFor more information, please check TF documents:\r\n\r\n[https://www.tensorflow.org/guide/gpu](url) \r\n[https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth](url)", "@Cilicili please try with the below settings:\r\n```\r\nfrom tensorflow.compat.v1.keras.backend import set_session\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\nconfig.log_device_placement = True  # to log device placement (on which device the operation ran)\r\nsess = tf.compat.v1.Session(config=config)\r\nset_session(sess)\r\n```\r\n\r\nI was facing the similar problem. When you run your code it occupies all memory of GPU and with the above provided settings it should work. The above settings help you with both tf1.x as well as tf2.", "I face this problem with the environment:\r\nubuntu 18.04\r\nNVIDIA driver 418.67\r\nGeforce RTX 2080\r\ntensorflow-gpu 1.12\r\n\r\nI try various methods to solve this problem. First, I think that it is an Incompatible problem of CUDA and Cudnn. I experiment with numerous versions of CUDA and Cudnn.  It has no work for me. I google this problem and review a solution in:\r\nhttps://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in\r\nIt gives me an idea for the reason for this problem---Out of Memory(OOM). My computer has 16G memory and 8G memory for 2080 GPU. In CPU model, it has no problem to train my DL model, but in GPU model, I run into this problem. \r\nI solve the problem by adding a limit for GPU memory:\r\n```python\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.8\r\nrun_config = tf.contrib.tpu.RunConfig(\r\n     ...,\r\n     session_config=config,\r\n     ...)\r\n\r\nestimator = tf.contrib.tpu.TPUEstimator(\r\n      ...,\r\n      config=run_config,\r\n      ...)\r\n```\r\nand then, it is work!", "> I face this problem with the environment:\r\n> ubuntu 18.04\r\n> NVIDIA driver 418.67\r\n> Geforce RTX 2080\r\n> \r\n> I try various methods to solve this problem. First, I think that it is an Incompatible problem of CUDA and Cudnn. I experiment with numerous versions of CUDA and Cudnn. It has no work for me. I google this problem and review a solution in:\r\n> https://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in\r\n> It gives me an idea for the reason for this problem---Out of Memory(OOM). My computer has 16G memory and 8G memory for 2080 GPU. In CPU model, it has no problem to train my DL model, but in GPU model, I run into this problem.\r\n> I solve the problem by adding a limit for GPU memory:\r\n> \r\n> ```python\r\n> config = tf.ConfigProto()\r\n> config.gpu_options.per_process_gpu_memory_fraction = 0.8\r\n> run_config = tf.contrib.tpu.RunConfig(\r\n>      ...,\r\n>      session_config=config,\r\n>      ...)\r\n> ```\r\n> \r\n> and then, it is work!\r\nAre you using tf1 or tf2?\r\n", "Workaround: \r\nFresh install TF 2.0 and ran a simple Minst tutorial, it was alright, opened another notebook, tried to run and encountered this issue.\r\nI existed all notebooks and restarted Jupyter and open only one notebook, ran it successfully\r\nIssue seems to be either memory or running more than one notebook on GPU\r\nThanks", "Workaround:\r\nhttps://github.com/tensorflow/tensorflow/issues/25160#issuecomment-568505081", "i had same problem with ubuntu + cuda 10.2 + cudnn 7.6.5\r\ni use the method which @oscarlinux  mentioned, and it works!!\r\nthank you! it saved the day!", "> Hi all. I am facing the same issue here:\r\n> \r\n> tensorflow-gpu-2.0.0\r\n> Ubuntu 18.04\r\n> cudatoolkit 10.0\r\n> cudnn 7.6.4\r\n> cuda 10.0\r\n> Nvidia driver 430\r\n> RTX 2080\r\n> \r\n> Does anyone know how to downgraded cudnn 7.6.4 to 7.4.1 version? or how do I just maintain the cudnn 7.6.4 version and use another way to solve it? Anyone? Thanks.\r\n\r\nmy desk top use GTX1080 even cudnn 7.6.5 works!! may be need to switch back to GTX1080", "> > I face this problem with the environment:\r\n> > ubuntu 18.04\r\n> > NVIDIA driver 418.67\r\n> > Geforce RTX 2080\r\n> > I try various methods to solve this problem. First, I think that it is an Incompatible problem of CUDA and Cudnn. I experiment with numerous versions of CUDA and Cudnn. It has no work for me. I google this problem and review a solution in:\r\n> > https://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in\r\n> > It gives me an idea for the reason for this problem---Out of Memory(OOM). My computer has 16G memory and 8G memory for 2080 GPU. In CPU model, it has no problem to train my DL model, but in GPU model, I run into this problem.\r\n> > I solve the problem by adding a limit for GPU memory:\r\n> > ```python\r\n> > config = tf.ConfigProto()\r\n> > config.gpu_options.per_process_gpu_memory_fraction = 0.8\r\n> > run_config = tf.contrib.tpu.RunConfig(\r\n> >      ...,\r\n> >      session_config=config,\r\n> >      ...)\r\n> > ```\r\n> > \r\n> > \r\n> > and then, it is work!\r\n> > Are you using tf1 or tf2?\r\n\r\nI think it is tf1.12, so the solution may not work for tf2.0 or tf2.1", "Same issue here (while running the official MNIST sample). My setup:\r\n\r\ntensorflow-gpu-2.0.0\r\nUbuntu 18.04\r\nCUDA 10.0\r\ncudnn 7.6.4\r\nNvidia Driver 430\r\nGTX-1080\r\n\r\nThis error is annoying because we have nearly zero hint on the root cause. Already try the GPU memory fix but no luck.", "Same here. I have RTX 2080 SUPER on Ubuntu 18.04 and tried different versions of tf/cuda/cudnn/nvidia-driver. I've also tried tf in Docker container and had no luck.\r\n\r\n`allow_growth` partially solves the issue but it leads to [a new one](https://github.com/tensorflow/tensorflow/issues/28287) when I tried to train [WGAN-GP](https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan_gp/wgan_gp.py)\r\n\r\ntf 2.1 with cuda 10.1 gives [new exception](https://github.com/tensorflow/tensorflow/issues/11812)", "UPDATE:\r\n\r\nJust update to TF 2.1  (with CUDA 10.1 and cuDNN 7.6.4). All work well now.", "Works for me\r\nTF2\r\nNVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.0\r\nGeForce RTX 2080ti\r\n\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.2\r\nconfig.gpu_options.allow_growth = True\r\n```", "It also works on tensorflow 2.1!\r\n\r\nThanks to @oscarlinux I found the solution on [tensorflow.org](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)\r\n\r\n```python\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n```\r\n\r\n> \"Although the error is gone for now, there is still so much for us to do.\r\nI believe in my heart, that if all of us work together, we can restore Tensorflow to its former glory. Perhaps even beyond. But it all must start with us.\"", "For me, I had the same problem because my server was being used by multiple devices so there wasn't enough memory to run the code on the dataset. \r\n\r\nSome solutions can be to free some memory and reduce size of the dataset.", "I had two Jupyter Notebook Tabs open. Both containing a GPU-initialization. Shutting down one tab and restarting the kernel in the other solved the issue.", "According the comment of @neolee  and @Man7hano , i change my system information to TF 2.1 with CUDA 10.1 and cuDNN 7.6.4 too (in addition, OS is windows10, gpu is GTX 1660 ti\uff0cpython is 3.7.4\uff09\uff0cwhen i run object_detection_tutorial.ipynb in jupyter notebook, the error still exist:\r\n `2020-01-29 21:56:35.944996: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-01-29 21:56:35.952224: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-01-29 21:56:35.958555: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/batchnorm/mul_1}}]]\r\n         [[Postprocessor/BatchMultiClassNonMaxSuppression/map/TensorArrayStack_4/range/_50]]\r\n2020-01-29 21:56:35.980198: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/batchnorm/mul_1}}]]`\r\nAnyone can give me a suggestion ? Thanks !\r\n\r\nI fixed it just now by adding below codess after \"import tensorflow as tf\":\r\n![Snipaste_2020-01-29_23-09-08](https://user-images.githubusercontent.com/43233772/73368722-6e643f00-42ec-11ea-8fb4-37a5fb5e05cd.png)\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus :\r\n\r\n    try:\r\n\r\n        for gpu in gpus:\r\n\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n        tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 4)])\r\n\r\n    except RuntimeError as e:\r\n\r\n        print(e)", "So I ran into this problem when I tried to re-install TensorFlow in an older Nvidia docker. After running my notebook I got the same error. What I learned after a long while was that the \"warning\" message that the error's talking about *shows up in the jupyter console running the jupyter notebook server*, and is not piped into stdout. \r\n\r\nThere I found out the error message that clearly said my TensorFlow (the one I got from pip) was built against a newer cudnn library than I had on the Nvidia docker image. The solution was to get the new cudnn that matched the major and minor of the TensorFlow built cudnn library from [cudnn archive](https://developer.nvidia.com/rdp/cudnn-archive) and install it:\r\n\r\n```\r\n$ tar -xzvf cudnn-10.2-linux-x64-v7.6.5.32.tgz\r\n$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include\r\n$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\r\n$ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\r\n```", "> > Hi all. I am facing the same issue here:\r\n> > tensorflow-gpu-2.0.0\r\n> > Ubuntu 18.04\r\n> > cudatoolkit 10.0\r\n> > cudnn 7.6.4\r\n> > cuda 10.0\r\n> > Nvidia driver 430\r\n> > RTX 2080\r\n> > Does anyone know how to downgraded cudnn 7.6.4 to 7.4.1 version? or how do I just maintain the cudnn 7.6.4 version and use another way to solve it? Anyone? Thanks.\r\n> \r\n> For TF2.0, I think it's the 'out of memory' problem. By default, TF use all your GPU memory then triger this issue. Therefore, you could initial your code with:\r\n> \r\n> `physical_devices = tf.config.experimental.list_physical_devices('GPU')`\r\n> `assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"`\r\n> `tf.config.experimental.set_memory_growth(physical_devices[0], True)`\r\n> \r\n> For more information, please check TF documents:\r\n> \r\n> [https://www.tensorflow.org/guide/gpu](url)\r\n> [https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth](url)\r\n\r\nCilicili solution indeed works, if you use TensorFlow 2.* in eager mode (no sessions!)\r\nthanks!\r\n", "Is it possible to understand from where TF is trying to load CuDNN?", "> \u6b64\u9519\u8bef\u53ef\u80fd\u4e0e\u4f7f\u7528\u8fdb\u884c\u5b89\u88c5TF\u6709\u5173`conda`\u3002\r\n> \r\n> \u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u662f\u8fd9\u6837\u7684\uff1a\r\n> \u5728\u547d\u4ee4\u884c\u4e2d\u53d1\u51fa\u4ee5\u4e0b\u547d\u4ee4\uff1a\r\n> `conda list cudnn`\r\n> \u5b83\u5c06\u6253\u5370\uff1a\r\n> ` Name Version Build Channel`\r\n> \r\n> \u5982\u679c\u4e0a\u8ff0\u7ed3\u679c\u4e0d\u662f\u7a7a\u7684\uff0c\u5219\u610f\u5473\u7740\u60a8\u4f7f\u7528conda\u5b89\u88c5\u7684TF\uff0c\u5f53\u4f7f\u7528conda\u5b89\u88c5TF\u65f6\uff0c\u5b83\u5c06\u5b89\u88c5\u6240\u6709\u4f9d\u8d56\u9879\uff0c\u751a\u81f3\u5305\u62ecCUDA\u548ccuDNN\uff0c\u4f46\u662fcuDNN\u7248\u672c\u5bf9\u4e8eTF\u975e\u5e38\u4f4e\uff0c\u56e0\u6b64\u4f1a\u5e26\u6765\u517c\u5bb9\u6027\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u60a8\u53ef\u4ee5\u5378\u8f7dconda\u5b89\u88c5\u7684cuDNN\u548cCUDA\uff0c\u7136\u540e\u8fd0\u884cTF\uff0c\u7136\u540e\u5b83\u5c06\u8d77\u4f5c\u7528\u3002\r\n\r\nWhat if it is empty?", "I have CUDA 10.0, cuDNN 7.4.1 (also tried 7.6.5), ubuntu 18.04\r\nFor tensorflow-gpu==2.0.0 setting up allow_growth = True didn't help\r\nsetting up allow_growth = True helped for tensorflow-gpu==1.14.0", "> I have CUDA 10.0, cuDNN 7.4.1 (also tried 7.6.5), ubuntu 18.04\r\n> For tensorflow-gpu==2.0.0 setting up allow_growth = True didn't help\r\n> setting up allow_growth = True helped for tensorflow-gpu==1.14.0\r\n\r\nI had the same issue for TF2, Cuda-10, Cudnn-7.4/7.6\r\n\r\nthis workaround worked for me for cudnn failure as mentioned above\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\nNote if you want to use Keras:fit_generator(), multiprocessing=True will again cause memory errors, so better to write your own custom multiprocessing queue.", "Why is this ticket closed? Has the Tensorflow team make any fix for this in the sources? I have been even compiling Tensorflow from scratch against CUDA 10.1 and CUDA 10.2 on ubuntu 18.04 using the latest sources and it still fails with the same error.\r\nThe workaround with allow_growth=True helps only with smaller models, but is still only a workaround and not the fix.", "Why is this ticket closed? Has the Tensorflow team make any fix for this in the sources? I have been even compiling Tensorflow from scratch against CUDA 10.1 and CUDA 10.2 on ubuntu 18.04 using the latest sources and it still fails with the same error.\r\nThe workaround with allow_growth=True helps only with smaller models, but is still only a workaround and not the fix.\r\n\r\n___________I agree with you!!", "Same problem here\r\n\r\nnvidia-driver-440.59\r\nUbuntu 18.04.4 LTS\r\nNvidia RTX 2060\r\nCUDA 10.2\r\nNvida Docker 2\r\nTF 2.1 (2.1.0-gpu-py3-jupyter)\r\n\r\nDoesn't matter if the container was just started or already run some times, always the same error, so it can't be memory usage.\r\n\r\nTrying to run \"https://www.tensorflow.org/tutorials/quickstart/advanced\"\r\n", "This thing happened to me after installing keras-vis and downgrading to scipy 1.1.0, but I'm not sure it's related, because reverting these changes, didn't solve the issues. However a simple reboot solved the issue. By the way, I often find that \"nonsense\" TF errors are solved by reboots. :)\r\n\r\nUpdate: I just quickly double checked, and downgrading to scipy 1.1.0 seemed to be reproduce and cancel this problem after upgrading back to 1.4.1.\r\n\r\nUpdate: Another black magic which seems to help a lot of times is just inserting this snippet.\r\n    config = tf.ConfigProto(allow_soft_placement=True,\r\n                            log_device_placement=False)\r\n    config.gpu_options.allow_growth = False\r\n    config.gpu_options.per_process_gpu_memory_fraction = 0.8\r\n    sess = tf.Session(graph=tf.get_default_graph(),\r\n                      config=config)", "@karkirowle\r\nThat gives me\r\n```\r\nAttributeError: module 'tensorflow' has no attribute 'ConfigProto'\r\n```\r\n\r\nFor me\r\n```\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.compat.v1.InteractiveSession(config=config)\r\n```\r\nis working, but that's still just a workaround. The docker hub tensorflow 2.1 container in my opinion should work out of the box without any error messages.", "Hello. I think that I have the same problem.\r\n\r\nnvidia-driver-442.19\r\nWindows 10\r\nNvidia RTX 2080 TI\r\nCUDA Version 10.0.130\r\nCUDnn = 7.6.5\r\n\r\n\u03bb pip3 list | grep tensorflow\r\ntensorflow            1.15.0\r\ntensorflow-estimator  1.15.1\r\n\r\nconda list cudnn : empty\r\n\r\nI'm trying to directly convert real-person videos to the motion of animation models (i.e. Miku, Anmicius) following the instructions located here :\r\n\r\nhttps://github.com/peterljq/OpenMMD\r\n\r\nand this :\r\n\r\nhttps://www.youtube.com/watch?v=hKx6jl9a5-I\r\n\r\nI'm stuck here,when it says : \"After that, proceed to the FCRN-DepthPrediction-vmd folder and run VideoToDepth.dat\"\r\n\r\nso :\r\n\r\n\u03bb python tensorflow/predict_video.py --model_path tensorflow/data/NYU_FCRN.ckpt --video_path rp.mov --baseline_path /Pers/cg/MMD/OpenMMD/3d-pose-baseline-vmd/json_rp_3d_20200224_191201_idx01 --interval 10 --verbose 3\r\n2020-02-24 19:32:27.891927: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\nINFO:__main__:\u6df1\u5ea6\u63a8\u5b9a\u51fa\u529b\u958b\u59cb\r\nDEBUG:__main__:width: 1920.0, height: 1080.0\r\nDEBUG:__main__:scale: 0.26666666666666666\r\nDEBUG:__main__:width: 512, height: 288\r\nWARNING:tensorflow:From tensorflow/predict_video.py:68: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From tensorflow/predict_video.py:68: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\network.py:161: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nWARNING:tensorflow:From K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\network.py:161: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nWARNING:tensorflow:From K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\network.py:127: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n\r\nWARNING:tensorflow:From K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\network.py:127: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n\r\nWARNING:tensorflow:From K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\network.py:193: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\r\n\r\nWARNING:tensorflow:From K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\network.py:193: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\r\n\r\nWARNING:tensorflow:From K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\network.py:291: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nWARNING:tensorflow:From K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\network.py:291: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nWARNING:tensorflow:From tensorflow/predict_video.py:75: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\nWARNING:tensorflow:From tensorflow/predict_video.py:75: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2020-02-24 19:32:30.767433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-02-24 19:32:30.791709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:01:00.0\r\n2020-02-24 19:32:30.791955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2020-02-24 19:32:30.795334: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2020-02-24 19:32:30.797579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll\r\n2020-02-24 19:32:30.798576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll\r\n2020-02-24 19:32:30.801752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll\r\n2020-02-24 19:32:30.803828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll\r\n2020-02-24 19:32:30.809762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-02-24 19:32:30.810085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-02-24 19:32:30.810332: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-02-24 19:32:30.812172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:01:00.0\r\n2020-02-24 19:32:30.812316: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2020-02-24 19:32:30.812387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2020-02-24 19:32:30.812455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll\r\n2020-02-24 19:32:30.812523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll\r\n2020-02-24 19:32:30.812590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll\r\n2020-02-24 19:32:30.812687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll\r\n2020-02-24 19:32:30.812762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-02-24 19:32:30.813033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-02-24 19:32:31.239492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-24 19:32:31.239602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2020-02-24 19:32:31.239664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2020-02-24 19:32:31.240105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9530 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:From tensorflow/predict_video.py:78: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\r\n\r\nWARNING:tensorflow:From tensorflow/predict_video.py:78: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\r\n\r\nINFO:tensorflow:Restoring parameters from tensorflow/data/NYU_FCRN.ckpt\r\nINFO:tensorflow:Restoring parameters from tensorflow/data/NYU_FCRN.ckpt\r\nINFO:__main__:\u6df1\u5ea6\u63a8\u5b9a: n=0\r\n2020-02-24 19:32:34.592124: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-02-24 19:32:35.977148: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.5.0 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n2020-02-24 19:32:35.979070: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.5.0 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1350, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1443, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node conv1/Conv2D}}]]\r\n         [[ConvPred/ConvPred/_791]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node conv1/Conv2D}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow/predict_video.py\", line 264, in <module>\r\n    main()\r\n  File \"tensorflow/predict_video.py\", line 261, in main\r\n    predict_video(args.model_path, args.video_path, args.baseline_path, interval, smoothed_2d)\r\n  File \"tensorflow/predict_video.py\", line 111, in predict_video\r\n    pred = sess.run(net.get_output(), feed_dict={input_node: img})\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node conv1/Conv2D (defined at C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\r\n         [[ConvPred/ConvPred/_791]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node conv1/Conv2D (defined at C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nOriginal stack trace for 'conv1/Conv2D':\r\n  File \"tensorflow/predict_video.py\", line 264, in <module>\r\n    main()\r\n  File \"tensorflow/predict_video.py\", line 261, in main\r\n    predict_video(args.model_path, args.video_path, args.baseline_path, interval, smoothed_2d)\r\n  File \"tensorflow/predict_video.py\", line 71, in predict_video\r\n    net = models.ResNet50UpProj({'data': input_node}, batch_size, 1, False)\r\n  File \"K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\network.py\", line 71, in __init__\r\n    self.setup()\r\n  File \"K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\fcrn.py\", line 6, in setup\r\n    .conv(7, 7, 64, 2, 2, relu=False, name='conv1')\r\n  File \"K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\network.py\", line 46, in layer_decorated\r\n    layer_output = op(self, layer_input, *args, **kwargs)\r\n  File \"K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\network.py\", line 166, in conv\r\n    output = convolve(input_data, kernel)\r\n  File \"K:\\Pers\\cg\\MMD\\OpenMMD\\FCRN-DepthPrediction-vmd\\tensorflow\\models\\network.py\", line 159, in <lambda>\r\n    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding='VALID')\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\", line 2010, in conv2d\r\n    name=name)\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\", line 1071, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"C:\\Users\\marietto2020\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\r\n    self._traceback = tf_stack.extract_stack()", "This problem is still an issue for me. I have to strictly work with TF2.0 to make it work, none of the solution works with tf1.14 or tf1.15 which I need to use because my professor doesn't want us using tf1 for any of her assignment. ", "@oluwayetty \r\nDid you try to call \r\n```\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.compat.v1.InteractiveSession(config=config)\r\n```\r\n?\r\nThis was working for me for TF 2.0", "@xam-ps  : excusme,I'm a newbie. I'm trying to do like you suggest,but for me it does not work. Where should I write the commands that u suggest ? I did that after having enabled tensorflow (that I have upgraded to 2.1),but this is what happened :\r\n\r\n(tensorflow) \u03bb config = tf.compat.v1.ConfigProto()\r\n\"config\" is not recognized as an interal or external command.", "For anyone who has this error with tf2, try using conda as your virtual environment. It worked for me. Conda often solves CUDA and CuDNN issues.\r\n`conda install tensorflow-gpu`", "So I followed this entire thread, and still have issues.\r\n\r\n- Windows 10 (v1903 - build 18362.657)\r\n- GTX 2060 (NVIDIA driver 442.50)\r\n- Conda venv\r\n- Jupyter Notebook\r\n- Python 3.7\r\n- TensorFlow 2.1\r\n- CUDA: 10.2\r\n- cuDNN: 7.6.5\r\n\r\n\r\nI've tried all of the above, and I can avoid this error when I don't have TensorFlow-GPU installed in my conda venv, but when I install TensorFlow-GPU, the problem arises again. \r\n\r\nI've followed all the installation instructions from [here](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#install-windows) and [here](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html) word for word. **I've installed them in the recommended locations. Do I need to do anything special for the venv?**\r\n\r\ntf.ConfigProto() and all the various mentions above didn't work. `pip uninstall/install hdf5` and `pip uninstall/install h5py` didn't work (yes I made sure not both were installed at the same time).\r\n\r\nI've even completely wiped my SSD of all Python, Anaconda, and various packages and started from scratch. Still didn't work. \r\n\r\nAny suggestions? I'm about to try Docker to see if that helps, but at this point I'm out of my depth. I'm a DS/stats guy and not a CS guy, unfortunately. \r\n\r\n\r\n\r\n", "@awrgold \r\nI was using tensorflow 2.1 trough (nvidia2) docker and have the issue anyways. So I don't think that solves the problem.", "I had same issue on ubuntu 18.04, tensorflow 2.1.0. I resolved it through allowing gpu growth. After a while I got same error and I don't know why. \r\n\r\nNow I find reason, it is because of asynchronous learning model that I implemented through threaing.Thread. Other models work properly. If you have this error, check if you are using threading or mulitiprocessing.", "> Did you try setting up allow_growth = True? That resolved the problem for me.\r\n\r\nWhen I set up allow_growth = True, the issue disappears. However, when I set two virtual GPU devices with only one physical GPU device, allow_growth can not be set True. So, how can I solve the issue? Do you have any idea? When I run my code in Colab, it is ok without setting allow_growth. I am quite confused.", "I tried the same, it doesn't help at all. For now I've been training on the CPU but I'd really like to speed things up. I've even tried going down to CUDA 9 and TF 1.15 to see if that helped, but to no avail.", "> > Did you try setting up allow_growth = True? That resolved the problem for me.\r\n> \r\n> When I set up allow_growth = True, the issue disappears. However, when I set two virtual GPU devices with only one physical GPU device, allow_growth can not be set True. So, how can I solve the issue? Do you have any idea? When I run my code in Colab, it is ok without setting allow_growth. I am quite confused.\r\n\r\nYou have to clarify your issue. Why do you need two virtual devices and one physical device? Secondly, do you use a docker container or a local environment? What are your specs? Colab uses other GPUs then you, why shouldn't it work there?", "@Man7hano \r\nThere is only one physical GPU in the local environment, therefore, I create two virtual GPU devices to train the neural network in a  distributed way. \r\nLocal env: RTX2060; Cuda 10.1; cuDNN 7.6.4. I do not use a docker container.\r\nI run Colab in Google drive with only one GPU device.\r\nI am not sure whether the issue about cuDNN initialization stems from the configuration allow_growth=True or env version.", "> config = tf.compat.v1.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = tf.compat.v1.InteractiveSession(config=config)\r\n\r\nthis worked for me. cuda 10.1, cudnn 7.6.5, tf 2.1, windows 10", "> @Man7hano\r\n> There is only one physical GPU in the local environment, therefore, I create two virtual GPU devices to train the neural network in a distributed way.\r\n> Local env: RTX2060; Cuda 10.1; cuDNN 7.6.4. I do not use a docker container.\r\n> I run Colab in Google drive with only one GPU device.\r\n> I am not sure whether the issue about cuDNN initialization stems from the configuration allow_growth=True or env version.\r\n\r\nAs far as I know, distribution makes only sense if you have multiple physical GPUs. Otherwise, you are splitting the performance of the physical one. If you have to split it, you should limit the memory growth of the virtual GPUs. You can find the docs on Tensorflow.\r\nGoogle Colab probably uses bigger GPUs then you are using and half the performance is just fine, or they reallocate to other GPUs available.\r\nHowever, I would suggest the official Docker container of Tensorflow for local development.", "I had this issue with TF 2.0, so I wiped everything out (conda, CUDA, cudatools, cudnn) and started over. I ran \"conda install tensorflow-gpu\" on my brand new miniconda install (3.7) and the issue reappeared. Adding:\r\n```\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nassert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```\r\n\r\nfixed it, for now.", "Please try set memory limit, that resolve my problem: REF: https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth", "> @oluwayetty\r\n> Did you try to call\r\n> \r\n> ```\r\n> config = tf.compat.v1.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = tf.compat.v1.InteractiveSession(config=config)\r\n> ```\r\n> \r\n> ?\r\n> This was working for me for TF 2.0\r\n\r\nHey, yes the solution above works for tf2.0 but not for tf1.14 or tf1.15. I needed it for a project I'm working on that's based on tf1.x. Thanks", "I face the same problem, finally I find because my cudnn version is wrong. I think you have the same issue. May this would help you.\r\n\r\n> 2020-02-24 19:32:35.979070: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.5.0 but source was compiled with: 7.6.0. CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library. If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n\r\n", "exm :\r\ncuda10.1\r\ncuDNN7.6\r\ntensorflow1.13\r\nit seems that all codes following doesn't work for me:\r\n![image](https://user-images.githubusercontent.com/61641190/77849210-86791180-71fc-11ea-9624-bdcd9f9c8f1e.png)\r\n\r\ncould u please give me a \"debug\" answer,thx\r\n\r\nps:the first three error:Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node Conv1_2/convolution}}]]\r\n\t [[{{node Mean_2}}]]\r\nthe last two error:\r\nmodule 'tensorflow' has no attribute 'config'", "Have the same issue even using official tensorflow docker (v 1.15.2 + object detection api for v 1.13.0)", "@HazelSCUT \r\nDo u have find solution?\r\nexm :\r\ncuda9.1\r\ncuDNN7.1\r\ntensorflow1.15", "Also noticed that the docker image works well on another machine (GTX 1080 Ti) while fails on mine (RTX 2080 SUPER). Tried to completely reinstall everything related to nvidia, including drivers (also tried to downgrade them) - nothing helped.", "@pryadchenko \r\nSame issue here. Using the official TF-Docker image. Working fine on GTX cards, having cuDNN error on RTX cards. Google should really do sth. about that!!\r\nNo clue why this bug is closed already...", "The same problem here. It appeared after upgrading GPU from Nvidia GTX1060 3GB to GTX 1660 Super 6GB.\r\nI've already tried to install clean: GPU drivers, CUDA 10.2, cuDNN for CUDA 10.2. Nothing works so far.\r\nI've also reinstalled Tensorflow (previous was a conda installation), this time I've used pip version 2,1,0-cp37-cp37-win-amd64.\r\n\r\nI keep digging...", "This error I got on my machine due to out of memory (The trace back didn't told me that), I found out because 4th solution (Given below) worked for me. And interesting thing was that I had two machines with following specs:\r\n\r\n### Machine 1:\r\n1. *OS:*           Windows 10 x64  \r\n2. *GPU:*         GTX 1060 6GB,  \r\n3. *Cuda:*       10.0, \r\n4. *CuDNN:*    v7.6.4.38 \r\n5. *TF:*             1.15 GPU\r\n6. *conda/pip?* Installed using ``` pip install tensorflow-gpu==1.15 ```\r\n7. *Python:*      3.7\r\n\r\n### Machine 2:\r\n1. *OS:*           Windows 10 x64  \r\n2. *GPU:*         GTX 1660 Ti 6GB,  \r\n3. *Cuda:*       10.0, \r\n4. *CuDNN:*    v7.6.4.38 \r\n5. *TF:*             1.15 GPU\r\n6. *conda/pip?* Installed using ``` pip install tensorflow-gpu==1.15 ```\r\n7. *Python:*      3.7\r\n\r\nThe code worked on Machine 1 very well but on Machine 2 it will give the error: *Failed to get convolution algorithm. This is probably because cuDNN failed to initialize*. In my case 2nd solution in 4 part (the last one) solved my issue.\r\n\r\nI've seen this error message for four different reasons, with different solutions: \r\n\r\n### 1. You have cache issues\r\nI regularly work around this error by shutting down my python process, removing the ~/.nv directory (on linux, rm -rf ~/.nv), and restarting the Python process. I don't exactly know why this works. It's probably at least partly related to the second option:\r\n\r\n### 2. You're out of memory\r\nThe error can also show up if you run out of graphics card RAM. With an nvidia GPU you can check graphics card memory usage with nvidia-smi. This will give you not only a readout of how much GPU RAM you have in use (something like 6025MiB /  6086MiB if you're almost at the limit) as well as a list of what processes are using GPU RAM.\r\n\r\nIf you've run out of RAM, you'll need to restart the process (which should free up the RAM) and then take a less memory-intensive approach. A few options are:\r\n\r\nreducing your batch size\r\nusing a simpler model\r\nusing less data\r\nlimit TensorFlow GPU memory fraction: For example, the following will make sure TensorFlow uses <= 90% of your RAM:\r\n```\r\nimport keras\r\nimport tensorflow as tf\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.9\r\nkeras.backend.tensorflow_backend.set_session(tf.Session(config=config))\r\n```\r\n\r\nThis can slow down your model evaluation if not used together with the items above, presumably since the large data set will have to be swapped in and out to fit into the small amount of memory you've allocated.\r\n\r\n### 3. You have incompatible versions of CUDA, TensorFlow, NVIDIA drivers, etc.\r\nIf you've never had similar models working, you're not running out of VRAM and your cache is clean, I'd go back and set up CUDA + TensorFlow using the best available installation guide - I have had the most success with following the instructions at https://www.tensorflow.org/install/gpu rather than those on the NVIDIA / CUDA site. Lambda Stack: https://lambdalabs.com/lambda-stack-deep-learning-software is also a good way to go.\r\n\r\n### 4. While using Keras, Keras layers(classes) were directly imported from keras instead of tensorflow.keras \r\n\r\nKeras is included in TensorFlow 2.0 above. So\r\n\r\nremove ``` import keras ``` and\r\nreplace ``` from keras.module.module import class ``` statement to --> from ``` tensorflow.keras.module.module import class ```\r\n\r\nFor example\r\nReplace \r\n``` from keras.layers import Conv3D,ConvLSTM2D,Conv3DTranspose, Input```\r\nwith this:\r\n``` from tensorflow.keras.layers import Conv3D,ConvLSTM2D,Conv3DTranspose, Input ```\r\n\r\nMaybe your GPU memory is filled. So use allow ``` growth = True ``` in GPU option. This is deprecated now. But use this below code snippet after imports may solve your problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.compat.v1.keras.backend import set_session\r\n\r\n\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\nconfig.log_device_placement = True  # to log device placement (on which device the operation ran)\r\nsess = tf.compat.v1.Session(config=config)\r\nset_session(sess)\r\nprint('\\nTensorflow GPU installed: '+str(tf.test.is_built_with_cuda()))\r\nprint('Is Tensorflow using GPU: \\n'+str(tf.test.is_gpu_available()))\r\n```\r\n### If the above code does not resolve the issue then try the following code, and put the following write after imports:\r\n```\r\nimport tensorflow as tf\r\n gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    #The variable GB is the memory size you want to use.\r\n    config = [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=(1024*GB))]\r\n    if gpus:\r\n      # Restrict TensorFlow to only allocate 1*X GB of memory on the first GPU\r\n      try:\r\n        tf.config.experimental.set_virtual_device_configuration(gpus[0], config)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n      except RuntimeError as e:\r\n        # Virtual devices must be set before GPUs have been initialized\r\n        print(e)\r\n``` \r\n\r\nIn the above code at line#4 the Variable GB represents the memory you want to to use from your total memory of GPU. It is recommended to use at least 1 GB less than your total GPU Memory size. If still problem exist try lowering the value of the Variable GB. Eventually it will work.", "@RobKwiatkowski Please try one of them : https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-609426655\r\n\r\nI hope this will solve your issue. And please let me know whether it solved your issue or not.", "@irdanish11, thank You for trying to help, but I think we have slightly different reasons for this issue. As for me, none of the cases you've listed are relevant: \r\n1) this issue come within just started container (no cache);\r\n2) this issue comes with even simple model (i.e. one conv layer with one filter) - so, it cannot be out of memory error at all;\r\n3) everything within an official docker image is compatible (i.e. CUDA, cuDNN, tf). Working around nvidia drivers on the host machine doesn't help at all;\r\n4) always import from tf.keras - dose not matter.\r\n\r\nAnyway, thank you!", "I use a new conda create --name myenv python=3.XX...\r\nI install tf-gpu=1.15.0 and keras=2.24,\r\nAnd I plus \r\n```\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.compat.v1.InteractiveSession(config=config)\r\n```\r\n\r\nFinally,I can run.\r\nI don't know why....\r\n ", "```\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.compat.v1.InteractiveSession(config=config)\r\n```\r\n\r\nthis works (one can find this snippet everywhere), but it is a hack, not a solution.", "for people getting this issue on an AWS notebook instance I found that using a conda_amazonei_tensorflow_p36 instead of a conda_tensorflow_p36 kernel solves it.", "I had this issue on a machine with multiple available GPUs, some of which were in use. I restricted my model to only use a single GPU that was not in use via masking away the rest before running my script; set the following environment variable\r\n\r\nCUDA_VISIBLE_DEVICES= (some GPU id)\r\n\r\nSeems to work for me, good luck!", "> @RobKwiatkowski Please try one of them : [#24828 (comment)](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-609426655)\r\n\r\nThe last part solved my issue. I dont quite understand what units the memory is in but I figured out how to make it run with a little experimentation. are the GB in SI units or are they in binary units? it looks like they are in Binary MiB units when looking at the output from the invidia-smi command but the math doesnt seem to work when I try to allocate my memory using that conversion. \r\n\r\nmore clearly- it looks like I have 7982MiB from invidia-smi. which would be 7982x(2^10)^2 which equals 8.37x10^9 bytes right? but in python, when I try to allocate even something as low as 7.0x10^4 I get an error and it exceeds my GPU memory.\r\n\r\n` [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=(7e4))]`\r\n\r\nrunning the above code exceeds my GPU memory when I check on invidia-smi. How is this possible? Am I not converting properly from Mib to bytes? ", "I am experiencing this issue when trying to execute my code on an AWS SageMaker ml.p2.xlarge instance.\r\n- TensorFlow 2.1.0\r\n- Cuda version 10.0.13 (from nvcc --version)\r\n- Python 3.6.1.\r\n\r\nI have tried the allow_growth=True suggestions which did not solve the issue. I also tried the conda_amazonei_tensorflow_p36 notebook instance that @raynaa-75 suggested but it is not compatible with some of my dependencies such as tensorflow_federated. Would anyone have any additional recommendations for potential solutions for the error?", "This solved the error for me:\r\nhttps://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in\r\n\r\nadjusting the IMAGES_PER_GPU  in the config file to a lower number solved my problem\r\n\r\nyou could also try to modify STEPS_PER_EPOCH, IMAGE_MAX_DIM, TRAIN_ROIS_PER_IMAGE or whatever other hyper-parameters which consume your GPU.\r\n\r\nAnother option is to choose a more powerful machine if you are working in the cloud", "It was an error caused by the CUDA version (10.2). I used this bash script to change the cuda version of the instance to 10.1 and it worked. \r\nhttps://github.com/phohenecker/switch-cuda\r\n", "Wrote my own custom layer for conv and that finally worked.", "This worked for me with tensorflow 2.1 and doesn't depend on tf.compat:\r\n`physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nfor physical_device in physical_devices:\r\n    tf.config.experimental.set_memory_growth(physical_device, True)`\r\n\r\nVideo Card: Nvidia GT 1030", "Was getting this with tf2.2.0rc4 on ubuntu 20.04 and py3.7, with an nvidia rtx 2070 (440 driver, cuda toolkit 10.1 most recent upgrade). Switching to python 3.8 remedied the issue.", "I was using the tensorflow docker container `tensorflow/tensorflow:latest-gpu-py3-jupyter` on an Nvidia RTX 2080 Super and\r\n```python\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU') \r\nfor physical_device in physical_devices: \r\n    tf.config.experimental.set_memory_growth(physical_device, True)\r\n```\r\nworked for me", "> I was using the tensorflow docker container `tensorflow/tensorflow:latest-gpu-py3-jupyter` on an Nvidia RTX 2080 Super and\r\n> \r\n> ```python\r\n> physical_devices = tf.config.experimental.list_physical_devices('GPU') \r\n> for physical_device in physical_devices: \r\n>     tf.config.experimental.set_memory_growth(physical_device, True)\r\n> ```\r\n> \r\n> worked for me\r\n\r\nit works thank you :)", "> I was using the tensorflow docker container `tensorflow/tensorflow:latest-gpu-py3-jupyter` on an Nvidia RTX 2080 Super and\r\n> \r\n> ```python\r\n> physical_devices = tf.config.experimental.list_physical_devices('GPU') \r\n> for physical_device in physical_devices: \r\n>     tf.config.experimental.set_memory_growth(physical_device, True)\r\n> ```\r\n> \r\n> worked for me\r\n\r\nWorks fine on tensorflow==2.2.0", "> This worked for me with tensorflow 2.1 and doesn't depend on tf.compat:\r\n> `physical_devices = tf.config.experimental.list_physical_devices('GPU') for physical_device in physical_devices: tf.config.experimental.set_memory_growth(physical_device, True)`\r\n> \r\n> Video Card: Nvidia GT 1030\r\n\r\nThank you very much, solved all my problems :)", "> > Did you try setting up allow_growth = True? That resolved the problem for me.\r\n> \r\n> Yes, it helps!\r\n> Thanks.\r\n> @aishwaryap\r\n> You can try setting up allow_growth:\r\n> \r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> sess = tf.Session(config=config)\r\n\r\nThis solution worked for me!! Thanks a lot!", "Did anyone use tensorflow 1.   success \uff1f\r\nI use this code \r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\nBut it doesn\u2019t work.\r\n", "```python\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU') \r\nfor physical_device in physical_devices: \r\n    tf.config.experimental.set_memory_growth(physical_device, True)\r\n```\r\nAlso worked for me on tensorflow 2.2.0. \r\n\r\nNow I just need to figure out the last error (3rd in a row):\r\n```ssh\r\ntype tensorflow.python.framework.ops.EagerTensor doesn't define __round__ method\r\n```\r\n\r\nedit: ok everything works now yay", "   I solved the problem by **uninstall tensorflow-gpu**  from conda and pip environment .  And use **conda to install tensorflow-gpu=1.14  conda install keras=2.2.5**.  The dependencies will also be installed. But there is still problem,  then  just tf-gpu and keras  are removed from conda env, by usage \"conda   remove --force tensorflow-gpu=1.14  \".  After that , using pip install tensorflow-gpu==1.14  and  keras==2.2.5.  If  raise error \" there is no module of tensorflow\" \uff0cusing pip  install tf-gpu again . Then the problem solved.", "I was facing the same issue recently. I have an RTX 2060 and CUDA 10.2, OS: Ubuntu.\r\n\r\nI installed tf-gpu 2.2 using conda. See image for cudnn and Cuda version.\r\n![nvidia-cuda-cudnn](https://user-images.githubusercontent.com/65457337/84581611-68768280-ade3-11ea-83c2-51e99e82a6d9.png)\r\n\r\nFor me, this worked:\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\n    #The variable GB is the memory size you want to use.\r\n    config = [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=(1024*GB))]\r\n    if gpus:\r\n      # Restrict TensorFlow to only allocate 1*X GB of memory on the first GPU\r\n      try:\r\n        tf.config.experimental.set_virtual_device_configuration(gpus[0], config)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n      except RuntimeError as e:\r\n        # Virtual devices must be set before GPUs have been initialized\r\n        print(e)\r\n\r\nThanks @irdanish11 \r\nlink to comment: https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-609426655\r\n\r\ni have two envs and the above method also worked on:\r\n![tf1](https://user-images.githubusercontent.com/65457337/84582087-a7a6d280-ade7-11ea-8df8-756bb2ff4ebc.png)\r\n\r\n\r\n\r\n_**> I also tried:\r\n>      config = tf.compat.v1.ConfigProto()\r\n>      config.gpu_options.allow_growth = True\r\n>      session = tf.compat.v1.InteractiveSession(config=config)\r\n> \r\n> but it didn't work.**_\r\n\r\n\r\nwhat also worked is:\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU') \r\nfor physical_device in physical_devices: \r\n    tf.config.experimental.set_memory_growth(physical_device, True)\r\n\r\n\r\n\r\n", "I am also facing this issue:\r\n\r\n> (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n> \t [[node sequential/conv1d/conv1d (defined at <ipython-input-13-75dbd6bef7ba>:1) ]]\r\n> \t [[gradient_tape/sequential/embed/embedding_lookup/Reshape/_40]]\r\n\r\nUnfortunetely I have tried both setting memory growth:\r\n\r\n> physical_devices = tf.config.experimental.list_physical_devices('GPU') \r\n> for physical_device in physical_devices: \r\n>     tf.config.experimental.set_memory_growth(physical_device, True)\r\n\r\nand limiting GPU memory allocation:\r\n\r\n> [gpus = tf.config.experimental.list_physical_devices('GPU')\r\n> if gpus:\r\n>     # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\r\n>     try:\r\n>         tf.config.experimental.set_virtual_device_configuration(\r\n>             gpus[0],\r\n>             [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 2)])\r\n>         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n>         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n>     except RuntimeError as e:\r\n>         # Virtual devices must be set before GPUs have been initialized\r\n>         print(e)](url)\r\n\r\nI'm not really sure if the latter worked at all though, as after hitting the error `nvidia-smi` returns the following processes:\r\n\r\n> | NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n> |-------------------------------+----------------------+----------------------+\r\n> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n> |===============================+======================+======================|\r\n> |   0  GeForce RTX 2060    Off  | 00000000:01:00.0 Off |                  N/A |\r\n> | N/A   59C    P2    28W /  N/A |   5925MiB /  5934MiB |      3%      Default |\r\n> +-------------------------------+----------------------+----------------------+\r\n>                                                                                \r\n> +-----------------------------------------------------------------------------+\r\n> | Processes:                                                       GPU Memory |\r\n> |  GPU       PID   Type   Process name                             Usage      |\r\n> |=============================================================================|\r\n> |    0      1165      G   /usr/lib/xorg/Xorg                            28MiB |\r\n> |    0      1329      G   /usr/bin/gnome-shell                          51MiB |\r\n> |    0      1581      G   /usr/lib/xorg/Xorg                           152MiB |\r\n> |    0      1755      G   /usr/bin/gnome-shell                         140MiB |\r\n> |    0      2219      G   /usr/lib/firefox/firefox                       3MiB |\r\n> |    0      6942      G   /usr/lib/firefox/firefox                       3MiB |\r\n> |    0      7295      G   /usr/lib/firefox/firefox                       3MiB |\r\n> |    0     14435      G   /usr/lib/firefox/firefox                       3MiB |\r\n> |    0     22390      C   /home/kuba/anaconda3/envs/tf2.1/bin/python  5531MiB |\r\n> +-----------------------------------------------------------------------------+\r\n\r\nI installed Tensorflow using `conda install tensorflow-gpu` and my setup is as follows:\r\n\r\n> _# Name                    Version                   Build  Channel\r\n> cudnn                     7.6.5                cuda10.1_0_  \r\n>  _# Name                    Version                   Build  Channel\r\n> cudatoolkit               10.1.243             h6bb024c_0  \r\n\r\nI've tried both with tensorflow 2.2 and 2.1, but the issue occurs no matter of the tf version.", "@KubaMichalczyk Your GPU is already in use i think. Does the training start or it does not start at all? You can also download any of the following two tools to monitor your gpu better:\r\nhttps://www.linuxuprising.com/2019/06/2-tools-for-monitoring-nvidia-gpus-on.html\r\n\r\nIt would be good to know if your training started or it returned some error.", "@vkyprmr So here's the interesting thing: the training starts and crashes just after first batch with tensorflow 2.1, but doesn' seem to start with tensorflow 2.2. I attach full error message from the latter:\r\n\r\n> \r\n> Epoch 1/20\r\n> \r\n> ---------------------------------------------------------------------------\r\n> UnknownError                              Traceback (most recent call last)\r\n> <ipython-input-12-75dbd6bef7ba> in <module>\r\n> ----> 1 history = model.fit(X_train, Y_train,\r\n>       2                     epochs=20,\r\n>       3                     batch_size=128,\r\n>       4                     validation_split=0.2,\r\n>       5                     callbacks=callbacks)\r\n> \r\n> ~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n>      64   def _method_wrapper(self, *args, **kwargs):\r\n>      65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n> ---> 66       return method(self, *args, **kwargs)\r\n>      67 \r\n>      68     # Running inside `run_distribute_coordinator` already.\r\n> \r\n> ~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n>     846                 batch_size=batch_size):\r\n>     847               callbacks.on_train_batch_begin(step)\r\n> --> 848               tmp_logs = train_function(iterator)\r\n>     849               # Catch OutOfRangeError for Datasets of unknown size.\r\n>     850               # This blocks until the batch has finished executing.\r\n> \r\n> ~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n>     578         xla_context.Exit()\r\n>     579     else:\r\n> --> 580       result = self._call(*args, **kwds)\r\n>     581 \r\n>     582     if tracing_count == self._get_tracing_count():\r\n> \r\n> ~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n>     642         # Lifting succeeded, so variables are initialized and we can run the\r\n>     643         # stateless function.\r\n> --> 644         return self._stateless_fn(*args, **kwds)\r\n>     645     else:\r\n>     646       canon_args, canon_kwds = \\\r\n> \r\n> ~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n>    2418     with self._lock:\r\n>    2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n> -> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n>    2421 \r\n>    2422   @property\r\n> \r\n> ~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n>    1659       `args` and `kwargs`.\r\n>    1660     \"\"\"\r\n> -> 1661     return self._call_flat(\r\n>    1662         (t for t in nest.flatten((args, kwargs), expand_composites=True)\r\n>    1663          if isinstance(t, (ops.Tensor,\r\n> \r\n> ~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n>    1743         and executing_eagerly):\r\n>    1744       # No tape is watching; skip to running the function.\r\n> -> 1745       return self._build_call_outputs(self._inference_function.call(\r\n>    1746           ctx, args, cancellation_manager=cancellation_manager))\r\n>    1747     forward_backward = self._select_forward_and_backward_functions(\r\n> \r\n> ~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n>     591       with _InterpolateFunctionError(self):\r\n>     592         if cancellation_manager is None:\r\n> --> 593           outputs = execute.execute(\r\n>     594               str(self.signature.name),\r\n>     595               num_outputs=self._num_outputs,\r\n> \r\n> ~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n>      57   try:\r\n>      58     ctx.ensure_initialized()\r\n> ---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n>      60                                         inputs, attrs, num_outputs)\r\n>      61   except core._NotOkStatusException as e:\r\n> \r\n> UnknownError: 2 root error(s) found.\r\n>   (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n> \t [[node sequential/conv1d/conv1d (defined at <ipython-input-12-75dbd6bef7ba>:1) ]]\r\n> \t [[gradient_tape/sequential/embed/embedding_lookup/Reshape/_40]]\r\n>   (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n> \t [[node sequential/conv1d/conv1d (defined at <ipython-input-12-75dbd6bef7ba>:1) ]]\r\n> 0 successful operations.\r\n> 0 derived errors ignored. [Op:__inference_train_function_1137]\r\n> \r\n> Function call stack:\r\n> train_function -> train_function", "@KubaMichalczyk \r\nDid you try reducing the batch size to 32???\r\n\r\nDid you install tensorflow with pip or conda?\r\n", "@vkyprmr Yes, I have tried with lower batch size as well. The error pops out always after first batch with tensorflow 2.1 and even before first batch with tensorflow 2.2. I installed it with conda.", "@KubaMichalczyk can you share your entire code?", "@vkyprmr sure, here it is: https://gist.github.com/KubaMichalczyk/0d01e2378edab0f3680e1de6fbcf30aa", "@KubaMichalczyk are you using Ubuntu? If so, perhaps another option is to try it out in Windows. I made a basic model, and it worked better in Windows than Ubuntu. Only drawback, Ubuntu has tf 2.2 whereas windows has 2.1", "Yes, I'm on Ubuntu, and I'm quite reluctant to switching to Windows. Thanks for suggestion though!", "@KubaMichalczyk  did you solve the issue with the \"Error: failed to get convolution algorithm\"? I think you might have to downgrade from 2.1 as i'm getting the same issue on windows with tf 2.1", "@KubaMichalczyk and @shini-tm can you guys may be give me your code and an idea about your data, maybe i can run it on my pc and let you guys know if it works on mine or not.", "I managed to not get the error when installing tensorflow 2.0 in a virtual environment. @vkyprmr its just this line that spits the error: pred_img = new_model.predict(test_img) in 2.1", "@vkyprmr You can easily run my code attached earlier as a gist - the data used are located in `tensorflow.keras.imdb`, so you should have it already available with your tf installation.\r\n@shini-tm No, I did not", "with me the training worked on your code @KubaMichalczyk \r\nI tried to run it dynamically, with allow_memory_growth=True and i got the following output:\r\n![dynamic](https://user-images.githubusercontent.com/65457337/85116110-1a1b1680-b21d-11ea-94e3-c8d49d1d181e.png)\r\n\r\n\r\nThen I tried to run it with static or given amount of GPU memory:\r\n![staticmode](https://user-images.githubusercontent.com/65457337/85116154-2d2de680-b21d-11ea-8651-198b70cb9624.png)\r\n\r\n\r\nBoth the times the training started and i terminated it myself. \r\n\r\nTensorflow version: 2.1.0 on Windows.\r\nUnfortunately I have uninstalled Ubuntu, but I am sure it would have run on Ubuntu as well.\r\n\r\nI just added a few lines of code to yours which included argparse, to run it with different batchsize and epochs from the command line/terminal. \r\n\r\nFYI: I ran it with a batch size of 64.\r\n\r\nHere is the code that i added or the changes i made in your code:\r\n`\r\nimport argparse\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.datasets import imdb\r\nfrom tensorflow.keras.preprocessing import sequence\r\n\r\n#%%\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"-gm\", \"--gpumemory\",dest = \"gpu_memory\", help=\"GPU Memory to use\")\r\nparser.add_argument(\"-m\", \"--mode\",dest = \"mode\", help=\"Mode: 'static':'s' or 'dynamic':'d'\")\r\nparser.add_argument(\"-bs\", \"--batchsize\",dest = \"batch_size\", help=\"Batch size\")\r\nparser.add_argument(\"-e\", \"--epochs\",dest = \"epochs\", help=\"Epochs\")\r\n\r\n\r\nargs = parser.parse_args()\r\n\r\nbatch_size = int(args.batch_size)\r\nmode = args.mode.lower()\r\nepochs = int(args.epochs)\r\n\r\n\r\n\r\n#%%\r\n\r\nif mode=='s' or mode=='static':\r\n    gpu_mem = int(args.gpu_memory)\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    #The variable GB is the memory size you want to use.\r\n    try:\r\n        config = [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=(1024*gpu_mem))]\r\n        if gpus:\r\n            # Restrict TensorFlow to only allocate 1*X GB of memory on the first GPU\r\n            try:\r\n                tf.config.experimental.set_virtual_device_configuration(gpus[0], config)\r\n                logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n                print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n            except RuntimeError as e:\r\n                # Virtual devices must be set before GPUs have been initialized\r\n                print(e)\r\n    except:\r\n        print('Static mode selected but no memory limit set. Please set a memory limit by adding the flag -gm=X (gb) or --gpumemory=x (gb) after -m=s or --memory=s')\r\n        quit()\r\nelse:\r\n    physical_devices = tf.config.experimental.list_physical_devices('GPU') \r\n    for physical_device in physical_devices: \r\n        tf.config.experimental.set_memory_growth(physical_device, True)`\r\n\r\nand at the end:\r\n`history = model.fit(X_train, Y_train,\r\n                    epochs=epochs,\r\n                    batch_size=batch_size,\r\n                    validation_split=0.2,\r\n                    callbacks=callbacks)`", "@shini-tm Sorry I did not fully understand you", "> @irdanish11, thank You for trying to help, but I think we have slightly different reasons for this issue. As for me, none of the cases you've listed are relevant:\r\n> \r\n> 1. this issue come within just started container (no cache);\r\n> 2. this issue comes with even simple model (i.e. one conv layer with one filter) - so, it cannot be out of memory error at all;\r\n> 3. everything within an official docker image is compatible (i.e. CUDA, cuDNN, tf). Working around nvidia drivers on the host machine doesn't help at all;\r\n> 4. always import from tf.keras - dose not matter.\r\n> \r\n> Anyway, thank you!\r\nIn reference: [609607243](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-609607243) pryadchenko\r\n@pryadchenko I agree with you on this point that you cannot have memory issues when you  have simple model (i.e. one conv layer with one filter), In my [original comment](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-609426655) I have represented a comparison between GTX1060 & GTX1660 Ti, both have same amount of memory the code worked fine on GTX1060 but gives the error on GTX1660 Ti, but I use the second solution it get solved. I also got the error on simplest Convnet  (i.e 1 conv layer and 1 filter) but with adding the code lines from 2nd solution solved the issue. I don't know what is the problem maybe there is an issue with tensorflow build for newer GPU's e.g GTX16 series.", "restarting the computer solved my problem :)", "@vkyprmr Finally, I managed to get it worked, firstly by running your code from terminal and then checking what's different in my jupyter notebook. The only difference I have found was checking if tensorflow is even available with `tf.test.gpu_device_name()`. It's super weird, but everytime I run a kernel with this line at the beginning cuDNN fails to initialise, but the problem is absent when I comment this line, restart the kernel and run again. Maybe it's something that `tf.test.gpu_device_name()` does under the hood? I also updated my nvidia drivers from 440.082 to 440.100, not sure if that helped.", "@KubaMichalczyk We had the same issue with this line (`tf.test.gpu_device_name()`). The problem was that calling it with `allow_memory_growth=True` was working fine, but without `allow_memory_growth=True` then the program would crash down the road with the cuDNN error.\r\n\r\nI really don't understand how a simple check `tf.test.gpu_device_name()` which is not supposed to modify **anything** under the hoods can cause such a big difference in behavior...", "> @vkyprmr Finally, I managed to get it worked, firstly by running your code from terminal and then checking what's different in my jupyter notebook. The only difference I have found was checking if tensorflow is even available with `tf.test.gpu_device_name()`. It's super weird, but everytime I run a kernel with this line at the beginning cuDNN fails to initialise, but the problem is absent when I comment this line, restart the kernel and run again. Maybe it's something that `tf.test.gpu_device_name()` does under the hood? I also updated my nvidia drivers from 440.082 to 440.100, not sure if that helped.\r\n\r\n@KubaMichalczyk I am glad that the issue is resolved. To be honest, I would never have figured that out myself. It is a really weird. Thanks for sharing your experience and knowledge.", "Epoch 1/15\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-11-01c6f78f4d4f> in <module>\r\n      4     epochs=epochs,\r\n      5     validation_data=val_data_gen,\r\n----> 6     validation_steps=total_val // batch_size\r\n      7 )\r\n\r\n~/miniconda3/envs/handn/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    322               'in a future version' if date is None else ('after %s' % date),\r\n    323               instructions)\r\n--> 324       return func(*args, **kwargs)\r\n    325     return tf_decorator.make_decorator(\r\n    326         func, new_func, 'deprecated',\r\n\r\n~/miniconda3/envs/handn/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1477         use_multiprocessing=use_multiprocessing,\r\n   1478         shuffle=shuffle,\r\n-> 1479         initial_epoch=initial_epoch)\r\n   1480 \r\n   1481   @deprecation.deprecated(\r\n\r\n~/miniconda3/envs/handn/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n~/miniconda3/envs/handn/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    846                 batch_size=batch_size):\r\n    847               callbacks.on_train_batch_begin(step)\r\n--> 848               tmp_logs = train_function(iterator)\r\n    849               # Catch OutOfRangeError for Datasets of unknown size.\r\n    850               # This blocks until the batch has finished executing.\r\n\r\n~/miniconda3/envs/handn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n~/miniconda3/envs/handn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    609       # In this case we have created variables on the first call, so we run the\r\n    610       # defunned version which is guaranteed to never create variables.\r\n--> 611       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    612     elif self._stateful_fn is not None:\r\n    613       # Release the lock early so that multiple threads can perform the call\r\n\r\n~/miniconda3/envs/handn/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2418     with self._lock:\r\n   2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2421 \r\n   2422   @property\r\n\r\n~/miniconda3/envs/handn/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1663          if isinstance(t, (ops.Tensor,\r\n   1664                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1665         self.captured_inputs)\r\n   1666 \r\n   1667   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~/miniconda3/envs/handn/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1744       # No tape is watching; skip to running the function.\r\n   1745       return self._build_call_outputs(self._inference_function.call(\r\n-> 1746           ctx, args, cancellation_manager=cancellation_manager))\r\n   1747     forward_backward = self._select_forward_and_backward_functions(\r\n   1748         args,\r\n\r\n~/miniconda3/envs/handn/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    596               inputs=args,\r\n    597               attrs=attrs,\r\n--> 598               ctx=ctx)\r\n    599         else:\r\n    600           outputs = execute.execute_with_cancellation(\r\n\r\n~/miniconda3/envs/handn/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-9-01c6f78f4d4f>:6) ]] [Op:__inference_train_function_933]\r\n\r\nFunction call stack:\r\ntrain_function\r\n", "Unable to get past:\r\n`UnknownError: 2 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.`\r\n\r\nFull output at: https://gist.github.com/somyamohanty/b5dddf0952e9931b5ad852c2a7ca5d1c\r\n**Config:**\r\nUbuntu 16.04\r\nCuda: 10.1\r\nDriver: 418.87.01\r\nCuddn: 7.5.4\r\nTensorflow: 2.2 (tried with 2.0)\r\nGPU: 1080Ti\r\n\r\nUsing the gist (provided by @KubaMichalczyk ). Tried most of the solutions given here. The only thing that works is downgrading tensorflow to 1.8. Looking at the GPU utilization, it does not reach the max memory utilization. Not sure whats going on. ", "even though mentioned cuDNN failed to initialize, it is not necessarily a version mismatch problem\r\nif you tried methods above and did not work\uff0cit is probably because GPU memory is exhausted\r\n`os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\"`\r\njust use CPU \uff1a\uff09", "Hi, it should be somehow cudnn and cuda incompatibility. Using this to solve\r\n`pip uninstall tensorflow-gpu`\r\n`conda install tensorflow-gpu==2.1.0`\r\n\r\nIt works for me", "> I solved the problem by **uninstall tensorflow-gpu** from conda and pip environment . And use **conda to install tensorflow-gpu=1.14 conda install keras=2.2.5**. The dependencies will also be installed. But there is still problem, then just tf-gpu and keras are removed from conda env, by usage \"conda remove --force tensorflow-gpu=1.14 \". After that , using pip install tensorflow-gpu==1.14 and keras==2.2.5. If raise error \" there is no module of tensorflow\" \uff0cusing pip install tf-gpu again . Then the problem solved.\r\n\r\n\r\n\r\n> ```python\r\n> physical_devices = tf.config.experimental.list_physical_devices('GPU') \r\n> for physical_device in physical_devices: \r\n>     tf.config.experimental.set_memory_growth(physical_device, True)\r\n> ```\r\n> \r\n> Also worked for me on tensorflow 2.2.0.\r\n> \r\n> Now I just need to figure out the last error (3rd in a row):\r\n> \r\n> ```\r\n> type tensorflow.python.framework.ops.EagerTensor doesn't define __round__ method\r\n> ```\r\n> \r\n> edit: ok everything works now yay\r\n\r\nThis works for me using the official Docker Tensorflow-gpu-latest in Ubuntu 20.4 and GTX1060Ti", "I faced the same problem and it got resolved by updating the Nvidia drivers to latest version 451.48\r\nFollowing is my configuration:\r\nWindows 10\r\n# Name                    Version                   Build  Channel\r\ncudnn                     7.6.5                cuda10.0_0\r\ncudatoolkit               10.0.130                      0    anaconda\r\ntf.__version__ = 2.0.0\r\n", "> Hi, it should be somehow cudnn and cuda incompatibility. Using this to solve\r\n> `pip uninstall tensorflow-gpu`\r\n> `conda install tensorflow-gpu==2.1.0`\r\n> \r\n> It works for me\r\n\r\n\r\n\r\n> Hi, it should be somehow cudnn and cuda incompatibility. Using this to solve\r\n> `pip uninstall tensorflow-gpu`\r\n> `conda install tensorflow-gpu==2.1.0`\r\n> \r\n> It works for me\r\n\r\nListen man this works", "> > Hi, it should be somehow cudnn and cuda incompatibility. Using this to solve\r\n> > `pip uninstall tensorflow-gpu`\r\n> > `conda install tensorflow-gpu==2.1.0`\r\n> > It works for me\r\n> \r\n> Listen man this works\r\n\r\nWhat combination work?  I am still getting \"cuDNN failed to initialize\"\r\nI had a previous conda install of tensorflow 2.2 so I ran:\r\n\r\nconda remove --name tf_gpu --all\r\nconda clean --all\r\nconda create -n tf_gpu -c anaconda -y tensorflow-gpu==2.1.0\r\nconda activate tf_gpu\r\ngit clone https://github.com/tensorflow/benchmarks.git\r\ncd benchmarks/scripts/tf_cnn_benchmarks/\r\npython3 tf_cnn_benchmarks.py --num_gpus=1 --model resnet50 --batch_size 32\r\n** still get the cuDNN message **\r\n\r\nI tried to move back to tensorflow-gpu=1.8.0, but conda wouldn't let me and said not compatible with cuda version 11 thats on my system from nvidia driver install.  What combination using conda works?  Thanks!", "> OK, I was able to execute my CNN. I'm using tensorflow tf-nightly-gpu-2.0-preview, and running on a ipython notebook. I had to add this to my notebook:\r\n> \r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> \r\n> [Here](https://www.tensorflow.org/guide/using_gpu) are some more details\r\n> \r\n> Also, this issue is associated with [24496] (#24496)\r\n\r\nThank you so much this really helped\r\n", "> \r\n> \r\n> > Did you try setting up allow_growth = True? That resolved the problem for me.\r\n> \r\n> Yes, it helps!\r\n> Thanks.\r\n> @aishwaryap\r\n> You can try setting up allow_growth:\r\n> \r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> sess = tf.Session(config=config)\r\n\r\nThis resolved my issue as well, but I think a permanent solution should be implemented. ", "To everyone here: TF 2.3 (compiled from source with CUDA 11 and cudnn 8.0) solved this issue for me, no need for any workaround now :)", "This problem also in general can happen if there is not enough VRAM available for the `model.fit()`. This cryptic exception happens instead of the common OOM error when using convolutional layers. (Using TF 2.2 on Linux) Allowing growth does not help in this case, if no VRAM is available.", "> OK, I was able to execute my CNN. I'm using tensorflow tf-nightly-gpu-2.0-preview, and running on a ipython notebook. I had to add this to my notebook:\r\n> \r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> \r\n> [Here](https://www.tensorflow.org/guide/using_gpu) are some more details\r\n> \r\n> Also, this issue is associated with [24496] (#24496)\r\n\r\nUsing Tf 2.1.0 CUDA 11.0 CuDNN 7.6.4 and using python 2.7 \r\nHad the same issue. Fixed it using the above fix.", "> OK, I was able to execute my CNN. I'm using tensorflow tf-nightly-gpu-2.0-preview, and running on a ipython notebook. I had to add this to my notebook:\r\n> \r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> \r\n> [Here](https://www.tensorflow.org/guide/using_gpu) are some more details\r\n> \r\n> Also, this issue is associated with [24496] (#24496)\r\n\r\nyeah, it's ok, but it only used half video memory", "This worked for me\r\nFor Linux:\r\nSet the `TF_FORCE_GPU_ALLOW_GROWTH` environment variable to `true`.\r\nIn your terminal, run this command.\r\n`$ export TF_FORCE_GPU_ALLOW_GROWTH=true` ", "it was a very horrible problem, I fixed by upgrading my cudnn to the version that corresponds to cuda10.0. \r\n\r\nI created a new conda environment with tf-gpu=1.15.0 (this ensures compatibility) then removed tf-gpu=1.15.0 from this environment, then installed tf-gpu=1.15.2 (which I was interested in). \r\n\r\nhope it helps some others. ", "I had the same problem in Ubuntu 16.04 with Nvidia 418 , Tensorflow-GPU 1.15 , Cuda 10.1, cuDNN 7.6.5.\r\nI downgraded the cuDNN to 7.6.4 with the following command and now it works!\r\n\r\n`sudo apt-get install  libcudnn7=7.6.4.38-1+cuda10.1  \\\r\n    libcudnn7-dev=7.6.4.38-1+cuda10.1`\r\n ", "I faced this problem on RTX2070 super while training efficientdet_d0. Reducing the batch size for training worked for me. (TF2.0 repo .config file had training size 64 I changed it to 4)\r\nI earlier faced the same problem and limit growth rate worked.\r\n'config = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)'\r\nps- I am using opensource NVIDIA drivers.", "I have a RTX 2070 and GTX 1060 running with a batch size of 16 that seems to work okay using proprietary Nvidia drivers\r\n", "> Same issue here. I have an RTX 2070, cuda 10, cudnn 7.4.1 and tensorflow 2.0 running on ubuntu 18.04. Downgraded cudnn to 7.3.0 but still same error. I see it helped for some people downgrading tensorflow but I guess that's not an option for me. Any help is much appreciated.\r\n\r\nHi @oscarlinux ,\r\nWere you able to solve the issue? I have similar situation and my environment setup is as thus below:\r\nSoftware Specs:\r\n Nvidia Geforce Experience Driver 418.96\r\n CudNN 10.1 (windows 7 version)\r\n Cuda 10.1.243\r\n Conda Python 3.7.4\r\n Tensorflow 2.1.0 \r\n Keras 2.3.1\r\n\r\nPC Specs:\r\n Windows 7 (64 bits)\r\n GTX 1080\r\n 48GB Intel i7-6700 (3.4GHz) \r\n\r\nPlease let me know if you were able to get a solution to the issue.\r\n\r\n\r\n", "> I also have the same error with TF 1.12,1.11, and I have Cuda 9.0, and cuDnn 7.3.1, 7.4.2. Sometimes it works but sometimes not, what is causing this error to happen. Did anyone solve this error?\r\n\r\nThis happens as your GPU memory runs out of memory.", "I solved the problem last week. This happens as the GPU memory runs out of memory.\r\nAppend the snippet below to your code; it works fine for me since then, at least.\r\n\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\nconfig = tf.compat.v1.ConfigProto(allow_soft_placement=True)\r\n\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.3\r\ntf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\r\n\r\n\r\nPS: the print line is not important!", "This happened to me when someone else on the same computer was performing training. I didn't know. When the training stopped the GPU was not used at all and then code executed without any issue.\r\n\r\nI believe it has to do something with GPU sharing.\r\n\r\nMy packages were \r\n```\r\n# Name                    Version                   Build  Channel\r\ntensorflow-estimator      2.3.0                    pypi_0    pypi\r\ntensorflow-gpu            2.3.0                    pypi_0    pypi\r\ncudnn                     7.6.5                cuda10.1_0 \r\n```", "I have the same issue with the following configuration : \r\nNvidia Quadro RTX3000\r\nCuda 10.1\r\nCudnn 7.6.4.38", "Same issue. I can vaguely workaround if I:\r\n\r\n1. `pip uninstall` both tensorflow and tensorflow-gpu\r\n2. `pip cache purge`\r\n3. go to the folder .../python38/Lib/Site-packages and delete tensorflow folders\r\n4. fresh `pip install tensorflow --no-cache-dir`\r\n\r\nBut that only works one time and then I have to repeat the whole process again... :(", "Check to confirm that your cuDNN version matches with CUDA and tensorflow versions. I think that's the reason. You may also consider re-installing as suggested but it didn't work for me until I fix the version issues.", "Was facing the same issue, I resolved it by reducing the batch size.", "> \r\n> \r\n> Check to confirm that your cuDNN version matches with CUDA and tensorflow versions. I think that's the reason. You may also consider re-installing as suggested but it didn't work for me until I fix the version issues.\r\n\r\nI just updated everything (CUDA + cuDNN + TF) to the latest version. It's all the same...", "It doesn't have to be the latest version for all. You need to check the matching version. For instance, CudNN 10.1 (windows 7 version); Cuda 10.1.243; and Tensorflow 2.1.0 were the versions that matched in my case. \r\n\r\nPS: the OS is also a factor to consider.", "There is a problem then, as Tensorflow [recommends](https://www.tensorflow.org/install/source_windows#gpu) a combination of cuDNN 7.4 and CUDA 10.1 but the cuDNN [archive](https://developer.nvidia.com/rdp/cudnn-archive) only has cuDNN 7.4 compatibility up to CUDA 10.0...\r\n\r\nEDIT: Meanwhile I'm using python 3.7 to fulfill the upper-mentioned criteria and **this seems to work so far!** (thank you @tsorewilly)", "\u53ef\u80fd\u662f\u4f60\u540c\u65f6\u6253\u5f00\u4e86\u4e0d\u540c\u7248\u672c\u4e0b\u7684spyder\u548cjupyter", "Windows 10 machine with RTX 2060 SUPER. The problem doesn't appear on TF2.1 and CUDA 10.1 but it does on ```tf-nightly 2.5.0-dev20201121```, latest CUDA (11.1, 8.0.5), Python 3.8, PyCharm but, what's even more interesting, the issue doesn't appear on the latter TF-nightly & CUDA 11.1 version on GTX780M! I don't understand how it can be...", "> \r\n> \r\n> Windows 10 machine with RTX 2060 SUPER. The problem doesn't appear on TF2.1 and CUDA 10.1 but it does on `tf-nightly 2.5.0-dev20201121`, latest CUDA (11.1, 8.0.5), Python 3.8, PyCharm but, what's even more interesting, the issue doesn't appear on the latter TF-nightly & CUDA 11.1 version on GTX780M! I don't understand how it can be...\r\n\r\nBy what I experienced it is more a question of the combination of the different versions, rather than one of them alone (TF + CUDA + cuDNN)", "i love tensorflow, but the gpu compatibility problem from their beginning is still a big question why dont they work on a robust solution!! instead of new versions, i believe they should focus on making tensorflow robust and save our valuable time!\r\n\r\ni am having same problem, \r\n``` Ubuntu 18.04\r\nRTX 2070 Super ```\r\n\r\ntried tensorflow 1.14 gpu with cuda 10.1 cudnn 7.6.5 . \r\n\r\ni used this command , `conda install -c anaconda tensorflow-gpu=1.14` \r\ni am having this issue whenever i try to run a convolutional neural network. SOMEBODY HELP!", "Another surprise happened to me. Two machines with identical specs, OS, graphic card drivers etc., ```tf-nightly 2.5.0-dev20201121```, latest CUDA (11.1, 8.0.5), Python 3.8. On one computer: everything work; the other one: has problems right after the message about successfully loading cublas64_11.dll and before cublasLt64_11.dll. No idea why! Wasted 1.5 working days, uninstalled, installed everything around CUDA and Python over and over again trying each possible combination of CUDA + cuDNN, copied and pasted all required files and settings from the working machine to the other one. \r\n\r\nI gave up. Staying with TF 2.3 since it works on every machine I work on so far.", "> This worked for me\r\n> For Linux:\r\n> Set the `TF_FORCE_GPU_ALLOW_GROWTH` environment variable to `true`.\r\n> In your terminal, run this command.\r\n> `$ export TF_FORCE_GPU_ALLOW_GROWTH=true`\r\n\r\nAfter going over all the proposed solutions, this one is what solved it for me \r\nNOTE:  windows 10, cuda 10.0, cudnn 7.6", "> This error may be related to installation TF with `conda`.\r\n> \r\n> The possible solution is like this:\r\n> In the command line issue this command:\r\n> `conda list cudnn`\r\n> It will print:\r\n> ` Name Version Build Channel`\r\n> \r\n> If the result is not empty as the above, so it means you used conda installed TF, when using conda for installing TF, then it will install all the dependencies even CUDA and cuDNN, but the cuDNN version is very low for TF, so it will bring compatibility problem. So you can uninstall the cuDNN and the CUDA which was installed by conda, and then run TF, then it will work.\r\n\r\nThanks, this worked on Tensorflow-GPU installed on Ubuntu 20.04 using conda.\r\n\r\nIf you have installed Tensorflow-gpu using Conda, then install the **cudnn** and **cudatoolkit** which were installed along with it and re-run the notebook.\r\n\r\n**NOTE**: Trying to uninstall only these two packages in conda would **force** a chain of other packages to be uninstalled as well. So, use the following command to uninstall only these packages\r\n\r\n**(1)** To remove the cuda\r\n\r\n`conda remove --force cudatookit`\r\n\r\n**(2)** To remove the cudnn\r\n\r\n`conda remove --force cudnn`\r\n\r\nNow run Tensorflow, it should work!", "I tried with:\r\n\r\n1. `pip uninstall tensorflow-gpu` as my pc just installed tensorflow-gpu\r\n2. `pip cache purge`\r\n3. Deleted tensorflow folders in .../python38/Lib/Site-packages\r\n4. `pip install tensorflow-gpu --no-cache-dir`\r\n\r\nBut it raise me same error.\r\n\r\nSystem information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIndows 10 64bit\r\nTensorFlow installed from (source or binary): pip install tensorflow-gpu\r\nTensorFlow version: tensorflow-gpu 2.4.0\r\nPython version: Python 3.7\r\nInstalled using virtualenv? pip? conda?: pip\r\nCUDA/cuDNN version: CUDA11, cuDNN 8.0.5\r\nGPU model and memory: RTX3070 8GB OC\r\n\r\nThe log shown as below:\r\n```\r\n2021-01-06 18:21:30.484673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:09:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.755GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-01-06 18:21:30.485159: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-06 18:21:30.485494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-06 18:21:30.486402: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-06 18:21:30.486726: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-06 18:21:30.487637: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-06 18:21:30.487971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-06 18:21:30.488935: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-06 18:21:30.489355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-06 18:21:30.489743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-06 18:21:36.156145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-06 18:21:36.156275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-01-06 18:21:36.157045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-01-06 18:21:36.157466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6589 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:09:00.0, compute capability: 8.6)\r\n2021-01-06 18:21:36.158785: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-06 18:21:37.935914: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-01-06 18:21:38.345639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-06 18:21:44.113075: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:44.113322: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:44.114653: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:44.116732: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:44.117351: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:44.118153: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:44.683728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-06 18:21:48.700894: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:48.701422: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2021-01-06 18:21:48.701512: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n```", "@hansheng0512 Install TensorFlow GPU on a conda environment and just uninstall the default **cudatoolkit** and **cudnn** that is installed along with it.\r\n\r\n**(1)** To remove the cuda\r\n\r\n`conda remove --force cudatookit`\r\n\r\n**(2)** To remove the cudnn\r\n\r\n`conda remove --force cudnn`", "> @hansheng0512 Install TensorFlow GPU on a conda environment and just uninstall the default **cudatoolkit** and **cudnn** that is installed along with it.\r\n> \r\n> **(1)** To remove the cuda\r\n> \r\n> `conda remove --force cudatookit`\r\n> \r\n> **(2)** To remove the cudnn\r\n> \r\n> `conda remove --force cudnn`\r\n\r\nHi,\r\n\r\nFYI I'm using pip only, I don't use conda environment as conda environment dont support CUDA 11. I'm using RTX3070", "> > @hansheng0512 Install TensorFlow GPU on a conda environment and just uninstall the default **cudatoolkit** and **cudnn** that is installed along with it.\r\n> > **(1)** To remove the cuda\r\n> > `conda remove --force cudatookit`\r\n> > **(2)** To remove the cudnn\r\n> > `conda remove --force cudnn`\r\n> \r\n> Hi,\r\n> \r\n> FYI I'm using pip only, I don't use conda environment as conda environment dont support CUDA 11. I'm using RTX3070\r\n\r\nOkay, I haven't tried it with pip. Goodluck!", "> OK, I was able to execute my CNN. I'm using tensorflow tf-nightly-gpu-2.0-preview, and running on a ipython notebook. I had to add this to my notebook:\r\n> \r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> \r\n> [Here](https://www.tensorflow.org/guide/using_gpu) are some more details\r\n> \r\n> Also, this issue is associated with [24496] (#24496)\r\n\r\nThis solution worked for me. Just to add on - if you set `allow_growth = True` during training, you have to configure the gpu in the same way when restoring the model otherwise you will have issues.", "> > OK, I was able to execute my CNN. I'm using tensorflow tf-nightly-gpu-2.0-preview, and running on a ipython notebook. I had to add this to my notebook:\r\n> > from tensorflow.compat.v1 import ConfigProto\r\n> > from tensorflow.compat.v1 import InteractiveSession\r\n> > config = ConfigProto()\r\n> > config.gpu_options.allow_growth = True\r\n> > session = InteractiveSession(config=config)\r\n> > [Here](https://www.tensorflow.org/guide/using_gpu) are some more details\r\n> > Also, this issue is associated with [24496] (#24496)\r\n> \r\n> This solution worked for me. Just to add on - if you set `allow_growth = True` during training, you have to configure the gpu in the same way when restoring the model otherwise you will have issues.\r\n\r\nYea it works! Actually what is the concept behind by enabling `allow_growth = True`?", "> `import tensorflow as tf`\r\n> `config = tf.ConfigProto()`\r\n> `config.gpu_options.allow_growth = True`\r\n> `sess = tf.Session(config=config)`\r\n\r\nI tried this solution but doesn't work, my system specifications are:\r\nTF version: 2.2\r\nOS: Windows Server 2019\r\ncuda version: 10.1\r\ncuDNN: 7.6.4\r\nGPU: GTeslaV100\r\ncan you help please?", "> > `import tensorflow as tf`\r\n> > `config = tf.ConfigProto()`\r\n> > `config.gpu_options.allow_growth = True`\r\n> > `sess = tf.Session(config=config)`\r\n> \r\n> I tried this solution but doesn't work, my system specifications are:\r\n> TF version: 2.2\r\n> OS: Windows Server 2019\r\n> cuda version: 10.1\r\n> cuDNN: 7.6.4\r\n> GPU: GTeslaV100\r\n> can you help please?\r\n\r\n```\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```\r\n\r\nTry with this.", "See, I made a solution. \r\nFirst install graphics respective of your graphics drivers\r\nInstall Anaconda\r\n```\r\nconda update conda\r\nconda update anaconda\r\n\r\nthen \r\n\r\nconda create -n py36 python=3.6\r\n\r\n# first thing you do\r\nconda install tensorflow-gpu=1.15\r\n# this install cudatoolkit=10.0 cudnn=7.6.5 and of course tensorflow-gpu=1.15\r\n```\r\nNow if you want to want to run tensorflow with eager execution then\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n```\r\nThis makes tensorflow>=2.0.0 codes to run and even you can make them\r\n\r\nelse if you want to stick with older version tensorflow<2.0.0 then run as usual with tf.Session(), tf.placeholders, tf.Variables, ... and so on.", "Confirming that I hit this error while training a model on an RTX 2060, and setting `TF_FORCE_ALLOW_GROWTH` to `True` resolved the error. ", "Had a similar issue on a machine with 2 A100s. There was some device ambiguity and so looping through the devices and setting the memory growth manually worked in a tensorflow 2.x environment.\r\n\r\n`gpu_devices = tf.config.experimental.list_physical_devices('GPU')`\r\n`for device in gpu_devices:  tf.config.experimental.set_memory_growth(device, True)`\r\n\r\nTaken from a different issue\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/25446#issuecomment-562232813", "What worked for my Win10 using Anaconda (Python 3.5.x and NVIDIA GTX1650 was:\r\n-Downgrade to CUDA 9.0 (with Matching CUDNN 7.x)\r\n-Downgrade to Tensorflow 1.8.0 (check by 'pip show tensorflow')\r\n-Downgrade to Tensorflow-GPU 1.8.0 (check by 'pip show tensorflow-gpu)\r\n-Ensure overwrite the CUDNN files (For some reason I had to overwrite the existing cudnn.lib file -C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib\\x64 with the CUDNN.zip download. The version downgrade left the more recent version, which had to be replaced to remove the \"failed convolution\" error to go away\r\n\r\n--Hope this helps--", "It probably because of the memory growth under tensorflow framework, so try to add \r\nimport tensorflow as tf\r\nfrom keras import backend as K\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4## max GPU occupation\r\nK.set_session(tf.Session(config=config))\r\nK.get_session().run(tf.global_variables_initializer())\r\nbefore the whole code... It works for me.", "try:\r\n\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n    except RuntimeError as e:\r\n        print(e)", "Hy I hope that  you all are doing good. I need to train my mrcnn model on gtx 3070. Model loads onto the gpu but stuck while starting training no error appears but it stuck. When I list tensorflow device it show GPU exists but training not starts.\r\n\r\n![Screenshot from 2021-04-18 12-53-28](https://user-images.githubusercontent.com/29427728/115138389-3bb77b00-a045-11eb-9534-90d55308cfef.png)\r\n\r\n\r\n\r\nVersions I am using:\r\n\r\n1. Tensorflow 2.4\r\n2. cudnn 8\r\n3. cuda 11.0\r\n4. nvidia-drivers 460\r\n\r\n\r\n![Screenshot from 2021-04-18 12-45-13](https://user-images.githubusercontent.com/29427728/115138252-681ec780-a044-11eb-9e7d-c567d1219df0.png)\r\n\r\n![Screenshot from 2021-04-18 12-45-37](https://user-images.githubusercontent.com/29427728/115138260-6ead3f00-a044-11eb-8e35-7fb1f976b51e.png)\r\n\r\n![Screenshot from 2021-04-18 12-47-57](https://user-images.githubusercontent.com/29427728/115138273-7ec51e80-a044-11eb-91df-b8f536cb8952.png)\r\n\r\n\r\nI will really be thankful to you for helping me out. Thank you\r\n\r\n", "> If the result is not empty as the above, so it means you used conda installed TF, when using conda for installing TF, then it will install all the dependencies even CUDA and cuDNN, but the cuDNN version is very low for TF, so it will bring compatibility problem. So you can uninstall the cuDNN and the CUDA which was installed by conda, and then run TF, then it will work.\r\n\r\nSo one should reinstall using the nvidia installer?\r\n\r\n", "This solve my problem [here](https://www.tensorflow.org/install/source#gpu). Try to match the verison.", "@sauravsolanki\r\n> This solve my problem [here](https://www.tensorflow.org/install/source#gpu). Try to match the verison.\r\n\r\nSo you managed to make it work with these following versions ?\r\ntensorflow-2.4.0 | python 3.6-3.8 | GCC&nbsp;7.3.1 | Bazel&nbsp;3.1.0 | cuDNN 8.0 | CUDA 11.0\r\n\r\n\r\n", "@q-55555 Yes.\r\n\r\n\r\n", "> This solve my problem [here](https://www.tensorflow.org/install/source#gpu). Try to match the verison.\r\n\r\nespecially cuDNN version and tensorflow version, I downgraded tf version to match cuDNN version, then it's ok.", "I had the same problem and I solved it with this code\r\n`import tensorflow as tf`\r\n`config = tf.compat.v1.ConfigProto()`\r\n`config.gpu_options.allow_growth = True`\r\n`sess = tf.compat.v1.InteractiveSession(config=config)`", "> i also has same problem\r\n> try it delete the old cuDNN SDK (i remember it's no for 9.0)\r\n> download the cudnn-9.0-windows10-x64-v7.4.1.5\r\n> new cudnn-9.0-windows10-x64-v7.4.2.24.zip also work well\r\n> for 9.0 9.0 9.0\r\n> it's very important\r\n> then it work well\r\n> \r\n> system win 10\r\n> tensorflow 1.12\r\n> CUDA 9.0\r\n> cuDNN SDK 7.4.1.5\r\n> GPU GTX1060\r\n\r\nis works !", "I got the same issue.\r\n`pip install --upgrade tensorflow-gpu==1.10.0` solves it.", "> Had a similar issue on a machine with 2 A100s. There was some device ambiguity and so looping through the devices and setting the memory growth manually worked in a tensorflow 2.x environment.\r\n> \r\n> `gpu_devices = tf.config.experimental.list_physical_devices('GPU')` `for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)`\r\n> \r\n> Taken from a different issue\r\n> \r\n> [#25446 (comment)](https://github.com/tensorflow/tensorflow/issues/25446#issuecomment-562232813)\r\n\r\nThanks, this also worked for me in a RTX 2070 Super, TF 2.2, CUDA 10.1 on Ubuntu 18.04", "> In the meanwhile I have tried with Cudnn versions : 7.1,7.0.5,7.3,7.4 , gcc6,still no luck, however I dont get any of these issues when i installed it from conda using conda install tensorflow-gpu. However I want to build from source hence I would prefer if this issue is resolved\r\n\r\nThanx!!!"]}, {"number": 24827, "title": "tensorflow.keras.backend.clip has different semantics than keras.backend.clip.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n\r\nTF built from master.\r\n\r\n- Are you willing to contribute it (Yes/No):\r\n\r\nNo\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently `tensorflow.keras.backend.clip` takes a tensor and two floating point python values for min and max. But a recent PR in keras-team/keras allowed now to pass three tensors for elementwise clipping. The PR I'm talking about is https://github.com/keras-team/keras/pull/11442 . If tensorflow.keras wants to stay compatible with the keras API, this feature should be implemented in `tensorflow.keras`\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo change to the API.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAll people currently using keras contrib. I need tensorflow.keras API to be entirely compatible with keras-team/keras to avoid using two codebases to support keras and tf.keras. Some layers use this clipping feature and don't work when I change the imports to tf.keras.\r\n\r\n**Any Other info.**\r\n\r\n", "comments": ["Gabriel is right: the semantics for `backend.py` in stand-alone Keras support floats, integers, and tensors, while tf.keras [only supports floats and ints](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py#L2311-L2329).\r\n\r\nThis would just need a copy / paste of the changes in [`tensorflow_backend.py` and `backend_test.py`](https://github.com/keras-team/keras/pull/11442/files), correct?", "I believe that this issue is fixed."]}, {"number": 24826, "title": "tf.estimator rebuild graph at per training step", "body": "hello everyone:\r\n    i'm working on reading source code of tf.estimator, I found a little problem of this implementation\uff0c\r\n\r\n```python\r\n  def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n    \"\"\"Initiate training with `input_fn`, without `DistributionStrategies`.\r\n    Args:\r\n      input_fn: A function that provides input data for training as minibatches.\r\n      hooks: List of `tf.train.SessionRunHook` subclass instances. Used for\r\n        callbacks inside the training loop.\r\n      saving_listeners: list of `tf.train.CheckpointSaverListener` objects. Used\r\n        for callbacks that run immediately before or after checkpoint savings.\r\n\r\n    Returns:\r\n      Loss from training\r\n    \"\"\"\r\n    worker_hooks = []\r\n    with ops.Graph().as_default() as g, g.device(self._device_fn):\r\n      random_seed.set_random_seed(self._config.tf_random_seed)\r\n      global_step_tensor = self._create_and_assert_global_step(g)\r\n\r\n      # Skip creating a read variable if _create_and_assert_global_step\r\n      # returns None (e.g. tf.contrib.estimator.SavedModelEstimator).\r\n      if global_step_tensor is not None:\r\n        training_util._get_or_create_global_step_read(g)  # pylint: disable=protected-access\r\n      \r\n      features, labels, input_hooks = (\r\n          self._get_features_and_labels_from_input_fn(\r\n              input_fn, model_fn_lib.ModeKeys.TRAIN))\r\n      worker_hooks.extend(input_hooks)\r\n    \r\n      estimator_spec = self._call_model_fn(\r\n          features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n      global_step_tensor = training_util.get_global_step(g)\r\n      return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n                                             hooks, global_step_tensor,\r\n                                             saving_listeners)\r\n```\r\n this code is part of Estimator.training, look at this\r\n```python\r\n estimator_spec = self._call_model_fn(\r\n          features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n```\r\nit will be called at every training step, that is a build graph operation, i think is called one time at the begining is better, am I right?", "comments": ["The code you are referring to is called only once everytime you call `estimator.train` to build the graph, and not per training step. the loop over training steps is here:\r\nhttps://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/estimator.py#L1402\r\n\r\nAlso, stackoverflow might be a better place for a clarification question like this unless you have seen this is actually a bug in practice. \r\n", "> The code you are referring to is called only once everytime you call `estimator.train` to build the graph, and not per training step. the loop over training steps is here:\r\n> https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/estimator.py#L1402\r\n> \r\n> Also, stackoverflow might be a better place for a clarification question like this unless you have seen this is actually a bug in practice.\r\n\r\nbut, this function is called after :  \r\n    estimator_spec = self._call_model_fn(\r\nthat  _call_model_fn is  at the front of  _train_with_estimator_spec fucntion:\r\nhttps://github.com/tensorflow/estimator/blob/781389ebd8dc87c71711f432e2eae135917429bd/tensorflow_estimator/python/estimator/estimator.py#L1154\r\n\r\nso, every estimator.train will call both _call_model_fn and _train_with_estimator_spec at per training step", "so, i suggest that : move _call_model_fn to initialize function to build graph, if it still in train function yet, that way  may be  like \"copy on write \" or \"init on first start\", but, may be someone like me , we think that train function it can be called in dynamic\uff08dynamic means i can change my input function in every train step\uff0c i will call train function for several times)"]}, {"number": 24825, "title": "Unsupported data type in placeholder op:2", "body": "When I tried to convert a .pb to .tflite, an error occurred.\r\nThe command line shows below:\r\n\r\n > tflite_convert \\\r\n>   --output_file=model_file.tflite \\\r\n>   --graph_def_file=model_file.pb \\\r\n>   --input_arrays=input1,input2 \\\r\n>   --output_arrays=output1,output2 \\\r\n>   --input_shapes=512,512,3:2\r\n\r\nAnd the errors :\r\n\r\n> RuntimeError: TOCO failed see console for info.\r\n> 2019-01-10 16:03:15.168961: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] **Unsupported data type in placeholder op: 2**\r\n> 2019-01-10 16:03:15.168987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] **Unsupported data type in placeholder op: 2**\r\n> 2019-01-10 16:03:15.168995: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2\r\n> 2019-01-10 16:03:15.169013: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2\r\n> 2019-01-10 16:03:15.169019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2\r\n> 2019-01-10 16:03:15.169025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2\r\n> 2019-01-10 16:03:15.177018: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 740 operators, 1073 arrays (0 quantized)\r\n> 2019-01-10 16:03:15.191193: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 740 operators, 1073 arrays (0 quantized)\r\n> 2019-01-10 16:03:15.205171: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 126 operators, 296 arrays (0 quantized)\r\n> 2019-01-10 16:03:15.207152: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 126 operators, 296 arrays (0 quantized)\r\n> 2019-01-10 16:03:15.208723: F tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:164] **An array, detect/cast, still does not have a known data type after all graph transformations have run.**\r\n> Aborted (core dumped)\r\n> \r\n> None\r\n\r\nI have no idea about this error, can anyone offer help.\r\n", "comments": ["@Archernarkiu Hope this can help you:\r\n\r\n```cc\r\nArrayDataType ConvertDataType(tensorflow::DataType dtype) {\r\n  if (dtype == DT_UINT8)\r\n    return ArrayDataType::kUint8;\r\n  else if (dtype == DT_FLOAT)\r\n    return ArrayDataType::kFloat;\r\n  else if (dtype == DT_BOOL)\r\n    return ArrayDataType::kBool;\r\n  else if (dtype == DT_INT32)\r\n    return ArrayDataType::kInt32;\r\n  else if (dtype == DT_INT64)\r\n    return ArrayDataType::kInt64;\r\n  else if (dtype == DT_STRING)\r\n    return ArrayDataType::kString;\r\n  else\r\n    LOG(INFO) << \"Unsupported data type in placeholder op: \" << dtype;\r\n  return ArrayDataType::kNone;\r\n}\r\n```", "Type 2 is DOUBLE (see also `tensorflow::DataType` definition). \r\n\r\nTensorFlow Lite doesn't support double type now. The easiest way is modify your graph and use float instead. \r\n\r\nI'm closing this ticket as this is working as intended. Please file a feature request if you think double is required for your use case. Then we'll prioritize accordingly. Thanks!", "Hey i'am facing similar problem with tflite conversion of a  keras model(plain keras not tf.keras) with Input layer type float16 in TF 1.15.0. \r\n\r\n` I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 19\r\nF tensorflow/lite/toco/import_tensorflow.cc:2679] Check failed: status.ok() Unexpected value for attribute 'T'. Expected 'DT_FLOAT'\r\nAborted (core dumped)`\r\n\r\nIs float16 placeholder not still supported, even though other operators are supported?"]}, {"number": 24824, "title": "ValueError: as_list() is not defined on an unknown TensorShape.", "body": "I want to test some example with my training model ,but it occur error during test ,console tell me the wrong in my code is focus on line 119:\r\n\r\nline112: cell_fw = LSTMCell(self.hidden_dim,forget_bias=0.4)\r\nline113: cell_bw = LSTMCell(self.hidden_dim,forget_bias=0.4)\r\nline114: (output_fw_seq, output_bw_seq), _ = \r\n            tf.nn.bidirectional_dynamic_rnn(\r\nline115:    cell_fw=cell_fw,\r\nline116:    cell_bw=cell_bw,\r\nline117:    inputs=self.word_embeddings,\r\nline118:    sequence_length=self.sequence_lengths,\r\nline119     dtype=tf.float32)\r\nThe console error as follows:\r\n\r\n File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 416, in bidirectional_dynamic_rnn\r\n    time_major=time_major, scope=fw_scope)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 632, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 697, in _dynamic_rnn_loop\r\n    const_time_steps, const_batch_size = inputs_got_shape[0].as_list()[:2]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 812, in as_list\r\n    raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\nValueError: as_list() is not defined on an unknown TensorShape.", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24823, "title": "tf.contrib.integrate.odeint can only accept time points in increasing order", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\ncurrently, tensorflow support odeint function. but it only supports input time points in ascending order. This restriction make solving back propagation of ODENet(https://arxiv.org/abs/1806.07366) impossible. ODENet requires giving initial condition at latter time point and integrate to get the function value of an earlier time point. besides, odeint function of scipy supports input time points given in any order.\r\n\r\n**Will this change the current api? How?**\r\n\r\nthis change would not change the current api. it just loose the restriction to the input time points.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nresearcher who is doing researches with ODENet will be happy to see it.\r\n\r\n**Any Other info.**\r\n", "comments": ["I would love to see this change implemented as well.\r\n\r\nIn the meantime, would it be possible to allow backward in time integration for ODEs by simply remove the '_assert_increasing(t)' function in 'tensorflow/tensorflow/contrib/integrate/python/ops/odes.py'? As far as I can tell, this seems to be an unnecessary conditions check (please correct me if I'm wrong). \r\n\r\n**Porposal:**\r\nRemove this code block on line 275, and its calls on line 389, 695 in tensorflow/tensorflow/contrib/integrate/python/ops/odes.py:\r\n\r\n    def _assert_increasing(t): \r\n        assert_increasing = control_flow_ops.Assert(\r\n        math_ops.reduce_all(t[1:] > t[:-1]), ['`t` must be monotonic increasing'])\r\n        return ops.control_dependencies([assert_increasing])\r\n", "Given the operations in current ODENet do not depend on the specific value of t, you could simply integrate `-ode_func` (negative derivative) with ascending time to solve ODE back in time.", "OK thx, by the way. by the way, do you know where odeint is in tf2.0?", "According to [20180907-contrib-sunset.md](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md), odeint will be moved to tensorflow/scientific, but I cannot find it yet. According to Issue #15833, odeint will be moved to tensorflow/probability (specifically, [this link](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/math/ode)), but I cannot find it yet. \r\n\r\nGuess we'll have to wait. ", "thx", "We have a BDF solver in TF-probability now. If anyone is interested in porting over the RK45 solver from tf.contrib.integrate to TF probability, that would be very welcome.", "@breadbread1984,\r\nCan you please let us know if [tfp.math.ode.Solver](https://www.tensorflow.org/probability/api_docs/python/tfp/math/ode/Solver) is the API that you are looking for? Thanks!\r\n ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24822, "title": "`pip install tensorflow` still install 1.5.0", "body": "I installed tensorflow via `pip install tensorflow`, but found it was still using a very old version tensorflow, and this caused the `tf.enable_eager_execution()` failed. Can we upgrade the tensorflow to a new version?\r\n\r\n```\r\n>>> tf.VERSION\r\n'1.5.0'\r\n>>> tf.enable_eager_execution()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute 'enable_eager_execution'\r\n```", "comments": ["That's strange, It should have given you TF 1.12 by default.\r\nCan you try this,\r\n>pip install tensorflow==1.12.0", "@ymodak thanks, this works.\r\n\r\n```\r\nroot@gyliu-c11:~/test/istio-1.0.5/samples/bookinfo/networking# pip install tensorflow==1.12\r\nCollecting tensorflow==1.12\r\n  Downloading https://files.pythonhosted.org/packages/bd/68/ec26b2cb070a5760707ec8d9491a24e5be72f4885f265bb04abf70c0f9f1/tensorflow-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl (83.1MB)\r\n    100% |################################| 83.1MB 56kB/s\r\nRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (0.7.1)\r\nRequirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.1.6)\r\nRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (3.6.1)\r\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.0.5)\r\nRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (0.2.1.post0)\r\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.11.0)\r\nRequirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (0.32.2)\r\nRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (0.6.1)\r\nRequirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.0.post1)\r\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.1.0)\r\nRequirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.12.1)\r\nRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.14.0)\r\nRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.16.0)\r\nRequirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (2.0.0)\r\nRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.0.6)\r\nRequirement already satisfied: setuptools in /usr/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (20.7.0)\r\nRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.14.1)\r\nRequirement already satisfied: futures>=3.1.1; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.2.0)\r\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.0.1)\r\nRequirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.12) (5.1.0)\r\nRequirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.12) (1.0.2)\r\nRequirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.9.0)\r\nInstalling collected packages: tensorflow\r\n  Found existing installation: tensorflow 1.5.0\r\n    Uninstalling tensorflow-1.5.0:\r\n      Successfully uninstalled tensorflow-1.5.0\r\nSuccessfully installed tensorflow-1.12.0\r\n```\r\n```\r\nroot@gyliu-c11:~/test/istio-1.0.5/samples/bookinfo/networking# python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18)\r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.enable_eager_execution()\r\n>>> tf.add(1, 2)\r\n2019-01-10 01:47:19.591231: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n<tf.Tensor: id=3, shape=(), dtype=int32, numpy=3>\r\n>>> tf.VERSION\r\n'1.12.0'\r\n```", "Awesome. I will close this issue now since its resolved.", "@ymodak but when people follow the guidance here https://github.com/tensorflow/tensorflow#installation , they will encounter same issue as me, I think the document needs to be updated?"]}, {"number": 24821, "title": "tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.", "body": "I am  trying to install  tensorflow after activating conda environment using pip\r\n$ conda activate /home/eartaert/anaconda3/\r\n$ pip install --upgrade /home/eartaert/Downloads/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\r\n\r\n but the following error is coming \r\n\r\ntensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\r\n \r\n\r\nuname -a results are as follows \r\nLinux ubuntu 4.15.0-29-generic #31~16.04.1-Ubuntu SMP Wed Jul 18 08:54:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\ncan anyone help please\r\n\r\n", "comments": ["@rpsg35 Could you follow the instructions [1](https://www.anaconda.com/blog/developer-blog/tensorflow-in-anaconda/), [2](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-tensorflow-on-ubuntu-16-04), and [3](https://towardsdatascience.com/tensorflow-gpu-installation-made-easy-use-conda-instead-of-pip-52e5249374bc). Please let us know how it progresses? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 24820, "title": "Rename polymorphic function and function", "body": "**TL;DR**: this PR renames:\r\n* `Function` to `ConcreteFunction`\r\n* `PolymorphicFunction` to `Function`\r\n\r\n**Motivation**\r\n\r\n`PolymorphicFunction` is a central part of TensorFlow 2.0, so it should have a simple and intuitive name. The fact that it's polymorphic is somewhat of a technical detail. This makes it harder to casually reference this object. Notice for example that the documentation of `tf.function` never mentions this name, and instead it resorts to saying `\"the callable returned by function\"`. \r\n\r\nThis PR renames `PolymorphicFunction` to `Function`, so people can simply refer to them as \"tensorflow functions\" or \"tf functions\" (I suggest to avoid calling them simply \"functions\", as in many contexts this is ambiguous).\r\nSince the name `Function` is already taken by the concrete functions generated by `get_concrete_function()`, this PR renames this class to `ConcreteFunction`, so people can just call them \"concrete functions\" or \"concrete TF functions\" (I suggest to avoid calling them \"graph functions\", as it is unclear whether this refers to TF functions or concrete functions).\r\n\r\n**Alternative**\r\n\r\nI noticed that concrete functions are often named \"graph functions\" in many of the comments and variable/function names. This name is pretty natural and explicit, and it would avoid ambiguity when talking about \"functions\" (are we talking about Python functions? TF Functions? Concrete Functions?). Perhaps it might be a better idea to use `GraphFunction` instead of `Function`, and `ConcreteGraphFunction` instead of `ConcreteFunction`. I'm happy to update this PR if you feel this naming would be preferable.\r\n", "comments": ["@alextp  seems some checks are failing , can you please verify once", "There's an \"assertRaises\" test in function_test.py:1734 which tests for the name ConcreteFunction. Can you change the name in the regexp?\r\n\r\n(I _hate_ those tests...)", "@ageron can you please make appropriate changes", "I just merged with HEAD, resolved some conflicts and fixed the issue. ~~You can merge when you want if the tests pass, or you can wait for me to also rename the Polymorphic/Monomorphic protos and `SavedPolymorphicFunction`, as you prefer.~~\r\n**Edit**: I just renamed `SavedPolymorphicFunction` & `MonomorphicFunction` protos to `SavedFunction` & `ConcreteFunction`, and I also renamed the `ConcreteFunction` proto's `concrete_function` attribute to `name`.  Hope that's okay.", "Can you resolve the conflicts?", "@ageron can you resolve conflicts ?", "Hi @alextp and @rthadur , I just resolved the conflicts again, wow this is a moving target!  ;-)\r\nI had to do this quickly, so I hope all the tests will pass.\r\nCheers", "Yikes, I had to do another merge with conflicts, Polymorphic keeps popping back up, it's like whack a mole!  ;-)", "Thanks @allenlavoie , indeed, I just resolved conflicts again.  If it fails once more, I'll just focus on the user facing classes.", "Great names. Thanks for dealing with all of the merge conflicts.", "Cool, thanks for your help @allenlavoie and @alextp !"]}, {"number": 24819, "title": "Invalid argument: Must have updates.shape = indices.shape[:batch_dim] + params_shape[slice_dim:]", "body": "I want to test some example with my training model ,but it occur error during test ,console tell me the wrong in my code is focus on line 94:\r\n\r\n line90\uff1avar_output_2 = tf.Variable(0, dtype=tf.float32,name=\"a\",trainable=False,validate_shape=False)\r\n line91: row_vector = tf.gather_nd(word_embeddings, self.word_error_embed, name=\"a_word_error_embed\")\r\n line92: sum_all = tf.reduce_sum(row_vector, 1, name=\"a_reduce_sum\")\r\n line93: var_output_3=tf.assign(var_output_2, word_embeddings, validate_shape=False)\r\n line94: word_embeddings = tf.scatter_nd_update(var_output_3, self.error_word, sum_all)\r\n\r\nThe console error as follows:\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Must have updates.shape = indices.shape[:batch_dim] + params_shape[slice_dim:], got updates.shape: [7,300], indices.shape: [7,1], params_shape: [7,9,300], slice_dim: 1, and batch_dim: 1\r\n     [[Node: words/ScatterNdUpdate = ScatterNdUpdate[T=DT_FLOAT, Tindices=DT_INT32, _class=[\"loc:@words/a\"], use_locking=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](words/Assign, _arg_word_error_embed_2_0_4/_57, words/a_reduce_sum)]]", "comments": []}, {"number": 24818, "title": "eigvalsh documentation error", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/eigvalsh\r\n\r\n**Describe the documentation issue**\r\n\r\nNote: If your program backpropagates through this function, you should replace it with a call to tf.linalg.eigvalsh (possibly ignoring the second output) to avoid computing the eigen decomposition twice. This is because the eigenvectors are used to compute the gradient w.r.t. the eigenvalues. See _SelfAdjointEigV2Grad in linalg_grad.py.\r\n\r\nThis warning is quite ambiguous since\r\n1) ling.eigvalsh returns only one output, while linalg.eigh returns two outputs\r\n2) it circularly references itself\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nYes if it devs agree that there's an issue.\r\n", "comments": ["Hi Yutong,\r\n\r\nThat docstring is here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_ops.py#L337\r\n\r\nWould you be able to make the contribution, please?"]}, {"number": 24817, "title": "TFDBG Segmentation Fault", "body": "TFDBG encounters a seg fault (see output at bottom) when debugging my TF code. Incidentally this code hangs on a GPU, but code and tfdbg both run without incident on CPU only (setting tf.device). As a side note, is tensorflow ever supposed to produce different execution results on CPU and GPU?\r\n\r\n- OS Platform and Distribution: AWS Deep Learning Base AMI (Ubuntu) Version 14.0 (ami-012b19f1736b6aae8)\r\n- TensorFlow installed from (source or binary): tensorflow-gpu==1.12.0 installed by pip\r\n- Python version: Python 3.6.7\r\n- CUDA/cuDNN version: 9.0/7.3.1\r\n- GPU model and memory: Tesla V100 with 16130MiB\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem. <== It's not clear why or where the seg fault occurs so I can't generate a minimal test case. I'd be happy to share my full code (a single python file) with you (the TF developers, eg via email) but don't want to post it on github at present.\r\n\r\n**Other info / logs**\r\n\r\n// Started tfdbg, called run() once to initialize the variables, then invoke_stepper and:\r\n\r\ntfdbg> step\r\n  -->(1 / 126)  [  H   ] model/transpose/sub/y\r\n     (2 / 126)  [      ] model/transpose/Range/start\r\n     (3 / 126)  [      ] model/TensorArrayStack/range/start\r\n\r\nContinued to model/transpose/sub/y:\r\n\r\nStepper used feeds:\r\n  (No feeds)\r\n\r\nTensor \"model/transpose/sub/y\":\r\n\r\n1\r\n\r\ntfdbg> s\r\n     (1 / 126)  [  H   ] model/transpose/sub/y\r\n  -->(2 / 126)  [  H   ] model/transpose/Range/start\r\n     (3 / 126)  [      ] model/TensorArrayStack/range/start\r\n     (4 / 126)  [      ] model/TensorArrayStack/range/delta\r\n\r\nContinued to model/transpose/Range/start:\r\n\r\nStepper used feeds:\r\n  (No feeds)\r\n\r\nTensor \"model/transpose/Range/start\":\r\n\r\n0\r\n\r\ntfdbg> s\r\n     (1 / 126)  [  H   ] model/transpose/sub/y\r\n     (2 / 126)  [  H   ] model/transpose/Range/start\r\n  -->(3 / 126)  [  H   ] model/TensorArrayStack/range/start\r\n     (4 / 126)  [      ] model/TensorArrayStack/range/delta\r\n     (5 / 126)  [      ] model/transpose/Range/delta\r\n\r\nContinued to model/TensorArrayStack/range/start:\r\n\r\nStepper used feeds:\r\n  (No feeds)\r\n\r\nTensor \"model/TensorArrayStack/range/start\":\r\n\r\n0\r\n\r\ntfdbg> s\r\n     (2 / 126)  [  H   ] model/transpose/Range/start\r\n     (3 / 126)  [  H   ] model/TensorArrayStack/range/start\r\n  -->(4 / 126)  [  H   ] model/TensorArrayStack/range/delta\r\n     (5 / 126)  [      ] model/transpose/Range/delta\r\n     (6 / 126)  [      ] model/TensorArray/size\r\n\r\nContinued to model/TensorArrayStack/range/delta:\r\n\r\nStepper used feeds:\r\n  (No feeds)\r\n\r\nTensor \"model/TensorArrayStack/range/delta\":\r\n\r\n1\r\n\r\ntfdbg> s\r\n     (3 / 126)  [  H   ] model/TensorArrayStack/range/start\r\n     (4 / 126)  [  H   ] model/TensorArrayStack/range/delta\r\n  -->(5 / 126)  [  H   ] model/transpose/Range/delta\r\n     (6 / 126)  [      ] model/TensorArray/size\r\n     (7 / 126)  [      ] model/TensorArray\r\n\r\nContinued to model/transpose/Range/delta:\r\n\r\nStepper used feeds:\r\n  (No feeds)\r\n\r\nTensor \"model/transpose/Range/delta\":\r\n\r\n1\r\n\r\ntfdbg> s\r\n     (4 / 126)  [  H   ] model/TensorArrayStack/range/delta\r\n     (5 / 126)  [  H   ] model/transpose/Range/delta\r\n  -->(6 / 126)  [  H   ] model/TensorArray/size\r\n     (7 / 126)  [      ] model/TensorArray\r\n     (8 / 126)  [      ] model/value/weights\r\n\r\nContinued to model/TensorArray/size:\r\n\r\nStepper used feeds:\r\n  (No feeds)\r\n\r\nTensor \"model/TensorArray/size\":\r\n\r\n1\r\n\r\ntfdbg> s\r\nSegmentation fault (core dumped)\r\nubuntu@ip-10-0-0-9:~$\r\n", "comments": ["i also have a very similar issue except it does not work on both cpu and gpu and it produces segmentationfault after i type run (i am using this with keras functional api).", ">\"Segmentation fault error stands for a failure of the parallelization of your training process on the GPU\"\r\n\r\nPlease take a look on the links [ref_1](https://stackoverflow.com/questions/54333809/how-to-remedy-segmentation-fault-core-dumped-error-when-trying-to-fit-a-kera), [ref_2](https://stackoverflow.com/questions/53302958/how-to-debug-tensorflow-segmentation-fault-in-model-fit) to understand more about the same. ", "@dgboy2000,\r\n\r\nWe see you were using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to latest stable version and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24817\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24817\">No</a>\n"]}, {"number": 24816, "title": "Wrong Error Raised: \"The graph couldn't be sorted in topological order\"", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip from anaconda\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 9.0\r\n- GPU model and memory: GeForce GTX 1080Ti / Tesla K80\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nThe tensorflow will raise `The graph couldn't be sorted in topological order` Error when executing the optimizer. While The error doesn't occur on tensorflow 1.10.0. This error is also posed [here](https://stackoverflow.com/questions/54088172/why-tensorflow-throws-the-graph-couldnt-be-sorted-in-topological-order/54120941#54120941) by another user.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe error should not be raised because there is no loop in the computation graph.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nReference: https://stackoverflow.com/questions/54088172/why-tensorflow-throws-the-graph-couldnt-be-sorted-in-topological-order/54120941#54120941\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nactivation = tf.nn.relu\r\nimg_plh = tf.placeholder(tf.float32, [None, 3, 3, 3])\r\nlabel_plh = tf.placeholder(tf.float32, [None])\r\nlayer = img_plh\r\nbuffer = []\r\nks_list = list(range(1, 10, 1)) + list(range(9, 0, -1))\r\nfor ks in ks_list:\r\n    buffer.append(tf.layers.conv2d(layer, 9, ks, 1, \"same\", activation=activation))\r\nlayer = tf.concat(buffer, 3)\r\nlayer = tf.layers.conv2d(layer, 1, 3, 1, \"valid\", activation=activation)\r\nlayer = tf.squeeze(layer, [1, 2, 3])\r\nloss_op = tf.reduce_mean(tf.abs(label_plh - layer))\r\noptimizer = tf.train.AdamOptimizer()\r\ntrain_op = optimizer.minimize(loss_op)\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    result = sess.run(train_op, {img_plh: np.zeros([2, 3, 3, 3], np.float32), label_plh: np.zeros([2], np.float32)})\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I am experiencing this, also in tensorflow 1.12.0 (from conda). I'm using a custom deep network and the same architecture sometimes raises this error and sometimes not (which makes me believe that is a tensorflow issue).\r\n\r\nI freeze the graph before starting the TF session but when this warning appears the process RAM usage keeps increasing indefinitely.", "I upgraded to 1.13.1 (from pip in python 3.6 & Windows 10). This problem still exists.\r\nBut it can be solved by move Activation Function to the place after tf.concat(buffer, 3)\r\n\r\n`for ks in ks_list:`\r\n`    buffer.append(tf.layers.conv2d(layer, 9, ks, 1, \"same\"))`\r\n`layer = tf.concat(buffer, 3)`\r\n`layer = activation(layer)`\r\n\r\nBy the way, how to add space and newline characters in `  buffer.append(tf.layers.conv2d(layer, 9, ks, 1, \"same\"))`", "Could you try to decrease batch size? This seems to be memory related.", "> Could you try to decrease batch size? This seems to be memory related.\r\n\r\nMy GPU has 8GB memory, and the problem still exists. In the example, the batch size of the input is only 2. If it is a memory problem, the batch size does not seem to be the critical condition that causes the problem. In my response, the problem may be related to the implementation of the activator and the optimizer, because the error disappears after moving the activator out of the For loop.", "> > Could you try to decrease batch size? This seems to be memory related.\r\n> \r\n> My GPU has 8GB memory, and the problem still exists. In the example, the batch size of the input is only 2. If it is a memory problem, the batch size does not seem to be the critical condition that causes the problem. In my response, the problem may be related to the implementation of the activator and the optimizer, because the error disappears after moving the activator out of the For loop.\r\n\r\nI do think it has to do with memory and the graph, even if not using all the RAM. My code is not a neural network per se and I'm not using any activation function, but when the number of units in a layer is low I don't get the error. Running the **same code** with more units in the hidden layers causes the error to appear. I think that the error may be caused by the graph growing too much to do any optimization that TF normally does. But that is just my guess", "> > > Could you try to decrease batch size? This seems to be memory related.\r\n> > \r\n> > \r\n> > My GPU has 8GB memory, and the problem still exists. In the example, the batch size of the input is only 2. If it is a memory problem, the batch size does not seem to be the critical condition that causes the problem. In my response, the problem may be related to the implementation of the activator and the optimizer, because the error disappears after moving the activator out of the For loop.\r\n> \r\n> I do think it has to be with memory and the graph, even if not using all the RAM. My code is not a neural network per se and I'm not using any activation function, but when the number of units in a layer is low I don't get the error. Running the **same code** with more units in the hidden layers causes the error to appear. I think that the error may be caused by the graph growing too much to do any optimization that TF normally does. But that is just my guess\r\n\r\nI think your guess is right.", "> I do think it has to do with memory and the graph, even if not using all the RAM. My code is not a neural network per se and I'm not using any activation function, but when the number of units in a layer is low I don't get the error. Running the **same code** with more units in the hidden layers causes the error to appear. I think that the error may be caused by the graph growing too much to do any optimization that TF normally does. But that is just my guess\r\n\r\nFor me, a *12-layer* network does not have this error, but  a *20-layer* network will have this error.\r\nIn addition, if you see this error, the weights updated are incorrect.", "I met the same problem. When double-check the updated weights there is slightly error after updating. Finally make the training very slow. \r\nAnd also, I created a small-size network, the problem exists as well.", "Any update on this issue ? I got the same problem here with Tensorflow 2.0.0alpha0 and beta1-gpu or cpu on Ubuntu 18.04 LTS and Windows 10.\r\nTried to reduce batch size, to move activations out of loops or to reduce number of layers / units to no avail.\r\nMy model / dataset works _fine_ with TF 1.9.0-gpu without this warning, and the time per step is even lower by more than 33% when compared to an execution on TF 2.0 (which gives the warning).\r\nGPU is an RTX 2070 8Gb by the way.\r\nThanks.", "I want to build a SSD netWork by myself ,and I meet the same problem .\r\n![image](https://user-images.githubusercontent.com/27055350/60565650-04f07200-9d97-11e9-8d35-2db98f9ce4ab.png)\r\n\r\n\r\ni have seen this method,it can run ok.\r\n![image](https://user-images.githubusercontent.com/27055350/60565680-20f41380-9d97-11e9-944f-1daeb9d330dd.png)\r\nthen i try to solve my problem like this method!\r\nthe old code as show belown\r\n![image](https://user-images.githubusercontent.com/27055350/60565766-64e71880-9d97-11e9-8435-3bc5adac7a29.png)\r\ni removed the Activation function\uff0clike this:\r\n![image](https://user-images.githubusercontent.com/27055350/60565914-e474e780-9d97-11e9-9eda-4f28867e7677.png) \r\nthe proplem disappeared\r\nconclusion\uff1a\r\n\r\nwhten many tensors which are actived  by Activation function are  concated  as a list ,this problem will appear!\r\ni think this is  a problem of tensorflow.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "confirmed again that it is the problem of tensorflow and I'm using r1.13.\r\n\r\nIn implementing spp layer by slicing and when used with dense bins (e.g. a 27x27 pooled feature map), tensorflow complains when the 27x27=729 tensors are concatenated in one concat op.\r\n\r\nBut it is confusing that it complains about the wrong topological order and such error message is misleading. \r\n\r\nIf such operation exceeds the current capability of graph optimizer, could it generates a message like \"operations with too much tensor\", instead of complaining about the graph topology? ", "Hm, while initially I thought my problem was related to this issue I know now it has nothing to do with concatenate layers but with my custom loss function. I will open a new issue. Thanks.", "For me it appear when a manual loop with too many steps  is used in my graph. However I replace it with tf.while_loop the warning still exist.", "Any news&process here? I also face the same problem.\r\nHere is my code:\r\n\r\n`weight_atom = tf.get_variable(name, [2, 2])`\r\n`#do repetition`\r\n    `weight_row = tf.concat([weight_atom] * 2, axis=1)`\r\n    `weight = tf.concat([weight_row] * 2, axis=0)`\r\n   ` out = tf.matmul(input, weights) `\r\n\r\nThe code create a (2,2) matrix then does repetition and multiply the result with the input which shape is (4,4)", "Same error occurs **only for validation** using identical generators for train and valid for the dummy example below (Ubuntu 18, tf 1.14, python 3.6):\r\n\r\n```\r\n# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Tue sep 15, 2019\r\n\r\n@author: Olivier Philip\r\n\"\"\"\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\r\n\r\nfrom keras.models import Model\r\nfrom keras.layers import Input, Conv1D, Dense, Lambda, UpSampling1D\r\nfrom keras.callbacks import TensorBoard\r\nfrom keras.optimizers import Adam\r\nfrom keras import backend as kbe\r\nimport numpy as np\r\n\r\n\r\ndef generate(cfg):\r\n    while True:\r\n        batch_data = np.random.rand(cfg['batch'], cfg['input_size'])\r\n        label_data = np.random.randint(0, 2, size=(cfg['batch'], cfg['window'], cfg['nb_filter']))\r\n        yield batch_data, label_data\r\n\r\n\r\ndef create_model(cfg):\r\n    lin, lout = list(), list()\r\n    lact = Input((cfg['input_size'],))\r\n    lin.append(lact)\r\n    lact = Dense(cfg['nb_neuron'], activation=cfg['acti'])(lact)\r\n    lact = Lambda(lambda x: kbe.expand_dims(x, axis=1))(lact)\r\n    lact = UpSampling1D(cfg['window'])(lact)\r\n    lact = Conv1D(cfg['nb_filter'], 3, padding='same', activation='sigmoid')(lact)\r\n    lout.append(lact)\r\n    model = Model(lin, lout)\r\n    model.compile(Adam(cfg['lr'], amsgrad=True), loss=['binary_crossentropy'], metrics=['binary_accuracy'])\r\n    return model\r\n\r\n\r\ndef train(cfg):\r\n    model = create_model(cfg)\r\n    g_tr = generate(cfg)\r\n    g_va = generate(cfg)\r\n    cb = list()\r\n    cb.append(TensorBoard(log_dir='tensorboard', write_grads=True, update_freq='batch'))\r\n    model.fit_generator(g_tr, steps_per_epoch=100,\r\n                        validation_data=g_va, validation_steps=10,\r\n                        epochs=cfg['epoch'], callbacks=cb)\r\n\r\n\r\ncfg = {\r\n    'input_size': 16, 'window': 7,\r\n    'batch': 16, 'epoch': 100, 'lr': 1e-5,\r\n    'acti': 'relu', 'nb_neuron': 8, 'nb_filter': 8,\r\n}\r\n\r\nif __name__ == \"__main__\":\r\n    train(cfg)\r\n```\r\n![graph](https://user-images.githubusercontent.com/4320648/64924932-1966d700-d7ea-11e9-8a36-eac04892023c.png)", "Any update? Face the same problem too.", "> confirmed again that it is the problem of tensorflow and I'm using r1.13.\r\n> \r\n> In implementing spp layer by slicing and when used with dense bins (e.g. a 27x27 pooled feature map), tensorflow complains when the 27x27=729 tensors are concatenated in one concat op.\r\n> \r\n> But it is confusing that it complains about the wrong topological order and such error message is misleading.\r\n> \r\n> If such operation exceeds the current capability of graph optimizer, could it generates a message like \"operations with too much tensor\", instead of complaining about the graph topology?\r\n\r\nI have the same problem when I use spp layer with tensorflow 1.14.If it is right when I  use spp layer and just ignore this misleading message?", "> > confirmed again that it is the problem of tensorflow and I'm using r1.13.\r\n> > In implementing spp layer by slicing and when used with dense bins (e.g. a 27x27 pooled feature map), tensorflow complains when the 27x27=729 tensors are concatenated in one concat op.\r\n> > But it is confusing that it complains about the wrong topological order and such error message is misleading.\r\n> > If such operation exceeds the current capability of graph optimizer, could it generates a message like \"operations with too much tensor\", instead of complaining about the graph topology?\r\n> \r\n> I have the same problem when I use spp layer with tensorflow 1.14.If it is right when I use spp layer and just ignore this misleading message?\r\n\r\nI have detected that after this warning performance is worse", "> > > confirmed again that it is the problem of tensorflow and I'm using r1.13.\r\n> > > In implementing spp layer by slicing and when used with dense bins (e.g. a 27x27 pooled feature map), tensorflow complains when the 27x27=729 tensors are concatenated in one concat op.\r\n> > > But it is confusing that it complains about the wrong topological order and such error message is misleading.\r\n> > > If such operation exceeds the current capability of graph optimizer, could it generates a message like \"operations with too much tensor\", instead of complaining about the graph topology?\r\n> > \r\n> > \r\n> > I have the same problem when I use spp layer with tensorflow 1.14.If it is right when I use spp layer and just ignore this misleading message?\r\n> \r\n> I have detected that after this warning performance is worse\r\n\r\nYes it seems that we can't ignore this warning...In my training, the loss get very high after a few iterations", "> > confirmed again that it is the problem of tensorflow and I'm using r1.13.\r\n> > In implementing spp layer by slicing and when used with dense bins (e.g. a 27x27 pooled feature map), tensorflow complains when the 27x27=729 tensors are concatenated in one concat op.\r\n> > But it is confusing that it complains about the wrong topological order and such error message is misleading.\r\n> > If such operation exceeds the current capability of graph optimizer, could it generates a message like \"operations with too much tensor\", instead of complaining about the graph topology?\r\n> \r\n> I have the same problem when I use spp layer with tensorflow 1.14.If it is right when I use spp layer and just ignore this misleading message?\r\n\r\nBetter not. Instead, you could try to do multiple concat instead of a huge concat. (yes, that's ugly)", "I faced the same issue because of a huge concat. Thanks to the comments above, I could work around it and avoid calling concat which resolved the issue for me.", "Faced it when use `concat` as `repeat_elements`", "I faced the same problem when I use\r\n`self.embeddingSentences = tf.reshape(tf.concat(self.embeddingSentences, axis=0), [config.batchSize, -1, config.model.hiddenSizes[-1] * 2])`  in Ubuntu 18.04, 12G GPU RAM, tf 1.12.0,\r\nto concat a list length 64  of 2D tensors which size were [15, 512]  to reshape it to a 3D tensors of [64, 15, 512] to be the batch input of the following Bi-LSTM network.\r\n\r\nAfter see the comment of \r\n\r\n> I faced the same issue because of a huge concat. Thanks to the comments above, I could work around it and avoid calling concat which resolved the issue for me.\r\n\r\nI change the code to \r\n`self.embeddingSentences = tf.reshape(self.embeddingSentences, [config.batchSize, -1, config.model.hiddenSizes[-1] * 2])`\r\neven though I am not sure if it is the right useage, **but this error disappear.**\r\n\r\nso I do think this error will be raised when we use huge concat.\r\n\r\nBut it may also be raised by other methods.", "I got the same issue, then I tried to replace the `tf.concat` with `tf.stack` function and the issue disappeared. I am doing so because I am concatenating into a new dimension. Hope this can help.", "I implement a custom `tf.feature_column.input_layer` and encounter the problem.\r\nand I reference the official implement, and add a `tf.reshape` to each tensor before concat, then the error disappeared!", "> I implement a custom `tf.feature_column.input_layer` and encounter the problem.\r\n> and I reference the official implement, and add a `tf.reshape` to each tensor before concat, then the error disappeared!\r\n\r\nHi Colin, that's interesting, could you share more about your findings about this?", "> > I implement a custom `tf.feature_column.input_layer` and encounter the problem.\r\n> > and I reference the official implement, and add a `tf.reshape` to each tensor before concat, then the error disappeared!\r\n> \r\n> Hi Colin, that's interesting, could you share more about your findings about this?\r\n\r\nHi @beihaifusang \r\nthis is part of my code, and bottom comment contain more info, hope to be helpful to you.\r\n\r\n```python\r\n\r\n\r\ndef build_input_layer(feature_spec, features):\r\n    with tf.variable_scope(name_or_scope=\"input_layer\"):\r\n        output_tensors = []\r\n        for name, prop in feature_spec.items():\r\n            if \"num_unique\" in prop[\"attr\"] and prop[\"attr\"][\"num_unique\"] <= 500:\r\n                tensor = tf.one_hot(features[name], depth=prop[\"attr\"][\"num_unique\"], name=\"one_hot_\" + name)\r\n                batch_size = array_ops.shape(tensor)[0]\r\n                tensor = array_ops.reshape(\r\n                    tensor, shape=(batch_size, prop[\"attr\"][\"num_unique\"]))\r\n                output_tensors.append(tensor)\r\n            else:\r\n                bucket_size = 10 ** 3\r\n                embedding_size = 32\r\n                num_unique = prop[\"attr\"][\"num_unique\"] if \"num_unique\" in prop[\"attr\"] else 0\r\n\r\n                embeddings = tf.get_variable(name=\"emb\" + name,\r\n                                             dtype=tf.float32,\r\n                                             shape=[1000, 32])\r\n                tensor = tf.nn.embedding_lookup(\r\n                    embeddings, features[name],\r\n                    name=\"lookup_table_\" + name)  # shape(batch_size, max_seq_len, embedding_size)\r\n                batch_size = array_ops.shape(tensor)[0]\r\n                tensor = array_ops.reshape(\r\n                    tensor, shape=(batch_size, 32))\r\n                output_tensors.append(tensor)\r\n\r\n        \"\"\"\r\n        log error:\r\n        E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:666] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.\r\n        reference code:\r\n        locate: tensorflow/python/feature_column/feature_column.py\r\n\r\n            ...\r\n            batch_size = array_ops.shape(tensor)[0]\r\n            output_tensor = array_ops.reshape(\r\n                tensor, shape=(batch_size, num_elements))\r\n            output_tensors.append(output_tensor)\r\n            if cols_to_vars is not None:\r\n              # Retrieve any variables created (some _DenseColumn's don't create\r\n              # variables, in which case an empty list is returned).\r\n              cols_to_vars[column] = ops.get_collection(\r\n                  ops.GraphKeys.GLOBAL_VARIABLES,\r\n                  scope=variable_scope.get_variable_scope().name)\r\n            if cols_to_output_tensors is not None:\r\n              cols_to_output_tensors[column] = output_tensor\r\n        _verify_static_batch_size_equality(output_tensors, ordered_columns)\r\n        return array_ops.concat(output_tensors, 1)\r\n        \"\"\"\r\n\r\n    return array_ops.concat(output_tensors, 1, name=\"concat_input_layer\")\r\n```", "I got this error as I was using tf.keras.backend.repeat_elements (TF=1.13.1). If the tensor is of type int32/int64, it is possible to use tf.tile instead. \r\n\r\nIn my case, I wanted to repeat and concat a binary 1-dim tensor, A, to be used for filtering another tensor. I managed to get the same result as with repeat_elements without the error with:\r\n`A = tf.expand_dims(A, axis=1)`\r\n`A = tf.tile(A, [1, num_repeats])`\r\n\r\nIt should also be possible to run tf.tile and then tf.reshape, but at least in my simple case what I wrote above was sufficient. tf.tile also supports higher dims:\r\nhttps://www.tensorflow.org/api_docs/python/tf/tile", "Any updates how to solve this problem ??", "For what it's worth, I have a similar issue in TF 2.0.0 with concatenate and K.repeat_elements.   Nothing complicated and it should work, but complained about sorting and gave other errors when attempting to predict or train.  In my case I was able to eliminate the repeat elements and allow automatic broadcasting to do the equivalent thing, but this won't always be possible.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24816\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24816\">No</a>\n"]}, {"number": 24815, "title": "If a branch contains summaries, tf.cond doesn't support Operations as branches", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave (10.14.2)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-rc2-3-ga6d8ffae09 1.12.0\r\n- Python version: 3.6.6 (Anaconda)\r\n\r\n**Describe the current behavior**\r\nThe code below produces a `TypeError`. The bug seems to rely on a combination of:\r\n1. `tf.cond`\r\n2. a `tf.contrib.summary` in one of the branches \r\n3. `tf.group`\r\n\r\n**Describe the expected behavior**\r\nThe code should run successfully.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nwriter = tf.contrib.summary.create_file_writer('tb')\r\nwith writer.as_default(), tf.contrib.summary.always_record_summaries():\r\n    op = tf.cond(\r\n        tf.random.normal([]) >= 0,\r\n        lambda: tf.group(tf.contrib.summary.scalar('loss', 0.2)),\r\n        tf.no_op)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(tf.contrib.summary.summary_writer_initializer_op())\r\nsess.run(op)\r\n```\r\n\r\n**Other info / logs**\r\nHere's the traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"repro.py\", line 8, in <module>\r\n    tf.no_op)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2086, in cond\r\n    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1941, in BuildCondBranch\r\n    original_result)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 381, in map_structure\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 381, in <listcomp>\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 81, in identity\r\n    return gen_array_ops.identity(input, name=name)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3454, in identity\r\n    \"Identity\", input=input, name=name)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 513, in _apply_op_helper\r\n    raise err\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 510, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 6168, in _operation_conversion_error\r\n    name, as_ref))\r\nTypeError: Can't convert Operation 'cond/group_deps' to Tensor (target dtype=None, name='input', as_ref=False)\r\n```", "comments": ["tf.cond seems to [handle](https://github.com/tensorflow/tensorflow/blob/35c49e7fc952d5c86e7e506eea468c4e2c7df49a/tensorflow/python/ops/control_flow_ops.py#L1820) the case when an Operation is returned but does [not](https://github.com/tensorflow/tensorflow/blob/35c49e7fc952d5c86e7e506eea468c4e2c7df49a/tensorflow/python/ops/control_flow_ops.py#L1814) when summaries are added. We should do it in both places.", "> tf.cond seems to [handle](https://github.com/tensorflow/tensorflow/blob/35c49e7fc952d5c86e7e506eea468c4e2c7df49a/tensorflow/python/ops/control_flow_ops.py#L1820) the case when an Operation is returned but does [not](https://github.com/tensorflow/tensorflow/blob/35c49e7fc952d5c86e7e506eea468c4e2c7df49a/tensorflow/python/ops/control_flow_ops.py#L1814) when summaries are added. We should do it in both places.\r\n\r\nIs issue sorted any further changes required?", "@saxenasaurabh, May I have try for this? And If you would like, please give me some reference, Thanks.", "Created pull request, https://github.com/tensorflow/tensorflow/pull/27635", "Wher Would I get source code  for it\r\n", "this issue is opened or closed I am not getting it", "> Wher Would I get source code for it\r\n\r\n@KeshavJB5 You can go to my fork, and clone the repository, then build tensorflow from binaries. Sorry for delayed answer.", "Hello everyone, \r\nIs someone already working on this? I have not done any open source contributions  to tensorflow yet. I'd like to take this issue if no one is currently working. is that okay?", "Please send a PR. I'd be happy to review.", "Hi @sakshiganeriwal , are you working on this? if not, could I work on it?  Thanks ", "@lsgrep I am working on it. Will send a PR in 2 days. Since its my first open source, I am trying to understand a few things. If i am not able to issue one in 2 days you can go ahead then :)", "@sakshiganeriwal  great then. please keep up the good work :)", "@saxenasaurabh  Hi, can I take on this issue? ", "Sure.", "@saxenasaurabh PR has been merged.  ", "@lsgrep fantastic! Thank you!"]}, {"number": 24814, "title": "TensorFlow binary was not compiled to use : AVX2 AVX512F FMA", "body": "Using the virtualenv install procedure (https://www.tensorflow.org/install/pip)\r\n(venv) user@ubuntu:~$ pip install --upgrade tensorflow\r\n\r\nInstalling collected packages: tensorflow\r\nSuccessfully installed tensorflow-1.12.0\r\n\r\n```\r\n(venv) user@ubuntu:~$ python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\n\r\n2019-01-09 21:39:35.073563: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\ntf.Tensor(276.40582, shape=(), dtype=float32)\r\n```\r\n\r\nI don't know if this is a warning or that TensorFlow is in fact installed.\r\nUbuntu 18.04\r\n\r\n```\r\n(venv) user@ubuntu:~$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.12.0-0-ga6d8ffae09 1.12.0\r\n```\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Its a warning message which you can safely ignore and continue using TensorFlow. Your TF installation is successful.", "Thanks!\n\nOn Wed, Jan 9, 2019 at 4:16 PM ymodak <notifications@github.com> wrote:\n\n> Its a warning message which you can safely ignore and continue using\n> TensorFlow. Your TF installation is successful.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24814#issuecomment-452888371>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Ap7Sxb6F8YLS0IxFh-XtbSpmUHGdqhEoks5vBmovgaJpZM4Z4ZF8>\n> .\n>\n", "I tried to build tensorflow from source, AVX2 and FMA is fixed but still getting AVX512F warning.\r\nPlease suggest the procedure or build parameter to fix AVX512F warning.", ">>> tf.__version__sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n2019-03-28 22:10:46.054232: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nDevice mapping: no known devices.\r\n2019-03-28 22:10:46.063724: I tensorflow/core/common_runtime/direct_session.cc:317] Device mapping:\r\n\r\nwhat is the above instruction meaning?\r\ndid i install tf-gpu correctly?", "> I tried to build tensorflow from source, AVX2 and FMA is fixed but still getting AVX512F warning.\r\n> Please suggest the procedure or build parameter to fix AVX512F warning.\r\n\r\nHi , has you fix it?\r\nI had the same problem and build tensorflow from source on intel xeon with:\r\nbazel build --config=opt --config=mkl --copt=\"-DEIGEN_USE_VML\" //tensorflow/tools/pip_package:build_pip_package\r\nbut when train with tensorflow still getting this warnning: not compiled to use: AVX512F", "`\r\n\r\n>>> device_name = tf.test.gpu_device_name() 2020-05-16 23:45:25.485785: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA >>> if not device_name: ...   raise SystemError('GPU device not found') ... print('Found GPU at: {}'.format(device_name)) SystemError: GPU device not found Found GPU at:\r\n--\r\n\u00a0\r\n>>> | >>>\r\n>>>\r\n\r\n`\r\n\r\nHi..! I am also facing this issue. ", "Please open new issue, fill in template", "2020-05-28 11:16:19.791171: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-05-28 11:16:19.819834: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3599930000 Hz\r\n2020-05-28 11:16:19.820503: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x467ba80 executing computations on platform Host. Devices:\r\n2020-05-28 11:16:19.820524: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\ntf.Tensor(736.5775, shape=(), dtype=float32)\r\n"]}, {"number": 24813, "title": "[TensorFlow Java] Fix iterable bug in generated code", "body": "The code generated in TensorFlow Java (for instance Merge and Concat) is incorrect because the generated files expect a single input where the C++ expects an Input List. This is because the iterable boolean in TypeOf gets reset during the call to TypesOf. By moving the original if statement lower, we fix this bug without changing the intent of the code.", "comments": ["Issue here: https://github.com/tensorflow/tensorflow/issues/24483\r\n\r\n", "Thanks @asimshankar for the quick reply and for setting up the framework for testing this! Let me know how it looks now"]}]