[{"number": 25640, "title": "Remove unnecessary use of getattr and setattr", "body": "This PR remove unnecessary use of `getattr` and `setattr`.\r\n\r\nFrom [`flake8-bugbear`](https://github.com/PyCQA/flake8-bugbear#list-of-warnings):\r\n\r\n> Do not call `getattr(x, 'attr')`, instead use normal property access: `x.attr`. Missing a default to `getattr` will cause an `AttributeError` to be raised for non-existent properties. There is no additional safety in using `getattr` if you know the attribute name ahead of time.\r\n>\r\n> Do not call `setattr(x, 'attr', val)`, instead use normal property access: `x.attr = val`. There is no additional safety in using `setattr` if you know the attribute name ahead of time.", "comments": ["@alextp , @ebrevdo request you to approve the PR if it looks okay", "This PR currently accumulates a lot of merge conflicts. If anyone wants to push this through the finish line, I am happy to reopen."]}, {"number": 25639, "title": "I want to reshape Tensor of shape[None,19,19,25] to [None,19,19,5,5] .Can anyone help me out", "body": "I want to reshape Tensor of shape[None,19,19,25] to [None,19,19,5,5] .Can anyone help me out", "comments": ["import numpy as np\r\nimport tensorflow as tf\r\na=tf.placeholder(tf.int32,shape=[None,19,19,25],name='Const')\r\na2 = tf.reshape(a, [-1, 19,19,5,5])           # -1 means \"all\"\r\na2.shape\r\n\r\nIn future, Please post this kind of support questions in Stackoverflow as there is a big community to support.  GitHub is mainly for Build/Installation and Bug/Performance issues. Thanks!", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25639)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25639)\r\n"]}, {"number": 25638, "title": "Can't build tensorflow master with CUDA & MPI enabled", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.22.0 (tried with 0.21.0, 0.20.0, 0.19.0)\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- Nvidia Driver: 410.48\r\n- CUDA/cuDNN version: 10.0.130 / 7.4.2\r\n- GPU model and memory: Tesla V100\r\n\r\nI use nvidia-docker/docker to build and test the master branch of tensorflow. Since a few days, I consistently get the same error over and over.\r\n\r\n```shell\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ntensorflow/contrib/mpi/mpi_rendezvous_mgr.cc: In lambda function:\r\ntensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:125:49: error: no matching function for call to 'tensorflow::TensorResponse::InitPartial(const tensorflow::RecvTensorResponse&)'\r\n           tr.InitPartial(mpi_response.response());\r\n                                                 ^\r\nIn file included from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:31:0:\r\n./tensorflow/core/distributed_runtime/tensor_coding.h:79:8: note: candidate: void tensorflow::TensorResponse::InitPartial(const tensorflow::RecvTensorResponse&, const tensorflow::AllocationAttributes&)\r\n   void InitPartial(const RecvTensorResponse& response,\r\n        ^\r\n./tensorflow/core/distributed_runtime/tensor_coding.h:79:8: note:   candidate expects 2 arguments, 1 provided\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/contrib/mpi/mpi_utils.h:25,\r\n                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:34,\r\n                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:467:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:441:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:441:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 555.689s, Critical Path: 278.37s\r\nINFO: 7384 processes: 7384 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nI build using the following:\r\n\r\n```dockerfile\r\nENV LD_LIBRARY_PATH='/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64/:/usr/local/cuda/extras/CUPTI/lib64' \\\r\n    BLAS_INCLUDE='/usr/local/cuda/targets/x86_64-linux/include' \\\r\n    BLAS_LIB='/usr/local/cuda/targets/x86_64-linux/lib' \\\r\n    CPLUS_INCLUDE_PATH='/usr/local/cuda/$CPLUS_INCLUDE_PATH' \\\r\n    CUDA_TOOLKIT_PATH=\"/usr/local/cuda\" \\\r\n    GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\" \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=5.2,6.0,6.1,7.0,7.5 \\\r\n    TF_CUDA_VERSION=10.0 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NCCL_VERSION=2 \\\r\n    TF_ENABLE_XLA=1 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_JEMALLOC=1 \\\r\n    TF_NEED_HDFS=1 \\\r\n    TF_NEED_MPI=1 \\\r\n    TF_NEED_VERBS=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n    TF_NEED_GDR=0 \\\r\n    TF_NEED_GCP=0 \\\r\n    TF_NEED_S3=0 \\\r\n    TF_NEED_TENSORRT=0\r\n\r\n# Get the TF branch for later build\r\nRUN rm -rf /opt/tensorflow && \\\r\n    git clone --branch=$TF_BUILD_BRANCH --depth=1 $TF_REPO /opt/tensorflow && \\\r\n    cd /opt/tensorflow && \\\r\n    rm -f /usr/local/cuda/lib64/stubs/libcuda.so.1 && \\\r\n    ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 && \\\r\n    echo \"/usr/local/cuda/lib64\" > /etc/ld.so.conf.d/cuda.conf && \\\r\n    echo \"/usr/local/cuda/targets/x86_64-linux/lib/stubs\" > /etc/ld.so.conf.d/cuda-stubs.conf && \\\r\n    bazel clean && \\\r\n    ldconfig && \\\r\n    tensorflow/tools/ci_build/builds/configured GPU \\\r\n    bazel build -c opt --copt=-mavx --config=cuda --verbose_failures \\\r\n        --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" \\\r\n        --action_env=LD_LIBRARY_PATH=${LD_LIBRARY_PATH} \\\r\n        tensorflow/tools/pip_package:build_pip_package && \\\r\n    mkdir /opt/tensorflow/pip_pkg && \\\r\n    bazel-bin/tensorflow/tools/pip_package/build_pip_package /opt/tensorflow/pip_pkg --gpu && \\\r\n    pip --no-cache-dir install --upgrade /opt/tensorflow/pip_pkg/tensorflow_*.whl && \\\r\n    rm -f /usr/local/cuda/lib64/stubs/libcuda.so.1 && \\\r\n    rm -rf /root/.cache && \\\r\n    rm -rf opt/tensorflow/pip_pkg\r\n```\r\n\r\nMy script is perfectly working for the branches:  `r1.12` and `r1.13`. and fails on `master`. I guess that you have updated something, because it seems that your CI builds are still passing.", "comments": ["I had to downgrade to bazel 0.18.1 in order to get it to build with a similar configuration.\r\n\r\nOnly difference was nvidia driver being 415.25 and using gcc 7.3.0.\r\n\r\nHowever, I don't know if 4.10.85 has cuda 10 support and if that would effect building tensorflow.", "There is no chance you can build `master` branch with Bazel 0.18.1. At least not without modifying it ...\r\n\r\n```\r\nCloning into '/opt/tensorflow'...\r\nExtracting Bazel installation...\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/root/.cache/bazel/_bazel_root/install/cdf71f2489ca9ccb60f7831c47fd37f1/_embedded_binaries/A-server.jar) to field java.lang.String.value\r\nWARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\n/opt/tensorflow /opt/tensorflow\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/root/.cache/bazel/_bazel_root/install/cdf71f2489ca9ccb60f7831c47fd37f1/_embedded_binaries/A-server.jar) to field java.lang.String.value\r\nWARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.18.1 installed.\r\nPlease upgrade your bazel installation to version 0.19.0 or higher to build TensorFlow!\r\n```", "Seems to be caused by [this](https://github.com/tensorflow/tensorflow/commit/2dcb0a07c88ce34196d8fe55ab8f415e537e1484#diff-e7a427f0a288fee9dca5408b8d4f5ef6). `TensorResponse::InitPartial` now takes `AllocationAttributes`. If you pass in the default `AllocationAttributes` from `tensorflow/core/framework/allocator.h` it ends up compiling fine (I compiled `r1.96`).\r\n\r\nI think @poxvoculi would have a better idea of how he wants to proceed. I'm not sure if the default `AllocationAttributes` are sensible (e.g., do we want to log failures? retry failures?). I'm also not sure if he would want a PR to `tensorflow` or `tensorflow/networking` now that contrib is going away.\r\n\r\nNote that all the CI builds don't enable MPI hence why they are succeeding. You could either disable MPI or apply the patch I'll post below. Hopefully, this helps.\r\n\r\n<details> \r\n<summary>\r\nmpi.patch\r\n</summary>\r\n\r\n```patch\r\ndiff --git a/tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc b/tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc\r\nindex b9967fe76d..67a76c5856 100644\r\n--- a/tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc\r\n+++ b/tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc\r\n@@ -24,6 +24,7 @@ limitations under the License.\r\n #include <utility>\r\n #include <vector>\r\n \r\n+#include \"tensorflow/core/framework/allocator.h\"\r\n #include \"tensorflow/core/common_runtime/device.h\"\r\n #include \"tensorflow/core/common_runtime/device_mgr.h\"\r\n #include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\r\n@@ -122,7 +123,7 @@ void MPIRemoteRendezvous::RecvFromRemoteAsync(\r\n         } else {\r\n           TensorResponse tr;\r\n           tr.InitAlloc(dst_device, recv_args.alloc_attrs);\r\n-          tr.InitPartial(mpi_response.response());\r\n+          tr.InitPartial(mpi_response.response(), AllocationAttributes());\r\n           const size_t nBytes = tr.tensor().TotalBytes();\r\n           void* data = const_cast<void*>(DMAHelper::base(&tr.tensor()));\r\n           MPI_Status status;\r\n```\r\n</details>", "@jbedorf mind to take a look here?", "The patch from Jesse seems to be the correct way to solve it. I'll submit the patch next week.", "This has been fixed in: https://github.com/tensorflow/tensorflow/pull/26720\r\n", "I am closing the issue as it was resolved. Please open a new issue if the bug persists in future. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25638\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25638\">No</a>\n"]}, {"number": 25637, "title": "c++ backend thread pool overhead", "body": "Is there a configuration to eliminate the intra and inter thread pool and always do computation in C++ client threads that call session->run().", "comments": ["@newpoo Could you describe more details about the approach and features you are looking? Thanks! ", "@jvishnuvardhan If possible, essentially we'd like to eliminate the two thread pools (inter_op and intra_op pools) inside tensorflow library , and do model computation in the same thread that calls session->run(). Because we run online inference on CPU, and we'd like to reduce the context switch introduced by these two thread pools.", "You can try to set the thread pool sizes to 1, but this will only buy you so much. The intra-op pool is relatively harmless, but the inter-op pool is necessary to avoid deadlocks in some cases. Note also that TF has more thread pools, like the event manager pool.\r\n\r\nInstead I recommend you look at using tf lite, which doesn't use threads by default.", "We were facing a similar issue where context switch from service threads to inter_op threads was very high. \r\n\r\nFound the `set_inter_op_thread_pool` in `tensorflow::RunOptions` on line https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L632 \r\n\r\nWhen this flag was set to `RunOptions.set_inter_op_thread_pool(-1)` before calling the `DirectSession::Run` we were able to bypass the inter-op thread-pool during graph execution. \r\n\r\nThe graph execution was driven by the caller thread as expected, which helped us resolve this issue. \r\n\r\nAlthough this still creates the inter-op thread pool, we were able disable it's effect by setting the pool size to 1 and bypassing it using the `RunOptions.set_inter_op_thread_pool(-1)`. \r\n"]}, {"number": 25636, "title": "1.13.0rc1 requires numpy >=1.16.0 instead of >=1.13.3 stated in setup.py in r1.13 branch???", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 1.13.0rc1\r\n- Python version: 3.7.1\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10 / 7.4.2\r\n- GPU model and memory: MX150 2GB\r\n\r\n**Describe the problem**\r\nTensorflow 1.13.0rc1 said the numpy requires >=1.13.3 (I had np 1.15.4) in setup.py but it only works with >=1.16.0. Either the setup.py is wrong or there is something wrong while building the wheel??\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```py\r\nimport tensorflow\r\n```\r\n\r\n**Any other info / logs**\r\n```py\r\nPyBfloat16_Type.tp_base != nullptr\r\n```", "comments": ["Sorry for the delay in resolving the issue @henrysky. Could you check whether this is an issue with the latest TF? Thanks!", "In some cases, I think there are compatibility issues between modules when you install TF using conda. Could you uninstall python and tensorflow and reinstall following the instructions [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12). Please let me know how it progresses. Thanks!", "I follow the instruction to use official python builds  instead of conda in a VM.\r\n\r\nFor python 3.6 and 3.7 I have tested, the error still the same when using numpy 1.15, but gone when using numpy 1.16", "@henrysky Please check required packages [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py) for TF1.13.1. In future, double check version of required packages before you install TF through Anaconda. Thanks!\r\n\r\nREQUIRED_PACKAGES = [\r\n'absl-py >= 0.7.0',\r\n'astor >= 0.6.0',\r\n'gast >= 0.2.0',\r\n'google_pasta >= 0.1.2',\r\n'keras_applications >= 1.0.6',\r\n'keras_preprocessing >= 1.0.5',\r\n'numpy >= 1.14.5, < 2.0',\r\n'six >= 1.10.0',\r\n'protobuf >= 3.6.1',\r\n'tensorboard >= 1.13.0, < 1.14.0',\r\n'tensorflow_estimator >= 1.13.0rc0, < 1.14.0rc0',\r\n'termcolor >= 1.1.0',\r\n]", "This is a clean windows 10 in VM. Using python instead of anaconda python\r\n\r\nnumpy 1.15.4 which tensorflow failed\r\n![image](https://user-images.githubusercontent.com/28623434/53762334-d7052a80-3e95-11e9-8aaa-8af054666c10.png)\r\n\r\nnumpy 1.16.2 which tensorflow okay\r\n![image](https://user-images.githubusercontent.com/28623434/53762466-3105f000-3e96-11e9-93f5-a4d965f109b1.png)", "@jvishnuvardhan I check with the latest tensorflow 2.0.0a0 same issue is there too", "```python\r\nIn [1]: import numpy\r\n\r\nIn [2]: numpy.version\r\nOut[2]: <module 'numpy.version' from '/Users/khbyun/anaconda3/envs/w4/lib/python3.6/site-packages/numpy/version.py'>\r\n\r\nIn [3]: numpy.version.version\r\nOut[3]: '1.15.1'\r\n\r\nIn [4]: import tensorflow\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nModuleNotFoundError: No module named 'numpy.core._multiarray_umath'\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nImportError: numpy.core.multiarray failed to import\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nSystemError                               Traceback (most recent call last)\r\n~/anaconda3/envs/w4/lib/python3.6/importlib/_bootstrap.py in _find_and_load(name, import_)\r\n\r\nSystemError: <class '_frozen_importlib._ModuleLockManager'> returned a result with an error set\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nImportError: numpy.core._multiarray_umath failed to import\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nImportError: numpy.core.umath failed to import\r\n2019-03-19 16:46:12.380357: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr\r\nAbort trap: 6\r\n```\r\n\r\nAccording to numpy issue 11871, tensorflow >= 1.13 should depends on numpy >= 1.16 ... \r\n\r\nhttps://github.com/numpy/numpy/issues/11871", "It happens on macOS, too. Not only on windows.", "@av8ramit Hi, Amit. Do you know the problem?", "The \"ModuleNotFoundError: No module name 'numpy.core._multiarray_umath' occurs when trying to use tensorflowjs_converter", "I can confirm that the error is still occuring. numpy==1.15.4 is not compatible with tensorflow==1.13.1, despite tensorflow v1.13.1 requiring only numpy>=1.13.3 ", "Going to fix this in 1.13 (with upcoming patch release), 1.14 (with upcoming patch release) and master (for new releases)", "Also notes that there is a workaround where you install numpy before tnesorflow or with the constraint at the same time as tensorflow: `pip install numpy && pip install tensorflow` or `pip install numpy >= 1.16 tensorflow == 1.13`"]}, {"number": 25635, "title": "Load to flash", "body": "", "comments": ["@ymodak  Please help proceeding with the next steps as I've some access issues(I'm trying to resolve)."]}, {"number": 25634, "title": "Always call PyObject_ClearWeakRefs", "body": "This seems to work in Python 2 and be necessary for correctness in Python 3. weakreflist is not set and yet there are weakrefs which need to be cleared.\r\n\r\nOddly it's only the weak key dictionary that fails in Python 3 before removing the conditional; using weakref.ref on its own seems fine.\r\n\r\nPiperOrigin-RevId: 229645639", "comments": ["This was cherry picked in https://github.com/tensorflow/tensorflow/pull/25671"]}, {"number": 25633, "title": "Mention 'num_parallel_calls' bugfix into 1.13 release note", "body": "Mention Bug #19945 in the release note of TF 1.13.\r\n\r\n>  Make `num_parallel_calls` of `tf.data.Dataset.interleave` and `tf.data.Dataset.map` work in Eager mode.\r\n", "comments": []}, {"number": 25632, "title": "Tensorflow v1.13.0-rc1 compilation fails.  Bazel 0.19.0", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: Source. \r\n- TensorFlow version:  v1.13.0-rc1\r\n- Python version: 3.6\r\n- Installed using:  within a conda virtual environment\r\n- Bazel version: 0.19.0 /  0.19.2 / 0.20.0 / 0.21.0  (same results)\r\n- GCC/Compiler version: 4.8.5\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: RTX 2070 8,192 MB\r\n\r\n**Describe the problem**\r\nThe compilation fails.  After running \r\n` bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...`\r\nI get the following output:\r\n`\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nERROR: /home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD:9:1: file '//tensorflow/python/tools/api/generator:api_init_files.bzl' does not contain symbol 'KERAS_API_INIT_FILES'\r\nERROR: /home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD:10:1: file '//tensorflow/python/tools/api/generator:api_init_files_v1.bzl' does not contain symbol 'KERAS_API_INIT_FILES_V1'\r\nERROR: /home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD:16:20: Traceback (most recent call last):\r\n\tFile \"/home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD\", line 12\r\n\t\tgen_api_init_files(name = \"keras_python_api_gen\", api...\", <5 more arguments>)\r\n\tFile \"/home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD\", line 16, in gen_api_init_files\r\n\t\tKERAS_API_INIT_FILES_V1\r\nname 'KERAS_API_INIT_FILES_V1' is not defined\r\nERROR: /home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD:33:20: Traceback (most recent call last):\r\n\tFile \"/home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD\", line 28\r\n\t\tgen_api_init_files(name = \"keras_python_api_gen_com...\", <7 more arguments>)\r\n\tFile \"/home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD\", line 33, in gen_api_init_files\r\n\t\tKERAS_API_INIT_FILES_V1\r\nname 'KERAS_API_INIT_FILES_V1' is not defined\r\nERROR: /home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD:50:20: Traceback (most recent call last):\r\n\tFile \"/home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD\", line 45\r\n\t\tgen_api_init_files(name = \"keras_python_api_gen_com...\", <7 more arguments>)\r\n\tFile \"/home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD\", line 50, in gen_api_init_files\r\n\t\tKERAS_API_INIT_FILES\r\nname 'KERAS_API_INIT_FILES' is not defined (did you mean 'gen_api_init_files'?)\r\nERROR: package contains errors: tensorflow/python/keras/api\r\nDEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nDEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:224:17: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:224:17: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:224:17: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:224:17: @//tensorflow/tools/lib_package:eager_cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:224:17: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:224:17: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nERROR: error loading package 'tensorflow/python/keras/api': Package 'tensorflow/python/keras/api' contains errors\r\nINFO: Elapsed time: 3.717s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (417 packages loaded)\r\n`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`\r\n  cd ~/tensorflow\r\n ./configure \r\n bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...\r\n`\r\n\r\n**Additional Info**\r\nSome weeks ago I compiled successfully tf v1.12 (and If i remember correctly also tf v1.13-rc0 with Cuda 10.0  and a previous Bazel version (0.17.2). \r\nThis time (with v1.13-rc1) I first got an error saying that I needed Bazel >= 0.19.0  so thats why i installed that version. But now I have those Error messages...", "comments": ["I am closing this issue. After deleting the tensorflow directory and cloning it again I don't get those errors anymore. Looks like there were some Bazel generated files in the folder (not tracket by git) that were causing the problems.    I have not been able to compile but now the errors are different.  \r\n ", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25632)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25632)\r\n"]}, {"number": 25630, "title": "Fix a typo preventing GPU images from installing CUDA", "body": "Due to an incredibly obtuse quirk of the Dockerfile language, the GPU\nimages weren't getting built properly after the recent ARG changes.", "comments": ["(FYI, @tjakob)", "Interesting, and thanks for the heads up."]}, {"number": 25629, "title": "tf.reshape function error using lambda layer in tensorflow backend keras", "body": "Example - size of spectrum is (2240,1)\r\ndef rehape(spectrum):\r\n    N_Sub=32\r\n   spectrum=tf.manip.reshape(encode_complex,[(tf.size(encode_complex)/N_Sub),N_Sub])\r\n   return spectrum\r\nThe resulting size is (2240,1) instead of (70,32)\r\nCan anyone help me where I am going wrong?\r\n<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["> Example - size of spectrum is (2240,1)\r\n> def rehape(spectrum):\r\n> N_Sub=32\r\n> spectrum=tf.manip.reshape(encode_complex,[(tf.size(encode_complex)/N_Sub),N_Sub])\r\n> return spectrum\r\n\r\nWhat is encode_complex? If you're trying to reshape `spectrum`, the command should be...\r\n`spectrum = tf.manip.reshape(spectrum, [(tf.size(spectrum) / N_Sub), N_Sub])`\r\n\r\nAlso, please keep in mind that this panel is meant for \"code/doc bugs, performance issues, feature requests and build/installation issues\". Your issue is essentially a question for [stackoverflow](https://stackoverflow.com/).", "I tried that still doesn't work. I get same output", "@pachpandepriti \r\nspectrum=tf.zeros([2240,1], tf.int32) \r\nN_Sub=32\r\nspectrum = tf.manip.reshape(spectrum, [(tf.size(spectrum) / N_Sub), N_Sub])\r\n\r\nPlease note that reshape fun looks for a tensor as input. The above worked for me and the final shape was (70,32) as you are expecting.\r\n\r\nIn future, please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25629)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25629)\r\n"]}, {"number": 25628, "title": "Allow functions of `DistributedVariable`s to implement `custom_gradient`", "body": "Previously, variables created by functions decorated with `custom_gradient` had to pass:\r\n```python \r\nisinstance(v, resource_variable_ops.ResourceVariable)\r\n```\r\nWith this patch, they instead must pass:\r\n```python\r\nresource_variable_ops.is_resource_variable(v)\r\n```\r\nThis allows functions that create [`DistributedVariable`](https://github.com/tensorflow/tensorflow/blob/a799b815e5b497fd85e36ec36e44df0f3cebf802/tensorflow/python/distribute/values.py#L429)s (which already implement the method [`_should_act_as_resource_variable`](https://github.com/tensorflow/tensorflow/blob/a799b815e5b497fd85e36ec36e44df0f3cebf802/tensorflow/python/distribute/values.py#L568)) to use `custom_gradient`, which is necessary for cross-replica batch normalization, among other things.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "> Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\r\n\r\nSigned the CLA", "CLAs look good, thanks!\n\n<!-- ok -->", "@alextp, I'm not familiar with this code. Are you the right person to review this?", "The change looks good, but I'd also love a unit test for it.", "I would be happy to submit a unit test; however since there is not yet any `custom_gradient_test.py`, it seems befitting of a separate PR. "]}, {"number": 25627, "title": "[ROCm] Misc updates to ROCm-specific ci_build files ", "body": "Misc updates to the ROCm-specific `ci_build` files `Dockerfile.rocm` and `rocm/run_py3_core.sh`.  This is needed to support our Community Supported Build efforts.  ", "comments": ["@tatianashp and @gunan, this is another PR to trim community build for ROCm path. Along with #25596 it'd be very helpful if you could expedite the review for this PR so we can make TensorFlow community supported builds for AMD ROCm platform online.", "@gunan - Made the commit.  Is that what you were thinking?  (the follow-on step will be to submit another PR with the `no_rocm` tags to the specific unit tests)", "Yes, looks great. Thanks!", "@hgadig FYI. none of the failures in those 3 failing targets seem to be related to this PR.", "> @hgadig FYI. none of the failures in those 3 failing targets seem to be related to this PR.\r\n\r\nYes. We are looking into this and helping to get merged."]}, {"number": 25626, "title": "Adding Python3.7 to release notes.", "body": "", "comments": ["@aselle FYI\r\nWe will need to copy from the new release notes for the final release. But you do not need to rebuild."]}, {"number": 25625, "title": "Dependence of output on batch size", "body": "The response from a convolutional neural network is sometimes dependent on the batch size. \r\n\r\nCode to reproduce \r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    reuse = False\r\n    init = tf.initializers.variance_scaling()\r\n    X = tf.placeholder(\r\n        dtype=tf.float32,\r\n        shape=[None] + [86]*3 + [1])\r\n\r\n    with tf.variable_scope(\"level1\"):\r\n        net1 = tf.layers.conv3d(X, 16, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init )\r\n\r\n    with tf.variable_scope(\"level2\"):\r\n        net2_in = tf.layers.conv3d(net1, 32, 2, strides=2, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='ds', kernel_initializer=init )\r\n        net2 = tf.layers.conv3d(net2_in, 32, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init )\r\n\r\n    with tf.variable_scope(\"level3\"):\r\n        net3_in = tf.layers.conv3d(net2, 64, 2, strides=2, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='ds', kernel_initializer=init )\r\n        net3 = tf.layers.conv3d(net3_in, 64, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init )\r\n\r\n    with tf.variable_scope(\"level4\"):\r\n        net4_in = tf.layers.conv3d(net3, 128, 2, strides=2, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='ds', kernel_initializer=init )\r\n        net4 = tf.layers.conv3d(net4_in, 128, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init)\r\n\r\n    with tf.variable_scope(\"level5\"):\r\n        net5 = tf.layers.conv3d_transpose(net4, 64, 2, strides=2, activation='elu',padding='same',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='trans', kernel_initializer=init)\r\n        net5 = tf.concat([net5, tf.slice(net3,[0]+[(18-14)//2]*3+[0],[-1]+[14]*3+[64])], axis=4) #tf.slice(net8, begin, tf.shape(net18))\r\n        net5 = tf.layers.conv3d(net5, 128, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init)\r\n\r\n    with tf.variable_scope(\"level6\"):\r\n        net6 = tf.layers.conv3d_transpose(net5, 32, 2, strides=2, activation='elu',padding='same',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='trans', kernel_initializer=init)\r\n        net6 = tf.concat([net6, tf.slice(net2,[0]+[(40-24)//2]*3+[0],[-1]+[24]*3+[32])], axis=4) #tf.slice(net5, begin, tf.shape(net18))\r\n        net6 = tf.layers.conv3d(net6, 64, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init)\r\n\r\n    with tf.variable_scope(\"level7\"):\r\n        net7 = tf.layers.conv3d_transpose(net6, 16, 2, strides=2, activation='elu',padding='same',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='trans', kernel_initializer=init)\r\n        net7 = tf.concat([net7, tf.slice(net1,[0]+[(84-44)//2]*3+[0],[-1]+[44]*3+[16])], axis=4) #tf.slice(net5, begin, tf.shape(net18))\r\n        net7 = tf.layers.conv3d(net7, 32, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init)\r\n\r\n    with tf.variable_scope(\"out\"):\r\n        net_out = tf.layers.conv3d(net7, 2, 1, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init)\r\n\r\n    model = net_out\r\n\r\n    sess = tf.Session()\r\n    sess.run(tf.local_variables_initializer())\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    batch_size = 34\r\n    full_batch = np.random.rand(154,86,86,86,1)\r\n    num_batches = (len(full_batch)//batch_size)\r\n    if len(full_batch) % batch_size: num_batches += 1\r\n    y_out_list = []\r\n    print(\"Batch size of 34\")\r\n\r\n    for i in range(0, num_batches):\r\n        batch = full_batch[i*batch_size:min(len(full_batch), (i+1)*batch_size)]\r\n        batch1 = full_batch[2*i*batch_size//2:min(len(full_batch), (2*i+1)*batch_size//2)]\r\n        batch2 = full_batch[(2*i+1)*batch_size//2:min(len(full_batch), (2*i+2)*batch_size//2)]\r\n        y_out = sess.run(model, feed_dict={X:batch})\r\n        y_out1 = sess.run(model, feed_dict={X:batch1})\r\n        y_out2 = sess.run(model, feed_dict={X:batch2})\r\n        print(\"Should be true if independent of batch size: \", np.all(y_out==np.concatenate([y_out1,y_out2])), \" Shape: \", y_out.shape)\r\n\r\n    batch_size = 20\r\n    full_batch = np.random.rand(154,86,86,86,1)\r\n    num_batches = (len(full_batch)//batch_size)\r\n    if len(full_batch) % batch_size: num_batches += 1\r\n    y_out_list = []\r\n    print(\"Batch size of 20\")\r\n\r\n    for i in range(0, num_batches):\r\n        batch = full_batch[i*batch_size:min(len(full_batch), (i+1)*batch_size)]\r\n        batch1 = full_batch[2*i*batch_size//2:min(len(full_batch), (2*i+1)*batch_size//2)]\r\n        batch2 = full_batch[(2*i+1)*batch_size//2:min(len(full_batch), (2*i+2)*batch_size//2)]\r\n        y_out = sess.run(model, feed_dict={X:batch})\r\n        y_out1 = sess.run(model, feed_dict={X:batch1})\r\n        y_out2 = sess.run(model, feed_dict={X:batch2})\r\n        print(\"Should be true if independent of batch size: \", np.all(y_out==np.concatenate([y_out1,y_out2])), \" Shape: \", y_out.shape)\r\n\r\n    batch_size = 42\r\n    full_batch = np.random.rand(154,86,86,86,1)\r\n    num_batches = (len(full_batch)//batch_size)\r\n    if len(full_batch) % batch_size: num_batches += 1\r\n    y_out_list = []\r\n    print(\"Batch size of 42\")\r\n\r\n    for i in range(0, num_batches):\r\n        batch = full_batch[i*batch_size:min(len(full_batch), (i+1)*batch_size)]\r\n        batch1 = full_batch[2*i*batch_size//2:min(len(full_batch), (2*i+1)*batch_size//2)]\r\n        batch2 = full_batch[(2*i+1)*batch_size//2:min(len(full_batch), (2*i+2)*batch_size//2)]\r\n        y_out = sess.run(model, feed_dict={X:batch})\r\n        y_out1 = sess.run(model, feed_dict={X:batch1})\r\n        y_out2 = sess.run(model, feed_dict={X:batch2})\r\n        print(\"Should be true if independent of batch size: \", np.all(y_out==np.concatenate([y_out1,y_out2])), \" Shape: \", y_out.shape)\r\n\r\n\r\n\r\n== cat /etc/issue ===============================================\r\nLinux  4.4.0-142-generic #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 4.4.0-142-generic #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.3)\r\nnumpydoc (0.7.0)\r\nprotobuf (3.5.2.post1)\r\ntensorflow (1.8.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = b'v1.8.0-3-gf91bd2f'\r\ntf.COMPILER_VERSION = b'v1.8.0-3-gf91bd2f'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Feb  8 18:17:16 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 970     Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   46C    P8    16W / 200W |    383MiB /  4036MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN Xp            Off  | 00000000:03:00.0 Off |                  N/A |\r\n| 23%   30C    P2    58W / 250W |  11750MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/lib64/libcudart_static.a\r\n/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n\r\n== cat /etc/issue ===============================================\r\nLinux  4.4.0-142-generic #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 4.4.0-142-generic #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy               1.15.4    \r\nprotobuf            3.6.1     \r\ntensorflow          1.9.0     \r\ntensorflow-fold     0.0.1     \r\ntensorflow-gpu      1.12.0    \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.12.0\r\ntf.GIT_VERSION = v1.12.0-0-ga6d8ffae09\r\ntf.COMPILER_VERSION = v1.12.0-0-ga6d8ffae09\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Feb  8 18:18:45 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 970     Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   44C    P8    16W / 200W |    383MiB /  4036MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN Xp            Off  | 00000000:03:00.0 Off |                  N/A |\r\n| 23%   30C    P2    58W / 250W |  11750MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/lib64/libcudart_static.a\r\n/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n\r\n\r\n", "comments": ["@plooney, I am not sure that I understand your issue. Can you provide a more detailed explanation? Do you mean to say that your convolutional network produces slightly different outputs when trained using batches of slightly different sizes? That is supposed to happen. Batch sizes affect the values obtained by trainable parameters.  ", "@Sudeepam97 Perhaps I am missing something. In the above example there is no training. I appreciate the effect of batch size on training. \r\n\r\nI am using a patch based approach to do image segmentation. When I have trained my model I break my images up into patches pass these through the CNN and combine the outputs to give the final output. The same model will give a different output depending on the batch size. The answer should not depend on the batch size as far as I understand it. \r\n\r\nThe example above shows that given a model and evaluating the model the batch size of 34 is distinct to that of 17 but should be the same. 42 is different to 21 but should be the same. This only happens for certain batch sizes. \r\n\r\nIn the following code I would expect all the elements in tensors to be the same. For lower values of integers some are. \r\n\r\n    tensors = []\r\n    full_batch = np.random.rand(154,86,86,86,1)\r\n    for batch_size in range(1,50):\r\n        num_batches = (len(full_batch)//batch_size)\r\n        if len(full_batch) % batch_size: num_batches += 1\r\n        y_out_list = []\r\n        print(\"Batch size of \", batch_size)\r\n\r\n        for i in range(0, num_batches):\r\n            batch = full_batch[i*batch_size:min(len(full_batch), (i+1)*batch_size)]\r\n            y_out = sess.run(model, feed_dict={X:batch})\r\n            y_out_list = np.concatenate([y_out_list,y_out]) if not y_out_list == [] else y_out\r\n        tensors.append(y_out_list)\r\n    for i, t in enumerate(tensors):\r\n        print(i, np.all(t==tensors[0]))\r\n\r\nThe output is \r\n\r\n0 True\r\n1 True\r\n2 True\r\n3 True\r\n4 True\r\n5 True\r\n6 False\r\n7 True\r\n8 True\r\n9 True\r\n10 True\r\n11 True\r\n12 True\r\n13 True\r\n14 True\r\n15 True\r\n16 True\r\n17 True\r\n18 True\r\n19 True\r\n20 False\r\n21 True\r\n22 True\r\n23 True\r\n24 True\r\n25 True\r\n26 True\r\n27 True\r\n28 True\r\n29 True\r\n30 False\r\n31 False\r\n32 False\r\n33 False\r\n34 False\r\n35 False\r\n36 False\r\n37 False\r\n38 False\r\n39 False\r\n40 False\r\n41 False\r\n42 False\r\n43 False\r\n44 False\r\n45 False\r\n46 False\r\n47 False\r\n48 False", "Many floating point operations are not deterministic on GPU and they also have limited precision. So it's very common to not obtain bit-wise identical results.", "@ppwwyyxx I seem to get consistent and expected results for certain batch sizes but not others. Is your explanation consistent with that observation?", "That's reasonable as well. Tensorflow (or cudnn) may choose to use one implementation for certain batch sizes and another implementation for others.", "@ppwwyyxx Consider the following\r\n\r\n    for i, t in enumerate(tensors):\r\n        print(i, np.mean(np.abs(t))/np.mean(np.abs(tensors[0])))\r\n\r\nThe output is \r\n\r\n0 1.0\r\n1 1.0\r\n2 1.0\r\n3 1.0\r\n4 1.0\r\n5 1.0\r\n6 1.0\r\n7 1.0\r\n8 1.0\r\n9 1.0\r\n10 1.0\r\n11 1.0\r\n12 1.0\r\n13 1.0\r\n14 1.0\r\n15 1.0\r\n16 1.0\r\n17 1.0\r\n18 1.0\r\n19 1.0\r\n20 1.0\r\n21 1.0\r\n22 1.0\r\n23 1.0\r\n24 1.0\r\n25 1.0\r\n26 1.0\r\n27 1.0\r\n28 1.0\r\n29 1.0\r\n30 0.98064667\r\n31 1.0\r\n32 1.0\r\n33 0.9253961\r\n34 1.0\r\n35 1.0\r\n36 1.0\r\n37 1.0\r\n38 1.0\r\n39 0.98133874\r\n40 0.9965034\r\n41 0.866809\r\n42 1.0\r\n43 1.0\r\n44 1.0\r\n45 1.0\r\n46 0.8182643\r\n47 1.0\r\n48 1.0\r\n\r\nSo fairly large differences for batch sizes of 34, 42, 47", "That's some very interesting results to see. Running the same code on my environment gives the ratios all between 0.9999 and 1.0.\r\n\r\nThis seems to be more of an environment issue, then. It might help to upgrade cudnn because AFAIK cudnn mentions in several release notes about improvement/bugfix of numerical issues.", "Upgraded to cudnn v7.4.2 with similar results\r\n\r\n0 1.0\r\n1 1.0\r\n2 1.0\r\n3 1.0\r\n4 1.0\r\n5 1.0\r\n6 1.0\r\n7 1.0\r\n8 1.0\r\n9 1.0\r\n10 1.0\r\n11 1.0\r\n12 1.0\r\n13 1.0\r\n14 1.0\r\n15 1.0\r\n16 1.0\r\n17 1.0\r\n18 1.0\r\n19 1.0\r\n20 1.0\r\n21 1.0\r\n22 1.0\r\n23 1.0\r\n24 1.0\r\n25 1.0\r\n26 1.0\r\n27 1.0\r\n28 1.0\r\n29 1.0\r\n30 0.967046\r\n31 1.0\r\n32 1.0\r\n33 0.9185082\r\n34 1.0\r\n35 1.0\r\n36 1.0\r\n37 1.0\r\n38 1.0\r\n39 0.97963786\r\n40 0.992207\r\n41 0.85242534\r\n42 1.0\r\n43 1.0\r\n44 1.0\r\n45 1.0\r\n46 0.79884666\r\n47 1.0\r\n48 1.0", "@ppwwyyxx Can you share details on your environment? ", "On CPU all are true and equal to 1", "I've seen the same issue in training when using  tf.data.TFRecordDataset. For certain values of batch size fails to train", "Running this on docker produces all 1s", "Closing this issue as it is not bug/performance or build/install type. Please post this kind of support questions in Stackoverflow. There is a big community to help answering support type questions. Thanks!", "@jvishnuvardhan Why isn't this a build or install issue? I have seen the same issue across two other machines. I don't see this is relevant to Stack overflow since it is not a programming problem. The problem is not present in Docker so I am using that as a workaround. "]}, {"number": 25624, "title": "Converter for TRT NMS using TRT plugin registry", "body": "- Load TRT plugin registry dynamically (dlopen).\r\n- Initialize TRT plugin registry always (even if the model doesn't need any plugin).\r\n- Add a converter for CombinedNonMaxSuppression\r\n- Add unit tests. ", "comments": ["@pooyadavoodi Would this speed up the `NonMaxSuppressionV3` layer (implemented via `tf.image.non_max_suppression`)? My application is currently bottlenecked by this layer. ", "@pooyadavoodi can you please check failed build", "@rthadur I'll handle that."]}, {"number": 25623, "title": "behaviour of tf.GPUOptions(allow_growth=False) seems different", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.5.2:\r\n- CUDA/cuDNN version: cuda-9.0, cudnn-7.3.1\r\n- GPU model and memory: Titan Xp, Titan V\r\n\r\n**Describe the current behavior**\r\n\r\nIn previous version, 1.10.0 wich I used, **per_process_gpu_memory_fraction** is occupied as soon as session is created.\r\n\r\nHowever with this version (1.12.0), GPU memory occupied after a single **sess.run()**.\r\n\r\n\r\n```\r\nconfig = tf.ConfigProto(\r\n        allow_soft_placement=True,\r\n        log_device_placement=False,\r\n        gpu_options=tf.GPUOptions(force_gpu_compatible=True,\r\n                                  per_process_gpu_memory_fraction=0.5,\r\n                                  allow_growth=False))\r\n\r\na = tf.constant(1)\r\nsess = tf.Session(config=config)\r\n# <- GPU memory is not fully occupied (v.1.12.0) \r\n#       but it was occupied right after in previous version (v.1.10.0)\r\nsess.run(a)\r\n# <- GPU memory is now fully occupied (v.1.12.0)\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nAfter session is created gpu memory should be fully occupied as specified in a variable **per_process_gpu_memory_fraction**\r\n\r\n\r\n**Code to reproduce the issue**\r\nshown above\r\n\r\n", "comments": ["Thanks for filing this issue. Could you also explain your preference for the old behavior?\r\n\r\nI haven't tried to reproduce this yet but adding some devs who may know if there was any change in the memory allocator implementation that may have caused this. \r\n@aaroey @poxvoculi \r\n", "Exactly what do you mean by GPU memory being fully occupied?  How are you testing that?  And, most importantly, why would the old behavior be preferable?", "What I mean by that if I set **per_process_gpu_memory_fraction** to 0.5, 50% of GPU memory is occupied as soon as session is created in v1.10.0 but in v1.12.0 only small fraction of GPU memory, say 5% of memory, is occupied right after session is created. \r\n\r\nThis new behaviour is easily addressed by executing arbitrary ops or variables in session right after it is created. That is if I run any ops or variables, 50% of GPU memory is successfully occupied right after a run.\r\n\r\nIn my case, before training any models, I should do some time-consuming computations before actual training or evaluation. I usually train 3~5 models on single GPU at once due to resource limitation and the machine is shared with teammates. Because of computations before training, I occupy certain amount of memory sufficient for my model before computations by opening a session to use certain portion of GPU exclusively. If not, other colleague unexpectedly came in and occupy GPU memory resulting in out of memory error after time-consuming computation which is annoying and waste of time.", "Thanks for the explanation.\r\n\r\nAs a workaround, did you consider running a dummy operation immediately after creating the session to make sure that memory is reserved immediately after your process starts?", "Yes, that's exactly what I do for the time being.\r\n\r\nBut the old behaviour makes more sense to me. Shouldn't memory be occupied as soon as session is created if **allow_growth** option is False?", "GPU allocators are BFCAllocators with a suballocator that gets large regions of GPU memory.  bfc_allocator.cc is also used for CPU memory.  In that second role it is capable of growing the regions it manages, on demand.  When used for GPU memory we constrain it to grab a single large region in once.  In either case, the first memory region is allocated by the suballocator on demand, the first time an actual AllocateRaw request is presented to the allocator.\r\n\r\nWhat likely caused the change you're seeing is that the allocation of Eigen GPU scratch buffers was moved from GPUDevice creation time to ReinitializeGPUDevice call time.  Hence now you see that the GPU region is not allocated until the first allocation triggered by your TF program.  That was done to break a difficult ordering problem related to properly configuring all the allocators.\r\n\r\n "]}, {"number": 25622, "title": "Remove unnecessary use of six.iterkeys", "body": "This PR replaces unnecessary use of:\r\n- `for key in six.iterkeys(dict)` with `for key in dict`\r\n- `list(iterkeys(dict))` with `list(dict)`\r\n- `sorted(iterkeys(dict))` with `sorted(dict)`\r\n\r\nto improve readability.", "comments": ["@alextp Sorry, I pushed another related commit to this branch. I haven't seen your review.", "I just noticed that we have python/util/nest* and python/data/util/nest* now. Ugh. This is unrelated to this PR.", "@ymodak  Please help proceeding with the next steps as I've some access issues(I'm trying to resolve)."]}, {"number": 25621, "title": "TopKV2 Op incorrectly requiring scalar input for k", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.2\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): ('v1.12.0-rc2-3-ga6d8ffae09', '1.12.0')\r\n- Python version: 2.7.10\r\n\r\n\r\n**Describe the current behavior**\r\nI want to be able to compute the top k results for different values of k in a batched evaluation metric. (e.g. for sample 1, compute top 2, sample 2 compute top 1, etc.)\r\ntf.nn.top_k is throwing a `ValueError` despite clearly using the `TopKV2` op under the hood.\r\n\r\nthe following toy example illustrates the problem:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ny_true = tf.constant([[0., 1., 1., 0], [0., 1., 0., 0.], [0., 0., 0., 1]])  \r\ny_pred = tf.constant([[.1, 0.4, 0.4, .1], [.1, .7, .1, .1], [.1, .1, .1, .7]])\r\n\r\n(confidences, indices) = tf.nn.top_k(y_pred, tf.constant([2, 1, 2]))\r\n```\r\n\r\nI get the following error: \r\n\r\n`ValueError: This Input must be scalar but has rank 1 for 'TopKV2_20' (op: 'TopKV2') with input shapes: [3,4], [3] and with computed input tensors: input[1] = <2 1 2>`\r\n\r\nDiving into the source code, it would seem that the TopKV2 op is _precisely_ intended for varying values of k, however, something at a higher level is preventing my code from utilizing this functionality. Additionally, there is a `top_kv2` function in `gen_nn_ops.py` that is not exported for use.\r\n\r\n**Describe the expected behavior**\r\n\r\nThat the TopKV2 op functions as specified, or that the `top_kv2` python binding is exposed.\r\n\r\n\r\n", "comments": ["@wickstopher the tf.nn.top_k op is expecting a scalar and you are providing a tensor. Please check an implementation [here](https://stackoverflow.com/questions/40808772/tensorflow-top-n-values-in-tensor) and [here](https://www.tensorflow.org/api_docs/python/tf/math/top_k) is the TF resource.\r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 25620, "title": "MirroredStrategy, dataset sharding, notion of keras epoch - inconsistent and confusing", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Both\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 1080 Ti\r\n\r\n**Background**\r\n\r\nWhile I understand that in machine learning community the notion/concept of epoch may vary a bit, one of the more widely adopted definition is - an epoch is to go over all the samples of the dataset once. To some extent it is consistent with keras api as where in the fit API we provide 'steps_per_epoch' as an indicator of when an 'epoch' is to be considered finished and most of the time steps_per_epoch is computed  to be equal to the number of batches of your training dataset.\r\n\r\n[I also appreciate that this definition is bit lose because if one is performing augmentation then above given definition of epoch starts to get bit murky]\r\n\r\n**Setup**\r\n\r\nI am trying to perform multi-gpu training using MirroredStrategy and Keras. My setup is - one machine with multiple gpus. In other words, not using the ClusterSpec i.e. not really \"distributed\" amongst many different machines.\r\n\r\n**Observed Behavior**\r\n\r\nI would expect that if I have a dataset of 100 samples and 2 GPUs, MirroredStrategy would distribute 50 samples to each GPU. An epoch will be done when both GPUs would have seen 50 samples each. Each GPU will see a portion of dataset reserved for it. In other words, they would see distinct samples.\r\n\r\nA quick look at the API (https://www.tensorflow.org/api_docs/python/tf/contrib/distribute/MirroredStrategy) makes you believe that if you set `auto_shard_dataset` to True you may achieve above goal. However, after looking in the source code I can see that `auto_shard_dataset` is taken into consideration when a multi-worker/cluster setup is used (https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/contrib/distribute/python/mirrored_strategy.py#L481).\r\n\r\nThis means that in my setup every GPUs would get 100 samples therefore the notion of 1 epoch in this case is essentially going over the dataset `twice` (but in parallel)\r\n\r\n**Expected Behavior**\r\n\r\nIt is often interesting to evaluate the performance of a network after 1 epoch (given the more widely accepted definition/convention of epoch amongst others i.e. an epoch is to go over the dataset once). \r\n\r\nGiven that MirroredStrategy does take the parameter `auto_shard_dataset` i.e. you (the tensorflow authors) did take notion of dividing the dataset in parts in consideration,  my question would be why it is not being used for the setup (1 machine multiple gpus) described above ? \r\n\r\n", "comments": ["Hi @ksachdeva, I know this is a late response, but in case this is all still confusing, I hope this explanation helps.\r\n\r\nIf using tf.distribute.MirroredStrategy in TF2 with the Keras API, the behavior is what you expect. If you have 100 examples in a batch, and 2 GPUs, then each GPU will receive 50 examples. You can see this point [documented here](https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_tfkerasmodelfit). In multi-worker training, sharding data into multiple parts is needed to ensure convergence and performance, and the tf.distribute.Strategy API takes care of this automatically in a multi-worker set up at the file level (although you can disable the auto_shard_policy) but this is not needed when just using a single machine with multiple GPUs.\r\n\r\n", "Thanks @nikitamaia "]}, {"number": 25619, "title": "Could I possibly count the detected object in tensorflow?", "body": "here is my code \r\n\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport sys\r\n\r\nsys.path.append(\"..\")\r\n\r\nfrom utils import label_map_util\r\nfrom utils import visualization_utils as vis_util\r\nfrom api import object_counting_api\r\n\r\nMODEL_NAME = 'inference_graph'\r\nIMAGE_NAME = 'tree.png'\r\n\r\nCWD_PATH = os.getcwd()\r\n\r\nPATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\r\n\r\nPATH_TO_LABELS = os.path.join(CWD_PATH,'training','labelmap.pbtxt')\r\n\r\nPATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\r\n\r\nNUM_CLASSES = 2\r\n\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n    od_graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n        serialized_graph = fid.read()\r\n        od_graph_def.ParseFromString(serialized_graph)\r\n        tf.import_graph_def(od_graph_def, name='')\r\n\r\n    sess = tf.Session(graph=detection_graph)\r\n\r\nimage_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n\r\ndetection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n\r\ndetection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\ndetection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n\r\nnum_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n\r\nimage = cv2.imread(PATH_TO_IMAGE)\r\nimage_expanded = np.expand_dims(image, axis=0)\r\n\r\n(boxes, scores, classes, num) = sess.run(\r\n    [detection_boxes, detection_scores, detection_classes, num_detections],\r\n    feed_dict={image_tensor: image_expanded})\r\n\r\nvis_util.visualize_boxes_and_labels_on_image_array(\r\n    image,\r\n    np.squeeze(boxes),\r\n    np.squeeze(classes).astype(np.int32),\r\n    np.squeeze(scores),\r\n    category_index,\r\n    use_normalized_coordinates=True,\r\n    line_thickness=1,\r\n    min_score_thresh=0.50)\r\n\r\ncv2.imshow('Object detector', image)\r\n\r\ncv2.waitKey(0)\r\n\r\ncv2.destroyAllWindows()\r\n", "comments": ["You can count the number of objects detected/ bounding boxes printed by making slight change in visualization_utils which is placed in the utils folder which you are importing.\r\nIn the visualization_utils, search for the function \"**visualize_boxes_and_labels_on_image_array**\".\r\nIn the function there is a for loop  stated like \"**for box, color in box_to_color_map.items():**\".\r\nTo count the number of objects detected all you have to do is count the number of times the for loop runs when call the function.\r\nIn my repository the line number of the above for loop is : **720**\r\n\r\nDo tell if it worked ;)", "@teffanymae Please try @aniketbote solution.\r\nAs this is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Thank you very much!! T_T\r\n\r\nwhat I did is add this code at **756**\r\n\r\n  tree = (len(box_to_color_map.items()))\r\n  cv2.putText(image,'Tree count: ' + str(tree),(10,150),font,0.5,(255,255,0),1,cv2.LINE_AA)", "I think it was resolved. I am closing the issue. In future, please post this kind of support questions in Stackoverflow. Thanks!", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25619)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25619)\r\n", "> Thank you very much!! T_T\r\n> \r\n> what I did is add this code at **756**\r\n> \r\n> tree = (len(box_to_color_map.items()))\r\n> cv2.putText(image,'Tree count: ' + str(tree),(10,150),font,0.5,(255,255,0),1,cv2.LINE_AA)\r\n\r\nhow to increment all count in the videos and finally print the total of all count \r\n\r\n@teffanymae  @aniketbote "]}, {"number": 25618, "title": "Docker image tensorflow/tensorflow:latest-gpu failed to use nvidia/cuda image", "body": "Hello guys,\r\n\r\nI am facing some problems when running image tensorflow/tensorflow:latest-gpu. When I try to execute the command ( which is found in [docs](https://www.tensorflow.org/install/docker#examples_using_gpu-enabled_images) ) :\r\n\r\n```shell\r\ndocker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu \\\r\n   python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\n```\r\n\r\nI get the following error:\r\n\r\n```\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting\r\ncontainer process caused \"process_linux.go:424: container init caused \\\"process_linux.go:407: running\r\nprestart hook 1 caused \\\\\\\"error running hook: exit status 1, stdout: , stderr: exec command: \r\n[/usr/bin/nvidia-container-cli --load-kmods configure --ldconfig=@/sbin/ldconfig.real --device=all --\r\ncompute --utility --require=cuda>=10.0 brand=tesla,driver>=384,driver<385 --pid=7142 \r\n/var/lib/docker/overlay2/c2f4ac7d7d905051e77682447a97f563cbdf7ccf2fe43afccdb521350adab0f4/mer\r\nged]\\\\\\\\nnvidia-container-cli: requirement error: unsatisfied condition: brand = tesla\\\\\\\\n\\\\\\\"\\\"\": unknown.\r\n\r\n```\r\nIt may be caused by not properly installed NVidia driver, as seen [here](https://devtalk.nvidia.com/default/topic/1046289/cuda-setup-and-installation/command-quot-docker-run-runtime-nvidia-rm-nvidia-cuda-9-0-base-nvidia-smi-quot-fails-with-error-/).\r\nBut turns out that when I try to run the following command ( also seen in [docs](https://www.tensorflow.org/install/docker#gpu_support) ):\r\n\r\n```shell\r\ndocker run --runtime=nvidia --rm nvidia/cuda nvidia-smi\r\n```\r\n\r\nIt gives me the same error. The above command only runs correctly when I run it with the nvidia/cuda:9.0-base image, specified in NVidia [github README](https://github.com/NVIDIA/nvidia-docker#ubuntu-140416041804-debian-jessiestretch):\r\n\r\n```shell\r\ndocker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi\r\n```\r\n\r\nI think it is a problem with tensorflow:lastest-gpu, which may be linking its image with nvidia/cuda:latest  instead of nvidia/cuda:9.0-base image.\r\nIs anyone else facing the same issue?\r\n\r\nMy configuration:\r\nUbuntu 18.04.1\r\nGeForce 930M\r\nDriver nvidia-driver-390 version 390.77, installed via Softwares and Updates.\r\n\r\n## EDIT\r\nTF version: Latest docker image tensorflow/tensorflow:latest-gpu which following the [docs](https://www.tensorflow.org/install/docker) does not need CUDA installed, just the driver.\r\n\r\n ", "comments": ["@lucasbsimao Please fill the [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Which TF version, CUDA/cuDNN, python etc you installed? Thanks!", "> @lucasbsimao Please fill the [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Which TF version, CUDA/cuDNN, python etc you installed? Thanks!\r\n\r\nAccording to [TensorFlow Docker Installation docs](https://www.tensorflow.org/install/docker) there is no need of previous installation of python or CUDA. Just Nvidia driver. ", "A bug (which has been fixed) in our Docker release CI led to the `latest-` tagged images containing CUDA 10 libraries but TF 1.12 (which needs CUDA 9). The images have been updated so that they use CUDA 10 and TF 1.13rc1 (which needs CUDA 10). Sorry about the confusion! This should be resolved now, but please comment again if not.\r\n\r\nI think your driver is too old to support CUDA 10 (see [NVidia's compatibility table](https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver)). If that's the case, you can either use one of the numbered 1.12 tags or upgrade your device driver to a version that supports CUDA 10 and TF 1.13.", "I ran into this issue today on my Ubuntu 18.04 box.\r\n\r\nFixed by installing the CUDA 10 by the following command:\r\n\r\n```sh\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb \\\r\n    && sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub \\\r\n    && sudo apt-get update \\\r\n    && sudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb \\\r\n    && sudo apt-get update \\\r\n    && sudo apt-get install -y cuda\r\n```\r\n\r\n### See\r\n\r\n- [Install CUDA 10 on Ubuntu 18.04](https://deeptalk.lambdalabs.com/t/docker-error-response-from-daemon-oci-runtime-create-failed-nvidia-container-cli-requirement-error-unsatisfied-condition-cuda-10-0/769)\r\n", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25618)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25618)\r\n", "Hello, \r\n\r\nI have the same problem.\r\nI use:\r\nUbuntu 16.04 \r\nGeForce GTX 1080 Ti\r\nnvidia driver version: 384.130\r\n\r\nTensorflow and other cuda related code run without any problem.\r\n\r\nBut in Docker, I have the following problem.\r\nWhen I run\r\n`sudo docker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu  python -c \"import tensorflow as tf; print('Hello world')\"\r\n`\r\nI get the following error:\r\n`docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused \"process_linux.go:424: container init caused \\\"process_linux.go:407: running prestart hook 1 caused \\\\\\\"error running hook: exit status 1, stdout: , stderr: exec command: [/usr/bin/nvidia-container-cli --load-kmods configure --ldconfig=@/sbin/ldconfig.real --device=all --compute --utility --require=cuda>=10.0 brand=tesla,driver>=384,driver<385 --pid=8886 /var/lib/docker/overlay2/43f665d0f341d27c84a4341e45f405ea2aba058a4be79c421cdf893ae3df459e/merged]\\\\\\\\nnvidia-container-cli: requirement error: unsatisfied condition: brand = tesla\\\\\\\\n\\\\\\\"\\\"\": unknown.\r\n`\r\n\r\n\r\n\r\nWhen I run \r\n`sudo docker run --runtime=nvidia -it --rm nvidia/cuda:9.0-base nvida-smi\r\n`\r\nit runs without an issue:\r\n```\r\nWed Mar  6 21:41:54 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.130                Driver Version: 384.130                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   41C    P5    37W / 250W |      0MiB / 11170MiB |      2%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nSo I tried to to run tensorflow with cuda 9.0 as well:\r\n`sudo docker run --runtime=nvidia -it --rm nvidia/cuda:9.0-base tensorflow/tensorflow:latest-gpu  python -c \"import tensorflow as tf; print('Hello world')\"`\r\n\r\nBut it doesn't allow me to run tensorflow with \"nvidia/cuda:9.0-base\":\r\n`docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused \"exec: \\\"tensorflow/tensorflow:latest-gpu\\\": stat tensorflow/tensorflow:latest-gpu: no such file or directory\": unknown.`\r\n\r\nIs there a way to select cuda version while running tensorflow in docker?", "> Hello,\r\n> \r\n> I have the same problem.\r\n> I use:\r\n> Ubuntu 16.04\r\n> GeForce GTX 1080 Ti\r\n> nvidia driver version: 384.130\r\n> \r\n> Tensorflow and other cuda related code run without any problem.\r\n> \r\n> But in Docker, I have the following problem.\r\n> When I run\r\n> `sudo docker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu python -c \"import tensorflow as tf; print('Hello world')\" `\r\n> I get the following error:\r\n> `docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused \"process_linux.go:424: container init caused \\\"process_linux.go:407: running prestart hook 1 caused \\\\\\\"error running hook: exit status 1, stdout: , stderr: exec command: [/usr/bin/nvidia-container-cli --load-kmods configure --ldconfig=@/sbin/ldconfig.real --device=all --compute --utility --require=cuda>=10.0 brand=tesla,driver>=384,driver<385 --pid=8886 /var/lib/docker/overlay2/43f665d0f341d27c84a4341e45f405ea2aba058a4be79c421cdf893ae3df459e/merged]\\\\\\\\nnvidia-container-cli: requirement error: unsatisfied condition: brand = tesla\\\\\\\\n\\\\\\\"\\\"\": unknown. `\r\n> \r\n> When I run\r\n> `sudo docker run --runtime=nvidia -it --rm nvidia/cuda:9.0-base nvida-smi `\r\n> it runs without an issue:\r\n> \r\n> ```\r\n> Wed Mar  6 21:41:54 2019       \r\n> +-----------------------------------------------------------------------------+\r\n> | NVIDIA-SMI 384.130                Driver Version: 384.130                   |\r\n> |-------------------------------+----------------------+----------------------+\r\n> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n> |===============================+======================+======================|\r\n> |   0  GeForce GTX 108...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n> |  0%   41C    P5    37W / 250W |      0MiB / 11170MiB |      2%      Default |\r\n> +-------------------------------+----------------------+----------------------+\r\n>                                                                                \r\n> +-----------------------------------------------------------------------------+\r\n> | Processes:                                                       GPU Memory |\r\n> |  GPU       PID   Type   Process name                             Usage      |\r\n> |=============================================================================|\r\n> |  No running processes found                                                 |\r\n> +-----------------------------------------------------------------------------+\r\n> ```\r\n> \r\n> So I tried to to run tensorflow with cuda 9.0 as well:\r\n> `sudo docker run --runtime=nvidia -it --rm nvidia/cuda:9.0-base tensorflow/tensorflow:latest-gpu python -c \"import tensorflow as tf; print('Hello world')\"`\r\n> \r\n> But it doesn't allow me to run tensorflow with \"nvidia/cuda:9.0-base\":\r\n> `docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused \"exec: \\\"tensorflow/tensorflow:latest-gpu\\\": stat tensorflow/tensorflow:latest-gpu: no such file or directory\": unknown.`\r\n> \r\n> Is there a way to select cuda version while running tensorflow in docker?\r\n\r\nI think angersson's answer has already showed you the answer. In your case, you cannot select 9.0 cuda because latest tensorflow image doesn't comply with it. But you can select the version of tensorflow image from docker. Which means, when you pulling tensorflow image from docker hub, don't tag the image with \"latest-gpu\" (which do not comply with cuda 9.0). Instead, pull image with TF 1.12-gpu. Then you code may work well.\r\nSo your code maybe should be looked as:\r\n`sudo docker run --runtime=nvidia -it --rm nvidia/cuda:9.0-base tensorflow/tensorflow:1.12.0-gpu python -c \"import tensorflow as tf; print('Hello world')\"`\r\nI am not sure if the tag should be \"1.12.0-gpu\", you need to check this by going to the official github site of tensorflow docker: https://hub.docker.com/r/tensorflow/tensorflow\r\nActually, due to your so powerful GPU, the better choice for you is to upgrade you gpu driver upto larger than 410.48. Then you can use both latest tensorflow image and CUDA 10.0"]}, {"number": 25617, "title": "Updated import_tensorflow.cc", "body": "Fixed the type errors", "comments": ["@pragyaak & @hgadig , i could see import/copy/bara is failed in this PR, but this changes are not related, could you please retrigger.", "@amitsrivastava78 the changes are merged now. Thanks!"]}, {"number": 25616, "title": "How to get the minimum and maximum cuda compute capability for the given version of TensorFlow.", "body": "Hello, is any way to get the supported versions of compute capablity. \r\nAn exemplary function could look like this:\r\n`>>>import tensorflow as tf`\r\n`>>>tf.get_supported_compute_capablity()`\r\n`(3.0, 7.0)`\r\n\r\n**System information**\r\n- TensorFlow version 1.10:\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n", "comments": ["Minimum compute capability can be seen here:\r\nhttps://www.tensorflow.org/install/gpu#hardware_requirements\r\n\r\nIf we are not supporting this value, that means we have a bug.\r\nWhile the API is an interesting idea, I think our docs cover this. And this is a pretty stable value that I don't think a new API symbol is that useful.\r\n\r\n", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25616)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25616)\r\n"]}, {"number": 25615, "title": "Typo error fixed in execute.h", "body": "", "comments": []}, {"number": 25614, "title": "Segmentation fault - Convolution in tflite", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from github:\r\n- TensorFlow version tf 1.13:\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0 20160609\r\n- CUDA : 9.0\r\n- RAM : 32GB\r\n\r\nHi,\r\n\r\nI encountered a segmentation fault in the Im2Col of Convolution while running a tflite model.\r\nI observed that when the Im2Col buffer size >  2GB, this crash occurs. \r\nFor example : \r\ninput : 1x828x1000x32 ; output: 1x828x1000x3  ;kernel: 9x9  does not crash\r\n\r\nbut for \r\ninput:1x832x100x32 ; output: 1x832x1000x3; kernel 9x9  convolution crashes.\r\n\r\n\r\nI want to know if there is any memory limit for convolution in tflite to run ?\r\n\r\nI kindly request you to let me know this. sooner the better\r\n\r\n\r\nAwaiting you reply,\r\nVedavyas.\r\n\r\n", "comments": ["@ved27 Could you share some reproducible code and also please mention what is your OS. Please provide as many details as possible to resolve this faster. Thanks! ", "Hi ,\r\n\r\n I'm unable to share the exact code. However, I am able to reproduce the issue setting the above input shapes and kernel sizes in the testing code of quantized convolution in tflite.\r\n \r\nAwaiting your reply,\r\n", "Hi,\r\n\r\nI kindly request you to look into this soon, as it would be helpful for me to rule out the models which seem to take high memory while executing tflite interpreter.\r\n\r\n\r\nAwaiting your reply,", "Hi,\r\n\r\nI kindly request you to address this issue. It has almost been 9 days. \r\nThankful if someone can give me a confirmation on the memory limitation of tflite \r\n\r\n\r\nRegards,\r\n", "@ved27 : Did you encounter this issue while running some model? If yes, would you be able to share the model? (Just for my reference).\r\n\r\nAny way i have found the root-cause for your issue.\r\n### **NOTE:** As of now TFLite does not have any memory limit for convolution.\r\nI have verified with 3GB memory allocation, there is no issue.\r\n\r\nBut in your case, the **root-cause is classic \"Integer overflow\"**.\r\n\r\nCurrently GEMM supports limited by (row * col < 2^31).\r\nAs in your case it has exceeded the limit, it is leading to memory corruption.\r\n\r\nCurrently i am working on a solution for this, will keep you updated if anything come up.\r\n\r\nWould suggest to use models which has lower memory requirement(lower than 2^31) for Convolution currently.\r\nIf my post helped you, would appreciate if you close this issue.\r\nPlease feel free to post if anything from your end, Thanks.", "Yeah. I debugged and found the root cause earlier.\r\nThere has been inconsistency between the datatype usage in various parts of tflite . (int / size_t / unsigned int ). \r\n\r\nAnd ultimately the GEMM's limitation , causing the crash to occur.  \r\n\r\n I wished a better implementation of convolution in tflite. Implementations other than Im2Col, such as Kn2row can be deployed .\r\n\r\nThank you much for the reply \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25614\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25614\">No</a>\n"]}, {"number": 25613, "title": "Mac OS X can not access clock_gettime", "body": "the function clock_gettime was replaced by clock_get_time", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@MKesenheimer request you to sign the cla in order to proceed with the PR. Thanks.", "Sorry, I tried requesting the CLA but there were errors in the process. I will try again.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25613) for more info**.\n\n<!-- ok -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n\r\nI signed the CLA!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 25612, "title": "Typo error fix devguide.md", "body": "", "comments": []}, {"number": 25611, "title": "Docker images with tags `1.13.0rc0` and `1.13.0rc0-py3` contains wrong version of TensorFlow", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 17.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **-**\r\n- TensorFlow installed from (source or binary): **Binary/Docker**\r\n- TensorFlow version: **???**\r\n- Python version: **2.7**, **3.5**\r\n- Installed using virtualenv? pip? conda?: **Docker**\r\n- Bazel version (if compiling from source): **-**\r\n- GCC/Compiler version (if compiling from source): **-**\r\n- CUDA/cuDNN version: **-**\r\n- GPU model and memory: **-**\r\n\r\nThe problem is that Docker [images](https://hub.docker.com/r/tensorflow/tensorflow/tags) with tags `1.13.0rc0` and `1.13.0rc0-py3` contains wrong version of `tensorflow`.\r\n\r\nJust to show:\r\n```\r\n$ docker run -it tensorflow/tensorflow:1.13.0rc0 bash\r\n\r\n________                               _______________                \r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / \r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\r\n\r\nroot@d1f1dd416bdd:/# pip show tensorflow\r\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\r\nName: tensorflow\r\nVersion: 1.12.0\r\n```", "comments": ["This was caused by a bug in our CI for the Docker images (it didn't check release candidates properly). I've updated the tags manually and fixed the CI.\r\n\r\nThanks for the report!\r\n\r\n```\r\ndocker run -it tensorflow/tensorflow:1.13.0rc0 bash                                                                                                         \r\n                                                                                                                                                              \r\n________                               _______________                                                                                                        \r\n___  __/__________________________________  ____/__  /________      __                                                                                        \r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /                                                                                        \r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ /                                                                                         \r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/                                                                                          \r\n                                                                                                                                                              \r\n                                                                                                                                                             \r\nroot@6c6909f2a600:/# pip show tensorflow                                                                                                                                                                                                                      \r\nName: tensorflow                                                                                                                                              \r\nVersion: 1.13.0rc0 \r\n```\r\n", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25611)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25611)\r\n"]}, {"number": 25610, "title": "Make of benchmark tool", "body": "Is there a make to build benchmark tool instead of bazel?", "comments": ["Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}]