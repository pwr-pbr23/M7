[{"number": 43010, "title": "TF_lite inference error", "body": "\r\n\r\nRuntimeError: tensorflow/lite/kernels/pad.cc:205 op_context.output->params.zero_point != op_context.constant_values->params.zero_point (0 != 255)Node number 2 (PADV2) failed to invoke.\r\n\r\ntensorflow 1.5.0gpu + windows10 +python3.5", "comments": ["@huafeihuayu Could you try conversion with TF 2.3 or tf-nightly? You can export your model to a saved model directory under TF 1.5 and you can try conversion from the generated saved model directory and run the converted model with the recent TF version.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43010\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43010\">No</a>\n", "This issue still exist in TF 2.3.1. \r\n@jdduke , @abattery , please help check it. Thanks!\r\n\r\nWe should reopen it!"]}, {"number": 43009, "title": "Fix cmake build on mingw compiler", "body": "* `/bigobj` is a flag for MSVC compiler, use `-Wa,-mbig-obj` instead for gcc.\r\n* use reinterpret_cast for the cast to void* from FARPROC .", "comments": ["@terryheo Could you take a look at this PR?"]}, {"number": 43008, "title": "R2.3", "body": "in tensorflow\\core\\framework\\op_kernel.h\r\nline 135 :   // Returns nullptr iff this op kernel is synchronous.\r\ncomment fix:   // Returns nullptr if this op kernel is synchronous.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43008) for more info**.\n\n<!-- need_sender_cla -->", "@niupanyu Thank you for your interest. We don't merge release branches back into master. \r\nCC @mihaimaruseac "]}, {"number": 43007, "title": "Fixed SeparableConv1D self._compute_causal_padding missing inputs arg", "body": "inputs argument is missing in a call of self._compute_causal_padding in SeparableConv1D. If you attempt to create a SeparableConv1D with causal padding, the following error message will appear:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__  **\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:386 call\r\n        outputs = layer(inputs, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py:2002 call\r\n        inputs = array_ops.pad(inputs, self._compute_causal_padding())\r\n\r\n    TypeError: _compute_causal_padding() missing 1 required positional argument: 'inputs'\r\n\r\nThis PR fixes it.", "comments": ["\"We are closing this PR as it has been duplicate of [#42466](https://github.com/tensorflow/tensorflow/pull/42466).  Thank you for your interest. \r\ncc @mihaimaruseac"]}, {"number": 43006, "title": "Issues while installing for raspberry pi through pip command", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\nWhen did pip install TensorFlow lite for raspberry pi it started installation but meanwhile showing error and couldn't install the whole package \r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["Please resolve this", "@shreekantgosavi \r\n\r\nCan you please refer this [link](https://www.tensorflow.org/lite/guide/build_rpi) to build tensorflow Lite for Raspberry Pi.Hope this helps. \r\nIf you are still facing issues in installation please share error log along with the exact sequence of commands / steps that you executed before running into the problem\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43005, "title": "Set static shape information for TimeDistributed layer", "body": "Fix for issue #41157.", "comments": []}, {"number": 43004, "title": "Error when build the pip package for Tensorflow 1.8", "body": "\r\n    Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow):No\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n    TensorFlow installed from (source or binary):Source\r\n    TensorFlow version (use command below):1.8\r\n    Python version:2.7.17/3.6.9\r\n    Bazel version (if compiling from source):0.10.0\r\n    GCC/Compiler version (if compiling from source):Tried 6.5, 5.5\r\n    CUDA/cuDNN version: 9.0/7.6.5\r\n    GPU model and memory:GTX 1080\r\n\r\nERROR: /home/yuyangleng/tensorflow/tensorflow/contrib/framework/BUILD:101:1: output 'tensorflow/contrib/framework/_objs/python/ops/_variable_ops_gpu/tensorflow/contrib/framework/kernels/zero_initializer_op_gpu.cu.pic.o' was not created\r\n\r\nERROR: /home/yuyangleng/tensorflow/tensorflow/contrib/framework/BUILD:101:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 79.510s, Critical Path: 9.19s\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n", "comments": ["@YuyangLeng,\r\nTensorFlow 1.x is not actively supported. Could you please try building TensorFlow 2.x and check if you are facing the same issue. \r\n\r\nAlso, make sure you have all the required dependencies installed as per the [tested build configurations](https://www.tensorflow.org/install/source#cpu). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43004\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43004\">No</a>\n"]}, {"number": 43003, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "### System information\r\n\r\n-   **just written import tensorflow**\r\n-   **Windows 10**\r\n-   **TensorFlow installed from pip**\r\n-   **TensorFlow version: Latest**\r\n-   **Python version**: 3.7.3\r\n-   **GPU model and memory**: I am using cpu version\r\n\r\n### Describe the problem\r\nI have been stuck on this problem forever: https://pastebin.com/n7y5cmEQ\r\n\r\nI have downloaded tensorflow on an i7-5600u, 16 gb ram device.\r\n\r\nIf I am missing any info please tell me\r\n\r\nThanks\r\n", "comments": ["Can you check if you are compliat with Windows requirements at https://www.tensorflow.org/install/pip?hl=en#windows", "I don't have the c++ thing, but I am using python,\r\nalso i don't know what is the manylinux2010 support", "Can you try to install TF following the guide steps?", "@kukuquack \r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the latest [microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.\r\nPlease, refer similar issue #42724 #42440 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "So I need to install all 2015,2017,2019 versions of the redistributable?\r\n", "@kukuquack Yes you can install 2019.\r\n\r\nAlso:\r\n> Requires Python 3.5\u20133.8, pip and venv >= 19.0\r\n> Make sure long paths are enabled on Windows.\r\n> Install the 64-bit Python 3 release for Windows (select pip as an optional feature).", "For some reason I can't find gpedit.msc on run.", "You can check this https://www.itechtics.com/enable-gpedit-windows-10-home/", ">>> import tensorflow\r\n2020-09-07 14:26:43.159687: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-09-07 14:26:43.181952: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.", "It works now thanks!", "@kukuquack \r\nThank you for your update, please feel free to move the ticket to closed status if resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43003\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43003\">No</a>\n"]}, {"number": 43002, "title": "Tf.Variable handling when tf.function is set appears unfixably broken", "body": "-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.15.6\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: No\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: v2.3.0-rc2-23-gb36436b087 2.3.0\r\n-   **Python version**: Python 3.7.4\r\n-   **Bazel version (if compiling from source)**: N/A\r\n-   **GCC/Compiler version (if compiling from source)**: N/A\r\n-   **CUDA/cuDNN version**: N/A\r\n-   **GPU model and memory**: N/A\r\n-   **Exact command to reproduce**: run below script\r\n\r\n### Describe the problem\r\nConsider the following case:\r\n\r\n(1) Running keras-style functional models\r\n(2) Training requires that each evaluation step runs against several modified versions of the input data in which different columns are replaced at different times\r\n(3) Running in eager mode is two orders of magnitude slower than running in graph mode, so running in eager mode is flat out.\r\n\r\n### Source code / logs\r\n\r\n(Approximate minimal case extracted from my source; the actual notebook is several thousand lines long and would merely confuse matters.)\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass BrokenLayer(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n        \r\n    def call(self, inputs):\r\n        transposed_inputs = tf.transpose(inputs)\r\n        \r\n        retval = []\r\n        for delta in [[0], [0, 1]]:\r\n            v = tf.Variable(\r\n                np.zeros(len(delta, 1)),\r\n                shape=tf.TensorShape((len(delta), None)))\r\n            \r\n            retval += [\r\n                tf.tensor_scatter_nd_update(\r\n                    tf.transpost(inputs),\r\n                    delta,\r\n                    v)]\r\n\r\n        return retval\r\n\r\n... No run this inside tf.keras.model.fit\r\n\r\nIt blows up because of the classic 'ValueError: tf.function-decorated function tried to create variables on non-first call.'\r\n\r\nThis behavior is by design, according to https://github.com/tensorflow/tensorflow/issues/27120. Unfortunately, despite spending the best part of a day trying to create a by-row constant tensor with a undefined second axis, I have found no way to replace the call to tf.Variable.\r\n\r\nLest it seem like this is a minor issue -- 'just create a Variable at class initialization and reuse that' is the first, obvous, suggestion -- *THAT DOES NOT WORK*. You need to be able to use tensor_scatter_nd_update to get there, and that's not possible, multiplying inputs by zero gives a tensor with the wrong shape. Using tensor array doesn't work, since it requires the each assignment be of known shape, and that brings us back to creating variables.\r\n\r\nThis design is completely broken, in what is not all that surprising a situation.", "comments": ["Is this still in the cluster of https://github.com/tensorflow/tensorflow/issues/42183#issuecomment-671522699?", "None of the work-arounds suggested in https://github.com/tensorflow/tensorflow/issues/27120 work. ", "@jwlm-home \r\n\r\nCan you please share colab link or simple standalone code with  proper indentation  and supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "Colab link to working example:\r\n\r\nhttps://colab.research.google.com/github/jwlm-home/variable_failure/blob/master/example.ipynb\r\n\r\nNotice a couple of things:\r\n\r\n(1) The call function within the custom layer is decorated with a @tf.function decorator. THat's because it will otherwise run in eager mode, in which case the second call to tf.Variable does not raise an error\r\n(2) Other than the custom layer, this uses completely vanilla training; there's no custom training loop here.", "Also:\r\n\r\n(3) To make this run completely correctly in eager mode, you need to add an extra if statement so that the variable initialization handles cases where the shape of the input tensor is fully specified.", "I think that you can try to workaround like: \r\nhttps://github.com/tensorflow/tensorflow/issues/26812#issuecomment-473736883\r\n\r\nSee also @alextp rationale at https://github.com/tensorflow/tensorflow/issues/26812#issuecomment-474595550\r\n", "Incorrect. That depends on the tensors having known dimensions.\n\nOn Mon, Sep 7, 2020 at 10:20 AM bhack <notifications@github.com> wrote:\n\n> I think that you can try to workaround like:\n> #26812 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/26812#issuecomment-473736883>\n>\n> See also @alextp <https://github.com/alextp> rationale at #26812 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/26812#issuecomment-474595550>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43002#issuecomment-688445339>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACLTODZU74FM4BBBFNG2JZTSEUI6TANCNFSM4Q4ZEE7A>\n> .\n>\n", "Is your colab working without `@tf.function`?", "As written, no. I'll add a cell which does work.\n\nOn Mon, Sep 7, 2020 at 10:25 AM bhack <notifications@github.com> wrote:\n\n> Is your colab working without @tf.function?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43002#issuecomment-688446988>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACLTODYDTBC72VI3S6IZ5BTSEUJSLANCNFSM4Q4ZEE7A>\n> .\n>\n", "OK, I've updated the code so it runs in eager mode but not in graph mode.\n\nOn Mon, Sep 7, 2020 at 10:26 AM John Merrill <john.merrill@gmail.com> wrote:\n\n> As written, no. I'll add a cell which does work.\n>\n> On Mon, Sep 7, 2020 at 10:25 AM bhack <notifications@github.com> wrote:\n>\n>> Is your colab working without @tf.function?\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/43002#issuecomment-688446988>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/ACLTODYDTBC72VI3S6IZ5BTSEUJSLANCNFSM4Q4ZEE7A>\n>> .\n>>\n>\n", "Your last formulation will fail also in both modes on `v = tf.Variable(..` with the upcoming version `pip install tf_nighlty`: \r\n```\r\n    TypeError: An op outside of the function building code is being passed\r\n    a \"Graph\" tensor. It is possible to have Graph tensors\r\n    leak out of the function building context by including a\r\n    tf.init_scope in your function building code.\r\n    For example, the following function will fail:\r\n      @tf.function\r\n      def has_init_scope():\r\n        my_constant = tf.constant(1.)\r\n        with tf.init_scope():\r\n          added = my_constant * 2\r\n    The graph tensor has name: broken_layer_8/zeros:0\r\n```", "Quite so. That's because removing the lambda thunk at line 22, yielding\n\n        v = tf.Variable(\n            replacement_values,\n            shape=tf.TensorShape((len(delta), None)))\n\ncauses this error:\n\nValueError: Tensor-typed variable initializers must either be wrapped\nin an init_scope or callable (e.g., `tf.Variable(lambda :\ntf.truncated_normal([10, 40]))`) when building functions. Please file\na feature request if this restriction inconveniences you.\n\n\nIn other words, fixing the leaking graph error causes another error --\nand it's exactly the one called out in the nightly build bug, up to\nthe 'init_scope' call.\n\n\nLet me see what I can do to fix this bug.\n\n\nOn Mon, Sep 7, 2020 at 3:14 PM bhack <notifications@github.com> wrote:\n\n> Your last formulation will fail also in both modes on v = tf.Variable(..\n> with the upcoming version pip install tf_nighlty:\n>\n>     TypeError: An op outside of the function building code is being passed\n>     a \"Graph\" tensor. It is possible to have Graph tensors\n>     leak out of the function building context by including a\n>     tf.init_scope in your function building code.\n>     For example, the following function will fail:\n>       @tf.function\n>       def has_init_scope():\n>         my_constant = tf.constant(1.)\n>         with tf.init_scope():\n>           added = my_constant * 2\n>     The graph tensor has name: broken_layer_8/zeros:0\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43002#issuecomment-688525410>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACLTOD4LCFKV3EXUJUYWD6TSEVLLBANCNFSM4Q4ZEE7A>\n> .\n>\n", "OK, fixed. It now gives the expected tf.variable creation error.\n\nOn Mon, Sep 7, 2020 at 3:38 PM John Merrill <john.merrill@gmail.com> wrote:\n\n> Quite so. That's because removing the lambda thunk at line 22, yielding\n>\n>         v = tf.Variable(\n>             replacement_values,\n>             shape=tf.TensorShape((len(delta), None)))\n>\n> causes this error:\n>\n> ValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable (e.g., `tf.Variable(lambda : tf.truncated_normal([10, 40]))`) when building functions. Please file a feature request if this restriction inconveniences you.\n>\n>\n> In other words, fixing the leaking graph error causes another error -- and it's exactly the one called out in the nightly build bug, up to the 'init_scope' call.\n>\n>\n> Let me see what I can do to fix this bug.\n>\n>\n> On Mon, Sep 7, 2020 at 3:14 PM bhack <notifications@github.com> wrote:\n>\n>> Your last formulation will fail also in both modes on v = tf.Variable(..\n>> with the upcoming version pip install tf_nighlty:\n>>\n>>     TypeError: An op outside of the function building code is being passed\n>>     a \"Graph\" tensor. It is possible to have Graph tensors\n>>     leak out of the function building context by including a\n>>     tf.init_scope in your function building code.\n>>     For example, the following function will fail:\n>>       @tf.function\n>>       def has_init_scope():\n>>         my_constant = tf.constant(1.)\n>>         with tf.init_scope():\n>>           added = my_constant * 2\n>>     The graph tensor has name: broken_layer_8/zeros:0\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/43002#issuecomment-688525410>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/ACLTOD4LCFKV3EXUJUYWD6TSEVLLBANCNFSM4Q4ZEE7A>\n>> .\n>>\n>\n", "I think that in your formulation the problem is that you are trying to create the variable twice cause it is connected to the two branch of the conditional case for `replacement_values`. This will conflict with the rule that you need to protect `tf.variable` against multiple evaluations:\r\n> tf.function may evaluate your python function more than once.\r\nWhat the RFC states instead is that you are allowed to create variables as long as variable creation only happens the first time your python function is evaluated.\r\n", "Yes, and that's exactly the bug. I need to be able to deal with the case\nwhere I must update a set of columns in an input during training. That\nrequires using scatter_nd -- and that, in turn, requires a tf.variable to\nhandle the case where the input has unspecified length -- which is, of\ncourse, the core case for fitting a model to a set of data in graph mode.\nThat, in turn, requires the creation of more than one tf.Variable during\ntraining.\n\nThat is both forbidden (by design) and required (as explained above).\nThat's exactly the bug.\n\n\nOn Mon, Sep 7, 2020 at 5:14 PM bhack <notifications@github.com> wrote:\n\n> I think that in your formulation the problem is that you are trying to\n> create the variable twice cause it is connected to the two branch of the\n> conditional case for replacement_values. This will conflict the rules\n> that you need to protect tf.variable against multiple evaluations:\n>\n> tf.function may evaluate your python function more than once.\n> What the RFC states instead is that you are allowed to create variables as\n> long as variable creation only happens the first time your python function\n> is evaluated.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43002#issuecomment-688549937>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACLTOD5QLANROKYWAZT3K2LSEVZM7ANCNFSM4Q4ZEE7A>\n> .\n>\n", "I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200907`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e2e8f729e35d45c73ceb1c79c873ec20/untitled321.ipynb). Thanks!", "`tf.scatter_nd_update` does work with Tensors. In general, there is no mathematical expression that should require a `tf.Variable` to implement.\r\n\r\nHowever, there is a problem with the original code: you may not mix TF and NumPy ops. In other words, you need to use `tf.zeros` instead of `np.zeros`, `tf.constant` instead of `np.array`, `tf.reshape` instead of `x.reshape`, etc.\r\n\r\nI'm including the corrected code below, which runs without error:\r\n\r\n```\r\n        delta = tf.constant([0])\r\n        \r\n        replacement_values = tf.zeros((len(delta), len(inputs)), dtype=tf.float32)\r\n                    \r\n        retval = tf.transpose(\r\n            tf.tensor_scatter_nd_update(\r\n                transposed_inputs,\r\n                tf.reshape(delta, (-1, 1)),\r\n                replacement_values))\r\n```\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43002\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43002\">No</a>\n", "It's self-evident that you did not even bother to read the error. The error has nothing to do with whether or not tensors can be used -- in fact, the issue arises because tf. Variable is required to make the code work in the general case, when len(inputs) is unavailable due to being called with a tensor of indeterminate dimension, and tf.Variable can only be called once.", "Again, tf.Variable is **not** required to handle indeterminate dimensions. So the premise of this issue is incorrect. If you have specific use-cases that you believe require a tf.Variable, feel free to include them here, although such questions are better asked on StackOverflow."]}, {"number": 43001, "title": "NXP FRDM K66F example for micro speech does not even build ", "body": "System information\r\n\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n    TensorFlow installed from (source or binary): source\r\n    TensorFlow version: bfc8733\r\n    GCC/Compiler version (if compiling from source): arm-none-eabi-g++ 8.2.1\r\n\r\nThe example I'm running can be found here: \r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech#deploy-to-nxp-frdm-k66f\r\n\r\nDescribe the problem\r\nWhen running\r\n\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"nxp_k66f\" generate_micro_speech_mbed_project\r\n\r\nit results in the following error:\r\n\r\n/make/Makefile TARGET=mbed TAGS=\"nxp_k66f\" generate_micro_speech_mbed_project\r\ntensorflow/lite/micro/tools/make/Makefile:313: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:313: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/third_party/gemmlowp/fixedpoint/fixedpoint.h', needed by 'generate_micro_speech_mbed_project'. Stop.\r\n", "comments": ["@petewarden Could you take a look at this issue?", "---\r\n\r\nUpdate\r\nI tried again on my other machine:\r\nOS: Ubuntu 20.04.1 LTS x86_64\r\nKernel: 5.4.0-47-generic\r\nTensorFlow:  b36436b\r\n\r\nThis time the following step(3) worked:\r\n` make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"nxp_k66f\" generate_micro_speech_mbed_project`\r\n\r\nHowever, at step 7, when trying to compile I get the following error: \r\n`mbed compile --target K66F --toolchain GCC_ARM --profile release`\r\n\r\n[mbed] Working path \"/home/carlo/Work/Demos/Speech/3rd/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed\" (library)\r\n[mbed] Program path \"/home/carlo/Work/Demos/Speech/3rd/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed\"\r\n[mbed] WARNING: Missing Python modules were not auto-installed.\r\n       The Mbed OS tools in this program require the following Python modules: click\r\n       You can install all missing modules by running \"pip install -r requirements.txt\" in \"/home/carlo/Work/Demos/Speech/3rd/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/mbed-os\"\r\n       On Posix systems (Linux, etc) you might have to switch to superuser account or use \"sudo\"\r\n/home/carlo/Work/Demos/Speech/3rd/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/mbed-os/tools/config/__init__.py:1154: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n  (unit_kind is \"application\" or\r\nBuilding project mbed (K66F, GCC_ARM)\r\nScan: mbed\r\nCompile [  0.4%]: mbed_tz_context.c\r\nCompile [  0.7%]: CAN.cpp\r\nCompile [  1.1%]: DigitalIn.cpp\r\nCompile [  1.5%]: I2CSlave.cpp\r\nCompile [  1.9%]: DigitalInOut.cpp\r\nCompile [  2.2%]: Ethernet.cpp\r\nCompile [  2.6%]: AnalogOut.cpp\r\nCompile [  3.0%]: BusIn.cpp\r\nCompile [  3.3%]: DigitalOut.cpp\r\nCompile [  3.7%]: BusOut.cpp\r\nCompile [  4.1%]: InterruptManager.cpp\r\n[Warning] InterruptManager.h@204,40: 'CallChain' is deprecated: CallChain has been deprecated and will be removed. [-Wdeprecated-declarations]\r\n[Warning] InterruptManager.cpp@63,60: 'CallChain' is deprecated: CallChain has been deprecated and will be removed. [-Wdeprecated-declarations]\r\n[Warning] InterruptManager.cpp@92,32: 'CallChain' is deprecated: CallChain has been deprecated and will be removed. [-Wdeprecated-declarations]\r\nCompile [  4.5%]: BusInOut.cpp\r\nCompile [  4.8%]: InterruptIn.cpp\r\nCompile [  5.2%]: ResetReason.cpp\r\nCompile [  5.6%]: PortIn.cpp\r\nCompile [  5.9%]: TableCRC.cpp\r\nCompile [  6.3%]: PortInOut.cpp\r\nCompile [  6.7%]: QSPI.cpp\r\nCompile [  7.1%]: PortOut.cpp\r\nCompile [  7.4%]: PwmOut.cpp\r\nCompile [  7.8%]: SerialWireOutput.cpp\r\nCompile [  8.2%]: AnalogIn.cpp\r\n[Error] mstd_type_traits@191,12: 'std::conditional_t' has not been declared\r\n[Error] mstd_type_traits@193,12: 'std::enable_if_t' has not been declared\r\n[Error] mstd_type_traits@210,51: expected template-name before '<' token\r\n[Error] mstd_type_traits@210,51: expected '{' before '<' token\r\n[Error] mstd_type_traits@217,51: expected template-name before '<' token\r\n[Error] mstd_type_traits@217,51: expected '{' before '<' token\r\n[Error] mstd_type_traits@1084,12: 'std::remove_const_t' has not been declared\r\n[Error] mstd_type_traits@1086,12: 'std::remove_volatile_t' has not been declared\r\n[Error] mstd_type_traits@1088,12: 'std::remove_cv_t' has not been declared\r\n[Error] mstd_type_traits@1090,12: 'std::add_const_t' has not been declared\r\n[Error] mstd_type_traits@1092,12: 'std::add_volatile_t' has not been declared\r\n[Error] mstd_type_traits@1094,12: 'std::add_cv_t' has not been declared\r\n[Error] mstd_type_traits@1096,12: 'std::remove_reference_t' has not been declared\r\n[Error] mstd_type_traits@1173,12: 'std::make_signed_t' has not been declared\r\n[Error] mstd_type_traits@1175,12: 'std::make_unsigned_t' has not been declared\r\n[Error] mstd_type_traits@1177,12: 'std::remove_extent_t' has not been declared\r\n[Error] mstd_type_traits@1179,12: 'std::remove_all_extents_t' has not been declared\r\n[Error] mstd_type_traits@1181,12: 'std::remove_pointer_t' has not been declared\r\n[Error] mstd_type_traits@1183,12: 'std::add_pointer_t' has not been declared\r\n[Error] mstd_type_traits@1185,12: 'std::aligned_storage_t' has not been declared\r\n[Error] mstd_type_traits@1187,12: 'std::decay_t' has not been declared\r\n[Error] mstd_type_traits@1189,12: 'std::common_type_t' has not been declared\r\n[Error] mstd_type_traits@1191,12: 'std::result_of_t' has not been declared\r\n[Error] mstd_type_traits@1195,42: 'remove_cv_t' is not a member of 'std'; did you mean 'remove_cv'?\r\n[Error] mstd_type_traits@1195,42: 'remove_cv_t' is not a member of 'std'; did you mean 'remove_cv'?\r\n[Error] mstd_type_traits@1195,59: 'remove_reference_t' is not a member of 'std'; did you mean 'remove_reference'?\r\n[Error] mstd_type_traits@1195,78: template argument 1 is invalid\r\n[Error] mstd_type_traits@1195,79: expected '{' before '>' token\r\n[Error] mstd_type_traits@1240,10: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n[Error] mstd_type_traits@1240,21: expected initializer before '<' token\r\n[Error] mstd_type_traits@1250,10: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n[Error] mstd_type_traits@1250,21: expected initializer before '<' token\r\n[Error] mstd_type_traits@1260,10: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n[Error] mstd_type_traits@1260,21: expected initializer before '<' token\r\n[Error] mstd_type_traits@1271,10: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n[Error] mstd_type_traits@1271,21: expected initializer before '<' token\r\n[Error] mstd_type_traits@1281,10: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n[Error] mstd_type_traits@1281,21: expected initializer before '<' token\r\n[Error] mstd_type_traits@1291,10: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n[Error] mstd_type_traits@1291,21: expected initializer before '<' token\r\n[Error] mstd_type_traits@1302,10: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n[Error] mstd_type_traits@1302,21: expected initializer before '<' token\r\n[ERROR] In file included from <command-line>:\r\n././BUILD/K66F/GCC_ARM-RELEASE/mbed_config.h:66: warning: \"NDEBUG\" redefined\r\n   66 | #define NDEBUG                                                                            // defined by application\r\n      | \r\n<command-line>: note: this is the location of the previous definition\r\nIn file included from ./mbed-os/platform/mbed_atomic.h:883,\r\n                 from ./mbed-os/platform/SingletonPtr.h:24,\r\n                 from ./mbed-os/drivers/AnalogIn.h:25,\r\n                 from ./mbed-os/drivers/source/AnalogIn.cpp:18:\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:191:12: error: 'std::conditional_t' has not been declared\r\n  191 | using std::conditional_t;\r\n      |            ^~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:193:12: error: 'std::enable_if_t' has not been declared\r\n  193 | using std::enable_if_t;\r\n      |            ^~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:210:51: error: expected template-name before '<' token\r\n  210 | struct conjunction<B1, BN...> : std::conditional_t<bool(B1::value), conjunction<BN...>, B1> { };\r\n      |                                                   ^\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:210:51: error: expected '{' before '<' token\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:217:51: error: expected template-name before '<' token\r\n  217 | struct disjunction<B1, BN...> : std::conditional_t<bool(B1::value), B1, disjunction<BN...>> { };\r\n      |                                                   ^\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:217:51: error: expected '{' before '<' token\r\nIn file included from ./mbed-os/platform/mbed_atomic.h:883,\r\n                 from ./mbed-os/platform/SingletonPtr.h:24,\r\n                 from ./mbed-os/drivers/AnalogIn.h:25,\r\n                 from ./mbed-os/drivers/source/AnalogIn.cpp:18:\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1084:12: error: 'std::remove_const_t' has not been declared\r\n 1084 | using std::remove_const_t;\r\n      |            ^~~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1086:12: error: 'std::remove_volatile_t' has not been declared\r\n 1086 | using std::remove_volatile_t;\r\n      |            ^~~~~~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1088:12: error: 'std::remove_cv_t' has not been declared\r\n 1088 | using std::remove_cv_t;\r\n      |            ^~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1090:12: error: 'std::add_const_t' has not been declared\r\n 1090 | using std::add_const_t;\r\n      |            ^~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1092:12: error: 'std::add_volatile_t' has not been declared\r\n 1092 | using std::add_volatile_t;\r\n      |            ^~~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1094:12: error: 'std::add_cv_t' has not been declared\r\n 1094 | using std::add_cv_t;\r\n      |            ^~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1096:12: error: 'std::remove_reference_t' has not been declared\r\n 1096 | using std::remove_reference_t;\r\n      |            ^~~~~~~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1173:12: error: 'std::make_signed_t' has not been declared\r\n 1173 | using std::make_signed_t;\r\n      |            ^~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1175:12: error: 'std::make_unsigned_t' has not been declared\r\n 1175 | using std::make_unsigned_t;\r\n      |            ^~~~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1177:12: error: 'std::remove_extent_t' has not been declared\r\n 1177 | using std::remove_extent_t;\r\n      |            ^~~~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1179:12: error: 'std::remove_all_extents_t' has not been declared\r\n 1179 | using std::remove_all_extents_t;\r\n      |            ^~~~~~~~~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1181:12: error: 'std::remove_pointer_t' has not been declared\r\n 1181 | using std::remove_pointer_t;\r\n      |            ^~~~~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1183:12: error: 'std::add_pointer_t' has not been declared\r\n 1183 | using std::add_pointer_t;\r\n      |            ^~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1185:12: error: 'std::aligned_storage_t' has not been declared\r\n 1185 | using std::aligned_storage_t;\r\n      |            ^~~~~~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1187:12: error: 'std::decay_t' has not been declared\r\n 1187 | using std::decay_t;\r\n      |            ^~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1189:12: error: 'std::common_type_t' has not been declared\r\n 1189 | using std::common_type_t;\r\n      |            ^~~~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1191:12: error: 'std::result_of_t' has not been declared\r\n 1191 | using std::result_of_t;\r\n      |            ^~~~~~~~~~~\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1195:42: error: 'remove_cv_t' is not a member of 'std'; did you mean 'remove_cv'?\r\n 1195 | struct remove_cvref : type_identity<std::remove_cv_t<std::remove_reference_t<T>>> { };\r\n      |                                          ^~~~~~~~~~~\r\n      |                                          remove_cv\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1195:42: error: 'remove_cv_t' is not a member of 'std'; did you mean 'remove_cv'?\r\n 1195 | struct remove_cvref : type_identity<std::remove_cv_t<std::remove_reference_t<T>>> { };\r\n      |                                          ^~~~~~~~~~~\r\n      |                                          remove_cv\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1195:59: error: 'remove_reference_t' is not a member of 'std'; did you mean 'remove_reference'?\r\n 1195 | struct remove_cvref : type_identity<std::remove_cv_t<std::remove_reference_t<T>>> { };\r\n      |                                                           ^~~~~~~~~~~~~~~~~~\r\n      |                                                           remove_reference\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1195:78: error: template argument 1 is invalid\r\n 1195 | struct remove_cvref : type_identity<std::remove_cv_t<std::remove_reference_t<T>>> { };\r\n      |                                                                              ^\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1195:79: error: expected '{' before '>' token\r\n 1195 | struct remove_cvref : type_identity<std::remove_cv_t<std::remove_reference_t<T>>> { };\r\n      |                                                                               ^~\r\nIn file included from ./mbed-os/platform/mbed_atomic.h:883,\r\n                 from ./mbed-os/platform/SingletonPtr.h:24,\r\n                 from ./mbed-os/drivers/AnalogIn.h:25,\r\n                 from ./mbed-os/drivers/source/AnalogIn.cpp:18:\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1240:10: error: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n 1240 |  -> std::enable_if_t<std::is_function<F>::value &&\r\n      |          ^~~~~~~~~~~\r\n      |          enable_if\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1240:21: error: expected initializer before '<' token\r\n 1240 |  -> std::enable_if_t<std::is_function<F>::value &&\r\n      |                     ^\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1250:10: error: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n 1250 |  -> std::enable_if_t<std::is_function<F>::value &&\r\n      |          ^~~~~~~~~~~\r\n      |          enable_if\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1250:21: error: expected initializer before '<' token\r\n 1250 |  -> std::enable_if_t<std::is_function<F>::value &&\r\n      |                     ^\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1260:10: error: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n 1260 |  -> std::enable_if_t<std::is_function<F>::value &&\r\n      |          ^~~~~~~~~~~\r\n      |          enable_if\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1260:21: error: expected initializer before '<' token\r\n 1260 |  -> std::enable_if_t<std::is_function<F>::value &&\r\n      |                     ^\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1271:10: error: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n 1271 |  -> std::enable_if_t<!std::is_function<F>::value &&\r\n      |          ^~~~~~~~~~~\r\n      |          enable_if\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1271:21: error: expected initializer before '<' token\r\n 1271 |  -> std::enable_if_t<!std::is_function<F>::value &&\r\n      |                     ^\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1281:10: error: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n 1281 |  -> std::enable_if_t<!std::is_function<F>::value &&\r\n      |          ^~~~~~~~~~~\r\n      |          enable_if\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1281:21: error: expected initializer before '<' token\r\n 1281 |  -> std::enable_if_t<!std::is_function<F>::value &&\r\n      |                     ^\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1291:10: error: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n 1291 |  -> std::enable_if_t<!std::is_function<F>::value &&\r\n      |          ^~~~~~~~~~~\r\n      |          enable_if\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1291:21: error: expected initializer before '<' token\r\n 1291 |  -> std::enable_if_t<!std::is_function<F>::value &&\r\n      |                     ^\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1302:10: error: 'enable_if_t' in namespace 'std' does not name a template type; did you mean 'enable_if'?\r\n 1302 |  -> std::enable_if_t<!std::is_member_pointer<std::decay_t<F>>::value ||\r\n      |          ^~~~~~~~~~~\r\n      |          enable_if\r\n./mbed-os/platform/cxxsupport/mstd_type_traits:1302:21: error: expected initializer before '<' token\r\n 1302 |  -> std::enable_if_t<!std::is_member_pointer<std::decay_t<F>>::value ||\r\n      |                     ^\r\n\r\n[mbed] ERROR: \"/usr/bin/python3\" returned error.\r\n       Code: 1\r\n       Path: \"/home/carlo/Work/Demos/Speech/3rd/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed\"\r\n       Command: \"/usr/bin/python3 -u /home/carlo/Work/Demos/Speech/3rd/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/mbed-os/tools/make.py -t GCC_ARM -m K66F --profile release --source . --build ./BUILD/K66F/GCC_ARM-RELEASE\"\r\n       Tip: You could retry the last command with \"-v\" flag for verbose output\r\n\r\n**Further notes:**\r\nI did install the required python packages found in mbed-os/requirements.txt so I'm not sure what the actual problem is. I've tried with both Python2.7 and Python3.8. Both pip and pip3. ", "@petewarden I think the example generates files for Mbed OS 5 in the `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"nxp_k66f\" generate_micro_speech_mbed_project` step whereas the `mbed new .` pulls in Mbed OS 6 which uses python 3 instead of python 2 and also different naming such as \"enable_if_t\" instead of \"enable_if\" so maybe this is where the problem is? ", "Hi, turns out the problem was related to some of the instructions for mbed-cli and Mbed OS either being erroneous or out of date. I've submitted a [PR to get these fixed](https://github.com/tensorflow/tensorflow/pull/43241) #43241 @thaink could you take a look? thank you in advanced ;)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43001\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43001\">No</a>\n"]}, {"number": 43000, "title": "Tensorflow problem in Python 3.6", "body": "Hi, I have installed these packages, pip list,\r\nbut python 3.6 tells me it takes tensorflow 2.2 or higher, but here it is installed!\r\nI don't know what's wrong...\r\nIn the Python 3.6 shell when I enter \"import keras\" or \"import tensorflow\" it gives me an error.\r\nThanks for your help.\r\nric\r\n\r\n- C:\\>pip list\r\nPackage                Version\r\n---------------------- ---------\r\nabsl-py                0.10.0\r\nastunparse             1.6.3\r\ncachetools             4.1.1\r\ncertifi                2020.6.20\r\nchardet                3.0.4\r\ncycler                 0.10.0\r\ngast                   0.3.3\r\ngoogle-auth            1.21.1\r\ngoogle-auth-oauthlib   0.4.1\r\ngoogle-pasta           0.2.0\r\ngrpcio                 1.31.0\r\nh5py                   2.10.0\r\nidna                   2.10\r\nimportlib-metadata     1.7.0\r\njoblib                 0.16.0\r\nKeras                  2.4.3\r\nKeras-Preprocessing    1.1.2\r\nkiwisolver             1.2.0\r\nMarkdown               3.2.2\r\nmatplotlib             3.3.1\r\nnumpy                  1.18.5\r\noauthlib               3.1.0\r\nopt-einsum             3.3.0\r\npandas                 0.25.3\r\nPillow                 7.2.0\r\npip                    20.2.2\r\nprotobuf               3.13.0\r\npyasn1                 0.4.8\r\npyasn1-modules         0.2.8\r\npyparsing              2.4.7\r\npython-dateutil        2.8.1\r\npytz                   2020.1\r\nPyYAML                 5.3.1\r\nrequests               2.24.0\r\nrequests-oauthlib      1.3.0\r\nrsa                    4.6\r\nscikit-learn           0.23.2\r\nscipy                  1.4.1\r\nsetuptools             50.3.0\r\nsix                    1.15.0\r\nsklearn                0.0\r\ntensorboard            2.3.0\r\ntensorboard-plugin-wit 1.7.0\r\ntensorflow             2.3.0\r\ntensorflow-estimator   2.3.0\r\ntermcolor              1.1.0\r\nthreadpoolctl          2.1.0\r\nurllib3                1.25.10\r\nWerkzeug               1.0.1\r\nwheel                  0.35.1\r\nwrapt                  1.12.1\r\nzipp                   3.1.0\r\n", "comments": ["@chipxx \r\nCan you please share the error logs.\r\nPlease verify if your CPU/Python is on 32 bits, please refer to these issues  #42751 ##36167.", "Hi, my CPU is 64 bits .\r\nI have installed Python 3.6  'Windows x86-64 executable installer'\r\n\r\nShell Python:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Python36\\lib\\site-packages\\keras\\__init__.py\", line 6, in <module>\r\n    'Keras requires TensorFlow 2.2 or higher. '\r\nImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`", "What Is the output of:\r\n\r\n```\r\n!python --version\r\n!python -m pip list\r\n```", "C:\\>python --version\r\nPython 3.6.0\r\n\r\nC:\\>python -m pip list\r\nPackage                Version\r\n---------------------- ---------\r\nabsl-py                0.10.0\r\nastunparse             1.6.3\r\nbackcall               0.2.0\r\ncachetools             4.1.1\r\ncertifi                2020.6.20\r\nchardet                3.0.4\r\ncolorama               0.4.3\r\ncycler                 0.10.0\r\ndecorator              4.4.2\r\ngast                   0.3.3\r\ngoogle-auth            1.21.1\r\ngoogle-auth-oauthlib   0.4.1\r\ngoogle-pasta           0.2.0\r\ngrpcio                 1.31.0\r\nh5py                   2.10.0\r\nidna                   2.10\r\nimportlib-metadata     1.7.0\r\nipykernel              5.3.4\r\nipython                7.16.1\r\nipython-genutils       0.2.0\r\njedi                   0.17.2\r\njoblib                 0.16.0\r\njupyter-client         6.1.7\r\njupyter-core           4.6.3\r\nKeras                  2.4.3\r\nKeras-Preprocessing    1.1.2\r\nkiwisolver             1.2.0\r\nMarkdown               3.2.2\r\nmatplotlib             3.3.1\r\nnumpy                  1.18.5\r\noauthlib               3.1.0\r\nopt-einsum             3.3.0\r\npandas                 0.25.3\r\nparso                  0.7.1\r\npickleshare            0.7.5\r\nPillow                 7.2.0\r\npip                    20.2.2\r\nprompt-toolkit         3.0.3\r\nprotobuf               3.13.0\r\npyasn1                 0.4.8\r\npyasn1-modules         0.2.8\r\nPygments               2.6.1\r\npyparsing              2.4.7\r\npython-dateutil        2.8.1\r\npytz                   2020.1\r\npywin32                228\r\nPyYAML                 5.3.1\r\npyzmq                  19.0.2\r\nrequests               2.24.0\r\nrequests-oauthlib      1.3.0\r\nrsa                    4.6\r\nscikit-learn           0.23.2\r\nscipy                  1.4.1\r\nsetuptools             50.3.0\r\nsix                    1.15.0\r\nsklearn                0.0\r\ntensorboard            2.3.0\r\ntensorboard-plugin-wit 1.7.0\r\ntensorflow             2.3.0\r\ntensorflow-estimator   2.3.0\r\ntermcolor              1.1.0\r\nthreadpoolctl          2.1.0\r\ntornado                6.0.4\r\ntraitlets              4.3.3\r\nurllib3                1.25.10\r\nwcwidth                0.2.5\r\nWerkzeug               1.0.1\r\nwheel                  0.35.1\r\nwrapt                  1.12.1\r\nzipp                   3.1.0\r\n", "What is the failed code snippet?", "Is this working?\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n```", "I wrote it on top. \r\n\"**import keras**\" then gives me error.", "You need to import Keras from Tensorflow as in my example.\n\nSee also https://github.com/tensorflow/tensorflow/issues/42961", "`Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"copyright\", \"credits\" or \"license()\" for more information.\r\n>>> **import tensorflow as tf**\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 32, in <module>\r\n    from tensorflow.core.framework import function_pb2\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 48, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: Impossibile trovare la procedura specificata.\r\n>>> from tensorflow import keras\r\nTraceback (most recent call last):\r\n  File \"<pyshell#1>\", line 1, in <module>\r\n    **from tensorflow import keras**\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 32, in <module>\r\n    from tensorflow.core.framework import function_pb2\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 48, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: Impossibile trovare la procedura specificata.\r\n>>> `\r\n\r\n\r\nerror...", "This is a different issue. Can you try with python 3.6.1?", "I will now try\r\n", "I upgraded to python 3.6.1 but the error is always there.\r\n`Python 3.6.1 (v3.6.1:69c0db5, Mar 21 2017, 18:41:36) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"copyright\", \"credits\" or \"license()\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: Impossibile trovare il modulo specificato.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: Impossibile trovare il modulo specificato.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`\r\n", "Can you check if you are compliat with Windows requirements at https://www.tensorflow.org/install/pip?hl=en#windows", "@chipxx \r\nPlease refer to [this link](https://github.com/tensorflow/tensorflow/issues/43003#issuecomment-688073182), and verify all dependencies.\r\nVerify the system [requirements](https://www.tensorflow.org/install/pip#system-requirements) or try virtual environment and update.", "Check also https://github.com/tensorflow/tensorflow/issues/43003", "Hi, okay solved. I put  vc++ 140_1.dll in python dll folder.  Than", "@chipxx Can you close this?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43000\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43000\">No</a>\n"]}, {"number": 42998, "title": "Go implementation does not provide SummaryImage/SummaryAudio", "body": "**System information**\r\n- Master branch of tensorflow/tensorflow latest. bb49eafc080caf205d5ba4478d9c93552ce46d57\r\n- Windows 10\r\n\r\n**Describe the current behavior**\r\n\r\nI can not decode bitmap to tensor on Go.\r\n\r\n```go\r\nfunc decodeBitmapGraph() (*tf.Graph, tf.Output, tf.Output, error) {\r\n\ts := op.NewScope()\r\n\tinput := op.Placeholder(s, tf.String)\r\n\toutput := op.ExpandDims(\r\n\t\ts,\r\n\t\top.DecodeBmp(s, input, op.DecodeBmpChannels(3)),\r\n\t\top.Const(s.SubScope(\"make_batch\"), int32(0)))\r\n\tgraph, err := s.Finalize()\r\n\treturn graph, input, output, err\r\n}\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThis code did work correctly in Go 1.5 or later. But latest tensorflow does not. Probably, you provide SummaryImage to manipulate images for TF_STRING. But Go does not provide APIs for that.\r\n\r\n\r\n*EDIT*\r\n\r\nCurrent implementation return\r\n\r\n>error making input tensor: Malformed TF_STRING tensor; element 0 out of range", "comments": ["@mattn \r\nCould you please verify this on latest version of tf and let us know.", "I confirmed latest version fix this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42998\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42998\">No</a>\n"]}, {"number": 42997, "title": "ValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable In TPU stragegy", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI use add_weight function to add variable to model In TPU strategy but it shows that /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    256       except Exception as e:  # pylint:disable=broad-except\r\n    257         if hasattr(e, 'ag_error_metadata'):\r\n--> 258           raise e.ag_error_metadata.to_exception(e)\r\n    259         else:\r\n    260           raise\r\n\r\nValueError: in user code:\r\n\r\n    <ipython-input-6-cf880f69ac70>:38 call  *\r\n        target_feat = self.build_(parents)\r\n    <ipython-input-6-cf880f69ac70>:26 build_  *\r\n        parent_weights = [tf.nn.relu(tf.cast(self.add_weight(initializer = tf.ones_initializer(), name='block{}_fusion{}'.format(1, j)), dtype=dtype)) for j in range(len(parents))]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:614 add_weight  **\r\n        caching_device=caching_device)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py:750 _add_variable_with_custom_getter\r\n        **kwargs_for_getter)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:145 make_variable\r\n        shape=variable_shape if variable_shape else None)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:260 __call__\r\n        return cls._variable_v1_call(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call\r\n        shape=shape)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2024 creator_with_resource_vars\r\n        created = self._create_variable(next_creator, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:870 _create_variable\r\n        **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_utils.py:291 create_mirrored_variable\r\n        value_list = real_mirrored_creator(**kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:861 _real_mirrored_creator\r\n        v = next_creator(**kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:199 <lambda>\r\n        previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py:2597 default_variable_creator\r\n        shape=shape)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:264 __call__\r\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1518 __init__\r\n        distribute_strategy=distribute_strategy)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1601 _init_from_args\r\n        raise ValueError(\"Tensor-typed variable initializers must either be \"\r\n\r\n    ValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable (e.g., `tf.Variable(lambda : tf.truncated_normal([10, 40]))`) when building functions. Please file a feature request if this restriction inconveniences you.\r\n**Describe the expected behavior**\r\nCan successful add new variable to model.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nThe standalone code can be see here https://colab.research.google.com/drive/1GB_kujPR2Mu8iayBuglmbB8ylfGh_YYk?usp=sharing, this code can sucessful run in GPU, CPU, but get error in TPU.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@weichen456 \r\nPlease provide with simple stand alone code for us to replicate the issue faced or if possible share a colab gist with the error reported.\r\nWith respect to error shared, please refer to:\r\n#36436 #41306 [link](https://stackoverflow.com/questions/59229898/tensor-typed-variable-initializers-must-either-be-wrapped-in-an-init-scope-or-ca) ", "> @weichen456\r\n> Please provide with simple stand alone code for us to replicate the issue faced or if possible share a colab gist with the error reported.\r\n> With respect to error shared, please refer to:\r\n> #36436 #41306 [link](https://stackoverflow.com/questions/59229898/tensor-typed-variable-initializers-must-either-be-wrapped-in-an-init-scope-or-ca)\r\n\r\nThe stand alone code can be seen [here ](https://colab.research.google.com/drive/1GB_kujPR2Mu8iayBuglmbB8ylfGh_YYk?usp=sharing)", "I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/f6f09a6da1c88c3b582e849bfd694462/untitled407.ipynb)", "Hi @weichen456, in the gist you provided the second cell executes without error. Is that a suitable workaround for you?", "> Hi @weichen456, in the gist you provided the second cell executes without error. Is that a suitable workaround for you?\r\nYes,  it is a suitable workaround. Thank you!", "Great, closing this issue now since you found a workaround. Feel free to reopen if you still feel this is unresolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42997\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42997\">No</a>\n"]}, {"number": 42996, "title": "Multiple branches executing in switch_case", "body": "", "comments": []}, {"number": 42995, "title": "Micro speech example not working on Sparkfun Edge", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Windows 10 Home 64-bit\r\n- Console: Git Bash\r\n- GNU Make 4.3\r\n- TensorFlow installed from (source or binary): (Doesn't apply)\r\n- Tensorflow version: (Doesn't apply)\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge\r\n\r\n**Describe the problem**\r\nI'm trying to deploy the [micro speech example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech) to my Sparkfun edge, and although it compiles, links and flashes correctly, the behavior is wrong. It does not recognize any commands and just flashes the \"unknown\" word LED and sometimes the \"no\" word LED, regardless of what you say. I've done exactly as the wiki explains. I've tried deploying to Arduino and I've had no problem.\r\n\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nI've carefully followed the steps on [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech#deploy-to-sparkfun-edge) but I'll try everything one more time as I write this issue:\r\n\r\nClone trensorflow repo\r\n`$ git clone https://github.com/tensorflow/tensorflow.git`\r\n`$ cd tensorflow`\r\n\r\nCompile the binary\r\n`$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge TAGS=\"cmsis-nn\" micro_speech_bin`\r\n\r\nSign the binary\r\n```\r\n$ cp tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/keys_info0.py \\\r\ntensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/keys_info.py\r\n```\r\n\r\n```\r\n$ python3 tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/create_cust_image_blob.py \\\r\n--bin tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech.bin \\\r\n--load-address 0xC000 \\\r\n--magic-num 0xCB \\\r\n-o main_nonsecure_ota \\\r\n--version 0x0\r\n```\r\n\r\n```\r\n$ python3 tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/create_cust_wireupdate_blob.py \\\r\n--load-address 0x20000 \\\r\n--bin main_nonsecure_ota.bin \\\r\n-i 6 \\\r\n-o main_nonsecure_wire \\\r\n--options 0x1\r\n```\r\nFlash the binary\r\n`export DEVICENAME=COM11`\r\n`export BAUD_RATE=921600`\r\n\r\nNow, with the board on boot mode(holding button `14` and press reset, and still holding `14`)\r\n```\r\npython3 tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/uart_wired_update.py \\\r\n-b ${BAUD_RATE} ${DEVICENAME} \\\r\n-r 1 \\\r\n-f main_nonsecure_wire.bin \\\r\n-i 6\r\n```\r\nIt flashes correctly(output from the console):\r\n```\r\n...\r\nSending Data Packet of length  1468\r\nSending Reset Command.\r\nDone.\r\n```\r\nWhen I try the program, saying some commands, the board just keeps flashing the blue LED(\"unknown word\") and sometimes the red LED(\"no\" word), no matter what I say. The room is quiet and I've tried different tones and volumes, no luck. At first I thought I had faulty hardware but I actually bought two boards and both show the same behavior. The Arduino version works fine for me on the Nano 33 BLE board, so I guess it has to be related to the Sparkfun Edge implementation, but I could not find the issue. Here is some output from the serial port when I'm issuing the commands:\r\n\r\n```\r\nApollo3 Burst Mode is Available\r\n\r\nApollo3 operating in Burst Mode (96MHz)\r\n\r\nHeard no (231) @256ms\r\nHeard unknown (203) @5568ms\r\nHeard unknown (202) @9472ms\r\nHeard unknown (201) @14208ms\r\nHeard unknown (201) @16128ms\r\nHeard unknown (224) @17664ms\r\nHeard no (208) @23424ms\r\nHeard unknown (201) @26688ms\r\nHeard unknown (207) @44800ms\r\n```\r\n\r\n", "comments": ["Hi Enrique,\r\n\r\n>  The Arduino version works fine for me on the Nano 33 BLE board\r\n\r\nIt's expected that the speech command demo works much better on Arduino board which has a better (Digital) mic. \r\n\r\nOn SparkFunEdge board, to make it easier for the board to recognize your sound, maybe try to repeat \"yes\" several times like \"yes yes yes\".", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42995\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42995\">No</a>\n"]}, {"number": 42994, "title": "Jolting training metrics after running Model.evaluate() during Model.fit()", "body": "I'm copying some text over from [a Stack Overflow post of mine from last week](https://stackoverflow.com/questions/63711560/tf-keras-model-training-accuracy-jumps-after-running-model-evaluate-during-tra?noredirect=1#comment112664862_63711560) which is yet to be answered. I hope this is the right place to post for advice - I tried developers@tensorflow.org but my email was blocked.\r\n\r\nMinimum working example demonstrating the problem on MNIST: https://colab.research.google.com/drive/1JLj23q5LjZu6cvcU7SQmfmY97qYwV5Gj?usp=sharing\r\n\r\nI'm training a TensorFlow Keras `Model` using `Model.fit()`. I'm also using callbacks to log my training accuracy metrics after every batch using TensorFlow's `on_train_batch_end()` syntax. In addition, I'm using another callback to run `Model.evaluate()` every 500 batches to compute validation set accuracy and update the `logs` dict passed around the callbacks during `Model.fit()`.\r\n\r\nLooking at the logged metrics vs. batch number shows very perplexing results. After the `Model.evaluate()` run, the training accuracy experiences a significant 'jolt', initially triggering a rapid increase in the logged training accuracy and subsequently triggering a significant drop training accuracy followed by a slower recovery. See the image below for an example.\r\n\r\n![jolt-acc](https://user-images.githubusercontent.com/26459412/92324130-9bbd5c00-f036-11ea-9165-cb3e994dc46d.png)\r\n\r\nMy guess is that it's something to do with the Model.evaluate()'s call to `reset_metrics()`, which loops through and calls the reset_states() method on each metric. I can't work out what `reset_states()` is doing and if this is relevant to the behaviour I'm observing. Are the metrics shown during `Model.fit()` actually some form of moving averages rather than the batch-wise metric? In that case, the `reset_states()` method would be resetting the moving average, producing the jolting behaviour that flattens out over time.\r\n\r\nI'm worried that calling `Model.evaluate()` within `Model.fit()` is silently training on my validation data, although I would expect better validation performance if that were the case.", "comments": ["@tom-andersson \r\nPlease provide complete stand alone code with tf version for us to replicate the issue faced or if possible share a colab gist with error reported.", "Hi @Saduf2019, I've included a Google Colab link after the first paragraph in my original post. This figure shows the same behaviour on MNIST. Printing the training and validation accuracies shows that they are not exactly equal to each other, but very close (equal to ~3 d.p.). Could this be the validation `logs` dict in `Model.evaluate()` corrupting the `logs` dict passed around during training perhaps?\r\n\r\n![jolting_acc_colab](https://user-images.githubusercontent.com/26459412/92372153-6b3bf780-f0f4-11ea-8190-8ab6a7924420.png)\r\n", "@tom-andersson \r\nI do not find any colab link, please check and share it.", "I am able to replicate the issue reported,please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e8b2e8bf3f2f4b717e46228fd2467f7a/untitled407.ipynb)", "@tom-andersson Thanks for the issue! You're right that calling `model.evaluate` during `model.fit` will reset the metrics. This won't affect training (your model won't train on the validation data), but it will cause the metrics to jump around the first few batches after calling `model.evaluate`. My suggestion would be to perform the `model.evaluate` call on epoch boundaries, since metrics are reset every epoch for `model.fit` anyway. You could always make each epoch smaller when using `tf.data` by passing `steps_per_epoch=500` to `model.fit`", "Thank you for clarifying @omalleyt12! Please can you point me to documentation where the metric averaging is expained? Is it a moving average of sorts? I couldn't find anything by Googling. What if I want the (noisy) batch-wise training metrics? Should I call `reset_metrics()` in `on_train_batch_begin()`?\r\n\r\nSetting `steps_per_epoch` is not an ideal solution for me as `on_epoch_end()` will trigger the shuffling of samples in my data generator. The reason I want to validate every N batches is that one epoch is very slow with my dataset/architecture, and I want to model checkpoint within an epoch. I expect I'm not the only person who would appreciate clarity on the best way to do this with `tf.keras`.\r\n\r\nAlso, if my data generator didn't shuffle in `on_epoch_end()`, would the training never see the batches after `steps_per_epoch` batches?"]}, {"number": 42993, "title": "The inconsistency of CTC API. Please optimize it.", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state**:\r\n\r\nThe API of CTC_loss and CTC_decoder is not consistent.\r\n\r\n1. `tf.nn.ctc_loss(labels, logits, label_length, logit_length, logits_time_major=True, unique=None, blank_index=None, name=None)`\r\nDefault blank label is 0 rather num_classes - 1, unless overridden by blank_index.\r\n\r\n2. `tf.nn.ctc_beam_search_decoder(inputs, sequence_length, beam_width=100, top_paths=1)`\r\nand\r\n`tf.nn.ctc_greedy_decoder(inputs, sequence_length, merge_repeated=True)`\r\nThis decoder treats blank label as num_classes -1 by default. And there is no option for setting blank label index.\r\n\r\nTherefore, it could be a bit inconvenient to train a model with CTC loss and decode with the beam search decoder.\r\nHope someone could optimize this API.\r\n\r\n**Will this change the current api? How?**\r\nOriginal one:\r\n`tf.nn.ctc_beam_search_decoder(inputs, sequence_length, beam_width=100, top_paths=1)`\r\n`tf.nn.ctc_greedy_decoder(inputs, sequence_length, merge_repeated=True)`\r\n\r\nNew one: \r\n`tf.nn.ctc_beam_search_decoder(inputs, sequence_length, beam_width=100, top_paths=1, blank_index=None)`\r\n`tf.nn.ctc_greedy_decoder(inputs, sequence_length, merge_repeated=True, blank_index=None)`\r\n\r\n**Who will benefit with this feature?**\r\nResearchers who are dealing with sequence recognition problems such as scene text recognition, OCR and speech recognition.", "comments": ["Reassigning to Martin Wicke to decide on priority for adding this feature.", "Adding the argument to the decoders is easy, I guess. Changing their default behavior, sadly, is not. So we can add the argument and have it default to num_classes-1. That's a bummer, I agree this is very ugly. \r\n\r\n@tensorflow/api-owners for discussion.", "I find the same problem. The CTC loss in training period for TensorFlow2.x default blank label is 0 rather num_classes - 1, but for tf.nn.ctc_beam_search_decoder it still use the num_classes -1 as blank by default and there is no option to set it. ", "By the way, the decode function tf.nn.ctc_greedy_decoder and tf.nn.ctc_beam_search_decoder may need to add argument logits_time_major=True to keep consistent with tf.nn.ctc_loss function.", "@BenjaminChoou: thank you for your proposal. Could you start a RFC doc, containing end-to-end workflow examples, so that we could get feedback from potential users of this API (which is quite specialized)? Please submit the RFC to the `tensorflow/community` repo.\r\n\r\n(for tf-api-owners)", "> @BenjaminChoou: thank you for your proposal. Could you start a RFC doc, containing end-to-end workflow examples, so that we could get feedback from potential users of this API (which is quite specialized)? Please submit the RFC to the `tensorflow/community` repo.\r\n> \r\n> (for tf-api-owners)\r\n\r\nNo problem. Would you be the sponsor?", "> @BenjaminChoou: thank you for your proposal. Could you start a RFC doc, containing end-to-end workflow examples, so that we could get feedback from potential users of this API (which is quite specialized)? Please submit the RFC to the `tensorflow/community` repo.\r\n> \r\n> (for tf-api-owners)\r\n\r\nThis RFC will be open for comment until October 17, 2020.\r\n| Status        |  Proposed                                            |\r\n:-------------- |:---------------------------------------------------- |\r\n| **RFC #**     | [295](https://github.com/tensorflow/community/pull/295) |\r\n| **Author(s)** | Mingjie Zhou (benjamin_chou@outlook.com), |\r\n| **Sponsor**   | Fran\u00e7ois Chollet (francois.chollet@gmail.com)                 |\r\n| **Updated**   | 2020-10-02                                           |\r\n\r\n# Objective\r\nChange the APIs of connectionist temporal classification (CTC) decoders (beam search decoder and greedy decoder) to ensure the consistency with the usage of CTC loss. Preserve the same arguments of API as far as possible.", "Thank you Benjamin!\n\nOn Fri, Oct 2, 2020 at 8:37 AM BenjaminChou <notifications@github.com>\nwrote:\n\n> @BenjaminChoou <https://github.com/BenjaminChoou>: thank you for your\n> proposal. Could you start a RFC doc, containing end-to-end workflow\n> examples, so that we could get feedback from potential users of this API\n> (which is quite specialized)? Please submit the RFC to the\n> tensorflow/community repo.\n>\n> (for tf-api-owners)\n>\n> This RFC will be open for comment until October 17, 2020.\n> Status Proposed\n> *RFC #* 295 <https://github.com/tensorflow/community/pull/295>\n> *Author(s)* Mingjie Zhou (benjamin_chou@outlook.com),\n> *Sponsor* Fran\u00e7ois Chollet (francois.chollet@gmail.com)\n> *Updated* 2020-10-02 Objective\n>\n> Change the APIs of connectionist temporal classification (CTC) decoders\n> (beam search decoder and greedy decoder) to ensure the consistency with the\n> usage of CTC loss. Preserve the same arguments of API as far as possible.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/42993#issuecomment-702804755>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG7XJDC5BILYQZMWPY3SIXXVFANCNFSM4Q37BYHQ>\n> .\n>\n", "Thanks for filing the RFC.  Let's continue this discussion there."]}, {"number": 42992, "title": "Cuda Library not found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Installed using PIP in a Conda environment\r\n- TensorFlow version: 2.3.0\r\n- Python version:- Installed using virtualenv? pip? conda?: Installed using PIP in a Conda environment\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\n\r\n**Describe the problem**\r\nI don't have a GPU in my Laptop. I installed tensorflow using Pip inside a Conda Environment.\r\nThe command that I used to install it was -\r\n```bash\r\npip install tensorflow\r\n```\r\nAnd I opened a Python Shell inside the CMD by typing\r\n```bash\r\npython\r\n```\r\nand I typed\r\n```python\r\n>>> import tensorflow\r\n```\r\nand I got this error -\r\n![image](https://user-images.githubusercontent.com/52103891/92318272-3d3cb180-f027-11ea-8f4b-d4b26079db98.png)\r\n\r\nBut when I tried to print out the version, using\r\n```python\r\nprint(tensorflow.__version__)\r\n```\r\nthe output was -\r\n![image](https://user-images.githubusercontent.com/52103891/92318285-8987f180-f027-11ea-89c7-8477f295ca87.png)\r\n\r\nPlease tell me whether I should Ignore this,\r\nI know that the error is telling to Ignore if you don't have a GPU in you PC.\r\nBut whenever I import it it is throwing this Error\r\n\r\nThis is the Exact sequence of command that I used\r\n![image](https://user-images.githubusercontent.com/52103891/92318320-1f238100-f028-11ea-8034-7a12d307ce3d.png)\r\n\r\n\r\n\r\nI also tried the same in Pycharm, By setting the same above used python interpreter. But it is also showing the same error\r\n```\r\nE:\\Anaconda3\\envs\\tensorflow\\pythonw.exe E:/Robotics/python/ML(techwithtim)/tensorflowEnv/test.py\r\n2020-09-05 23:13:00.132025: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-09-05 23:13:00.132610: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/52103891/92318380-4038a180-f029-11ea-8706-cf931586c50a.png)\r\n\r\nAnd I followed this Tutorial for the Installation and Setup -\r\nhttps://www.youtube.com/watch?v=ujTCoH21GlA", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42992\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42992\">No</a>\n", "@vaibhav-75007,\r\nYes, you can safely ignore these GPU related logs. \r\n\r\nIf you want to disable these logs, please change the log level using the below code. \r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\n```\r\nThanks!", "> @vaibhav-75007,\r\n> Yes, you can safely ignore these GPU related logs.\r\n> \r\n> If you want to disable these logs, please change the log level using the below code.\r\n> \r\n> ```\r\n> import os\r\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\n> import tensorflow as tf\r\n> ```\r\n> \r\n> Thanks!\r\n\r\n@amahendrakar \r\nShould I include that line of code in all of my projects??\r\nKindly reply", "But should I add this line of code I. My every project with tensorflow??\n\nOn Sun, Sep 6, 2020, 12:50 PM Abhilash Mahendrakar <notifications@github.com>\nwrote:\n\n> @vaibhav-75007 <https://github.com/vaibhav-75007>,\n> Yes, you can safely ignore these GPU related logs.\n>\n> If you want to disable these logs, please change the log level using the\n> below code.\n>\n> import os\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n> import tensorflow as tf\n>\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/42992#issuecomment-687715009>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AMNQVUYP2ESI6LTNDHOXJKTSEMZ4PANCNFSM4Q35DSNA>\n> .\n>\n", "> @vaibhav-75007,\r\n> Yes, you can safely ignore these GPU related logs.\r\n> \r\n> If you want to disable these logs, please change the log level using the below code.\r\n> \r\n> ```\r\n> import os\r\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\n> import tensorflow as tf\r\n> ```\r\n> \r\n> Thanks!\r\n\r\nshould i add this line off code in all my projects?? @amahendrakar .\r\nPlease reply", "@vaibhav-75007,\r\nYes, you'll have to add the code snippet to your projects. Alternatively, you can install the TensorFlow CPU package using the below command. \r\n\r\n`pip install tensorflow-cpu`\r\n\r\nThanks!"]}, {"number": 42991, "title": "tf.convert_to_tensor can't convert a float32 tensor to a float64 one", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThe following code threw exception:  ValueError: Tensor conversion requested dtype float64 for Tensor with dtype float32: <tf.Tensor: shape=(6,), dtype=float32, numpy=array([-0.6  , -0.04 , -0.001,  0.3  ,  0.4  ,  1.   ], dtype=float32)>\r\n\r\n```\r\n    labels = [-0.6, -0.04, -0.001, .3, .4, 1.0]\r\n    labels = tf.convert_to_tensor(labels, dtype=tf.float32)\r\n    labels = tf.convert_to_tensor(labels, dtype=tf.float64)\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt should convert it.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Only conversion from raw data to TF_Tensor is supported in the API **tf.convert_to_tensor()**.\r\nConversation between TF_Tensor of different dtypes is not supported now in **tf.convert_to_tensor()**.", "As @Dynmi said, `tf.convert_to_tensor()` is for converting a python list or a `numpy.array`. For converting a tensor into another tensor use `tf.cast(tensor, tf.float64)`", "@dyu41 \r\nCan you please verify the above comments and update.", "tf.cast() works. But it would be better that tf.convert_to_tensor() defaults to tf.cast() directly in this case. ", "`tf.convert_to_tensor()` is for making something that's not a `tf.Tensor` (eg. a python list or a `np.array`) into a `tf.Tensor`. `tf.cast` is for converting a `tf.Tensor` into another dtype.\r\nChanging the API would make `tf.convert_to_tensor()` do something it's not meant to do. ", "@dyu41 \r\nPlease update as per above comment.", "It will be better to have the cast automatically. As in my case, tf.searchsorted expects float64 even I only  use float32 in my code.  The other reason can be to have the same code for both numpy array and tensor. \r\n\r\nThanks,", "> The other reason can be to have the same code for both numpy array and tensor.\r\n\r\nThat is not wanted by TensorFlow a Tensor and a np array are two completely different things and should have different APIs.\r\n\r\n> As in my case, tf.searchsorted expects float64 even I only use float32 in my code.\r\n\r\nI'm not sure what you mean. If you mean tf.searchsorted needs a tf.float64 tensor and does not work on other dtypes, this is an issue with tf.searchsorted. But it's more likely that you didn't specify the `tf.float32` somewhere and it is automatically detected as `tf.float64`. Also, automatic casting would need resources and you normally don't want that. It's more likely you need to specify a dtype somewhere and you don't want automatic casting somewhere.", "@dyu41 \r\nPlease update as per above comment.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42991\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42991\">No</a>\n"]}, {"number": 42990, "title": "tape.gradient slow with RNN GRU model", "body": "I have a trained  keras model, I'm trying to find the gradient of `target_preds` with respect of `sample_inputs` in the trained model. I tried to do it with CNN model and it computed in couple of seconds. When I insert a model that is RNN, the function it take it seems that the same code executes for long time (couple of minutes), and when I interrupt the code- it (is running) at line   `grads = tape.gradient(target_preds, sample_inputs)`).\r\n\r\nI'm using Tensorflow 2.3.0\r\n\r\nThe relevant code is below:\r\n\r\n\r\n```\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(sample_inputs)\r\n        preds = model(sample_inputs)\r\n        if (target_range is None):\r\n            target_preds = preds[:, :]\r\n        else:\r\n            target_preds = preds[:, target_range]\r\n    \r\n    if(jacobian):\r\n        grads = tape.jacobian(target_preds, sample_inputs)\r\n    else:\r\n        grads = tape.gradient(target_preds, sample_inputs)\r\n    return grads\r\n```\r\n\r\nOne more important note:\r\nwhen I used tensorflow 1 I tried the apply the same operation to the same model using: \r\n```\r\n...\r\ngradients = model.optimizer.get_gradients(model.output[:, c], model.input)\r\n...\r\nget_gradients = keras.backend.function(inputs=input_tensors, outputs=gradients)\r\n...\r\ngradients = get_gradients(_input)\r\n```\r\nThis way it was computed in seconds,  but Tensorflow 2 stopped supporting this. ", "comments": ["@ofiryaish \r\n\r\nLooks like code is incomplete. Can you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "I think I found the problem, and it's not a technical one (even though it's really true that performing gradient for GRU is much slower than performing for CNN). It's just much more faster to perform gradient on a batch rather performing on singles, and I didn't take the advantage of this, and therefore it took hours to complete. \r\n\r\nSo in overall, you can close the topic. By the way, If you got, in general, a tip for making the gradient procedure faster (maybe using function?) , I would be happy to hear.\r\n\r\nthank you", "@ofiryaish \r\n\r\nI am closing the issue as per your comments.Thanks!"]}, {"number": 42989, "title": "Inference time almost 5x SLOWER than Pytorch Same Model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9.12 (stretch)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: 10.1.243/7.6.5\r\n- GPU model and memory: 1 x NVIDIA Tesla P4\r\n- Pytorch version: 1.5.0\r\n\r\n**Describe the current behavior**\r\nI am implementing this [paper](https://arxiv.org/pdf/2004.02147.pdf) and found an existing implementation in Pytorch. So I ported the exact model to tensorflow with the keras api. Nonetheless, the inference time is almost 5x slower when running inference on GPU for the tensorflow version. I tested with the tesla P4 but also on a NVIDIA Xavier AGX and got same results (5 times slower). What am I missing? :s\r\n\r\nI got the following results:\r\npytorch: 82.5 FPS 12.11 ms\r\ntensorflow: 20 FPS 49.9 ms\r\n\r\n**Describe the expected behavior**\r\nIf you check the code, it refers to the same architecture and number of parameters, one would expect similar running times.\r\n\r\n**Standalone code to reproduce the issue**\r\nYou can check both pytorch and tensorflow implementation in this repo https://github.com/charlielito/test, and test the speed with:\r\n```\r\npython bisenetv2_tensorflow.py\r\n```\r\n```\r\npython bisenetv2_torch.py\r\n```\r\n\r\n\r\n", "comments": ["Have you tried to profilo your model: https://www.tensorflow.org/guide/profiler ?", "**Update**: I was calling the model with `model(image)`, but when wrapped with a tf.function (`mode=tf.function(model)` ) it runs now way faster (around 60 FPS), but still slower than pytorch", "I suggest you to profile your model with the Tensorflow tool and with the Pytorch one with https://pytorch.org/tutorials/recipes/recipes/profiler.html.\n", "I notice that `grouped` convolutions are new in tensorflow 2.3.0. I used that because the original pytorch implementation uses that for implementing depthwise separable convolutions. Nonetheless, I replaced all grouped convolution with the corresponding ` tf.keras.layers.DepthwiseConv2D` and with that I got comparable results, even faster than pytorch! I you want to check in the mentioned repo the implementation is in `bisenetv2_tensorflow_depthconv.py`.\r\n**Recap**\r\nPytorch: 82.5 FPS\r\nTensorflow: 57 FPS\r\nTensorflow with DepthwiseConv2D: 92 FPS\r\n\r\nIs the new grouped convolution implementation somehow not optimal? ", "How about the accuracy? Did you need to fine-tune the model?", "I think you should call `model.predict(image)` for inference instead of `model(image)` that returns a `Tensor` causing some unnecessary overheat", "@Leslie-Fang I haven't trained it yet, so I don't know. Nonetheless, because it is exactly the same model I expect comparable accuracy results. No need of fine tuning.\r\n\r\n@lyonguyen8697 I think I forgot to mention it, but I am testing with batch_size=1, and as the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict) says: `\"using __call__ is recommended for faster execution\"`", "@charlielito Can you test the performance with  `pip install tf_nighlty`?", "Cause on nightly on your model we have a more explicit:\r\n\r\n>UnimplementedError:  Fused conv implementation does not support grouped convolutions for now.\r\n\t [[node BiSeNetV2/segment_branch_2/sequential_78/ge_layer_s2_6/sequential_75/batch_normalization_132/FusedBatchNormV3 (defined at <ipython-input-2-ffb1581d5ded>:206) ]] [Op:__inference_function_21059]\r\n\r\nEDIT:\r\nPlease double check this locally cause it could be still a CPU only side effect of https://github.com/tensorflow/tensorflow/issues/42957", "@bhack  just tested the latest version of `tf_nightly` and it runs way faster. Now the same implementation matches pytorch with ~83 FPS (grouped convolutions). I had to use CUDA 11 though, and couldn't test pytorch because there are not pre-built packages for that version of CUDA. So my guess is that in  `tf_nightly` the grouped convolutions performance was improved?\r\nI think the issue is resolved\r\n", "If your code was quite the same yes. I think you can close this and eventually reopen or just comment when you have a new run with pytorch/CUDA 11"]}, {"number": 42988, "title": "Conv1D (and probablyall other Conv layers) with dilation_rate != 1  does not reliably handle changes in input size", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\nBug is present with  tensorflow 2.1 and tensorflow 2.2 from Anaconda \r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04 and  Debian 4.9.189-3+deb9u2 (2019-11-11)\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nnot mobile\r\n\r\n- TensorFlow installed from (source or binary):\r\n\r\nbinary\r\n\r\n- TensorFlow version (use command below):\r\n\r\nunknown 2.1.0 and unknown 2.2.0\r\n\r\n- Python version:\r\n\r\n3.7\r\n\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n\r\n- CUDA/cuDNN version:\r\n10.1.243/7.6.5\r\n\r\n- GPU model and memory:\r\n\r\nFGeForce GTX 1050 Ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThis extremely simple sequnec of calls of a Conv1D layer fails on the third call\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ncc = tf.keras.layers.Conv1D(1, (3,), padding=\"same\", dilation_rate=3)\r\n\r\nres1= cc(np.zeros((1,100,1), dtype=np.float32))\r\nres2= cc(np.zeros((1,101,1), dtype=np.float32))\r\nres3= cc(np.zeros((1,100,1), dtype=np.float32))\r\n\r\n**Describe the expected behavior**\r\n\r\nThe third call should run similarly as the others \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n[test_code.zip](https://github.com/tensorflow/tensorflow/files/5178711/test_code.zip)\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nError message:\r\nres1 done\r\nres2 done\r\n2020-09-06 00:05:58.405706: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at spacetobatch_op.cc:219 : Invalid argument: padded_shape[0]=107 is not divisible by block_shape[0]=3\r\nTraceback (most recent call last):\r\n  File \"./test_conv_dila.py\", line 12, in <module>\r\n    res3= cc(np.zeros((1,100,1), dtype=np.float32))\r\n  File \"/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 968, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 207, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1106, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 638, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 621, in _with_space_to_batch_call\r\n    input=inp, block_shape=dilation_rate, paddings=paddings)\r\n  File \"/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 9491, in space_to_batch_nd\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6653, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[0]=107 is not divisible by block_shape[0]=3 [Op:SpaceToBatchND]\r\n\r\n\r\n", "comments": ["Tracing through the TF code I found the problem and propose here a fix. \r\n\r\nIn \r\n\r\ntensorflow/core/python/keras/layers/convolutional.py\r\n\r\nThe original code starting from line 193 in tf21 is\r\n```\r\n    recreate_conv_op = (\r\n        call_input_shape[1:] != self._build_conv_op_input_shape[1:])\r\n\r\n    if recreate_conv_op:\r\n      self._convolution_op = nn_ops.Convolution(\r\n          call_input_shape,\r\n          filter_shape=self.kernel.shape,\r\n          dilation_rate=self.dilation_rate,\r\n          strides=self.strides,\r\n          padding=self._padding_op,\r\n          data_format=self._conv_op_data_format)\r\n```\r\nSo whenever the input shape does not fit the input shape used to build the layer the op is updated. \r\nUnfortunately then, the shape matching the updated convolutional_op is not updated !\r\nAs long as all inputs are always equal to the build shape this is fine. If all inputs are different \r\nfrom the build shape this is fine as well.\r\n\r\nBut as soon as an input shape != to the build shape was used once, the build_shape and the op are out of sync \r\nand the convolutional op then fails if the call_input_shape equals the build shape in which case \r\nrecreate_conv_op will be false so that the op will not be adapted to the input shape.\r\n\r\nFix is easy. One simply needs to update the   self._build_conv_op_input_shape variable\r\nwith the input shape for that the convolutional op has been created right after the assignment of the new \r\nop like this\r\n\r\n```\r\n    recreate_conv_op = (\r\n        call_input_shape[1:] != self._build_conv_op_input_shape[1:])\r\n\r\n    if recreate_conv_op:\r\n      self._convolution_op = nn_ops.Convolution(\r\n          call_input_shape,\r\n          filter_shape=self.kernel.shape,\r\n          dilation_rate=self.dilation_rate,\r\n          strides=self.strides,\r\n          padding=self._padding_op,\r\n          data_format=self._conv_op_data_format) \r\n       self._build_conv_op_input_shape = call_input_shape\r\n```\r\n\r\nAfter this change the script triggering the error runs fine.", "The logic changed. In TF 2.3 your failing example doesn't fail anymore. Can you verify?", "@roebel\r\nI ran your code on tf 2.3 and tf-nightly the error does not exist, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/09c7bdfcd06ffabb2e9972f027d7170a/untitled402.ipynb). Please feel free to move the issue to closed status as the issue is resolved is latest versions of tf.", "Fine then - thanks a lot for your comments.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42988\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42988\">No</a>\n"]}, {"number": 42987, "title": "toco", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@MateoFlz \r\n\r\nCan you please let us know which TF version you are using?.Please fill [issue templete.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This bug has no information."]}, {"number": 42986, "title": "LSTM Input Shape Error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04 (through WSL 2 on Windows 10, Version 2004, Build 20206.1000)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.3.0**\r\n- Python version: **3.7.9**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **10.1, 7.6.5.32**\r\n- GPU model and memory: **Titan V, 12 GiB HBM2**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nRunning LSTM example code produces error\r\n\r\n```\r\nValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 28, 28, 1]\r\n```\r\n\r\n**Describe the expected behavior**\r\nSample code works.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Source:**\r\n======\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nfrom tensorflow.compat.v1.train import Saver\r\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nBATCH_SIZE = 64\r\n\r\n(ds_train, ds_test), ds_info = tfds.load(\r\n    \"mnist\",\r\n    split=[\"train\", \"test\"],\r\n    shuffle_files=True,\r\n    as_supervised=True,\r\n    with_info=True,\r\n)\r\n\r\n\r\ndef normalize_img(image, label):\r\n    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\r\n    return tf.cast(image, tf.float32) / 255.0, label\r\n\r\n\r\nds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\nds_train = ds_train.cache()\r\nds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\r\nds_train = ds_train.batch(128)\r\nds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\nds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\nds_test = ds_test.batch(128)\r\nds_test = ds_test.cache()\r\nds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\nshape = (28, 28, 1)\r\nprint(shape)\r\n\r\nmodel = Sequential(\r\n    [\r\n        LSTM(128, input_shape=shape, return_sequences=True),\r\n        Dropout(0.2),\r\n        LSTM(128),\r\n        Dropout(0.2),\r\n        Dense(32, activation=\"relu\"),\r\n        Dropout(0.2),\r\n        Dense(10, activation=\"softmax\"),\r\n    ]\r\n)\r\n\r\nmodel.compile(\r\n    loss=\"sparse_categorical_crossentropy\",\r\n    optimizer=Adam(lr=1e-3, decay=1e-5),\r\n    metrics=[\"accuracy\"],\r\n)\r\n\r\nmodel.fit(\r\n    ds_train,\r\n    epochs=6,\r\n    validation_data=ds_test,\r\n)\r\n\r\nmodel.save(\"mnist_model.h5\")\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n**Results:**\r\n=======\r\n\r\n```\r\n2020-09-05 13:12:19.582270: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-05 13:12:20.625604: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-09-05 13:12:20.819050: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2020-09-05 13:12:20.819191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:41:00.0 name: TITAN V computeCapability: 7.0\r\ncoreClock: 1.455GHz coreCount: 80 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 607.97GiB/s\r\n2020-09-05 13:12:20.819233: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-05 13:12:20.820267: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-05 13:12:20.821208: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-05 13:12:20.821395: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-05 13:12:20.822386: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-05 13:12:20.822973: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-05 13:12:20.825025: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-05 13:12:20.825824: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2020-09-05 13:12:20.827090: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2020-09-05 13:12:20.827504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-05 13:12:20.827843: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-05 13:12:20.834505: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2304005000 Hz\r\n2020-09-05 13:12:20.836670: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556ef1d85530 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-05 13:12:20.836696: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-05 13:12:21.114332: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2020-09-05 13:12:21.114811: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556ef1df11a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-05 13:12:21.114841: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN V, Compute Capability 7.0\r\n2020-09-05 13:12:21.115526: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2020-09-05 13:12:21.115846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:41:00.0 name: TITAN V computeCapability: 7.0\r\ncoreClock: 1.455GHz coreCount: 80 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 607.97GiB/s\r\n2020-09-05 13:12:21.115883: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-05 13:12:21.115904: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-05 13:12:21.115913: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-05 13:12:21.115943: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-05 13:12:21.115971: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-05 13:12:21.115983: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-05 13:12:21.115992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-05 13:12:21.116516: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2020-09-05 13:12:21.117082: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2020-09-05 13:12:21.117231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-05 13:12:21.117269: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-05 13:12:21.793471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-05 13:12:21.793510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-05 13:12:21.793518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-05 13:12:21.794276: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2020-09-05 13:12:21.794418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\r\n2020-09-05 13:12:21.794805: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2020-09-05 13:12:21.794965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10218 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:41:00.0, compute capability: 7.0)\r\n(28, 28, 1)\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 46, in <module>\r\n    Dense(10, activation=\"softmax\"),\r\n  File \"/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\", line 142, in __init__\r\n    self.add(layer)\r\n  File \"/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\", line 206, in add\r\n    layer(x)\r\n  File \"/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 663, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 926, in __call__\r\n    input_list)\r\n  File \"/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1092, in _functional_construction_call\r\n    input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\r\n  File \"/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\", line 180, in assert_input_compatibility\r\n    str(x.shape.as_list()))\r\nValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 28, 28, 1]\r\n```", "comments": ["I do not know why, but for some reason the following works. I do not see the difference between this and the above, so an explanation would be appreciated for my edification:\r\n\r\n**FYI**: \r\n\r\nattrs == 20.1.0\r\ntraitlets == 5.0.0\r\ngoogle-auth == 1.21.0\r\nabsl-py == 0.10.0\r\nisort == 5.4.2\r\ntensorflow-metadata == 0.22.2\r\ntifffile == 2020.8.25\r\n\r\n**Source:**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nfrom tensorflow.compat.v1.train import Saver\r\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nBATCH_SIZE = 64\r\n\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(LSTM(128, input_shape=x_train.shape[1:], return_sequences=True))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(LSTM(128))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(32, activation=\"relu\"))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(10, activation=\"softmax\"))\r\n\r\n# model = Sequential(\r\n#     [\r\n#         LSTM(128, input_shape=x_train.shape[1:], return_sequences=True),\r\n#         Dropout(0.2),\r\n#         LSTM(128),\r\n#         Dropout(0.2),\r\n#         Dense(32, activation=\"relu\"),\r\n#         Dropout(0.2),\r\n#         Dense(10, activation=\"softmax\"),\r\n#     ]\r\n# )\r\n\r\nmodel.compile(\r\n    loss=\"sparse_categorical_crossentropy\",\r\n    optimizer=Adam(lr=1e-3, decay=1e-5),\r\n    metrics=[\"accuracy\"],\r\n)\r\n\r\nmodel.fit(\r\n    x_train,\r\n    y_train,\r\n    epochs=6,\r\n    validation_data=(x_test, y_test),\r\n)\r\n\r\nmodel.save(\"mnist_model.h5\")\r\n```", "Change shape from `(28, 28, 1)` to `(28, 28)` in your first code-snippet, and it works as expected.", "@adam-grant-hendry,\r\nPlease check @WindQAQ's comment and also [this](https://stackoverflow.com/a/44583919) similar StackOverflow comment and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42985, "title": "Cannot Download MNIST on WSL2 - Temporary Failure in Name Resolution", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux 18.04 LTS (through WSL 2; Windows 10 Version 2004 Build 20206.1000)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version (use command below): **2.3.0**\r\n- Python version: **3.7.9**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **10.1, 7.6.5.32**\r\n- GPU model and memory: **Titan V, 12 GB of HBM 2**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nCannot download MNIST data set on Ubuntu 18.04 from WSL 2. Creating `/etc/wsl.conf` with\r\n\r\n```\r\n[network]\r\ngenerateResolvConf = false\r\n```\r\n\r\n and adding `nameserver 8.8.8.8` to `/etc/resolv.conf` does not resolve this issue.\r\n\r\n**Describe the expected behavior**\r\nMNIST example data downloads\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Source:**\r\n=====\r\n```\r\n\"\"\"test.py\"\"\"\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n**Result:**\r\n======\r\n```\r\n> python test.py\r\n2020-09-05 12:02:27.043432: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.7/urllib/request.py\", line 1350, in do_open\r\n    encode_chunked=req.has_header('Transfer-encoding'))\r\n  File \"/usr/lib/python3.7/http/client.py\", line 1277, in request\r\n    self._send_request(method, url, body, headers, encode_chunked)\r\n  File \"/usr/lib/python3.7/http/client.py\", line 1323, in _send_request\r\n    self.endheaders(body, encode_chunked=encode_chunked)\r\n  File \"/usr/lib/python3.7/http/client.py\", line 1272, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"/usr/lib/python3.7/http/client.py\", line 1032, in _send_output\r\n    self.send(msg)\r\n  File \"/usr/lib/python3.7/http/client.py\", line 972, in send\r\n    self.connect()\r\n  File \"/usr/lib/python3.7/http/client.py\", line 1439, in connect\r\n    super().connect()\r\n  File \"/usr/lib/python3.7/http/client.py\", line 944, in connect\r\n    (self.host,self.port), self.timeout, self.source_address)\r\n  File \"/usr/lib/python3.7/socket.py\", line 707, in create_connection\r\n    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\r\n  File \"/usr/lib/python3.7/socket.py\", line 752, in getaddrinfo\r\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\r\nsocket.gaierror: [Errno -3] Temporary failure in name resolution\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 278, in get_file\r\n    urlretrieve(origin, fpath, dl_progress)\r\n  File \"/usr/lib/python3.7/urllib/request.py\", line 247, in urlretrieve\r\n    with contextlib.closing(urlopen(url, data)) as fp:\r\n  File \"/usr/lib/python3.7/urllib/request.py\", line 222, in urlopen\r\n    return opener.open(url, data, timeout)\r\n  File \"/usr/lib/python3.7/urllib/request.py\", line 525, in open\r\n    response = self._open(req, data)\r\n  File \"/usr/lib/python3.7/urllib/request.py\", line 543, in _open\r\n    '_open', req)\r\n  File \"/usr/lib/python3.7/urllib/request.py\", line 503, in _call_chain\r\n    result = func(*args)\r\n  File \"/usr/lib/python3.7/urllib/request.py\", line 1393, in https_open\r\n    context=self._context, check_hostname=self._check_hostname)\r\n  File \"/usr/lib/python3.7/urllib/request.py\", line 1352, in do_open\r\n    raise URLError(err)\r\nurllib.error.URLError: <urlopen error [Errno -3] Temporary failure in name resolution>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 12, in <module>\r\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n  File \"/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/datasets/mnist.py\", line 62, in load_data\r\n    '731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1')\r\n  File \"/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 282, in get_file\r\n    raise Exception(error_msg.format(origin, e.errno, e.reason))\r\nException: URL fetch failure on https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz: None -- [Errno -3] Temporary failure in name resolution\r\n```", "comments": ["System restart was required. Afterwards, data downloaded, but the following error was output:\r\n\r\n```\r\npython test.py                                                                                                                                                                                                    \u2500\u256f\r\n2020-09-05 17:09:54.933859: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-05 17:09:57.545767: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Failed precondition: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'\".\r\nDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /home/hendra11/tensorflow_datasets/mnist/3.0.1...\r\nWARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\r\nlocal data directory. If you'd instead prefer to read directly from our public\r\nGCS bucket (recommended if you're running on GCP), you can instead pass\r\n`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\r\n\r\nDl Completed...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:04<00:00,  1.08s/ file]\r\nDl Completed...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:04<00:00,  1.23s/ file]\r\nDataset mnist downloaded and prepared to /home/hendra11/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\r\n```", "@adam-grant-hendry \r\nPlease refer to issues with similar error reported and let us know:\r\n#39851 [link](https://stackoverflow.com/questions/63177156/tensorflow-dataloading-issue) [link1](https://stackoverflow.com/questions/63549478/error-loading-tensorflow-datasets-error-with-google-clound)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42985\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42985\">No</a>\n"]}, {"number": 42984, "title": "tf.GradientTape().gradient() returns None", "body": "I am using a training loop with GradientTape, however, it returns a list of Nones : [None, None, None, None, None, None, None, None]\r\n\r\nColab Code : https://colab.research.google.com/gist/26345211/3fa3d06165d1ffa0ddf66b3847933e11/untitled1.ipynb", "comments": ["Please, can you compile the ISSUE template and post a very minimal but complete code snippet or Colab to reproduce this?", "Colab code to reproduce problem https://colab.research.google.com/gist/26345211/3fa3d06165d1ffa0ddf66b3847933e11/untitled1.ipynb\r\n\r\nThanks", "Are you sure about how you have organized your code?\nIf you want to customize `fit` please thake a look at https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit", "i was using the method suggested by https://keras.io/guides/writing_a_training_loop_from_scratch/ because i need to control the batches, having different sizes for each train step, may i ask should the tape.gradient function return a list of Nones when passing a <class 'tensorflow.python.framework.ops.EagerTensor'> loss and the trainable weights, thanks\r\n", "Try to print `tf.trainable_variables()`. ", "[<tf.Variable 'dense_170/kernel:0' shape=(2, 200) dtype=float32, numpy=\r\narray([[-0.11934789, -0.1352001 ,  0.15143515, -0.1664248 ,  0.08370231,\r\n        -0.0574129 ,  0.04057987, -0.11563553,  0.08567913,  0.01829536,\r\n        -0.12126006, -0.15800497, -0.02911088,  0.16553976,  0.09800433,\r\n         0.149087  , -0.09987754,  0.11516781, -0.11730537,  0.14414169,\r\n        -0.16216229,  0.0759239 , -0.0246084 ,  0.09173761,  0.14502476,\r\n        -0.16168769, -0.17053609, -0.10016961, -0.1682262 ,  0.06890272,\r\n         0.12712629,  0.09973384,  0.15260048, -0.14766552,  0.07615718,\r\n         0.03655802, -0.00600314, -0.11907463, -0.04321401, -0.00364923,\r\n        -0.02990401, -0.04188004,  0.04240695, -0.00858107, -0.03533657,\r\n         0.08475854, -0.06375738, -0.14394803,  0.11829166,  0.13027205,\r\n         0.03562872,  0.11894961,  0.13421594,  0.14814965,  0.07457224,\r\n         0.09998365,  0.17004092, -0.11659545,  0.12831531, -0.04100744,\r\n        -0.1044281 , -0.03507359, -0.12602231,  0.14934869, -0.01369838,\r\n        -0.07268311,  0.00377378,  0.02297716, -0.03628358, -0.08242856,\r\n        -0.07548593,  0.16678096,  0.10422631, -0.12451006,  0.09573375,\r\n         0.1058668 , -0.01345657,  0.08683477, -0.07413413, -0.05021814,\r\n         0.13835119,  0.13127823,  0.05542783,  0.09895901, -0.08051584,\r\n         0.12203635,  0.10648994, -0.07792949, -0.14885972,  0.11363418,\r\n         0.09021987,  0.03759827, -0.11419782, -0.14595217, -0.00871478,\r\n        -0.0214756 , -0.01005411,  0.10152201,  0.0363231 , -0.05391209,\r\n         0.14047246, -0.01496807, -0.03499922,  0.14476655, -0.0228022 ,\r\n         0.0288797 ,  0.16593175,  0.12221389, -0.01039183, -0.06863403,\r\n         0.15217276,  0.12026028,  0.07615587,  0.08128609, -0.04791162,\r\n        -0.03509092, -0.04989045,  0.0875421 ,  0.05429304,  0.08238496,\r\n        -0.13811164,  0.1360908 ,  0.03587066, -0.02798456,  0.08433725,\r\n         0.05121964, -0.09319008, -0.02171211, -0.05205952,  0.04770884,\r\n         0.01676673,  0.08286162,  0.11017866,  0.11852576,  0.14665104,\r\n        -0.12999156, -0.14146811, -0.12743834, -0.00377761, -0.01454405,\r\n        -0.11586215,  0.09552155, -0.06755261,  0.08772294, -0.17160678,\r\n         0.16554986, -0.0599165 ,  0.16171281, -0.10038833, -0.1163206 ,\r\n        -0.05406104,  0.02886513, -0.00667229, -0.06307232,  0.15343834,\r\n        -0.12317878, -0.1617271 , -0.0965053 , -0.08161932,  0.06251732,\r\n        -0.02203311, -0.01277681, -0.11201872,  0.04478924,  0.1330178 ,\r\n         0.07984813,  0.06214549, -0.0318653 ,  0.01621217, -0.15752117,\r\n         0.01456334, -0.05896328,  0.16608708, -0.10469592,  0.01082632,\r\n         0.09841321,  0.07566023, -0.11623213, -0.16790162, -0.07444404,\r\n         0.07748966,  0.0717774 , -0.03526573,  0.09977408, -0.1127055 ,\r\n        -0.08577759, -0.15779656, -0.09698769, -0.01308313, -0.03118102,\r\n        -0.0938562 ,  0.16747855,  0.1589102 , -0.04588754,  0.05656254,\r\n         0.08696176, -0.08806266, -0.04132643,  0.06806661,  0.14456044],\r\n       [-0.10428564, -0.15791386,  0.02120054,  0.11033089, -0.06676511,\r\n         0.14481051, -0.0815791 , -0.11019517,  0.16783284,  0.07278715,\r\n         0.01204659,  0.10498835, -0.03229962, -0.03176318, -0.06429406,\r\n         0.13343503,  0.17106526, -0.0557727 ,  0.0828505 , -0.14933793,\r\n         0.01169305, -0.0319875 ,  0.07549353, -0.01127364,  0.1226825 ,\r\n         0.09658615,  0.06953707, -0.1346741 ,  0.16969268, -0.14627716,\r\n         0.0014288 , -0.08763614, -0.16144164,  0.16214313,  0.17176618,\r\n         0.01518376, -0.05609345, -0.05498549,  0.06774706,  0.07839967,\r\n        -0.04208188, -0.10734637,  0.08433898,  0.09654813, -0.03963901,\r\n         0.13249324,  0.08987449, -0.11276677,  0.02028385, -0.07729493,\r\n        -0.00311604,  0.00111803, -0.09860735,  0.14777292, -0.14840819,\r\n        -0.16552101, -0.01740001,  0.07162577,  0.05117765,  0.12964584,\r\n         0.13930185, -0.16431916, -0.11170466, -0.0405774 , -0.01645522,\r\n        -0.0211271 , -0.11903223, -0.08359186,  0.13136448, -0.1012122 ,\r\n        -0.10399726,  0.15069433, -0.15947321,  0.07663617,  0.08661167,\r\n        -0.06397992, -0.15692128,  0.16765158,  0.04610191,  0.14699821,\r\n        -0.05399698,  0.00597455, -0.05686578,  0.06058389,  0.16753374,\r\n         0.06940012, -0.13194956, -0.00995463,  0.004189  , -0.0857358 ,\r\n         0.1268064 ,  0.0496822 ,  0.11057   ,  0.02427693, -0.08382649,\r\n         0.03264667, -0.10066491,  0.09526803,  0.00589877,  0.1444508 ,\r\n         0.04751974, -0.0185039 , -0.07235246, -0.15569532, -0.13325973,\r\n         0.11971502, -0.12895551,  0.14561369, -0.04767046, -0.05738258,\r\n        -0.03600778, -0.03899279, -0.11869541, -0.09502641,  0.07849739,\r\n         0.07425387, -0.06586947, -0.12955034,  0.14531638,  0.15874003,\r\n         0.08980124, -0.05513218, -0.04856721, -0.14975086, -0.15261206,\r\n         0.03739762, -0.07880352,  0.08073036,  0.00023076,  0.08112435,\r\n        -0.01473074, -0.05158723, -0.02684282, -0.00161324, -0.11474515,\r\n        -0.06409728,  0.00597031,  0.02635483, -0.15490544, -0.00992393,\r\n        -0.11707377,  0.10534434, -0.01435816, -0.13151708,  0.09501757,\r\n         0.15055238,  0.01165627,  0.00938176, -0.11095423, -0.1229444 ,\r\n        -0.05359643,  0.03379017, -0.15393159, -0.06208837, -0.03671506,\r\n        -0.10619913, -0.12907328, -0.03056993, -0.06976721, -0.08462089,\r\n         0.09103395,  0.05432582, -0.12622164,  0.1626849 , -0.08785553,\r\n        -0.05476701,  0.06668425,  0.05976455, -0.14520708, -0.10134706,\r\n        -0.09647678,  0.0860929 , -0.1499141 , -0.03556553,  0.1309581 ,\r\n        -0.15902199, -0.10812031,  0.11185198,  0.05879991, -0.02932681,\r\n         0.02434954,  0.13393985, -0.06720367, -0.1347403 ,  0.17149551,\r\n         0.1492532 ,  0.01755236, -0.01293883,  0.05481164, -0.1531047 ,\r\n         0.01992184,  0.06466942,  0.06392187,  0.10336344,  0.07975443,\r\n         0.15884219, -0.16851538,  0.13306163, -0.01791409,  0.08858497]],\r\n      dtype=float32)>, <tf.Variable 'dense_170/bias:0' shape=(200,) dtype=float32, numpy=\r\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_171/kernel:0' shape=(200, 160) dtype=float32, numpy=\r\narray([[-0.01449096,  0.11891404,  0.08263467, ..., -0.08579256,\r\n        -0.08509979, -0.00434957],\r\n       [-0.04339363, -0.05501469,  0.09409983, ...,  0.04408891,\r\n         0.02022919, -0.12373063],\r\n       [-0.06650787,  0.04460543, -0.07150993, ...,  0.00240223,\r\n         0.03154723, -0.05291989],\r\n       ...,\r\n       [-0.09521142,  0.00500374,  0.10540338, ..., -0.10856126,\r\n        -0.09479183,  0.07833758],\r\n       [ 0.05737944, -0.02785622,  0.10655414, ...,  0.12198822,\r\n         0.01530035,  0.12406929],\r\n       [-0.01402406,  0.01471251,  0.02110964, ...,  0.04371321,\r\n        -0.01435061, -0.11811783]], dtype=float32)>, <tf.Variable 'dense_171/bias:0' shape=(160,) dtype=float32, numpy=\r\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_172/kernel:0' shape=(160, 128) dtype=float32, numpy=\r\narray([[ 0.04160011,  0.14410284, -0.07255455, ...,  0.06120677,\r\n        -0.04498374,  0.05597709],\r\n       [-0.06058273,  0.080964  , -0.00544643, ...,  0.027395  ,\r\n         0.10751486,  0.13421553],\r\n       [-0.02582756,  0.02726382, -0.07045017, ...,  0.08898853,\r\n         0.08676286,  0.13659385],\r\n       ...,\r\n       [-0.0703565 , -0.08782355, -0.06662799, ...,  0.07700838,\r\n        -0.0908928 ,  0.01814529],\r\n       [-0.03862739,  0.01946414, -0.00362122, ..., -0.02184659,\r\n         0.06519993,  0.09575835],\r\n       [ 0.12136286,  0.12789068, -0.01524675, ...,  0.10884467,\r\n        -0.10154472, -0.05638957]], dtype=float32)>, <tf.Variable 'dense_172/bias:0' shape=(128,) dtype=float32, numpy=\r\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_173/kernel:0' shape=(128, 102) dtype=float32, numpy=\r\narray([[ 0.11994411, -0.08830806,  0.09730266, ..., -0.02735101,\r\n        -0.11835159, -0.1292572 ],\r\n       [-0.04823325, -0.10645641, -0.06589925, ..., -0.09688081,\r\n         0.05347882,  0.14744021],\r\n       [ 0.00104479,  0.00565375, -0.05909484, ...,  0.09337248,\r\n        -0.03811433, -0.04787574],\r\n       ...,\r\n       [ 0.08004022, -0.00987501, -0.00712064, ...,  0.1572399 ,\r\n        -0.12534381, -0.05836434],\r\n       [ 0.1083601 ,  0.02383608, -0.13886343, ...,  0.08544815,\r\n        -0.10386243,  0.06986031],\r\n       [ 0.13212802, -0.08592711, -0.09173632, ..., -0.00902449,\r\n         0.02533138,  0.09833591]], dtype=float32)>, <tf.Variable 'dense_173/bias:0' shape=(102,) dtype=float32, numpy=\r\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n      dtype=float32)>, <tf.Variable 'dense_174/kernel:0' shape=(102, 81) dtype=float32, numpy=\r\narray([[-0.13250442, -0.11466676,  0.09851864, ..., -0.15670484,\r\n         0.16924924, -0.17573619],\r\n       [-0.08160742,  0.002113  ,  0.14913452, ..., -0.00202712,\r\n        -0.07034627, -0.1408208 ],\r\n       [-0.10016426,  0.1475001 , -0.08389228, ...,  0.10627356,\r\n         0.00324109, -0.11617346],\r\n       ...,\r\n       [-0.00065637, -0.09371959, -0.06874178, ...,  0.12663746,\r\n         0.06000863, -0.00193936],\r\n       [ 0.04684326,  0.10960776, -0.02201189, ..., -0.1663261 ,\r\n        -0.02349019, -0.08495095],\r\n       [-0.15987751, -0.05693918,  0.03338403, ..., -0.14821112,\r\n         0.0755958 ,  0.12068567]], dtype=float32)>, <tf.Variable 'dense_174/bias:0' shape=(81,) dtype=float32, numpy=\r\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_175/kernel:0' shape=(81, 65) dtype=float32, numpy=\r\narray([[-0.02078907, -0.0187443 ,  0.14361703, ..., -0.08362422,\r\n        -0.10623015, -0.11473078],\r\n       [-0.01310347,  0.04681331, -0.18914118, ...,  0.03470579,\r\n        -0.09747564, -0.05477503],\r\n       [ 0.1440683 , -0.0049884 , -0.10156316, ...,  0.1902642 ,\r\n        -0.06114322,  0.11446577],\r\n       ...,\r\n       [-0.10218544, -0.1603615 ,  0.01736538, ..., -0.09153688,\r\n        -0.19702204,  0.02647094],\r\n       [-0.03277858, -0.01679772, -0.10808269, ...,  0.00512141,\r\n         0.18170598, -0.1633698 ],\r\n       [-0.04154885,  0.11578539,  0.09879005, ..., -0.07965922,\r\n        -0.20085736,  0.0203298 ]], dtype=float32)>, <tf.Variable 'dense_175/bias:0' shape=(65,) dtype=float32, numpy=\r\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n      dtype=float32)>, <tf.Variable 'dense_176/kernel:0' shape=(65, 52) dtype=float32, numpy=\r\narray([[-0.06344108, -0.11232378, -0.20549749, ..., -0.15604648,\r\n         0.02152152,  0.09581529],\r\n       [-0.14496394,  0.0125726 , -0.06720552, ..., -0.09541251,\r\n         0.12769593, -0.14659809],\r\n       [-0.1349557 ,  0.02857833, -0.05722477, ..., -0.17551786,\r\n         0.13835962,  0.10340054],\r\n       ...,\r\n       [ 0.07086708, -0.17986119,  0.05870135, ..., -0.05127212,\r\n         0.04822843,  0.01309113],\r\n       [-0.10938089, -0.05535942,  0.17507245, ...,  0.05741088,\r\n         0.10420044, -0.05306868],\r\n       [ 0.0470479 ,  0.02511998, -0.08551997, ..., -0.13162914,\r\n        -0.17675096, -0.00280182]], dtype=float32)>, <tf.Variable 'dense_176/bias:0' shape=(52,) dtype=float32, numpy=\r\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0.], dtype=float32)>, <tf.Variable 'dense_177/kernel:0' shape=(52, 41) dtype=float32, numpy=\r\narray([[ 0.24056605,  0.13016745,  0.24860209, ..., -0.10263579,\r\n        -0.22992235,  0.23266497],\r\n       [ 0.0291594 , -0.14026415, -0.02536179, ...,  0.17263454,\r\n         0.09796062, -0.11340095],\r\n       [ 0.04632729,  0.12658027,  0.02941811, ...,  0.24071139,\r\n        -0.07859647, -0.1329661 ],\r\n       ...,\r\n       [-0.16441944, -0.23054132,  0.01691437, ..., -0.07269198,\r\n        -0.02200401, -0.11094089],\r\n       [ 0.08670676, -0.1425199 ,  0.18649909, ...,  0.10516405,\r\n        -0.12943657, -0.08731034],\r\n       [-0.06016003,  0.01989737, -0.00507528, ..., -0.21756603,\r\n        -0.21989025, -0.05852264]], dtype=float32)>, <tf.Variable 'dense_177/bias:0' shape=(41,) dtype=float32, numpy=\r\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_178/kernel:0' shape=(41, 33) dtype=float32, numpy=\r\narray([[ 0.2551451 , -0.08163103,  0.09458616, ...,  0.01688999,\r\n         0.0630509 , -0.20163739],\r\n       [-0.2597019 , -0.04386258, -0.25949702, ...,  0.10323077,\r\n        -0.280416  ,  0.19154406],\r\n       [ 0.1360192 ,  0.13865757,  0.02848551, ...,  0.05804026,\r\n         0.04214838, -0.23099662],\r\n       ...,\r\n       [ 0.22179797, -0.01286918,  0.00865653, ...,  0.16882607,\r\n        -0.21707624,  0.2337515 ],\r\n       [-0.25272235,  0.16974998, -0.12176448, ..., -0.2271406 ,\r\n        -0.21278402, -0.23659456],\r\n       [ 0.27495125,  0.14714894, -0.20009963, ...,  0.20196095,\r\n         0.23976043,  0.05757129]], dtype=float32)>, <tf.Variable 'dense_178/bias:0' shape=(33,) dtype=float32, numpy=\r\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n      dtype=float32)>, <tf.Variable 'dense_179/kernel:0' shape=(33, 1) dtype=float32, numpy=\r\narray([[ 0.36110505],\r\n       [-0.31218484],\r\n       [-0.07384148],\r\n       [ 0.04206073],\r\n       [-0.24719031],\r\n       [ 0.351539  ],\r\n       [ 0.06218505],\r\n       [-0.07741374],\r\n       [ 0.09920397],\r\n       [-0.14365393],\r\n       [ 0.14466831],\r\n       [-0.3287453 ],\r\n       [-0.06388441],\r\n       [ 0.11404553],\r\n       [ 0.07901695],\r\n       [-0.40070096],\r\n       [-0.01842627],\r\n       [ 0.30787864],\r\n       [ 0.3694426 ],\r\n       [ 0.00287768],\r\n       [-0.3958364 ],\r\n       [ 0.24366245],\r\n       [ 0.26944283],\r\n       [-0.26299012],\r\n       [ 0.16940388],\r\n       [-0.16430134],\r\n       [-0.16894057],\r\n       [-0.40945658],\r\n       [ 0.35159257],\r\n       [-0.02541405],\r\n       [-0.3427515 ],\r\n       [ 0.14129457],\r\n       [-0.1145269 ]], dtype=float32)>, <tf.Variable 'dense_179/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\r\n\r\nThe obj above is the model.trainable_variables\r\npassing it to grads = tape.gradient(loss_value, model.trainable_variables) also results in Nones, thanks\r\n", "I think It Is better that you post your model on stackoverflow.\nIn the meantime you can also try ti fix in your custom loss:\n```\n/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self.obj[item] = s\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Were you able to figure out the solution ??\r\n"]}, {"number": 42983, "title": "merge master branch into child branch", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 42982, "title": "AttributeError: module 'tensorflow_addons.layers' has no attribute 'SpectralNormalization'", "body": "", "comments": ["Please, compile the ISSUE template. Thanks", "@kanishk-awadhiya,\r\nIn order to expedite the trouble-shooting process, please provide the complete code to reproduce the issue and also the TensorFlow version you are using. \r\n\r\nAlso, make sure you have updated the tensorflow_addons package to the latest version i.e. v0.11.2.\r\n\r\nI was able to import the `SpectralNormalization` module without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c819ef3de49e6d809e305dec47a67eee/42982.ipynb). Thanks!"]}, {"number": 42981, "title": "How to fix \"AttributeError: module 'tensorflow.keras.backend' has no attribute 'get_session'\"", "body": "Hello i'm trying to make a deepfake without coding (because i dont know how lmao) and i have a issue , here my code \r\n\r\nfd = MTCNNFaceDetector(sess=K.get_session(), model_path=\"./mtcnn_weights/\")\r\nsave_interval = 5 # perform face detection every {save_interval} frames\r\nsave_path = \"./faceA/\"\r\npreprocess_video(fn_source_video, fd, save_interval, save_path)\r\nsave_path = \"./faceB/\"\r\npreprocess_video(fn_target_video, fd, save_interval, save_path)\r\n\r\nthank you and sorry if i seem dumb", "comments": ["Looks like you are using TF2 where `get_session()` is deprecated.\r\nFor workarounds see https://stackoverflow.com/a/58442121/11127923 ", "@faniiiiii \r\nCan you please fill in the issue template with system details, complete code for us to replicate and tf version used so we could help you resolve the issue reported. [you may refer to [this link](https://stackoverflow.com/questions/56646940/how-to-properly-saving-loaded-h5-model-to-pb-with-tf2)] #40869\r\n\r\nPlease try : [try to replace tf.keras.backend.get_session() with tf.compat.v1.keras.backend.get_session()]\r\n\r\nFor tensorflow prior to version 2, do:\r\nfrom tensorflow.compat.v1.keras import backend as K\r\nK.set_session()", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42979, "title": "bazel-bin does not contain full API", "body": "I a trying to use C++ API of tensorflow 2.x after run \"bazel build //tensorflow:libtensorflow_cc.so\" command build was succeeded but some of the API are missing in bazel-bin\\tensorflow like \"tensorflow\\tensorflow\\core\\public\" is I am doing anything wrong?\r\n**System information**\r\n- OS Platform and Distribution (Window 10):\r\n- TensorFlow installed from (source): yes\r\n- TensorFlow version: 2.3.0 (latest)\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 3.4.1\r\n- CUDA/cuDNN version: No GPU support \r\n\r\n\r\n\r\n\r\n**exact sequence of commands**\r\nI follow [this](https://www.tensorflow.org/install/source_windows) but change build option as bazel build //tensorflow:libtensorflow_cc.so\r\n\r\n**logs**\r\nthis is the following output I am getting after running \"bazel build //tensorflow:libtensorflow_cc.so\" command\r\n```\r\nC:\\Users\\username\\Documents\\TF\\tensorflow>bazel build //tensorflow:libtensorflow_cc.so\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\users\\username\\documents\\tf\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/python/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\username\\documents\\tf\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from c:\\users\\...\\documents\\tf\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/python/python.exe --action_env PYTHON_LIB_PATH=C:/python/lib/site-packages --python_path=C:/python/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\username\\documents\\tf\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\username\\documents\\tf\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\users\\username\\documents\\tf\\tensorflow\\.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:windows in file c:\\users\\username\\documents\\tf\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\username\\documents\\tf\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Analyzed target //tensorflow:libtensorflow_cc.so (202 packages loaded, 19125 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow:libtensorflow_cc.so up-to-date:\r\n  bazel-bin/tensorflow/libtensorflow_cc.so\r\nINFO: Elapsed time: 37794.951s, Critical Path: 1324.20s\r\nINFO: 8556 processes: 8556 local.\r\nINFO: Build completed successfully, 10656 total actions\r\n\r\n```", "comments": ["it may be an oversight. Would you like to send a Pull request adding the dependency to libtensorflow_cc ?", "Thanks for responding back to me.I don' have anything to check in since I didn't change any code what I did just follow the build instruction and all I got is bazel build folder in /tensorflow i.e\r\n/tensorflow/bazel-bin/\r\n/tensorflow/bazel-out/\r\n/tensorflow/bazel-tensorflow/\r\n/tensorflow/bazel-testlogs/\r\nIn total all of size near 38.2 GBs \r\nAll I want is to use c++ API of tensorflow.", "bazel-* directory sizes are expected, because they include all intermediate files created by bazel. Sometimes multiple times!\r\nWhat you need is only this file: `bazel-bin/tensorflow/libtensorflow_cc.so` and the headers, which are less than 1GB.\r\n@av8ramit do we have the C++ packages built and uploaded to GCS?\r\nCould you share which script our user should use to get the C++ distributable for TF?\r\n\r\n@Aksh-kumar if we provided a prebuilt tar package, would you be able to use that instead?", "for tar package It need to contain all header file and libtensorflow_cc.dll, and libtensorflow_cc.lib beacause I want to use in c++ project and I want to use all [c++APIs](https://www.tensorflow.org/api_docs/cc)  which present in\r\ntensorflow/c\r\ntensorflow/cc\r\ntensorflow/core\r\n.....", "@gunan unfortunately we only have the C API uploaded to [GCS](https://storage.googleapis.com/libtensorflow-nightly) via our libtensorflow distributable. Libtensorflow_cc is being planned for Q4 of this year.\r\n\r\n", "@Aksh-kumar all of those libraries should already be linked into `bazel-bin/tensorflow/libtensorflow_cc.so`\r\nFor the headers, you can simply copy the headers from the source tree, you do not need to look for them under bazel-* folders.", "ok @gunan I will try that one.", "It works for me I added the tensorflow/tensorflow path in include directory. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42979\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42979\">No</a>\n", "I just want to know Is there is any easier way to use tensorflow c++ API, since I am facing lot of difficulties, I google this thing but I could not found any reliable solution, I even try to build by using bazel, build was succeeded but the problem I am facing. It does not contain full header file, for example array_ops.h present in \"bazel-bin/tensorflow/cc/ops/array_ops.h\" location an standerd_ops.h present in \"tensorflow/cc/ops/standard_ops.h\" location, even I include both of these I am getting conflicting declaration  while using them like while declaring  tensorflow::GraphDef graph_def I am getting conflicting declaration since it is present in multiple places i.e. tensorflow/cc/framework/scope.h and tensorflow/core/graph/graph.h and one more place so ultimately build is failing, can you please suggest me any simpler way to use it.", "Unfortunately, there is no simpler way that I know of at this time until we start `libtensorflow_cc` distributed builds.", "thanks for responding back to me, can you please tell me when we will expect c++ API .. any rough estimation?", "We are trying our best to deliver them in Q4, but we also welcome community support.", "Thanks I will definitely try to give my best in community support.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42979\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42979\">No</a>\n"]}]