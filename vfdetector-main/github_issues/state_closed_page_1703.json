[{"number": 1810, "title": "segmentation fault 11 when call Node's name method", "body": "I try to write a cnn using tensorflow and I want to use the c++ api. When I create a node using the api in   cc folder, it is fine, for example:\nNode\\* images_identity = Identity(images, b.opts().WithName(\"images_identity\"));\nbut when I use images_identity->name(), it will show a segmentation fault 11. It confuses me a lot. How should i get the name of the node if i want to ?\n", "comments": ["However when I call image_identity->DebugString(), it succeeds. I see in the source code that DebugString() calls name(). But in the return string there are messy code,the character '{' all become messy code while others are correct.\n", "I use tensorflow by compiling the source code into a dynamic-link library named libtensorflow_cc.so using the following command :\nbazel build -c opt //tensorflow:libtensorflow_cc.so\nand then I write the above code in a main function. At last I got a executable file but error (segmentation fault 11) appears in the code images_identity->name() on my mac. But the DebugString method can be invoked correctly.\n", "@josh11b: Did you get a chance to look at this?  @cuiguoxin: Could you try running valgrind on TensorFlow to see where the problem begins?    \n", "Could this be related to #2646 ?\n", "@cuiguoxin Closing this due to inactivity and it seems to most likely be the same issue as #2646. Feel free to open it again if you can indeed reproduce with a recent version of TensorFlow.\n"]}, {"number": 1809, "title": "add a wiki reference for CNN in tutorial", "body": "add a wiki reference for CNN in tutorial\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1808, "title": "Python 3.5 Wheel", "body": "Thanks for all your work so far.\n\nIs there any chance of providing a wheel for Python 3.5? I have been having a rough time trying to install it from scratch and a wheel would be very helpful!\n", "comments": ["This is most certainly not a recommended approach, but I downloaded the py34 wheel and simply renamed the file to py35 and tried installing it. It has been working so far.\n", "It should work, we don't do anything special for 3.5 -- the only difference would be the name (in fact, our Mac wheels are built with 3.5 without any changes).\n\nWe may in future provide more binaries, ideally directly in pypi, but I will close this issue for now.\n", "With PR #2585, we now have Linux Python 3.5 whl files built and tested nightly. The links to the whl files and build history can be found in the main README.md: \nhttps://github.com/tensorflow/tensorflow/\n", "Fantastic, many thanks for preparing the 3.5 wheel, it is much appreciated.\n"]}, {"number": 1807, "title": "KeyError: u'PlaceholderWithDefault' while running transfer learning model through Python", "body": "I followed the transfer learning example, and trained on my own image to create my model file. Then in the classify_image.py file, in place of the original 'classify_image_graph_def.pb' file, I replaced it with my model. However, while running I get the following error:\n\n```\nTraceback (most recent call last):\n  File \"classify_image.py\", line 214, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"classify_image.py\", line 210, in main\n    run_inference_on_image(image)\n  File \"classify_image.py\", line 158, in run_inference_on_image\n    create_graph()\n  File \"classify_image.py\", line 141, in create_graph\n    _ = tf.import_graph_def(graph_def, name='')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 227, in import_graph_def\n    op_def = op_dict[node.op]\nKeyError: u'PlaceholderWithDefault'\n\n```\n\nI can run my model fine through the terminal using bazel. Only through Python it is crashing. I'm using version 0.7.1.\nDo I need to make any more changes in code before running?\n", "comments": ["It sounds like you're using github sources from master/HEAD, rather than from r0.7 branch.  Try installing one of our pip nightlies if you want to use source code from HEAD.\n", "@vrv I checked out to branc r0.7, and tried doing the same with my previously created model, but I still get the same error?\n", "If you created the model with HEAD, it's going to have ops have been added since r0.7, so that's not going to work :(.  \n", "But the image retraining thing mentioned [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/image_retraining/index.md). That is not available in the r0.7 branch. How do I run such model?\n", "I reinstalled tensorflow with this [release](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-slave/lastStableBuild/), now I am getting a different error.\n\n>  Traceback (most recent call last):\n>   File \"classify_image.py\", line 214, in <module>\n>     tf.app.run()\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n>     sys.exit(main(sys.argv))\n>   File \"classify_image.py\", line 210, in main\n>     run_inference_on_image(image)\n>   File \"classify_image.py\", line 169, in run_inference_on_image\n>     softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2305, in get_tensor_by_name\n>     return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2202, in as_graph_element\n>     \"graph.\" % (repr(name), repr(op_name)))\n> KeyError: \"The name 'softmax:0' refers to a Tensor which does not exist. The operation, 'softmax', does not exist in the graph.\"\n", "@vrv Sorry to make this seem like a progress report, but I fixed all the above issues, the code is running now, but for whatever input image I seem to give as input, it's giving me the same output. Same prediction values. I am not able to get the output I got from running it via bazel. What could be the problem here?\n", "@eldor4do\n\n> @vrv Sorry to make this seem like a progress report, but I fixed all the above issues, the code is running now, but for whatever input image I seem to give as input, it's giving me the same output. Same prediction values. I am not able to get the output I got from running it via bazel. What could be the problem  here?\n\nHow'd you fix it?\n", "@janislejins Updating tensorflow should do the trick.\n"]}, {"number": 1806, "title": "Added support for unsupervised data feeder", "body": "", "comments": []}, {"number": 1805, "title": "Update skflow README", "body": "I will make a note in tensorflow/skflow to direct new users to post issues and PRs here instead. \n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 1804, "title": "add Gradient Descent reference in tutorial", "body": "Add a wiki reference for Gradient Descent in TF mechanics 101\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins , test this please.\n"]}, {"number": 1803, "title": "Core Dump Error on running examples", "body": "I am getting Core dump errors on running the example codes (e.g convolutional.py on mnist).  Installed it through pip (Linux 64 bit, gpu enabled).  The output is below. \n\nDoing simple graph construction  ( [here](https://www.tensorflow.org/versions/r0.7/how_tos/using_gpu/index.html) ) to check gpu device works fines. \n\nI already checked [1534](https://github.com/tensorflow/tensorflow/issues/1534) but in my case the compute mode for GPU is set to default. So I don't think that is the problem. \n\nWhat could be the problem then ? \n\n---\n\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:02:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.60GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0xf047a0000 extends to 0x11aa9d1e67\nFloating point exception (core dumped)\n\n---\n", "comments": ["Can you please try running your script under `gdb`, to get a stack trace?\n", "I am not very experienced with gdb but I quickly ran the script under gdb. The output and the backtrace is shown below. Please let me know if this is not what you were looking for ?  \n\n---\n\nGNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1\nCopyright (C) 2014 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later http://gnu.org/licenses/gpl.html\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\nhttp://www.gnu.org/software/gdb/bugs/.\nFind the GDB manual and other documentation resources online at:\nhttp://www.gnu.org/software/gdb/documentation/.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from python...Reading symbols from /usr/lib/debug//usr/bin/python2.7...done.\ndone.\nStarting program: /usr/bin/python -m tensorflow.models.image.mnist.convolutional\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n[New Thread 0x7ffff4b12700 (LWP 30948)]\n[New Thread 0x7ffff4311700 (LWP 30949)]\n[New Thread 0x7ffff1b10700 (LWP 30950)]\n[New Thread 0x7fffef30f700 (LWP 30951)]\n[New Thread 0x7fffecb0e700 (LWP 30952)]\n[New Thread 0x7fffea30d700 (LWP 30953)]\n[New Thread 0x7fffe7b0c700 (LWP 30954)]\n[New Thread 0x7fffdf198700 (LWP 30955)]\n[New Thread 0x7fffde997700 (LWP 30956)]\n[New Thread 0x7fffdc196700 (LWP 30957)]\n[New Thread 0x7fffd9995700 (LWP 30958)]\n[New Thread 0x7fffd7194700 (LWP 30959)]\n[New Thread 0x7fffd4993700 (LWP 30960)]\n[New Thread 0x7fffd2192700 (LWP 30961)]\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\n[New Thread 0x7fffb5df9700 (LWP 30962)]\n[New Thread 0x7fffb55f8700 (LWP 30963)]\n[New Thread 0x7fffb4df7700 (LWP 30964)]\n[New Thread 0x7fffb45f6700 (LWP 30965)]\n[New Thread 0x7fffb3df5700 (LWP 30966)]\n[New Thread 0x7fffb35f4700 (LWP 30967)]\n[New Thread 0x7fffb2df3700 (LWP 30968)]\n[New Thread 0x7fffa3fff700 (LWP 30969)]\n[New Thread 0x7fffa37fe700 (LWP 30974)]\n[New Thread 0x7fffa2ffd700 (LWP 30975)]\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:02:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00GiB\n[New Thread 0x7fff9ebfc700 (LWP 30976)]\n[New Thread 0x7fff9e3fb700 (LWP 30977)]\n[New Thread 0x7fff9dbfa700 (LWP 30978)]\n[New Thread 0x7fff91986700 (LWP 30979)]\n[New Thread 0x7fff91185700 (LWP 30980)]\n[New Thread 0x7fff90984700 (LWP 30981)]\n[New Thread 0x7fff73fff700 (LWP 30982)]\n[New Thread 0x7fff737fe700 (LWP 30983)]\n[New Thread 0x7fff72ffd700 (LWP 30984)]\n[New Thread 0x7fff727fc700 (LWP 30985)]\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.60GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0xf047a0000 extends to 0x11aa9d1e67\n\nProgram received signal SIGFPE, Arithmetic exception.\n[Switching to Thread 0x7fff737fe700 (LWP 30983)]\n0x00007fffc6dc7182 in tensorflow::functor::FillPhiloxRandom<Eigen::GpuDevice, tensorflow::random::TruncatedNormalDistribution<tensorflow::random::SingleSampleAdapter<tensorflow::random::PhiloxRandom>, float> >::operator()(tensorflow::OpKernelContext_, Eigen::GpuDevice const&, tensorflow::random::PhiloxRandom, float_, long long, tensorflow::random::TruncatedNormalDistributiontensorflow::random::SingleSampleAdapter<tensorflow::random::PhiloxRandom, float>) ()\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n\n**--------backtrace-------**\n\n#0  0x00007fffc6dc7182 in tensorflow::functor::FillPhiloxRandom<Eigen::GpuDevice, tensorflow::random::TruncatedNormalDistribution<tensorflow::random::SingleSampleAdapter<tensorflow::random::PhiloxRandom>, float> >::operator()(tensorflow::OpKernelContext_, Eigen::GpuDevice const&, tensorflow::random::PhiloxRandom, float_, long long, tensorflow::random::TruncatedNormalDistributiontensorflow::random::SingleSampleAdapter<tensorflow::random::PhiloxRandom, float>) ()\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#1  0x00007fffc68d5ba6 in tensorflow::(anonymous namespace)::PhiloxRandomOp<Eigen::GpuDevice, tensorflow::random::TruncatedNormalDistribution<tensorflow::random::SingleSampleAdapter<tensorflow::random::PhiloxRandom>, float> >::Compute(tensorflow::OpKernelContext_) ()\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007fffc6d00886 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel_, tensorflow::OpKernelContext_) ()\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fffc6eda98a in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fffc6ece560 in std::_Function_handler<void (), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::_)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fffc6f8c4e7 in tensorflow::thread::ThreadPool::WorkerLoop() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fffc4deda60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n#7  0x00007ffff7bc4182 in start_thread (arg=0x7fff737fe700) at pthread_create.c:312\n#8  0x00007ffff78f147d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\n(gdb)\n", "Thanks, that's super helpful! So now we know that the issue is in one of the [random ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_op.cc) and it's probably `tf.truncated_normal()`.\n\nTo help us track this down, could you please share your Python code for where you invoke `tf.truncated_normal()`? This will make it obvious if there's a corner case that's not being handled.\n", "I am simply trying to run the example convolutional.py code (for mnist dataset) provided with tensorflow.  tensorflow.models.image.mnist.convolutional to be precise. \n", "Not sure if it is related, but I seem to be getting a SIGFPE as well. This is while trying seq2seq:\n\nProgram terminated with signal SIGFPE, Arithmetic exception.\n#0  0x00007f4755d30d3c in tensorflow::functor::FillPhiloxRandom<Eigen::GpuDevice, tensorflow::random::UniformDistribution<tensorflow::random::PhiloxRandom, float> >::operator()(tensorflow::OpKernelContext_, Eigen::GpuDevice const&, tensorflow::random::PhiloxRandom, float_, long long, tensorflow::random::UniformDistribution<tensorflow::random::PhiloxRandom, float>) () from /deep/u/avati/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n(gdb) bt\n#0  0x00007f4755d30d3c in tensorflow::functor::FillPhiloxRandom<Eigen::GpuDevice, tensorflow::random::UniformDistribution<tensorflow::random::PhiloxRandom, float> >::operator()(tensorflow::OpKernelContext_, Eigen::GpuDevice const&, tensorflow::random::PhiloxRandom, float_, long long, tensorflow::random::UniformDistribution<tensorflow::random::PhiloxRandom, float>) () from /deep/u/avati/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#1  0x00007f4755d2d276 in tensorflow::(anonymous namespace)::PhiloxRandomOp<Eigen::GpuDevice, tensorflow::random::UniformDistribution<tensorflow::random::PhiloxRandom, float> >::Compute(tensorflow::OpKernelContext_) ()\n   from /deep/u/avati/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007f4755e65fc6 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel_, tensorflow::OpKernelContext_) () from /deep/u/avati/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007f475604e219 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) () from /deep/u/avati/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007f4756042bf0 in std::_Function_handler<void (), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::_)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) () from /deep/u/avati/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007f47561a617d in tensorflow::thread::ThreadPool::Impl::WorkerLoop() () from /deep/u/avati/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007f4753ac3a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n#7  0x00007f4776b52182 in start_thread (arg=0x7f46e57fa700) at pthread_create.c:312\n#8  0x00007f477617247d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\n", "Thanks for sending in these stack traces. It does look like an issue (perhaps due to an unexpected input?) in the `tensorflow::functor::FillPhiloxRandom` class that's used to compute random tensors (in e.g. `tf.truncated_normal()`.\n\nI'm not particularly familiar with this code, so I'll hand off to @zheng-xq, who wrote it.\n", "It turns out I had missed seeing this error message -\n\nFailed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\npython: external/eigen_archive/eigen-eigen-4c94692de3e5/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceCuda.h:41: void Eigen::initializeDeviceProp(): Assertion `status == cudaSuccess' failed.\n\nAfter upgrading CUDA driver, things progress fine now!\n", "@mrry, @zheng-xq: This whole \"hand off\" thing doesn't seem to work very well. :)\n\n@anuragkr90: Is your issue different from @avati's?\n", "@girving Yes, it is different from @avati's. I am not getting the CUDA devices error.  \n", "@anuragkr90: A floating point exception typically means integer division by zero.  Could you recompile the code in debug (pass `-c dbg` to Bazel) and rerun gdb to see where it's dividing by zero?  Otherwise we'll probably need a minimized test case for reproduction purposes.\n", "@anuragkr90, I am not able to reproduce this problem locally. \n\nCould you try to upgrade your Cuda driver, and use the Cuda SDK, and see if the problem still persists? \n", "\ud83d\ude2c\n", "@girving  @zheng-xq I think I have updated drivers. But in any case let me redo everything from scratch\n", "Hello! \nI am getting the same error. In my code I basically put names of images in a FIFO, then read images,randomly cropped them and pass the result to a tf.train.batch with 3 or more threads.\n\nWhen I remove the random_crop the error doesn't occur.\nWhen I use 1 or 2 threads, the error occur time to time and always with more.\nWhen I run it in gdb I get  the same backtrace.\n`\n", "There have been a lot of a code changes since this was last posted. Have @anuragkr90  @pescam managed to try it on one of our later builds? If not, I will close this due to lack of activity.\n", "I think the issue can be closed from my side. I don't face the same issue on other machines. \n", "I faced the same problem with the r0.9 version and solved it by moving the creation of the Coordinator and the call to `start_queue_runners` after the variable initialization with `sess.run(tf.initialize_all_variables())`.\n", "I didn't try it on latest build but JulienB-78 solution solved my problem.\n", "Closing this for now, since workaround seems to be in place.\n"]}, {"number": 1802, "title": "Max Pooling Across Batch Dimension", "body": "I would like to use Tensor Flow to do max pooling over images. For example, with 10 images of size 500x500, this would yield a single image of size 500x500, where each pixel is the max of that pixel coordinate across the 10 images.\n\nTo do this, I tried:\n\n```\n`image_pool = tf.nn.max_pool(input, ksize=[10, 1, 1, 1], strides=[10, 1, 1, 1], padding='VALID')`\n```\n\nHere, the ideas is that for each batch of images, every sub-batch of 10 images is pooled into a single image. However, this gives me the error message:\n\n```\n`ValueError: Current implementation does not support pooling in the batch dimension.`\n```\n\nTherefore it appears that right now, this function is not yet supported in this way. So, firstly, it would be great if a later version would support this. And secondly, I'm wondering if there is another way to achieve what I want by using existing tools in Tensor Flow, or otherwise?\n", "comments": ["Easiest thing to do is to tf.split() your tensor into batches of 10, call tf.reduce_max() across each of those tensors along the batch dimension, and then concat them back together.  It sounds like you're not really looking to slide a window over the batch dimension since your kernel size is equal to your stride.\n"]}, {"number": 1801, "title": "Can't reshape tensor", "body": "I'm getting a strange error when trying basic reshape operations:\n\n``` python\nimport tensorflow as tf\nt = tf.constant([0, 1])\ntf.reshape([2, 1], t) # should be [[0], [1]]\n```\n\nThis gives me the following error:\n\n```\nValueError: Cannot reshape a tensor with 2 elements to shape [0, 1] (0 elements)\n```\n", "comments": ["your arguments to reshape are backwards\n"]}, {"number": 1800, "title": "Update tensorflow to use new version of protobuf library", "body": "This is necessary to remove the need for libpthread workarounds when building binaries for Android (as seen in tensorflow/examples/android/BUILD).\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@googlebot I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins: test this please\n", "@martinwicke @vrv Is there a way to view the failing test.log files for this PR? I'm also getting 233 test failures when I try to test a clean copy of master locally:\n\n```\nFile \"/usr/local/google/home/andrewharp/.cache/bazel/_bazel_andrewharp/111e4735042cad380f0b87cff4bce3f6/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/user_ops/ackermann_test.runfiles/google/protobuf/symbol_database.py\", line 180, in <module>\n    _DEFAULT = SymbolDatabase(pool=descriptor_pool.Default())\nAttributeError: 'module' object has no attribute 'Default'\n```\n\nAnd 233 test failures locally with this branch, but with a different error:\n\n```\nFile \"/usr/local/google/home/andrewharp/.cache/bazel/_bazel_andrewharp/7d6b411acf30faa7da8a8cd3f21cf053/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/user_ops/ackermann_test.runfiles/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: cannot import name descriptor\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n```\n", "Also, just switching branches gives me issues where it thinks I have a pending commit to google/protobuf. \n\nIf I'm on my local master branch and do \"git checkout update_pb_lib\" and it tells me I have a pending commit to google/protobuf (it wants to make it go back to using the old version).\n\nThen I do \"git checkout update_pb_lib -- google/protobuf\" because I want to revert this unwanted commit and use the version I selected in \"update_pb_lib\", but it just ignores it and the pending commit remains active. I assume git is treating it differently from a regular file because it's actually a submodule, but I'm not sure how I'm supposed to switch local branches cleanly with this behavior.\n", "It seems like `git submodule update --init --recursive` does the trick to wipe the incorrect pending local commit.\n", "@tensorflow-jenkins: test this please\n", "@davidzchen \nI've updated my commit to get past the initial build errors related to https://github.com/google/protobuf/commit/985c968443e5124327fb600a91856192df4476ac, but now I get this:\n\n> ImportError: Traceback (most recent call last):\n>   File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-fastbuild/bin/tensorflow/python/string_to_number_op_test.runfiles/tensorflow/python/__init__.py\", line 49, in <module>\n>     from tensorflow.core.framework.graph_pb2 import *\n>   File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/workspace/bazel-out/local_linux-fastbuild/bin/tensorflow/python/string_to_number_op_test.runfiles/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n>     from google.protobuf import descriptor as _descriptor\n> ImportError: cannot import name descriptor\n\nDoes anything stand out to you that I might be missing here? I'm not sure how it's finding google.protobuf and not descriptor google.protobuf.descriptor, given that your PR didn't do anything to descriptor specifically.\n", "That's odd. I'll try to reproduce and debug this. I also saw your comment on my PR. Are you still having that issue as well?\n\nBy the way, would it be possible to merge #1289 first? That way, we can update the protobuf dependency more easy since all we would need to do is update the WORKSPACE file rather than having to update the submodule reference. The failures in the last CI run on #1289 don't seem to be related to that PR.\n", "Thanks! Adding the extra parameters to our skylark macro calls to point it to protoc and protobuf_library fixed the original errors I mentioned in your PR. Though I suppose it's possible I did that incorrectly, leading to the current descriptor error.\n\n@vrv What do you think about getting #1289 in?\n", "Sure, making protobuf a workspace dep looks good.  Enough bazel improvements have occurred to force users to upgrade at this point (and we can now detect the version)\n"]}, {"number": 1799, "title": "One hot embedding?", "body": "I'd like to embed integer labels into a fixed dimension space via the classic one-hot embedding. Here is an example:\n\n```\nonehot(inputs=[0, 2], num_labels=4) -> [[1, 0, 0, 0], [0, 0, 1, 0]]\n```\n\nIt seems that sparse_to_dense does something similar, but not quite what I want. I've found some solutions online but they are rather convoluted. Is there a straightforward way to do this in tensorflow?\n", "comments": ["`sparse_to_dense` will do what you want, you just have to index your data first.  For your example, you would want\n\n``` python\ninputs, num_labels = [0,2], 4\nindexedInputs = [[i, inputs[i]] for i in range(len(inputs))]\ntf.sparse_to_dense(indexedInputs, [len(inputs),num_labels], 1)  # produces [[1,0,0,0], [0,0,1,0]]\n```\n", "Is there any way to do this with variable-length inputs (shape=[None])?\n", "Sure, the `tf.range` function will give you an integer range in the length of your input node, then you can reshape them both as matrices and `tf.concat` them together to get the right data matrix that you can run through `tf.sparse_to_dense`.  The documentation for those functions can help you get things set up properly.\n\nIn future, [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) is a much better place for a question like this.\n", "I did check StackOverflow but found only convoluted answers, and I wanted to ask if there wasn't a more idiomatic way to perform this fairly ubiquitous operation. Since that doesn't appear to exist, perhaps this could be a feature request for it?\n\n~~I've also tried tf.range but have encountered some errors when trying to reshape it #1801~~ Ignore that! Even so, I don't see how this would allow for variable length inputs, since tf.range needs to take a concrete limit.\n", "As a quick note, the [`tf.one_hot()`](https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#one_hot) method appears to be arriving in the next release of TensorFlow. I haven't checked, but it may also be available already in the nightly build- see the instructions to install it [here](https://github.com/tensorflow/tensorflow#installation)\n", "@samjabrahams That's great, thanks for the tip!\n", "Not a huge deal, but for some reason tf.one_hot insists on taking int64 as input - it should probably also work with the other int types. I'll just add in the cast myself for now.\n", "I've been taking a look at the code since this came up, and I think there's room for improvement. In addition to it needing to be more flexible, I think it makes sense for the `on_value` and `off_value` parameters to have default values of 1 and 0 (instead of forcing the user to define it by hand for these most common values).\n\nI was thinking about modifying it myself to get some more practice with the inner workings of the software, but I'll ping @ebrevdo to see what he thinks, as he wrote the method.\n", "You should clarify what you mean by the improvements you'd like to make.  Regarding on/off value:\n\non_value and off_value must be provided to the kernel (because they're inputs, not attributes, and as such can be tensors).  It _is_ reasonable to add defaults of 1 and 0 for the python wrapper.  You'll have to hide the auto-generated one_hot function in the BUILD rule for array_ops and then write your own python wrapper that calls gen_array_ops._one_hot().  to make sure that the default type is float32, you should use np.float32(0) for the off_value and np.float32(1) for the on value.\n\nPRs welcome; happy to review.\n", "Thanks for getting back. I'll try to describe the improvements more precisely:\n1. `on_value` / `off_value`: Like you said- create a Python wrapper that has a default value for these inputs. Thank you for the \"to-do list\" style overview- should make this process much smoother to go through!\n2. Handling different int types for `indices` and `depth`: `indices` is required to be a Tensor of `int64`, and `depth` is required to be a Tensor of `int32`. It would be nice if the user was able to pass in any integer Tensor (or at least either `int64` or `int32`) and have the method handle it. Do you think this could be handled in the Python wrapper? The idea would be to check for an integer-type Tensor and cast it to the proper format before passing it on to the kernel. It's somewhat inefficient, but very simple.\n", "Also- with this changes, should I make tweaks to the OneHot definition in [ops.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/ops.pbtxt)?\n", "It's fine to use math_ops.to_int??()  as a stop gap.  ops.pbtxt is auto\nupdated and only depends on the c++ side, you don't have to update it.\nOn Apr 9, 2016 11:06 AM, \"Sam Abrahams\" notifications@github.com wrote:\n\n> Also- with this changes, should I make tweaks to the OneHot definition in\n> ops.pbtxt\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/ops.pbtxt\n> ?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1799#issuecomment-207823500\n", "@vladfi1 : The one_hot() updates were recently merged into master. Would you like to test it out and see if it works well enough to close this issue?\n", "@samjabrahams I've been using `one_hot` for a few weeks without problems. I recently reinstalled the master branch of tensorflow with `sudo pip3 install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl` but there doesn't seem to be any update to the signature of `one_hot` - it still takes only int64 tensors and required manual on_value and off_value.\n", "You need to use a nightly build to see the change.\nOn Apr 29, 2016 10:03 AM, \"Vlad Firoiu\" notifications@github.com wrote:\n\n> @samjabrahams https://github.com/samjabrahams I've been using one_hot\n> for a few weeks without problems. I recently reinstalled the master branch\n> of tensorflow with sudo pip3 install\n> https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl\n> but there doesn't seem to be any update to the signature of one_hot - it\n> still takes only int64 tensors and required manual on_value and off_value.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1799#issuecomment-215812405\n", "Ok, I installed the nightly build, which provides default on_value and off_value. However `one_hot` still demands `int64` for the input. If I pass in `int32` I get an error:\n\n```\n>>> tf.one_hot(tf.constant(1, dtype=tf.int32), 10)\nTensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(\"Const:0\", shape=(), dtype=int32)'\n```\n", "I did some looking into this, and it looks like the error is due to using `tf.convert_to_tensor` as the way to handle different-typed tensors on [these lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1860-L1861). This works when passing in Numpy arrays, but not with TensorFlow constants.\n\n~~@ebrevdo @girving, would it be possible to import `tf.cast` in `tensorflow/python/ops/array_ops.py`? I think using that instead of `convert_to_tensor` would fix this.~~\n\nI figured out that `cast` is available from `gen_math_ops`. I'll make some tweaks and send in a pull request.\n", "@vladfi1 `one_hot` should be compatible with indices of type `int32` and `int64`, now.\n", "Fixed, so closing.\n", "what do you do if the classes are strings, not ints?", "You first convert the strings to ints via one of the helper classes/functions in `tf.lookup`.  You will need to have a vocabulary in mind that maps strings to integers."]}, {"number": 1798, "title": "Saving custom graph collections in graph_def", "body": "Would be useful to facilitate, e.g., restoring desired subset of nodes when re-loading model with graph_def if custom graph collections were persisted.\n", "comments": ["You can use import_meta_graph and export_meta_graph.\n", "@yaroslavvb Thanks for the tip.\n\nSeems that`import_meta_graph` requires instantiating `tf.train.Saver`, which fails with `ValueError: No variables to save` - possibly because I am essentially freezing variables via assign ops once trained. Is there no way to divorce the Saver protocol from restoring a graph and corresponding collections ?\n", "oooh, that's no bueno, @sherrym may know a workaround\n", "You should be to do this:\n\n``` Python\n  with tf.Session() as sess:\n    new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')\n    new_saver.restore(sess, 'my-save-dir/my-model-10000')\n    # tf.get_collection() returns a list. In this example we only want the\n    # first one.\n    train_op = tf.get_collection('train_op')[0]\n    for step in xrange(1000000):\n      sess.run(train_op)\n```\n", "@meereeum Any updates?\n", "Hi @sherrym , no luck so far with `meta_graphs`.\n\nmeta_graph is saved successfully (or, at least, no errors thrown) via:\n\n```\ntf.train.export_meta_graph(filename=outfiles['meta'], collection_list=[self.restore_key],\n                           graph_def = sesh.graph_def, saver_def = tf.train.SaverDef())\n```\n\nTrying to restore via:\n\n```\ntf.train.import_meta_graph(outfiles['meta'])\n```\n\nThrows: `IOError: Cannot parse file ./model.meta: 2:1 : Expected identifier..`\n\nNot sure if this is the problem, but I have not found any documentation of what the `meta_info_def` kwarg (of `export_meta_graph`) expects.\n", "What did you set to outfiles['meta'] to? Could you pass in an absolute path to your meta file, something like this:\n\ntf.train.import_meta_graph(\"/tmp/model.meta\")\n\nif your model.meta file is indeed in /tmp?\n\nSherry\n", "The instructions and examples for using meta_graph can be found here:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/meta_graph/index.md\n", "@meereeum: Closing for now since it seems like it might have been caused by a malformed use of `import_meta_graph`.  I'm happy to reopen if it's a persistent issue. \n"]}, {"number": 1797, "title": "No way to re-enter previous name/variable-scoped context", "body": "As far as I can tell, there is no way to re-enter the same name_scope or variable_scope (once closed) without the name being \"unique-ified.\"\n\nThe [example in the docs](https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#Graph.name_scope) shows how to re-enter the original scope from within a nested scope - but not when context is closed.\n\nPerhaps this could be solved by enabling access to the scope reference as `tf.Variable.scope` ?\n\nCurrent hack-y workaround is to manually specify variable name as `\"{}/name\".format(previous_scope_name)`\n", "comments": ["It is actually possible to re-enter a closed variable scope, as in the example below.\n\n``` python\nwith tf.variable_scope('test') as vscope: pass\nwith tf.variable_scope(vs) as vscope_:\n     assert vscope.name == vscope_.name\n```\n\nHowever, the re-entered variable scope does not copy the reuse status of the original one.\n", "@gaohuazuo hmmm that assert statement returns True (as long as you are consistent about `vscope` vs `vs`); however...\n\n```\nwith tf.variable_scope('foo') as vs:\n    x = tf.Variable([1])\nwith tf.variable_scope(vs) as vs_:\n    assert vs.name == vs_.name\n    y = tf.Variable([1])\nx.name\n>>> u'foo/Variable:0'\ny.name\n>>> u'foo_1/Variable:0'\n```\n\nwith corresponding non-overlapping name-scopes in tensorboard. This is not changed by passing `reuse = True`.\n", "Anyway, if possible I think it would be useful to be able to access name-scope of a given variable....without writing a (simple but) hacky regex. The current approach I am using to build layers in a more functional manner does not lend itself to the workflow described above.\n", "@ebrevdo: @meereeum's example with `y.name == 'foo_1/Variable:0'` looks like a bug.  Could you take a look?\n", "I'll assign to lukasz, he may have code fixing this.\n", "I'm working on a CL/PR that at least adds the name scope to variable scope. Will report back here once that's in.\n", "A recent CL correcting this is now live on master. It includes a unit test that re-enters a variable scope and the original name scope this variable scope was created in (by using the new .original_name_scope property). See here: [variable_scope_test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/variable_scope_test.py#L297). Closing this now, please re-open or make a new issue if you find more problems!\n"]}, {"number": 1796, "title": "Tensorflow serving with transfer learning retrained inception model", "body": "Hi\n\nTrying to use a retrained inception model (as in the transfer learning tutorial) with Tensorflow serving as described by the example, [here](https://tensorflow.github.io/serving/serving_basic). However, I am unclear how to set up the exporter.\n\nThe exporter flow as shown in the tutorial looks as such:\n\n`\n export_path = sys.argv[-1] print 'Exporting trained model to', export_path saver = tf.train.Saver(sharded=True) model_exporter = exporter.Exporter(saver) signature = exporter.classification_signature(input_tensor=x, scores_tensor=y) model_exporter.init(sess.graph.as_graph_def(), default_graph_signature=signature) model_exporter.export(export_path, tf.constant(FLAGS.export_version), sess) \n`\n\nSince there is no \"input_tensor\" and \"scores_tensor\" as described by this tutorial within the Inception transfer learning, I am unsure of what tensors to use to get the signature. The bottleneck tensor? The ground_truth tensor?\n\nI'd love an explanation of how to set up this retrained model with Tensorflow Serving as shown in this example!\n\nThanks\n\nOren\n", "comments": ["retrain.py is to take a trained inception model, reuse all layers up to bottleneck layer, and train a new top layer for new image classifications. Therefore, from an inference perspective, the input is the same as the inception model being used, and the output is the output of the new top layer.\n\ninput_tensor=jpeg_data_tensor\nscores_tensor=final_tensor\n", "Closing since this seems to be a (hopefully answered?) question rather than an issue that needs fixing.  Please ask to reopen if you think a doc-fix is warranted, or if there's a real bug here. \n", "I'm struggling to server a 'fine tuned' retrained inceptions model. After retraining on just two classes I get an error when trying to export:\n\n`File \"/home/ubuntu/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/clie\nnt/session.py\", line 743, in _do_call                                                                                            \n    raise type(e)(node_def, op, message)                                                                                         \ntensorflow.python.framework.errors.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [1001] rhs s\nhape= [3]                                                                                                                        \n         [[Node: save/Assign_30 = Assign[T=DT_FLOAT, _class=[\"loc:@logits/logits/biases\"], use_locking=true, validate_shape=true,\n _device=\"/job:localhost/replica:0/task:0/cpu:0\"](logits/logits/biases, save/restore_slice_30)]]                                 \nCaused by op u'save/Assign_30', defined at:`\n", "```\r\nclassify_inputs_tensor_info = tf.saved_model.utils.build_tensor_info(\r\n        jpeg_data_tensor)\r\nclasses_output_tensor_info = tf.saved_model.utils.build_tensor_info(\r\n        **???**)\r\nscores_output_tensor_info = tf.saved_model.utils.build_tensor_info(final_tensor)\r\n```\r\n\r\nThank you fro your answer @fangweili. Btw, what should we fill in **classes_output_tensor_info**? Thank you @fangweili "]}, {"number": 1795, "title": "Add Cuda/Cudnn version request", "body": "Helps with GPU issues.\n", "comments": ["Done.\n"]}, {"number": 1794, "title": "Attempt at adding support for cudnnr5 API only (no LSTM/RNN additions", "body": "yet).\n\nTested only on R5 on a subset of tests so far.  Have not yet\ntested that it doesn't break R4 -- Jenkins tests should cover\nthat.\n", "comments": ["Jason from Stream Executor looked at this and gave his LGTM, so merging.\n"]}, {"number": 1793, "title": "Missing gradient for tf.nn.max_pool_with_argmax", "body": "I swapped out the first max_pool operation in [convolutional.py](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/image/mnist/convolutional.py) tutorial for a max_pool_with_argmax operation and there doesn't seem to exist a gradient for that op:\n\n`(virtenv)$ python convolutional.py`\n`Extracting data/train-images-idx3-ubyte.gz`\n`Extracting data/train-labels-idx1-ubyte.gz`\n`Extracting data/t10k-images-idx3-ubyte.gz`\n`Extracting data/t10k-labels-idx1-ubyte.gz`\n`Traceback (most recent call last):`\n`File \"convolutional.py\", line 316, in <module>`\n`tf.app.run()`\n`File \"/home/name/virtenv/local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run`\n`sys.exit(main(sys.argv))`\n`File \"convolutional.py\", line 244, in main`\n`global_step=batch)`\n`File \"/home/name/virtenv/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 190, in minimize`\n`colocate_gradients_with_ops=colocate_gradients_with_ops)`\n`File \"/home/name/virtenv/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 241, in compute_gradients`\n`colocate_gradients_with_ops=colocate_gradients_with_ops)`\n`File \"/home/name/virtenv/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 453, in gradients`\n`(op.name, op.type))`\n`LookupError: No gradient defined for operation 'MaxPoolWithArgmax' (op type: MaxPoolWithArgmax)`\n### Environment info\n\nOperating System: Ubuntu 14.04.1 LTS\n\nIf installed from binary pip package, provide:\n\n(virtenv)$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n(virtenv)$ python -c \"import tensorflow; print(tensorflow.**version**)\"\n0.7.1\n### Steps to reproduce\n1. Change line 192-5 of [convolutional.py](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/image/mnist/convolutional.py) from:\n   \n   `pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')`\n   \n   to:\n   \n   `pool, pos = tf.nn.max_pool_with_argmax(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')`\n2. Run:\n   \n   `(virtenv)$ python convolutional.py`\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["I had the same issue when I was trying to get the max indices.\n", "this is a bit of a hack, but copying the gradient for maxpool and registering it for max_pool_with_argmax works. The function signature has an additional arg (presumably this has to do with the argmax). Also the op doesn't have a data_format, so that is left default (NHWC).\n\n``` python\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import gen_nn_ops\n@ops.RegisterGradient(\"MaxPoolWithArgmax\")\ndef _MaxPoolWithArgmaxGrad(op, grad, some_other_arg):\n  return gen_nn_ops._max_pool_grad(op.inputs[0],\n                                   op.outputs[0],\n                                   grad,\n                                   op.get_attr(\"ksize\"),\n                                   op.get_attr(\"strides\"),\n                                   padding=op.get_attr(\"padding\"),\n                                   data_format='NHWC')\n```\n", "@mwalton: `some_other_arg` should be called `unused_argmax_grad`, but otherwise that's basically it.  Would you be interesting it submitting your change as a PR?  We'd have to add tests if so (mirroring those for the gradient of the normal max_pool.\n", "@girving  Sure! Should I go ahead and put in the PR; or implement the tests first?\n", "@mwalton: The tests should go in the same PR.\n", "@mwalton Not sure if you've made any progress on this, but I believe the correct change would be the following:\n\n``` python\n@ops.RegisterGradient(\"MaxPoolWithArgmax\")\ndef _MaxPoolGradWithArgmax(op, grad, unused_argmax_grad):\n  return gen_nn_ops._max_pool_grad_with_argmax(op.inputs[0],\n                                               grad,\n                                               op.outputs[1],\n                                               op.get_attr(\"ksize\"),\n                                               op.get_attr(\"strides\"),\n                                               padding=op.get_attr(\"padding\"))\n```\n\nSeems to be working on a model I'm training. Haven't had time to write tests for it yet, nor do I know how/what tests are needed for registering an existing op (that appears to have **some** test coverage, see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/pooling_ops_test.py#L521). \n\n@girving If you can provide some direction on what tests I'd need to add for this change, I can try to pull together a PR. \n", "@bcaine Awesome!  Tests for gradients are pretty easy: just use `tf.test.compute_gradient_error` and verify that the error is small (1e-4 or 1e-3, typically, due to numerical differences).  You can look at other `compute_gradient_error` examples and mimic.\n", "Are there any updates on this issue? It seems that adding this gradient was attempted in #4014, but not taken place due to commit conflicts. Should I cherry-pick commit dc3d4782cbdf2d98b into a PR?", "@Enet4 Definitely looks stagnated, so feel free to take over.", "Hi. I trained a network with @bcaine register ops solution in python. Now I exported my graph using freeze_graph.py to use my network in C++. But there, this gradient is not registered.\r\nCan anyone help how this can be for C++/ source?", "Hey, is anyone doing anything about the lack of gradient?"]}, {"number": 1792, "title": "FIFOQueue stopping early when graph restored from checkpoint", "body": "I'm running into an issue when evaluating a previously-trained graph on test data: namely, the FIFOQueue returned by `tf.train.string_input_producer` is iterating fewer times over the data than the `num_epochs` parameter I give it.\n\nMore specifically, I have my data stored as `.tfrecords` files, separated into training/validation/test sets.  To both train and evaluate the model, I do the following:\n1. Get the input data from a call to `tf.train.string_input_producer(trainingdata, num_epochs=N, shuffle=True)`\n2. instantiate a `tf.train.Coordinator()`\n3. Call `tf.train.start_queue_runners`\n4. Run the following loop:\n\n``` python\ntry:\n    while not coord.should_stop():\n        # run the appropriate operations\nexcept tf.errors.OutOfRangeError:\n    print('Halting -- epoch limit reached.')\nfinally:\n    coord.request_stop()\ncoord.join(threads)\n```\n\nFor training, I train the model for multiple epochs on the training data, saving a checkpoint every half epoch.\n\nThen, for evaluation, I restore the graph from one of those checkpoints as follows:\n1. Instantiate the input variables from the data pipeline as above\n2. Use those to construct the graph\n3. Call `tf.train.Saver().restore()` to restore the graph from the appropriate checkpoint\n\nThe problem is, an `OutOfRangeError` is getting thrown the very first time I call `sess.run()` on any node.  If I keep increasing the number of epochs for the evaluation, I can eventually get it to actually iterate over the data (e.g. using `num_epochs=4` may give me 1 iteration over the data).  It's consistent with the same checkpoint file (e.g. if `num_epochs=4` gives me 1 iteration, `num_epochs=5` gives me 2, etc.), but it varies between checkpoint files.  It also does not seem to be consistent with the number of training epochs that had been completed when the checkpoint was saved.\n\nMy expectation is that using `num_epochs=1` when initializing the data pipeline for the evaluation should be fine for doing a single iteration over those data points.  What am I missing?\n## Environment info\n\nOperating System: RHEL\nTF commit hash: f82ad360140a2078afcef6af40ad6ec75bd11c1 (version 7.0)\nPython version: 3.5.1\n", "comments": ["I've found my problem; I was not aware that using the `num_epochs` parameter of `tf.train.string_input_producer` adds an epochs variable to the graph (in `tensorflow/python/training/input.py`, for the curious).  Since I'm not currently restricting which variables are saved/restored in my model, that tracker is getting restored from the checkpoint, so the model thinks it's read the data more than it has.\n\nI've implemented two solutions: a hack to reset that variable's value to 0 for models that are already trained, and limiting my save/restore variables to only the graph vars I care about for future models.\n"]}, {"number": 1791, "title": "`GLIBCXX_3.4.14' not found (required by tensorflow/_bin/build-runfiles)", "body": "I get that a version of GLIBC is not found when I try to build. I am running Linux Redhat Enterprise 6, with `java/1.8.0_31,  gcc/5.2.0,  python/2.7.5, cuda/7.5`.\n\nAny help will be greatly appreciated. Thank you -- \n\n```\n[davido@dev1 tensorflow]$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nWarning: ignoring _JAVA_OPTIONS in environment.\nWARNING: Output base '/home/davido/.cache/bazel/_bazel_davido/cc29e96f6fa78af6982c629de060eac7' is on NFS. This may lead to surprising failures and undetermined behavior.\n...........\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nERROR: /home/davido/tensorflow/tensorflow/cc/BUILD:28:1: Creating runfiles tree bazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc.runfiles [for host] failed: build-runfiles failed: error executing command /home/davido/.cache/bazel/_bazel_davido/cc29e96f6fa78af6982c629de060eac7/tensorflow/_bin/build-runfiles bazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc.runfiles_manifest ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/home/davido/.cache/bazel/_bazel_davido/cc29e96f6fa78af6982c629de060eac7/tensorflow/_bin/build-runfiles: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.14' not found (required by /home/davido/.cache/bazel/_bazel_davido/cc29e96f6fa78af6982c629de060eac7/tensorflow/_bin/build-runfiles)\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 7.223s, Critical Path: 0.05s\n```\n", "comments": ["I have the same problem on our internal Linux machine with CentOS 6. \n", "I had this same issue, but compiling Bazel and TensorFlow manually with GCC 4.9 managed to fix this for me.\n", "@martinwicke: Is there a way we can fix this kind of thing on our end, or is it just a bazel issue? \n", "@damienmg is there a solution to this that does not involve compiling bazel from source? Otherwise, that sounds like the right thing to do.\n", "This is just a Bazel issue, this bug should be reopened on our issue tracker IMO.\n\nRecompiling Bazel from source should be the correct solution.\n", "Thanks @damienmg!  I'll close the issue on this end. \n"]}, {"number": 1790, "title": "ImportError: No module named 'tensorflow.tensorboard.tensorboard'", "body": "When I try:\n\n```\ntensorboard --logdir=/path/to/eventlogs\n```\n\nI get\n\n```\nImportError: No module named 'tensorflow.tensorboard.tensorboard'\n```\n## Packages\n\nI use Ubuntu 14.04, Python 3.4.3 and I installed tensorflow using:\n\n```\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp34-none-linux_x86_64.whl\n```\n", "comments": ["The reason for the error is that `tensorboard.py` resides inside `tensorboard\\backend\\` instead of `tensorboard\\` as expected by it\n", "If I run:\n\n``` shell\n~/tensorflow$  python tensorboard/backend/tensorboard.py --logdir=/path/to/eventlogs\n```\n\nit works.\n", "Check your bin path to see if there's a stale / old tensorboard.py ?\n", "Thanks. I did check and found I posted some wrong information.\n\n`tensorboard` in `/bin` of `Python 3.4 virtualenv` works, its the `tensorboard` installed along with Anaconda (Python 3.5.1) that doesn't. \n\nwhat I did:\n\n```\npip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp34-none-linux_x86_64.whl\n```\n\nProbably this `.whl` installation is not meant for Anaconda.\n", "Yes, we don't release anaconda wheels (yet) :(.  You might want to try building from sources.\n", "oh, cool. No problem, thanks. I am happy it works with Py3.4 now. :) :+1: \n", "One workaround I did was to navigate to the directory of tensorboard.py and edit the import statement.\r\n\r\nif __name__ == '__main__':\r\n    import sys\r\n    #import tensorflow.tensorboard.tensorboard\r\n    import tensorboard.main\r\n    #sys.exit(tensorflow.tensorboard.tensorboard.main())\r\n    sys.exit(tensorboard.main.main())\r\n\r\nNot sure if this is good practice though", "My PyCharm is not getting tensorboard, when i use: tensorboard --logdir=\"logs\\\" . It gives: \r\nerror: 'tensorboard' is not recognized as an internal or external command, operable program or batch file.\r\n^-- the env set in Pycharm setting has tensorboard packages located at: envs\\tensorflow\\Lib\\site-packages\\tensorflow\\contrib\\tensorboard\r\nand\r\nenvs\\tensorflow\\Lib\\site-packages\\tensorboard\r\n\r\nbut, as stated in the comments above, I cannot find tensorboard.py, even in the backend package\r\nthere is: import_pb_to_tensorboard.py; but i don't think that's what is needed\r\n\r\n----------------------------------------------------------------------------------------------------------------\r\n\r\nIf I try to: python -m tensorboard --logdir=\"logs\\\"; then it says:\r\nC:\\Users\\<name>\\AppData\\Local\\Programs\\Python\\Python36-32\\python.exe: No module named tensorboard\r\n^-- this location does not have tensorflow / tensorboard\r\n\r\n-----------------------------------------------------------------------------------------------------------------\r\nAny suggestions to resolve this would be highly appreciated\r\nThanks!\r\n\r\nP.S.: Using it on Windows 10, through anaconda\r\n(I've tried reinstalling anaconda and tensorflow from the official sites again.. didn't work!)"]}, {"number": 1789, "title": "Add reference for ReLu in tutorial", "body": "Add a wiki reference for ReLu in tutorial TF mechanics 101\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 1788, "title": "Add missing parenthesis in description", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 1787, "title": "CUDNN_STATUS_BAD_PARAM with included cifar10 model", "body": "### Environment info\n\nOperating System:\nDebian 3.16.7-ckt20-1+deb8u3 (2016-01-17) x86_64 GNU/Linux\nPython 2.7\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n   0.7.1\n### Steps to reproduce\n1. Go to /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10\n2. Run python cifar10_train.py as root\n3. Observe output \n   F tensorflow/stream_executor/cuda/cuda_dnn.cc:383] could not set cudnn filter descriptor: CUDNN_STATUS_BAD_PARAM\n   Aborted\n### What have you tried?\n1. Messing with LD_LIBRARY_PATH and making sure all cuda packages are updated to 7.5 (and CudNN 5.0). Training the simple softmax models on GPU is possible.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nroot@leviathan:/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10# python cifar10_train.py \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\n... Loads other CUDA libraries as above ...\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:01:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB\n... and so on up to 16.00 GB:\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.60GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x5047a0000 extends to 0x7aaa4019a\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:383] could not set cudnn filter descriptor: CUDNN_STATUS_BAD_PARAM\nAborted\n", "comments": ["If you are installing the pip package for 0.7.1, check that you are using cudnn r4 (make sure the libraries being loaded are the correct version). \n", "Ah, I am using cudnn r5. Is it not backwards compatible? Another error was preventing me from using r4. Thank you, and I will open another bug if r4 continues to give me trouble!\n", "No, it's not backwards compatible from the pip installation, the APIs are often different.\n\nhttps://github.com/tensorflow/tensorflow/pull/1794 is a change to allow compiling from source to support cudnn r5.  Since it's RC, we're unlikely to package it in our official PIPs until it is officially released (as they work out bugs), but that commit should add basic support for it.\n", "Alright thanks!\n", "Hello can you tell me what is the right cuda and cudnn version to use? Should I be using cuda 7.0 and cudnn v3 then?\n", "for 0.8.0 pips, cuda 7.5 and cudnn v4.\n\nIf you build from sources, you can use anything from cuda 6.5-cuda 7.5 and cudnn v2 - cudnn v5\n", "If you have the error `CUDNN_STATUS_BAD_PARAM`, make sure that the problem does not come from your code.\r\nFor example if you give an empty array as an input. It happened to me and this was the reason.\r\nI'm running v5 with TF built from the sources.", "Just to weigh in on @philipperemy  's answer, CUDNN_STATUS_BAD_PARAM can indeed be sort of unrelated to cuda, and hide a \"simple\" code error. In my case, I was trying to convolute an input with a kernel bigger than the actual input. Code to reproduce : \r\n\r\n\r\n```\r\nclass _network(nn.Module):\r\n    def __init__(self):\r\n        super(_network, self).__init__()\r\n\r\n        self.test_module = nn.Sequential(\r\n            nn.Conv2d(1, int(64 / 2), 4, 2, 1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n        )\r\n\r\n    def forward(self, input):\r\n        output=self.test_module(input)\r\n        return output\r\n\r\ninput=Variable(torch.zeros(64,1,1,1).cuda())\r\n\r\nnetwork=_network()\r\nnetwork=network.cuda()\r\noutput=network(input)\r\n```\r\n\r\nThis gives the CUDNN_STATUS_BAD_PARAM error with Pytorch 0.3.1 and Python 3.6. Removing the \"cuda\" from the network and the input yields a much more informative error on what is actually happening : \r\n\r\n> RuntimeError: Calculated input size: (3 x 3). Kernel size: (4 x 4). Kernel size can't greater than actual input size at /pytorch/torch/lib/THNN/generic/SpatialConvolutionMM.c:46\r\n", "@Soltius63 thanks for the explanation :)"]}, {"number": 1786, "title": "feature request: support for cudnn v5 rc", "body": "Anyone can help this?\nhttps://developer.nvidia.com/rdp/cudnn-download\n", "comments": ["FYI, https://github.com/Theano/Theano/issues/4342#\n", "I looked into this just now briefly -- there are a lot of API changes that are somewhat annoying to #ifdef around but I made progress. \n\nedit: (Looking into how to plumb some more APIs, not sure how to expose all of the various algorithms for convolution like in the Theano example).\n", "https://github.com/vrv/tensorflow/commit/a76449737fed046b7e9f9871dd2d1d5c2f51ead0\n\nBuilt for me with R5, I was able to run a few benchmarks, though not a huge difference, presumably because I'm not choosing the 'optimal algorithms'.\n", "They claim to be getting rather [substantial speed improvements](https://pbs.twimg.com/media/CfTGrS9XIAENPCv.jpg) on RNNs and LSTMs.\n", "#1794 added basic support for v5 RC, so you can at least install the library on your system and it will work.  We haven't performance optimized it or added the RNN/LSTM API yet.\n", "I'm not sure why but I am unable to do a Bazel PIP build past cuDNN.\n\nI'm using the instructions and forked TF from here.\nhttps://gist.github.com/ageitgey/819a51afa4613649bd18\n\nTF_UNOFFICIAL_SETTING=1 ./configure\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n\n[TF_BazelPIPBuild_TerminalOutput.txt](https://github.com/tensorflow/tensorflow/files/211039/TF_BazelPIPBuild_TerminalOutput.txt)\n", "I'm not sure stream executor works for OS X yet (hence #664).  I did test cudnn v5 on my local machine and it seemed to work, so I'm not entirely sure why it's still failing for you -- might double check that the symlinks and cudnn header file and ./configure options have been set properly.  \n", "will TF integrate the cuDNN LSTM API? Did anyone run benchmarks on how much faster it is? The nasty thing I can see from the cuDNN API is in the backward pass, we need to pass a temp scratch pointer from the forward pass -- not too sure how to impl that.\n", "I think there is a small bug here: the 'format' argument to cudnnSetFilter4dDescriptor should be based on the filter layout not the data layout.\n\nAlso, CUDNN_ACTIVATION_CLIPPED_RELU can be used for Relu6 and ReluX.\n", "Thanks @benbarsdell: want to send a PR to fix either one?  Also when I wrote that up, it wasn't clear to me how a 'filter' could have a format of NCHW, which is why I used the data layout.  Filters don't talk about batch, just input and output channel / height / width, so I wasn't really sure what to do there.\n", "@benbarsdell okay I think with the commit above and dfcad66c5ecc6e49aff0fa379832982809b2b447 we addressed your issues -- let me know if there's still something left there.\n\nWill leave this open until we figure out what we're doing about the LSTM/RNN API.\n", "We're sort of working on this now.\n", "Was about to ask about the RNN and LSTM support in cuDNN v5, and found this thread. While we're on the subject, what about support for tensor descriptors? Also new in v5.\n", "@vrv: Assigning to you for now. \n", "@vrv So, what did you all decide you're doing about CuDNN v5's RNN/LSTM API?\n", "We're still trying to integrate it -- it's pretty hard :(\n", "Is cuDNN v5.1 compatible with theano 0.8.2? (minus LSTM, RNN optimizations)  I had cuDNN 5005 and cuda 7.5  working with an older kepler card. I THINK i need cuda 8.0 to work with my gxt 1060. Should I update to win10? I am on 8.1 atm. Things are not quite right yet (ie.. processes seems to stall or are just slower than I am patient.\n", "@vrv Friendly ping, is this still being worked on?\n", "The LSTM components are, yes.\n", "So I have cuda 8.0 RC (cudnn 5.0 (5005)) working with my gtx 1060 , keras 1.0.6 and theano 0.9.0 dev2.\nI am still curious to know the state of support for cuDNN 5.1, if it works with theano yet? 0.8.2 or 0.9.0 dev2. If not, soon?\n", "Cudnn RNN ops were added here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cudnn_rnn\n\nClosing this bug since that's probably the major thing desired from cudnn v5.\n\nAnd I'm sure v6 is around the corner... \n"]}, {"number": 1785, "title": "Switch to pass graph instead of GraphDef to summaryWriter", "body": "Simple change since `GraphDef` is deprecated\ncc: @ilblackdragon \n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "All tests passed. The sanity failure can be ignored for now. Admins, feel free to merge as you see appropriate.\n"]}, {"number": 1784, "title": "Add support for stochastic depth networks", "body": "This issue is a feature request to add support to tensorflow to implement networks with stochastic depth (http://arxiv.org/abs/1603.09382).\n", "comments": ["tf.cond should be enough to make this work\n", "Agreed with @alexatknit, seems like we have the primitives to implement this already -- though if you have a more concrete feature request, we'll take a look.\n"]}, {"number": 1783, "title": "feature request: complex64 support in diag op", "body": "[`tf.diag`](https://www.tensorflow.org/versions/r0.7/api_docs/python/math_ops.html#diag) only supports `float32`, `float64`, `int32`, `int64` right now; is it possible to also include `complex64`? \n\nI may get around to extending the op to do this for my own purposes, but perhaps someone else is better positioned to add this functionality. Or is there some deeper reason it's not supported?\n", "comments": []}, {"number": 1782, "title": "Why doesn't TensorBoard order objects according to information flow?", "body": "I would infer that TensorBoard orders objects according to how information flows through the graph, but this doesn't appear to be the case. Namely, considering Inception-v3. The operation makes it clear that, in each conv unit, batch normalization is performed on the output of the convolution layer:\n\n```\nconv = tf.nn.conv2d(inputs, weights, [1, stride, stride, 1],\n                    padding=padding)\nif batch_norm_params is not None:\n  with scopes.arg_scope([batch_norm], is_training=is_training,\n                        trainable=trainable, restore=restore):\n    outputs = batch_norm(conv, **batch_norm_params)\n```\n\ni.e., `outputs` is `batch_norm(conv(inputs, ...), ...)`\n\nHowever, the TensorBoard representation of the graph orders `batch_norm` before the `conv` operation. \n\n![screenshot from 2016-04-05 11 15 36](https://cloud.githubusercontent.com/assets/6423093/14293082/e17a1776-fb1f-11e5-8073-341d087f11cb.png)\n\nGranted, these don't appear to be connected by a line. However, it _does_ order the convolutions correctly:\n\n![screenshot from 2016-04-05 11 17 32](https://cloud.githubusercontent.com/assets/6423093/14293119/031168da-fb20-11e5-9c7a-8f2c6596088f.png)\n\nIs this due to the fact that these nodes are disconnected \"given\" `Identity...` and are thereby ordered alphabetically? Is there a way to disable this, and have it structure the graph according to the flow of information? Presumably it can disambiguate between the various `Identity...` nodes. Or am I interpreting this situation wrong?\n", "comments": ["Hi @eriophora,\n\nTo make sure I understand your question: Are you asking for \"weights\" to be left of \"BatchNorm\" inside the conv0 namespace? Left-to-right ordering is meaningless in the graph visualizer. What matters is the bottom-to-top ordering. We leave left-to-right ordering to the layout optimizer to figure it out, which sometimes can swap nodes to reduce edge crossing and we can't control it (even when no edges are shared).\n", "Closing due to lack of response. Please re-open if you would like this readdressed.\n"]}, {"number": 1781, "title": "[Question] Nightly wheels", "body": "Are you generating the wheels nightly?\nIf so, that would be interesting to have access to them, given as is of course.\n", "comments": ["The nightly binaries are linked from the top-level readme: https://github.com/tensorflow/tensorflow\n", "Oops. I should have re-checked before asking. Thanks\n"]}]