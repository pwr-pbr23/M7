[{"number": 14388, "title": "mutex.h, private member is inaccessible", "body": "### System information\r\n- **msvc v140**:\r\n\r\n### Problem\r\nmutex.h, Ln 145\r\ncondition_variable cannot access private member mu_ in mutex, when compiling a standalone c++ project on Windows. \r\n\r\n### Solution\r\nForward declare condition variable in mutex.h. I have created a pull request.\r\n", "comments": ["Waiting https://github.com/tensorflow/tensorflow/pull/14389\r\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 14387, "title": "tensorflow-gpu looks for the wrong driver version", "body": "### System information\r\n- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.4\r\n- **Python version**: 2.7.12\r\n- **CUDA/cuDNN version**: 8.0 / 6.0\r\n- **GPU model and memory**: GTX 1060 \r\n- **GPU driver version**: 387.12\r\n- **Exact command to reproduce**: \r\n`import tensorflow as tf`\r\n\r\n\r\n### Problem\r\nIt looks for the wrong version of libnvidia-fatbinaryloader.so.xxx.xx. \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/stefano/PycharmProjects/lstm/robot_LSTM.py\", line 2, in <module>\r\n    from keras.models import Sequential\r\n  File \"/home/stefano/PycharmProjects/lstm/env/local/lib/python2.7/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"/home/stefano/PycharmProjects/lstm/env/local/lib/python2.7/site-packages/keras/utils/__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"/home/stefano/PycharmProjects/lstm/env/local/lib/python2.7/site-packages/keras/utils/conv_utils.py\", line 3, in <module>\r\n    from .. import backend as K\r\n  File \"/home/stefano/PycharmProjects/lstm/env/local/lib/python2.7/site-packages/keras/backend/__init__.py\", line 83, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/home/stefano/PycharmProjects/lstm/env/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/stefano/PycharmProjects/lstm/env/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/stefano/PycharmProjects/lstm/env/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/stefano/PycharmProjects/lstm/env/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/stefano/PycharmProjects/lstm/env/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/stefano/PycharmProjects/lstm/env/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/stefano/PycharmProjects/lstm/env/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libnvidia-fatbinaryloader.so.384.90: cannot open shared object file: No such file or directory\r\n```\r\n", "comments": ["Can you include some more details and an exact sequence of commands / code to reproduce, as per [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new)? Thanks!", "> Can you include some more details and an exact sequence of commands / code to reproduce, as per the Github new issue template? Thanks!\r\n\r\nI edited my post. I just cannot import Tensorflow with GPU support. Is there any other useful information that I could add?", "This looks like a duplicate of [this stack overflow question](https://stackoverflow.com/questions/42678439/importerror-libnvidia-fatbinaryloader-so-375-39-cannot-open-shared-object-file), and ultimately looks like an environment configuration issue (i.e. not a bug).\r\n\r\nTry some of the recommendations on the SO question, and if you need more help, ask another SO question to get better help. Stack Overflow is better suited for problems that are not bugs or feature requests. Thanks!"]}, {"number": 14386, "title": "Fix typo <Copybara Experiment DO NOT MERGE>", "body": "Fix typo in tensorflow/python/client/timeline.py", "comments": []}, {"number": 14385, "title": "golang: added Session.ListDevices method", "body": "Implemented Session.ListDevices() method", "comments": ["Can one of the admins verify this patch?", "My use case was I was trying to balance session creation manually across multiple GPUs so I needed to fetch a number of available GPUs first together with available memory. So I decided to create an empty session at the start and then fetch all info via ListDevices().", "Jenkins, test this please", "Jenkins, test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "windows rerun at http://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/5945/"]}, {"number": 14384, "title": "Cannot make an input layer that takes scalars with the keras functional api", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source (branch 1.4)\r\n- **TensorFlow version (use command below)**: 1.4.0-dev\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: nVidia 1080Ti 11G\r\n- **Exact command to reproduce**: run the script below\r\n\r\n### Describe the problem\r\n\r\nThe following is an attempt to use the keras functional api to make a model that accepts scalars as input:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.keras.api.keras.models import Model\r\nfrom tensorflow.contrib.keras.api.keras.layers import Input\r\ni = Input(batch_shape=(None, ), dtype=tf.uint8, name=\"input\")\r\nm = Model(inputs=i, outputs=i)\r\n```\r\nwhich fails like so:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-54-dd00bec66757> in <module>()\r\n----> 1 i = Input(batch_shape=(None, ), dtype=tf.uint8, name=\"input\")\r\n      2 m = Model(inputs=i, outputs=i)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in Input(shape, batch_size, name, dtype, sparse, tensor, **kwargs)\r\n    611     dtype = K.floatx()\r\n    612   if not shape and tensor is None:\r\n--> 613     raise ValueError('Please provide to Input either a `shape`'\r\n    614                      ' or a `tensor` argument. Note that '\r\n    615                      '`shape` does not include the batch '\r\n\r\nValueError: Please provide to Input either a `shape` or a `tensor` argument. Note that `shape` does not include the batch dimension.\r\n```\r\nUsing a sequential model instead works fine:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.keras.api.keras.models import Sequential\r\nfrom tensorflow.contrib.keras.api.keras.layers import InputLayer\r\nm = Sequential()\r\nm.add(InputLayer(batch_input_shape=(None, ), dtype=tf.uint8, name=\"input\"))\r\n```\r\nAdditionally using the external keras works fine:\r\n```\r\nimport tensorflow as tf\r\nfrom keras.models import Model\r\nfrom keras.layers import Input\r\ni = Input(batch_shape=(None, ), dtype=tf.uint8, name=\"input\")\r\nm = Model(inputs=i, outputs=i)\r\n```\r\nIt appears that this broke when the tensor and batch_size arguments were added to the internal version.\r\nHere is one possible way to fix it which I can propose if people are ok with it:\r\n```\r\ndiff --git a/tensorflow/python/keras/_impl/keras/engine/topology.py b/tensorflow/python/keras/_impl/keras/engine/topology.py\r\nindex f9be782..74df725 100644\r\n--- a/tensorflow/python/keras/_impl/keras/engine/topology.py\r\n+++ b/tensorflow/python/keras/_impl/keras/engine/topology.py\r\n@@ -605,21 +605,20 @@ def Input(  # pylint: disable=invalid-name\r\n       raise ValueError('Only provide the shape OR '\r\n                        'batch_shape argument to '\r\n                        'Input, not both at the same time.')\r\n-    batch_size = batch_shape[0]\r\n-    shape = batch_shape[1:]\r\n+  else:\r\n+    if not shape and tensor is None:\r\n+      raise ValueError('Please provide to Input either a `shape`'\r\n+                       ' or a `batch_shape` or a `tensor` argument.'\r\n+                       ' Note that `shape` does not include the batch '\r\n+                       'dimension.')\r\n+    batch_shape = (batch_size,) + tuple(shape)\r\n   if kwargs:\r\n     raise ValueError('Unrecognized keyword arguments:', kwargs.keys())\r\n \r\n   if dtype is None:\r\n     dtype = K.floatx()\r\n-  if not shape and tensor is None:\r\n-    raise ValueError('Please provide to Input either a `shape`'\r\n-                     ' or a `tensor` argument. Note that '\r\n-                     '`shape` does not include the batch '\r\n-                     'dimension.')\r\n   input_layer = InputLayer(\r\n-      input_shape=shape,\r\n-      batch_size=batch_size,\r\n+      batch_input_shape=batch_shape,\r\n       name=name,\r\n       dtype=dtype,\r\n       sparse=sparse,\r\n```\r\n\r\n", "comments": ["FYI, it is not possible to model.predict() at the moment with a scalar input_shape, even though it is totally valid to put scalars into a feed_dict using session.run(). In fact, some tensorflow operations like tf.image.decode_jpeg require scalar inputs. The missing scalar support is a separate keras issue that I have reported here: https://github.com/fchollet/keras/issues/8436", "@fchollet FYI, feel free to assign this issue to others.\r\n", "You are supposed to specify `shape` and `batch_size` as two separate arguments. We are deprecating the `batch_shape` argument from the public API.", "@fchollet I understand the desire to deprecate batch_shape but that means there is no way to define an input that takes a scalar. Are you suggesting that accepting scalar input will not be supported at all? Or are you saying that the implementation needs to be different, perhaps with a new kwarg or a special sentinel value for shape? I personally think packing scalars into 1d vectors is pretty ugly, but if the decision is keras will not support scalars, I will abide by it.", "@fchollet  so what is the solution here? Is there a way to define a scalar keras variable, or are we supposed to have it as vector and slice it?\r\nI think with the Tensorflow 2 and even heavier usage of keras API, this thing will be issue for even more people.", "In case anyone else needs help with this - My solution was to use `shape=(1,)`. Equivalently, I think, you could do `batch_shape=(batch_size,1)`"]}, {"number": 14383, "title": "Revert \"[OpenCL SYCL] Add support for triSYCL in TensorFlow\"", "body": "Reverts tensorflow/tensorflow#12882\r\n\r\n```\r\nAnalyzing: 394 targets (250 packages loaded)\r\nERROR: /tmpfs/tmp/bazel/external/local_config_sycl/sycl/BUILD:14:14: Traceback (most recent call last):\r\n\tFile \"/tmpfs/tmp/bazel/external/local_config_sycl/sycl/BUILD\", line 12\r\n\t\tconfig_setting(name = \"using_sycl_ccpp\", values =...\"})\r\n\tFile \"/tmpfs/tmp/bazel/external/local_config_sycl/sycl/BUILD\", line 14, in config_setting\r\n\t\t{\"define\": \"using_sycl=true\", \"define\": \"using_trisycl=false\"}\r\nDuplicated key \"define\" when creating dictionary\r\nERROR: /tmpfs/tmp/bazel/external/local_config_sycl/sycl/BUILD:22:14: Traceback (most recent call last):\r\n\tFile \"/tmpfs/tmp/bazel/external/local_config_sycl/sycl/BUILD\", line 20\r\n\t\tconfig_setting(name = \"using_sycl_trisycl\", value...\"})\r\n\tFile \"/tmpfs/tmp/bazel/external/local_config_sycl/sycl/BUILD\", line 22, in config_setting\r\n\t\t{\"define\": \"using_sycl=true\", \"define\": \"using_trisycl=true\"}\r\nDuplicated key \"define\" when creating dictionary\r\n```", "comments": ["Fixes #14375", "CLosing in favor of #14379"]}, {"number": 14382, "title": "Looks like an rc1 string got into master. Here is the fix.", "body": "", "comments": []}, {"number": 14381, "title": "Can't access predictions ", "body": "\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: Tensorflow 1.4.0\r\n- Python version: Python 3.6\r\n- CUDA/cuDNN version: CUDA 8 / cuDNN 6\r\n- GPU model and memory: GTX M950\r\n\r\n\r\n\r\n### Describe the problem\r\nWhen trying to print the predictions from DNNClassifier class i only get \"<generator object Estimator.predict at 0x000001AFE1E24EB8>\". I used the exact code written in the Tensorflow tutorials.\r\n\r\n### Source code / logs\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": np.array(X_train)}, y=y_train,num_epochs=None,shuffle=False)\r\ntest_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": np.array(X_test)}, y=None,shuffle=False)\r\nclassifier.train(input_fn=train_input_fn, steps=100)\r\npreds = classifier.predict(input_fn=test_input_fn)\r\nprint(preds)\r\n", "comments": ["`preds` is indeed a generator object that cannot be printed on its own. [The Estimator Tutorial](https://www.tensorflow.org/get_started/estimator) demonstrates this:\r\n\r\n```python\r\npredictions = list(classifier.predict(input_fn=predict_input_fn))\r\npredicted_classes = [p[\"classes\"] for p in predictions]\r\n\r\nprint(\r\n    \"New Samples, Class Predictions:    {}\\n\"\r\n    .format(predicted_classes))\r\n```\r\n\r\nIf you have more issues like this, please post a question on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow), which is more suited to individual issues that are not bugs or feature requests. Thanks!", "Can we pass batched features from dataset iteration into GBT estimator for prediction instead input_fn?"]}, {"number": 14380, "title": "Bazel Build Fails with \"undeclared inclusion(s) in rule '@nccl_archive//:nccl'\"", "body": "### System information\r\nThe environment used is:\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1455194/tf_env.txt)\r\n\r\nThe branch downloaded is tensorflow-master branch\r\n\r\nWhen I run the configure script the configuration is\r\n[.tf_configure.bazelrc](https://github.com/tensorflow/tensorflow/files/1455688/tf_configure.bazelrc.txt)\r\n \r\n\r\n### Describe the problem\r\nWhen I run\r\n```\r\nbazel build --config=opt --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nThe build fails with the error\r\n``` \"/home/kevin/.cache/bazel/_bazel_kevin/1bfc1cc47be0eab35fef533f37af7359/external/nccl_archive/BUILD:33:1: undeclared inclusion(s) in rule '@nccl_archive//:nccl':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/core.cu.cc':\r\n  '/usr/lib/gcc/x86_64-linux-gnu/4.9/include-fixed/limits.h'\r\n  '/usr/lib/gcc/x86_64-linux-gnu/4.9/include-fixed/syslimits.h'\r\n  '/usr/lib/gcc/x86_64-linux-gnu/4.9/include/stddef.h'\r\n  '/usr/lib/gcc/x86_64-linux-gnu/4.9/include/stdarg.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nFAILED: Build did NOT complete successfully\r\n```\r\nbut the files are present:\r\n```\r\nls /usr/lib/gcc/x86_64-linux-gnu/4.9/include-fixed/limits.h\r\n/usr/lib/gcc/x86_64-linux-gnu/4.9/include-fixed/limits.h\r\n```\r\nThere appears to be a problem with bazel build with MKL enabled.  I have been able to compile this with MKL disabled. MKL for Ubuntu should be supported, yes?", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "@bignamehyp  All the info was provided but I guess I was too concise.  I've added verbiage to the OP.", "I got bazel to successfully complete but without MKL support by running\r\n```\r\nbazel clean --expunge\r\n```\r\nthen hand-editing the configure file to\r\n[.tf_configure.bazelrc](https://github.com/tensorflow/tensorflow/files/1455717/tf_configure.bazelrc.txt)\r\n then running\r\n```\r\nbazel build --config=opt --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nSo I guess the problem is with the configuration and/or compilation of MKL.  \r\n\r\nMy reading of the documentation indicates that the necessary files for MKL for Ubuntu should be automatically downloaded.  Is this not the case?", "MKL was added as of TensorFlow 1.2 and currently only works on Linux. It also does not work when also using --config=cuda.\r\n\r\n", "@zheng-xq @tfboyd ", "It does not work with the --config=cuda.  Assigning to Gunhan there is work in this area with no ETA.  Also if you are using GPU I would go with AVX2 or AVX it should get most of the benefits you need for work mostly done on the GPU.  AVX2 does help for GPU work in my testing.  The input pipeline (image processing) benefits from it but I did not see a big difference until 8x K80s and did not have time to test o P100s or other situations.  Nor do I know the exact calls that benefit.  Not super informational but a little.  :-)", "In the meantime how about a better error message, either in configure.py or in bazel?", "I know this may seem confrontational but it is not.  You could add a message to configure.py.  It would not stop people from adding --config=cuda but you could give a warning based on the options they pick.  On the flip side, I am not sure how many people run into this problem.  If they see MKL on the performance page it says --config=cuda does not work.  If they see it in the config.py then a message would help them and you could be a hero.  (not sarcasm, the internet is a hard place to read intent)  \r\n\r\nI would not expect people to see everything on the website as that is crazy.  I did [document the --config=cuda does not work](https://www.tensorflow.org/performance/performance_guide#tensorflow_with_intel_mkl_dnn).  And if you are interested in some gains with various config options on CPU.  ", "@tfboyd no offense taken.  We both want the same thing, to make TensorFlow better.  Thanks for the info.\r\n\r\n-- (johnsrude from a different GitHub account)", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I got the same problem, and found a unsuccessful (to me) solution at #10665 \r\n\r\nHope this helps @johnsrude ", "Could you try with bazel 0.8 and report what you see?\r\nI think on ubuntu 16, we may have a problem with older bazel versions.", "@gunan  I am using redhat, unfortunately and still failed with bazel 0.8.1.  I will discuss the problem in #10665 since there is another person on that issue.  Thanks!", "I have the same problem, can anyone help to check \" undeclared inclusion(s) in rule '@nccl_archive//:nccl' \" issue?\r\nit will be really appreciated. :)\r\n\r\n**System information:**\r\nCentOS 7.3\r\ngcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nBuild label: 0.8.1- (@non-git)\r\ncuda 9.0.176\r\ncudnn 7.0.5\r\ntensorflow 1.4.0\r\n\r\n**Build command:**\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Error message:**\r\nINFO: Found 1 target...\r\nERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/nccl_archive/BUILD:33:1: undeclared inclusion(s) in rule '@nccl_archive//:nccl':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/libwrap.cu.cc':\r\n'/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'\r\n'/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'\r\n'/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\r\n'/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 6.973s, Critical Path: 4.56s\r\nFAILED: Build did NOT complete successfully", "I think this is a problem with the default toolchain in bazel.\r\nWill wait for a response from bazel team.", "@gunan Thanks for commenting. Is there any approach to install Tensorflow 1.4 on a system with Cuda9.0/Cudnn7.0/NCCL 2.0 ?", "@gunan Thanks!  Could you point out the problem that I can follow up with?  I am not familiar with bazel but I would like to understand better about it.", "Github auto-referenced the issue above. I filed the issue in bazel repository here: https://github.com/bazelbuild/bazel/issues/4365", "Can someone with a redhat or centos system try running this command and share its output?\r\n```\r\ngcc -E  -xc++ - -v\r\n```", "@gunan \r\nI run with CentOS.\r\n[root@localhost ~]# gcc -E -xc++ - -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nTarget: x86_64-redhat-linux\r\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux\r\nThread model: posix\r\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-11) (GCC) \r\nCOLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'\r\n /usr/libexec/gcc/x86_64-redhat-linux/4.8.5/cc1plus -E -quiet -v -D_GNU_SOURCE - -mtune=generic -march=x86-64\r\nignoring nonexistent directory \"/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include-fixed\"\r\nignoring nonexistent directory \"/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../x86_64-redhat-linux/include\"\r\n#include \"...\" search starts here:\r\n#include <...> search starts here:\r\n /usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../include/c++/4.8.5\r\n /usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../include/c++/4.8.5/x86_64-redhat-linux\r\n /usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../include/c++/4.8.5/backward\r\n /usr/lib/gcc/x86_64-redhat-linux/4.8.5/include\r\n /usr/local/include\r\n /usr/include\r\nEnd of search list.\r\n", "Thank you very much!\r\nthe output does show the header folders you are missing. I am curious why you see the failure you reported. I will continue pushing from the bazel side.", "On CentOS I was able to successfully build cuda and non-cuda TF,  at head using bazel.\r\nI was not able to reproduce the problem later reported in this issue.\r\n\r\nI will now test building with MKL.", "using docker, I was able to successfully build pip package with the following parameters:\r\nbase image: `nvidia/cuda:9.0-cudnn7-devel-centos7`\r\nTF revision: `master`\r\nbazel version: `0.8.1`\r\ngcc/g++ version: `4.8.5`\r\n\r\nCommands tried:\r\n```\r\nbazel build --config=cuda --config opt --config=mkl tensorflow/tools/pip_package:build_pip_package\r\nbazel build --config=cuda --config opt tensorflow/tools/pip_package:build_pip_package\r\nbazel build --config=cuda -c opt tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nMaybe there was a problem before, but it seems to be resolved at head. I will close this issue now, but if you still see this problem at the master branch, please file a new issue with detailed instructions to reproduce the problem, preferably using docker.", "If the issue pops up again, looking at the generated crosstool, e.g. by\r\n```\r\ncat $(bazel info output_base)/external/local_config_cuda/crosstool/CROSSTOOL\r\n```\r\nShould shed some light on the problem.\r\n\r\nIt usually boils down to crosstool referencing files via symlink, while compilers resolve the symlinks when outputting the .d files and vice versa.", "@gunan Thanks for the extensive checks.  But I still got the problem no matter with master or r1.4 branches.  Also I shared the same output as @beanliao that the missing paths are in the search path.\r\n\r\nI took a look as suggested by @ilya-biryukov - I found the following lines look suspicious:\r\n\r\n  cxx_builtin_include_directory: \"/include/c++/4.8.5\"\r\n  cxx_builtin_include_directory: \"/include/c++/4.8.5/x86_64-redhat-linux\"\r\n  cxx_builtin_include_directory: \"/include/c++/4.8.5/backward\"\r\n  cxx_builtin_include_directory: \"/lib/gcc/x86_64-redhat-linux/4.8.5/include\"\r\n\r\nThese paths missed the `/usr/` part.  But adding it won't solve the problem.", "Also I successfully built the CPU version. And then I got the error below with \"--config=cuda\":\r\n\r\nERROR: /path_to_tensorflow/tensorflow/contrib/resampler/BUILD:51:1: undeclared inclusion(s) in rule '//tensorflow/contrib/resampler:python/ops/_resampler_ops_gpu':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/contrib/resampler/kernels/resampler_ops_gpu.cu.cc':\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdint.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/x86intrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/ia32intrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/mmintrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/xmmintrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/mm_malloc.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/emmintrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/immintrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/fxsrintrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/adxintrin.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/float.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdbool.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 12.777s, Critical Path: 12.35s\r\nFAILED: Build did NOT complete successfully\r\n\r\nI believe that this is the same error as nccl one and it related more to the configuration of the CROSSTOOL.", "@jiayiliu Could you file a new issue with all the information asked by the issue template for me to be able to reproduce the problem? Please do not skip anything in the template, as I will need all the information to have a chance to reproduce the issue. As I mentioned in https://github.com/tensorflow/tensorflow/issues/14380#issuecomment-355214575 it may be your bazel version, your OS version, GCC version, or a combination but unless I can reproduce your problem, there is nothing I can do for you.\r\n\r\nAlso, please add the output of `gcc -E -xc++ - -v` to the issue you file. Thanks.", "@jiayiliu, you could try applying the following patch locally via `git apply <patch-name>`:\r\n[0001-Don-t-use-no-canonical-prefixes-when-getting-include.txt](https://github.com/tensorflow/tensorflow/files/1607470/0001-Don-t-use-no-canonical-prefixes-when-getting-include.txt). There's a good chance your issue will be fixed by it (see the explanation below).\r\nAlso, have you considered using a recently added downloadable clang toolchain for cuda_clang configuration? Just configure with `TF_NEED_CUDA=1 TF_CUDA_CLANG=1 TF_DOWNLOAD_CLANG=1` (or choose the appropriate options when running `configure.py`).\r\n\r\n@gunan, @jiayiliu I bet the difference is that bazel's [autoconf](https://github.com/bazelbuild/bazel/blob/39a23a0e3d63c538b8aa9f6f94a3a3916998e973/tools/cpp/unix_cc_configure.bzl#L151) (in CPU build) does not pass `-no-canonical-prefixes`, while tensorflow's [GPU crosstool](https://github.com/tensorflow/tensorflow/blob/a1b155b6f51c539974e1bf73d0e0d15b388b9219/third_party/gpus/cuda_configure.bzl#L116) passes `-no-canonical-prefixes`.\r\n\r\nI've checked it locally and it does not seem to break anything for me in gpu builds, so I guess we should just follow what bazel does and don't set `-no-canonical-prefixes` when getting the list of includes in `cuda_configure.bzl`. I'll send a patch for review with that change.\r\n\r\n  ", "my system version: centos7+gcc4.8.5+bazel 0.8.1\r\nI find its tricky to fix this , just change the default gcc path when running configure from /bin/gcc to /usr/bin/gcc", "@mhlopko @lberki is it possible the default toolchains do not work if the compiler is under `/bin/gcc`.", "@gunan I check that in configure.py it use which('gcc') to find gcc, when i run this it returns /bin/gcc, but when i run it manualy use shutil.which('gcc') it returns /usr/bin/gcc", "Met the similar problem too.  I removed the path of bazel cache files in \"/root/.cache/bazel\"(change root to your user path) and rebuild, build completed successfully.", "@argman  \r\nThanks.  It does work by \" just change the default gcc path when running configure from /bin/gcc to /usr/bin/gcc\"", "Hi all,\r\nI am facing the same issues on undeclared inclusions on REHL Scientific Linux 6 ONLY when compiling with cuda (CPU mode works), according to the discussion above and other threads such as [#4365](https://github.com/bazelbuild/bazel/issues/4365), I did the following additional changes to my previous issue on an older Tensorflow version [#8529](https://github.com/tensorflow/tensorflow/issues/8529). It finally worked and actually the main solution comes from removing the \"-no-canonical-prefixes\" option. Proposals of adding --cxxopts such as --cxxopt=\"-I/opt/rh/devtoolset-4/root/usr/include/c++/5.3.1/include/\" --cxxopt=\"-I/opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.3.1/include\" are in the end not required.\r\n\r\nHere is an overview of my config:\r\nREHL SL6, bazel 0.8.0, tensorflow r1.5, CUDA 8.0, cudnn7.0.4, using gcc and compilation stuff from the devtoolset-4 module\r\n\r\nBuild process:\r\n\r\n0. delete the bazel cache folder\r\n1. setup the crosstool files following but updating paths to the devtoolset-**4** version [#8529](https://github.com/tensorflow/tensorflow/issues/8529).\r\n2. apply the fix proposed by ilya-biryukov removing the \"-no-canonical-prefixes\" option in third_party/gpus/cuda_configure.bzl file\r\n ```\r\n-  result = repository_ctx.execute([cc, \"-no-canonical-prefixes\",\r\n-                                   \"-E\", \"-x\" + lang, \"-\", \"-v\"])\r\n\r\n+  result = repository_ctx.execute([cc, \"-E\", \"-x\" + lang, \"-\", \"-v\"])\r\n   index1 = result.stderr.find(_INC_DIR_MARKER_BEGIN)\r\n   if index1 == -1:\r\n \r\n```\r\n3. use the build command:\r\nbazel build --linkopt='-lrt' -c opt --copt=-mavx --copt=-mavx2 --copt=-msse4.2 --copt=-msse4.1 --copt=-mfma --copt=-msse3 --config=cuda --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow/tools/pip_package:build_pip_package\r\n\r\nIt finally worked... waiting for this for sooooo long, thanks to all with your hints and discussions !\r\n", "I encountered this issue after upgrading from fedora 28 directly to fedora 30. bazel clean --expunge resolved the issue.", "We've had discussions with the Bazel's external repositories team, and they're delivering `bazel sync --configure` in Bazel 0.29 command that will only refetch configure-like external repositories (which nccl_archive is). Good news is that we won't have to throw everything out with `clean --expunge`, only artifacts affected by the update will be recomplited. Bad news is that you still have to manually call `sync --reconfigure` when you update your system dependencies.", "Relevant Bazel commit - https://github.com/bazelbuild/bazel/commit/4a1380fed159f09b29df461b85e0469f8cd81d34\r\n"]}, {"number": 14379, "title": "Fix sycl BUILD bazel syntax error", "body": "Fix build issues from #12882", "comments": ["@martinwicke FYI", "@martinwicke all the GPU issues we are seeing look really bad.\r\nIs it a known issue?", "look like a bad GPU on a machine?"]}, {"number": 14378, "title": "Fix typo. <Copybara Experiment DO NOT MERGE>", "body": "Fix typo in tensorflow/python/client/session_clusterspec_prop_test.py", "comments": []}, {"number": 14377, "title": "Fix indenting on cache check in Network layer class", "body": "Resolves [#14054](https://github.com/tensorflow/tensorflow/issues/14054)\r\n\r\nThe `else` and ensuing code on the cache check was indented one spot too far. This resulted in the cache hit code not being executed when there was a cache hit (and the cache miss code never being called because there was a cache hit). \r\n", "comments": ["Can one of the admins verify this patch?", "I apologize that this has languished for so long.\r\n\r\nThis code has moved to network.py. Could you rebase?\r\n\r\n@fchollet, should we make this fix internally?", "Looking at the conflict (lines 2125-2135), it looks like this issue has already been resolved. So this PR is probably moot at this point.", "Oops, no, I take that back. I just reread your comment and see the bug is in the network.py class. I will rebase.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Thanks! Testing now.", "@tensorflow-jenkins test this please"]}, {"number": 14376, "title": "Typo fixed in RELEASE.md", "body": "The spelling of `backwards` is a non-American variant. For consistency, consider replacing it with the American English spelling.", "comments": ["Can one of the admins verify this patch?", "Thanks for the pull request! I'm not sure we should update this. Closing."]}, {"number": 14375, "title": "Build is broken at HEAD", "body": "From ci.bazel.io (mac and Linux: https://ci.bazel.io/blue/organizations/jenkins/TensorFlow/detail/TensorFlow/1544/pipeline/):\r\n\r\n```\r\nERROR: /home/ci/.cache/bazel/_bazel_ci/9630feff78264ece04615e9d6bad98b7/external/local_config_sycl/sycl/BUILD:14:14: Traceback (most recent call last):\r\n\tFile \"/home/ci/.cache/bazel/_bazel_ci/9630feff78264ece04615e9d6bad98b7/external/local_config_sycl/sycl/BUILD\", line 12\r\n\t\tconfig_setting(name = \"using_sycl_ccpp\", values =...\"})\r\n\tFile \"/home/ci/.cache/bazel/_bazel_ci/9630feff78264ece04615e9d6bad98b7/external/local_config_sycl/sycl/BUILD\", line 14, in config_setting\r\n\t\t{\"define\": \"using_sycl=true\", \"define\": \"using_trisycl=false\"}\r\nDuplicated key \"define\" when creating dictionary.\r\nERROR: /home/ci/.cache/bazel/_bazel_ci/9630feff78264ece04615e9d6bad98b7/external/local_config_sycl/sycl/BUILD:22:14: Traceback (most recent call last):\r\n\tFile \"/home/ci/.cache/bazel/_bazel_ci/9630feff78264ece04615e9d6bad98b7/external/local_config_sycl/sycl/BUILD\", line 20\r\n\t\tconfig_setting(name = \"using_sycl_trisycl\", value...\"})\r\n\tFile \"/home/ci/.cache/bazel/_bazel_ci/9630feff78264ece04615e9d6bad98b7/external/local_config_sycl/sycl/BUILD\", line 22, in config_setting\r\n\t\t{\"define\": \"using_sycl=true\", \"define\": \"using_trisycl=true\"}\r\nDuplicated key \"define\" when creating dictionary.\r\n```\r\n\r\nRepro:\r\n```\r\n$ bazel build @local_config_sycl//sycl:sycl\r\n..........\r\nERROR: /usr/local/google/home/dmarting/.cache/bazel/_bazel_dmarting/7d9873875fd56d2bc53371f0098a5332/external/local_config_sycl/sycl/BUILD:14:14: Traceback (most recent call last):\r\n        File \"/usr/local/google/home/dmarting/.cache/bazel/_bazel_dmarting/7d9873875fd56d2bc53371f0098a5332/external/local_config_sycl/sycl/BUILD\", line 12\r\n                config_setting(name = \"using_sycl_ccpp\", values =...\"})\r\n        File \"/usr/local/google/home/dmarting/.cache/bazel/_bazel_dmarting/7d9873875fd56d2bc53371f0098a5332/external/local_config_sycl/sycl/BUILD\", line 14, in config_setting\r\n                {\"define\": \"using_sycl=true\", \"define\": \"using_trisycl=false\"}\r\nDuplicated key \"define\" when creating dictionary\r\nERROR: /usr/local/google/home/dmarting/.cache/bazel/_bazel_dmarting/7d9873875fd56d2bc53371f0098a5332/external/local_config_sycl/sycl/BUILD:22:14: Traceback (most recent call last):\r\n        File \"/usr/local/google/home/dmarting/.cache/bazel/_bazel_dmarting/7d9873875fd56d2bc53371f0098a5332/external/local_config_sycl/sycl/BUILD\", line 20\r\n                config_setting(name = \"using_sycl_trisycl\", value...\"})\r\n        File \"/usr/local/google/home/dmarting/.cache/bazel/_bazel_dmarting/7d9873875fd56d2bc53371f0098a5332/external/local_config_sycl/sycl/BUILD\", line 22, in config_setting\r\n                {\"define\": \"using_sycl=true\", \"define\": \"using_trisycl=true\"}\r\nDuplicated key \"define\" when creating dictionary\r\nERROR: error loading package '@local_config_sycl//sycl': Package 'sycl' contains errors\r\nINFO: Elapsed time: 2.250s\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\n```\r\nAfter configuring.\r\n\r\n\r\nBisecting blame fe197f7dc5bd3b986141bcdaa27928206e74741a\r\n\r\n/cc @gunan", "comments": ["+1 macOS 10.13.1 (17B48)", "Fix is now merged."]}, {"number": 14374, "title": "Get all placeholders of the graph", "body": "According to [this](https://stackoverflow.com/questions/38933793/how-to-get-reference-by-name-of-variable-placeholder):, there are two ways to get placeholders of the graph. \r\n\r\nHowever, neither of them are very convenient.  \r\n\r\nWhen I reuse a pretrained model, it is common to inspect the shape of **all placeholders**. But there is no API to **get all placeholders easily**.\r\n\r\nIs it good for all placeholders to be added to a \"placeholder\" collection automatically? ", "comments": ["@martinwicke  where do you think is the best place to add this feature request. @aselle  suggests contrib.graph_util", "Sounds right.", "I am working on it.", "@bignamehyp Does `contrib.graph_util` mean `tensorflow/contrib/framework/python/framework/graph_util.py` or something else?", "Yes, that one.", "How about to allow the input of `get_placeholders` to be anything, that can be passed to `tf.Session.run`?\r\n\r\nHere the code that I use to get the placeholders:\r\n```python\r\ndef get_placeholder(tensor, return_op=False):\r\n    \"\"\"\r\n    Returns a list of placeholders,\r\n    if placeholder order is predictable else a set of placeholders.\r\n\r\n    >>> a = tf.placeholder(tf.float32)\r\n    >>> b = a ** 2\r\n    >>> get_placeholder(b)\r\n    [<tf.Tensor 'Placeholder:0' shape=<unknown> dtype=float32>]\r\n    >>> get_placeholder({'b': b})\r\n    [<tf.Tensor 'Placeholder:0' shape=<unknown> dtype=float32>]\r\n    >>> get_placeholder([b])\r\n    [<tf.Tensor 'Placeholder:0' shape=<unknown> dtype=float32>]\r\n\r\n    >>> from IPython.lib.pretty import pprint\r\n    >>> a = tf.placeholder(tf.float32, name='a')\r\n    >>> b = tf.placeholder(tf.float32, name='b')\r\n    >>> c = a + b\r\n    >>> pprint(get_placeholder(c))\r\n    [<tf.Tensor 'a:0' shape=<unknown> dtype=float32>,\r\n     <tf.Tensor 'b:0' shape=<unknown> dtype=float32>]\r\n    >>> pprint(get_placeholder(a + 2))\r\n    [<tf.Tensor 'a:0' shape=<unknown> dtype=float32>]\r\n    >>> pprint(get_placeholder(c, True))\r\n    [<tf.Operation 'a' type=Placeholder>, <tf.Operation 'b' type=Placeholder>]\r\n\r\n    \"\"\"\r\n    tensor_list = nest.flatten(tensor)\r\n\r\n    def get_inputs(obj):\r\n        if isinstance(obj, tf.Tensor):\r\n            return obj.op.inputs\r\n        elif isinstance(obj, tf.Operation):\r\n            return obj.inputs\r\n        else:\r\n            raise TypeError(obj)\r\n\r\n    todo = list(chain(*[list((t, i) for i in get_inputs(t)) for t in tensor_list]))\r\n\r\n    visited = set()\r\n    placeholders = list()\r\n\r\n    counter = 0\r\n\r\n    while len(todo) > 0:\r\n        parent, tensor = todo.pop(0)\r\n\r\n        if tensor in visited:\r\n            continue\r\n        visited.add(tensor)\r\n        if tensor.op.type == 'Placeholder':\r\n            placeholders.append(tensor)\r\n            counter += 1\r\n\r\n        todo.extend(list((tensor, i) for i in tensor.op.inputs))\r\n\r\n    if return_op:\r\n        placeholders = [p.op for p in placeholders]\r\n\r\n    return placeholders\r\n```", "```python\r\nplaceholders = [ op for op in graph.get_operations() if op.type == \"Placeholder\"]\r\n```", "Closing as this is resolved\r\n\r\n\r\n\r\n"]}, {"number": 14373, "title": "pip3 syntax error", "body": "I put in the code in python but it does not work i made sure that pip3 and python is updated and installed but it gives me this:\r\n>>> pip3 install tensorflow\r\n  File \"<stdin>\", line 1\r\n    pip3 install tensorflow\r\n               ^\r\nSyntaxError: invalid syntax\r\n>>> pip install tensorflow\r\n  File \"<stdin>\", line 1\r\n    pip install tensorflow\r\n              ^\r\nSyntaxError: invalid syntax\r\n>>>\r\n>>> pip3 install --upgrade tensorflow\r\n  File \"<stdin>\", line 1\r\n    pip3 install --upgrade tensorflow\r\n               ^\r\nSyntaxError: invalid syntax\r\n>>> pip3 install --upgrade tensorflow\r\n  File \"<stdin>\", line 1\r\n    pip3 install --upgrade tensorflow\r\n               ^\r\n(I tryed multiple times)\r\n\r\n\r\n\r\n ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14372, "title": "golang: added Session.ListDevices method", "body": "Implemented Session.ListDevices() method", "comments": ["Can one of the admins verify this patch?", "Please ignore this one"]}, {"number": 14371, "title": "Fix #14324 and another compile error with mkl", "body": "This fix fixes #14324 and another compile error with mkl below\r\n\r\n> ERROR: /home/zhang/tensorflow/tensorflow/core/kernels/BUILD:780:1: C++ compilation of rule '//tensorflow/core/kernels:slice_op' failed (Exit 1)\r\ntensorflow/core/kernels/slice_op.cc: In member function 'void tensorflow::MklSliceOp<Device, T>::HandleCase4D(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<long long int>&, const tensorflow::gtl::ArraySlice<long long int>&, tensorflow::Tensor*)':\r\ntensorflow/core/kernels/slice_op.cc:393:50: error: 'input' was not declared in this scope\r\n         context->eigen_device<Device>(), result, input, begin, size);\r\n                                                  ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\r\nUpdate:\r\nFound a similar PR #14329 has been posted already.\r\nBut this fix improves that fix by removing redundant code. Should I close this PR and create a new one to improve that fix?", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 14370, "title": "Change `NHWC`/`NCHW` to `NWC`/`NCW` for conv1d", "body": "While working on #13105 I noticed that in the current code base `conv1d` uses `NHWC`/`NCHW` which should really be `NWC`/`NCW`.\r\n\r\nThis fix addresses this issue and keep `NHWC`/`NCHW` compatible internally so that users will not be impacted.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Not sure we'll be able to change this since it's not backwards-compatible.", "Thanks @martinwicke for the tip of `@deprecated_arg_values`. The PR has been updated with `@deprecated_arg_values` added. Please take a look.", "Jenkins, test this please.", "(From API review)\r\n\r\nIs using \"width\" for 1D somehow better than \"height\"? (Or the other way around).\r\nSounds like `W` or `H` are also inappropriate for 1D values.\r\n\r\nI'd be tempted to leave things as-is for now?", "Thanks @asimshankar for the review. The NCW/NWC was mainly to follow other ops in nn like `convolution`:\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/convolution\r\n\r\n\r\ndata_format: A string or None. Specifies whether the channel dimension of the input and output is the last dimension (default, or if data_format does not start with \"NC\"), or the second dimension (if data_format starts with \"NC\"). For N=1, the valid values are \"NWC\" (default) and \"NCW\". For N=2, the valid values are \"NHWC\" (default) and \"NCHW\". For N=3, the valid values are \"NDHWC\" (default) and \"NCDHW\".\r\n\r\n\r\nI also noticed that a similar op `conv1d_transpose` (as opposed to `conv1d` in this PR) is using `NCW/NWC` (in Ln 2328 of the same file). That was why I opened this PR.\r\n\r\nNot sure what is the best way to handle the inconsistencies but happy to do one way or another.", "Ok, checked with @asimshankar, this is fine."]}, {"number": 14368, "title": "golang: ~2x speedup for encodeTensor()", "body": "before:\r\n\r\n$ go test -bench=.\r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com/tensorflow/tensorflow/tensorflow/go\r\nBenchmarkNewTensor/[150528]-8                200           6792809 ns/op\r\nPASS\r\nok      github.com/tensorflow/tensorflow/tensorflow/go  2.116s\r\n\r\nafter:\r\n\r\n$ go test -bench=.\r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com/tensorflow/tensorflow/tensorflow/go\r\nBenchmarkNewTensor/[150528]-8                500           3269740 ns/op\r\nPASS\r\nok      github.com/tensorflow/tensorflow/tensorflow/go  2.021s\r\n\r\n", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "It seems the `//tensorflow/go:test` test is failing as UInt32 and UInt64 have not been supported yet.\r\n\r\nSee Ln 74 of `tensorflow/go/tensor_test.go`\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5fb5d8207b576e57acd2fac00da429284eb26647/tensorflow/go/tensor_test.go#L74-L78\r\n\r\nand compare Ln 334 and 316 of `tensorflow/go/tensor.go`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5fb5d8207b576e57acd2fac00da429284eb26647/tensorflow/go/tensor.go#L316\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5fb5d8207b576e57acd2fac00da429284eb26647/tensorflow/go/tensor.go#L334\r\n\r\n\r\nCreated a PR #14408 for the fix."]}, {"number": 14367, "title": "golang: added Session.ListDevices method", "body": "Implemented Session.ListDevices() method", "comments": ["Can one of the admins verify this patch?", "Please ignore this one"]}, {"number": 14366, "title": "Question: How to feed a batch of variable sized images to the Tensorflow C/C++ or Java API on Android?", "body": "I asked this question on Stackoverflow before but got no answers: https://stackoverflow.com/questions/46906269/how-to-process-variable-sized-images-with-tensorflow-java-api\r\n\r\nIn short: I have a CNN model that is doing some preprocessing of the images in the tensorflow graph before running the CNN on the data. This preprocessing includes resizing the images after normalizing them etc. Thus, I need to feed my model with a batch of differently sized images. While I could try to do this in OpenCV, I don't want to, as it seems messy to reimplement the complicated preprocessing and not as efficient. \r\n\r\nI found a way to do it in the standard version of tensorflow. I encoded the images as png strings and used the new Java API 1.4 to feed a batch of these strings to my tensorflow model. There I first decoded the png string to get back the image.\r\n\r\nUnfortunately, this does not work on Android. There are several problems:\r\n* png encoding is not available (I then used .bmp; but that has no grayscale support; I then made my gray image colored and make it gray again in the model (https://github.com/tensorflow/tensorflow/issues/13942) )\r\n* The Android API does not seem to be able to handle string tensors (https://github.com/tensorflow/tensorflow/issues/14291)\r\n\r\nSo my question is, if it is possible to feed a model with a batch of differently sized images via the C/C++ or Java API on Android.", "comments": ["@rmlarsen @aselle  \r\nFYI (please feel free to cc anyone else who may be interested)\r\n", "any ideas?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 177 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 194 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 209 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 224 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 239 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 254 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 268 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This is not really possible since decode ops are usually expected to be done in host language, and it would be likely be more efficient to do it before invoking tensorflow in any case. This is because there is usually much faster since there are platform specific accelerations (even so as much as to make the camera output smaller images in the first place). So closing as expected behavior.", "BTW, it's possible to use TensorFlow with OpenCV easily and efficiently, sharing data, and managing memory even from Java using JavaCPP: http://bytedeco.org/news/2018/07/17/bytedeco-as-distribution/\r\n/cc @asimshankar ", "@andreas-eberle : regarding the two issues you pointed out, an alternative would be to use [selective registration](https://github.com/tensorflow/tensorflow/blob/915cf10d4bcbb51f079f6841aeaf876b430a920b/tensorflow/python/tools/print_selective_registration_header.py#L15) (CC @petewarden) to include the appropriate kernels, but that would require building from source.\r\n\r\nThanks @saudet for the JavaCPP suggestion.\r\n\r\n(Closing this out as per @aselle's previous comment)"]}, {"number": 14365, "title": "Unable to read saved model from SMB directory", "body": "I am using Tensorflow GPU version 1.1.0, my model can run well if I save it to computer 's hard drives. However, if I save checkpoint files to a samba network drive, I can not restore it and here are the logs.\r\n\r\n\r\n2017-11-08 17:25:27.795833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\r\n2017-11-08 17:26:13.335734: W tensorflow/core/framework/op_kernel.cc:1152] Unavailable: /path/to/network/drive/runs/1510129519/checkpoints/model-100.index\r\n2017-11-08 17:26:13.339605: W tensorflow/core/framework/op_kernel.cc:1152] Unknown: /path/to/network/drive/runs/1510129519/checkpoints/model-100.index; Input/output error\r\n2017-11-08 17:26:13.361089: W tensorflow/core/framework/op_kernel.cc:1152] Unavailable: /path/to/network/drive/runs/1510129519/checkpoints/model-100.index\r\n\t [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_2/tensor_names, save/RestoreV2_2/shape_and_slices)]]\r\n^C^C^C^C^C^C2017-11-08 17:26:58.362630: W tensorflow/core/framework/op_kernel.cc:1152] Unavailable: /path/to/network/drive/runs/1510129519/checkpoints/model-100.index\r\n\r\nMany thanks", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14364, "title": "What is the instruction of checking which version of cuda and cudnn the tensorflow is running on?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:1.4\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:8.0/6.0\r\n- **GPU model and memory**:Tesla P100-PCIE-16GB\r\n- **Exact command to reproduce**:\r\n\r\n\r\n\r\n### Describe the problem\r\n\r\nWhat is the instruction of checking which version of cuda and cudnn the tensorflow is running on?\r\nlike the tensorflow version could be checked by the instruction tf.__version__ .\r\nI want to check my tensorflow 1.4 running on the 8.0 cuda and 6.0 cudnn, not running on the 9.0 cuda and 7.0 cudnn.\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14363, "title": "Failed to synchronize the stop event", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n\r\n- **TensorFlow version (use command below)**:\r\nb'v1.4.0-0-gd752244' 1.4.0\r\n\r\n- **Python version**: \r\n3.5.2\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.7.0\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\n\r\n- **CUDA/cuDNN version**:\r\n9.0/7.0\r\n\r\n- **GPU model and memory**:\r\nTesla V100-SXM2-16GB\r\n\r\n- **Exact command to reproduce**:\r\n```\r\ngit clone https://github.com/ljanyst/image-segmentation-fcn.git\r\ncd image-segmentation-fcn                                       \r\nwget http://www.cvlibs.net/download.php?file=data_road.zip\r\nunzip data_road.zip                                     \r\n./train.py  --data-dir data_road\r\n````\r\n### Describe the problem\r\nIt seems like I am hitting some sort of a CUDA/cuDNN synchronization/race issue. Please see the snippet in the next section for the exact error message. The problem only happens with the KITTI dataset. The exact same TensorFlow code works fine for the Cityscapes dataset. Also, the problem only happens on Tesla V100. I tested the same exact software configuration on Tesla K80 and GeForce GTX1080 Ti as well, and things work fine.\r\n\r\n### Source code / logs\r\n```\r\n2017-11-08 12:24:52.838039: E tensorflow/stream_executor/cuda/cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2017-11-08 12:24:52.838090: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x51f18f0: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2017-11-08 12:24:52.838106: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x51f18f0: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2017-11-08 12:24:52.838137: F tensorflow/stream_executor/cuda/cuda_dnn.cc:3218] failed to set stream for cudnn handle: CUDNN_STATUS_MAPPING_ERROR\r\nzsh: abort (core dumped)  ./train.py --data-dir data_road\r\n```\r\n", "comments": ["@zheng-xq, can you take a look at this?", "The synchronization error is only what finds out the issue. The root cause is some GPU kernels had an illegal address access.\r\n\r\nIf someone wants to root cause this, first it is needed to find the offending kernel. In our past experience, it could be either a kernel bug, or a degenerate data entry. ", "Thanks for the hint @zheng-xq ! I have had a closer look, and the offending kernel seems to be:\r\n\r\n```\r\nCUDA Exception: Warp Out-of-range Address\r\n\r\nThread 28 \"python\" received signal CUDA_EXCEPTION_5, Warp Out-of-range Address.\r\n[Switching focus to CUDA kernel 1994, grid 1995, block (0,0,0), thread (128,0,0), device 0, sm 0, warp 6, lane 0]\r\n0x00007ffe7ac23a50 in volta_scudnn_128x128_stridedB_splitK_xregs_large_nn_v1_LOOP<<<(5,1,160),(256,1,1)>>> ()\r\n(cuda-gdb) info cuda kernels\r\n  Kernel Parent Dev Grid Status   SMs Mask   GridDim  BlockDim Invocation \r\n*   1994      -   0 1995 Active 0xffffffff (5,1,160) (256,1,1) .text.volta_scudnn_128x128_stridedB_splitK_xregs_large_nn_v1() \r\n```\r\n\r\nI was unable to get any useful host-side stack trace because there appears to by something wrong with the DWARF symbols: `Unable to access DWARF register number 83886081`. I am not sure whether it's about the symbols in CUDA/cuDDN or in TensorFlow. Do you think that recompiling TensorFlow in debug mode will help? If so, how do I pass extra parameters to nvcc with Bazel?", "That seems to be a Cudnn bug. Adding NVIDIA folks. \r\n\r\n@benbarsdell, @nluehr, any insight to debug this Cudnn kernel exception?\r\n", "@ljanyst Let's talk offline so that I can have a look at this with debug info. Thanks.", "@zheng-xq @ljanyst We have a repro and a fix. Roll out is planned in cuDNN 7.0.5 mid-December.", "Hello\r\nI installed tensorflow 1.4 cudnn 6 and cuda 8.0\r\nI have the same problem \"cuda_event.cc:49 Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\" when I try yo train with tensoflow\r\nI have tried to install anothers versions but I have the same issue always and in some computers, not only mine. Do you know what I have to do? Thanks", "Came here to report the exact same thing with our Volta, using the Tensorflow container on NVIDIA GPU Cloud. We will be happy to test the fix with cuDNN 7.0.5 and follow-up. Please let us know if there are any other updates on this issue or if more information is needed.", "Same problem with WaveNet on V100:\r\n```\r\n2017-12-05 14:08:26.119341: E tensorflow/stream_executor/cuda/cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2017-12-05 14:08:26.119423: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0xbab66e0: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2017-12-05 14:08:26.119435: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0xbab66e0: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2017-12-05 14:08:26.119470: F tensorflow/stream_executor/cuda/cuda_dnn.cc:3218] failed to set stream for cudnn handle: CUDNN_STATUS_MAPPING_ERROR\r\n```\r\nIs there a more precise timeline info on the fix? We would gladly try a beta version of cuDNN, if any exists.\r\nThank you!", "Confirmed that cuDNN 7.0.5 from https://developer.nvidia.com/rdp/cudnn-download fixes this on the AWS p3.8xlarge (Volta 4 GPU)", "@tukushan Glad to hear you can confirm the fix. @ManuelaPa @mholt @RerRayne @ljanyst Should be fixed by using cuDNN 7.0.5.", "After installing cuDNN 7.0.5, I am still seeing this error. :( I'm using the TensorFlow container from NVIDIA GPU cloud. Anyone know if there's any extra steps I need to take? I extracted the library files and moved them into place according to the installation instructions...", "@mholt - If you're using the NGC image for TensorFlow, you don't actually need to install cuDNN directly, as it's installed in the container for you already.  The NGC frameworks release 17.12 includes cuDNN 7.0.5 and should fix this issue.  Or are you saying you tried 17.12 and still see an issue?", "Ah, sorry, I mistakenly thought cuDNN was installed outside the container (but maybe that is CUDA actually) -- pulling and using the latest container fixed it. Looks like that patch did it. Thank you!", "@juliebernauer it works after updating. Thank you a lot!", "Things work for me too now. Thanks @juliebernauer !", "@juliebernauer Thanks for the answer! I installed cudnn 7.0.5 but it's not compatible with tensorflow 1.4, it needs cudnn64_6.dll, do you know that I should do? \r\nThank you", "@ManuelaPa The developer website lists Win7 and Win10 versions for both CUDA9.0 and CUDA9.1. So one has to make sure to download and install the one needed - after removing previous versions on Windows (this is not the case on Linux). Can you please try this? This should work for you. If not, may I suggest you send a ticket or report a bug on the NVIDIA developer website?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Update seems to fix the issue, closing this. ", "@juliebernauer so my understanding is v9.1 has the fix. but I'm having the same error \r\n```\r\n2018-06-01 09:49:37.379160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:\r\nname: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:06:00.0\r\ntotalMemory: 22.38GiB freeMemory: 22.21GiB\r\n2018-06-01 09:49:37.379206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-06-01 09:49:37.673924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-01 09:49:37.673988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0\r\n2018-06-01 09:49:37.673996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N\r\n2018-06-01 09:49:37.674543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21559 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n2018-06-01 09:49:44.636341: E tensorflow/stream_executor/cuda/cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-06-01 09:49:44.636422: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x6ab0fb0: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-06-01 09:49:44.636433: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x6ab0fb0: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-06-01 09:49:44.636483: F tensorflow/stream_executor/cuda/cuda_dnn.cc:2328] failed to set stream for cudnn handle: CUDNN_STATUS_MAPPING_ERROR\r\nAborted\r\n```\r\n\r\nI'm on Red hat EL 7, CUDA V9.1.85 (as seen from `nvcc --version`) and TF 1.7. \r\n\r\nDo I need to upgrade to CUDA 9.2?", "@weiliu620 you want to make sure you are indeed using the cudnn version mentioned above. Upgrading CUDA won't change that by default (but might get you to use a different dir). Cleaning your LD_LIBRARY_PATH might help.", "I still encounter the same problem as others reported, with CUDA 9.0 and cudnn 7.0.2. \r\n\r\nIf I tried cudnn 7.1.2, I got a different error:\r\n```  File \"/home/xxx/.conda/envs/tf2/lib/python3.5/site-packages/tensorflow/python\r\n/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: cudnn PoolForward launch \r\nfailed\r\n         [[Node: AvgPool3D_15 = AvgPool3D[T=DT_FLOAT, data_format=\"NDHWC\", ksize\r\n=[1, 2, 2, 2, 1], padding=\"SAME\", strides=[1, 2, 2, 2, 1], _device=\"/job:localho\r\nst/replica:0/task:0/device:GPU:1\"](ExpandDims_1)]]\r\n         [[Node: mul_29/_23 = _Recv[client_terminated=false, recv_device=\"/job:l\r\nocalhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/t\r\nask:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_47_mul_29\", te\r\nnsor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'AvgPool3D_15', defined at:\r\n [hiding lines related to customer codes]\r\n  File \"/home/xxx/.conda/envs/tf2/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 177, in avg_pool3d\r\n    padding=padding, data_format=data_format, name=name)\r\n  File \"/home/xxx/.conda/envs/tf2/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/xxx/.conda/envs/tf2/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"/home/xxx/.conda/envs/tf2/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): cudnn PoolForward launch failed\r\n         [[Node: AvgPool3D_15 = AvgPool3D[T=DT_FLOAT, data_format=\"NDHWC\", ksize=[1, 2, 2, 2, 1], padding=\"SAME\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"](ExpandDims_1)]]\r\n         [[Node: mul_29/_23 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_47_mul_29\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n\r\nThese happen for multiple versions of tensorflow I tried, from 1.5 to 1.7", "I have this problem on CUDA 10.0. I'm using TF 1.10.0, keras 2.2.2, Window 10, GPU Nvidia mx150.\r\nSome NNs work with no problem, some fail.", "I confirm it solves the issue", "```\r\n2019-03-19 13:31:35.622328: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: FMA\r\n2019-03-19 13:31:35.642103: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4336885000 Hz\r\n2019-03-19 13:31:35.642675: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55a2ee4e12b0 executing computations on platform Host. Devices:\r\n2019-03-19 13:31:35.642712: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-19 13:31:35.860957: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55a2ee4be0b0 executing computations on platform CUDA. Devices:\r\n2019-03-19 13:31:35.861029: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\r\n2019-03-19 13:31:35.862019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.11GiB\r\n2019-03-19 13:31:35.862059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-03-19 13:31:35.864736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-19 13:31:35.864767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-03-19 13:31:35.864792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-03-19 13:31:35.865597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6915 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n[I 13:32:16.864 LabApp] Saving file at /mobilenet-v2.ipynb\r\n2019-03-19 13:33:04.306841: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n2019-03-19 13:33:05.855707: E tensorflow/stream_executor/cuda/cuda_driver.cc:981] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-03-19 13:33:05.855752: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0x7f5e1c700f30: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-03-19 13:33:05.855766: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0x7f5e1c700f30: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-03-19 13:33:05.855786: F tensorflow/stream_executor/cuda/cuda_dnn.cc:194] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n[I 13:33:17.075 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports\r\nkernel 7b15ebfd-d35b-4c14-a383-59f53ea73a89 restarted\r\n```\r\n\r\n```\r\nnvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:01_CDT_2018\r\nCuda compilation tools, release 10.0, V10.0.130\r\n```\r\n\r\nGetting this same error also. I've dropped the batch size to 50. \r\n\r\nSpecs:\r\n```\r\nManjaro Linux 4.19.28-1-MANJARO #1 SMP PREEMPT Sun Mar 10 08:32:42 UTC 2019 x86_64 GNU/Linux\r\n```\r\n\r\n```\r\nKeras                2.2.4      \r\nKeras-Applications   1.0.7      \r\nKeras-Preprocessing  1.0.9\r\nPython 3.7.1\r\ntensorboard          1.13.1     \r\ntensorflow           1.13.1     \r\ntensorflow-estimator 1.13.0     \r\ntensorflow-gpu       1.13.1\r\npycuda               2018.1.1\r\n\r\ncuda 10.0.130-2\r\npycuda-headers 2018.1.1-4\r\npython-pycuda 2018.1.1-4\r\npython-tensorflow-cuda 1.13.1-2\r\ntensorflow-cuda 1.13.1-2\r\n```", "@zheng-xq   hello, I also met this error using RTX2070. but after replacing a new gpu, I have never seen this error so far. I agree with you on the fact that  the synchronization error is only what finds out the issue. The root cause is some GPU kernels had an illegal address access. Hope to get  some insight from @benbarsdell , @nluehr . Thank you!", "> Same problem with WaveNet on V100:\r\n> \r\n> ```\r\n> 2017-12-05 14:08:26.119341: E tensorflow/stream_executor/cuda/cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n> 2017-12-05 14:08:26.119423: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0xbab66e0: CUDA_ERROR_ILLEGAL_ADDRESS\r\n> 2017-12-05 14:08:26.119435: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0xbab66e0: CUDA_ERROR_ILLEGAL_ADDRESS\r\n> 2017-12-05 14:08:26.119470: F tensorflow/stream_executor/cuda/cuda_dnn.cc:3218] failed to set stream for cudnn handle: CUDNN_STATUS_MAPPING_ERROR\r\n> ```\r\n> \r\n> Is there a more precise timeline info on the fix? We would gladly try a beta version of cuDNN, if any exists.\r\n> Thank you!\r\n\r\nhow you fix it    mine is 2080ti    ,same question    how to fix it", "> @tukushan Glad to hear you can confirm the fix. @ManuelaPa @mholt @RerRayne @ljanyst Should be fixed by using cuDNN 7.0.5.\r\ncudnn version is 7.0.5   and  I have to change the CUDA and tensorflow version    too???????", "> > @tukushan Glad to hear you can confirm the fix. @ManuelaPa @mholt @RerRayne @ljanyst Should be fixed by using cuDNN 7.0.5.\r\n> > cudnn version is 7.0.5   and  I have to change the CUDA and tensorflow version    too???????\r\n\r\n\r\n\r\n> dropped the batch size to 50.\r\n\r\nand teh error is solveed???", "> Internal: error destroying CUDA event in context 0x7f5e1c700f30: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n> 2019-03-19 13:33:05.855766: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0x7f5e1c700f30: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n> 2019-03-19 13:33:05.855786: F tensorflow/stream_executor/cuda/cuda_dnn.cc:194] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n> [I 13:33:17.075 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports\r\n\r\nmy versions and errors are same as yours : cuda 10.0 ,tf\r\n13.1     the error is same: tensorflow/stream_executor/cuda/cuda_dnn.cc:194] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream\r\n\r\n\r\n\r\nhow to slover them ??????", "> I confirm it solves the issue\r\n\r\nhow to slover it", "> > I confirm it solves the issue\r\n> \r\n> how to slover it\r\n\r\nHave you solved it? ", "mine is 2080ti and it only happens when it happens once. the first several train will be fine but if by accident it happens. i have no way to reset from rebooting. \r\nI'm using tf 1.4, cuda 10.1 and cudnn7"]}, {"number": 14362, "title": "\u5728Windows\u4e0a\u5b89\u88c5TensorFlow", "body": "\u672c\u4efd\u6307\u5bfc\u5c06\u544a\u8bc9\u60a8\u5982\u4f55\u5728 Windows \u4e0a\u5b89\u88c5 TensorFlow\u3002\r\n## \u9009\u62e9\u51c6\u5907\u5b89\u88c5\u7684 TensorFlow \u7c7b\u578b\r\n\u60a8\u9700\u8981\u5728\u4ee5\u4e0b\u4e24\u79cd TensorFlow \u7c7b\u578b\u4e2d\u9009\u62e9\u60a8\u60f3\u5b89\u88c5\u7684\u7c7b\u578b\uff1a\r\n\r\n-  **\u4ec5\u652f\u6301 CPU \u7684 TensorFlow\u3002** \u5982\u679c\u60a8\u7684\u7cfb\u7edf\u5e76\u6ca1\u6709 NVIDIA\u00ae GPU \u8fd9\u6837\u7684 GPU\uff0c\u90a3\u4e48\u60a8\u53ea\u80fd\u5b89\u88c5\u4ec5\u652f\u6301 CPU \u7684 TensorFlow\u3002\u9700\u8981\u7279\u522b\u8bf4\u660e\u7684\u662f\uff0c\u76f8\u6bd4\u8d77\u53e6\u5916\u4e00\u4e2a\u7248\u672c\u7684 TensorFlow\uff0c\u8be5\u7248\u672c\u7684 TensorFlow \u901a\u5e38\u66f4\u52a0\u5bb9\u6613\u5b89\u88c5\uff08\u4e00\u822c\u800c\u8a005\u523010\u5206\u949f\u5373\u53ef\u5b8c\u6210\u5b89\u88c5\uff09\uff0c\u6240\u4ee5\u5373\u4f7f\u60a8\u62e5\u6709\u4e00\u4e2a NVIDIA \u7684 GPU\uff0c\u6211\u4eec\u4e5f\u66f4\u63a8\u8350\u60a8\u4f18\u5148\u5b89\u88c5\u8fd9\u4e2a\u7248\u672c\u3002\r\n\r\n\r\n- **\u652f\u6301 GPU \u7684 TensorFlow\u3002** \u4e00\u822c\u800c\u8a00\uff0cTensorFlow \u7684\u7a0b\u5e8f\u5728 GPU \u4e0a\u7684\u8fd0\u884c\u901f\u5ea6\u8981\u660e\u663e\u9ad8\u4e8e\u5728 CPU \u4e0a\u7684\u3002\u56e0\u6b64\uff0c\u5982\u679c\u60a8\u7684\u7cfb\u7edf\u62e5\u6709\u7b26\u5408\u4ee5\u4e0b\u8981\u6c42\u7684 NVIDIA \u00ae GPU\uff0c\u4e14\u60a8\u9700\u8981\u8fd0\u884c\u7684\u5e94\u7528\u6ce8\u91cd\u6027\u80fd\uff0c\u90a3\u4e48\u60a8\u6700\u7ec8\u9700\u8981\u5b89\u88c5\u6b64\u7248\u672c\u7684TensorFlow\u3002\r\n\r\n## \u8fd0\u884c\u652f\u6301 GPU TensorFlow \u7248\u672c\u7684\u9700\u6c42\r\n\u4e0d\u8bba\u60a8\u91c7\u7528\u672c\u4efd\u6307\u5bfc\u63d0\u4f9b\u7684\u54ea\u79cd\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5\uff0c\u5982\u679c\u60a8\u9700\u8981\u5b89\u88c5 GPU \u7684 TensorFlow\uff0c\u60a8\u5fc5\u987b\u5728\u60a8\u7684\u8bbe\u5907\u4e0a\u5b89\u88c5\u4ee5\u4e0bNVIDIA\u8f6f\u4ef6\uff1a\r\n\r\n- CUDA\u00ae Toolkit 8.0\u3002\u5173\u4e8e\u5b83\u7684\u8be6\u7ec6\u8bf4\u660e\u8bf7\u770b[ NVIDIA \u5b98\u65b9\u6587\u6863](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)\u3002\u8bf7\u786e\u4fdd\u60a8\u8bb2 Cuda \u76f8\u5173\u7684\u8def\u5f84\u540d\u90fd\u6309\u7167 NVIDIA \u6587\u6863\u7684\u63cf\u8ff0\u65b9\u6cd5\u52a0\u5165\u5230\u4e86%PATH%\u7cfb\u7edf\u53d8\u91cf\u4e2d\u3002\r\n\r\n- \u4e0e CUDA Toolkit 8.0 \u76f8\u5173\u7684 NVIDIA \u9a71\u52a8\u3002\r\n\r\n- cuDNN v6.1\u7248\u672c\u3002\u5173\u4e8e\u5b83\u7684\u8be6\u7ec6\u8bf4\u660e\u8bf7\u770b[ NVIDIA \u5b98\u65b9\u6587\u6863](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e00\u822c\u800c\u8a00\uff0ccuDNN\u7684\u5b89\u88c5\u5730\u5740\u548c\u5176\u4ed6CUDA DLL\u662f\u4e0d\u540c\u7684\u3002\u540c\u65f6\uff0c\u8bf7\u786e\u8ba4\u60a8\u628a\u60a8\u5b89\u88c5cuDNN DLL\u7684\u5b89\u88c5\u5730\u5740\u52a0\u5165\u5230\u4e86%PATH%\u7cfb\u7edf\u53d8\u91cf\u4e2d\u3002\r\n\r\n- \u5e26\u6709 CUDA Compute Capability 3.0 \u6216\u66f4\u9ad8\u7248\u672c\u7684 GPU \u5361\u3002\u8bf7\u5728[ NVIDIA \u5b98\u65b9\u6587\u6863\u4e2d](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)\u67e5\u8be2\u5177\u6709\u6761\u4ef6\u7684GPU\u6e05\u5355\u3002\r\n\r\n\u5982\u679c\u60a8\u7684\u7248\u672c\u4e0e\u4e0a\u8ff0\u5305\u6240\u8981\u6c42\u7684\u7248\u672c\u4e0d\u540c\uff0c\u8bf7\u6539\u5230\u6211\u4eec\u6240\u8981\u6c42\u7684\u7248\u672c\u3002\u7279\u522b\u8bf4\u660e\u7684\u662f\uff0ccuDNN \u7684\u7248\u672c\u5fc5\u987b\u4e0e\u8981\u6c42\u7684\u4e00\u81f4\uff1a\u5982\u679c\u65e0\u6cd5\u627e\u5230 cuDNN64_6.dll\uff0c\u90a3\u4e48 TensorFlow \u5c06\u65e0\u6cd5\u52a0\u8f7d\u3002\u5982\u679c\u60a8\u60f3\u4f7f\u7528\u5176\u4ed6\u7248\u672c\u7684 cuDNN\uff0c\u60a8\u9700\u8981\u4ece\u6e90\u4ee3\u7801\u5f00\u59cb\u91cd\u65b0\u7f16\u8bd1\u3002\r\n\r\n## \u5b89\u88c5 TensorFlow \u7684\u65b9\u5f0f\r\n\u60a8\u9700\u8981\u9009\u62e9\u5b89\u88c5 TensorFlow \u7684\u65b9\u5f0f\u3002\u5f53\u524d\u7684\u53ef\u9009\u65b9\u5f0f\u5982\u4e0b\uff1a\r\n- \u539f\u751f\u7684 pip \u65b9\u6cd5\r\n- \u4f7f\u7528 Anaconda\r\n\r\n\u539f\u751f\u7684 pip \u65b9\u6cd5\u53ef\u4ee5\u76f4\u63a5\u5728\u60a8\u7684\u7cfb\u7edf\u4e0a\u5b89\u88c5 TensorFlow \u800c\u5e76\u4e0d\u9700\u8981\u865a\u62df\u73af\u5883\uff08virtual environment\uff09\u3002\u7531\u4e8e\u539f\u751f\u7684\u4e00\u4e2apip\u5b89\u88c5\u5e94\u7528\u5e76\u6ca1\u6709\u88ab\u9694\u79bb\u5728\u4e00\u4e2a\u72ec\u7acb\u7684\u5e94\u7528\u4e2d\uff0c\u4f7f\u7528 pip \u5b89\u88c5\u65b9\u6cd5\u53ef\u80fd\u4f1a\u5f71\u54cd\u5230\u7cfb\u7edf\u91cc\u5176\u4ed6\u57fa\u4e8ePython\u7684\u5b89\u88c5\u3002\u4f46\u662f\uff0c\u5982\u679c\u60a8\u4e86\u89e3\u60a8\u7cfb\u7edf\u91cc\u7684 pip \u548c Python \u73af\u5883\uff0c\u90a3\u4e48\u4f7f\u7528\u539f\u751fpip\u5b89\u88c5\u4ec5\u4ec5\u53ea\u9700\u8981\u4e00\u6761\u547d\u4ee4\u5c31\u591f\u4e86\u3002\u800c\u4e14\uff0c\u5982\u679c\u60a8\u4f7f\u7528\u539f\u751f\u7684 pip \u5b89\u88c5\u65b9\u6cd5\uff0c\u90a3\u4e48\u7528\u6237\u53ef\u4ee5\u4ece\u7cfb\u7edf\u7684\u4efb\u4f55\u8def\u5f84\u53bb\u8fd0\u884c TensorFlow \u7a0b\u5e8f\u3002\r\n\r\n\u5728 Anaconda \u4e2d\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 conda \u53bb\u521b\u5efa\u4e00\u4e2a\u865a\u62df\u73af\u5883\uff08virtural environment\uff09\u3002\u4f46\u662f\uff0c\u5982\u679c\u662f\u4f7f\u7528 Anaconda \u65b9\u5f0f\uff0c\u6211\u4eec\u4f9d\u7136\u63a8\u8350\u4f7f\u7528 pip \u5b89\u88c5\u547d\u4ee4\u6765\u5b89\u88c5 TensorFlow\uff0c\u800c\u4e0d\u662f conda \u5b89\u88c5\u547d\u4ee4\u3002\r\n\r\n**\u6ce8\u610f\uff1a** conda \u5305\u662f\u7531\u793e\u533a\u63d0\u4f9b\u7684\uff0c\u800c\u4e0d\u662f\u5b98\u65b9\u3002\u4e5f\u5c31\u662f\u8bf4\uff0cTensorFlow \u56e2\u961f\u5e76\u4e0d\u4f1a\u6d4b\u8bd5\u4e5f\u4e0d\u4f1a\u7ef4\u62a4 conda \u5305\u3002\u4f7f\u7528 conda \u5305\u9700\u8981\u60a8\u81ea\u5df1\u627f\u62c5\u98ce\u9669\u3002\r\n\r\n## \u4f7f\u7528\u539f\u751fpip\u5b89\u88c5\r\n\u5982\u679c\u60a8\u7684\u673a\u5668\u4e0a\u6ca1\u6709\u5b89\u88c5\u4ee5\u4e0b\u7248\u672c\u7684Python\uff0c\u8bf7\u7acb\u523b\u5b89\u88c5\uff1a\r\n- [Python 3.5.x 64-bit from python.org](https://www.python.org/downloads/release/python-352/)\r\n- [Python 3.6.x 64-bit from python.org](https://www.python.org/downloads/release/python-362/)\r\n\r\n\u5728 Windows \u4e0a\uff0cTensorFlow \u652f\u6301 Python3.5.x \u7248\u672c\u548c Python 3.6.x \u7248\u672c\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c Python 3 \u4f7f\u7528\u7684\u662f pip3 \u5305\u7ba1\u7406\uff0c \u8fd9\u4e5f\u60a8\u7528\u6765\u5b89\u88c5 TensorFlow \u7684\u7a0b\u5e8f\u3002\r\n\u5728\u5b89\u88c5 TensorFlow \u65f6\uff0c\u8bf7\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\u3002\u7136\u540e\u5728\u7ec8\u7aef\u4e0a\u8fd0\u884c\u6b63\u786e\u7684 pip3 \u5b89\u88c5\u547d\u4ee4\u3002 \u5982\u679c\u662f\u5b89\u88c5\u4ec5\u652f\u6301 CPU \u7684 TensorFlow \u7248\u672c\uff0c\u8bf7\u8f93\u5165\u4e0b\u9762\u7684\u547d\u4ee4\uff1a\r\n`C:\\> pip3 install --upgrade tensorflow`\r\n\r\n\u5982\u679c\u662f\u5b89\u88c5\u652f\u6301 GPU \u7684 TensorFlow \u7248\u672c\uff0c\u8bf7\u8f93\u5165\u4e0b\u9762\u7684\u547d\u4ee4\uff1a\r\n`C:\\> pip3 install --upgrade tensorflow-gpu`\r\n\r\n## \u4f7f\u7528 Anaconda \u8fdb\u884c\u5b89\u88c5\r\n**Anaconda \u7684\u5b89\u88c5\u5305\u662f\u7531\u793e\u533a\u63d0\u4f9b\uff0c\u5e76\u975e\u5b98\u65b9\u63d0\u4f9b\u7684\u3002**\r\n\u5728 Anaconda \u7684\u73af\u5883\u4e0b\uff0c\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u8fdb\u884c TensorFlow \u7684\u5b89\u88c5\uff1a\r\n1.\u9075\u5faa [Anaconda \u4e0b\u8f7d\u7ad9\u70b9](https://www.anaconda.com/download/)\u91cc\u7684\u8bf4\u660e\u8fdb\u884c Anaconda \u7684\u4e0b\u8f7d\u548c\u5b89\u88c5\u3002 \r\n2.\u8bf7\u901a\u8fc7\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u6765\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a tensorflow \u7684 conda \u73af\u5883\uff1a\r\n`C:\\> conda create -n tensorflow pip python=3.5`\r\n3.\u901a\u8fc7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u6765\u6fc0\u6d3b\u4e00\u4e2a conda \u73af\u5883\uff1a\r\n`C:\\> activate tensorflow`\r\n`(tensorflow)C:\\>  # Your prompt should change `\r\n4.\u5728\u60a8\u7684 conda \u73af\u5883\u91cc\u8f93\u5165\u6b63\u786e\u7684\u547d\u4ee4\u6765\u5b89\u88c5 TensorFlow\u3002 \u5982\u679c\u662f\u5b89\u88c5\u4ec5\u652f\u6301 CPU \u7684 TensorFlow \u7248\u672c\uff0c\u8bf7\u8f93\u5165\u4e0b\u9762\u7684\u547d\u4ee4\uff1a\r\n`(tensorflow)C:\\> pip install --ignore-installed --upgrade tensorflow `\r\n\u5982\u679c\u662f\u5b89\u88c5\u652f\u6301 GPU \u7684 TensorFlow \u7248\u672c\uff0c\u8bf7\u8f93\u5165\u4e0b\u9762\u7684\u547d\u4ee4\uff1a\r\n`(tensorflow)C:\\> pip install --ignore-installed --upgrade tensorflow-gpu `\r\n\r\n## \u5e38\u89c1\u5b89\u88c5\u95ee\u9898\r\n\u6211\u4eec\u4f7f\u7528 Stack Overflow \u6765\u8bb0\u5f55 TensorFlow \u7684\u5b89\u88c5\u95ee\u9898\u548c\u4fee\u6b63\u65b9\u6cd5\u3002\u4e0b\u8868\u4e2d\u5305\u542b\u6709\u4e00\u4e9b\u5e38\u89c1\u5b89\u88c5\u95ee\u9898\u5728 Stack Overflow \u4e0a\u7684\u56de\u7b54\u94fe\u63a5\u3002\u5982\u679c\u60a8\u9047\u5230\u7684\u9519\u8bef\u6d88\u606f\u6216\u5b89\u88c5\u95ee\u9898\u4e0d\u5728\u4e0b\u8868\u4e2d\uff0c\u8bf7\u5728 Stack Overflow \u4e0a\u641c\u7d22\u5b83\u7684\u7b54\u6848\u3002\u5982\u679c Stack Overflow \u4e0a\u5e76\u6ca1\u6709\u663e\u793a\u8fd9\u4e2a\u9519\u8bef\u6d88\u606f\u6216\u8005\u5b89\u88c5\u95ee\u9898\u7684\u7b54\u6848\uff0c\u8bf7\u5728 Stack Overflow \u4e0a\u63d0\u4e00\u4e2a\u5173\u4e8e\u8fd9\u4e2a\u9519\u8bef\u6d88\u606f\u6216\u8005\u5b89\u88c5\u95ee\u9898\u7684\u65b0\u95ee\u9898\uff0c\u5e76\u7ed9\u8fd9\u4e2a\u95ee\u9898\u6307\u5b9a\u4e00\u4e2a `tensorflow` \u7684\u6807\u7b7e\u3002\r\n\r\n| Stack Overflow \u94fe\u63a5 | \u9519\u8bef\u6d88\u606f |\r\n| ---------- | --------------- |\r\n| [41007279](https://stackoverflow.com/questions/41007279/tensorflow-on-windows-couldnt-open-cuda-library-cudnn64-5-dll) | [...\\stream_executor\\dso_loader.cc] Couldn't open CUDA library nvcuda.dll |\r\n| [41007279](https://stackoverflow.com/questions/41007279/tensorflow-on-windows-couldnt-open-cuda-library-cudnn64-5-dll) | [...\\stream_executor\\cuda\\cuda_dnn.cc] Unable to load cuDNN DSO |\r\n| [42006320](https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso) | ImportError: Traceback (most recent call last):  File \"...\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in  <br>from google.protobuf import descriptor as _descriptor  <br>ImportError: cannot import name 'descriptor'  |\r\n| [42011070](https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso) | No module named \"pywrap_tensorflow\" |\r\n| [42217532](https://stackoverflow.com/questions/42217532/tensorflow-version-1-0-0-rc2-on-windows-opkernel-op-bestsplits-device-typ) | OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits |\r\n| [43134753](https://stackoverflow.com/questions/43134753/tensorflow-wasnt-compiled-to-use-sse-etc-instructions-but-these-are-availab) | The TensorFlow library wasn't compiled to use SSE instructions |", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I think this PR is sent to the wrong repo? @elijahxyc ", "@tensorflow-jenkins @googlebot sorry, this pull request is sent to the wrong repo, you can close it now.", "Sorry to pull it wrong."]}, {"number": 14361, "title": "tf.name_scope() is not propagated to variables created by Keras layers", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.12.6\r\n- **TensorFlow installed from (source or binary)**: BINARY\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen using Keras layers in combination with `tf.name_scope(a_name)`, `a_name` is added to the name of the layers. I would also expect it to be added to the name of the weights/variables that Keras creates internally. However, that is not the case. Having an automatic way of adding a name scope to the weights is highly desirable, though.\r\n\r\n### Source code / logs\r\n```\r\nwith tf.name_scope(\"test\"):\r\n     a = tf.keras.layers.Input(shape=(5,))\r\n     b = tf.keras.layers.Dense(5)(a)\r\n```\r\n\r\n_Output_:\r\n\r\nExecuting `a.name` returns `test/input_1:0` as expected. \r\n\r\n`tf.trainable_variables()` returns:\r\n`[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32_ref>]`", "comments": ["@martinwicke, can you take a look at this?", "How about using \u02cbwith tf.variable_scope(\"test\")`? ", "`tf.name_scope` will NOT affect the name of Variable, use `tf.variable_scope` instead.\r\n`tf.name_scope` will affect the name of Operation and thus Tensor.", "@roytseng-tw is right.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.variable_scope(\"test\"):\r\n     a = tf.keras.layers.Input(shape=(5,))\r\n     b = tf.keras.layers.Dense(5)(a)\r\n     print(tf.trainable_variables())\r\n# output\r\n# [<tf.Variable 'test/dense/kernel:0' shape=(5, 5) dtype=float32_ref>, \r\n# <tf.Variable 'test/dense/bias:0' shape=(5,) dtype=float32_ref>]\r\n```", "That is true. `tf.variable_scope` does what I was expecting from `tf.name_scope`. I guess one can combine both to have both variables and operations named. \r\nA more correct name for `tf.name_scope` would probably be `tf.operation_scope`, though. But probably not realistic to change now.", "Hi, @humcasma. In fact, `tf.variable_scope` always creates an auxiliary name scope (for convenience) when used, so it is indeed what you desired. Moreover, you are also free to combine `tf.variable_scope` with `tf.name_scope` to custom them separately.", "This appears to be resolved (and does not seem to indicate a bug), so I'm closing the issue to keep the issue tracker focused. Feel free to reopen if needed. ", "> @roytseng-tw is right.\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> \r\n> with tf.variable_scope(\"test\"):\r\n>      a = tf.keras.layers.Input(shape=(5,))\r\n>      b = tf.keras.layers.Dense(5)(a)\r\n>      print(tf.trainable_variables())\r\n> # output\r\n> # [<tf.Variable 'test/dense/kernel:0' shape=(5, 5) dtype=float32_ref>, \r\n> # <tf.Variable 'test/dense/bias:0' shape=(5,) dtype=float32_ref>]\r\n> ```\r\n\r\nWhen I do this it comes up empty?\r\n\r\n![image](https://user-images.githubusercontent.com/23745329/58649766-f5cf5d80-82da-11e9-9a74-519b32046626.png)\r\n", "@alexandervandekleut your code has nothing to do with scopes. `tf.trainable_variables` is a global collection, which isn't used by Keras. It's also deprecated. \r\n\r\nTo access the variables in layers, use their `.trainable_variables` property.", "> @alexandervandekleut your code has nothing to do with scopes. `tf.trainable_variables` is a global collection, which isn't used by Keras. It's also deprecated.\r\n> \r\n> To access the variables in layers, use their `.trainable_variables` property.\r\n\r\nThanks for the fast reply!\r\n\r\nI copy and pasted code from above in this thread, which worked for them but not for me - so something has changed between then and now. Thanks for letting me know about how that works though - all of the stackoverflow answers didn't mention that."]}, {"number": 14360, "title": "Cannot build TF 1.4 with CUDNN 1.4", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.4.5 \r\n- **Bazel version (if compiling from source)**: 0.7.0- (@non-git)\r\n- **GCC/Compiler version (if compiling from source)**: 4.9\r\n- **CUDA/cuDNN version**: 8.0/5.0.5\r\n- **GPU model and memory**: Tesla K80\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_packag\r\n\r\n### Describe the problem\r\n\r\nI'm trying to install Tensorflow 1.4 from sources with CUDA 8.0 and CUDNN 5.0.5. It's indicated in the documentation that it should work with CUDNN 3 and higher. \r\n\r\nUnfortunately, it doesn't work and ends up with an error that seems to indicate that CUDNN v6 is needed (I may be wrong on the cause of the error). \r\n\r\nHere is the error:\r\n\r\n```\r\nERROR: /home/localuser/tensorflow/tensorflow/stream_executor/BUILD:52:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1).\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'cudnnStatus_t perftools::gputools::cuda::wrap::WrapperShim__cudnnSetRNNDescriptor_v6::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...)':\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:140:30: error: '::cudnnSetRNNDescriptor_v6' has not been declared\r\n       cudnnStatus_t retval = ::__name(args...);                    \\\r\n                              ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:235:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\r\n   __macro(cudnnSetRNNDescriptor_v6)                           \\\r\n   ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:240:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R5'\r\n CUDNN_DNN_ROUTINE_EACH_R5(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\r\n```\r\n\r\nIs there anything I can do to install TF 1.4 using CUDNN 5 ?\r\n\r\nThanks", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "It turned out it was a bug in Tensorflow after all. This is fixed by applying #13255"]}, {"number": 14359, "title": "Keras model.trainable_weights does not return all trainable weights", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.12.6\r\n- **TensorFlow installed from (source or binary)**: BINARY\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen creating a Keras model where two \"towers\" are merged with help of a Lambda layer, not all trainable weights are added to `trainable_weights`.\r\n\r\n### Source code / logs\r\nThe code below creates a model with four trainable variables: 2 weights and 2 biases. However, `model.trainable_weights` returns only one weight and one bias. \r\n\r\n```\r\ndef func(x,y):\r\n    return tf.add(x,y)\r\n\r\na= tf.keras.layers.Input(shape= (64,))\r\nb = tf.keras.layers.Dense(5)(a)\r\nc = tf.keras.layers.Dense(5)(a)\r\nd = tf.keras.layers.Lambda(func, arguments={'y':c})(b)\r\nmodel = tf.keras.models.Model(a, d)\r\nmodel.trainable_weights\r\n```\r\n_Output_:\r\n`[<tf.Variable 'dense/kernel:0' shape=(64, 5) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32_ref>]\r\n`", "comments": ["@martinwicke, can you take a look at this?", "Your `Lambda` layer unfortunately breaks the model topology. So when you're defining your model, it's not taking into account the layers that came before `c`, because `c` is not being tracked as an input to your `Lambda` layer. Here's how to do it correctly in the general case:\r\n\r\n```python\r\ndef func(inputs):\r\n    return tf.add(inputs[0], inputs[1])\r\n\r\na = tf.keras.Input(shape=(64,))\r\nb = tf.keras.layers.Dense(5)(a)\r\nc = tf.keras.layers.Dense(5)(a)\r\nd = tf.keras.layers.Lambda(func)([b, c])\r\nmodel = tf.keras.models.Model(a, d)\r\nmodel.trainable_weights\r\n```\r\n\r\nBut really in your case you could do:\r\n\r\n```python\r\na = tf.keras.Input(shape=(64,))\r\nb = tf.keras.layers.Dense(5)(a)\r\nc = tf.keras.layers.Dense(5)(a)\r\nd = tf.keras.layers.add([b, c])\r\nmodel = tf.keras.models.Model(a, d)\r\nmodel.trainable_weights\r\n```"]}, {"number": 14358, "title": "Not found Op type not registered 'CountExtremelyRandomStats'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS sierra 10.12.6\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: \r\n- **CUDA/cuDNN version**:NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: \r\nbazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=rf_quora --model_base_path=/serving/rf_model &> rf_log &\r\n\r\nYou can collect some of this information using our environment capture script:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin xxxxxxxxxx 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.0.0 (clang-900.0.38)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin xxxxxxxxxx 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.8)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nabc.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n I am getting the following error while running bazel command in the docker container.\r\ncommand ran :- \r\nbazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=rf_quora --model_base_path=/serving/rf_model &> rf_log &error:- Not found: Op type not registered 'CountExtremelyRandomStats' in binary running on 864822af1c6c. Make sure the Op and Kernel are registered in the binary running in this process.\r\n\r\nI tried my search with the following link but in vain. https://github.com/tensorflow/tensorflow/issues/11847\r\n\r\nI am trying to do inference using tensorflow tensorserving, but I am getting blocked by the above error. \r\nUsing Tensorflow 1.3 and using tensor_forest api present in tf.contrib.tensor_forest_python.\r\nCan anyone help me with this error as it is blocking my testing. Note:- I have successfully ran tensorserving inference for mnist and inception examples models.\r\n-- | --\r\n\r\n### Source code / logs\r\n2017-11-08 07:27:45.038637: I tensorflow_serving/model_servers/main.cc:147] Building single TensorFlow model file config:  model_name: rf_quora model_base_path: /serving/rf_model\r\n2017-11-08 07:27:45.038846: I tensorflow_serving/model_servers/server_core.cc:441] Adding/updating models.\r\n2017-11-08 07:27:45.038879: I tensorflow_serving/model_servers/server_core.cc:492]  (Re-)adding model: rf_quora\r\n2017-11-08 07:27:45.140519: I tensorflow_serving/core/basic_manager.cc:705] Successfully reserved resources to load servable {name: rf_quora version: 1}\r\n2017-11-08 07:27:45.140575: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: rf_quora version: 1}\r\n2017-11-08 07:27:45.140608: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: rf_quora version: 1}\r\n2017-11-08 07:27:45.140647: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:360] Attempting to load native SavedModelBundle in bundle-shim from: /serving/rf_model/1\r\n2017-11-08 07:27:45.140685: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:236] Loading SavedModel from: /serving/rf_model/1\r\n2017-11-08 07:27:45.159712: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-08 07:27:45.192628: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:284] Loading SavedModel: fail. Took 51931 microseconds.\r\n2017-11-08 07:27:45.192711: E tensorflow_serving/util/retrier.cc:38] Loading servable: {name: rf_quora version: 1} failed: Not found: Op type not registered 'CountExtremelyRandomStats' in binary running on 864822af1c6c. Make sure the Op and Kernel are registered in the binary running in this process.\r\n", "comments": ["I'm not familar with rf_quora but I think CountExtremelyRandomStats op is deprecated from tensor_forest. ", "Hi ,\r\nrf_quora is a directory which contains the variabels folder and protobuf files (formed using savemodel builder).  \r\nI am not using CountExtremelyRandomStats but I am not sure whether it is being referenced or not.\r\nI have successfully ran training and inference but if I do inference using tensorserving, then I am facing the error.", "Please double check if you have referenced CountExtremelyRandomStats, which was deprecated already.", "\r\n[train_file.txt](https://github.com/tensorflow/tensorflow/files/1470326/train_file.txt)\r\nHi,\r\nYa I indeed checked it more than once.\r\nI am attaching the my training code.\r\nPlease let me know if I need to do anything or else for any code issues.\r\n\r\nRan the below command in docker container.\r\nbazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=rf_quora --model_base_path=/serving/output_new &> rf_log &\r\nError:-\r\nNot found: Op type not registered 'CountExtremelyRandomStats' in binary running on 864822af1c6c. Make sure the Op and Kernel are registered in the binary running in this process.\r\nroot@864822af1c6\r\n\r\n\r\n", "Hi @bignamehyp,\r\ndid u get time to look.", "@smsrikanthreddy Did you find something in /tensorflow/BUILD file:\r\nin `deps` segment of `cc_binary` whose `name=libtensorflow.so`, there's no deps about contrib.\r\nHowever, in /tensorflow/tools/pip_package/BUILD, there exist something about `contrib`.\r\nSo we use python, can use `tf.contrib`; but cannot use `contrib.op` when use lib", "@smsrikanthreddy You can try insert two line to /tensorflow/BUILD:\r\ncc_binary(\r\n    name = \"libtensorflow.so\",\r\n    linkopts = select({\r\n        \"//tensorflow:darwin\": [\r\n            \"-Wl,-exported_symbols_list\",  # This line must be directly followed by the exported_symbols.lds file\r\n            \"//tensorflow/c:exported_symbols.lds\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//tensorflow:windows_msvc\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-s\",\r\n            \"-Wl,--version-script\",  #  This line must be directly followed by the version_script.lds file\r\n            \"//tensorflow/c:version_script.lds\",\r\n        ],\r\n    }),\r\n    linkshared = 1,\r\n    deps = [\r\n(+)\t\"//tensorflow/contrib:contrib_kernels\",\r\n(+)\t\"//tensorflow/contrib:contrib_ops_op_lib\",\r\n        \"//tensorflow/c:c_api\",\r\n        \"//tensorflow/c:exported_symbols.lds\",\r\n        \"//tensorflow/c:version_script.lds\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n", "Hi @InsZVA ,\r\nI tried your suggestions and accordingly,  I have updated the /serving/tensorflow/tensorflow/BUILD file with the below lines,\r\ntf_cc_shared_object(\r\n    name = \"libtensorflow.so\",\r\n    linkopts = select({\r\n        \"//tensorflow:darwin\": [\r\n            \"-Wl,-exported_symbols_list\",  # This line must be directly followed by the exported_symbols.lds file\r\n            \"//tensorflow/c:exported_symbols.lds\",\r\n            \"-Wl,-install_name,@rpath/libtensorflow.so\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//tensorflow:windows_msvc\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-s\",\r\n            \"-Wl,--version-script\",  #  This line must be directly followed by the version_script.lds file\r\n            \"//tensorflow/c:version_script.lds\",\r\n        ],\r\n    }),\r\n    deps = [\r\n\t\"//tensorflow/contrib:contrib_kernels\",\r\n\t\"//tensorflow/contrib:contrib_ops_op_lib\",\r\n        \"//tensorflow/c:c_api\",\r\n        \"//tensorflow/c:exported_symbols.lds\",\r\n        \"//tensorflow/c:version_script.lds\",\r\n        \"//tensorflow/c/eager:c_api\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n\r\nI don't find the cc_binary in the BUILD file.\r\nSo updated the BUILD file with above code and even after adding the two lines I got the same error.\r\n\r\n[versions used for serving]\r\n/serving#  git branch --list -a\r\n* master\r\n  remotes/origin/HEAD -> origin/master\r\n  remotes/origin/gh-pages\r\n  remotes/origin/kirilg-patch-2\r\n  remotes/origin/master\r\n  remotes/origin/r0.5.1\r\n  remotes/origin/r0.6\r\n  remotes/origin/r1.0\r\n  remotes/origin/r1.3\r\n", "Hi, @smsrikanthreddy , Your tf version in this issue is 1.3.0, so you should switch to r1.3 branch and build libtensorflow.so", "Hi @InsZVA i I am having the same issue but on Ubuntu 16.04 and I am running it in a docker container. I did all of the following: \r\n\r\n- switched to branch r1.3 (because I am running TF v1.3.0)\r\n- Added the two additional lines you mentioned to serving/tensorflow/tensorflow/BUILD\r\n- built libtensorflow.so (using the instructions from here:    https://malmaud.github.io/TensorFlow.jl/latest/build_from_source.html)\r\n- And then I copied libtensorflow.so to my usr/bin directory in attempt to follow the directions from the link above. This supposedly installs the TF binary.\r\n- I also tried setting an environment variable called LIBTENSORFLOW to the original path of libtensorflow.so\r\n\r\nI am still having the same error. Is there something that I've done wrong?", "@kemusa I think you may try to determine whether your program really use the library you have just built and the file which contains the op `CountExtremelyRandomStats` was exactly built in that library. The reason I add 2 lines in that BUILD file is I find the file `/tensorflow/tensorflow/contrib/tensor_forest/kernels/count_extremely_random_stats_op.cc` will be built by the rule `cc_library[name=v2_ops]`(`/tensorflow/tensorflow/contrib/tensor_forest/BUILD`). And it will eventually be built by the two lines I add in `/tensorflow/tensorflow/BUILD`.", "I faced this error when I tried to use Go load a savedmodel I trained with Python. And I found the library other language(c, c++, go etc) use is built by the rule in BUILD files. And I addethat two lines in file and succeed. So I think if you face a error like \"Op type not registered XXX\", you can check whether it was built into the library you use.", "Hi @InsZVA,\r\nThanks for your reply.\r\n\r\nEven I am also facing the same issue as @kemusa ,\r\n\r\n- I switched to r1.3 and updated the BUID file with the two lines \r\n- then built libtensorflow.so and  copied the built file to /usr/local/lib file in my docker container \r\n-  then ran the bazel command to serve (the random forest code attached above) but again I got \r\n   the same error \"Op type not registered XXX\".\r\n\r\n\r\n\r\n", "@smsrikanthreddy following up on @InsZVA's point, it seems that we need to find the build rules that add CountExtremelyRandomStats. If you look at the link below we can see the rules associate with tensorForest:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/tensor_forest/BUILD\r\n\r\nI attempted to add the following lines to cc_binary[name=\"libtensorflow.so\"].\r\n        \"//tensorflow/contrib/tensor_forest:v2_ops\",\r\n        \"//tensorflow/contrib/tensor_forest:tensor_forest_v4_kernels\",\r\n        \"//tensorflow/contrib/tensor_forest:tensor_forest_v4_ops_op_lib\"\r\n\r\nAnd then copied to /usr/local/lib. But I got the same. issue. I'll try to see what I'm doing wrong over the weekend.", "@ThomasColthurst WDYT?", "@nataliaponomareva is the current maintainer of TensorForest.\r\n\r\nMy recommendation is to use TensorFlow 1.4, from which CountExtremelyRandomStats has been completely removed.", "Thanks @ThomasColthurst and @drpngx ,\r\nAfter upgrading tensorflow version from 1.3 to 1.4,  issue got resolved.", "i used v1.15.0 and met this.\r\n  \r\nOp type not registered 'FertileStatsResourceHandleOp' in binary running on 74b5282c732f.\r\n\r\n\r\n"]}]