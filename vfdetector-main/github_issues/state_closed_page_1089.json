[{"number": 20600, "title": "Tensorflow 1.8 graph destructor hanging indefinitely", "body": "I'm sorry to dump this on y'all on a weekend, but we're having strange problems with our CI when trying to upgrade to Tensorflow 1.8.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Stretch\r\n- **TensorFlow installed from (source or binary)**: Binary (CPU wheel)\r\n- **TensorFlow version (use command below)**: `v1.8.0-0-g93bc2e2072`\r\n- **Python version**: `3.6.6`\r\n- **Bazel version**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory** n/a\r\n\r\n**Exact command to reproduce**: ... it's complicated\r\n\r\n### Describe the problem\r\nWhen we run our test suite, it will intermittently hang (we've let it run overnight without any progress). We've traced this problem to TensorFlow 1.8 -- when we set `TF_C_API_GRAPH_CONSTRUCTION=0`, our test suite passes without problems and when we use Tensorflow 1.6 and 1.7, we don't seem to have any problems.\r\n\r\nUnfortunately, I have not been able to create a minified, reproducible test case (it's also frustrating because I can't replicate this locally, despite using the same Docker image that our CI build is using). I have, however, been able to use [Pyflame](https://github.com/uber/pyflame) and gdb to grab stack traces (see https://gist.github.com/alanhdu/9ce45f9061a8e48ba2b0728489cb4ee7).\r\n\r\nI think the relevant lines are:\r\n```\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/c_api_util.py:__del__:50\r\n/usr/local/lib/python3.6/inspect.py:getmodule:732\r\n/usr/local/lib/python3.6/inspect.py:findsource:780\r\n/usr/local/lib/python3.6/inspect.py:getframeinfo:1445\r\n/usr/local/lib/python3.6/inspect.py:getouterframes:1471\r\n/usr/local/lib/python3.6/inspect.py:stack:1494\r\n/usr/local/lib/python3.6/site-packages/coverage/debug.py:short_stack:143\r\n/usr/local/lib/python3.6/site-packages/coverage/collector.py:__init__:107\r\n/usr/local/lib/python3.6/site-packages/coverage/control.py:_init:266\r\n/usr/local/lib/python3.6/site-packages/coverage/control.py:load:675\r\n/usr/local/lib/python3.6/site-packages/pytest_cov/embed.py:init:67\r\n/usr/local/lib/python3.6/site-packages/pytest_cov/embed.py:multiprocessing_start:23\r\n/usr/local/lib/python3.6/multiprocessing/util.py:_run_after_forkers:132\r\n/usr/local/lib/python3.6/multiprocessing/process.py:_bootstrap:251\r\n/usr/local/lib/python3.6/multiprocessing/popen_fork.py:_launch:73\r\n/usr/local/lib/python3.6/multiprocessing/popen_fork.py:__init__:19\r\n/usr/local/lib/python3.6/multiprocessing/context.py:_Popen:277\r\n/usr/local/lib/python3.6/multiprocessing/process.py:start:105\r\n/usr/local/lib/python3.6/multiprocessing/pool.py:_repopulate_pool:239\r\n/usr/local/lib/python3.6/multiprocessing/pool.py:__init__:174\r\n/usr/local/lib/python3.6/multiprocessing/context.py:Pool:119\r\n```\r\n\r\nwhich seems to be hanging at\r\n\r\n```\r\n#0  __pthread_cond_destroy (cond=0x56312aa153d0) at pthread_cond_destroy.c:76\r\n#1  0x00007f6eef4deebb in tensorflow::thread::ThreadPool::Impl::~Impl() () from /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#2  0x00007f6eef4df89a in tensorflow::thread::ThreadPool::~ThreadPool() () from /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007f6eef87d665 in tensorflow::SingleThreadedCpuDevice::~SingleThreadedCpuDevice() ()\r\n   from /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007f6eef87d85a in tensorflow::GraphRunner::~GraphRunner() () from /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007f6ef13e3aef in TF_DeleteGraph () from /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007f6ef1038784 in _wrap_TF_DeleteGraph () from /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007f6f4a6c78e9 in _PyCFunction_FastCallDict (func_obj=0x7f6ef4de69d8, args=0x7f6e84006730, nargs=<optimized out>, kwargs=kwargs@entry=0x0)\r\nat Objects/methodobject.c:234\r\n```\r\n\r\nDo y'all happen to know what could have caused this error? Even tips on how to debug this would be super helpful.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler @tatatodd I believe I updated the issue with the relevant information. Let me know if you need any more information (or have something else you want to us to look at!).", "@tatatodd I believe we are still having this problem (even with Tensorflow 1.10). Do you mind providing us any guidance on where to to start looking for the bug?", "any update on this issue? I met a similar problem. multiprocessing + tensorflow 1.8. Hang in `DeleteGraph`\r\n\r\n![image](https://user-images.githubusercontent.com/2911134/44730227-0a47b500-ab13-11e8-9409-33b30766f80c.png)\r\n", "Problem solved. According to the comments of `tf.Graph()` constructor, it is not thread-safe. If you create one, you should delete it if it is useless before making new processes. Default graph is an exception, it is a property of current thread.\r\n\r\nSo I add another \r\n\r\n```python\r\ndel temp_graph_created\r\n```\r\n\r\n to avoid the problem.", "@alanhdu we see the PR https://github.com/hill-a/stable-baselines/pull/428 is merged  . please refer to the above comment https://github.com/tensorflow/tensorflow/issues/20600#issuecomment-416804754 and let us know if you still need any help here. If it is resolved then please feel free to move this issue to close status ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20600\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20600\">No</a>\n"]}, {"number": 20599, "title": "Typo in taskset for CPU affinity", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 20598, "title": "Cmake build with GPU fails on Windows 7", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 64-Bit\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: commit id: b2fe2a874bade4782aaca5c44bf29e7ff6c39200\r\n- **Python version**: 3.55\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: 3.6.3\r\n- **CUDA/cuDNN version**: CUDA 9.0 cuDNN 7.1\r\n- **GPU model and memory**: NVIDIA NVS 510 2GB\r\n- **Exact command to reproduce**:\r\nFirst:\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.10/swig.exe -DPYTHON_EXECUTABLE=C:/Users/{username}/Anaconda3/envs/py35/python.exe -DPYTHON_LIBRARIES=C:/Users/{username}/Anaconda3/envs/py35/libs/python35.lib -DPYTHON_INCLUDE_DIRS=C:/Users/{username}/Anaconda3/envs/py35/include -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\" -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n\r\nThen\r\nMSBuild /p:Configuration=Release /filelogger tf_python_build_pip_package.vcxproj\r\n\r\n\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThis is what is at the end of the log:\r\n\"C:\\Users\\{username}\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (1) ->\r\n\"C:\\Users\\{username}\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vcxproj\" (default target) (2) ->\r\n(CustomBuild target) -> \r\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [C:\\Users\\{username}\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vcxproj]\r\n\r\n\r\n\"C:\\Users\\{username}\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (1) ->\r\n\"C:\\Users\\{username}\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_api.vcxproj\" (default target) (264) ->\r\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [C:\\Users\\{username}\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_api.vcxproj]\r\n\r\n    27022 Warning(s)\r\n    2 Error(s)\r\n\r\nI searched in the log for the keyword \"Error:\" and came across this passage:\r\n\r\n  Traceback (most recent call last):\r\n    File \"C:/Users/{username}/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n      from tensorflow.python.util import tf_decorator\r\n    File \"C:\\Users\\{username}\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n      from tensorflow.core.framework.graph_pb2 import *\r\n    File \"C:\\Users\\{username}\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n      from google.protobuf import descriptor as _descriptor\r\n  ImportError: No module named 'google'\r\n\r\nTime Elapsed 18:43:53.58\r\n### Source code / logs\r\n[msbuild - Copy.zip](https://github.com/tensorflow/tensorflow/files/2171451/msbuild.-.Copy.zip)\r\n\r\n\r\n\r\nThank you in advance :)", "comments": ["@Ducanhvu do the \"common installation problems\" at the bottom of https://www.tensorflow.org/install/install_windows\r\n\r\nshed any light? In particular, this StackOverflow link [https://stackoverflow.com/questions/42006320/tensorflow-pip-installation-issue-cannot-import-name-descriptor] has some suggestions around import problems.", "@cy89 Thank you for your response. Unfortunately, that has not changed anything and I'm getting the same error.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20597, "title": "Proposal: Add GPU Details to TensorFlow data store", "body": "We\u2019re looking to extend the TensorFlow and TensorBoard user interface to display interesting GPU details for each node in a neural network. The GPU Details that we plan to display focus on the usage of tensor cores. Tensor cores enable deep learning applications to run significantly faster.\r\n\r\nThe UI TensorBoard proposal is here: https://github.com/tensorflow/tensorboard/issues/1271\r\n\r\nIn order to display the interesting GPU Details information in TensorBoard, we need to add the GPU Details to a data store.  One idea is for NodeDef to encapsulate a new gpu_details protobuf.  See proposed protobuf definitions below.\r\n\r\n Update existing NodeDef\r\n```\r\nmessage NodeDef {\r\n  string name = 1;\r\n  string op = 2;\r\n  repeated string input = 3;\r\n  string device = 4;\r\n  map<string, AttrValue> attr = 5;\r\n\r\n  GpuDetails gpu_details = 6;\t\t// new, optional\r\n};\r\n```\r\n\r\nCreate new GpuDetails protobuf\r\n```\r\nmessage GpuDetails {\r\n  repeated GpuKernelAttrs gpuKernelAttrs = 1;\r\n};\r\n```\r\n\r\nCreate new GpuKernelAttrs protobuf\r\n```\r\nmessage GpuKernelAttrs {\r\n  string kernel_name = 1;\r\n  int32 calls = 2;\r\n  float total_duration = 3;\r\n  float average_duration = 4;\r\n  float minimum_duration = 5;\r\n  float maximum_duration = 6; \r\n}\r\n```\r\n\r\nWe're aware that this is a change to a fundamental data structure.  Another potential solution we considered is adding a new event type in TF that captures the GPU Details information.\r\n\r\nWe're open to other suggestions you have - any guidance here would be appreciated.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hello, here are the responses to prompts:\r\n```\r\nHave I written custom code: No\r\nOS Platform and Distribution: Windows, Linux, MAC\r\nTensorFlow installed from: github\r\nTensorFlow version: 1.9\r\nBazel version: bazel release 0.13.0\r\nCUDA/cuDNN version: V9.0.176/7.0.4\r\nGPU model and memory: TITAN V \r\nExact command to reproduce: N/A\r\n```", "Thanks for reaching out @cvinson830 \r\n\r\nAt first glance, it seems that these stats are more suitable for [`NodeExecStats`](https://github.com/tensorflow/tensorflow/blob/4297b9d492327072c0d64b7243925eba192fe028/tensorflow/core/framework/step_stats.proto#L53) rather than `NodeDef`, which is what [TensorBoard uses to render memory stats](https://github.com/tensorflow/tensorboard/blob/3f9c511ef1b6fb45a3fd11ecddde7442ce1771f9/tensorboard/plugins/graph/tf_graph_common/graph.ts#L436)\r\n\r\nWe do already collect some profiling information for GPUs, see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py - which generates a chrome trace formatted timeline from the `StepStats` protocol buffer.\r\n\r\nI'd have to think more about the details here, but hopefully these pointers are useful.\r\nCould you also add some detail on the kind of information we'd collect and how?\r\nCC @zheng-xq - who has more knowledge and experience around GPU profiling and stats than I do.", "Hi @asimshankar\r\n\r\nThanks for the response.  I took a look at links you sent: step_stats.proto and timeline.py.  It might work.  Before we go down that path, I'll share with you what information we're collecting in a little more detail.\r\n\r\nAt first, we want to display in TB:\r\n1) What percentage of a node is using a tensor cores (using this ratio): \r\n\"duration of kernel(s) using tensor cores\" / \"duration of all kernels on node\".\r\n\r\n2) List of kernels on each node: calls.  For each kernel show in a table: total duration, avg duration, max duration, min duration\r\n\r\nA screen shot of TB proposal is here: https://github.com/tensorflow/tensorboard/issues/1271\r\n\r\nTo answer the question as to HOW we're collecting this information: we have access to an internal API whereby we can correlate the tensorcore/kernel information with the nodes in a neural network.  We would augment the current TB event file with this information, then update TB to read and display this information.\r\n\r\nWe're open to any suggestions you have.  Thanks,\r\n-Chris\r\n", "This sounds good to me. `NodeExecStats` is probably the right place for this data, and it shouldn't cost us anything to add additional fields.", "Hi, yes this is still in the works.  This sprint I'm going to work on putting this information in the NodeExecStats protobuf.  I'll likely submit the pull request to TF and TB this month.", "@asimshankar Hi Asim, we'd like to augment a node in a neural network with this extra gpu information.  The node is associated with the NodeDef protobuf.  Question: How is a node in a neural network associated with NodeExecStats?", "@asimshankar \r\n+ @jart \r\n\r\nA little more detail may help: \r\n\r\nThe GraphDef protobuf (graph.proto) is the topmost object in our TB event file.  When we create a TB event file and load it in TB, we see the GRAPHS tab selected, and our neural network rendered properly.\r\n\r\nGraphDef contains a repeated list of NodeDef protobufs (node_def.proto).  This was the basis for the proposal of adding gpu_details protobuf to NodeDef.\r\n\r\nYour guidance is to not use NodeDef protobuf, and use NodeExecStats instead.  I'm happy to take your advice.  However, I do not see where the NodeExecStats protobuf (step_stats.proto) gets added to the graphdef protobuf.  \r\n\r\nQuestion: What is the relationship between NodeDef and NodeExecStats?  In other words, how can I fill a gpu_details protobuf inside NodeExecStats object and relate it to each NodeDef in a GraphDef?", "@asimshankar or @jart , would either of you be able to answer @cvinson830's questions above. Currently, this issue is blocking us from using the suggested NodeExecStats.", "Sorry, somehow missed @cvinson830 's update.\r\n\r\nThe GraphDef protobuf describes the computation, it does not contain any runtime statistics.\r\nNodeExecStats is filled in when tracing is turned on in a call to `Session::Run` and includes a bunch of information about the execution of the node. NodeExecStats for each node of the graph that was executed in the `Session::Run` call is included in the StepStats proto message and the node is linked to the corresponding node in the graph by the string name of the node.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e079a8ab1eea2507018770f1e9fe5d5d2793c9c7/tensorflow/core/framework/step_stats.proto#L58\r\n\r\nFor example:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.random_uniform([2, 2])\r\ny = tf.reduce_sum(x)\r\n\r\nwith tf.Session() as sess:\r\n  metadata = tf.RunMetadata()\r\n  sess.run(y, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=metadata)\r\n  print(metadata) # This will include NodeExecStats for each node on each device\r\n```\r\n\r\nAs mentioned earlier, the node name is `NodeExecStats` is used to associate the stats with a node in the graph and render information on Tensorboard (see https://github.com/tensorflow/tensorboard/blob/3f9c511ef1b6fb45a3fd11ecddde7442ce1771f9/tensorboard/plugins/graph/tf_graph_common/graph.ts#L436)\r\n\r\nLet me know if that helps. Thanks.", "@asimshankar \r\n\r\nIn our workflow, the network file is not created via TensorFlow's  Session::Run call via python.  Rather, it is created using the protobuf methods in the GraphDef protobuf in the file \"graph.proto\" via C++.  The kernel information (ie, the gpu_details protobuf) is obtained from an external data source, and is added to the graphdef object outside of the TensorFlow framework.\r\n\r\nGiven that flow, is there an alternate way to connect StepStats to the graphdef protobuf?  Or is it ok to encapsulate the gpu_details within the node_def protobuf?  (I have the latter scenario working, fwiw).\r\n", "I may be misunderstanding something - isn't the information being added related to _execution_ of the graph? So, which `GraphDef` is being modified in the prototype you have? For example, let's say I'm a Python user that does something like this:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ny = tf.matmul([[1.]], [[2.]])\r\nwith tf.Session() as sess:\r\n  sess.run(y)\r\n```\r\nHow will I get access to the `GpuDetails` protobuf?\r\n\r\n(I'm probably missing something, but I still don't quite understand why these stats are a property of the `GraphDef`, which describes the computation to execute, not the results/statistics of the  execution)", "@asimshankar\r\n\r\nFrom 30,000' our program does these three things:\r\n1) Read in existing TensorFlow network file \r\n2) Correlate our gpuDetails business data with each node in the network\r\n3) Write out TensorBoard event file.\r\n\r\nThe user does not get access to or manipulate GpuDetails protobuf.\r\nInstead, our program is injecting our gpuDetails data into each node on an existing graph.  \r\n", "In that case, I wonder if it would be cleaner to separate the two by:\r\n\r\n1. Enhancing the `GpuKernelAttrs` message with a `node_name` that will link the stats with a precise node in the graph.\r\n\r\n2. Write it out as a separate, self-contained file.\r\n\r\n3. Write a TensorBoard plugin that can consume this file (and use the `node_name` to correlate with the `GraphDef`).\r\n\r\nThis way the plugin and the tooling is self contained. No need to change `StepStats` or `GraphDef`\r\nDoes that sound reasonable? (If not, perhaps an in-person meeting/video-conference would be more effective than going back and forth over email/comments. Though, happy to chat either way)", "I agree with the separation. It does make a cleaner flow. However, there is a stipulation that we may not have an event file at all then. Our product is creating performing the correlation and generating the complete event file that is then consumed by Tensorboard. \r\n\r\nHowever, I agree that it would be more productive if we set up a meeting/video-conference. Is there a time that works well for you this week?", "I'll reach out to you guys over email to figure out a time this week.", "Nagging Assignee @jart: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this as per offline discussion with @cvinson830  - where the general idea was that this seems like it can be done an an independent TensorBoard plugin, without needing any changes to TensorFlow.\r\n\r\nFeel free to reopen if I'm mistaken.", "Thanks @asimshankar Yes we abstracted our data into a separate protobuf, thus decoupling from existing TF protobufs."]}, {"number": 20596, "title": "Remove unused gcp and hdfs config flags, as these are on by default now.", "body": "PiperOrigin-RevId: 202753310", "comments": []}, {"number": 20595, "title": "Make protocol used in estimator customizable.", "body": "Example code as follow:\r\n\r\nconfig = tf.estimator.RunConfig(protocol='grpc+verbs')\r\nnn = tf.estimator.Estimator(model_fn=model_fn,\r\n                            model_dir=model_dir,\r\n                            params=params,\r\n                            config=config)\r\n\r\nRELNOTES: The protocol used for Estimator training is now configurable in RunConfig.", "comments": ["I added a few reviewers to get some opinions. This looks like a good change to me, but does this work end to end? Any interactions we should know about?", "Looks good to me also. Left few nits. ", "@martinwicke @jhseu  I approved. But raise red flag if you see anything wrong. ", "@lilbedwin: There's a (I believe trivial) test failure, can you fix that? ", "I think I have found the bug which causes UT fail and fixed it,  commit updated.\r\nCan you try again?\r\n\r\nThanks.", "@martinwicke, I'm trying to run the full UT in my owner docker. It seems that latest dev docker image with bazel 0.11.0 could not compile the latest master branch of tensorflow repo.\r\n\r\nHere is my cmd and error:\r\n1, I pull the latest dev docker image: docker pull registry.hub.docker.com/tensorflow/tensorflow:latest-devel\r\n2, I start a docker container by: docker run -it --name=dev registry.hub.docker.com/tensorflow/tensorflow:latest-devel bash\r\n3, git clone tensorflow; cd tensorflow; ./configure\r\n4, bazel build //tensorflow/python/estimator:run_config_test\r\n\r\nThen I found errors like this:\r\nERROR: /home/admin/lilbedwin/tensorflow/third_party/gpus/cuda_configure.bzl:117:1: file '@bazel_tools//tools/cpp:windows_cc_configure.bzl' does not contain symbol 'setup_vc_env_vars'\r\nERROR: error loading package '': Extension file 'third_party/gpus/cuda_configure.bzl' has errors\r\nERROR: error loading package '': Extension file 'third_party/gpus/cuda_configure.bzl' has errors\r\nINFO: Elapsed time: 1.838s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\nThen I try to update the bazel version to 0.14.1, then it's working know. \r\nI'm not sure if this is an issue.", "@martinwicke @kokoro-team, I have checked the remaining UT fails, which are in //tensorflow/tools/ci_build:gen_ci_sanity_out.\r\n\r\nTwo fails as follows:\r\n\r\n5. do_bazel_nobuild: bazel nobuild\r\n  FAIL\r\nerror msg:\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/contrib/lite/delegates/eager/BUILD:22:1: no such package 'testing/base/public': BUILD file not found on package path and referenced by '//tensorflow/contrib/lite/delegates/eager:util_test'\r\nERROR: Analysis of target '//tensorflow/contrib/lite/delegates/eager:util_test' failed; build aborted: no such package 'testing/base/public': BUILD file not found on package path\r\n\r\n9. do_pip_smoke_test: Pip Smoke Test: Checking py_test dependencies exist in pip package\r\n  FAIL\r\nerror msg:\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/contrib/lite/delegates/eager/BUILD:22:1: no such package 'testing/base/public': BUILD file not found on package path and referenced by '//tensorflow/contrib/lite/delegates/eager:util_test'\r\nERROR: Analysis of target '//tensorflow/contrib/lite/delegates/eager:util_test' failed; build aborted: no such package 'testing/base/public': BUILD file not found on package path\r\n\r\nIt seems that these fails have nothing to do with my commit, can you check again? \r\n\r\nThanks.", "@martinwicke \uff0canother UT fails in the latest ci build:\r\n\r\nhttps://source.cloud.google.com/results/invocations/33d0a78d-7ae6-48b2-978a-b5575e531f53/targets/%2F%2Ftensorflow%2Fpython%2Feager:function_test/log\r\n\r\nStill, it seems that it has nothing to with my commit, would you please check again?\r\n\r\nThanks.", "@josh11b @asimshankar can we do an API check now, I'd love for this to land in 1.10, assuming we are ok with it.", "(test failure is unrelated)"]}, {"number": 20594, "title": "grpc_master_service method names backward-incompatible", "body": "After grpc updated(#20513 ), the master service method names are:\r\n```\r\n//generated automatically\r\n\r\nstatic const char* MasterService_method_names[] = {\r\n  \"/tensorflow.grpc.MasterService/CreateSession\",\r\n  \"/tensorflow.grpc.MasterService/ExtendSession\",\r\n  \"/tensorflow.grpc.MasterService/PartialRunSetup\",\r\n  \"/tensorflow.grpc.MasterService/RunStep\",\r\n  \"/tensorflow.grpc.MasterService/CloseSession\",\r\n  \"/tensorflow.grpc.MasterService/ListDevices\",\r\n  \"/tensorflow.grpc.MasterService/Reset\",\r\n  \"/tensorflow.grpc.MasterService/MakeCallable\",\r\n  \"/tensorflow.grpc.MasterService/RunCallable\",\r\n  \"/tensorflow.grpc.MasterService/ReleaseCallable\",\r\n};\r\n```\r\nbut the old version is \r\n[v1.8 grpc_master_service_impl.cc](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/core/distributed_runtime/rpc/grpc_master_service_impl.cc)\r\n```\r\nstatic const char* grpcMasterService_method_names[] = {\r\n    \"/tensorflow.MasterService/CreateSession\",\r\n    \"/tensorflow.MasterService/ExtendSession\",\r\n    \"/tensorflow.MasterService/PartialRunSetup\",\r\n    \"/tensorflow.MasterService/RunStep\",\r\n    \"/tensorflow.MasterService/CloseSession\",\r\n    \"/tensorflow.MasterService/ListDevices\",\r\n    \"/tensorflow.MasterService/Reset\",\r\n    \"/tensorflow.MasterService/MakeCallable\",\r\n    \"/tensorflow.MasterService/RunCallable\",\r\n    \"/tensorflow.MasterService/ReleaseCallable\",\r\n};\r\n```\r\n\r\nSo older version (e.g. 1.8) clients cannot talk to new version server and vice versa,  as I have tested. Should this be fixed?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: yes\r\nOS Platform and Distribution: Linux fedora28\r\nTensorFlow installed from: source & bin both tested\r\nTensorFlow version: 1.8 & newest\r\nBazel version: 0.14.1\r\nCUDA/cuDNN version: 9.1\r\nGPU model and memory: 11G\r\nExact command to reproduce: use v1.8 as client, and newest version from source as server, call session.list_devices() or session.run()\r\n\r\nI believe all above information are irrelevant for this issue.\r\n", "@naurril thanks for reporting, that's indeed a problem. It's probably too late for this version to change back. I'm closing this.\r\n\r\n@gunan to keep in mind for releases."]}, {"number": 20593, "title": "Updating the version to 1.9.0 official.", "body": "", "comments": []}, {"number": 20592, "title": "TensorFlow failing on Bazel CI with Ubuntu 14.04", "body": "https://buildkite.com/bazel/tensorflow/builds/863\r\n\r\nFull log: https://buildkite.com/bazel/bazel-with-downstream-projects-bazel/builds/330#f4d9112b-3525-45b1-a2c7-09bc95905c6d\r\n\r\nError message:\r\n```\r\nexternal/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/bigtable/internal/rowreaderiterator.h:57:44: error: ambiguous overload for 'operator*' (operand type is 'std::remove_reference<const google::cloud::v0::internal::optional<google::cloud::bigtable::v0::Row>&>::type {aka const google::cloud::v0::internal::optional<google::cloud::bigtable::v0::Row>}')\r\n```\r\n\r\nBecause it's only failing on 14.04, I suspect it's caused by compiling with an old gcc version.\r\n\r\nculprit: https://github.com/tensorflow/tensorflow/commit/2e764644d6\r\n\r\nDo we care about building TF on Ubuntu 14.04? If so we might need to fix code from https://github.com/GoogleCloudPlatform/google-cloud-cpp\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "+1.  I meet the same problem in CentOS 7.  Any help?", "@jiangcore \r\ni comment out line 126,127,128 in external/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/internal/optional.h \r\nand line 57 in external/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/bigtable/internal/rowreaderiterator.h\r\nit works!", "This issue is better submitted to [Google Cloud Platform issues](https://github.com/GoogleCloudPlatform/google-cloud-cpp/issues) since it is not a TensorFlow bug or feature request.", "@tommiebupt  I am experiencing the same problem with TF 1.12 and compiling with GCC 4.9\r\nWhere are the files to edit? The only ones I could find are under the folder \"bazel-tensorflow\", but I cannot modify those without creating a problem with the cache of bazel.\r\n\r\nMoreover, there is a comment on line # 59  of `rowreaderiterator.h`, which says that line # 60 should be commented out with GCC 4.8. Should be the same with 4.9? Maybe this is the reason.\r\n\r\nThere is a way to exclude Google cloud from the beginning? Thanks a lot in advance, and sorry for so many questions."]}, {"number": 20591, "title": "Possibly outdated docs on iOS selective registration", "body": "The documentation at [`tensorflow/examples/ios/README.md #reducing-the-binary-size`](https://github.com/tensorflow/tensorflow/blob/c941c087a9dfd5b27eff00ead928c9ee208e9a35/tensorflow/examples/ios/README.md#reducing-the-binary-size) features the following (two-year-old) snippet:\r\n\r\n> After that, you can manually look at modifying the list of kernels included in `tensorflow/contrib/makefile/tf_op_files.txt` to reduce the number of implementations to the ones you're actually using in your own model. We're hoping to automate this step in the future, but for now manually removing them is the best approach.\r\n\r\nToday, selective registration uses `tensorflow/core/framework/ops_to_register.h` and commit\r\nc4ef927b5eaf144dbf1e0419c0d1d3fd968177bd introduced the `OPTIMIZE_FOR_GRAPH` option in `tensorflow/contrib/makefile/build_all_ios.sh` which automates its creation.\r\n\r\nI think the documentation (and possibly tooling) for iOS selective registration need to be updated.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This is not a runtime issue, most fields aren't relevant.\r\n\r\nHave I written custom code: no\r\nOS Platform and Distribution: ( Linux Ubuntu 16.04.2 )\r\nTensorFlow installed from: source\r\nTensorFlow version: 1.9.0-dev20180426\r\nBazel version: ( 0.15.0 )\r\nCUDA/cuDNN version: ( 9.2 / 7 )\r\nGPU model and memory: ( NVIDIA Geforce GTX 750 Ti )\r\nExact command to reproduce: N/A", "Nagging Assignee @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @MarkDaoust: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for the report. \r\n\r\n@Androbin, you seem to know a thing or two about this new setup I could approve a docs-update PR if you send one.\r\n\r\nIt may be hard to find people to help with this given that the old tf-mobile set up is getting deprecated in favor of TFLite. Ref:  164099ee4.\r\n\r\nMaybe @gargn can advise. Who's the right person to update this doc?\r\n", "Not sure. I'm guessing either @miaout17 or @petewarden might have some context. I added both of them to this bug.", "`examples/ios/README.md` links to `contrib/makefile/README.md` which has an (up-to-date) \"Optimization\" section. In the iOS \"Reducing the binary size\" section, I'd replace the offending text with a hint to that section. #21521 ", "I'm not familiar with TF Mobile selective registration. \r\nI think the readme change make sense. "]}, {"number": 20590, "title": "Profiler: Check Failed tf_start", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Archlinux\r\n- **TensorFlow installed from (source or binary)**: pip tf-nightly\r\n- **TensorFlow version (use command below)**: v1.8.0-3463-g39ea5a7044 1.10.0-dev20180620\r\n- **Python version**: 3.6.5\r\n\r\n### Describe the problem\r\nI am trying do some profiling as described here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md\r\n\r\nSession setup code\r\n```python\r\nrun_meta = tf.RunMetadata()\r\nbuilder = tf.profiler.ProfileOptionBuilder\r\n    opts = builder(builder.time_and_memory()).order_by('micros').build()\r\n    opts2 = tf.profiler.ProfileOptionBuilder.trainable_variables_parameter()\r\n    with tf.contrib.tfprof.ProfileContext('.tmp/trace',\r\n                                      trace_steps=range(0, 2),\r\n                                      dump_steps=[2]) as pctx:\r\n      pctx.add_auto_profiling('op', opts, [0, 1])\r\n      pctx.add_auto_profiling('scope', opts2, [0, 1])\r\n      self._session = tf.Session(\r\n          target='', graph=self._graph,\r\n          config=session_util.config_proto(\r\n              session_settings['config_proto'],\r\n              session_settings['gpu_options']))\r\n    self.pctx = pctx\r\n```\r\n\r\nThen at a different point in my code I do\r\n```python\r\nloss = self._session.run(fetches, ..., run_meta)\r\n```\r\n\r\nThis works for the first step, but at the second step it throws `F tensorflow/core/profiler/internal/print_model_analysis.cc:125] Check failed: tf_stat`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "So what does this error tell me to do?\r\nIt is nice that it throws an error message. But this one clearly does not indicate what went wrong.\r\nWhy do I need to consult people on stackoverflow when in the first place a better error message would help me solve this by myself?"]}, {"number": 20589, "title": "Can't restore saved model in windows 10", "body": "I have trained a model in ubuntu (16.04) machine and when I tried that model in different ubuntu (16.04), model is able to predict. But the same thing is not happening in windows(10).\r\n\r\nConfigurations in both ubuntu and windows:\r\n1. python - 3.6.5\r\n2. Tensorflow - 1.8.0\r\n\r\nWe have used Estimator api, I am seeing error logs in estimator.py only.\r\nError Log:\r\nCaused by op 'save/RestoreV2', defined at:\r\n  File \"test_model.py\", line 70, in <module>\r\n    for i in predictions:\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 507, in predict\r\n    ,hooks=all_hooks) as mon_sess:\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 816, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 539, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1002, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1007, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 696, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 458, in create_session\r\n    self._scaffold.finalize()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 212, in finalize\r\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 910, in _get_saver_or_default\r\n    saver = Saver(sharded=True, allow_empty=True)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1338, in __init__\r\n    self.build()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1347, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1384, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 829, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 525, in _AddShardedRestoreOps\r\n    name=\"restore_shard\"))\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 472, in _AddRestoreOps\r\n    restore_sequentially)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 886, in bulk_restore\r\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1546, in restore_v2\r\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nOutOfRangeError (see above for traceback): Read fewer bytes than requested\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nPlease help me.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Sure, Please find them below.\r\n\r\nHave I written custom code : **Yes**\r\nOS Platform and Distribution : **Windows 10, x64**\r\nTensorFlow installed from : **pip (pip install --upgrade tensorflow)**\r\nTensorFlow version : **1.8**\r\nBazel version : NA\r\nCUDA/cuDNN version : NA\r\nGPU model and memory : **Memory**\r\nExact command to reproduce : **model.predic**t is raising this issue ", "Nagging Assignee @asimshankar: It has been 89 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It is resolved now.\r\nI am not sure the right troubleshoot, but when I restructured my model such a way that it does occupy less space then this issue has not been observed.\r\n\r\nEarlier my model size was 6.4GB and now it got reduced to 800MB, after making this change it is working.\r\n\r\nThanks for your support.\r\n "]}, {"number": 20588, "title": "simple_estimator_example.py does not working", "body": "Hello. \r\nTried to figure out with your\r\nand try https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/simple_estimator_example.py\r\nbut it doesn't working with that errors:\r\n\r\n/usr/bin/python3.5 /___SOME_PATH____/simple_estimator_example.py\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp0qhilmok\r\n2018-07-06 17:41:20.153970: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-07-06 17:41:20.269549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-07-06 17:41:20.269961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.83GiB\r\n2018-07-06 17:41:20.269973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-07-06 17:41:20.475627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-06 17:41:20.475650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 \r\n2018-07-06 17:41:20.475655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N \r\n2018-07-06 17:41:20.475841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 7559 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-07-06 17:41:20.564665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-07-06 17:41:20.564699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-06 17:41:20.564705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 \r\n2018-07-06 17:41:20.564709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N \r\n2018-07-06 17:41:20.564810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7559 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/home/bocharick/HDD/UbuntuFiles/PycharmProjects/LanguageDetector/bin/estimator_tst.py\", line 89, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/bocharick/HDD/UbuntuFiles/PycharmProjects/LanguageDetector/bin/estimator_tst.py\", line 73, in main\r\n    estimator.train(input_fn=input_fn, steps=10)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 366, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1117, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1160, in _train_model_distributed\r\n    self.config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 794, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 269, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/lib/python3/dist-packages/six.py\", line 686, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 263, in _call_for_each_tower\r\n    self, *merge_args, **merge_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 652, in _distributed_apply\r\n    reduced_grads = distribution.batch_reduce(\"sum\", grads_and_vars)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 840, in batch_reduce\r\n    return self._batch_reduce(method_string, value_destination_pairs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 310, in _batch_reduce\r\n    value_destination_pairs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 171, in batch_reduce\r\n    raise ValueError(\"`value_destination_pairs` must be a list or a tuple of \"\r\nValueError: `value_destination_pairs` must be a list or a tuple of tuples of PerDevice objects and destinations\r\n\r\nProcess finished with exit code 1\r\n\r\nI tried two versions of tensorflow:\r\n1) 1.10.nigthly\r\n2) 1.9.rc2\r\nPython 3.5\r\n\r\nSame problems both TF versions.\r\nThank you. Help me, plz", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "**0) Have I written custom code:**\r\nYes. For test on my single GeForce GTX 1070 machine I changed one code line:\r\nBEFORE:\r\n\r\n> distribution = tf.contrib.distribute.MirroredStrategy([\"/device:GPU:0\", \"/device:GPU:1\"])\r\n\r\nAFTER:\r\n\r\n> distribution = tf.contrib.distribute.MirroredStrategy([\"/device:GPU:0\"])\r\n\r\n**1) OS Platform and Distribution:**\r\n-Linux UbuntuPC 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n**2) TensorFlow installed from:**\r\nsudo pip3.5 install ***\r\n**3) TensorFlow version:**\r\nChecked on two versions of tf:\r\n     a) 1.10.nightly\r\n     tf_nightly_gpu-1.10.0.dev20180620-cp35-cp35m-manylinux1_x86_64.whl\r\n     b) 1.9.rc2\r\n     tensorflow_gpu-1.9.0rc2-cp35-cp35m-manylinux1_x86_64.whl\r\n**4) Bazel version:**\r\n> Extracting Bazel installation...\r\n> Build label: 0.13.1\r\n> Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n> Build time: Wed May 23 11:17:23 2018 (1527074243)\r\n> Build timestamp: 1527074243\r\n> Build timestamp as int: 1527074243\r\n\r\n**5) CUDA/cuDNN version**\r\nCUDA - 9.0.176.2\r\ncuDNN - 7.1.2\r\n**6) GPU model and memory:**\r\nNvidia GeForce GTX 1070 8Gb\r\n**7) Exact command to reproduce:**\r\n\r\n> $ python3.5 simple_estimator_example.py", "I've made some tests on my multi gpu machine. and found some strange (for me) behavior.\r\n1) If i set \r\n\r\n> distribution = tf.contrib.distribute.MirroredStrategy([\"/device:GPU:0\"])\r\n\r\nOR\r\n\r\n> distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=1)\r\n\r\nIt has an error, which I had earlier.\r\n\r\n2) If i set this parameter as >=2 it works fine. **BUT!** It works on my single-gpu machine with this parameter, but it lost half of data. If I set num_gpus=4, i lost 3/4 of my data on my single-gpu machine.\r\nI've tested it on 4-gpu machine, set this parameter as 40. So it work, but 36/40 of data was never saw by model.\r\nI hope this is unexpected behavior.\r\nAnd why it's not working with num_gpus==1 (It's both not working on single-gpu machine and on multi-gpu machine)?", "Nagging Assignee @yuefengz: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I believe @yuefengz already fixed the error in this commit : https://github.com/tensorflow/tensorflow/commit/0cb022678a143bd2b41f83fff89a4d2f6d54fd3e#diff-0cdfc5bf8ae2d22bee6308579f191fd0\r\n\r\nPlease re-open if you still see the issue"]}, {"number": 20587, "title": "log_prob() for tf.distributions.Dirichlet returns values > 0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution**: Debian Stretch, 64 bits\r\n- **TensorFlow installed from**: virtualenv / pip\r\n- **TensorFlow version**: 1.9.0rc2\r\n- **Python version**: 3.5\r\n- **Bazel version**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: Geforce GTX 1080 Ti, 11172 MiB\r\n- **Exact command to reproduce**: See the description and source code below.\r\n\r\n### Describe the problem\r\nThe method log_prob() for tf.distributions.Dirichlet sometimes returns values greater than 0, which is not mathematically possible.\r\n\r\n### Source code / logs\r\nPiece of code that fails:\r\n\r\n```\r\ndist = tf.distributions.Dirichlet(\r\n    tf.exp(log_alphas),\r\n    validate_args=True,\r\n    allow_nan_stats=False,\r\n)\r\n\r\nobjective = tf.reduce_sum(dist.log_prob(target))\r\n```\r\n\r\nManual solution that works properly, using the mathematical definition for the Dirichlet:\r\n\r\n```\r\ndef dirichlet_log_prob(log_alphas, x):\r\n    return tf.reduce_sum(\r\n        tf.lgamma(tf.reduce_logsumexp(log_alphas, axis=1)) +\r\n        tf.reduce_sum(tf.multiply(tf.exp(log_alphas) - 1, tf.log(x)) - tf.lgamma(log_alphas), axis=1)\r\n    )\r\n\r\nobjective = dirichlet_log_prob(log_alphas, target)\r\n```\r\n\r\nData used in both examples:\r\n\r\n```\r\nlog_alphas = [\r\n    [-1.6231717, -2.112546 , -2.1379914, -2.0885904, -1.9052815, -1.759051 ],\r\n    [-1.4579483, -1.8557736, -1.9981235, -1.8574138, -1.5318792, -1.3888079],\r\n    [-1.4608178, -1.8787189, -1.9842598, -1.8636204, -1.6005809, -1.4765729],\r\n    [-1.4781257, -1.8943347, -2.0051253, -1.880945 , -1.613416 , -1.4802316],\r\n    [-1.447619 , -1.9153082, -1.9177424, -1.8622615, -1.7686105, -1.712473 ],\r\n    [-1.490995 , -2.0010724, -2.0137913, -1.9128757, -1.8797761, -1.8842031],\r\n    [-1.5973192, -2.0856535, -2.1033392, -2.0476089, -1.9143845, -1.8061923],\r\n    [-1.5467792, -2.0158603, -2.0372252, -1.9859502, -1.8327508, -1.7130013]\r\n]\r\ntarget = [\r\n    [2.7760255e-01, 3.2513769e-06, 3.6051267e-01, 3.2513769e-06, 8.0276497e-02, 2.8160176e-01],\r\n    [6.5851367e-01, 3.4333350e-06, 1.7245641e-01, 3.4333350e-06, 2.1149343e-02, 1.4787373e-01],\r\n    [3.1866562e-02, 2.0533266e-02, 3.3333222e-06, 2.0566598e-02, 6.1813128e-01, 3.0889896e-01],\r\n    [3.4036295e-06, 1.3675784e-01, 3.4036295e-06, 8.6322856e-01, 3.4036295e-06, 3.4036295e-06],\r\n    [1.6921267e-01, 2.1246435e-02, 7.6704454e-01, 2.1246435e-02, 2.1246435e-02, 3.4490965e-06],\r\n    [3.4373238e-06, 4.7641307e-02, 3.4373238e-06, 9.5234495e-01, 3.4373238e-06, 3.4373238e-06],\r\n    [1.3007499e-01, 4.5678858e-02, 6.3374266e-02, 7.3981225e-01, 3.4293435e-06, 2.1056170e-02],\r\n    [5.3522962e-01, 3.5806104e-06, 1.7695376e-01, 3.5806104e-06, 4.4113118e-02, 2.4369633e-01]\r\n]\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "Updated the info now; I'm using virtualenv with PyCharm. No bazel or gcc within the venv/ folder; the GCC version on the host is 6.3.0.", "log_prob() can indeed be >0 for some configurations of alpha.", "Hi!\r\n  As mentioned, note that the log_prob() > 0 for small alphas. Take for example a beta distribution. When the concentration parameters are small, the mass is very much concentrated around zero and 1, and unlikely in the middle. This causes the pdf function to be very large at small and large values, causing the log_pdf to be > 0.\r\n\r\nNote that in the implementation above, the normalizing constant is computed by using lgamma(log_alpha) instead of lgamma(alpha) which causes the discrepancy."]}, {"number": 20586, "title": "REGISTER_KERNEL_BUILDER fails type constraint check for valid integral types", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18.2 Sonya (based on Ubuntu 16.04)\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: latest master (last commit https://github.com/tensorflow/tensorflow/commit/b2fe2a874bade4782aaca5c44bf29e7ff6c39200)\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0\r\n- **CUDA/cuDNN version**: 9.2/7.1\r\n- **GPU model and memory**: GX1080Ti 11GB\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nWhen writing a custom op and registering a kernel builder using `REGISTER_KERNEL_BUILDER`, specifying a type constraint of int64_t will fail on some machines. This also fails for unsigned types too (as you would expect).\r\n\r\nThe problem occurs due to lines 381-403 in `tensorflow/core/framework/types.h`\r\n```\r\nMATCH_TYPE_AND_ENUM(float, DT_FLOAT);\r\nMATCH_TYPE_AND_ENUM(double, DT_DOUBLE);\r\nMATCH_TYPE_AND_ENUM(int32, DT_INT32);\r\nMATCH_TYPE_AND_ENUM(uint32, DT_UINT32);\r\nMATCH_TYPE_AND_ENUM(uint16, DT_UINT16);\r\nMATCH_TYPE_AND_ENUM(uint8, DT_UINT8);\r\nMATCH_TYPE_AND_ENUM(int16, DT_INT16);\r\nMATCH_TYPE_AND_ENUM(int8, DT_INT8);\r\nMATCH_TYPE_AND_ENUM(string, DT_STRING);\r\nMATCH_TYPE_AND_ENUM(complex64, DT_COMPLEX64);\r\nMATCH_TYPE_AND_ENUM(complex128, DT_COMPLEX128);\r\nMATCH_TYPE_AND_ENUM(int64, DT_INT64);\r\nMATCH_TYPE_AND_ENUM(uint64, DT_UINT64);\r\nMATCH_TYPE_AND_ENUM(bool, DT_BOOL);\r\nMATCH_TYPE_AND_ENUM(qint8, DT_QINT8);\r\nMATCH_TYPE_AND_ENUM(quint8, DT_QUINT8);\r\nMATCH_TYPE_AND_ENUM(qint16, DT_QINT16);\r\nMATCH_TYPE_AND_ENUM(quint16, DT_QUINT16);\r\nMATCH_TYPE_AND_ENUM(qint32, DT_QINT32);\r\nMATCH_TYPE_AND_ENUM(bfloat16, DT_BFLOAT16);\r\nMATCH_TYPE_AND_ENUM(Eigen::half, DT_HALF);\r\nMATCH_TYPE_AND_ENUM(ResourceHandle, DT_RESOURCE);\r\nMATCH_TYPE_AND_ENUM(Variant, DT_VARIANT);\r\n```\r\nIn this case, `int64` is being `typedef`'d to `long long` (`tensorflow/core/platform/default/integral_types.h`), however on some systems `int64_t` is `typedef`'d as `__int64` (or potentially some other variation that results in a 64-bit signed type). Because of this `int64_t != to int64` and the type constraint check fails.\r\n\r\nOn my system in particular, this code\r\n```\r\n#include <cstdint>\r\n#include <iostream>\r\n\r\nint main(void) {\r\n    std::cout << \"int64_t...............: \"\r\n              << \"(\" << typeid(int64_t).name() << \") \" << sizeof(int64_t) << std::endl;\r\n    std::cout << \"long..................: \"\r\n              << \"(\" << typeid(long).name() << \") \" << sizeof(long) << std::endl;\r\n    std::cout << \"long int..............: \"\r\n              << \"(\" << typeid(long int).name() << \") \" << sizeof(long int) << std::endl;\r\n    std::cout << \"long long int.........: \"\r\n              << \"(\" << typeid(long long int).name() << \") \" << sizeof(long long int) << std::endl;\r\n    std::cout << \"long long.............: \"\r\n              << \"(\" << typeid(long long).name() << \") \" << sizeof(long long) << std::endl;\r\n    std::cout << \"uint64_t..............: \"\r\n              << \"(\" << typeid(uint64_t).name() << \") \" << sizeof(int64_t) << std::endl;\r\n    std::cout << \"unsigned long.........: \"\r\n              << \"(\" << typeid(unsigned long).name() << \") \" << sizeof(unsigned long) << std::endl;\r\n    std::cout << \"unsigned long int.....: \"\r\n              << \"(\" << typeid(unsigned long int).name() << \") \" << sizeof(unsigned long int) << std::endl;\r\n    std::cout << \"unsigned long long int: \"\r\n              << \"(\" << typeid(unsigned long long int).name() << \") \" << sizeof(unsigned long long int) << std::endl;\r\n    std::cout << \"unsigned long long....: \"\r\n              << \"(\" << typeid(unsigned long long).name() << \") \" << sizeof(unsigned long long) << std::endl;\r\n    std::cout << \"long long == int64_t? \" << (std::is_same<long long, int64_t>::value ? \"yes\" : \"no\") << std::endl;\r\n    std::cout << \"(sizeof(long long) == 8) && std::is_signed<long long>::value? \"\r\n              << ((sizeof(long long) == 8) && std::is_signed<long long>::value ? \"yes\" : \"no\") << std::endl;\r\n    std::cout << \"(sizeof(int64_t) == 8) && std::is_signed<int64_t>::value? \"\r\n              << ((sizeof(int64_t) == 8) && std::is_signed<int64_t>::value ? \"yes\" : \"no\") << std::endl;\r\n    std::cout << \"unsigned long long == uint64_t? \"\r\n              << (std::is_same<unsigned long long, uint64_t>::value ? \"yes\" : \"no\") << std::endl;\r\n    std::cout << \"(sizeof(unsigned long long) == 8) && !std::is_signed<unsigned long long>::value? \"\r\n              << ((sizeof(unsigned long long) == 8) && !std::is_signed<unsigned long long>::value ? \"yes\" : \"no\")\r\n              << std::endl;\r\n    std::cout << \"(sizeof(uint64_t) == 8) && !std::is_signed<uint64_t>::value? \"\r\n              << ((sizeof(uint64_t) == 8) && !std::is_signed<uint64_t>::value ? \"yes\" : \"no\") << std::endl;\r\n    return 0;\r\n}\r\n\r\n```\r\nproduces this output\r\n```\r\nint64_t...............: (l) 8\r\nlong..................: (l) 8\r\nlong int..............: (l) 8\r\nlong long int.........: (x) 8\r\nlong long.............: (x) 8\r\nuint64_t..............: (m) 8\r\nunsigned long.........: (m) 8\r\nunsigned long int.....: (m) 8\r\nunsigned long long int: (y) 8\r\nunsigned long long....: (y) 8\r\nlong long == int64_t? no\r\n(sizeof(long long) == 8) && std::is_signed<long long>::value? yes\r\n(sizeof(int64_t) == 8) && std::is_signed<int64_t>::value? yes\r\nunsigned long long == uint64_t? no\r\n(sizeof(unsigned long long) == 8) && !std::is_signed<unsigned long long>::value? yes\r\n(sizeof(uint64_t) == 8) && !std::is_signed<uint64_t>::value? yes\r\n\r\n```\r\n\r\nOn a system where the current type constraint check works, the same code outputs this\r\n```\r\nint64_t...............: (x) 8\r\nlong..................: (l) 8\r\nlong int..............: (l) 8\r\nlong long int.........: (x) 8\r\nlong long.............: (x) 8\r\nuint64_t..............: (y) 8\r\nunsigned long.........: (m) 8\r\nunsigned long int.....: (m) 8\r\nunsigned long long int: (y) 8\r\nunsigned long long....: (y) 8\r\nlong long == int64_t? yes\r\n(sizeof(long long) == 8) && std::is_signed<long long>::value? yes\r\n(sizeof(int64_t) == 8) && std::is_signed<int64_t>::value? yes\r\nunsigned long long == uint64_t? yes\r\n(sizeof(unsigned long long) == 8) && !std::is_signed<unsigned long long>::value? yes\r\n(sizeof(uint64_t) == 8) && !std::is_signed<uint64_t>::value? yes\r\n```\r\n\r\nSince using an `int64_t` should be supported (as it is equivalent to `long long`), I propose that the code be modified to either use the integral types defined in `cstdint` (`int8_t`, `int16_t`, `int32_t`, `int64_t`), or the check should be changed to look for a 64-bit type that is signed using a combination of `sizeof` and `std::is_signed`\r\n\r\n### Source code / logs\r\nThe following example produces the error\r\n```\r\n#include <tensorflow/core/framework/op.h>\r\n#include <tensorflow/core/framework/op_kernel.h>\r\n\r\nREGISTER_OP(\"ExampleOp\").Attr(\"T: {int32, int64, uint32, uint64}\");\r\n\r\ntemplate <typename T>\r\nclass ExampleOp : public tensorflow::OpKernel {\r\npublic:\r\n  explicit ExampleOp(tensorflow::OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(tensorflow::OpKernelContext* context) override {}\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ExampleOp\").Device(tensorflow::DEVICE_CPU).TypeConstraint<int32_t>(\"T\"), ExampleOp<int32_t>);\r\nREGISTER_KERNEL_BUILDER(Name(\"ExampleOp\").Device(tensorflow::DEVICE_CPU).TypeConstraint<int64_t>(\"T\"), ExampleOp<int64_t>);\r\nREGISTER_KERNEL_BUILDER(Name(\"ExampleOp\").Device(tensorflow::DEVICE_CPU).TypeConstraint<uint32_t>(\"T\"), ExampleOp<uint32_t>);\r\nREGISTER_KERNEL_BUILDER(Name(\"ExampleOp\").Device(tensorflow::DEVICE_CPU).TypeConstraint<uint64_t>(\"T\"), ExampleOp<uint64_t>);\r\n```\r\n\r\nHere is the resulting compilation log\r\n```\r\n/usr/bin/c++  -Dmwe_EXPORTS -isystem /usr/local/lib/python3.5/dist-packages/tensorflow/include -I../src -march=native -mtune=native -fPIC -O3 -DNDEBUG -fPIC   -march=native -mtune=native -MD -MT CMakeFiles/mwe.dir/mwe.cpp.o -MF CMakeFiles/mwe.dir/mwe.cpp.o.d -o CMakeFiles/mwe.dir/mwe.cpp.o -c ../mwe.cpp\r\nIn file included from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:23:0,\r\n                 from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:23,\r\n                 from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:26,\r\n                 from ../mwe.cpp:2:\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/types.h: In instantiation of \u2018struct tensorflow::DataTypeToEnum<long int>\u2019:\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h:82:62:   required from \u2018tensorflow::KernelDefBuilder& tensorflow::KernelDefBuilder::TypeConstraint(const char*) [with T = long int]\u2019\r\n../mwe.cpp:16:1:   required from here\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:356:3: error: static assertion failed: Specified Data Type not supported\r\n   static_assert(IsValidDataType<T>::value, \"Specified Data Type not supported\");\r\n   ^~~~~~~~~~~~~\r\nIn file included from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:28:0,\r\n                 from ../mwe.cpp:2:\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h: In instantiation of \u2018tensorflow::KernelDefBuilder& tensorflow::KernelDefBuilder::TypeConstraint(const char*) [with T = long int]\u2019:\r\n../mwe.cpp:16:1:   required from here\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h:82:62: error: \u2018v\u2019 is not a member of \u2018tensorflow::DataTypeToEnum<long int>\u2019\r\n   return this->TypeConstraint(attr_name, DataTypeToEnum<T>::v());\r\n                                          ~~~~~~~~~~~~~~~~~~~~^~\r\nIn file included from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:23:0,\r\n                 from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:23,\r\n                 from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:26,\r\n                 from ../mwe.cpp:2:\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/types.h: In instantiation of \u2018struct tensorflow::DataTypeToEnum<long unsigned int>\u2019:\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h:82:62:   required from \u2018tensorflow::KernelDefBuilder& tensorflow::KernelDefBuilder::TypeConstraint(const char*) [with T = long unsigned int]\u2019\r\n../mwe.cpp:20:1:   required from here\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:356:3: error: static assertion failed: Specified Data Type not supported\r\n   static_assert(IsValidDataType<T>::value, \"Specified Data Type not supported\");\r\n   ^~~~~~~~~~~~~\r\nIn file included from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:28:0,\r\n                 from ../mwe.cpp:2:\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h: In instantiation of \u2018tensorflow::KernelDefBuilder& tensorflow::KernelDefBuilder::TypeConstraint(const char*) [with T = long unsigned int]\u2019:\r\n../mwe.cpp:20:1:   required from here\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h:82:62: error: \u2018v\u2019 is not a member of \u2018tensorflow::DataTypeToEnum<long unsigned int>\u2019\r\n   return this->TypeConstraint(attr_name, DataTypeToEnum<T>::v());\r\n                                          ~~~~~~~~~~~~~~~~~~~~^~\r\n```\r\n\r\nEDIT: Added a minimal working example", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "This is still an issue. A PR (#21042) has been created to address the problem.", "@alextp it looks like you've got opinions about this from the PR comment. Would you please run with this?", "Shouldn't we fix this by changing the typedefs using #ifdefs on the currently broken architectures?\r\n\r\nAlso my understanding is that calling the registration macros with our typedefs should work (that is, tensorflow::int64 instead of int64_t). If that's not the case then I think we have a bug which should be fixed in the way proposed in the PR.", "Using `#ifdef`s (or similar) to figure out when a system is \"broken\" sounds like a pretty ugly hack to me. What would you even check for in this instance? \r\n\r\nAs I explained in the PR, using `tensorflow::int64` and friends is only a viable solution if everyone knows\r\n\r\n1. that these types exist (I had no idea of their existence until I stumbled across this error and began digging through the source to find the reason), and\r\n1. that it is required that they are used to achieve the intended results\r\n\r\nIt is not obvious from any documentation that I have seen that either of these two things are the case, and I think it has been a large fluke that this issue has not arisen until now.\r\n\r\nI would also argue that it would be more precise, and just better overall, if the `typedef`s were changed to use the standard types that are defined in `cstdint` (`std::int64_t` and friends) as these types will guarantee that values have an exact width on every system that supports the type. A `long long` is guaranteed to be \"at least 64 bits\", so there may be some systems where a `long long` is larger than 64 bits. In instances where a type must be _exactly_ 64 bits using a `long long` is asking for trouble.\r\n\r\nIn either case, the PR that I have proposed will guard you against all of these issues and will allow anything that the system deems is \"a signed integral type that is 64 bits in size\" to be matched up to the `DT_INT64` enum value. To me, this seems like what should be the intended behaviour.", "@Bidski the I agree with you that we should have much better documentation about adding ops to TensorFlow, and on how to write C++ code in the tensorflow project in general. Using the right int64 type is part of it, as is using the right string type (not std::string but tensorflow::string). I also agree that we should fix the current typedefs in the tensorflow codebase to be more precise (as you pointed out, \"int64 = long long\" is ok only on some architectures and compilers, and defining it to be int64_t would be better.\r\n\r\nI'd welcome PRs which clarify our documentation or improve our typedefs.\r\n\r\nI am a little concern about the code complexity vs benefit tradeoff in your pull request. While more precise, it's also more code, and can be wrong in ways that might be difficult for future maintainers to improve. Despite currently leading to broken code, the simple conceptual model of \"1:1 mapping from these types to these enums\" is very debuggable, while the model of \"any type which matches these properties will map to these enums\" can be weird to debug as new types arise (for example, what if there's something which makes the mapping no longer 1:1?).\r\n\r\nThe desired use case for these mappings, in the kernel registration path, is for most ops to be registered via the tf-maintained macros TF_CALL_int64, TF_CALL_ALL_TYPES, etc, which are expected to be free of bugs on all platforms we support.\r\n\r\nDoes this sound reasonable to you?", "@alextp I agree with your concerns over the PR code complexity and the 1:1 mapping. The mapping is already not 1:1 with the PR code since `long long` and `int64_t` would be mapped to `DT_INT64` and we already know that some systems have an issue with `long long != int64_t` (namely mine). So while the forward mapping (type to enum) should be fine, the reverse mapping could be a problem.\r\n\r\nI am happy with a solution that improves the documentation to make it clearer that there are correct types that should be used. Not just for adding ops, but for using the C++ API in general (it is quite obvious that there are tensorflow specific types in the Python API since all types are prefixed with a `tf.`, perhaps just less reliance on `using namespace tensorflow;` in the examples would resolve this issue). Further to this, I think the `typedef`s should be changed to use the C++ standard types (defined in `cstdint`).", "@Bidski I'd happily approve a PR to change our typedefs to be standard. Want to do it?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I think we should close this issue for now. Please reopen if you still think it should be addressed.", "` auto gt_pts_trans = tensorflow::ops::Transpose(root, gt_pts, NULL);  `\r\nI used 'tensorflow::ops::Transpose()' and occurred error despite gt_pts  is  DT_FLOAT\r\n\r\nerror: static assertion failed: Specified Data Type not supported\r\n   static_assert(IsValidDataType<T>::value, \"Specified Data Type not supported\");\r\nerror: \u2018v\u2019 is not a member of \u2018tensorflow::DataTypeToEnum<long int>\u2019\r\n       Tensor t(DataTypeToEnum<RealT>::v(), TensorShape());\r\n\r\n                                        \r\n", "@maohuui this is a completely unrelated error on a completely unrelated piece of code. Please file a new issue following the template instead of spamming existing issues."]}, {"number": 20585, "title": "non-chief workers hang in training distributed seq2seq model when time-major is true", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs High Sierra version 10.13.3 (also found in Linux RHEL 6.5)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0 (also found in 1.8.0)\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A CPU version\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**:\r\npython nonchief_hang.py --out_dir=/tmp/rnn/work0 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=0 --job_name=worker --time_major\r\npython nonchief_hang.py --out_dir=/tmp/rnn/work1 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=1 --job_name=worker --time_major\r\npython nonchief_hang.py --out_dir=/tmp/rnn/ps0 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=0 --job_name=ps --time_major\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nIn a distributed seq2seq model (see a short example below), when time-major is set to true, the non-chief workers hang, further debugging showed that the non-chief workers were waiting for embedding matrices to be initialized. The chief worker trains without issues.\r\n\r\nIf we remove (\"--time_major\") from the command line, then non-chief workers run fine.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nThis is a simple seq2seq model following the implementation of [tensorflow/nmt](https://github.com/tensorflow/nmt), with optimizer removed.\r\n\r\n```python\r\nimport argparse\r\nimport sys\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\nFLAGS = None\r\n\r\n\r\ndef main(_):\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n  # Create a cluster from the parameter server and worker hosts.\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n  # Create and start a server for the local task.\r\n  server = tf.train.Server(cluster,\r\n                           job_name=FLAGS.job_name,\r\n                           task_index=FLAGS.task_index)\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n  elif FLAGS.job_name == \"worker\":\r\n\r\n    # Assigns ops to the local worker by default.\r\n    with tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n        cluster=cluster)):\r\n\r\n      # Build model...\r\n      batch_size = 24\r\n      src_len = 25\r\n      source = np.ones((batch_size, src_len), dtype=int)\r\n\r\n      embedding_encoder = tf.get_variable(\r\n          \"embedding_encoder\", (99, 128), tf.float32)\r\n\r\n      embedding_decoder = tf.get_variable(\r\n          \"embedding_decoder\", (99, 128), tf.float32)\r\n\r\n      with tf.variable_scope(\"encoder\") as decoder_scope:\r\n        encoder_emb_inp = tf.nn.embedding_lookup(\r\n           embedding_encoder, source)\r\n\r\n        encoder_cell = tf.contrib.rnn.BasicLSTMCell(\r\n          num_units=256,\r\n          forget_bias=1.0)\r\n\r\n        if FLAGS.time_major:\r\n            sequence_length=np.ones(src_len, dtype=int)*batch_size\r\n        else:\r\n            sequence_length=np.ones(batch_size, dtype=int)*src_len\r\n \r\n        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\r\n            encoder_cell,\r\n            encoder_emb_inp,\r\n            dtype=tf.float32,\r\n            sequence_length=sequence_length,\r\n            time_major=FLAGS.time_major,\r\n            swap_memory=True)\r\n\r\n      with tf.variable_scope(\"decoder\") as decoder_scope:\r\n        target_input = np.ones((batch_size, src_len), dtype=int)\r\n        decoder_emb_inp = tf.nn.embedding_lookup(\r\n            embedding_decoder, target_input) \r\n\r\n        # Helper\r\n        if FLAGS.time_major:\r\n           dummy = tf.fill([src_len], batch_size)\r\n        else:\r\n           dummy = tf.fill([batch_size], src_len)\r\n\r\n        helper = tf.contrib.seq2seq.TrainingHelper(\r\n            decoder_emb_inp, dummy,\r\n            time_major=FLAGS.time_major)\r\n\r\n        decoder_cell = tf.contrib.rnn.BasicLSTMCell(\r\n            num_units=256,\r\n            forget_bias=1.0)\r\n        decoder_initial_state = encoder_state\r\n\r\n        # Decoder\r\n        my_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n            decoder_cell,\r\n            helper,\r\n            decoder_initial_state)\r\n\r\n        # Dynamic decoding\r\n        decoder_outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\r\n            my_decoder,\r\n            output_time_major=FLAGS.time_major,\r\n            swap_memory=True,\r\n            scope=decoder_scope)\r\n\r\n      global_step = tf.train.get_or_create_global_step()\r\n      \r\n      train_op = decoder_outputs\r\n\r\n    # The StopAtStepHook handles stopping after running given steps.\r\n    hooks=[tf.train.StopAtStepHook(last_step=100)]\r\n\r\n    # The MonitoredTrainingSession takes care of session initialization,\r\n    # restoring from a checkpoint, saving to a checkpoint, and closing when done\r\n    # or an error occurs.\r\n    with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                           is_chief=(FLAGS.task_index == 0),\r\n                                           checkpoint_dir=FLAGS.out_dir,\r\n                                           hooks=hooks) as mon_sess:\r\n      # This is an infinite while loop since global_step does not increment at all.\r\n      # In a real training, global_step is passed to an optimizer, then increments\r\n      # after each step.\r\n      while not mon_sess.should_stop():\r\n        x = mon_sess.run(train_op)\r\n        global_step_val = global_step.eval(session=mon_sess)\r\n        print('global_step = {0}, time_major is {1}'.format(global_step_val, FLAGS.time_major))\r\n\r\nif __name__ == \"__main__\":\r\n  parser = argparse.ArgumentParser()\r\n  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\r\n  # Flags for defining the tf.train.ClusterSpec\r\n  parser.add_argument(\r\n      \"--ps_hosts\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"Comma-separated list of hostname:port pairs\"\r\n  )\r\n  parser.add_argument(\r\n      \"--worker_hosts\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"Comma-separated list of hostname:port pairs\"\r\n  )\r\n  parser.add_argument(\r\n      \"--job_name\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"One of 'ps', 'worker'\"\r\n  )\r\n  # Flags for defining the tf.train.Server\r\n  parser.add_argument(\r\n      \"--task_index\",\r\n      type=int,\r\n      default=0,\r\n      help=\"Index of task within the job\"\r\n  )\r\n  parser.add_argument(\r\n      \"--out_dir\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"output dir\"\r\n  )\r\n\r\n  parser.add_argument(\r\n      \"--time_major\",\r\n      action='store_true',\r\n      help=\"time major\"\r\n  )\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```\r\n", "comments": ["@lukaszkaiser would you assign to someone who can look into this if you don't have time?", "@ebrevdo : I'm really not sure what the problem is here and who can debug it properly, can you find a person who'd know? Thanks!", "I think there's an error you're not seeing in the logs.\r\n\r\n  if FLAGS.time_major:\r\n            sequence_length=np.ones(src_len, dtype=int)*batch_size\r\n        else:\r\n            sequence_length=np.ones(batch_size, dtype=int)*src_len\r\n\r\nThis is problematic. You should always have a vector of size batch_size, regardless of time_major or not.  In your case you may even be able to just skip passing in this argument.", "The reason I think there's an error is that passing the wrong size sequence_length should cause either a ValueError or tf runtime error to be raised.", "@ebrevdo Thanks for looking into issue. I really appreciate it.\r\nWhat you said was correct, sequence_length should always be 1-D vector of batch_size length. \"time_major\" dictates the dimension of the input tensor, whether time should be on the first or second axis. I have fixed the code. It works with both case. For future reference, the modified code is pasted below.\r\n\r\n```python\r\nimport argparse\r\nimport sys\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\nFLAGS = None\r\n\r\n\r\ndef main(_):\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n  # Create a cluster from the parameter server and worker hosts.\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n  # Create and start a server for the local task.\r\n  server = tf.train.Server(cluster,\r\n                           job_name=FLAGS.job_name,\r\n                           task_index=FLAGS.task_index)\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n  elif FLAGS.job_name == \"worker\":\r\n\r\n    # Assigns ops to the local worker by default.\r\n    with tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n        cluster=cluster)):\r\n\r\n      # Build model...\r\n\r\n      batch_size = 24\r\n      src_len = 25\r\n      source = np.ones((batch_size, src_len), dtype=int)\r\n\r\n      embedding_encoder = tf.get_variable(\r\n          \"embedding_encoder\", (99, 128), tf.float32)\r\n\r\n      embedding_decoder = tf.get_variable(\r\n          \"embedding_decoder\", (99, 128), tf.float32)\r\n\r\n      with tf.variable_scope(\"encoder\") as decoder_scope:\r\n        encoder_emb_inp = tf.nn.embedding_lookup(\r\n           embedding_encoder, source)\r\n\r\n        encoder_cell = tf.contrib.rnn.BasicLSTMCell(\r\n          num_units=256,\r\n          forget_bias=1.0)\r\n        sequence_length=tf.fill([batch_size], src_len)\r\n\r\n        if FLAGS.time_major:\r\n            # swap (batch, time) axes if time_major\r\n            encoder_emb_inp=tf.transpose(encoder_emb_inp, [1, 0, 2])\r\n\r\n        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\r\n            encoder_cell,\r\n            encoder_emb_inp,\r\n            dtype=tf.float32,\r\n            sequence_length=sequence_length,\r\n            time_major=FLAGS.time_major,\r\n            swap_memory=True)\r\n\r\n      with tf.variable_scope(\"decoder\") as decoder_scope:\r\n        target_input = np.ones((batch_size, src_len), dtype=int)\r\n        decoder_emb_inp = tf.nn.embedding_lookup(\r\n            embedding_decoder, target_input) \r\n\r\n        # Helper\r\n        if FLAGS.time_major:\r\n           # swap (batch, time) axes if time_major\r\n           decoder_emb_inp = tf.transpose(decoder_emb_inp, [1, 0, 2])\r\n\r\n        helper = tf.contrib.seq2seq.TrainingHelper(\r\n            decoder_emb_inp, sequence_length,\r\n            time_major=FLAGS.time_major)\r\n\r\n        decoder_cell = tf.contrib.rnn.BasicLSTMCell(\r\n            num_units=256,\r\n            forget_bias=1.0)\r\n        decoder_initial_state = encoder_state\r\n\r\n        # Decoder\r\n        my_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n            decoder_cell,\r\n            helper,\r\n            decoder_initial_state)\r\n\r\n        # Dynamic decoding\r\n        decoder_outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\r\n            my_decoder,\r\n            output_time_major=FLAGS.time_major,\r\n            swap_memory=True,\r\n            scope=decoder_scope)\r\n\r\n      global_step = tf.train.get_or_create_global_step()\r\n      \r\n      train_op = decoder_outputs\r\n\r\n    # The StopAtStepHook handles stopping after running given steps.\r\n    hooks=[tf.train.StopAtStepHook(last_step=100)]\r\n\r\n    # The MonitoredTrainingSession takes care of session initialization,\r\n    # restoring from a checkpoint, saving to a checkpoint, and closing when done\r\n    # or an error occurs.\r\n    with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                           is_chief=(FLAGS.task_index == 0),\r\n                                           checkpoint_dir=FLAGS.out_dir,\r\n                                           hooks=hooks) as mon_sess:\r\n      # This is an infinite while loop since global_step does not increment at all.\r\n      # In a real training, global_step is passed to an optimizer, then increments\r\n      # after each step.\r\n      while not mon_sess.should_stop():\r\n        x = mon_sess.run(train_op)\r\n        global_step_val = global_step.eval(session=mon_sess)\r\n        print('global_step = {0}, time_major is {1}'.format(global_step_val, FLAGS.time_major))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  parser = argparse.ArgumentParser()\r\n  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\r\n  # Flags for defining the tf.train.ClusterSpec\r\n  parser.add_argument(\r\n      \"--ps_hosts\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"Comma-separated list of hostname:port pairs\"\r\n  )\r\n  parser.add_argument(\r\n      \"--worker_hosts\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"Comma-separated list of hostname:port pairs\"\r\n  )\r\n  parser.add_argument(\r\n      \"--job_name\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"One of 'ps', 'worker'\"\r\n  )\r\n  # Flags for defining the tf.train.Server\r\n  parser.add_argument(\r\n      \"--task_index\",\r\n      type=int,\r\n      default=0,\r\n      help=\"Index of task within the job\"\r\n  )\r\n  parser.add_argument(\r\n      \"--out_dir\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"output dir\"\r\n  )\r\n\r\n  parser.add_argument(\r\n      \"--time_major\",\r\n      action='store_true',\r\n      help=\"time major\"\r\n  )\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```", "@ebrevdo I just realized there could still be a bug in tensorflow. \r\nIndeed, the code above works perfectly. Note I used the following \"lookup-then-transpose\" to handle time_major:\r\n```python\r\n        encoder_emb_inp = tf.nn.embedding_lookup(\r\n           embedding_encoder, source)\r\n        if FLAGS.time_major:\r\n            # swap (batch, time) axes if time_major\r\n            encoder_emb_inp=tf.transpose(encoder_emb_inp, [1, 0, 2])\r\n```\r\nAnother approach could be \"transpose-then-lookup\":\r\n```python\r\n      source = np.ones((batch_size, src_len), dtype=int)\r\n      if FLAGS.time_major:\r\n          source = tf.transpose(source)\r\n      ...\r\n      with tf.variable_scope(\"encoder\") as decoder_scope:\r\n        encoder_emb_inp = tf.nn.embedding_lookup(\r\n           embedding_encoder, source)\r\n```\r\nthe latter causes non-chief worker to hang. There is no error message, the worker just keeps starting master sessions. I found the latter hung because it is the approach taken by tensorflow/nmt, which you co-authored (thanks for the great contribution, I benefit enormously from the nmt code). When I try to extended nmt to distributed setting, I found the issue.\r\n\r\nDo you know why \"transpose-then-lookup\" causes the non-chief worker to hang? I attached the \"transpose-then-lookup\" code for your reference.\r\n\r\n```python\r\nimport argparse\r\nimport sys\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\nFLAGS = None\r\n\r\ndef main(_):\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n  # Create a cluster from the parameter server and worker hosts.\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n  # Create and start a server for the local task.\r\n  server = tf.train.Server(cluster,\r\n                           job_name=FLAGS.job_name,\r\n                           task_index=FLAGS.task_index)\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n  elif FLAGS.job_name == \"worker\":\r\n\r\n    # Assigns ops to the local worker by default.\r\n    with tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n        cluster=cluster)):\r\n\r\n      # Build model...\r\n\r\n      batch_size = 24\r\n      src_len = 25\r\n      source = np.ones((batch_size, src_len), dtype=int)\r\n\r\n      if FLAGS.time_major:\r\n          source = tf.transpose(source)\r\n\r\n      embedding_encoder = tf.get_variable(\r\n          \"embedding_encoder\", (99, 128), tf.float32)\r\n\r\n      embedding_decoder = tf.get_variable(\r\n          \"embedding_decoder\", (99, 128), tf.float32)\r\n\r\n      with tf.variable_scope(\"encoder\") as decoder_scope:\r\n        encoder_emb_inp = tf.nn.embedding_lookup(\r\n           embedding_encoder, source)\r\n\r\n        encoder_cell = tf.contrib.rnn.BasicLSTMCell(\r\n          num_units=256,\r\n          forget_bias=1.0)\r\n        sequence_length=tf.fill([batch_size], src_len)\r\n\r\n        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\r\n            encoder_cell,\r\n            encoder_emb_inp,\r\n            dtype=tf.float32,\r\n            sequence_length=sequence_length,\r\n            time_major=FLAGS.time_major,\r\n            swap_memory=True)\r\n\r\n      with tf.variable_scope(\"decoder\") as decoder_scope:\r\n        target_input = np.ones((batch_size, src_len), dtype=int)\r\n\r\n        if FLAGS.time_major:\r\n            target_input = tf.transpose(target_input)\r\n\r\n        decoder_emb_inp = tf.nn.embedding_lookup(\r\n            embedding_decoder, target_input) \r\n\r\n        # Helper\r\n        helper = tf.contrib.seq2seq.TrainingHelper(\r\n            decoder_emb_inp, sequence_length,\r\n            time_major=FLAGS.time_major)\r\n\r\n        decoder_cell = tf.contrib.rnn.BasicLSTMCell(\r\n            num_units=256,\r\n            forget_bias=1.0)\r\n        decoder_initial_state = encoder_state\r\n\r\n        # Decoder\r\n        my_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n            decoder_cell,\r\n            helper,\r\n            decoder_initial_state)\r\n\r\n        # Dynamic decoding\r\n        decoder_outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\r\n            my_decoder,\r\n            output_time_major=FLAGS.time_major,\r\n            swap_memory=True,\r\n            scope=decoder_scope)\r\n\r\n      global_step = tf.train.get_or_create_global_step()\r\n      \r\n      train_op = decoder_outputs\r\n\r\n    # The StopAtStepHook handles stopping after running given steps.\r\n    hooks=[tf.train.StopAtStepHook(last_step=100)]\r\n\r\n    # The MonitoredTrainingSession takes care of session initialization,\r\n    # restoring from a checkpoint, saving to a checkpoint, and closing when done\r\n    # or an error occurs.\r\n    with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                           is_chief=(FLAGS.task_index == 0),\r\n                                           checkpoint_dir=FLAGS.out_dir,\r\n                                           hooks=hooks) as mon_sess:\r\n      # This is an infinite while loop since global_step does not increment at all.\r\n      # In a real training, global_step is passed to an optimizer, then increments\r\n      # after each step.\r\n      while not mon_sess.should_stop():\r\n        x = mon_sess.run(train_op)\r\n        global_step_val = global_step.eval(session=mon_sess)\r\n        print('global_step = {0}, time_major is {1}'.format(global_step_val, FLAGS.time_major))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  parser = argparse.ArgumentParser()\r\n  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\r\n  # Flags for defining the tf.train.ClusterSpec\r\n  parser.add_argument(\r\n      \"--ps_hosts\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"Comma-separated list of hostname:port pairs\"\r\n  )\r\n  parser.add_argument(\r\n      \"--worker_hosts\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"Comma-separated list of hostname:port pairs\"\r\n  )\r\n  parser.add_argument(\r\n      \"--job_name\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"One of 'ps', 'worker'\"\r\n  )\r\n  # Flags for defining the tf.train.Server\r\n  parser.add_argument(\r\n      \"--task_index\",\r\n      type=int,\r\n      default=0,\r\n      help=\"Index of task within the job\"\r\n  )\r\n  parser.add_argument(\r\n      \"--out_dir\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"output dir\"\r\n  )\r\n\r\n  parser.add_argument(\r\n      \"--time_major\",\r\n      action='store_true',\r\n      help=\"time major\"\r\n  )\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```", "It still looks like you've got an error in either creating or running the\ngraph that you're just not catching. See if you're accidentally piping or\nthrowing out your stderr, or maybe it's in another log file?\n\nOn Sat, Aug 4, 2018, 5:56 PM Jun Shi <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> I just realized there could still\n> be a bug in tensorflow.\n> Indeed, the code above works perfectly. Note I used the following\n> \"lookup-then-transpose\" to handle time_major:\n>\n>         encoder_emb_inp = tf.nn.embedding_lookup(\n>            embedding_encoder, source)\n>         if FLAGS.time_major:\n>             # swap (batch, time) axes if time_major\n>             encoder_emb_inp=tf.transpose(encoder_emb_inp, [1, 0, 2])\n>\n> Another approach could be \"transpose-then-lookup\":\n>\n>       source = np.ones((batch_size, src_len), dtype=int)\n>       if FLAGS.time_major:\n>           source = tf.transpose(source)\n>       ...\n>       with tf.variable_scope(\"encoder\") as decoder_scope:\n>         encoder_emb_inp = tf.nn.embedding_lookup(\n>            embedding_encoder, source)\n>\n> the latter causes non-chief worker to hang. There is no error message, the\n> worker just keeps starting master sessions. I found the latter hung because\n> it is the approach taken by tensorflow/nmt, which you co-authored (thanks\n> for the great contribution, I benefit enormously from the nmt code). When I\n> try to extended nmt to distributed setting, I found the issue.\n>\n> Do you know why \"transpose-then-lookup\" causes the non-chief worker to\n> hang? I attached the \"transpose-then-lookup\" code for your reference.\n>\n> import argparseimport sys\n> import tensorflow as tfimport numpy as npimport timeFLAGS = None\n> def main(_):\n>   ps_hosts = FLAGS.ps_hosts.split(\",\")\n>   worker_hosts = FLAGS.worker_hosts.split(\",\")\n>\n>   # Create a cluster from the parameter server and worker hosts.\n>   cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n>\n>   # Create and start a server for the local task.\n>   server = tf.train.Server(cluster,\n>                            job_name=FLAGS.job_name,\n>                            task_index=FLAGS.task_index)\n>   if FLAGS.job_name == \"ps\":\n>     server.join()\n>   elif FLAGS.job_name == \"worker\":\n>\n>     # Assigns ops to the local worker by default.\n>     with tf.device(tf.train.replica_device_setter(\n>         worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n>         cluster=cluster)):\n>\n>       # Build model...\n>\n>       batch_size = 24\n>       src_len = 25\n>       source = np.ones((batch_size, src_len), dtype=int)\n>\n>       if FLAGS.time_major:\n>           source = tf.transpose(source)\n>\n>       embedding_encoder = tf.get_variable(\n>           \"embedding_encoder\", (99, 128), tf.float32)\n>\n>       embedding_decoder = tf.get_variable(\n>           \"embedding_decoder\", (99, 128), tf.float32)\n>\n>       with tf.variable_scope(\"encoder\") as decoder_scope:\n>         encoder_emb_inp = tf.nn.embedding_lookup(\n>            embedding_encoder, source)\n>\n>         encoder_cell = tf.contrib.rnn.BasicLSTMCell(\n>           num_units=256,\n>           forget_bias=1.0)\n>         sequence_length=tf.fill([batch_size], src_len)\n>\n>         encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n>             encoder_cell,\n>             encoder_emb_inp,\n>             dtype=tf.float32,\n>             sequence_length=sequence_length,\n>             time_major=FLAGS.time_major,\n>             swap_memory=True)\n>\n>       with tf.variable_scope(\"decoder\") as decoder_scope:\n>         target_input = np.ones((batch_size, src_len), dtype=int)\n>\n>         if FLAGS.time_major:\n>             target_input = tf.transpose(target_input)\n>\n>         decoder_emb_inp = tf.nn.embedding_lookup(\n>             embedding_decoder, target_input)\n>\n>         # Helper\n>         helper = tf.contrib.seq2seq.TrainingHelper(\n>             decoder_emb_inp, sequence_length,\n>             time_major=FLAGS.time_major)\n>\n>         decoder_cell = tf.contrib.rnn.BasicLSTMCell(\n>             num_units=256,\n>             forget_bias=1.0)\n>         decoder_initial_state = encoder_state\n>\n>         # Decoder\n>         my_decoder = tf.contrib.seq2seq.BasicDecoder(\n>             decoder_cell,\n>             helper,\n>             decoder_initial_state)\n>\n>         # Dynamic decoding\n>         decoder_outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n>             my_decoder,\n>             output_time_major=FLAGS.time_major,\n>             swap_memory=True,\n>             scope=decoder_scope)\n>\n>       global_step = tf.train.get_or_create_global_step()\n>\n>       train_op = decoder_outputs\n>\n>     # The StopAtStepHook handles stopping after running given steps.\n>     hooks=[tf.train.StopAtStepHook(last_step=100)]\n>\n>     # The MonitoredTrainingSession takes care of session initialization,\n>     # restoring from a checkpoint, saving to a checkpoint, and closing when done\n>     # or an error occurs.\n>     with tf.train.MonitoredTrainingSession(master=server.target,\n>                                            is_chief=(FLAGS.task_index == 0),\n>                                            checkpoint_dir=FLAGS.out_dir,\n>                                            hooks=hooks) as mon_sess:\n>       # This is an infinite while loop since global_step does not increment at all.\n>       # In a real training, global_step is passed to an optimizer, then increments\n>       # after each step.\n>       while not mon_sess.should_stop():\n>         x = mon_sess.run(train_op)\n>         global_step_val = global_step.eval(session=mon_sess)\n>         print('global_step = {0}, time_major is {1}'.format(global_step_val, FLAGS.time_major))\n>\n> if __name__ == \"__main__\":\n>   parser = argparse.ArgumentParser()\n>   parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n>   # Flags for defining the tf.train.ClusterSpec\n>   parser.add_argument(\n>       \"--ps_hosts\",\n>       type=str,\n>       default=\"\",\n>       help=\"Comma-separated list of hostname:port pairs\"\n>   )\n>   parser.add_argument(\n>       \"--worker_hosts\",\n>       type=str,\n>       default=\"\",\n>       help=\"Comma-separated list of hostname:port pairs\"\n>   )\n>   parser.add_argument(\n>       \"--job_name\",\n>       type=str,\n>       default=\"\",\n>       help=\"One of 'ps', 'worker'\"\n>   )\n>   # Flags for defining the tf.train.Server\n>   parser.add_argument(\n>       \"--task_index\",\n>       type=int,\n>       default=0,\n>       help=\"Index of task within the job\"\n>   )\n>   parser.add_argument(\n>       \"--out_dir\",\n>       type=str,\n>       default=\"\",\n>       help=\"output dir\"\n>   )\n>\n>   parser.add_argument(\n>       \"--time_major\",\n>       action='store_true',\n>       help=\"time major\"\n>   )\n>   FLAGS, unparsed = parser.parse_known_args()\n>   tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20585#issuecomment-410487365>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4SaQE8gkdYzMFe64GB_AqTqlNqMks5uNkLTgaJpZM4VE4Qr>\n> .\n>\n", "For \"transpose-then-lookup\" code, the chief worker ran fine, but non-chief worker hung. After I set `tf.logging.set_verbosity(tf.logging.DEBUG)`, I saw the following on the non-chief worker. It seems embedding_encoder and embedding_decoder were not ready on the non-chief worker. No other error messages were seen.\r\n\r\n```\r\njunshi15$ python nonchief_hang_v3.py --time_major --out_dir=/tmp/rnn/work1 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=1 --job_name=worker\r\n\r\n2018-08-06 07:27:35.223313: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-08-06 07:27:35.224656: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n2018-08-06 07:27:35.224674: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:1111, 1 -> localhost:3333}\r\n2018-08-06 07:27:35.225027: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:3333\r\nINFO:tensorflow:Graph was finalized.\r\n2018-08-06 07:27:42.742037: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 6f967f601337d97c with config:\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, encoder/rnn/basic_lstm_cell/kernel, encoder/rnn/basic_lstm_cell/bias, decoder/basic_lstm_cell/kernel, decoder/basic_lstm_cell/bias, global_step, ready: None\r\n2018-08-06 07:28:12.781871: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 3139c55b7e2bdbef with config:\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None\r\n2018-08-06 07:28:42.816610: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 7d5684fbb32ca03b with config:\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None\r\n2018-08-06 07:29:12.848483: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 34122298c96817e7 with config:\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None\r\n2018-08-06 07:29:42.877087: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 0e9164b2458e92cf with config:\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None\r\n2018-08-06 07:30:12.905306: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session fb3c939889939c88 with config: \r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None\r\n2018-08-06 07:30:42.936056: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session c3e8a41826633f8a with config:\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None\r\n2018-08-06 07:31:12.966590: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 284b8fd3e8c8772e with config:\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None\r\n```", "Do you have logs for the non chief worker?\n\nOn Mon, Aug 6, 2018, 7:41 AM Jun Shi <notifications@github.com> wrote:\n\n> For \"transpose-then-lookup\" code, the chief worker ran fine, but non-chief\n> worker hung. After I set tf.logging.set_verbosity(tf.logging.DEBUG), I\n> saw the following on the non-chief worker. It seems embedding_encoder and\n> embedding_decoder were not ready on the non-chief worker. No other error\n> messages were seen.\n>\n> junshi15$ python nonchief_hang_v3.py --time_major --out_dir=/tmp/rnn/work1 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=1 --job_name=worker\n> 2018-08-06 07:27:35.223313: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2018-08-06 07:27:35.224656: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}2018-08-06 07:27:35.224674: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:1111, 1 -> localhost:3333}2018-08-06 07:27:35.225027: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:3333INFO:tensorflow:Graph was finalized.2018-08-06 07:27:42.742037: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 6f967f601337d97c with config:INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, encoder/rnn/basic_lstm_cell/kernel, encoder/rnn/basic_lstm_cell/bias, decoder/basic_lstm_cell/kernel, decoder/basic_lstm_cell/bias, global_step, ready: None2018-08-06 07:28:12.781871: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 3139c55b7e2bdbef with config:INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None2018-08-06 07:28:42.816610: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 7d5684fbb32ca03b with config:INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None2018-08-06 07:29:12.848483: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 34122298c96817e7 with config:INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None2018-08-06 07:29:42.877087: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 0e9164b2458e92cf with config:INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None2018-08-06 07:30:12.905306: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session fb3c939889939c88 with config: INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None2018-08-06 07:30:42.936056: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session c3e8a41826633f8a with config:INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None2018-08-06 07:31:12.966590: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 284b8fd3e8c8772e with config:INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: embedding_encoder, embedding_decoder, ready: None\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20585#issuecomment-410731343>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1fMHemEUWqVHVn1oAjak13WZPUQks5uOFWygaJpZM4VE4Qr>\n> .\n>\n", "Yes, the logs for non-chief worker were posted above. I was wondering if you meant chief worker. I posted the logs for the chief worker below:\r\n```\r\njunshi15$ python nonchief_hang_v3.py --time_major  --out_dir=/tmp/rnn/work0 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=0 --job_name=worker\r\n2018-08-06 09:31:55.765383: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-08-06 09:31:55.766691: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n2018-08-06 09:31:55.766719: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:1111, 1 -> localhost:3333}\r\n2018-08-06 09:31:55.767065: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:1111\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-08-06 09:31:57.797718: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 6e427cebcbefcb54 with config:\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/rnn/work0/model.ckpt.\r\nglobal_step = 0, time_major is True\r\nglobal_step = 0, time_major is True\r\n```", "@junshi15 \r\nWe see that you are using old version of tensorflow  1.x which is not actively supported, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .To migrate your code please see [this](https://www.tensorflow.org/guide/migrate#1_replace_v1sessionrun_calls) guide . Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20585\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20585\">No</a>\n"]}, {"number": 20584, "title": "Can't Import Tensor Flow in Annaconda Python 3.6", "body": "I just installed CUDA 9.2 and CUDANN and Tensor Flow on Windows 10.  I'm using Anaconda 3.6 Python.\r\n\r\nHere is a trace that I get when I try to import tensor flow.  It's telling me it can't load a dll but its is not telling me which one.  Can you help.  \r\n\r\nHere is a directory listing of the CUDA dlls in my path and the trace.  \r\n\r\n********************  Directory Listing ************************\r\n\r\nPS C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\bin> dir\r\n\r\n\r\n    Directory: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\bin\r\n\r\n\r\nMode                LastWriteTime         Length Name\r\n----                -------------         ------ ----\r\nd-----         7/5/2018   8:32 PM                crt\r\n-a----        4/12/2018   1:20 AM         202752 bin2c.exe\r\n-a----        4/12/2018   1:21 AM       54600704 cublas64_92.dll\r\n-a----        4/12/2018   1:20 AM         370176 cuda-memcheck.exe\r\n-a----        4/12/2018   1:20 AM        4092416 cudafe++.exe\r\n-a----        4/12/2018   1:21 AM         299008 cudart32_92.dll\r\n-a----        4/12/2018   1:21 AM         368128 cudart64_90.dll\r\n-a----        4/12/2018   1:21 AM         368128 cudart64_92.dll\r\n-a----         7/5/2018  10:17 PM      336443392 cudnn64_7.dll\r\n-a----        4/12/2018   1:21 AM       87017472 cufft64_92.dll\r\n-a----        4/12/2018   1:21 AM         197632 cufftw64_92.dll\r\n-a----        4/12/2018   1:21 AM        3692032 cuinj32_92.dll\r\n-a----        4/12/2018   1:21 AM        4625408 cuinj64_92.dll\r\n-a----        4/12/2018   1:20 AM        1695744 cuobjdump.exe\r\n-a----        4/12/2018   1:21 AM       47990784 curand64_92.dll\r\n-a----        4/12/2018   1:21 AM      114172416 cusolver64_92.dll\r\n-a----        4/12/2018   1:21 AM       65776128 cusparse64_92.dll\r\n-a----        4/12/2018   1:20 AM         284672 fatbinary.exe\r\n-a----        4/12/2018   1:20 AM        1306112 gpu-library-advisor.exe\r\n-a----        4/12/2018   1:21 AM         203264 nppc64_92.dll\r\n-a----        4/12/2018   1:21 AM       10174464 nppial64_92.dll\r\n-a----        4/12/2018   1:21 AM        3985920 nppicc64_92.dll\r\n-a----        4/12/2018   1:21 AM        1010688 nppicom64_92.dll\r\n-a----        4/12/2018   1:21 AM        6928896 nppidei64_92.dll\r\n-a----        4/12/2018   1:21 AM       51902976 nppif64_92.dll\r\n-a----        4/12/2018   1:21 AM       25260032 nppig64_92.dll\r\n-a----        4/12/2018   1:21 AM        6574592 nppim64_92.dll\r\n-a----        4/12/2018   1:21 AM       15030272 nppist64_92.dll\r\n-a----        4/12/2018   1:21 AM         177152 nppisu64_92.dll\r\n-a----        4/12/2018   1:21 AM        2621440 nppitc64_92.dll\r\n-a----        4/12/2018   1:21 AM        8543232 npps64_92.dll\r\n-a----        4/12/2018   1:20 AM         241152 nvblas64_92.dll\r\n-a----        4/12/2018   1:20 AM         379904 nvcc.exe\r\n-a----        4/12/2018   1:20 AM            310 nvcc.profile\r\n-a----        4/12/2018   1:20 AM       18156032 nvdisasm.exe\r\n-a----        4/12/2018   1:21 AM       66096128 nvgraph64_92.dll\r\n-a----        4/12/2018   1:20 AM        7804928 nvlink.exe\r\n-a----        4/12/2018   1:20 AM        4115968 nvprof.exe\r\n-a----        4/12/2018   1:20 AM         220160 nvprune.exe\r\n-a----        4/12/2018   1:20 AM        3213312 nvrtc-builtins64_92.dll\r\n-a----        4/12/2018   1:20 AM       15532544 nvrtc64_92.dll\r\n-a----        4/12/2018   1:20 AM             53 nvvp.bat\r\n-a----        4/12/2018   1:20 AM        7686144 ptxas.exe\r\n\r\n********************  Python Output  ************************\r\n\r\nPS C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\bin> python\r\nPython 3.6.0 |Anaconda 4.3.0 (64-bit)| (default, Dec 23 2016, 11:57:41) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n\r\n", "comments": ["> File \"C:\\Program Files\\Anaconda3\\lib\\importlib_init.py\", line 126, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: The specified module could not be found\r\n\r\nmaybe you can just add print(name[leval:]) in importlib_init.py to locate which dll load failed.", "Nagging Assignee @shivaniag: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "What is your CPU model? I suspect this may be a duplicate of https://github.com/tensorflow/tensorflow/issues/19584", "Thank you for getting back to me. I found that the problem was caused by the fact that I installed the latest version of the CUDA library. I downgraded to one version earlier and everything worked.\u00a0\nRegardsCraig\n\n\nSent from Yahoo Mail for iPhone\n\n\nOn Sunday, July 29, 2018, 5:25 PM, Gunhan Gulsoy <notifications@github.com> wrote:\n\n\nWhat is your CPU model? I suspect this may be a duplicate of #19584\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n \n\n\n", "Thanks for the response.\r\nClosing this issue then."]}, {"number": 20583, "title": "Unable to run fetch_imagenet_models.sh\uff0ccan not download the models of VGG16\uff0cThe url is False.", "body": "If you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\u00b7 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n\r\n\u00b7 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10-x64\r\n\r\n\u00b7 TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow.git\r\n\r\n\u00b7 TensorFlow version (use command below): r1.8, command :\r\ngit checkout -b v1.8 -f origin/r1.8\r\n\r\n\u00b7 Python version: Anaconda3 - python3.6\r\n\r\n\u00b7 Bazel version (if compiling from source): I used CMAKE 3.11.1\r\n\r\n\u00b7 GCC/Compiler version (if compiling from source): both Visual Studio 2015 and Visual Studio 2015' MSBuild\r\n\r\n\u00b7 CUDA/cuDNN version: CUDA9.0, cudnn-9.0-win10-7.1\r\n\r\n\u00b7 GPU model and memory: GTX-860m with 2Gb Memory\r\n\r\n### Describe the problem\r\nWhen configuring the environment of the Faster-RCNN project  from github\uff1a\r\nUnable to run file \u201cfetch_imagenet_models.sh\u201d\uff0cUnable to download the models of VGG16 cause the url is not OK,  so I can not get download \"voc_0712_80k-110k.tgz\".\r\n\r\n`\r\ncd $FRCN_ROOT\r\n./data/scripts/fetch_imagenet_models.sh\r\n`\r\n### Source code / logs\r\n`\r\ncd $FRCN_ROOT\r\n./data/scripts/fetch_imagenet_models.sh\r\n`", "comments": []}, {"number": 20582, "title": "TF-hub command line cannot download module", "body": "When I try retrain my dataset as said in https://www.tensorflow.org/tutorials/image_retraining,\r\nno matter which model I choose to fill in the --tfhub_module option from\r\nhttps://www.tensorflow.org/hub/modules/image#mobilenet\r\n\r\nI always stucked in \"INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/imagenet/mobilenet_v1_100_128/feature_vector/1'.\"\r\n\r\nI have tried to change tfhub.dev/ into tensorflow.google.cn/hub/modules/ ,but fail too.\r\n\r\nI am using tensorflow version1.7\r\nDo I missed something?How can I fix it?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI didn't write custom code\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 1.7.0\r\nPython version: 2.7.15\r\nBazel version (if compiling from source): 0.13.1\r\nGCC/Compiler version (if compiling from source): None\r\nCUDA/cuDNN version:None\r\nGPU model and memory: None\r\nExact command to reproduce:\r\n\r\npython retrain.py \\\r\n    --image_dir ~/flower_photos \\\r\n    --tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/1", "Please post this question to [TensorFlow Hub issues](https://github.com/tensorflow/hub/issues)."]}, {"number": 20581, "title": "Update sqlite to 2.24.0", "body": "This fix updates sqlite from 3.23.1 to 3.24.0.\r\n\r\nIn PR #20246 the sqlite was updated but for some reason the PR was not applied. This fix updates the sqlite again.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20580, "title": "Add a deserialization warning about Pickle in vocab files in some models to the security guide", "body": "https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md\r\nhave no any mentions that models often have Pickle files attached like vocabs.\r\nThese files should be validated before running. An exploit example is available here: https://gist.github.com/mgeeky/cbc7017986b2ec3e247aab0b01a9edcd", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "This has too many conflicts. Please resolve and merge to master."]}, {"number": 20579, "title": "[Feature Request] Support S3 KMS client-side decryption when loading data", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: V100\r\n\r\n### Describe the problem\r\nIt would be great to support S3 KMS client side decryption within TensorFlow, ideally transparently.\r\n\r\nWe use S3 with KMS to store sensitive data and decrypting it before loading it into a `tf.Dataset` can get really gross/be a major bottle neck, in part because AWS's boto3 [client library does not support client side KMS encryption](https://docs.aws.amazon.com/general/latest/gr/aws_sdk_cryptography.html) so decrypting data needs to be done manually ([example here](https://github.com/tedder/s3-client-side-encryption/blob/master/get.py)).\r\n\r\nHowever the AWS C++ client does support KMS client-side decryption.\r\n\r\nI'm not super familiar with TensorFlow's codebase but seems like there could be a couple of ways this could be added:\r\n\r\n1) Another contrib `tf.Dataset` implementation ([example here of an existing Dataset contrib](https://github.com/tensorflow/tensorflow/pull/19712/files))\r\nIn which case an extra method would likely need to be added to [`s3_file_system.h`](https://github.com/tensorflow/tensorflow/blob/935d5d8550ad06bc77e41e9a3d987658d3731be9/tensorflow/core/platform/s3/s3_file_system.h) that uses the C++ client for decryption, or handle the envelope encryption as part of how the Python Dataset class loads data\r\n\r\nor\r\n\r\n2) From what I can tell `tf.python.lib.io.file_io` uses [`NewRandomAccessFile`](https://github.com/tensorflow/tensorflow/blob/935d5d8550ad06bc77e41e9a3d987658d3731be9/tensorflow/core/platform/s3/s3_file_system.h#L30) from `s3_file_system.h` when loading datasets from S3 (as shown in the [s3 deploy docs](https://www.tensorflow.org/versions/r1.9/deploy/s3), correct me if I am wrong). \r\nAnother envar could be added (`S3_KMS_ARN`) and if set the C++ S3 client would decrypt client side. To me this would seem like the better solution since no new datasets would need to be worried about, encrypted files in S3 would behave like regular files.\r\n\r\nAn example of the C++ S3 encryption client can be found [here](https://aws.amazon.com/blogs/developer/amazon-s3-encryption-client-is-now-available-for-c/).\r\n\r\n### Exact command to reproduce\r\nN/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "@mrry This looks like a possible data pipeline feature.  Who could advise on this?", "I don't know if anyone on the team is an S3 expert, but I could see us welcoming such a contribution.", "I gave it a try to enable kms support with s3, but got the following:\r\n```\r\nRange-Get Operations are not allowed with Strict Authenticated Encryption mode.\r\n```\r\nNot very familiar with AWS kms though. So I am wondering if it is possible to have range get with encryption at the the moment.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "This feature request has a more specific discussion here https://github.com/tensorflow/tensorflow/issues/48009 , which suggests the implementation , closing this request. Thank you "]}, {"number": 20578, "title": "[Intel MKL] Adding support for python3 to MKL Dockerfile", "body": "Fixing a bug that locked MKL Dockerfile to python 2.", "comments": ["@angersson This change makes it so that we can parameterize the version of python that we build the MKL containers with. Please have a look. ", "@angersson Comments?", "@claynerobison Gah, sorry, I totally missed this (and while working on the Dockerfile stuff, for shame). Looks good to me, although this may have been made out-of-date by the new features available with my design doc that you've already taken a look at (thanks!)"]}, {"number": 20577, "title": "Unsupported tensor size Error with TF-TRT integration", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip install\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  3.4\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9/7.1\r\n- **GPU model and memory**: gtx 1080ti\r\n- **Exact command to reproduce**:\r\n\r\n`import tensorflow as tf\r\nfrom tensorflow.contrib import tensorrt as trt #MDFY\r\nfrom tensorflow.python.framework import tensor_util\r\n\r\ndef load_graph(frozen_graph_filename):\r\n    # We load the protobuf file from the disk and parse it to retrieve the \r\n    # unserialized graph_def\r\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    # Then, we can use again a convenient built-in function to import a graph_def into the \r\n    # current default Graph\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(\r\n            graph_def\r\n        )\r\n    return graph\r\ng = load_graph(path_to_pb_file)\r\nwith tf.Session(graph=g) as sess:\r\n\ttrt_graph = trt.create_inference_graph(\r\ninput_graph_def=tf.get_default_graph().as_graph_def(),\t\r\noutputs=nodenames,\r\nmax_batch_size=1,\r\nmax_workspace_size_bytes=1 << 25,\r\nprecision_mode=\"FP32\",  # TRT Engine precision \"FP32\",\"FP16\" or \"INT8\"\r\nminimum_segment_size=2  # minimum number of nodes in an engine\r\n)\r\n[print(n.name) for n in g.as_graph_def().node]\r\n\r\n`\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI want to tracebacack which layer the error is originating from, but the log information is not sufficient to do so, is there a workaround ?\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nTraceback (most recent call last):\r\n  File \"xyz.py\", line zz, in <module>\r\n    minimum_segment_size=2  # minimum number of nodes in an engine\r\n  File \"/home/dhingratul/opt/Python-3.4.4/py34/lib/python3.4/site-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 115, in create_inference_graph\r\n    int(msg[0]))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Unsupported tensor size: 2\r\n", "comments": ["Hi @dhingratul -- it's hard to help debug without further information, including the graph file you are trying to load. However, based on your question, it sounds like you are just looking for better access to debugging information? @samikama -- you have a todo in trt_convert regarding the returned status message; do you have any advice as to how to best debug here? ", "Yes, its hard to debug without access to the layer that throws that error, as DNNs often have a lot of layers, that cannot be manually inspected.", "@dhingratul, It is hard to read your report, but the error says unsupported tensor size. It should be handled better with PR #20755 and recent PRs. Could you please try with a more recent commit?", "Is this with TRT 4GA ? And also, would I need to build from source or can I just copy the changed files and import them directly in python?", "Yes, Please try with TRT4.0 GA and master. You would need to rebuild with TRT4.0.", "I have multiple version of TRT which i use within different virtual envs, I build TRT 4 with .tar not .deb, will that work, or do i need to build it with .deb ?", "I believe you are talking about TFTRT. Yes you can use TRT4 tarballs, just point configure.py to correct installation directory and update your LD_LIBRARY_PATH in your virtual env to use TRT4 libraries from the tarball.", "I built tensorflow from source, and re-ran the above script, there are no errors, but the conversions doesn't happen. When i print out the nodes from trt_graph above, i don't see anything. ", "@dhingratul, 2 dimensional tensor is not supported in TRT so it should not have been included in the segment in the first place. Would it be possible to share the model or a reproducer?\r\n", "Please see the attached script and a test model.pb file. \r\n\r\n\r\n[files.tar.gz](https://github.com/tensorflow/tensorflow/files/2203164/files.tar.gz)\r\n", "Nagging Assignees @karmel, @aaroey: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @dhingratul, I believed this is fixed by #21075. Moreover, I tried your attached script and it works well (after adding argument `name=''` to the `tf.import_graph_def()` call). I'm closing this one, and feel free to let me know if there are any other questions.", "@aaroey I can confirm this works with the fix mentioned and TRT 4GA", "Thanks @dhingratul for the confirmation."]}, {"number": 20576, "title": "[Intel MKL]: Fixing AVX Performance Issue", "body": "This PR fixes the MKL performance on machines with AVX instruction set by using some functionality from MKL binary version. Also, a new config option was added. If that option is used, only MKL-DNN open source lib will be used, without depending on MKL binary version. ", "comments": ["Can you look at the build failures?", "@martinwicke sorry, there was a file missing from this PR. Can you re-run the test?", "I think you have to export build_defs.bzl in your BUILD file?", "@martinwicke I did.", "Ah, never mind. The license check failed. It is there to ensure that for each third_party package we depend on which requires we include the license in the distribution, we actually include the license in the distribution. MKL-DNN may have a license that falls in that category? In that case, the fix would be to include the license in the deps for the pip package.\r\n\r\n@yifeif can you correct me or fill in details?", "Yea we include the licenses for the third party packages we depends on.\r\n`Missing the licenses for the following external dependencies:\r\n//third_party/mkl_dnn`\r\nMind adding the license to the following targets? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/BUILD#L123\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/BUILD#L111\r\nand\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/BUILD#L153\r\n\r\n\r\n", "Thanks for the info @martinwicke and @yifeif . I made the change. ", "@yifeif it is still failing after adding the license. I am confused why the error message says \"Please remove the licenses for the following external dependencies:\r\n@mkl_dnn//\". Any idea? ", "Thanks for the commit @mahmoud-abuzaina. Left a comment inline.", "@martinwicke the tests seem to be passing now. Can we merge it?", "@yifeif is copybara import stuck? It's been a long time since the label was applied...", "@martinwicke it looks like all tests are passing now. Maybe we can merge this.", "hi @mahmoud-abuzaina sorry for the delay, I'm working on bring this change in internally. Once it gets submitted, it will get merged. Will ping back if anything comes up. Thanks!", "Thank you!"]}, {"number": 20575, "title": "Boosted Trees Multi-Label Regression ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I plan to write custom code. \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Docker - GPU \r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: V9.0.176\r\n- **GPU model and memory**:  4 x GTX 1080 Ti \r\n- **Exact command to reproduce**: \r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen can I expect boosted trees to support multiple **label_dimension** ? Currently, **_HOLD_FOR_MULTI_CLASS_SUPPORT** is the placeholder for the implementation in _contrib_, while **label_dimension=_HOLD_FOR_MULTI_DIM_SUPPORT** is the placeholder in v1.8. Is there a pre-release implementation that is available to test before it is a part of TF? \r\n\r\n[https://arxiv.org/pdf/1710.11547.pdf](https://arxiv.org/pdf/1710.11547.pdf) is where I am getting the information about multi-class boosting. \r\n\r\nPage for [BoostedTreesRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesRegressor)\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["The documentation for BoostedTreesClassifier (https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier) mentions that multi class support is not implemented yet. Is there an ETA on it ?\r\n", "Please post a new issue by filling all the information in [Feature Request template](https://github.com/tensorflow/tensorflow/issues/new/choose) expressing interest in this feature. Thanks!"]}, {"number": 20574, "title": "fix issue with nsync which copies the headers every build", "body": "reworked nsync to be like png and zlib cmake files.\r\nit copied the nsync headers on every build and this triggered rebuild of the whole tensorflow project on windows", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I have signed it!\n\nOn Thu, 5 Jul 2018 at 21:34, googlebot <notifications@github.com> wrote:\n\n> CLAs look good, thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/20574#issuecomment-402813252>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABF5cWxE23EUUtUbyNU8J93a7d18GOUsks5uDlxJgaJpZM4VEVdq>\n> .\n>\n", "From the cmake file i see that it needs the same,\nThough it did not occured to be on windows 10.\nnsync there forced rebuild every time.\nbut it did not occured to me.\nI guess depends on the build systems. cmake that produces msbuild project\nfiles work with dates to check dependencies. I am not familiar with bazel\nand ninja though, if they check contents of the file, may be different.\nanyway, i will investigate more for highwash, may be it was not compiled in\nmy build.\n\nOn Wed, 11 Jul 2018 at 01:27, Mike Burrows <notifications@github.com> wrote:\n\n> *@m3bm3b* commented on this pull request.\n>\n> It looks good, but I'm confused.\n>\n> When I created this file, I copied its structure\n> from highwayhash.cmake.\n> So why aren't we having the same problem\n> with that file?\n> Or if we are having the problem with that file,\n> shouldn't we fix that also?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/20574#pullrequestreview-136032547>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABF5cQ_zKG7552nKxQe3XHJnWxuPqj0zks5uFSpBgaJpZM4VEVdq>\n> .\n>\n", "better to rework highwash, actually now only highwash follows different\npattern than the rest of the extern cmake files\n\nOn Wed, 11 Jul 2018 at 06:18, Stefan Dyulgerov <stefan.dyulgerov@gmail.com>\nwrote:\n\n> From the cmake file i see that it needs the same,\n> Though it did not occured to be on windows 10.\n> nsync there forced rebuild every time.\n> but it did not occured to me.\n> I guess depends on the build systems. cmake that produces msbuild project\n> files work with dates to check dependencies. I am not familiar with bazel\n> and ninja though, if they check contents of the file, may be different.\n> anyway, i will investigate more for highwash, may be it was not compiled\n> in my build.\n>\n> On Wed, 11 Jul 2018 at 01:27, Mike Burrows <notifications@github.com>\n> wrote:\n>\n>> *@m3bm3b* commented on this pull request.\n>>\n>> It looks good, but I'm confused.\n>>\n>> When I created this file, I copied its structure\n>> from highwayhash.cmake.\n>> So why aren't we having the same problem\n>> with that file?\n>> Or if we are having the problem with that file,\n>> shouldn't we fix that also?\n>>\n>> \u2014\n>> You are receiving this because you authored the thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/20574#pullrequestreview-136032547>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABF5cQ_zKG7552nKxQe3XHJnWxuPqj0zks5uFSpBgaJpZM4VEVdq>\n>> .\n>>\n>\n", "Do you plan to make the corresponding change to highwayhash.cmake in this PR?\r\nI'm happy to review it if you do.\r\n", "i am doing this right now, pull request will come shortly\n\nOn Wed, 11 Jul 2018 at 20:53, Mike Burrows <notifications@github.com> wrote:\n\n> Do you plan to make the corresponding change to highwayhash.cmake in this\n> PR?\n> I'm happy to review it if you do.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/20574#issuecomment-404255523>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABF5cTAaZ4tjYmpxJ-vtMqyJ8SztDd5Jks5uFjujgaJpZM4VEVdq>\n> .\n>\n", "Hello Mike,\nI did a pull request 4 days ago for highwayhash and still nothing.\nHow these things are processed in tensorflow?\nUsually coding takes half an hour and putting in git takes very long time\n\nOn Wed, 11 Jul 2018 at 20:54, Stefan Dyulgerov <stefan.dyulgerov@gmail.com>\nwrote:\n\n> i am doing this right now, pull request will come shortly\n>\n> On Wed, 11 Jul 2018 at 20:53, Mike Burrows <notifications@github.com>\n> wrote:\n>\n>> Do you plan to make the corresponding change to highwayhash.cmake in this\n>> PR?\n>> I'm happy to review it if you do.\n>>\n>> \u2014\n>> You are receiving this because you authored the thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/20574#issuecomment-404255523>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABF5cTAaZ4tjYmpxJ-vtMqyJ8SztDd5Jks5uFjujgaJpZM4VEVdq>\n>> .\n>>\n>\n", "I have no idea about how pull requests get assigned to reviewers.\r\n\r\nI naively had expected you to add the highwayhash cmake \r\nchange to this pull request, but perhaps that's \r\nnot possible or inadvisable for some reason.\r\n\r\n", "well, i create new branch for every fix as git best practices recommends\nand after that i say create pull request\n\nhttps://github.com/tensorflow/tensorflow/pull/20890\n\nOn Tue, 17 Jul 2018 at 22:46, Mike Burrows <notifications@github.com> wrote:\n\n> I have no idea about how pull requests get assigned to reviewers.\n>\n> I naively had expected you to add the highwayhash cmake\n> change to this pull request, but perhaps that's\n> not possible or inadvisable for some reason.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/20574#issuecomment-405703966>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABF5cbqB9fF96j5kAFY8jKnmnpm7NvqDks5uHj8igaJpZM4VEVdq>\n> .\n>\n", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This got migrated to PR 21206, and it has all the necessary reviews and approvals.\r\nI would expect it to appear in the next day or two, unless the final phase of testing\r\nuncovers something unexpected.\r\n", "Closing this because changes were successfully merged in PR 21206.  Thanks."]}, {"number": 20573, "title": "Features parameters for retrain.py used model", "body": "\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Using retrain.py to train my own data**:\r\n- **Windows**:\r\n- **install tensorflow using cmd**:\r\n- ** tensorflow version 2.0**:\r\n- ** python version 3.6.6rc1**: \r\n- **No bazel**:\r\n\r\n\r\n### Describe the problem\r\nNeed help regarding setting parameters for the model in ClassifierActivity.java .I have trained by custom dataset using retrain.py gave on the tensorflow.But when I run the app on the android by deploying my own model. The app runs successfully but not labeling the detecting images. I think there is some issue with parameters \r\nprivate static final int INPUT_SIZE = 299;\r\n  private static final int IMAGE_MEAN = 128;\r\n  private static final float IMAGE_STD = 128;\r\n  private static final String INPUT_NAME = \"Mul\";\r\n  private static final String OUTPUT_NAME = \"final_result\";\r\nOr there is another issue you need to tell me.Please help as I have to submit my work as soon as possible. \r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20572, "title": "update pin of bazel-toolchain repo for bazel 0.15.0 config release", "body": "bazel-toolchain repo updated for the bazel 0.15.0 config release\r\n\r\nhttps://releases.bazel.build/bazel-toolchains.html", "comments": ["@nlopezgi @xingao267 could you help review this?", "@gunan @yifeif could you please review this pr? Thanks!", "Could you re-send this internally? It is much faster to propagate the change everywhere that way.", "Also, on github we are not merging any changes to migrate to single source of truth.", "thanks, sent internally. @erain can you close this PR please?"]}, {"number": 20571, "title": "Cannot query shape of opaque params in CudnnGRU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Yes\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.9.0-rc0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: NVIDIA GTX 1080 Ti, 11176MiB\r\n\r\n### Describe the problem\r\nWhen trying to compute the number of parameters in `tf.contrib.cudnn_rnn.CudnnGRU`, TensorFlow will crash.\r\n\r\nThe reason to do this is to share the weights between `CudnnGRU` and `CudnnCompatibleGRUCell`. There is no documentation with examples on how to do this (see #20273 ), but there is a high-level description from the tf 1.2 docs:\r\n```\r\nCudnn implementation of the GRU model. Cudnn RNN has an opaque parameter buffer that can be used for inference and training. But it is possible that the layout of the parameter buffers changes between generations. So it is highly recommended to use RNNParamsSaveable to save and restore weights and biases in a canonical format.\r\n\r\nThis is a typical use case:\r\n  * The user creates a CudnnRNN model.\r\n  * The user query that parameter buffer size.\r\n  * The user creates a variable of that size that serves as the parameter buffers.\r\n  * The user either initialize the parameter buffer, or load the canonical weights into the parameter buffer.\r\n  * The user calls the model with the parameter buffer for inference, or training.\r\n  * If training, the user creates a Saver object.\r\n  * If training, the user creates a RNNParamsSaveable object from the parameter buffer for it to be later saved in the canonical format. When creating a RNNParamsSaveable object, a name could be provided, which is useful in distinguishing the names of multiple RNNParamsSaveable objects (e.g. for an encoder-decoder model).\r\n  * Once a while, the user saves the parameter buffer into model checkpoints with Saver.save().\r\n  * When restoring, the user creates a RNNParamsSaveable object and uses Saver.restore() to restore the paramter buffer from the canonical format to a user-defined format, as well as to restore other savable objects in the checkpoint file.\r\n```\r\nSo the second step in this process fails.\r\n\r\nAlso to consider: [the test file has deprecated code and will fail](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_ops_test.py), using `cuddn_gru_instance.params_size()` rather than `.count_params()`, which is no longer defined. These test cases are the only resource available for users attempting to find out how undocumented features should be used, but they're outdated.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nrnn = tf.contrib.cudnn_rnn.CudnnGRU(1, 7, direction='unidirectional')\r\nrnn.build([2,3,5])\r\nrnn.count_params()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/danielwatson/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1298, in count_params\r\n    weight_shapes = [w.get_shape().as_list() for w in self.weights]\r\n  File \"/home/danielwatson/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1298, in <listcomp>\r\n    weight_shapes = [w.get_shape().as_list() for w in self.weights]\r\n  File \"/home/danielwatson/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 903, in as_list\r\n    raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@protoget could you comment on this?\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@protoget Any thoughts on this?", "tf1.2 doc on this is deprecated. We started to have more usable CudnnRNN classes since 1.6.\r\n\r\nIt probably doesn't help you achieve much by querying the size of opaque params, which is subject to change depending on cudnn version.\r\n\r\nTo share params between CudnnLSTM and CudnnCompatibleLSTM, one could export a CudnnLSTM checkpoint and import into a CudnnCompatibleLSTM graph. Check this unittest for an example how that could be done.\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_test.py#L904", "@protoget That makes sense. There's a [related issue](https://github.com/tensorflow/tensorflow/issues/20273) I opened about putting this in documentation, rather than making users have to read source to know how to use these classes. I'll close this given your explanation."]}]