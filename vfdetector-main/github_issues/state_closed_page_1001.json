[{"number": 23335, "title": "Use bash in makefile to avoid unknown [[ error ", "body": "When building `tflite` with makefile using this command, \r\n```\r\nmake -f tensorflow/contrib/lite/tools/make/Makefile\r\n```\r\nwe observed this error for determining host architecture\r\n```\r\n/bin/sh: 1: [[: not found\r\n```", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@aselle Request you to take a look and approve this PR. Thanks !", "@darthsuogles could you please resolve the conflicts? Thanks!", "@darthsuogles gentle ping to resolve the conflicts. Thanks!", "@darthsuogles Did you get a chance to look on conflicts? Please let us know on the update. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 23334, "title": "Bug: issue initializing iterator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nVersion: v1.11.0-0-gc19e29306c 1.11.0\r\n **Python version**:\r\n3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nSee source code below\r\n\r\n### Describe the problem\r\nThere seems to be an issue initializing an iterator created with `tf.data.Iterator.form_structure` a second time in the _same_ session where the dataset contains `Function` instances.\r\n\r\n### Source code / logs\r\nTo produce problem\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef g():\r\n  for i in range(10):\r\n    yield i\r\n\r\niterator = tf.data.Iterator.from_structure(tf.int64, tf.TensorShape([]))\r\nx = iterator.get_next()\r\nsess = tf.Session()\r\n\r\ndataset_1 = tf.data.Dataset.from_generator(g, output_types=tf.int64)\r\nsess.run(iterator.make_initializer(dataset_1))\r\nsess.run(x)\r\n\r\ndataset_2 = tf.data.Dataset.from_generator(g, output_types=tf.int64)\r\nsess.run(iterator.make_initializer(dataset_2))\r\nsess.run(x)\r\n```\r\nExecuting the above leads to the following error\r\n```\r\nNotFoundError (see above for traceback): Function tf_data_structured_function_wrapper_uv0gphzJzQY is not defined.\r\n```", "comments": ["It seems fine when I run your code on an environment that replicates yours. Can you try again and provide more information? ", "See updated version. The previous example worked for me when I used e.g. version 1.10. The updated example fails for me for both version 1.10 and 1.11. If it still works for you, let me know what other information you think could be helpful.", "This does appear to be a bug in the latest version. One workaround is to ensure that all calls to `make_initializer()` have been made before creating the `tf.Session`. For example, the following program should work as intended:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef g():\r\n  for i in range(10):\r\n    yield i\r\n\r\niterator = tf.data.Iterator.from_structure(tf.int64, tf.TensorShape([]))\r\nx = iterator.get_next()\r\n\r\ndataset_1 = tf.data.Dataset.from_generator(g, output_types=tf.int64)\r\ndataset_2 = tf.data.Dataset.from_generator(g, output_types=tf.int64)\r\n\r\ninit_1 = iterator.make_initializer(dataset_1)\r\ninit_2 = iterator.make_initializer(dataset_2)\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(init_1)\r\n  print sess.run(x)\r\n\r\n  sess.run(init_2)\r\n  print sess.run(x)\r\n```", "Update: we have a fix for this case, and it's currently pending review.", "Ok, thanks!"]}, {"number": 23333, "title": "tf.scatter_update does not update variable with float type on GPU ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04.1 LTS\r\n- TensorFlow installed from (source or binary): 1.11\r\n- TensorFlow version (use command below): pip \r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA release 9.0, V9.0.176\r\n- GPU model and memory: TITAN Xp / 12Gb\r\n\r\n**Describe the current behavior**\r\n\r\nOn GPU, tf.scatter_update does not update the variable if the type of the variable is defined as float. However, the same code works without a problem if the code is executed on CPU. If you run the code I provided here for you, you will get random values like the one that comes in the following every time you run the code:  \r\n\r\n[array([-0.27238935, -0.11273658, -0.18543988, -0.34574947, -0.4174637 ,\r\n       -0.35478222], dtype=float32), 6]\r\n\r\nThis problem does not happen on a CPU, or if the variable type is set to integer. \r\n\r\n**Describe the expected behavior**\r\nIt should simply update the variable and should output the following in a deterministic manner: \r\n\r\n\r\n[array([19., 19., 19., 19., 19., 19.], dtype=float32), 6]\r\n\r\nI suppose this is a bug in the caching process involved if the code is run on a GPU.  \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n    import tensorflow as tf\r\n    import os\r\n\r\n    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\r\n    def cond(size, i):\r\n\t    return tf.less(i,size)\r\n\r\n    def body(size, i):\r\n\t    b=2*7.5+c\r\n\r\n\t    with tf.variable_scope(\"a\", reuse=tf.AUTO_REUSE):\r\n\t\t    a = tf.get_variable(\"a\",[6],dtype=tf.float32)\r\n\r\n\t        a = tf.scatter_update(a,i,b)\r\n\r\n\t        with tf.control_dependencies([a]):\r\n\t\t        return (size, i+1)\r\n\r\n    with tf.Session() as sess:\r\n        c=tf.constant(4.0)\r\n        i = tf.constant(0)\r\n        size = tf.constant(6)\r\n        _,i = tf.while_loop(cond,\r\n                body,\r\n                [size, i])\r\n    \r\n        a = tf.get_variable(\"a\",[6],dtype=tf.float32)\r\n    \r\n        init = tf.initialize_all_variables()\r\n        sess.run(init)\r\n        print(sess.run([a,i]))\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This is working as expected.\r\n` sess.run([a,i])` does not define the evaluation order of `a` and `i`.  `a` can be evaluated either before or after or during the loop, and before the loop `a` is randomly initialized. Therefore you can get different results.\r\nSee documentation: https://www.tensorflow.org/api_docs/python/tf/Session#run\r\n> Order in which fetches operations are evaluated inside the call is undefined.", "Thanks for your reply. But what would be the solution then ? how can we force i to be evaluated sooner in the same session as a ? also, as I mentioned before, this problem does not occur if the code is executed on a CPU. ", "You can just `sess.run(a)` after finishing `sess.run(i)`.\r\n\r\nAnother bug in your code above is that the second `get_variable` should be under the variable scope \"a\", otherwise you're creating a new variable.", "Why the `bug/performance` label? The code is working as expected as far as I can see.", "Thanks. It works as expected now. ", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23333)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23333)\r\n"]}, {"number": 23332, "title": "ctc_loss gives  No valid path found incorrectly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I used stock functions as is but through keras\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win 10 \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.8\r\n- Python version: 3.6.3\r\n- CUDA/cuDNN version: 7.1 to the best of my memory\r\n- GPU model and memory: ti 1070 6bg \r\n\r\nI am using ctc_loss() and I get the error \"T:\\src\\github\\tensorflow\\tensorflow\\core\\util\\ctc\\ctc_loss_calculator.cc:144] No valid path found.\" \r\nI am pretty sure that all the data I fed in included some valid path - that is the length of the labels is ~.5x the length of the inputs and the labels have very few repetitions. \r\n\r\nIts a bit hard to reproduce since I am not sure which sample is causing the issue.\r\nI can copy paste the batch that I feed into my network if that helps? \r\n \r\n", "comments": []}, {"number": 23331, "title": "SpatialSoftmax", "body": "It seams to me that `width_lin` should be used when we want to look up the corresponding value to x_loc: [this line](https://github.com/tensorflow/tensorflow/blob/6bfb36b241dadfecb345edb0589a8d0ae72dc968/tensorflow/contrib/layers/python/layers/layers_test.py#L3566)\r\n```sh\r\n x_lin = np.expand_dims(np.array([height_lin[i] for i in x_loc]), 1)\r\n```\r\nshould be\r\n\r\n```sh\r\n x_lin = np.expand_dims(np.array([width_lin[i] for i in x_loc]), 1)\r\n``` \r\n", "comments": ["Nice catch, this seems true if naming convention `x~width` holds. Since this implies that all tests of unequal height/width would be transposed, wouldn't that mean that they *should* fail as is right now? [Ex](https://github.com/tensorflow/tensorflow/blob/6bfb36b241dadfecb345edb0589a8d0ae72dc968/tensorflow/contrib/layers/python/layers/layers_test.py#L3646)", "@ragulpr Thanks for your follow up.\r\nI don't think the tests would fail because somehow even in spatial_softmax layer implementation width and height are reversed, [here](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/layers/python/layers/layers.py#L2960). \r\n\r\n```sh\r\n      pos_x, pos_y = array_ops.meshgrid(\r\n          math_ops.lin_space(-1., 1., num=height),\r\n          math_ops.lin_space(-1., 1., num=width),\r\n          indexing='ij')\r\n```", "Related, in the discussion #6271 they talk about height and width in that order without raising an eyebrow, so I think we're the ones confused about the (x,y)-[convention](https://highresolutions.com/blog/2010/03/what-comes-first-width-or-height/) \ud83d\ude04. I don't think this is a bug then, just a strange convention so I propose closing issue."]}, {"number": 23330, "title": "download tensorflow to run on  python 3.7.1 : version not satisfied", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIN 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): GITHUB\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nNot able to download tensorflow\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nC:\\WINDOWS\\system32> pip install tensorflow\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Try python 3.6", "#17022"]}, {"number": 23329, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 1809 x64 OS Build : 17763.55\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 1.10.0\r\n- Python version: 3.6.6 x64\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: I'm using CPU version\r\n- GPU model and memory: I'm using CPU version\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nHere this the command that i use : import tensorflow as tf\r\n\r\n**Any other info / logs**\r\n\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\nreturn importlib.import_module(mname)\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib_init_.py\", line 126, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\nFile \"\", line 994, in _gcd_import\r\nFile \"\", line 971, in _find_and_load\r\nFile \"\", line 955, in _find_and_load_unlocked\r\nFile \"\", line 658, in _load_unlocked\r\nFile \"\", line 571, in module_from_spec\r\nFile \"\", line 922, in create_module\r\nFile \"\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in \r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\nreturn importlib.import_module('pywrap_tensorflow_internal')\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib_init.py\", line 126, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"decensor.py\", line 8, in \r\nfrom libs.pconv_hybrid_model import PConvUnet\r\nFile \"C:\\Users\\minat\\Desktop\\DeepCreamPy\\libs\\pconv_hybrid_model.py\", line 4, in \r\nfrom keras.models import Model\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras_init_.py\", line 3, in \r\nfrom . import utils\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\utils_init_.py\", line 6, in \r\nfrom . import conv_utils\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in \r\nfrom .. import backend as K\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend_init_.py\", line 89, in \r\nfrom .tensorflow_backend import *\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in \r\nimport tensorflow as tf\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_init_.py\", line 22, in \r\nfrom tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python_init_.py\", line 49, in \r\nfrom tensorflow.python import pywrap_tensorflow\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in \r\nraise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\nreturn importlib.import_module(mname)\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib_init_.py\", line 126, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\nFile \"\", line 994, in _gcd_import\r\nFile \"\", line 971, in _find_and_load\r\nFile \"\", line 955, in _find_and_load_unlocked\r\nFile \"\", line 658, in _load_unlocked\r\nFile \"\", line 571, in module_from_spec\r\nFile \"\", line 922, in create_module\r\nFile \"\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in \r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\nreturn importlib.import_module('pywrap_tensorflow_internal')\r\nFile \"C:\\Users\\minat\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib_init.py\", line 126, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions. Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": []}, {"number": 23328, "title": "TensorRT error:*** stack smashing detected ***: python3 terminated", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nnot sure\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n1.7.0  and 1.10.0\r\n- Python version:\r\n3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n 5.4.0\r\n- CUDA/cuDNN version:\r\nCUDA9.0 / CUDNN 7.0.5\r\n- GPU model and memory:\r\n1080TI / 12g\r\n\r\nI am using tensorflow version 1.7, cuda9.0, python 3.5 \uff0cand following the readme.md. and I am getting following error.\r\n\r\n\r\nray@ray-MS-7B48:~/data/models/research/tensorrt$ python3 tensorrt.py --frozen_graph=resnetv2_imagenet_frozen_graph.pb --image_file=image.jpg --native --fp32 --fp16 --int8 --output_dir=./output\r\nWARNING:tensorflow:From /home/ray/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\n2018-10-28 16:42:07.777004: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-28 16:42:07.889329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-10-28 16:42:07.889593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.31GiB\r\n2018-10-28 16:42:07.889607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-10-28 16:42:08.417071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-28 16:42:08.417094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \r\n2018-10-28 16:42:08.417100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \r\n2018-10-28 16:42:08.417464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8935 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nRunning native graph\r\nINFO:tensorflow:Starting execution\r\n2018-10-28 16:42:08.973439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-10-28 16:42:08.973468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-28 16:42:08.973474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \r\n2018-10-28 16:42:08.973477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \r\n2018-10-28 16:42:08.973566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8935 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Starting Warmup cycle\r\nINFO:tensorflow:Starting timing.\r\nINFO:tensorflow:Timing loop done!\r\nRunning FP32 graph\r\n2018-10-28 16:42:30.856792: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\n2018-10-28 16:42:31.229082: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:383] MULTIPLE tensorrt candidate conversion: 2\r\n*** stack smashing detected ***: python3 terminated\r\nAborted (core dumped)\r\n\r\n", "comments": ["@ray-mami, Do you have **TensorRT 3.0.4 for Ubuntu 14.04** installed in the system? This error usually happens when user installs TensorRT 3.0.4 for Ubuntu 16.04 instead of 14.04 when using pip package.", "> @ray-mami, Do you have **TensorRT 3.0.4 for Ubuntu 14.04** installed in the system? This error usually happens when user installs TensorRT 3.0.4 for Ubuntu 16.04 instead of 14.04 when using pip package.\r\n\r\nNo \uff0c TensorRT 3.0.4 is not installed in the system.  **LD_LIBRARY_PATH** contains the path of  TensorRT-4.0.1.6", "Nagging Assignee @samikama: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing since user is using incompatible TRT library. "]}, {"number": 23327, "title": "tfp.distributions.TransformedDistribution can't handle input and output events with different ndims", "body": "TransformedDistribution can infer event_ndims from base distribution's event_ndims. When the output event (usually an image) has different ndims from input event (usually a 1-D vector), the training would crash at TransformedDistribution::_log_prob(self,y). It calls underlying bijector's Bijector::inverse_log_det_jacobian(self,y,event_ndims) and pass inferred event_ndims which equals 1 according to base distribution's ndims (MultivariateNormalDiag in my case). but y is an image which has ndims of 3 (height x width x channel). The ndims sanity check would fail in Bijector::_call_inverse_log_det_jacobian(self,y,event_ndims).\r\n\r\nI recommend that TransformedDistribution infer the forward_event_ndims and inverse_event_ndims separately from base distribution and underlying bijector respectively to get proper behavior.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):pip repo\r\n- TensorFlow version (use command below):1.11\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:n/a\r\n- GPU model and memory:n/a", "comments": []}, {"number": 23326, "title": "DataLossError (see above for traceback): not an sstable (bad magic number)", "body": "hi, @tensorflow.org.I got a problem in this case. I can run this code in windows, but I can't run this in my Ubuntu 16.04.I got a DataLossError. But the model was found in this case. And this model can run in windows.\r\n\r\nI got this Error. So sad.\r\n```\r\nDataLossError (see above for traceback): not an sstable (bad magic number)\r\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\n```\r\nAnd the code is like this.\r\n```\r\nckpt_filename = '/home/yangziyuan/SSD-Tensorflow/checkpoints/ssd_300_vgg.ckpt'\r\nisess.run(tf.global_variables_initializer())\r\nsaver = tf.train.Saver()\r\nsaver.restore(isess.ckpt_filename)\r\n```\r\nBut saver.restore() stopped everything.\r\nSo anyone has any clue? Thanks for your guys time and patience.\r\n\r\n", "comments": ["@DDDoriM1  - Hi, please check if [this](https://stackoverflow.com/questions/41048819/how-to-restore-a-model-by-filename-in-tensorflow-r12) helps.", "> @DDDoriM1 - Hi, please check if [this](https://stackoverflow.com/questions/41048819/how-to-restore-a-model-by-filename-in-tensorflow-r12) helps.\r\n\r\nThanks a lot. I've checked this link. It seems like can not solve this problem. Actually there is only .ckpt in this model file. And the version I used is tensorflow-gpu 1.11. I can't run this code in LInux, but it can work in Windows. So confused.\r\n\r\nThanks!!!!!", "I got the same problem.\r\ntensorflow-gpu 1.10", "@DDDoriM1 - Ok. Can you please fill all the details as per this [template](https://github.com/tensorflow/tensorflow/issues/new/choose) so that we can start looking into the issue. We follow a process where we start looking into the bug only if the details are provided as per the template. Thank you!", "Are you sure you have the file and it's not truncated? What's the magic that you're getting instead?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "I still encounter the same problem. There is no problem for the loading paths. What is the solution?"]}, {"number": 23325, "title": "tf.contrib.image.dense_image_warp to support dynamic shape tensor input", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.11.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI used optical flow to warp images with TF function tf.contrib.image.dense_image_warp . However, the current version  does not support dynamic shapes. \r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nPeople use this function to warp images.\r\n\r\n**Any Other info.**\r\nThe source code of dense_image_warp:\r\n***********\r\n```\r\n  with ops.name_scope(name):\r\n    batch_size, height, width, channels = image.get_shape().as_list()\r\n    # The flow is defined on the image grid. Turn the flow into a list of query\r\n    # points in the grid space.\r\n    grid_x, grid_y = array_ops.meshgrid(\r\n        math_ops.range(width), math_ops.range(height))\r\n    stacked_grid = math_ops.cast(\r\n        array_ops.stack([grid_y, grid_x], axis=2), flow.dtype)\r\n    batched_grid = array_ops.expand_dims(stacked_grid, axis=0)\r\n    query_points_on_grid = batched_grid - flow\r\n    query_points_flattened = array_ops.reshape(query_points_on_grid,\r\n                                               [batch_size, height * width, 2])\r\n    # Compute values at the query points, then reshape the result back to the\r\n    # image grid.\r\n    interpolated = _interpolate_bilinear(image, query_points_flattened)\r\n    interpolated = array_ops.reshape(interpolated,\r\n                                     [batch_size, height, width, channels])\r\n    return interpolated\r\n```\r\n\r\n*************\r\nI think changing \r\n`batch_size, height, width, channels = image.get_shape().as_list()`\r\nto\r\n`batch_size, height, width, channels = tf.shape(image).as_list()`\r\nmight be sufficient.", "comments": ["Hello all, I have made a PR, https://github.com/tensorflow/tensorflow/pull/23394.\r\nWould you be interested to review it?\r\nThanks.", "@zldrobit  -  Thanks for the contribution. Your PR has been reviewed and approved. We're looking into that and it'll be merged soon. ", "Close this issue since the PR has been merged."]}, {"number": 23324, "title": "spatialsoftmax", "body": "It seams to me that `width_lin` should be used when we want to look up the corresponding value to x_loc: \r\n```sh\r\n x_lin = np.expand_dims(np.array([height_lin[i] for i in x_loc]), 1)\r\n```\r\nshould be\r\n\r\n```sh\r\n x_lin = np.expand_dims(np.array([width_lin[i] for i in x_loc]), 1)\r\n``` \r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23322, "title": "How can parse_fn be put into tf.data.map without arguments?", "body": "**System information**\r\n- TensorFlow version: 1.10.1\r\n\r\n**Describe the documentation issue**\r\nI am familiar with parsing tfrecord back to tensor without using tf.data API. And now I'm trying to use this API to construct a more robust pipeline. The code goes like this:\r\n`def parse_fn(serialized):\r\n    features = {\r\n        'image': tf.FixedLenFeature([], tf.string),\r\n        'label': tf.FixedLenFeature([], tf.int64)\r\n    }\r\n    parse_exp = tf.parse_single_example(serialized=serialized, features=features)\r\n    data = parse_exp['image']\r\n    data = tf.decode_raw(data, tf.uint8)\r\n    data = tf.cast(data, tf.float32)\r\n    return data, parse_exp['label']\r\n\r\n\r\ndef input_fn(data_list, batch_size=32, shuffle_size=1024, prefetch_size=2):\r\n    files = tf.data.Dataset.list_files(data_list)\r\n    dataset = files.interleave(tf.data.TFRecordDataset)\r\n    dataset = dataset.shuffle(buffer_size=shuffle_size)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.map(parse_fn, num_parallel_calls=4)\r\n    dataset = dataset.batch(batch_size)\r\n    dataset = dataset.prefetch(buffer_size=prefetch_size)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    images_batch, labels_batch = iterator.get_next()\r\n    return {'image': images_batch}, labels_batch`\r\n\r\nBut based on my knowledge, tfrecord file is a protocol buffer file containing binary data. So here are my 2 consecutive questions...\r\n1. How can tf know which part and which part should be shuffled before parse the file into tf datatype? \r\n2. How can the parse_fn function be substituted into map func without argument? where will the 'serialized' argument come from? What if I want to add more arguments to parse_fn, what can I do?\r\n\r\nLastly, in this case I use 'label' as the int64List dict name to store data into tfrecord so that I can extract the corresponding data using key 'label' here again. But why I didn't get error when I use the other name as the key in order to get data? This makes me feel weird because when I was using TFRecordReader.read() to extract binary data, it is pretty sensitive that I should use exactly the same key name so that there would be no errors.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23321, "title": "Win10, AMD: Failed to load the native TensorFlow runtime.", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Pip\r\n- TensorFlow version: tf nightly gpu [the last one until this date i suppose]\r\n- Python version: 3.6.4 Anaconda\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: APU AMD A10 - AMD GPU R6 M340DX 2GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI had installed tf cpu in my computer, and then i wanted to use tf in my gpu, so i installed the nightly \r\ngpu version. After i installed it, i couldn't use more tensorflow\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nimport tensorflow as tf\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-4-1c24dce58f6f> in <module>()\r\n      3 import numpy as np\r\n      4 import os\r\n----> 5 import tensorflow as tf\r\n      6 import time\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow.python.tools import component_api_helper as _component_api_helper\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\dcifuen3\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\dcifuen3\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\dcifuen3\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["You can only use `tensorflow` with a Nvidia graphics card.", "> You can only use `tensorflow` with a Nvidia graphics card.\r\n\r\nBut that's with the usual gpu installation of tensorflow, i used the nightly gpu installation that it's for gpu support in OpenCL, and as you know, AMD uses OpenCL.\r\n\r\nI don't know if have to do the issue in the [original repo of the nightly version](https://github.com/benoitsteiner/tensorflow-opencl) (but looking at the issues there, there are many unresponded for a long time) or if i can do it here ", "Look at this [https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu). Unfortunately, the `tf-nightly-gpu` requires a Nvidia graphics card", "tf-nightly-gpu uses OpenCL that runs in both Nvidia (OpenCL 1.2) and AMD. tf-nightly-gpu was developed to run tensorflow in gpus different than nvidia, thanks to opencl, but it seems like they're no offering more support, because the last commit was like a year ago, and there are many unresponded issues. ", "Well, you could try this https://medium.com/tensorflow/amd-rocm-gpu-support-for-tensorflow-33c78cc6a6cf", "@diegoxfx  - Is this still an issue ?", "@harshini-gadige Yes, because the solution that rosgor gave me is for Linux, and my problem is in Windows. I'm not sure if tf-nightly-gpu can run in windows with amd, or i'm trying something that is impossible from the beginning", "@gunan  -  PTAL", "There is no official support for AMD GPUs yet.\r\nAnd `tf-nightly-gpu` definitely cannot use OpenCL yet.\r\nYou will need to reach out to AMD if you are using ROCm, or Codeplay if you are using opencl."]}, {"number": 23320, "title": "[Feature Request] Addition of new operation to Tensorflow Lite for \"ENet\"", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using): **[Tensorflow Lite v1.11.0 (Self-Build)](https://github.com/PINTO0309/Tensorflow-bin.git)**\r\n- Hardware: RaspberryPi3\r\n- OS: Raspbian Stretch\r\n- Are you willing to contribute it (Yes/No): No (Because, There are few Custom Operation tutorials, and I can not write C++ programs.)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIf I implement \"semantic segmentation\" model \"ENet\", I can not do it unless you support various behaviors of the unpooling layer.\r\n\r\n- **Layer that I would like to support with Tensorflow Lite**\r\n    - ~FloorMod~\r\n    - ~Range~\r\n    - ~Rank~\r\n    - Abs\r\n    - MaxPoolWithArgmax\r\n    - ~ScatterNd~\r\n    - SparseTensor\r\n    - sparse_add\r\n    - ~gather_nd~\r\n\r\n- **Sample message of Unsupport Error**\r\n```Python\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 5, in <module>\r\n    interpreter = tf.contrib.lite.Interpreter(model_path=\"semanticsegmentation_enet_non_quantized.tflite\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/interpreter.py\", line 53, in __init__\r\n    model_path))\r\nValueError: Didn't find custom op for name 'FloorMod' with version 1\r\nDidn't find custom op for name 'Range' with version 1\r\nDidn't find custom op for name 'Rank' with version 1\r\nDidn't find custom op for name 'Abs' with version 1\r\nDidn't find custom op for name 'MaxPoolWithArgmax' with version 1\r\nDidn't find custom op for name 'ScatterNd' with version 1\r\nRegistration failed.\r\n```\r\n\r\n**Will this change the current api? How?**\r\nYes.\r\nCustom Operation must be officially implemented.\r\n\r\n**Who will benefit with this feature?**\r\n- Engineers who study automatic driving technology in general\r\n- Engineer who studies fast inference using lightweight mobile\r\n- Engineers to study lightweight models\r\n\r\n**Any Other info.**\r\nMy repository and sample program are below.  \r\nFor Python2.x / Python3.x.\r\n  \r\n**https://github.com/PINTO0309/TensorFlow-ENet.git**  \r\n**https://github.com/PINTO0309/TensorflowLite-UNet.git**  \r\n  \r\n<details><summary>\u3010Reference\u3011 Model Logic</summary><div>\r\n\r\n```Python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.layers.python.layers import initializers\r\nslim = tf.contrib.slim\r\n\r\n@slim.add_arg_scope\r\ndef prelu(x, scope, decoder=False):\r\n\r\n    #If decoder, then perform relu and just return the output\r\n    if decoder:\r\n        return tf.nn.relu(x, name=scope)\r\n\r\n    alpha= tf.get_variable(scope + 'alpha', x.get_shape()[-1],\r\n                       initializer=tf.constant_initializer(0.0),\r\n                        dtype=tf.float32)\r\n    pos = tf.nn.relu(x)\r\n    neg = alpha * (x - abs(x)) * 0.5\r\n    return pos + neg\r\n\r\ndef spatial_dropout(x, p, seed, scope, is_training=True):\r\n\r\n    if is_training:\r\n        keep_prob = 1.0 - p\r\n        input_shape = x.get_shape().as_list()\r\n        noise_shape = tf.constant(value=[input_shape[0], 1, 1, input_shape[3]])\r\n        output = tf.nn.dropout(x, keep_prob, noise_shape, seed=seed, name=scope)\r\n\r\n        return output\r\n\r\n    return x\r\n\r\ndef unpool(updates, mask, k_size=[1, 2, 2, 1], output_shape=None, scope=''):\r\n\r\n    with tf.variable_scope(scope):\r\n        mask = tf.cast(mask, tf.int32)\r\n        input_shape = tf.shape(updates, out_type=tf.int32)\r\n        #  calculation new shape\r\n        if output_shape is None:\r\n            output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\r\n\r\n        # calculation indices for batch, height, width and feature maps\r\n        one_like_mask = tf.ones_like(mask, dtype=tf.int32)\r\n        batch_shape = tf.concat([[input_shape[0]], [1], [1], [1]], 0)\r\n        batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int32), shape=batch_shape)\r\n        b = one_like_mask * batch_range\r\n        y = mask // (output_shape[2] * output_shape[3])\r\n        x = (mask // output_shape[3]) % output_shape[2] #mask % (output_shape[2] * output_shape[3]) // output_shape[3]\r\n        feature_range = tf.range(output_shape[3], dtype=tf.int32)\r\n        f = one_like_mask * feature_range\r\n\r\n        # transpose indices & reshape update values to one dimension\r\n        updates_size = tf.size(updates)\r\n        indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\r\n        values = tf.reshape(updates, [updates_size])\r\n        ret = tf.scatter_nd(indices, values, output_shape)\r\n        return ret\r\n\r\n@slim.add_arg_scope\r\ndef initial_block(inputs, is_training=True, scope='initial_block'):\r\n\r\n    #Convolutional branch\r\n    net_conv = slim.conv2d(inputs, 13, [3,3], stride=2, activation_fn=None, scope=scope+'_conv')\r\n    net_conv = slim.batch_norm(net_conv, is_training=is_training, fused=True, scope=scope+'_batchnorm')\r\n    net_conv = prelu(net_conv, scope=scope+'_prelu')\r\n\r\n    #Max pool branch\r\n    net_pool = slim.max_pool2d(inputs, [2,2], stride=2, scope=scope+'_max_pool')\r\n\r\n    #Concatenated output - does it matter max pool comes first or conv comes first? probably not.\r\n    net_concatenated = tf.concat([net_conv, net_pool], axis=3, name=scope+'_concat')\r\n    return net_concatenated\r\n\r\n@slim.add_arg_scope\r\ndef bottleneck(inputs,\r\n               output_depth,\r\n               filter_size,\r\n               regularizer_prob,\r\n               projection_ratio=4,\r\n               seed=0,\r\n               is_training=True,\r\n               downsampling=False,\r\n               upsampling=False,\r\n               pooling_indices=None,\r\n               output_shape=None,\r\n               dilated=False,\r\n               dilation_rate=None,\r\n               asymmetric=False,\r\n               decoder=False,\r\n               scope='bottleneck'):\r\n\r\n    #Calculate the depth reduction based on the projection ratio used in 1x1 convolution.\r\n    reduced_depth = int(inputs.get_shape().as_list()[3] / projection_ratio)\r\n\r\n    with slim.arg_scope([prelu], decoder=decoder):\r\n\r\n        #=============DOWNSAMPLING BOTTLENECK====================\r\n        if downsampling:\r\n            #=============MAIN BRANCH=============\r\n            #Just perform a max pooling\r\n            net_main, pooling_indices = tf.nn.max_pool_with_argmax(inputs,\r\n                                                                   ksize=[1,2,2,1],\r\n                                                                   strides=[1,2,2,1],\r\n                                                                   padding='SAME',\r\n                                                                   name=scope+'_main_max_pool')\r\n\r\n            #First get the difference in depth to pad, then pad with zeros only on the last dimension.\r\n            inputs_shape = inputs.get_shape().as_list()\r\n            depth_to_pad = abs(inputs_shape[3] - output_depth)\r\n            paddings = tf.convert_to_tensor([[0,0], [0,0], [0,0], [0, depth_to_pad]])\r\n            net_main = tf.pad(net_main, paddings=paddings, name=scope+'_main_padding')\r\n\r\n            #=============SUB BRANCH==============\r\n            #First projection that has a 2x2 kernel and stride 2\r\n            net = slim.conv2d(inputs, reduced_depth, [2,2], stride=2, scope=scope+'_conv1')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')\r\n            net = prelu(net, scope=scope+'_prelu1')\r\n\r\n            #Second conv block\r\n            net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], scope=scope+'_conv2')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\r\n            net = prelu(net, scope=scope+'_prelu2')\r\n\r\n            #Final projection with 1x1 kernel\r\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\r\n            net = prelu(net, scope=scope+'_prelu3')\r\n\r\n            #Regularizer\r\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\r\n\r\n            #Finally, combine the two branches together via an element-wise addition\r\n            net = tf.add(net, net_main, name=scope+'_add')\r\n            net = prelu(net, scope=scope+'_last_prelu')\r\n\r\n            #also return inputs shape for convenience later\r\n            return net, pooling_indices, inputs_shape\r\n\r\n        #============DILATION CONVOLUTION BOTTLENECK====================\r\n        #Everything is the same as a regular bottleneck except for the dilation rate argument\r\n        elif dilated:\r\n            #Check if dilation rate is given\r\n            if not dilation_rate:\r\n                raise ValueError('Dilation rate is not given.')\r\n\r\n            #Save the main branch for addition later\r\n            net_main = inputs\r\n\r\n            #First projection with 1x1 kernel (dimensionality reduction)\r\n            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')\r\n            net = prelu(net, scope=scope+'_prelu1')\r\n\r\n            #Second conv block --- apply dilated convolution here\r\n            net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], rate=dilation_rate, scope=scope+'_dilated_conv2')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\r\n            net = prelu(net, scope=scope+'_prelu2')\r\n\r\n            #Final projection with 1x1 kernel (Expansion)\r\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\r\n            net = prelu(net, scope=scope+'_prelu3')\r\n\r\n            #Regularizer\r\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\r\n            net = prelu(net, scope=scope+'_prelu4')\r\n\r\n            #Add the main branch\r\n            net = tf.add(net_main, net, name=scope+'_add_dilated')\r\n            net = prelu(net, scope=scope+'_last_prelu')\r\n\r\n            return net\r\n\r\n        #===========ASYMMETRIC CONVOLUTION BOTTLENECK==============\r\n        #Everything is the same as a regular bottleneck except for a [5,5] kernel decomposed into two [5,1] then [1,5]\r\n        elif asymmetric:\r\n            #Save the main branch for addition later\r\n            net_main = inputs\r\n\r\n            #First projection with 1x1 kernel (dimensionality reduction)\r\n            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')\r\n            net = prelu(net, scope=scope+'_prelu1')\r\n\r\n            #Second conv block --- apply asymmetric conv here\r\n            net = slim.conv2d(net, reduced_depth, [filter_size, 1], scope=scope+'_asymmetric_conv2a')\r\n            net = slim.conv2d(net, reduced_depth, [1, filter_size], scope=scope+'_asymmetric_conv2b')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\r\n            net = prelu(net, scope=scope+'_prelu2')\r\n\r\n            #Final projection with 1x1 kernel\r\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\r\n            net = prelu(net, scope=scope+'_prelu3')\r\n\r\n            #Regularizer\r\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\r\n            net = prelu(net, scope=scope+'_prelu4')\r\n\r\n            #Add the main branch\r\n            net = tf.add(net_main, net, name=scope+'_add_asymmetric')\r\n            net = prelu(net, scope=scope+'_last_prelu')\r\n\r\n            return net\r\n\r\n        #============UPSAMPLING BOTTLENECK================\r\n        #Everything is the same as a regular one, except convolution becomes transposed.\r\n        elif upsampling:\r\n            #Check if pooling indices is given\r\n            if pooling_indices == None:\r\n                raise ValueError('Pooling indices are not given.')\r\n\r\n            #Check output_shape given or not\r\n            if output_shape == None:\r\n                raise ValueError('Output depth is not given')\r\n\r\n            #=======MAIN BRANCH=======\r\n            #Main branch to upsample. output shape must match with the shape of the layer that was pooled initially, in order\r\n            #for the pooling indices to work correctly. However, the initial pooled layer was padded, so need to reduce dimension\r\n            #before unpooling. In the paper, padding is replaced with convolution for this purpose of reducing the depth!\r\n            net_unpool = slim.conv2d(inputs, output_depth, [1,1], scope=scope+'_main_conv1')\r\n            net_unpool = slim.batch_norm(net_unpool, is_training=is_training, scope=scope+'batch_norm1')\r\n            net_unpool = unpool(net_unpool, pooling_indices, output_shape=output_shape, scope='unpool')\r\n\r\n            #======SUB BRANCH=======\r\n            #First 1x1 projection to reduce depth\r\n            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\r\n            net = prelu(net, scope=scope+'_prelu1')\r\n\r\n            #Second conv block -----------------------------> NOTE: using tf.nn.conv2d_transpose for variable input shape.\r\n            net_unpool_shape = net_unpool.get_shape().as_list()\r\n            output_shape = [net_unpool_shape[0], net_unpool_shape[1], net_unpool_shape[2], reduced_depth]\r\n            output_shape = tf.convert_to_tensor(output_shape)\r\n            filter_size = [filter_size, filter_size, reduced_depth, reduced_depth]\r\n            filters = tf.get_variable(shape=filter_size, initializer=initializers.xavier_initializer(), dtype=tf.float32, name=scope+'_transposed_conv2_filters')\r\n\r\n            # net = slim.conv2d_transpose(net, reduced_depth, [filter_size, filter_size], stride=2, scope=scope+'_transposed_conv2')\r\n            net = tf.nn.conv2d_transpose(net, filter=filters, strides=[1,2,2,1], output_shape=output_shape, name=scope+'_transposed_conv2')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\r\n            net = prelu(net, scope=scope+'_prelu2')\r\n\r\n            #Final projection with 1x1 kernel\r\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm4')\r\n            net = prelu(net, scope=scope+'_prelu3')\r\n\r\n            #Regularizer\r\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\r\n            net = prelu(net, scope=scope+'_prelu4')\r\n\r\n            #Finally, add the unpooling layer and the sub branch together\r\n            net = tf.add(net, net_unpool, name=scope+'_add_upsample')\r\n            net = prelu(net, scope=scope+'_last_prelu')\r\n\r\n            return net\r\n\r\n        #OTHERWISE, just perform a regular bottleneck!\r\n        #==============REGULAR BOTTLENECK==================\r\n        #Save the main branch for addition later\r\n        net_main = inputs\r\n\r\n        #First projection with 1x1 kernel\r\n        net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')\r\n        net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')\r\n        net = prelu(net, scope=scope+'_prelu1')\r\n\r\n        #Second conv block\r\n        net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], scope=scope+'_conv2')\r\n        net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\r\n        net = prelu(net, scope=scope+'_prelu2')\r\n\r\n        #Final projection with 1x1 kernel\r\n        net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\r\n        net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\r\n        net = prelu(net, scope=scope+'_prelu3')\r\n\r\n        #Regularizer\r\n        net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\r\n        net = prelu(net, scope=scope+'_prelu4')\r\n\r\n        #Add the main branch\r\n        net = tf.add(net_main, net, name=scope+'_add_regular')\r\n        net = prelu(net, scope=scope+'_last_prelu')\r\n\r\n        return net\r\n\r\n#Now actually start building the network\r\ndef ENet(inputs,\r\n         num_classes,\r\n         batch_size,\r\n         num_initial_blocks=1,\r\n         stage_two_repeat=2,\r\n         skip_connections=True,\r\n         reuse=None,\r\n         is_training=True,\r\n         scope='ENet'):\r\n\r\n    #Set the shape of the inputs first to get the batch_size information\r\n    inputs_shape = inputs.get_shape().as_list()\r\n    inputs.set_shape(shape=(batch_size, inputs_shape[1], inputs_shape[2], inputs_shape[3]))\r\n\r\n    with tf.variable_scope(scope, reuse=reuse):\r\n        #Set the primary arg scopes. Fused batch_norm is faster than normal batch norm.\r\n        with slim.arg_scope([initial_block, bottleneck], is_training=is_training),\\\r\n             slim.arg_scope([slim.batch_norm], fused=True), \\\r\n             slim.arg_scope([slim.conv2d, slim.conv2d_transpose], activation_fn=None): \r\n            #=================INITIAL BLOCK=================\r\n            net = initial_block(inputs, scope='initial_block_1')\r\n            for i in xrange(2, max(num_initial_blocks, 1) + 1):\r\n                net = initial_block(net, scope='initial_block_' + str(i))\r\n\r\n            #Save for skip connection later\r\n            if skip_connections:\r\n                net_one = net\r\n\r\n            #===================STAGE ONE=======================\r\n            net, pooling_indices_1, inputs_shape_1 = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, downsampling=True, scope='bottleneck1_0')\r\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_1')\r\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_2')\r\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_3')\r\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_4')\r\n\r\n            #Save for skip connection later\r\n            if skip_connections:\r\n                net_two = net\r\n\r\n            #regularization prob is 0.1 from bottleneck 2.0 onwards\r\n            with slim.arg_scope([bottleneck], regularizer_prob=0.1):\r\n                net, pooling_indices_2, inputs_shape_2 = bottleneck(net, output_depth=128, filter_size=3, downsampling=True, scope='bottleneck2_0')\r\n                \r\n                #Repeat the stage two at least twice to get stage 2 and 3:\r\n                for i in xrange(2, max(stage_two_repeat, 2) + 2):\r\n                    net = bottleneck(net, output_depth=128, filter_size=3, scope='bottleneck'+str(i)+'_1')\r\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=2, scope='bottleneck'+str(i)+'_2')\r\n                    net = bottleneck(net, output_depth=128, filter_size=5, asymmetric=True, scope='bottleneck'+str(i)+'_3')\r\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=4, scope='bottleneck'+str(i)+'_4')\r\n                    net = bottleneck(net, output_depth=128, filter_size=3, scope='bottleneck'+str(i)+'_5')\r\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=8, scope='bottleneck'+str(i)+'_6')\r\n                    net = bottleneck(net, output_depth=128, filter_size=5, asymmetric=True, scope='bottleneck'+str(i)+'_7')\r\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=16, scope='bottleneck'+str(i)+'_8')\r\n\r\n            with slim.arg_scope([bottleneck], regularizer_prob=0.1, decoder=True):\r\n                #===================STAGE FOUR========================\r\n                bottleneck_scope_name = \"bottleneck\" + str(i + 1)\r\n\r\n                #The decoder section, so start to upsample.\r\n                net = bottleneck(net, output_depth=64, filter_size=3, upsampling=True,\r\n                                 pooling_indices=pooling_indices_2, output_shape=inputs_shape_2, scope=bottleneck_scope_name+'_0')\r\n\r\n                #Perform skip connections here\r\n                if skip_connections:\r\n                    net = tf.add(net, net_two, name=bottleneck_scope_name+'_skip_connection')\r\n\r\n                net = bottleneck(net, output_depth=64, filter_size=3, scope=bottleneck_scope_name+'_1')\r\n                net = bottleneck(net, output_depth=64, filter_size=3, scope=bottleneck_scope_name+'_2')\r\n\r\n                #===================STAGE FIVE========================\r\n                bottleneck_scope_name = \"bottleneck\" + str(i + 2)\r\n\r\n                net = bottleneck(net, output_depth=16, filter_size=3, upsampling=True,\r\n                                 pooling_indices=pooling_indices_1, output_shape=inputs_shape_1, scope=bottleneck_scope_name+'_0')\r\n\r\n                #perform skip connections here\r\n                if skip_connections:\r\n                    net = tf.add(net, net_one, name=bottleneck_scope_name+'_skip_connection')\r\n\r\n                net = bottleneck(net, output_depth=16, filter_size=3, scope=bottleneck_scope_name+'_1')\r\n\r\n            #=============FINAL CONVOLUTION=============\r\n            logits = slim.conv2d_transpose(net, num_classes, [2,2], stride=2, scope='fullconv')\r\n            probabilities = tf.nn.softmax(logits, name='logits_to_softmax')\r\n\r\n        return logits, probabilities\r\n\r\n\r\ndef ENet_arg_scope(weight_decay=2e-4,\r\n                   batch_norm_decay=0.1,\r\n                   batch_norm_epsilon=0.001):\r\n\r\n  # Set weight_decay for weights in conv2d and separable_conv2d layers.\r\n  with slim.arg_scope([slim.conv2d],\r\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\r\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\r\n\r\n    # Set parameters for batch_norm.\r\n    with slim.arg_scope([slim.batch_norm],\r\n                        decay=batch_norm_decay,\r\n                        epsilon=batch_norm_epsilon) as scope:\r\n      return scope\r\n```\r\n</div></details><br>", "comments": ["I close this issue due to the emergence of high performance models such as Deeplab v3 and Deeplab-slim."]}, {"number": 23319, "title": "tflite_convert - ValueError: Invalid tensors 'input' were found.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\nInstalled from binary.\r\n- TensorFlow version: Apparently this problem exists in all versions.  The versions I have tested: 1.5 | 1.11 | 1.12.0-rc2 | tf-nightly\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: I have the CPU version.\r\n- GPU model and memory: I have the CPU version.\r\n\r\n**Describe the current behavior**\r\nI want to convert Mobilenet_0.50_224 frozen graph using tflite_convert or toco to a TensorFlow Lite model, but I get the following error:\r\n`2018-10-27 21:31:49.213953: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nTraceback (most recent call last):\r\n  File \"/home/ramtin/Desktop/Model/venv/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 100, in _convert_model\r\n    converter = _get_toco_converter(flags)\r\n  File \"/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 87, in _get_toco_converter\r\n    return converter_fn(**converter_kwargs)\r\n  File \"/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 283, in from_frozen_graph\r\n    sess.graph, input_arrays)\r\n  File \"/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py\", line 189, in get_tensors_from_tensor_names\r\n    \",\".join(invalid_tensors)))\r\nValueError: Invalid tensors 'input' were found.\r\n`\r\n**Describe the expected behavior**\r\n tflite_convert could be able to convert the frozen graph to a tflite model successfully.\r\n**Code to reproduce the issue**\r\nI run the following command:\r\n```\r\ntflite_convert \\\r\n  --output_file=foo.tflite \\\r\n  --graph_def_file=frozen_graph.pb \\\r\n  --input_arrays=input \\\r\n  --output_arrays=MobilenetV1/Predictions/Reshape_1\r\n```\r\n\r\n**Other info / logs**\r\nI have tried this on both the default Mobilenet_0.50_224 frozen graph and the retrained graph which is the output of retrain.py script from [Tensorflow for poets 2](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/scripts/retrain.py).\r\nI also tried to use toco, but the result was the same.\r\nHowever, I can optimize the graph for Tensorflow Mobile without any  problem.\r\n", "comments": ["Update: I installed \"Ubuntu 16.04.3 LTS\" and now the problem is solved.", "I have this error on WINDOW 10"]}, {"number": 23318, "title": "tf.nn.conv2d_transpose with the same inputs produces different outputs on different calls on GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS (Xenial Xerus)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.10.1-0-g4dcfddc\r\n- Python version: Python 3.6.5\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): 5.4.0-6ubuntu1~16.04.10\r\n- CUDA/cuDNN version: V9.0.176/7.1.4.18\r\n- GPU model and memory: GeForce GTX 1080 Ti with 12GB memory\r\n\r\n**Describe the current behavior**\r\n`tf.nn.conv2d_transpose` with the same inputs produces different outputs on different calls on GPU.\r\n\r\n**Describe the expected behavior**\r\n`tf.nn.conv2d_transpose` with the same inputs is expected to produce the same outputs on different calls on GPU.\r\n\r\n**Code to reproduce the issue**\r\nThe code below\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\n\r\nx = tf.random_uniform([1, 1024, 32, 32])\r\nfilter = tf.random_uniform([4, 4, 96, 1024])\r\n\r\ny1 = tf.nn.conv2d_transpose(x, filter, [1, 96, 64, 64], strides=[1, 1, 2, 2], padding='SAME', data_format='NCHW')\r\ny2 = tf.nn.conv2d_transpose(x, filter, [1, 96, 64, 64], strides=[1, 1, 2, 2], padding='SAME', data_format='NCHW')\r\n\r\n# show difference between y1 and y2\r\nprint(tf.reduce_mean(tf.abs(y1 - y2)))\r\n```\r\n\r\nproduces a non-zero result like\r\n\r\n```\r\ntf.Tensor(8.213489e-06, shape=(), dtype=float32)\r\n```\r\n\r\n**Other info / logs**\r\n`tf.nn.conv2d_transpose` with the same inputs produces the same outputs on different calls on **CPU**.", "comments": ["@aselle Assigning to you since you were working on a [similar issue](https://github.com/tensorflow/tensorflow/issues/11882) earlier. I have tested the above snippet in TF 1.12 and it still persists. Thanks!", "This is not a good match for me. Probably some GPU CudNN expert. Maybe reedwm?", "@reedwm Can you please take a look at this issue? Thanks!", "This is likely due to the fact `conv2d_transpose` is non-deterministic on the GPU. @ekelsen, can you confirm?", "Yes, I believe it can be non-deterministic.  But the auto-tuner is also non-deterministic.  So basically all bets are always off right now.", "@aiueogawa I think it was resolved. As mentioned above, non-deterministic nature of some ops results in minutely different results for the same input on GPU. Please close the issue if it was resolved. Thanks!", "I think it was resolved. I am closing the issue. Please open a new ticket if you see similar issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23318\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23318\">No</a>\n"]}, {"number": 23317, "title": "Add scaling op and transform to contrib.image", "body": "This add scale and scales_to_projective_transforms ops to contrib.image. This is useful for efficient data augmentation since we can compose transformations.", "comments": ["Hello, can someone review this ?", "@ringw  -  Could you please review this PR and approve it or suggest changes(if required)", "@drauh sorry we sat on this for so long. We're working through our backlog of issues and PRs.\r\n\r\nWith contrib deprecated I'd rather not take this PR. tensorflow/addons might be a good place for it, but I'm.a little sceptical that these functions add a lot of value over transform(). \r\n\r\nI will close this PR. "]}, {"number": 23316, "title": "Wrong gradients in combination with placeholders.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 14.04.5 LTS`, `Debian 4.18.10-2`, `Archlinux`\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v1.4.0-19-ga52c8d9 1.4.1`, `v1.10.1-0-g4dcfddc5d1 1.10.1`, `1.10.0`, \r\n- Python version: `3.6.0`, `3.4.3`, `3.7.0`\r\n- CUDA/cuDNN version: `9.0/7.1.2`, `8.0.61/6.0.21`, `CPU only`\r\n- GPU model and memory: Titan 12GB\r\n\r\n**Describe the current behavior**\r\nIncorrect gradient computation when placeholder weights are involved. In a situation where two variables are involved, optimization of one of the variables leads to wrong gradients for the other variable when placeholders weights are used.\r\n\r\n**Describe the expected behavior**\r\nGradients should be correct and there should be no difference in gradients whether a placeholder or another variable is used.\r\n\r\n**Code to reproduce the issue**\r\nHere is a minimal example to reproduce the issue (the case of `indirect = True` can be used as a workaround and shows the expected output in contrast of `indirect = False`):\r\n\r\n```\r\nimport tensorflow as tf                                                                                   \r\n                                                                                                          \r\ndef test(indirect):                                                                                       \r\n    print(\"indirect:\", indirect)                                                                          \r\n    a = tf.Variable(1.0)                                                                                  \r\n    b = tf.Variable(2.0)                                                                                  \r\n    weight_var = tf.Variable(3.0, trainable = False)                                                      \r\n                                                                                                          \r\n    weight_placeholder = tf.placeholder(tf.float32)                                                       \r\n    if indirect:                                                                                          \r\n        weight = tf.assign(weight_var, weight_placeholder)                                                \r\n    else:                                                                                                 \r\n        weight = weight_placeholder                                                                       \r\n                                                                                                          \r\n    common_loss = tf.square(a*b)                                                                          \r\n    b_only_loss = tf.square(b)                                                                            \r\n                                                                                                          \r\n    common_grads = tf.gradients(                                                                          \r\n            common_loss,                                                                                  \r\n            [b])                                                                                          \r\n    b_only_grads = tf.gradients(                                                                          \r\n            weight*b_only_loss,                                                                           \r\n            [b])                                                                                          \r\n    summed_grads = tf.gradients(                                                                          \r\n            common_loss + weight*b_only_loss,                                                             \r\n            [b])                                                                                          \r\n    grad_diff = tf.abs(common_grads[0] + b_only_grads[0] - summed_grads[0])                               \r\n                                                                                                          \r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)                                      \r\n    opt_a = optimizer.minimize(common_loss, var_list=[a])                                                 \r\n                                                                                                          \r\n    feed={weight_placeholder: 0.0}                                                                        \r\n    fetch={\"common_grads\": common_grads, \"b_only_grads\": b_only_grads,                                    \r\n           \"summed_grads\": summed_grads, \"grad_diff\": grad_diff,                                          \r\n           \"opt_a\": opt_a}                                                                                \r\n    s = tf.Session()                                                                                      \r\n    s.run(tf.global_variables_initializer())                                                              \r\n    r = s.run(fetch, feed)                                                                                \r\n    for k in sorted(r):                                                                                   \r\n        print(\"{:16}\".format(k), r[k])                                                                    \r\n    assert r[\"grad_diff\"] < 1e-11, r[\"grad_diff\"]\r\n                                                                                                          \r\n                                                                                                          \r\nif __name__ == \"__main__\":                                                                                \r\n    print(\"tensorflow\", tf.GIT_VERSION, tf.VERSION)                                                       \r\n    test(True)                                                                                            \r\n    test(False)\r\n```\r\n\r\nExample output:\r\n\r\n```\r\ntensorflow b'unknown' 1.10.0\r\nindirect: True\r\nb_only_grads     [0.0]\r\ncommon_grads     [4.0]\r\ngrad_diff        0.0\r\nopt_a            None\r\nsummed_grads     [4.0]\r\nindirect: False\r\nb_only_grads     [0.0]\r\ncommon_grads     [4.0]\r\ngrad_diff        32.0\r\nopt_a            None\r\nsummed_grads     [-28.0]\r\nTraceback (most recent call last):\r\n  File \"minimal_example.py\", line 47, in <module>\r\n    test(False)\r\n  File \"minimal_example.py\", line 41, in test\r\n    assert r[\"grad_diff\"] < 1e-11, r[\"grad_diff\"]\r\nAssertionError: 32.0\r\n```\r\n\r\n**Other info / logs**\r\nThe output is not deterministic and sometimes produces correct results so the code might have to be run multiple times. The code also works fine without the `opt_a` call. Constant values for the weight also work fine, just in combination with a placeholder it seems to be problematic. The problem remains the same for other values of the weight and thus in general violates commutativity of the gradient but the case of a weight of zero illustrates the problem best because the zero weighted summand really should not have any influence on the gradients. Finally, the gradients are not just reported falsely but optimization of `b`, e.g. `optimizer.minimize(common_loss + weight*b_only_loss, var_list=[b])`, produces wrong results (i.e. gradient descent uses wrong gradients).\r\n", "comments": ["@pesser,\r\nCan you please let us know if we can close this issue, as the **`Placeholders`** are not relevant in **`Tensorflow Versions, 2.x`**? Thanks!", "The issue persists with recent versions and `tf.compat.v1` but it can be closed from my side since I'm not relying on it.", "@pesser,\r\nYour code could be executed successfully without any **`Assertion Error`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/384d9ff5857dfcbd74801ea1ec2d9b3d/gh_23316.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23316\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23316\">No</a>\n"]}, {"number": 23315, "title": "[surface book1] What version of CUDA should be installed on the Microsoft GeForce 940MX?", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information** windows 10\r\n- TensorFlow version: 1.11\r\n- Doc Link: https://www.tensorflow.org/install/install_windows?hl=zh-cn\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Use the last version", "latest ? \r\n9.0 ?", "ok, thanks. i will try it.", "The latest maybe 10.0...............", "i'm trying this blog. https://www.anaconda.com/blog/developer-blog/tensorflow-in-anaconda/\r\nit's good way to install tensorflow-gpu.", "> latest ?\r\n> 9.0 ?\r\n\r\nYes, that one"]}, {"number": 23314, "title": "Changing learning rate of the optimizer in eager mode", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.11\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: K80 \r\n\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using `tf.keras` with `eager` execution enabled, I don't have the control over learning rate of the optimizer i.e. I am unable to change the learning rate on the fly. If you are using pure `keras` with no `eager` mode, then you can change the learning rate of the keras optimizer in the following way \r\n\r\n`K.set_value(self.mode.optimizer.lr, value)`\r\n\r\nBut as the `keras` optimizers aren't supported in `eager` mode, I don't have any control over the learning rate of my optimizer.\r\n\r\n**Describe the expected behavior**\r\nThe expected behavior is that I should be able to change the learning rate of my optimizer after a batch ends or after an epoch ends\r\n\r\n\r\n", "comments": ["Here's a quick example of setting the LR using a callable, w/ the Gradient Tape way I mentioned on Twitter: https://colab.research.google.com/drive/10OIcUdwRiHbQK4p32MEABG_G4_D44YKH\r\n\r\nThis probably isn't quite what you meant -- could you add a code snippet showing the behavior you'd like?", "Yes, that can be another way to tweak the LR within batches/epochs. If I don't use `tf.keras` and `eager`, and switch to pure `Keras` instead with `tf` backend., I can do this in a much cleaner way. Take a look at this: https://colab.research.google.com/drive/1I-vvsD4zhBUxcMh6LEadqUOF9sVgHy2T#scrollTo=t9ZHnrWefHuq", "Is this still an issue ?\r\n", "Yes and No. But Josh and Francois have made it clear that the optimizers are going to be merged in TF 2.0 very soon.", "@AakashKumarNain  - Do you want to keep this open until TF 2.0 ? My suggestion is to open a new issue(if it still exists after TF 2.0) with the latest data."]}, {"number": 23313, "title": "(roadmap request)python 3.7 and windows build roadmap", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12.0rc\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nbuild tensorflow with python 3.7 gets error ( related to google protobuf c++ code  ), windows bazel build can't recognize path\r\n**Will this change the current api? How?** \r\nno\r\n**Who will benefit with this feature?**\r\nalmost everyone\r\n**Any Other info.**\r\n\r\nprotobuf for python 3.7 is stable and published on pip, and tensorflow is not compatible with that.\r\nwindows bazel-build has path-related error( which is an exact path ), maybe this is caused by different path-mechanism and/or regional(korean, in my case) encoding-related\r\n\r\ncould you tensorflowers give us specific roadmap for python 3.7 compatibility and windows 10(for all countries) build support roadmap?", "comments": ["Once all dependencies are compatible with python 3.7, new version will be released. If I'm right, the developers will finish this by TensorFlow 1.13.\r\n\r\nAlso see #20517.", "@alanpurple  -  Hi, could you please check the latest comment by @fo40225  in the issue 20517.\r\nAlso feel free to close this issue as we are maintaining a single issue for Python 3.7 compatibility with TF. Thanks !"]}, {"number": 23312, "title": "Bug: `tf.cumsum` is not numerically stable when sequence is long.", "body": "\r\n**System information**\r\n- Have I written custom code: yes, but very little\r\n- OS Platform and Distribution: CentOS 5\r\n- TensorFlow version (use command below): both 1.18 and 1.12\r\n- Python version: both python 2 and 3\r\n- GPU model and memory: TitanX and 1080Ti\r\n\r\n**Describe the current behavior**\r\n`cumsum` has small (possibly rounding or something similar) errors, and these errors accumulates quickly as the sequence length increase, e.g. a time sequence of length 1000. \r\n\r\n**Describe the expected behavior**\r\n`cumsum` should be as accurate as the `float32` precision.\r\n\r\n**Code to reproduce the issue**\r\nHere is the colab gist to reproduce the result\r\n\r\nhttps://colab.research.google.com/gist/rex-yue-wu/cb19dc2af6f8709002b6d412611e972a/unstable-tf-cumsum.ipynb\r\n", "comments": ["@azaks2 can you or someone else take a look?", "A simple cumsum of numbers [0 .. 5793] already starts having a significant error due to precision.\r\n>>> numpy.float32(16776528 + 5793)\r\n16782320.0\r\n>>> numpy.float64(16776528 + 5793)\r\n16782321.0\r\n\r\nIt might be that other implementation (non Eigen) are smarter and do accumulation in 64 bit, so the error does not accumulate. Note you can use K.set_floatx('float64') to get the necessary precision\r\n", "Nagging Assignee @azaks2: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 23311, "title": "Hey. Just saw that the vocabulary size is a parameter you have to define manually? Seems not very intuitive for me as it can be computed automatically.", "body": "_Originally posted by @b-3-n in https://github.com/tensorflow/tensorflow/issues/2734#issuecomment-224732629_", "comments": ["I am  facing \" tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[47] = 20000 is not in [0, 20000)\" error ,similar to what @b-3-n(you) was facing and I am unable to resolve it,please eleborate your solution if you have one.\r\n\r\nMode : train\r\n\r\nPreparing data in working_dir/\r\nCreating 3 layers of 256 units.\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nCreated model with fresh parameters.\r\nReading development and training data (limit: 0).\r\n  reading data line 100000\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: indices[47] = 20000 is not in [0, 20000)\r\n         [[Node: model_with_buckets/embedding_attention_seq2seq_2/rnn/embedding_wrapper_14/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding_attention_seq2seq/rnn/embedding_wrapper/embedding/read\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/rnn/embedding_wrapper/embedding/read, model_with_buckets/embedding_attention_seq2seq_2/rnn/embedding_wrapper_14/Reshape)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"execute.py\", line 352, in <module>\r\n  File \"execute.py\", line 180, in train\r\n    previous_losses.append(loss)\r\n  File \"C:\\Users\\makke\\ml\\tensor\\seq2seq_model.py\", line 230, in step\r\n    outputs = session.run(output_feed, input_feed)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: indices[47] = 20000 is not in [0, 20000)\r\n         [[Node: model_with_buckets/embedding_attention_seq2seq_2/rnn/embedding_wrapper_14/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding_attention_seq2seq/rnn/embedding_wrapper/embedding/read\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/rnn/embedding_wrapper/embedding/read, model_with_buckets/embedding_attention_seq2seq_2/rnn/embedding_wrapper_14/Reshape)]]\r\n\r\nCaused by op 'model_with_buckets/embedding_attention_seq2seq_2/rnn/embedding_wrapper_14/embedding_lookup', defined at:\r\n  File \"execute.py\", line 352, in <module>\r\n  File \"execute.py\", line 148, in train\r\n  File \"execute.py\", line 109, in create_model\r\n    print(\"Created model with fresh parameters.\")\r\n  File \"C:\\Users\\makke\\ml\\tensor\\seq2seq_model.py\", line 158, in __init__\r\n    softmax_loss_function=softmax_loss_function)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\legacy_seq2seq\\python\\ops\\seq2seq.py\", line 1180, in model_with_buckets\r\n    decoder_inputs[:bucket[1]])\r\n  File \"C:\\Users\\makke\\ml\\tensor\\seq2seq_model.py\", line 157, in <lambda>\r\n    lambda x, y: seq2seq_f(x, y, False),\r\n  File \"C:\\Users\\makke\\ml\\tensor\\seq2seq_model.py\", line 121, in seq2seq_f\r\n    feed_previous=do_decode)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\legacy_seq2seq\\python\\ops\\seq2seq.py\", line 850, in embedding_attention_seq2seq\r\n    encoder_cell, encoder_inputs, dtype=dtype)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\core_rnn.py\", line 197, in static_rnn\r\n    (output, state) = call_cell()\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\core_rnn.py\", line 184, in <lambda>\r\n    call_cell = lambda: cell(input_, state)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\core_rnn_cell_impl.py\", line 595, in __call__\r\n    embedding, array_ops.reshape(inputs, [-1]))\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\", line 111, in embedding_lookup\r\n    validate_indices=validate_indices)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1359, in gather\r\n    validate_indices=validate_indices, name=name)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\makke\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): indices[47] = 20000 is not in [0, 20000)\r\n         [[Node: model_with_buckets/embedding_attention_seq2seq_2/rnn/embedding_wrapper_14/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding_attention_seq2seq/rnn/embedding_wrapper/embedding/read\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/rnn/embedding_wrapper/embedding/read, model_with_buckets/embedding_attention_seq2seq_2/rnn/embedding_wrapper_14/Reshape)]]\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23310, "title": "Win10 + Anaconda3  cannot install tensorflow-gpu", "body": "Hi All,\r\nI'm new to setup GPU environment. Has been stuck for a few hours, hope to get some solutions.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- TensorFlow installed from (source or binary): pip install --ignore-installed --upgrade tensorflow-gpu\r\n- Python version: Python 3.5.3\r\n- CUDA/cuDNN version: CUDA 9.0.176,  cudnn-9.0-windows10-x64-v7.3.1.20\r\n- GPU model and memory: NVIDIA GeForce GTX 1050, 4GB\r\n\r\n\r\n**Describe the current behavior**\r\nCannot install. Error message as below. \r\n\r\n(PY35) C:\\>pip install --ignore-installed --upgrade tensorflow-gpu\r\nCollecting tensorflow-gpu\r\n  Using cached https://files.pythonhosted.org/packages/43/93/07f5cae2c8e02b37c4d64fa9c9eb1f8d3b39128247d0c1acd9110da45f67/tensorflow_gpu-1.11.0-cp35-cp35m-win_amd64.whl\r\nCollecting six>=1.10.0 (from tensorflow-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\r\nCollecting tensorboard<1.12.0,>=1.11.0 (from tensorflow-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/9b/2f/4d788919b1feef04624d63ed6ea45a49d1d1c834199ec50716edb5d310f4/tensorboard-1.11.0-py3-none-any.whl\r\nCollecting grpcio>=1.8.6 (from tensorflow-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/5e/8c/da9316699398607a22c91e39e16e4c0f3e8233e0faa88ed52df736f2b1d6/grpcio-1.16.0-cp35-cp35m-win_amd64.whl\r\nCollecting keras-preprocessing>=1.0.3 (from tensorflow-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/fc/94/74e0fa783d3fc07e41715973435dd051ca89c550881b3454233c39c73e69/Keras_Preprocessing-1.0.5-py2.py3-none-any.whl\r\nCollecting termcolor>=1.1.0 (from tensorflow-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\r\n    Complete output from command python setup.py egg_info:\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"c:\\users\\ericx\\appdata\\local\\conda\\conda\\envs\\py35\\lib\\site-packages\\setuptools\\__init__.py\", line 191, in <module>\r\n        monkey.patch_all()\r\n      File \"c:\\users\\ericx\\appdata\\local\\conda\\conda\\envs\\py35\\lib\\site-packages\\setuptools\\monkey.py\", line 101, in patch_all\r\n        patch_for_msvc_specialized_compiler()\r\n      File \"c:\\users\\ericx\\appdata\\local\\conda\\conda\\envs\\py35\\lib\\site-packages\\setuptools\\monkey.py\", line 138, in patch_for_msvc_specialized_compiler\r\n        msvc = import_module('setuptools.msvc')\r\n      File \"c:\\users\\ericx\\appdata\\local\\conda\\conda\\envs\\py35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n        return _bootstrap._gcd_import(name[level:], package, level)\r\n      File \"c:\\users\\ericx\\appdata\\local\\conda\\conda\\envs\\py35\\lib\\site-packages\\setuptools\\msvc.py\", line 58, in <module>\r\n        from distutils.msvc9compiler import Reg\r\n      File \"c:\\users\\ericx\\appdata\\local\\conda\\conda\\envs\\py35\\lib\\distutils\\msvc9compiler.py\", line 254\r\n        log.debug(\"Unable to find vcvarsall.bat Eric edit C:\\Users\\ericx\\AppData\\Local\\conda\\conda\\envs\\r-tensorflow\\Lib\\site-packages\\numpy\\distutils\")\r\n                 ^\r\n    SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 41-42: truncated \\UXXXXXXXX escape\r\n\r\n    ----------------------------------------\r\nCommand \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\ericx\\AppData\\Local\\Temp\\pip-install-mxk6dgpl\\termcolor\\\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nCannot install, every time same error. ", "comments": ["Hi @ericxumit \r\nI'm not a tensorflow pro either (yet). To me it looks like you need to install Visual Studio.\r\nSimply google your error \"Unable to find vcvarsall.bat\" and find for example this one:\r\nhttps://blogs.msdn.microsoft.com/pythonengineering/2016/04/11/unable-to-find-vcvarsall-bat/\r\n", "@aaniin Hi Hans, thanks for the nice input. You are definitely right, the Visual Studio was not installed properly. I spent another two hours to fix it yesterday night (or this morning). Windows is really making trouble to config, now know why many people prefer Linux (though I'm not there yet, maybe moving to).\r\n\r\nA little bit more sharing on how I fixed the issue, since the situation was not 100% the same as stated online. It's a big weird somewhere.\r\n\r\n1. I've installed the VS2015 but didn't include VC++ in my installation. Note the new VS installer changed totally, we can only install 2017, which won't support many structure so I still go back to find 2015 package on line for installation. Refer to this article to add VC++ back to my installation, the picture helped me a lot since I'm not a developer and not familiar with those installation configurations. [https://social.msdn.microsoft.com/Forums/en-US/1071be0e-2a46-4c30-9546-ea9d7c4755fa/where-is-vcvarsallbat-file?forum=visualstudiogeneral](url)\r\n\r\n\r\n2. In the environment variables, under system variables, \r\n- add system variable VS90COMNTOOLS = %VS140COMNTOOLS%\r\n- add system variable VS140COMNTOOLS = C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ , not the C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools as many articles introduced.\r\n\r\n\r\n3. This is the weird piece. Though I corrected Visual Studio installation and add path to the environment, installation still failed, even after power cycle. I have to trace back to the msvc9compiler.py which locates in conda\\conda\\pkgs\\python-3.5.3-3\\Lib\\distutils, and look at the code. I was thinking about to direct to the path of vcvarsall.bat directly in this python file, however before doing that I wrote one line of code to print where vcvarsall path is to try to trace what's wrong. The issue just get resolved and my installation passed... \r\n\r\nIt's interesting why the system behaves in this way, any ideas? But anyway my problem get resolved and hope the process gives hints to other guys who might face the same issue. \r\n\r\n\r\n", "@ericxumit - Appreciate your efforts to post the steps here. "]}, {"number": 23309, "title": "Cannot build model on windows 8 - valueerror", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 8.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):https://www.tensorflow.org/install/\r\n- TensorFlow version:1.11\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):No idea\r\n- CUDA/cuDNN version:No\r\n- GPU model and memory:No\r\n\r\n**Describe the problem**\r\n  File \"Text_generation01.py\", line 109, in <module>\r\n    model.build(tf.TensorShape([BATCH_SIZE, seq_length]))\r\n  File \"C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\t\r\nensorflow\\python\\keras\\engine\\network.py\", line 788, in build\r\n    raise ValueError('Currently, you cannot build your model if it has '\r\nValueError: Currently, you cannot build your model if it has positional or keywo\r\nrd arguments that are not inputs to the model, but are required for its `call` m\r\nethod. Instead, in order to instantiate and build your model, `call` your model\r\non real tensor data with all expected call arguments.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nmodel.build(tf.TensorShape([BATCH_SIZE, seq_length]))\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@SonamAwachar  - Install Visual Studio 2015 since you're using Windows 8", "Hi Harshini,\n\nThanks for getting back to me. Actually I tried it on Linux using Conda and\nits working fine. Sure i will keep this thing in mind for future\nprojects.Thanks\n\nSonam\n\nOn Thu, Nov 1, 2018 at 5:00 PM harshini-gadige <notifications@github.com>\nwrote:\n\n> @SonamAwachar <https://github.com/SonamAwachar> - Install Visual Studio\n> 2015 since you're using Windows 8\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23309#issuecomment-435200927>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Ah9PvEMs9ZvcJ8B3twPF7BNQYyhFrqHCks5uq278gaJpZM4X9MXu>\n> .\n>\n\n\n-- \nThanks\nSonam Awachar\n+91-9763478749\n"]}, {"number": 23308, "title": "Cannot import tensorflow into my file. Error message suggests that this is a build issue I think. ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \"pip install tensorflow\" from cmd\r\n- TensorFlow version: 1.11.0\r\n- Python version: 3.6.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am getting an error message when trying to import tensorflow in my python file. I will post the error message below. I have uninstalled and reinstalled tensorflow using pip multiple times (like 3) using the same command every time : \"pip install tensorflow\"\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI installed tensorflow using pip in cmd using the command \"pip install tensorflow.\" I am running this in my x64 laptop and this device does not have a GPU. Tensorflows says that it installs correctly. However, when I open my python file in my text editor and I type \"import tensorflow\" and then I try to run the code, it gives me a long error message. \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThis is the error message I recieve in the terminal when trying to run my python file. Note that even though I do not have a gpu and did not install it using pip with the gpu configuration, it gives me an error that other people get trying to install the that uses their GPU. I don't know if that's important or not. \r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\18ngc\\Documents\\python-workspace\\workshop\\__main__.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\18ngc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@ChandlerMunro  - Hi, please refer [this link](https://kb.froglogic.com/display/KB/Article+-+Errors+with+%28third+party%29+Python+modules) which helps you to resolve the issue. \r\n\r\nYou may also refer these : #17386  #19584", "I think the problem was my cpu does not support the new AVX instructions. I found a tensorflow 1.0.0 on github and installed that instead of \"pip instasll tensorflow\". Now, tensorflow imports fine. \r\nSo the new tensorflow packages no longer work with cpu's that don't have avx instructions? I am guessing this is what is causing these problems. "]}, {"number": 23307, "title": "Missing Documentation: Multiple links return 404 in Python API Guide", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: Version 2.0\r\n- Doc Link: The following docs all have links that return a 404 error:\r\n\r\n  - [tensorflow/contrib/util/\\_\\_init\\_\\_.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/util/__init__.py)\r\n    - [https://www.tensorflow.org/api_guides/python/contrib.util](https://www.tensorflow.org/api_guides/python/contrib.util)\r\n  - [tensorflow/python/lib/io/python_io.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/python_io.py)\r\n    - [https://www.tensorflow.org/api_guides/python/python_io](https://www.tensorflow.org/api_guides/python/python_io)\r\n  - [tensorflow/python/client/client_lib.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/client_lib.py)\r\n    - [https://www.tensorflow.org/api_guides/python/client](https://www.tensorflow.org/api_guides/python/client)\r\n  - [tensorflow/contrib/crf/\\_\\_init\\_\\_.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/crf/\\_\\_init\\_\\_.py)\r\n    - [https://www.tensorflow.org/api_guides/python/contrib.crf](https://www.tensorflow.org/api_guides/python/contrib.crf)\r\n  - [tensorflow/python/debug/\\_\\_init\\_\\_.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/__init__.py)\r\n    - [https://tensorflow.org/api_guides/python/tfdbg](https://tensorflow.org/api_guides/python/tfdbg)\r\n  - [tensorflow/contrib/rnn/\\_\\_init\\_\\_.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/__init__.py)\r\n    - [https://tensorflow.org/api_guides/python/contrib.rnn](https://tensorflow.org/api_guides/python/contrib.rnn)\r\n  - [tensorflow/core/api_def/base_api/api_def_SegmentMax.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SegmentMax.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_SegmentMean.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SegmentMean.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_SegmentMin.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SegmentMin.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_SegmentProd.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SegmentProd.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_SegmentSum.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SegmentSum.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_UnsortedSegmentMax.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_UnsortedSegmentMax.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_UnsortedSegmentSum.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_UnsortedSegmentSum.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_SparseSegmentSqrtN.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SparseSegmentSqrtN.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_SparseSegmentMean.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SparseSegmentMean.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_SparseSegmentMeanWithNumSegments.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SparseSegmentMeanWithNumSegments.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_SparseSegmentSqrtNWithNumSegments.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SparseSegmentSqrtNWithNumSegments.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_SparseSegmentSum.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SparseSegmentSum.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_SparseSegmentSumWithNumSegments.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SparseSegmentSumWithNumSegments.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_UnsortedSegmentMin.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_UnsortedSegmentMin.pbtxt)\r\n  - [tensorflow/core/api_def/base_api/api_def_UnsortedSegmentProd.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_UnsortedSegmentProd.pbtxt)\r\n    - [https://tensorflow.org/api_guides/python/math_ops#Segmentation](https://tensorflow.org/api_guides/python/math_ops#Segmentation)\r\n  - [tensorflow/contrib/framework/\\_\\_init\\_\\_.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/__init__.py)\r\n    - [https://tensorflow.org/api_guides/python/contrib.framework](https://tensorflow.org/api_guides/python/contrib.framework)\r\n  - [tensorflow/contrib/layers/\\_\\_init\\_\\_.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/__init__.py)\r\n    - [https://tensorflow.org/api_guides/python/contrib.layers](https://tensorflow.org/api_guides/python/contrib.layers)\r\n  - [tensorflow/python/training/training.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/training.py)\r\n    - [https://www.tensorflow.org/api_guides/python/train](https://www.tensorflow.org/api_guides/python/train)\r\n  - [tensorflow/python/summary/summary.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/summary/summary.py)\r\n    - [https://tensorflow.org/api_guides/python/summary](https://tensorflow.org/api_guides/python/summary)\r\n  - [tensorflow/python/ops/session_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/session_ops.py)\r\n    - [https://tensorflow.org/api_guides/python/session_ops](https://tensorflow.org/api_guides/python/session_ops)\r\n  - [tensorflow/python/ops/string_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/string_ops.py)\r\n    - [https://tensorflow.org/api_guides/python/string_ops](https://tensorflow.org/api_guides/python/string_ops)\r\n  - [tensorflow/python/ops/state_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/state_ops.py)\r\n    - [https://tensorflow.org/api_guides/python/state_ops](https://tensorflow.org/api_guides/python/state_ops)\r\n  - [tensorflow/python/ops/math_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py)\r\n    - [https://tensorflow.org/api_guides/python/math_ops#segmentation](https://tensorflow.org/api_guides/python/math_ops#segmentation)\r\n  - [tensorflow/python/ops/functional_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/functional_ops.py)\r\n    - [https://tensorflow.org/api_guides/python/functional_ops](https://tensorflow.org/api_guides/python/functional_ops)\r\n  - [tensorflow/python/ops/check_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/check_ops.py)\r\n    - [https://tensorflow.org/api_guides/python/check_ops](https://tensorflow.org/api_guides/python/check_ops)\r\n  - [tensorflow/python/ops/array_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py)\r\n    - [https://tensorflow.org/api_guides/python/array_ops](https://tensorflow.org/api_guides/python/array_ops)\r\n  - [tensorflow/python/ops/control_flow_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py)\r\n    - [https://tensorflow.org/api_guides/python/control_flow_ops](https://tensorflow.org/api_guides/python/control_flow_ops)\r\n  - [tensorflow/python/ops/sparse_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/sparse_ops.py)\r\n    - [https://tensorflow.org/api_guides/python/sparse_ops](https://tensorflow.org/api_guides/python/sparse_ops)\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?** No\r\n", "comments": ["@MarkDaoust - Hi Mark, Could you please check this. ", "Hi Alexa, thanks for the report.\r\n\r\nWe're trying to clean out the api_guides/ directory. Everything in there was out-dated.\r\n\r\nIt looks like we were a little over-zealous. \r\nThese broken links will all be deleted when we publish r1.12. In a few days.\r\n\r\nI'm going to close this, because I believe there is no further work to do on it. But please re-open if it's still broken after the r1.12 release.\r\n\r\nThanks."]}, {"number": 23306, "title": "Broken URL in TensorFlow Tutorial for Image Retraining", "body": "On [https://www.tensorflow.org/hub/tutorials/image_retraining](https://www.tensorflow.org/hub/tutorials/image_retraining)\r\n\r\nThe URL of this command is broken\r\n`curl -LO http://download.tensorflow.org/example_images/flower_photos.tgz\r\ntar xzf flower_photos.tgz`\r\n\r\nThe working URL is should be the following\r\n`curl -O http://download.tensorflow.org/example_images/flower_photos.tgz`\r\n\r\nIt was previously discussed at\r\n[https://github.com/tensorflow/tensorflow/issues/8459](https://github.com/tensorflow/tensorflow/issues/8459)", "comments": []}, {"number": 23305, "title": "Bug fix + tests: keras.backend.batch_dot", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@googlebot Signed.", "CLAs look good, thanks!\n\n<!-- ok -->", "@fchollet Are unit tests really needed for error conditions?\r\n", "@farizrahman4u Can you please address the requested changes so we can merge the PR? Thanks!", "There were more changes in batch_dot at keras-team/keras. PR has been updated with those changes.\r\n", "Nagging Reviewer @caisq, @fchollet: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 59 days with no activity and the `awaiting review` label has been applied."]}]