[{"number": 37524, "title": "r2.2-rc1 cherry-pick request: [Intel MKL] Fix performance regression in DNNL 1", "body": "Fix ~30% performance regression in some models due to lack of primitive memory reordering cache when using DNNL 1. This only affects TF-MKL and not stock TF.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37524) for more info**.\n\n<!-- need_author_consent -->", "@guizili0 Could you please post `@googlebot I consent.`?", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37524) for more info**.\n\n<!-- ok -->"]}, {"number": 37523, "title": "Cannot build tensorflow from docker image tensorflow/tensorflow:devel-gpu", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n\r\nBase image: [tensorflow/tensorflow:devel-gpu](https://hub.docker.com/layers/tensorflow/tensorflow/devel-gpu/images/sha256-a29a94004f4f9fbf2a3810d4e9fcd2cd4caa5f18c179a7dea7b1b0db6e3fdbde?context=explore)\r\nImage hash: a29a94004f4f9fbf2a3810d4e9fcd2cd4caa5f18c179a7dea7b1b0db6e3fdbde\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: v3.6\r\n- Bazel version (if compiling from source): 2.0.0 \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0 \r\n- GPU model and memory: Tesla V100\r\n\r\n**Describe the problem**\r\n\r\nDocker image [tensorflow/tensorflow:devel-gpu](https://hub.docker.com/layers/tensorflow/tensorflow/devel-gpu/images/sha256-a29a94004f4f9fbf2a3810d4e9fcd2cd4caa5f18c179a7dea7b1b0db6e3fdbde?context=explore) is expected to provide a reliable environment to build tensorflow from source code with bazel. \r\n\r\nHowever, the bazel build fails in the container from either `docker build` or `docker run`. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n(1) Make sure the host has nvidia drivers support. \r\n```\r\n$ nvidia-smi\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n```\r\n```\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:01_CDT_2018\r\nCuda compilation tools, release 10.0, V10.0.130\r\n```\r\n(2) Download the latest devel-gpu image:\r\n```\r\n$ docker image pull tensorflow/tensorflow:devel-gpu\r\ndevel-gpu: Pulling from tensorflow/tensorflow\r\nDigest: sha256:a29a94004f4f9fbf2a3810d4e9fcd2cd4caa5f18c179a7dea7b1b0db6e3fdbde\r\nStatus: Image is up to date for tensorflow/tensorflow:devel-gpu\r\ndocker.io/tensorflow/tensorflow:devel-gpu\r\n```\r\n(3) Build a new docker image which builds tensorflow\r\n```\r\n$ cat Dockerfile\r\nFROM tensorflow/tensorflow:devel-gpu\r\nWORKDIR /tensorflow_src\r\nRUN bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n```\r\n$ docker build .\r\n```\r\n\r\n(4) Notice the link error from bazel\r\n```\r\nImportError: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_pywrap_py_utils.so: undefined symbol: _ZN10tensorflow8internal10LogMessageC1EPKcii\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\nFull error log could be found at the end. \r\n\r\n(5) Alternative: build from docker container\r\nInstead of running `docker build`, this error can be reproducible from `docker run` and `bazel build` from the container as well.\r\n\r\n```\r\n$ docker run -it tensorflow/tensorflow:devel-gpu bash\r\n#  bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n(6) Alternative: other devel-gpu images\r\nThis error is reproducible to other images from the past week.  \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nRROR: /tensorflow_src/tensorflow/python/keras/api/BUILD:103:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 776, in <module>\r\n    main()\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 772, in main\r\n    lazy_loading, args.use_relative_imports)\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 629, in create_api_files\r\n    compat_api_versions, lazy_loading, use_relative_imports)\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 503, in get_api_init_text\r\n    _, attr = tf_decorator.unwrap(attr)\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 219, in unwrap\r\n    elif _has_tf_decorator_attr(cur):\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 124, in _has_tf_decorator_attr\r\n    hasattr(obj, '_tf_decorator') and\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_pywrap_py_utils.so: undefined symbol: _ZN10tensorflow8internal10LogMessageC1EPKcii\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /tensorflow_src/tensorflow/tools/pip_package/BUILD:62:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit 1)\r\nINFO: Elapsed time: 1622.950s, Critical Path: 719.42s\r\nINFO: 19283 processes: 19283 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["It seems some dependency is missed. Do \r\n```\r\n> c++filt _ZN10tensorflow8internal10LogMessageC1EPKcii\r\ntensorflow::internal::LogMessage::LogMessage(char const*, int, int)\r\n```\r\nthen you can find that `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)` is in `//tensorflow/core/platform:logging`. Add `//tensorflow/core/platform:logging` then you'll find other undefined symbols. \r\n\r\nAnyway, the following should work\r\n\r\n```diff\r\ndiff --git a/tensorflow/compiler/tf2tensorrt/BUILD b/tensorflow/compiler/tf2tensorrt/BUILD\r\nindex 9da873a..371a580 100644\r\n--- a/tensorflow/compiler/tf2tensorrt/BUILD\r\n+++ b/tensorflow/compiler/tf2tensorrt/BUILD\r\n@@ -560,6 +560,9 @@ pybind_extension(\r\n     module_name = \"_pywrap_py_utils\",\r\n     deps = [\r\n         \":py_utils\",\r\n+        \"//tensorflow/core/platform:env\",\r\n+        \"//tensorflow/core/platform:logging\",\r\n+        \"//tensorflow/core/platform:status\",\r\n         \"@pybind11\",\r\n     ],\r\n )\r\n\r\n```", "Is building from -devel docker image supported workflow? It seems important enough for developer productivity to deserve an integration test.", "@freedomtan \r\nAs there is a pr to monitor this issue, please confirm if  we may move this issue to closed status.", "@Saduf2019 you should ask @lezh who opened this issue, not me :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37523\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37523\">No</a>\n"]}, {"number": 37522, "title": "Using @tf.function will turn the EagerTensor into Tensor", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@DracarysLu \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nPlease, provide the simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.Please, fill the [issue templete](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!", "> Using @tf.function will turn the EagerTensor into Tensor\r\n\r\nYes, that is the point. Please [read the docs](https://www.tensorflow.org/guide/eager#work_with_functions).", "> @DracarysLu\r\n> \r\n> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n> \r\n> Please, provide the simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.Please, fill the [issue templete](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!\r\n\r\n\r\nThe environment is Tensorflow 2.1, no matter with the OS (Both in Linux and Windows).\r\n\r\nWhen I add the \"@tf.function\" before the train_step function and print the result of loss, it goes wrong. \r\n\r\n```\r\n@tf.function\r\n   def train_step(train_batch, labels):\r\n       with tf.GradientTape() as tape:\r\n           outputs = model(train_batch, training=True)\r\n           loss = loss_obj(labels, outputs)\r\n       gradients = tape.gradient(loss, model.trainable_variables)\r\n       optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n       train_loss(loss)\r\n       print(loss.numpy())\r\n       train_accuracy(labels, outputs)\r\n```\r\nAnd here is the error message:\r\n```\r\n AttributeError: in converted code:\r\n    <ipython-input-6-a47c909007ec>:10 train_step  *\r\n        print(loss.numpy())\r\n    AttributeError: 'Tensor' object has no attribute 'numpy'.\r\n```\r\n\r\nBut when I remove the \"@tf.function\", it works well.\r\n```\r\n   epoch: 1\r\n   2.4107723\r\n   2.4120736\r\n   1.7468656\r\n   1.4507847\r\n   1.3938136\r\n   1.2365413\r\n   0.9930383\r\n   0.93578833\r\n   0.81582385\r\n   0.8927727\r\n   0.7690362\r\n```", "Again, this is expected behaviour. The `.numpy()` call only works on `EagerTensor` objects, i.e. when running in Eager mode. The whole point of `tf.function` decoration is to run a function in graph mode to optimize runtimes, which results in tensors not having a `.numpy` method.\r\nYou may however try to use `tf.print`.", "> Again, this is expected behaviour. The `.numpy()` call only works on `EagerTensor` objects, i.e. when running in Eager mode. The whole point of `tf.function` decoration is to run a function in graph mode to optimize runtimes, which results in tensors not having a `.numpy` method.\r\n> You may however try to use `tf.print`.\r\n\r\nThanks!", "@DracarysLu \r\n\r\n Please close this thread if it solves your question. Thanks!\r\n"]}, {"number": 37521, "title": "Add concatenated_categorical_column in feature column api.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 2.1\r\n- Are you willing to contribute it (Yes/No): Yes. And the code change is ready.\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n*Feature:*\r\nAdd a new feature column api *concatenated_categorical_column* to concatenate SparseTensors from multiple categorical columns into one SparseTensor.\r\nThe id range of each categorical column is [0, num_buckets_0], [0, num_buckets_1] ... [0, num_buckets_n]. The ids will be conflict in the combined sparse id tensor because they all start from 0. In this api, we will add offset in sparse id tensor from each categorical column to avoid this conflict. After concatenation, the id range of the concatenated column is [0, num_bucket_0 + num_bucket_1 + ... + num_bucket_n].\r\nThe api definition is as follows\r\n```python\r\ntf.feature_column.concatenated_categorical_column(categorical_columns)\r\n\r\nArgs:\r\n    categorical_columns: List of categorical columns created by a categorical_column_with_* function.\r\n```\r\n\r\n*Current Behavior:*\r\nThere is no feature column api to concatenate multiple categorical columns. Users cannot do this transformation with feature column.\r\n\r\n*Why we need this api:*\r\nA dataset sometimes contains many categorical features. If we map each categorical feature to an embedding vector separately, we need to create many variables for the embedding tables, one for each categorical feature. Besides the embedding table weights, there is additional overhead to create a variable. **As a result, the size of the model will be very huge if we create many embedding variables even though the size of each embedding table is small, and the performance of embedding lookup may be inefficient.**\r\n![image](https://user-images.githubusercontent.com/18071380/74301254-795bac80-4d8d-11ea-98e3-2632e47df669.png)\r\n\r\nIn order to reduce the number of variables for the embedding table, we can concatenate the sparse id tensors from multiple categorical feature into a big sparse tensor, and also merge the embedding tables for each categorical feature into one table. However, the id of each categorical feature start from 0. And the same IDs will map to the same embedding vectors by looking up in the merged embedding table. In the following figure, we can see that embedding vectors of \"marital-status\" are the same as \"education\".\r\n![image](https://user-images.githubusercontent.com/18071380/74301128-00f4eb80-4d8d-11ea-97eb-7db2e798ffb2.png)\r\n\r\nSo, we need to add an offset into IDs of \"marital-status\" so that the \"martial-status\" feature can get its embedding vectors by looking up in the merged embedding table. \r\n![image](https://user-images.githubusercontent.com/18071380/74301135-06eacc80-4d8d-11ea-9f2b-2a577ed52b0f.png)\r\n\r\n**Will this change the current api? How?**\r\nNo, we won't change the existed Api. \r\nWe will add a new feature column api *concatenated_categorical_column*.\r\n\r\n**Who will benefit with this feature?**\r\nThe TensorFlow users who will develop the model with many categorical features and want to map these feature to embedding vectors.\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @brightcoder01 we're no longer supporting new features for feature columns and planning to deprecate them from 2.2 and forward.\r\nThis feature sounds reasonable, but it should be achievable through keras natively, i.e., we're exporting a preprocessing layer called index_lookup to convert categorical inputs to integers, and you can add the offset manual before passing to embedding layer.\r\nFor more info, check this [RFC](https://github.com/tensorflow/community/pull/188)", "Hi, @tanzhenyu , Thanks for your reply.\r\nFrom the API spec for TF2.2rc, the proposed layers are not included in tf.keras.preprocessing package. Will these layers be released TF2.2?\r\nWhat's more, will the API in tf.feature_column be removed in TF2.2 or later version?", "@tanzhenyu \r\nHow can we use ```tf.estimator.LinearClassifier``` since there is a arg ```feature_columns``` if  tf.feature_column is removed ?"]}, {"number": 37520, "title": "Documented tf.keras.preprocessing purpose", "body": "Fixes : #37445 \r\nDocs about `tf.feature_column` not added yet since I'm not sure about the differences between them. Can someone plz help about that??", "comments": []}, {"number": 37519, "title": "Update func_graph.py", "body": "Fixing error message. removed \"contrib\" usage.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37519) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\n\nOn Wed, Mar 11, 2020 at 4:19 PM googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here with @googlebot\n> I signed it! and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37519>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37519#issuecomment-597927096>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIBT34VDUQ2VIDP3XTUKVX3RHAL6HANCNFSM4LGASEIQ>\n> .\n>\n\n\n-- \nRajan\n", "@spidyDev thank you for your contribution, please sign CLA.", "@googlebot I signed it!\n\nOn Wed, Mar 11, 2020 at 9:11 PM gbaned <notifications@github.com> wrote:\n\n> @spidyDev <https://github.com/spidyDev> thank you for your contribution,\n> please sign CLA.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37519#issuecomment-597996755>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIBT34X5VQH4QE5S7C2NADLRHBOGPANCNFSM4LGASEIQ>\n> .\n>\n\n\n-- \nRajan\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37519) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it..", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37519) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@googlebot I signed it!\n\nOn Wed, Mar 11, 2020 at 9:53 PM googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here with @googlebot\n> I signed it! and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37519>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37519#issuecomment-598005853>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIBT34WOXFDORWVBS3LL4R3RHBTE3ANCNFSM4LGASEIQ>\n> .\n>\n\n\n-- \nRajan\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37519) for more info**.\n\n<!-- ok -->"]}, {"number": 37518, "title": "[ROCm] Fix for broken ROCm CSB - 200311", "body": "This commit fixes regressions introduced in the ROCm CSB by th following commit :\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/df00d7ebbfb1ac799d1f98dd724fce10cb84329e\r\n\r\nThe above commit classifies puts \"cudnn_rnn_kernels\" under \"if_cuda\" in the tensorflow/core/BUILD file. That target is mis-named (should really be gpu_rnn_kernels) and is required by ROCm too. This commit simply changes the condition from \"if_cuda\" to \"if_cuda_or_rocm\"\r\n\r\n--------------------------\r\n\r\n/cc @whchung @chsigg @nvining-work \r\n", "comments": ["gentle ping"]}, {"number": 37517, "title": "[New Feature] tf.data pipeline serialization to json for performance analysis", "body": "This PR introduces changes to TF Data and aims to generate automatically a full dump of the TF Data API on request for static analysis.\r\n\r\n# Motivations\r\n\r\n## PR Background\r\nData loading remains one of the major pain point for users and one the place where the average users have little (if any) idea on what they are doing right / wrong in terms of performance and what they could do to improve. Given the number of blog articles / youtube just on this topic by Google, I think it's fair to assume that we all understand how critical this topic is.\r\n\r\n## Objective\r\nProvide a simple way (controlled by an env var, and not by a python API call) to dump the configuration of the `tf.data` pipeline requested by the user for a later analysis (aka. sort of a static code analyizer)\r\n\r\nThis PR proposes to _plug_ in the tf.data python in a fully transparent way to the user with no performance cost. And serialize the complete pipeline to a json file at a location determined by an env var: `TF_DATASET_SERIALIZE_OUTPUT_DIR=/path/to/export/dir`\r\n\r\n## Fine, but why would I want to serialize my tf data pipeline ?\r\nOnce the serialization is embedded in Tensorflow, we can follow different routes:\r\n- Provide a Tensorboard plugin that consumes this json file and produce recommendations\r\n- Provide a custom python package that would do the job in a CLI way\r\n- Integrate with already existing DL profiling solution such as DLProf: https://docs.nvidia.com/deeplearning/frameworks/dlprof-user-guide/\r\n\r\n## What can I recommendation shall I expect from this:\r\n\r\nThese are few examples, and are far from a finite list:\r\n- Did the user requested prefetch of CPU ?\r\n- Did the user requested prefetch on device (e.g. GPU) ?\r\n- Did the user process the data in a parallel fashion or still rely on sequential reading ?\r\n- Did the user use any non TF native ops such as `Data.from_generator()` or any `py_func()`\r\n- Did the user exploited any fused operation available (e.g. shuffle and repeat, map and batch, etc.)\r\n- Did the user read from raw data or from TF Records ?\r\n- Did the user use a sufficient number of CPU threads or rely of `tf.data.experimental.AUTOTUNE` ?\r\n\r\n## Is this proposal only benefiting GPU users ?\r\nThis proposal is fully agnostic to the device used as data-loading is critical for any kind of device the user might use.\r\nNonetheless, optimization recommendations could slightly vary from a device to device basis.\r\n\r\n## To be noted\r\nTF Data serialization can't be the one stop solution to maximize tf.data performance, *however*, making sure that user apply best practices in their data loading pipeline and from their fine tune the number of threads or other parameters sounds reasonable. IMHO, if every users would already be following all the \"rules\" cited above that would already be amazing.\r\n\r\n# Important implementation points:\r\n\r\n### Typical Node JSON fomat:\r\n```json\r\n\"00\": {\r\n           \"__class__\": \"ParallelInterleaveDataset\",\r\n            \"block_length\": 8,\r\n            \"calling_context\": {\r\n                \"filename\": \"/path/to/my/project/utils/dataloading.py\",\r\n                \"function\": \"get_dataset\",\r\n                \"line_number\": 81\r\n            },\r\n            \"cycle_length\": \"AUTOTUNE\",\r\n            \"map_func\": \"<function get_dataset.<locals>.fake_data_gen at 0x7fa6b826b488>\",\r\n            \"num_parallel_calls\": \"AUTOTUNE\",\r\n            \"output_dtype\": \"(tf.float32, tf.int32)\",\r\n            \"output_shape\": \"((3, 256, 256), ())\",\r\n            \"variant_tensor_inputs\": \"Tensor(\\\"TensorSliceDataset:0\\\", shape=(), dtype=variant)\",\r\n            \"variant_tensor_outputs\": \"Tensor(\\\"ParallelInterleaveDatasetV2:0\\\", shape=(), dtype=variant)\"\r\n        }\r\n```\r\n### We want to see the following information:\r\n- Trace what python calls (not the CPP OPs) the user requested with which arguments\r\n     - tf.data.DatasetV2 subclass (see `__class__` above)\r\n     - `__init__` arguments\r\n     - optimizations applied: tf.data Options\r\n- Track *where* in the python code did the user do X, Y and Z (see `calling_context` above)\r\n- Track the output dtype, shape and tensors\r\n- Be able to see on what device is the output tensor\r\n- Some datapoint related to runtime environment:\r\n   - number of CPU threads\r\n   - version of tensorflow\r\n   - Is the model using distributed training ? If yes with `tf.distributed` or `horovod`  ?\r\n   - if installed, version of libraries that might have an impact on data loading (NVIDIA DALI, Horovod, etc.)\r\n\r\n\r\n# Example Output:\r\n\r\n```json\r\n{\r\n    \"DISTRIBUTED_MODE\": null,\r\n    \"HOROVOD_VERSION\": \"0.19.0\",\r\n    \"NVIDIA_DALI_VERSION\": \"0.19.0\",\r\n    \"TENSORFLOW_VERSION\": \"1.15.2\",\r\n    \"TF_DATA_PIPELINE\": {\r\n        \"00\": {\r\n            \"__class__\": \"TensorSliceDataset\",\r\n            \"calling_context\": {\r\n                \"filename\": \"/path/to/my/project/utils/dataloading.py\",\r\n                \"function\": \"get_dataset\",\r\n                \"line_number\": 74\r\n            },\r\n            \"output_dtype\": \"tf.int32\",\r\n            \"output_shape\": \"()\",\r\n            \"variant_tensor_outputs\": \"Tensor(\\\"TensorSliceDataset:0\\\", shape=(), dtype=variant)\"\r\n        },\r\n        \"01\": {\r\n            \"__class__\": \"ParallelInterleaveDataset\",\r\n            \"block_length\": 8,\r\n            \"calling_context\": {\r\n                \"filename\": \"/path/to/my/project/utils/dataloading.py\",\r\n                \"function\": \"get_dataset\",\r\n                \"line_number\": 81\r\n            },\r\n            \"cycle_length\": \"AUTOTUNE\",\r\n            \"map_func\": \"<function get_dataset.<locals>.fake_data_gen at 0x7fa6b826b488>\",\r\n            \"num_parallel_calls\": \"AUTOTUNE\",\r\n            \"output_dtype\": \"(tf.float32, tf.int32)\",\r\n            \"output_shape\": \"((3, 256, 256), ())\",\r\n            \"variant_tensor_inputs\": \"Tensor(\\\"TensorSliceDataset:0\\\", shape=(), dtype=variant)\",\r\n            \"variant_tensor_outputs\": \"Tensor(\\\"ParallelInterleaveDatasetV2:0\\\", shape=(), dtype=variant)\"\r\n        },\r\n        \"02\": {\r\n            \"__class__\": \"_ShuffleAndRepeatDataset\",\r\n            \"buffer_size\": 2048,\r\n            \"calling_context\": {\r\n                \"filename\": \"/path/to/my/project/utils/dataloading.py\",\r\n                \"function\": \"get_dataset\",\r\n                \"line_number\": 93\r\n            },\r\n            \"count\": null,\r\n            \"output_dtype\": \"(tf.float32, tf.int32)\",\r\n            \"output_shape\": \"((3, 256, 256), ())\",\r\n            \"seed\": null,\r\n            \"variant_tensor_inputs\": \"Tensor(\\\"ParallelInterleaveDatasetV2:0\\\", shape=(), dtype=variant)\",\r\n            \"variant_tensor_outputs\": \"Tensor(\\\"ShuffleAndRepeatDataset:0\\\", shape=(), dtype=variant)\"\r\n        },\r\n        \"03\": {\r\n            \"__class__\": \"_MapAndBatchDataset\",\r\n            \"batch_size\": 256,\r\n            \"calling_context\": {\r\n                \"filename\": \"/path/to/my/project/utils/dataloading.py\",\r\n                \"function\": \"get_dataset\",\r\n                \"line_number\": 116\r\n            },\r\n            \"drop_remainder\": true,\r\n            \"map_func\": \"<function get_dataset.<locals>.data_mapping_fn at 0x7fa6a37750d0>\",\r\n            \"num_parallel_calls\": \"AUTOTUNE\",\r\n            \"output_dtype\": \"(tf.float32, tf.int32)\",\r\n            \"output_shape\": \"((256, 3, 256, 256), (256,))\",\r\n            \"use_legacy_function\": false,\r\n            \"variant_tensor_inputs\": \"Tensor(\\\"ShuffleAndRepeatDataset:0\\\", shape=(), dtype=variant)\",\r\n            \"variant_tensor_outputs\": \"Tensor(\\\"MapAndBatchDataset:0\\\", shape=(), dtype=variant)\"\r\n        },\r\n        \"04\": {\r\n            \"__class__\": \"PrefetchDataset\",\r\n            \"buffer_size\": \"AUTOTUNE\",\r\n            \"calling_context\": {\r\n                \"filename\": \"/path/to/my/project/utils/dataloading.py\",\r\n                \"function\": \"get_dataset\",\r\n                \"line_number\": 124\r\n            },\r\n            \"output_dtype\": \"(tf.float32, tf.int32)\",\r\n            \"output_shape\": \"((256, 3, 256, 256), (256,))\",\r\n            \"slack_period\": null,\r\n            \"variant_tensor_inputs\": \"Tensor(\\\"MapAndBatchDataset:0\\\", shape=(), dtype=variant)\",\r\n            \"variant_tensor_outputs\": \"Tensor(\\\"PrefetchDataset:0\\\", shape=(), dtype=variant)\"\r\n        },\r\n        \"05\": {\r\n            \"__class__\": \"_OptionsDataset\",\r\n            \"calling_context\": {\r\n                \"filename\": \"/path/to/my/project/utils/dataloading.py\",\r\n                \"function\": \"get_dataset\",\r\n                \"line_number\": 157\r\n            },\r\n            \"options\": {\r\n                \"experimental_deterministic\": false,\r\n                \"experimental_distribute\": {\r\n                    \"auto_shard\": true,\r\n                    \"num_devices\": null\r\n                },\r\n                \"experimental_optimization\": {\r\n                    \"apply_default_optimizations\": false,\r\n                    \"autotune\": true,\r\n                    \"autotune_algorithm\": null,\r\n                    \"autotune_buffers\": true,\r\n                    \"autotune_cpu_budget\": 20,\r\n                    \"filter_fusion\": true,\r\n                    \"filter_with_random_uniform_fusion\": null,\r\n                    \"hoist_random_uniform\": true,\r\n                    \"map_and_batch_fusion\": true,\r\n                    \"map_and_filter_fusion\": true,\r\n                    \"map_fusion\": true,\r\n                    \"map_parallelization\": true,\r\n                    \"map_vectorization\": {\r\n                        \"enabled\": true,\r\n                        \"use_choose_fastest\": true\r\n                    },\r\n                    \"noop_elimination\": true,\r\n                    \"parallel_batch\": true,\r\n                    \"prefetch_to_device\": \"/gpu:0\",\r\n                    \"prefetch_to_device_buffer\": 2,\r\n                    \"shuffle_and_repeat_fusion\": true\r\n                },\r\n                \"experimental_slack\": false,\r\n                \"experimental_stateful_whitelist\": null,\r\n                \"experimental_stats\": {\r\n                    \"aggregator\": null,\r\n                    \"counter_prefix\": \"\",\r\n                    \"latency_all_edges\": null,\r\n                    \"prefix\": \"\"\r\n                },\r\n                \"experimental_threading\": {\r\n                    \"max_intra_op_parallelism\": 20,\r\n                    \"private_threadpool_size\": 20\r\n                }\r\n            },\r\n            \"output_dtype\": \"(tf.float32, tf.int32)\",\r\n            \"output_shape\": \"((256, 3, 256, 256), (256,))\",\r\n            \"variant_tensor_inputs\": \"Tensor(\\\"PrefetchDataset:0\\\", shape=(), dtype=variant)\",\r\n            \"variant_tensor_outputs\": \"Tensor(\\\"PrefetchDataset:0\\\", shape=(), dtype=variant)\"\r\n        },\r\n        \"06\": {\r\n            \"__class__\": \"_MaxIntraOpParallelismDataset\",\r\n            \"calling_context\": {\r\n                \"filename\": \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\",\r\n                \"function\": \"_apply_options\",\r\n                \"line_number\": 527\r\n            },\r\n            \"max_intra_op_parallelism\": 20,\r\n            \"output_dtype\": \"(tf.float32, tf.int32)\",\r\n            \"output_shape\": \"((256, 3, 256, 256), (256,))\",\r\n            \"variant_tensor_inputs\": \"Tensor(\\\"PrefetchDataset:0\\\", shape=(), dtype=variant)\",\r\n            \"variant_tensor_outputs\": \"Tensor(\\\"MaxIntraOpParallelismDataset:0\\\", shape=(), dtype=variant)\"\r\n        },\r\n        \"07\": {\r\n            \"__class__\": \"_PrivateThreadPoolDataset\",\r\n            \"calling_context\": {\r\n                \"filename\": \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\",\r\n                \"function\": \"_apply_options\",\r\n                \"line_number\": 530\r\n            },\r\n            \"num_threads\": 20,\r\n            \"output_dtype\": \"(tf.float32, tf.int32)\",\r\n            \"output_shape\": \"((256, 3, 256, 256), (256,))\",\r\n            \"variant_tensor_inputs\": \"Tensor(\\\"MaxIntraOpParallelismDataset:0\\\", shape=(), dtype=variant)\",\r\n            \"variant_tensor_outputs\": \"Tensor(\\\"PrivateThreadPoolDataset:0\\\", shape=(), dtype=variant)\"\r\n        },\r\n        \"08\": {\r\n            \"__class__\": \"_OptimizeDataset\",\r\n            \"calling_context\": {\r\n                \"filename\": \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\",\r\n                \"function\": \"_apply_options\",\r\n                \"line_number\": 545\r\n            },\r\n            \"optimization_configs\": [\r\n                \"map_vectorization:use_choose_fastest:true\"\r\n            ],\r\n            \"optimizations\": [\r\n                \"filter_fusion\",\r\n                \"hoist_random_uniform\",\r\n                \"inject_prefetch\",\r\n                \"map_and_batch_fusion\",\r\n                \"map_and_filter_fusion\",\r\n                \"map_fusion\",\r\n                \"map_parallelization\",\r\n                \"map_vectorization\",\r\n                \"noop_elimination\",\r\n                \"parallel_batch\",\r\n                \"shuffle_and_repeat_fusion\",\r\n                \"make_sloppy\"\r\n            ],\r\n            \"output_dtype\": \"(tf.float32, tf.int32)\",\r\n            \"output_shape\": \"((256, 3, 256, 256), (256,))\",\r\n            \"variant_tensor_inputs\": \"Tensor(\\\"PrivateThreadPoolDataset:0\\\", shape=(), dtype=variant)\",\r\n            \"variant_tensor_outputs\": \"Tensor(\\\"OptimizeDataset:0\\\", shape=(), dtype=variant)\"\r\n        },\r\n        \"09\": {\r\n            \"__class__\": \"_ModelDataset\",\r\n            \"autotune_algorithm\": \"HILL_CLIMB\",\r\n            \"calling_context\": {\r\n                \"filename\": \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\",\r\n                \"function\": \"_apply_options\",\r\n                \"line_number\": 559\r\n            },\r\n            \"cpu_budget\": 20,\r\n            \"output_dtype\": \"(tf.float32, tf.int32)\",\r\n            \"output_shape\": \"((256, 3, 256, 256), (256,))\",\r\n            \"variant_tensor_inputs\": \"Tensor(\\\"OptimizeDataset:0\\\", shape=(), dtype=variant)\",\r\n            \"variant_tensor_outputs\": \"Tensor(\\\"ModelDataset:0\\\", shape=(), dtype=variant)\"\r\n        },\r\n        \"options\": {\r\n            \"experimental_deterministic\": false,\r\n            \"experimental_distribute\": {},\r\n            \"experimental_optimization\": {\r\n                \"apply_default_optimizations\": false,\r\n                \"autotune\": true,\r\n                \"autotune_algorithm\": null,\r\n                \"autotune_buffers\": true,\r\n                \"autotune_cpu_budget\": 20,\r\n                \"filter_fusion\": true,\r\n                \"filter_with_random_uniform_fusion\": null,\r\n                \"hoist_random_uniform\": true,\r\n                \"map_and_batch_fusion\": true,\r\n                \"map_and_filter_fusion\": true,\r\n                \"map_fusion\": true,\r\n                \"map_parallelization\": true,\r\n                \"map_vectorization\": {\r\n                    \"enabled\": true,\r\n                    \"use_choose_fastest\": true\r\n                },\r\n                \"noop_elimination\": true,\r\n                \"parallel_batch\": true,\r\n                \"prefetch_to_device\": \"/gpu:0\",\r\n                \"prefetch_to_device_buffer\": 2,\r\n                \"shuffle_and_repeat_fusion\": true\r\n            },\r\n            \"experimental_slack\": false,\r\n            \"experimental_stateful_whitelist\": null,\r\n            \"experimental_stats\": {},\r\n            \"experimental_threading\": {\r\n                \"max_intra_op_parallelism\": 20,\r\n                \"private_threadpool_size\": 20\r\n            }\r\n        }\r\n    },\r\n    \"TOTAL_CPU_THREADS\": 20\r\n}\r\n```", "comments": ["I am in general opposed to introducing this large of a tf.data change into the core API without understanding what the motivation for this API would be so that I can evaluate whether it would benefit most tf.data users (or a subset of tf.data users significantly).\r\n\r\nIf you have a use case for static analysis of dataset structure, I would prefer to expose the `Dataset._as_serialized_graph` method.\r\n\r\nOne of the drawbacks of the current approach is that it requires future changes to tf.data datasets to be reflected in the `_save_configuration` methods and if they are not, the `serialize_configuration` result will be incomplete. In contrast, `Dataset._as_serialized_graph` calls in C++ API which is required to be implemented by any tf.data C++ op kernel.", "@jsimsa my bad I did a terrible job explaining the motivations of this PR, I pushed this in a rush for the yesterday Google/NVIDIA meeting. It's fixed now, I updated the head message with all the necessary information.\r\n\r\n@sanjoy is planning to organize a meeting in the coming weeks. Feel free to join in. This PR was designed as a conversation starter to scope a future RFC. \r\nCC: @WhiteFangBuck\r\n\r\n> If you have a use case for static analysis of dataset structure, I would prefer to expose the Dataset._as_serialized_graph method.\r\n\r\nUnfortunately this would not comply with one of core requirements. One of the key ideas is to be fully transparent to the user code and therefore we do not want to ask the user to \"request\" anything. We would like to be able to set an env var and TF does the rest automatically. A bit like you can enable XLA automatically by setting an env var.\r\n\r\n> One of the drawbacks of the current approach is that it requires future changes to tf.data datasets to be reflected in the _save_configuration methods and if they are not, the serialize_configuration result will be incomplete. In contrast, Dataset._as_serialized_graph calls in C++ API which is required to be implemented by any tf.data C++ op kernel.\r\n\r\nI would somehow agree on this. However, this is no different than how Keras handle serialization, many implements custom serialization strategies and the Keras API is vastly larger and harder to maintain than a couple of DatasetV2 classes. However, I hear your point, and this can probably be improved using a decorator approach to intercept the __init__ call. However, obviously that won't work in every cases and will most likely still require a fair amount of `self._save_configuration()` calls.\r\nI looked in CPP serialization API, and it doesn't seem to be able to provide what we look here. The objective is to \"dump\" a configuration of the Python API calls, arguments and where in the user code. Not to log what the CPP OPs receive as arguments. I'd love to be able to leverage an existing piece of code. However, in this case I'm not sure it is any helpful (and I would be super happy if I happen to be wrong)", "Thank you for your response. I am open to adding support for API (such as environment variable) that would control whether and where to dump the tf.data graph serialization.\r\n\r\nReinventing the wheel by creating a new serialization format while we already have one is not a good idea and I also disagree with the choice of implementing this at Python level.\r\n\r\nAll of the following questions:\r\n\r\n> - Did the user requested prefetch of CPU ?\r\n> - Did the user requested prefetch on device (e.g. GPU) ?\r\n> - Did the user process the data in a parallel fashion or still rely on sequential reading ?\r\n> - Did the user use any non TF native ops such as Data.from_generator() or any py_func()\r\n> - Did the user exploited any fused operation available (e.g. shuffle and repeat, map and batch, etc.)\r\n> - Did the user read from raw data or from TF Records ?\r\n> - Did the user use a sufficient number of CPU threads or rely of tf.data.experimental.AUTOTUNE?\r\n\r\ncan be answered based on the information in the serialized GraphDef proto. Furthermore, unlike your Python-based JSON serialization, the GraphDef proto will contain precise information about computation performed by user-defined functions.\r\n\r\nPS: Keras' handling of serialization is not a good precedent to follow here.\r\n\r\nPPS: Here is an example serialization: https://pastebin.com/yX2jfRXv\r\n\r\n", "@jsimsa we can certainly leverage some of the information existing here. \r\nHowever, this is far from sufficient and a number of information are missing.\r\n\r\n```text\r\nnode {\r\n  name: \"MapAndBatchDataset/_9\"\r\n  op: \"MapAndBatchDataset\"\r\n  input: \"RepeatDataset/_5\"\r\n  input: \"Const/_6\"\r\n  input: \"Const/_7\"\r\n  input: \"Const/_8\"\r\n  attr {\r\n    key: \"Targuments\"\r\n    value {\r\n      list {\r\n      }\r\n    }\r\n  }\r\n  attr {\r\n    key: \"f\"\r\n    value {\r\n      func {\r\n        name: \"__inference_tf_data_experimental_map_and_batch__map_fn_4328\"\r\n      }\r\n    }\r\n  }\r\n  attr {\r\n    key: \"output_shapes\"\r\n    value {\r\n      list {\r\n        shape {\r\n          dim {\r\n            size: -1\r\n          }\r\n        }\r\n        shape {\r\n          dim {\r\n            size: -1\r\n          }\r\n          dim {\r\n            size: 3\r\n          }\r\n        }\r\n        shape {\r\n          dim {\r\n            size: -1\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n  attr {\r\n    key: \"output_types\"\r\n    value {\r\n      list {\r\n        type: DT_INT64\r\n        type: DT_INT64\r\n        type: DT_DOUBLE\r\n      }\r\n    }\r\n  }\r\n  attr {\r\n    key: \"preserve_cardinality\"\r\n    value {\r\n      b: true\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n- missing batch size\r\n- missing `drop_remainder`\r\n- missing input & output device \r\n- num_parallel_calls ? Manually set or autotuned ?\r\n- missing python calling context (what file/what line)\r\n\r\nI really like the idea of using any existing piece of code, sounds like a really good idea. \r\n\r\n**Q1:** Could you give me instructions to export the tf.data pipeline to file as you did, that way I can do a side by side comparison ?\r\n\r\n**Q2:** Is it doable to add the missing information from CPP side only ? And how much work would you estimate this ?\r\n\r\n**Q3:** I suppose \"readibility\" of the report is not a major concern, however in this case, we gotta say that the json I attached is vastly more readable and understandable than the extract you attached. I suppose it's a minor concern as it's not supposed to be user-read. However it may make developing a recommender system more convoluted (e.g. one good example is how tensor shapes are represented)\r\n\r\n------------\r\nAs an additional point, I would like to highlight that the average user does not have any understanding of the CPP implementations or even how it works behind the hood. If we want to issue effective recommendations, we need to stay \"close\" to user code. Which means, close to the user API. If by doing all this work in CPP we can't identify what the user exactly did (because it's post processed in the Python API), then it will vastly limit the number of recommendations we can issue or their precision\r\n\r\nNonetheless, I truly agree that the more we can rely on something already existing and requiring less maintenance the better. I only care about the end-result, I'm 100% opened to any technical solution that would meet the objective (or even over-deliver) :+1: \r\n", "Most of the information is not missing, you are just not looking in the right place. Operation inputs (such as batch_size, num_parallel_calls, and drop_remainder) are separate nodes. Looking at the `Const` nodes will provide you with the answer.\r\n\r\nAs for Python calling context, I believe there is a way to capture this information and I would rather focus effort on improving the existing serialization with calling context, then developing new serialization from scratch.\r\n\r\nQ1: If you want simply want to play around with things, I suggest adding `LOG(INFO) << graph_def.DebugString()` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/rewrite_utils.cc#L157-L159). This will log the serialized version of the graph and this code path is invoked any time an iterator is created for a dataset.\r\n\r\nQ2: I believe so. @caisq might be a good person to comment on how to add the python call site information to the graphdef as he has worked on related things for TF debugger\r\n\r\nQ3: It is indeed not a concern. You can always write a GraphDef to JSON (or any other format) pretty-printer. Everything you see in the serialization (including shapes) is a [proto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/rewrite_utils.cc#L157-L159) so it has well-defined schema and bindings for parsing the information across various languages. Protobufs are best practice for efficient serialization so I see no issues here.\r\n\r\nRE: your last point. I agree with you that it is important to provide users with recommendations that they can understand. Neverthess, I think it makes more sense to analyze performance using an artifact that is closer to what is executed (and figure out how to maintain metadata to present meaningful feedback to the user) instead of analyzing an artifact that is close to what user sees and make assumptions about how will the user program be \"compiled\" into graph computation.", "@DEKHTIARJonathan Can you please check reviewer comments and keep us posted. Thanks!", "Still in heavy discussions between NVIDIA-Google", "@DEKHTIARJonathan Any update on this PR, please. Thanks!", "@gbaned : please do not close. We are still waiting on @jsimsa to know how to collect Python context from C++ and therefore be able to use protobuf serialization as suggested.", "@DEKHTIARJonathan Hi, I worked on Python stack capturing on Tensorflow.  Do you need this for remote execution as well or local execution sufficient?  Stack trace capturing is fast but the the actual decoding is expansive so we prefer to defer that.  But then we won't be able to decode inside a remote kernel, because the decoding needs to be on the same memory space.", "@kkimdev I think local context is good enough. When users do remote data processing I believe we can assume they are sufficiently power users to know what they are doing.\r\n\r\nWith regards to the \"decode cost\" I wouldn't worry too much, it's a one time cost that should be computed only if requested (controlled by an env var) when the dataloading graph is defined (not at runtime). So as any performance analysis it has some overhead and I believe it is a perfectly acceptable tradeoff (only on demand + one time cost not at runtime).\r\n\r\nPlease let me know if you have any other question", "FYI: Created a PR with more description https://github.com/tensorflow/tensorflow/pull/39555", "@DEKHTIARJonathan Can you please resolve conflicts? Thanks!", "@DEKHTIARJonathan Can you please resolve conflicts? Thanks!", "@gbaned the conflicts won't be solve. For now we have agreed to keep that PR stalled in its current form. We need to re-investigate how we are going to approach this issue. Might take a few months. Please do not close ;) Thanks", "@DEKHTIARJonathan  Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Closing as we don't have time to make much progress. Will re-open once we are ready to move forward"]}, {"number": 37516, "title": "Posenet Android Example. TestActivity crashes. Incorrect sample image size. comp:lite type: support", "body": "**Describe the current behavior**\r\nTestActivity Crashes. input image is wrong size. \r\n\r\n**Describe the expected behavior**\r\n\r\nLine 209: Posenet.kt\r\ngetInterpreter().runForMultipleInputsOutputs(inputArray, outputMap)\r\n\r\ninputArray has a direct byte buffer of size 1088659, \r\nInterpreter/model expects a TensorFlowLite buffer of 792588 bytes\r\n\r\nThe bitmap being used for this TestActivity is 560X353, while the size of the bitmap during the execution of CameraActivity is 257 x 257, which does correspond to the correct buffer size.   \r\n\r\nException message:\r\nCannot convert between a TensorFlowLite buffer with 792588 bytes and a ByteBuffer with 1088652 bytes.\r\n\r\n**Solution:**\r\nchoose a different sample image matching the constraint of producing a buffer of 792588 bytes, and change TestActivity:drawableToBitmap accordingly\r\n\r\nFor example, I know an image of 257 x 257 works (via CameraActivity), and then I change\r\nLine 33 in TestActivity:drawableToBitmap to    \r\nval bitmap = Bitmap.createBitmap(257, 257, Bitmap.Config.ARGB_8888)\r\n\r\nEven better would be maintaining the ratio of the input image (rather than hardcoding 257 x 257) and fulfilling the constraint of proper number of bytes for the model. Then any image could be tested, while avoiding unappealing stretching and skewing. \r\n \r\n\r\n", "comments": ["Would it be easier if I created pull requests instead of opening issues, if I have a solution to a bug.", "@amitDaMan Please go ahead and raise a PR. Also reference this issue thread in the request. Thanks!", "In general, this [TestActivity](https://github.com/tensorflow/examples/blob/master/lite/examples/posenet/android/app/src/main/java/org/tensorflow/lite/examples/posenet/TestActivity.kt) is not used in the app or not even used for testing in real code. It is only kept for reference.\r\n\r\nHi @amitDaMan, Please go ahead and raise a PR to fix this. Thank you! :-)\r\n\r\nLet me close this bug to reduce tracking cost. Thx! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37516\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37516\">No</a>\n", "@lintian06  I have raised a PR for this minor change, it uses Tensorflow lite support library to resize the resource image to fit the requisite input size for the posenet model, thank you. "]}, {"number": 37515, "title": "Tensorflow 2.1 Error \u201cwhen finalizing GeneratorDataset iterator\u201d - a memory leak?", "body": "**Reopening of issue [#35100](https://github.com/tensorflow/tensorflow/issues/35100),** as more and more people report to still have the same problem:\r\n\r\n**Problem description**\r\n\r\nI am using TensorFlow 2.1.0 for image classification under Centos Linux. As my image training data set is growing, I have to start using a Generator as I do not have enough RAM to hold all pictures. I have coded the Generator based on this tutorial.\r\n\r\nIt seems to work fine, until my program all the sudden gets killed without an error message:\r\n\r\n```\r\nEpoch 6/30\r\n2020-03-08 13:28:11.361785: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n43/43 [==============================] - 54s 1s/step - loss: 5.6839 - accuracy: 0.4669\r\nEpoch 7/30\r\n2020-03-08 13:29:05.511813: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n 7/43 [===>..........................] - ETA: 1:04 - loss: 4.3953 - accuracy: 0.5268Killed\r\n```\r\n\r\nLooking at the growing memory consumption with linux's top, I suspect a memory leak?\r\n\r\n**What I have tried**\r\n\r\n- The suggestion to switch to TF nightly build version. For me it did not help, also downgrading to TF2.0.1 did not help\r\n\r\n- There is a discussion suggesting that it is important, that 'steps_per_epoch' and 'batch size' correspond (whatever this exactly means) - I played with it without finding any improvement.\r\n\r\n- Trying to narrow down by looking at the size development of all variables in my Generator\r\n\r\n**Relevant code snippets**\r\n\r\n```\r\nclass DataGenerator(tf.keras.utils.Sequence):\r\n    'Generates data for Keras'\r\n    def __init__(self, list_IDs, labels, dir, n_classes):\r\n        'Initialization'\r\n        config = configparser.ConfigParser()\r\n        config.sections()\r\n        config.read('config.ini')\r\n\r\n        self.dim = (int(config['Basics']['PicHeight']),int(config['Basics']['PicWidth']))\r\n        self.batch_size = int(config['HyperParameter']['batchsize'])\r\n        self.labels = labels\r\n        self.list_IDs = list_IDs\r\n        self.dir = dir\r\n        self.n_channels = 3\r\n        self.n_classes = n_classes\r\n        self.on_epoch_end()        \r\n\r\n\r\n    def __len__(self):\r\n        'Denotes the number of batches per epoch'\r\n        return math.floor(len(self.list_IDs) / self.batch_size)\r\n\r\n    def __getitem__(self, index):\r\n        'Generate one batch of data'\r\n        # Generate indexes of the batch\r\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n\r\n        # Find list of IDs\r\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\r\n\r\n        # Generate data\r\n        X, y = self.__data_generation(list_IDs_temp)\r\n\r\n        return X, y, [None]\r\n```\r\n\r\nbeing called by\r\n\r\n```\r\n        training_generator = datagenerator.DataGenerator(train_files, labels, dir, len(self.class_names))\r\n        self.model.fit(x=training_generator,\r\n                    use_multiprocessing=False,\r\n                    workers=6, \r\n                    epochs=self._Epochs, \r\n                    steps_per_epoch = len(training_generator),\r\n                    callbacks=[LoggingCallback(self.logger.debug)])\r\n```\r\n\r\nI have tried running the exact same code under Windows 10, which gives me the following error:\r\n\r\n```\r\nEpoch 9/30\r\n2020-03-08 20:49:37.555692: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n41/41 [==============================] - 75s 2s/step - loss: 2.0167 - accuracy: 0.3133\r\nEpoch 10/30\r\n2020-03-08 20:50:52.986306: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n 1/41 [..............................] - ETA: 2:36 - loss: 1.6237 - accuracy: 0.39062020-03-08 20:50:57.689373: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at matmul_op.cc:480 : Resource exhausted: OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2020-03-08 20:50:57.766163: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Resource exhausted: OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n         [[{{node MatMul_6}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n 2/41 [>.............................] - ETA: 2:02 - loss: 1.6237 - accuracy: 0.3906Traceback (most recent call last):\r\n  File \"run.py\", line 83, in <module>\r\n    main()\r\n  File \"run.py\", line 70, in main\r\n    accuracy, num_of_classes = train_Posture(unique_name)\r\n  File \"run.py\", line 31, in train_Posture\r\n    acc = neuro.train(picdb, train_ids, test_ids, \"Posture\")\r\n  File \"A:\\200307 3rd Try\\neuro.py\", line 161, in train\r\n    callbacks=[LoggingCallback(self.logger.debug)])\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 599, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n         [[node MatMul_6 (defined at A:\\200307 3rd Try\\neuro.py:161) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n [Op:__inference_distributed_function_764]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n\r\n2020-03-08 20:51:00.785175: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n```", "comments": ["@aaudiber could you please take a look? thank you", "Trying to find a work around I have reduces the epochs to 1, and instead tried a loop, which gives me a slightly different error, but still a memory leak:\r\n\r\n```\r\nStart training\r\nStarting Epoch 1 of 25\r\n2020-03-17 21:30:09.914586: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 4113907200 exceeds 10% of free system memory.\r\n2020-03-17 21:30:10.268434: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 4113907200 exceeds 10% of free system memory.\r\n43/43 [==============================] - 111s 3s/step - loss: 871.6517 - accuracy: 0.0999\r\nStarting Epoch 2 of 25\r\n43/43 [==============================] - 116s 3s/step - loss: 136.0917 - accuracy: 0.1930\r\nStarting Epoch 3 of 25\r\n43/43 [==============================] - 113s 3s/step - loss: 67.1135 - accuracy: 0.2776\r\nStarting Epoch 4 of 25\r\n43/43 [==============================] - 116s 3s/step - loss: 50.1236 - accuracy: 0.3205\r\nStarting Epoch 5 of 25\r\n43/43 [==============================] - 120s 3s/step - loss: 24.6999 - accuracy: 0.4353\r\nStarting Epoch 6 of 25\r\n43/43 [==============================] - 120s 3s/step - loss: 21.4684 - accuracy: 0.4484\r\nStarting Epoch 7 of 25\r\n2020-03-17 21:43:49.960918: W tensorflow/core/framework/op_kernel.cc:1737] OP_REQUIRES failed at matmul_op.cc:481 : Resource exhausted: OOM when allocating tensor with shape[1279200,804] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nKilled\r\n```\r\nHere is the small change I did to the code (original see above):\r\n\r\n```\r\n        for i in range(0,self._Epochs):\r\n            print(\"Epoch {} of {}\".format(i+1,self._Epochs))\r\n            self.model.fit(x=training_generator,\r\n                        use_multiprocessing=False,\r\n                        workers=6, \r\n                        epochs=1, \r\n                        steps_per_epoch = len(training_generator),\r\n                        callbacks=[LoggingCallback(self.logger.debug)])\r\n```", "@Tuxius Is it possible to reproduce the issue using fake data? If you can provide a minimal, self-contained repro, that will help a lot in finding the root cause.", "I have also the same issue: **Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled** ", "I am seeing this issue as well.  Memory increases at the beginning of each epoch and fills up quickly.", "I am having similar issues (Error..finalizing GeneratorDataset iterator..) with the same script I have successfully run before. I recently upgraded to Tensorflow v2.1. I just downgraded to an earlier Tensorflow version and the script works without error; downgraded via \"conda install tensorflow=1.15.0\", miniconda3 python v3.7, numpy 1.18.1. \r\n\r\nNote: I use Keras in R to access tensorflow backend functions. It is possible that there is some conflict between Keras and Tensorflow?? \r\n\r\nI am putting together a sample script/data to try and reproduce this error for the group here. Hopefully we can identify a solution.\r\n\r\n-------------------------\r\n-------------------------\r\n\r\nI can confirm that I got my script working using the procedure below (from terminal):\r\n\r\n- conda install tensorflow=2.0.0\r\n- conda install -c conda-forge keras=2.3.0\r\n- source ~/path/miniconda3/bin/activate root\r\n- Rscript ~/path/DL_script.R\r\n\r\nMy script runs successfully without error. I tested separate versions of tensorflow and it appears that tensorflow 2.1 is not compatible with keras version or there is some other conflict that was resolved when running the procedures above. I also verified that installing tensorflow via conda was sufficient -- I did not have to specify \"conda install tensorflow-gpu\" to get tensorflow to use native GPU on my system.  From terminal, \"nvidi-smi\" shows that GPU is being used when running my code, and also, from within R, \"tf$test$is_gpu_available()\" shows that GPU returns TRUE.\r\n\r\nHopefully this helps people.", "Got same problem here. And this only happens when I specify the number of workers. But removing this argument will slow down the process. ", "Yes, I can confirm that setting the number of workers to 1 or just leaving out the argument completely solves the problem! It doesn't crash anymore and the memory consumption is stable. Only with workers set to >1 it crashes.", "I update tf to tf-nightly. Error was gone. ", "Also updated to tf-nightly 2.2.0.dev20200319, but still get a crash with workers > 1. Also tried several other of the last nightlies, still get crashed. Only with workers = 1 it runs for me :-(", "@aaudiber: I created a minimal, self-contained repro for you:\r\n\r\nrun.py:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport datagenerator\r\n\r\nPicX = 300\r\nPicY = 300\r\nColor = (255,255,255)\r\n\r\ndef main():\r\n    print(\"Starting a minimal, self-contained error reproduction\")\r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.Flatten(input_shape=(PicX, PicY, 3)))\r\n    model.add(tf.keras.layers.Dense(600, activation='relu'))    \r\n    model.add(tf.keras.layers.Dense(150, activation='relu'))        \r\n    model.add(tf.keras.layers.Dense(3, activation='softmax'))\r\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n    training_generator = datagenerator.DataGenerator(100, PicX, PicY, Color)\r\n    print(\"Starting training\")\r\n    model.fit(x=training_generator, workers=1, epochs=50, steps_per_epoch = len(training_generator))\r\n    print(\"Fit without error with one worker!\")\r\n    model.fit(x=training_generator, workers=6, epochs=50, steps_per_epoch = len(training_generator))\r\n    print(\"Fit without error with six worker!\") #For me it crashed before\r\n\r\nif __name__ == '__main__':\r\n    main()   \r\n```\r\n\r\ndatagenerator.py:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom PIL import Image, ImageDraw\r\n\r\nclass DataGenerator(tf.keras.utils.Sequence):\r\n    def __init__(self, BatchSize, PicX, PicY, Color):\r\n        self._BatchSize = BatchSize\r\n        self._dim = (PicX, PicY)\r\n        self._Color = Color\r\n        \r\n    def __len__(self):\r\n        return 100\r\n        \r\n    def create_random_form(self):\r\n        img = Image.new('RGB', self._dim, (50,50,50))\r\n        draw = ImageDraw.Draw(img)\r\n        label = np.random.randint(3)\r\n        x0 = np.random.randint(int((self._dim[0]-5)/2))+1\r\n        x1 = np.random.randint(int((self._dim[0]-5)/2))+int(self._dim[0]/2)\r\n        y0 = np.random.randint(int((self._dim[1]-5)/2))\r\n        y1 = np.random.randint(int((self._dim[1]-5)/2))+int(self._dim[1]/2)\r\n        if label == 0:\r\n            draw.rectangle((x0,y0,x1,y1), fill=self._Color)\r\n        elif label == 1:\r\n            draw.ellipse((x0,y0,x1,y1), fill=self._Color)                \r\n        else:\r\n            draw.polygon([(x0,y0),(x0,y1),(x1,y1)], fill=self._Color)     \r\n        return img, label\r\n        \r\n    def __getitem__(self, index):\r\n        X = np.empty((self._BatchSize, *self._dim, 3))\r\n        y = np.empty((self._BatchSize), dtype=int)\r\n        for i in range(0,self._BatchSize):\r\n            img, label = self.create_random_form()\r\n            X[i,] = tf.keras.preprocessing.image.img_to_array(img) / 255.0\r\n            y[i] = label\r\n        return X, y\r\n```\r\n\r\nfor me this works with workers = 1 but crashes with workers = 6 ...", "Thank you very much for providing the reproduction and narrowing it down to the use of the `workers` argument. The GeneratorDataset warning is a red herring. The root cause is a memory leak in Keras, which I created a fix for and verified that it resolves your issue. The fix should be submitted later this week.", "I used \"conda install tensorflow-gpu\" to install my tensorflow environment.\r\nHow do I consume this fix into my conda env? =)", "I'm using the tensorflow image tensorflow/tensorflow:2.1.0-gpu-py3 from docker hub: https://hub.docker.com/r/tensorflow/tensorflow/tags/?page=1\r\n\r\nI'm also interested in consuming the fix. I'm using tf.keras. ", "@geetachavan1 Thanks!", "@wendell-hom\r\n> I used \"conda install tensorflow-gpu\" to install my tensorflow environment.\r\n> How do I consume this fix into my conda env? =)\r\n\r\nThis issue happen to me today, and I also happen to use conda so I think I might share it to you as well,\r\n\r\n```shell\r\nconda create -n tf22 python=3.7 cudnn cupti cudatoolkit=10.1.243\r\npip install tensorflow==2.2.0rc3\r\nconda activate tf22\r\n```\r\n\r\nFor tf2, tensorflow already support GPU if it can open all the libary", "I am with `Python 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 14:00:49) [MSC v.1915 64 bit (AMD64)] on win32` and Tensorflow `2.1.0` I am getting this `2021-02-06 07:19:20.301919: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n` error while executing this project `https://github.com/JackonYang/captcha-tensorflow`. :(", "We're in 2022 and still there is no clear answer for the problem. I get the same message on all versions of tensorflow and with all kinds of gpu configurations. Nothing is working. This happens by end of each training process. "]}, {"number": 37514, "title": "[MLIR/XLA] fix wrong tilingSize dimension in lhlo-fuse-linalg pass", "body": "The default action of lhlo-fuse-linalg pass is to tile all loops which is equal to `getNumLoops()` instead of `getNumInputsAndOutputs()`. The number of inputs and outputs has nothing to do with the dimension number.\r\n\r\nAs shown in the added test, for generic ops having 4-dimension operands , original behavior is to tile the first three loops which is equal to `getNumInputsAndOutputs()` (we have two inputs and one output), and new behavior is to tile all the four loops.", "comments": ["Could you expand the PR description to say why it was wrong and what would have happened vs what should have happened? Thanks", "@jpienaar \r\n\r\nAccording to my understanding, the default action of lhlo-fuse-linalg pass is to tile all loops which is equal to `genericOp.getNumLoops()` instead of `getNumInputsAndOutputs()`. The number of inputs and outputs has nothing to do with the dimension number.\r\n\r\nAs shown in the added test, for generic ops having 4-dimension operands , original behavior is to tile the first three loops which is equal to  `getNumInputsAndOutputs()` (we have two inputs and one output),  and new behavior is to tile all the four loops.", "Could you add that to the PR description so that the commit would be easy to follow in git logs for future folks?", "Done! Thanks!"]}, {"number": 37513, "title": "\"Segmentation fault\" on Amazon deep learning AMI, gpu-compute instance", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Deep Learning AMI (Amazon Linux) Version 27.0 (ami-0e2a8509db267f072)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): source activate tensorflow2_p36 on AMI\r\n- Python version: - Bazel 3.6\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource):  None\r\n- CUDA/cuDNN version: - GPU model and memory:\r\np3.2xlarge AWS instance.  NVIDIA Tesla V100 15gb. 10.1 cuda, 7.6 cudnn\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nmodel.fit() just stops randomly with \"segmentation fault\" no other info\r\n\r\n**Describe the expected behavior**\r\n\r\nNo errors or at least more debugging info\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@ben-arnao \r\n\r\nRequest you to share simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@ben-arnao \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37512, "title": "InaccessibleTensorError when appending to list, while looping.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): binary\r\n- Python version: - Bazel\r\nversion (if compiling from source): 2.7.15\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:  N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI get an `InaccessibleTensorError` when looping over a tensor and appending to a python list, only sometimes. \r\n\r\n\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n######################\r\n[codalab link](https://colab.research.google.com/drive/1dMyCJnC9iPieSLEiPgXKWiJhJHftqLtQ)\r\n#######################\r\n\r\nPython script:\r\nThe error happens at the line `my_list.append(new_vals)`.\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\n\r\n\r\ndef list_each_row_times_two(OG_values):\r\n    my_list = []\r\n    for old_vals in OG_values:\r\n        new_vals = tf.math.multiply(old_vals, 2)\r\n        my_list.append(new_vals) #Fails here\r\n        #tf.print(new_vals) #replace above line with this, it works\r\n    return my_list\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    test_func = tf.function(list_each_row_times_two)\r\n    ##########################################\r\n    ########      This works!!   #############\r\n    ##########################################\r\n    _OG_values = []\r\n    for _ in range(3):\r\n        _OG_values.append(tf.random.uniform((2,)))\r\n    new_values = test_func(_OG_values)\r\n    print('We did it!')\r\n    ##########################################\r\n    ########     !!!! DOES NOT WORK!!    #####\r\n    ##########################################\r\n    OG_values = tf.random.uniform((3, 2))\r\n    new_values = test_func(OG_values)\r\n    print('We did it! again!')\r\n\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nWe did it!\r\nTraceback (most recent call last):\r\n  File \"unkown_looping_bug.py\", line 30, in <module>\r\n    new_values = test_func(OG_values)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 606, in _call\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2362, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 983, in func_graph_from_py_func\r\n    expand_composites=True)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/util/nest.py\", line 568, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 945, in convert\r\n    x = deps_ctx.mark_as_return(x)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/framework/auto_control_deps.py\", line 167, in mark_as_return\r\n    tensor = array_ops.identity(tensor)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 267, in identity\r\n    ret = gen_array_ops.identity(input, name=name)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 3829, in identity\r\n    \"Identity\", input=input, name=name)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 742, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 591, in _create_op_internal\r\n    inp = self.capture(inp)\r\n  File \"/home/philip/ros_ws/src/real_world/venv/local/lib/python2.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 641, in capture\r\n    % (tensor, tensor.graph, self))\r\ntensorflow.python.framework.errors_impl.InaccessibleTensorError: The tensor 'Tensor(\"Mul:0\", shape=(2,), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=while_body_56, id=139762147406480); accessed from: FuncGraph(name=list_each_row_times_two, id=139762146659984).\r\n```\r\n", "comments": ["Was able to reproduce the issue with [TF 2.1](https://colab.sandbox.google.com/gist/amahendrakar/5a83dc663fb1354abc81409a1516c9e3/37512.ipynb) and [TF-nightly](https://colab.sandbox.google.com/gist/amahendrakar/68b7db6ce215b021abc9d40cd923f528/tf-nightly-37512.ipynb). Please find the attached gist. Thanks!", "@ammirato I think this is tracing behavior of the functions in the graph. When you run the first `This works` block of code, it creates a function graph and ops within the graph. In this first block, the argument `_OG_values` is a list of tensor. \r\n```\r\n    _OG_values = []\r\n    for _ in range(3):\r\n        _OG_values.append(tf.random.uniform((2,)))\r\n    #print(_OG_values)\r\n    new_values = test_func(_OG_values)\r\n```\r\nWhenever there is a change in input signature of argument that passed to function graph, it tries to create another function graph. In the second code block, the input argument `OG_values` is a  tensor of shape (3, 2) which means it cannot access original graph and the ops (MUL here) defined within it. The error clearly describes it. Thanks! \r\n\r\nIf there is no change in input signature, then everything should work as expected. Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "@jvishnuvardhan Nope I don't think that's it. Did you try it?\r\n\r\n[Here's another codalab without the first block](https://colab.research.google.com/drive/18mZ5h8Z2RXwXUmHnGeIoVIS0nOxmHxZG). same error. Here's the code:\r\n\r\n\r\n```\r\n\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\n\r\n\r\ndef list_each_row_times_two(OG_values):\r\n    my_list = []\r\n    for old_vals in OG_values:\r\n        new_vals = tf.math.multiply(old_vals, 2)\r\n        my_list.append(new_vals) #Fails here\r\n        #tf.print(new_vals) #replace above line with this, it works\r\n    return my_list\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    test_func = tf.function(list_each_row_times_two)\r\n    ##########################################\r\n    ########     !!!! DOES NOT WORK!!    #####\r\n    ##########################################\r\n    OG_values = tf.random.uniform((3, 2))\r\n    new_values = test_func(OG_values)\r\n    print('We did it! again!')\r\n\r\n```", "Python lists don't work well with TF control flow at the moment. You can find more info about this error in the [AutoGraph reference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/common_errors.md#inaccessibletensorerror-the-tensor-name-is-defined-in-another-function-or-code-block). The reason why it works in the first case is because the input is a Python list, so the `for` loop runs in Python at graph construction (the graph will cpotain the body replicated three times). In the second case, because it iterates over a Tensor, the loop must run in the TF graph (as a tf.while_loop), and you can't use Python collections in there.\r\n\r\nWe hope to support this properly in the future (see below), but for now we have a few alternatives:\r\n\r\n1. TensorArray:\r\n\r\n```\r\n    my_list = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\n    for old_vals in OG_values:\r\n        new_vals = tf.math.multiply(old_vals, 2)\r\n        my_list = my_list.write(my_list.size(), new_vals)\r\n    return my_list.stack()\r\n```\r\n\r\n2. concat:\r\n\r\n```\r\n    my_list = tf.zeros([0])\r\n    for old_vals in OG_values:\r\n        tf.autograph.experimental.set_loop_options(\r\n            shape_invariants=[(my_list, tf.TensorShape([None]))])\r\n        new_vals = tf.math.multiply(old_vals, 2)\r\n        my_list = tf.concat([my_list, new_vals], 0)\r\n    return my_list\r\n```\r\n\r\nAs a side note, there is an experimental option in AutoGraph that lets you work with TensorArrays using the list syntax, but that's not yet ready to use.\r\n\r\n```\r\ndef list_each_row_times_two(OG_values):\r\n    my_list = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\n    for old_vals in OG_values:\r\n        new_vals = tf.math.multiply(old_vals, 2)\r\n        my_list.append(new_vals)\r\n    return my_list.stack()\r\n\r\n\r\ntest_func = tf.function(\r\n    list_each_row_times_two,\r\n    experimental_autograph_options=tf.autograph.experimental.Feature.LISTS)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37512\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37512\">No</a>\n", "> Python lists don't work well with TF control flow at the moment. You can find more info about this error in the [AutoGraph reference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/common_errors.md#inaccessibletensorerror-the-tensor-name-is-defined-in-another-function-or-code-block). The reason why it works in the first case is because the input is a Python list, so the `for` loop runs in Python at graph construction (the graph will cpotain the body replicated three times). In the second case, because it iterates over a Tensor, the loop must run in the TF graph (as a tf.while_loop), and you can't use Python collections in there.\r\n> \r\n> We hope to support this properly in the future (see below), but for now we have a few alternatives:\r\n> \r\n> 1. TensorArray:\r\n> \r\n> ```\r\n>     my_list = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\n>     for old_vals in OG_values:\r\n>         new_vals = tf.math.multiply(old_vals, 2)\r\n>         my_list = my_list.write(my_list.size(), new_vals)\r\n>     return my_list.stack()\r\n> ```\r\n> \r\n> 1. concat:\r\n> \r\n> ```\r\n>     my_list = tf.zeros([0])\r\n>     for old_vals in OG_values:\r\n>         tf.autograph.experimental.set_loop_options(\r\n>             shape_invariants=[(my_list, tf.TensorShape([None]))])\r\n>         new_vals = tf.math.multiply(old_vals, 2)\r\n>         my_list = tf.concat([my_list, new_vals], 0)\r\n>     return my_list\r\n> ```\r\n> \r\n> As a side note, there is an experimental option in AutoGraph that lets you work with TensorArrays using the list syntax, but that's not yet ready to use.\r\n> \r\n> ```\r\n> def list_each_row_times_two(OG_values):\r\n>     my_list = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\n>     for old_vals in OG_values:\r\n>         new_vals = tf.math.multiply(old_vals, 2)\r\n>         my_list.append(new_vals)\r\n>     return my_list.stack()\r\n> \r\n> \r\n> test_func = tf.function(\r\n>     list_each_row_times_two,\r\n>     experimental_autograph_options=tf.autograph.experimental.Feature.LISTS)\r\n> ```\r\n\r\nTensorArray solution is very slow!"]}, {"number": 37511, "title": "Error retrieving regularization losses after adding them to a pretrained model.", "body": "It appears that adding regularization losses to a pre-trained model results in some issue which leads to an error when retrieving the losses. I am having this problem with the TensorFlow 2.2.  Its git version is `v1.12.1-26428-gcb73044`.\r\n\r\nThe reproducible code is shown below:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self._model = tf.keras.applications.ResNet101(include_top=False, weights='imagenet')\r\n\r\n    def build(self, input_shape=None):\r\n        for layer in self._model.layers:\r\n            if type(layer) == tf.keras.layers.Conv2D:\r\n                layer.add_loss( lambda : tf.keras.regularizers.l2(1e-5)(layer.kernel))\r\n\r\n    def call(self, x):\r\n        return self._net(x)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    m = Model()\r\n    x = tf.random.uniform(shape=(1,3,512,512))\r\n   m.build()\r\n   print(m.losses)\r\n```\r\n   \r\n\r\nThe error message is -\r\n\r\n `AttributeError: 'Activation' object has no attribute 'kernel'`\r\n\r\n    \r\n", "comments": ["I was able to reproduce the issue with Tf-nightly.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/74bc7c8242362aefd7db0798b57ea2ef/untitled440.ipynb). Thanks!", "@ujjwal-ai , Please find below code. It resolves the error.\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self._model = tf.keras.applications.ResNet101(include_top=False, weights='imagenet')\r\n\r\n    def build(self, input_shape=None):\r\n        for layer in self._model.layers:\r\n            if type(layer) == tf.keras.layers.Conv2D:\r\n                layer.kernel_regularizer = tf.keras.regularizers.l2(1e-5)\r\n\r\n    def call(self, x):\r\n        return self._model(x)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    m = Model()\r\n    x = tf.random.uniform(shape=(1, 512, 512, 3))\r\n    m.build()\r\n    m.call(x)\r\n    print(m.losses)\r\n```\r\n\r\nIt turns out that `add_loss()` is not compatible with eager execution. Please read more about it [here](https://github.com/tensorflow/compression/issues/9).", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37511\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37511\">No</a>\n", "> @ujjwal-ai , Please find below code. It resolves the error.\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> \r\n> class Model(tf.keras.Model):\r\n>     def __init__(self):\r\n>         super(Model, self).__init__()\r\n>         self._model = tf.keras.applications.ResNet101(include_top=False, weights='imagenet')\r\n> \r\n>     def build(self, input_shape=None):\r\n>         for layer in self._model.layers:\r\n>             if type(layer) == tf.keras.layers.Conv2D:\r\n>                 layer.kernel_regularizer = tf.keras.regularizers.l2(1e-5)\r\n> \r\n>     def call(self, x):\r\n>         return self._model(x)\r\n> \r\n> \r\n> if __name__ == \"__main__\":\r\n>     m = Model()\r\n>     x = tf.random.uniform(shape=(1, 512, 512, 3))\r\n>     m.build()\r\n>     m.call(x)\r\n>     print(m.losses)\r\n> ```\r\n> \r\n> It turns out that `add_loss()` is not compatible with eager execution. Please read more about it [here](https://github.com/tensorflow/compression/issues/9).\r\n\r\nHey, thanks for the reference solution. I used this code to add regularization losses for pretrained model. But when I executed the code and listed down the losses, I got empty list. I am expecting a list of regularization losses based on number of convolutional layers in pretrained model.\r\n"]}, {"number": 37510, "title": "tf.config.experimental.list_physical_devices('GPU') stopped listing my gpu after update from 2.0.1 to 2.1.0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm using the following lines to set allow growth on TF 2.0 like I used to with tf.config and sessions on TF1:\r\n```python\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nassert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```\r\nI've updated from 2.0.1 to 2.1.0 (via `pip install`) and suddenly `tf.config.experimental.list_physical_devices('GPU')` stopped listing my GPU as an available device.\r\n\r\n**Describe the expected behavior**\r\nWhen downgrading to 2.0.1 (again via `pip install`) everything works again.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nassert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Could you please provide information about your system configuration? Which cuda/cuDNN versions have you installed? You can find tested configurations at https://www.tensorflow.org/install/source#tested_build_configurations. ", "Ok, from your link I have to assume it's a CUDA version issue. I'm currently on Ubuntu 18.04 with CUDA 10.0 + cuDNN 7.6.5\r\nUnfortunately I cannot test this right now, I'll close the issue and comment again with results once I find the time.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37510\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37510\">No</a>\n", "Tensorflow 2.1 is not compatible with CUDA 10.0. You will have install CUDA 10.1 along with 10.0 or make new conda environment and with tensorflow-gpu=2.1.0. Conda will install Cuda 10.1 for that environment.\r\n`conda install anaconda tensorflow-gpu=2.1.0\r\n`"]}, {"number": 37509, "title": "Roadmap link is broken.", "body": "\r\n\r\n## URL(s) with the issue:\r\nREADME.md  Resources's roadmap link is broken.\r\nhttps://www.tensorflow.org/community/roadmap\r\n\r\n## Description of issue (what needs changing):\r\nIt is occurred Page not found errer.\r\n\r\n", "comments": ["I'd like to fix this, but I can't find the proper link. Can anyone plz help??", "@ryujaehun , Please refer to [this commit](https://github.com/tensorflow/docs/commit/297a36784c1f665a21e076301d7fd3603797be33) in the doc repo. It specifies that they have removed the roadmap page on Jan-10, 2020. The reason may be because they may be updating the readme and will upload new readme as soon as they complete gathering the newly added features.", "Closing this issue since its resolved. Thanks!"]}, {"number": 37508, "title": "[TF2.0] How to save model just keep recent 5 models by using keras?", "body": "in eager mode, we can use `CheckpointManager` to save recent 5 models.\r\n```\r\nmanager = tf.train.CheckpointManager(checkpoint, directory='./save', checkpoint_name='model.ckpt', max_to_keep=5)\r\n```\r\nbut how to save model just keep recent 5 models by using kears? \r\n( model.complie() , model.fit(), callbacks )", "comments": ["@SmileTM,\r\nCould you please look into keras [ModelCheckpoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) and let us know if it helps? Thanks!", "> @SmileTM,\r\n> Could you please look into keras [ModelCheckpoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) and let us know if it helps? Thanks!\r\n\r\nModelCheckpoint can't achieve this function. I hope it can achieve this function `max_to_keep` in the future .\r\n\r\nBut I can achieve this function by Overriding `Clallback`.  \r\n \r\n```\r\nclass Save_Callbacks(tf.keras.callbacks.Callback):\r\n    def __init__(self, checkpoint_manager):\r\n        self.checkpoint_manager = checkpoint_manager\r\n\r\n    def on_train_batch_end(self, batch, logs=None):\r\n        super().on_train_batch_end(batch, logs)\r\n        self.checkpoint_manager.save()\r\n\r\ncheckpoint = tf.train.Checkpoint(model=model, optimizer=model.optimizer)\r\ncheckpoint_manager = tf.train.CheckpointManager(checkpoint, 'save', max_to_keep=5)\r\nsave_callbacks=  Save_Callbacks(checkpoint_manager )\r\n```"]}, {"number": 37507, "title": "Fix typo", "body": "Fix miscellaneous typos and format.", "comments": []}, {"number": 37506, "title": "Tensorflow hexagon tflite benchmark fails with quantized mobilenetv2", "body": "\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version:  Build from: git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source):gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- Device: Redmi Note 7 Pro (Hexagon 685 DSP),  Android 10.0; MIUI 11\r\n\r\n**Describe the current behavior**\r\nWhen i try to run the official **quantized mobilenet v2 model** in tflite benchmark  using **hexagon** delegate, it fails with the following error:-\r\n\r\n> adb shell /data/local/tmp/benchmark_model_tf15 --graph=/data/local/tmp/mobilenet_v2_1.0_224_quant.tflite --enable_op_profiling=true --use_hexagon=true\r\n> adb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)\r\n> STARTING!\r\n> Min num runs: [50]\r\n> Min runs duration (seconds): [1]\r\n> Max runs duration (seconds): [150]\r\n> Inter-run delay (seconds): [-1]\r\n> Num threads: [1]\r\n> Benchmark name: []\r\n> Output prefix: []\r\n> Min warmup runs: [1]\r\n> Min warmup runs duration (seconds): [0.5]\r\n> Graph: [/data/local/tmp/mobilenet_v2_1.0_224_quant.tflite]\r\n> Input layers: []\r\n> Input shapes: []\r\n> Input value ranges: []\r\n> Use legacy nnapi : [0]\r\n> Allow fp16 : [0]\r\n> Require full delegation : [0]\r\n> Enable op profiling: [1]\r\n> Max profiling buffer entries: [1024]\r\n> CSV File to export profiling data to: []\r\n> Use gpu : [0]\r\n> Allow lower precision in gpu : [1]\r\n> Use Hexagon : [1]\r\n> Hexagon lib path : [/data/local/tmp]\r\n> Hexagon Profiling : [0]\r\n> Use nnapi : [0]\r\n> Use xnnpack : [0]\r\n> Loaded model /data/local/tmp/mobilenet_v2_1.0_224_quant.tflite\r\n> INFO: Initialized TensorFlow Lite runtime.\r\n> loaded libcdsprpc.so\r\n> INFO: Created TensorFlow Lite delegate for Hexagon.\r\n> INFO: Hexagon delegate: 65 nodes delegated out of 65 nodes.\r\n> \r\n> ----------------\r\n> Timestamp: Wed Mar 11 12:45:53 2020\r\n> \r\n> \r\n> Log\r\n> hexagon/src/newnode.c:413:node 2 (Quantize_int32_ref): bad input count 12\r\n> hexagon/src/newnode.c:763:node id=0x2 ctor fail\r\n> \r\n> ----------------\r\n> ERROR: Failed: Failed to prepare graph.\r\n> . STATE: FAILED_TO_PREPARE_GRAPH\r\n> **ERROR: Node number 65 (TfLiteHexagonDelegate) failed to prepare.**\r\n> \r\n> ERROR: Restored previous execution plan after delegate application failure.\r\n> Failed to apply Hexagon delegate.\r\n> Benchmarking failed.\r\n\r\nAll the '.so' files were initially copied to path: /data/local/tmp on device and the delegate was successfully created. I tried adding 'use_nnapi'= true along with use_hexagon option ; but it does not make any difference. \r\n\r\nI followed the instruction to build the hexagon delegate and libraries from the official documentation\r\n\r\n> bazel build --config=android_arm64 \\\r\n>   tensorflow/lite/experimental/delegates/hexagon/hexagon_nn:libhexagon_interface.so\r\n> adb push bazel-bin/tensorflow/lite/experimental/delegates/hexagon/hexagon_nn/libhexagon_interface.so /data/local/tmp\r\n> adb push libhexagon_nn_skel*.so /data/local/tmp\r\n(32 bit version gave an error, so i used arm64)\r\n\r\nHexagon library: [v1.14](https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.14.run)\r\nModel:[ mobilenet_v2_1.0_224_quant](https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz)\r\n\r\nAlso does the hexagon model execute a tflite generated by **post-training quantization(full INT8)** ?\r\n\r\nWhen  i tried another  **custom quantized** model(post-training quantization) with int8 inputs and outputs, it shows :  **INFO: Hexagon delegate: 0 nodes delegated out of 158 nodes.**  and seems to fall back to CPU.\r\n\r\n**Describe the expected behavior**\r\nThe benchmark model should run the quantized model without any problems.\r\n\r\n**Other info / logs**\r\nAndroid NDK: 20, Benchmark tool built from latest source with bazel 2.0\r\n\r\nHere are the two models that i have tried to benchmark and the corresponding benchmark library files:-\r\n[hexfiles.zip](https://github.com/tensorflow/tensorflow/files/4317362/hexfiles.zip)\r\n", "comments": ["Hi,\r\nI just used your reproduce instructions and worked for me. I think you might have out of sync environment.\r\nCan you please retry with a fresh setup (git clone, download the files and repush them and retry).\r\n\r\nThanks", "For the post training quantized model (int8 version) as you saw from the message 0 nodes delegated.\r\nWe don't support them at the moment, but they are coming soon - work in progress. Stay tuned.\r\n\r\nThanks", "Did it work on same phone ??\r\nCan you post the complete result of benchmark ??\r\nSo does that mean INT8 or UINT8 models  for hexagon delegate only works for quantization aware models(doc mentions something like symmetric quantization)?", "I don't have the exact same phone sadly, but from the error message you got it shouldn't matter. (Unless you have other failure not included here).\r\n\r\nDid you retry ?\r\n\r\nCurrently we only support uint8 (quantization aware training), the int8 post training quantization is coming soon - It's already work in progress.", "Can you run this command and tell me the value.\r\nadb shell cat /sys/devices/soc0/soc_id\r\n\r\n", "Ok thanks, i will try a fresh build in colab itself and shall post the results...\r\nUnfortunately, i don't have the device right  now . I will get back to you tomorrow", "Thanks.\r\n\r\ngetting the soc_id will help me getting as close to the same environment.\r\n\r\nThanks", "Hey, it worked this time with hexagon delegate !!!\r\nI made a fresh build of benchmark tool and hexagon and it worked without any error.\r\nThe command you mentioned gave me value: 355.\r\n\r\nHowever the **opencl gpu delegate failed** with corresponding float version of  model gave error: **segmentation fault.** on the **latest benchmark model** binary.\r\n\r\n`adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/mobilenet_v2_1.0_224.tflite --enable_op_profiling=true --use_gpu=true`\r\n\r\nI was hoping to compare the hexagon delegate performance with gpu ...\r\nAnyway, it worked with my old benchmark tool binary:\r\nThe hexagon delegate took 5.2 ms while gpu took 21.49 seconds...It's impressive !!!\r\n\r\nThanks ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37506\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37506\">No</a>\n", "Thanks for the update. Also, note that executing on the DSP (Hexagon) consumes less power that's another gain :) \r\n\r\n21ms looks a lot for OpenCL GPU.\r\nPlease file a new issue if you're still having problems.\r\n\r\nThanks"]}, {"number": 37505, "title": "Memory leak in model.fit", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\nminimal working example\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nwindows-server 2016\r\n- TensorFlow installed from (source or binary): \r\nconda\r\n- TensorFlow version (use command below): \r\ntf 2.1.0\r\n- Python version: \r\n3.7\r\n- CUDA/cuDNN version: \r\nCUDA 10.1\r\n- GPU model and memory:\r\nK80 - 24Gb\r\n\r\n**Describe the current behavior**\r\nmemory use increases with consecutive training runs, probably related to #35524, #33030, #35124, #35835\r\nside note: I do not understand the warning but this seems to be handled in #37500\r\n**Describe the expected behavior**\r\nmemory should stay constant\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nfrom tensorflow.keras.datasets import cifar10\r\nimport tensorflow.keras.callbacks as callbacks\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.callbacks import LearningRateScheduler\r\nfrom tensorflow.keras.optimizers import SGD\r\nfrom tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Activation, Dense\r\nfrom tensorflow.keras.models import Model\r\nimport tensorflow.keras.utils as kutils\r\nimport numpy as np\r\nimport psutil\r\nimport gc \r\n\r\n\r\nbatch_size = 128\r\nepochs = 5\r\nnum_classes = 10\r\n\r\n\r\ndef buildmodel():\r\n    img_input = Input(shape=(32, 32, 3))\r\n\r\n    x = Conv2D(16, (3, 3), padding='same')(img_input)\r\n    x = Activation(\"relu\")(x)\r\n    x = Conv2D(16, (3, 3), padding='same')(x)\r\n    x = Activation(\"relu\")(x)\r\n    x = GlobalAveragePooling2D()(x)\r\n    prediction = Dense(num_classes,activation='softmax', name = 'classifier') (x)  \r\n    model = Model(inputs=img_input, outputs=prediction)\r\n    return model\r\n\r\n(trainX, trainY), (testX, testY) = cifar10.load_data()\r\nmean = np.mean(trainX, axis=0)\r\nstd = np.std(trainX)\r\n\r\ntrainX = trainX.astype('float32')\r\ntrainX = (trainX - mean) / std\r\ntestX = testX.astype('float32')\r\ntestX = (testX - mean) / std\r\n\r\ntrainY = kutils.to_categorical(trainY)\r\ntestY = kutils.to_categorical(testY)\r\n\r\ngenerator = ImageDataGenerator()\r\ngenerator.fit(trainX)\r\nval_generator = ImageDataGenerator()\r\n\r\nfor i in range(10):\r\n    tf.keras.backend.clear_session()\r\n    model = buildmodel()\r\n    sgd = SGD(lr=0.1, momentum=0.9, nesterov=True)\r\n    model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\r\n    model.fit(generator.flow(trainX, trainY, batch_size=batch_size), epochs=epochs, validation_data=val_generator.flow(testX, testY, batch_size = batch_size),verbose=0, workers = 20)\r\n    print('memory usesd: ' + str(psutil.virtual_memory().used // 1e6))\r\n    gc.collect()\r\n```\r\n\r\noutput:\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nmemory usesd: 38012.0\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nmemory usesd: 38563.0\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nmemory usesd: 39288.0\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nmemory usesd: 40005.0\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nmemory usesd: 40730.0\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nmemory usesd: 41490.0\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nmemory usesd: 42216.0\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nmemory usesd: 42937.0\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nmemory usesd: 43659.0\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nmemory usesd: 44403.0\r\n\r\n\r\n", "comments": ["I have replicated the code shared above, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/1df4ffa08b3cd2491fa3395fe4815dac/37505.ipynb)", "I am trying to fix this issue, I do find the issue on Colab, but on my MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports), there seems no such memory leak issue.\r\n\r\nHere is the output of the program (warning msgs are removed);\r\n```text\r\nmemory usesd: 8968.0\r\nmemory usesd: 9009.0\r\nmemory usesd: 8402.0\r\nmemory usesd: 8299.0\r\nmemory usesd: 8257.0\r\nmemory usesd: 7761.0\r\nmemory usesd: 8642.0\r\nmemory usesd: 8833.0\r\nmemory usesd: 9008.0\r\nmemory usesd: 9267.0\r\n```", "@howl-anderson : do you have something running in parallel? Your memory usage seems not to be constant over time", "@nkoerb Yes, it's a little weird, I will test again without open other applications on Mac and test it on Ubuntu 18.04 too.", "@nkoerb New test on Mac (without open other applications):\r\n\r\n```text\r\nmemory usesd: 6879.0\r\nmemory usesd: 7259.0\r\nmemory usesd: 7620.0\r\nmemory usesd: 7966.0\r\nmemory usesd: 8323.0\r\nmemory usesd: 8694.0\r\nmemory usesd: 9029.0\r\nmemory usesd: 9392.0\r\nmemory usesd: 9486.0\r\nmemory usesd: 9480.0\r\n```", "@howl-anderson looks leaky to me", "@nkoerb Agree! I am will try to fix this later.", "This issue likely relates to #37653", "Hey, I was having the same issue. \r\nMy current workaround is to save the model before deleting it and clearing the backend after each training iteration. Then I reload the model before calling fit again. \r\nI'm no longer experiencing the memory leak with this workaround.", "@DanyelJei how do you clear the backend, because I also used clear_session() and garbage collector? And what exactly do you mean with 'training iteration': full training, epoch or single iteration on a batch?", "Have a similar issue with increasing memory usage when using the ImageDataGenerator and the fit function. Any idea if it will be fixed in 2.2.0?", "I have the same memory leaking problem with model.fit when using dataset from generator. \r\nHopefully it will be fixed some time soon.", "Confirm a memory leak, similar symptoms. Seems to eat about 4 Mbytes/second on my workload. tf2.2.0-1, arch 64bit.", "Also experiencing this issue", "Hey, same issue here on Windows machine. Seems to be related to multiple model fits over generators, it clearly seems like a memory leak from tensorflow side, deleting all variables and calling garbage collector won't fix the issue. ~~Managed to evade the problem by setting `workers = 1`.~~\r\n\r\nEdit: I thought setting `workers=1` would fix the issue as described in a stackoverflow post I can't find right now. Trying now with `workers=0`.\r\nEdit2: Nope, seems this doesn't work either.\r\nEdit3: Managed to make it work with `workers=0` by letting each iteration of training in the loop to be in a function, so that the variables are local, followed by a call to `tf.keras.backend.clear_sesion()` and then `gc.collect()`.", "> Hey, same issue here on Windows machine. Seems to be related to multiple model fits over generators, it clearly seems like a memory leak from tensorflow side, deleting all variables and calling garbage collector won't fix the issue. ~~Managed to evade the problem by setting `workers = 1`.~~\n> \n> \n> \n> Edit: I thought setting `workers=1` would fix the issue as described in a stackoverflow post I can't find right now. Trying now with `workers=0`.\n> \n> Edit2: Nope, seems this doesn't work either.\n> \n> Edit3: Managed to make it work with `workers=0` by letting each iteration of training in the loop to be in a function, so that the variables are local, followed by a call to `tf.keras.backend.clear_sesion()` and then `gc.collect()`.\n\nHi, could you provide some information or example code of fix with your edit3? ", "i noticed the validation_data is actually causing the memory leak. not sure if it's because of my implementation cus I'm using dataset as a wrapper of my multiprocessing data loader. I'm using `tf.data.Dataset.from_generator(generator_fun, ...)` What happens to me is that if you dig into the source code, you will actually see that the validation data and the train x in the fit function are treated differently. \r\n\r\nIn my case, my `generator_fun` is actually getting called every epoch for validation set but for training x, it's not called every time. Then everytime my `generator_fun` generated a new generator which causes the memory leak. I verified this behavior by digging down to the `model.fit` source code. I'm on tf 2.2.0 and you can see  below under [tf.python.keras.engine.training.py](https://github.com/tensorflow/tensorflow/blob/64dbe97a4d5a9a2cc69cacae7762157cd201a1d2/tensorflow/python/keras/engine/training.py#L859) :\r\n\r\n```\r\n        # Run validation.\r\n        if validation_data and self._should_eval(epoch, validation_freq):\r\n          val_x, val_y, val_sample_weight = (\r\n              data_adapter.unpack_x_y_sample_weight(validation_data))\r\n          val_logs = self.evaluate(\r\n              x=val_x,\r\n              y=val_y,\r\n              sample_weight=val_sample_weight,\r\n              batch_size=validation_batch_size or batch_size,\r\n              steps=validation_steps,\r\n              callbacks=callbacks,\r\n              max_queue_size=max_queue_size,\r\n              workers=workers,\r\n              use_multiprocessing=use_multiprocessing,\r\n              return_dict=True)\r\n          val_logs = {'val_' + name: val for name, val in val_logs.items()}\r\n          epoch_logs.update(val_logs)\r\n```\r\nSo the dataset passed in will be put into the evaluate function where it probably called the as_numpy_iterator() and that caused my `generator_fun` got called again and again. However the training generator didn't show this behavior. Then I modified my generator_fun so that every time it's called it's not creating a new instance but reusing the same generator that has already been created, then the memory leak is gone. \r\n\r\n\r\n I'm not sure how the ImageDataGenerator.flow_from_directory work but maybe you can check that. Also i notice they modified this part of code on master. I haven't tried nightly yet. maybe they already fixed it. Just a different perspective. \r\n\r\n", "Is there any solution? I am still facing it now..", "I ran into a similar problem. Workaround which seems to work for me is to force garbage collector:\r\n\r\n\r\n```\r\nimport gc\r\n...\r\nmodel.fit(...)\r\ngc.collect()\r\n```", "@nkoerb Related to performance, there were some significant improvements in the recent version. I ran your code with `tf-nightly` and I don't see any improvement in the memory as much as you have noticed. When I ran your code with `tf-nightly`, i see the following ouput.\r\n\r\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\r\n170500096/170498071 [==============================] - 2s 0us/step\r\nmemory usesd: 3981.0\r\nmemory usesd: 3981.0\r\nmemory usesd: 3976.0\r\nmemory usesd: 3975.0\r\nmemory usesd: 3975.0\r\nmemory usesd: 3973.0\r\nmemory usesd: 3973.0\r\nmemory usesd: 3970.0\r\n\r\nPlease take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/8557bb197806600d5c998e37f90f8e08/37505.ipynb).\r\n\r\nCan you please verify once with `tf-nightly` and close the issue if this was resolved for you. Thanks!\r\n", "@jvishnuvardhan Thanks for your help and great work, memory is constant with tf-nightly! \r\nAs a side note: When I remove either `gc.collect()` or `tf.keras.backend.clear_session()` memory is leaking, so both commands are needed for constant memory usage when performing consecutive trainings.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37505\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37505\">No</a>\n", "Is the solve of this issue applied to the current tensorflow release 2.3.2?\r\nI am still having memory leakage with the default version of tensorflow.", "@krenerd Can you please open a new issue with a simple standalone code to reproduce your issue? Thanks! You can refer this issue in the new issue. Thanks", "I encountered similar problems with keras.fit + tf.dataset, for me the problem is `prefetch`. I changed\r\n```python\r\ndatasets = datasets.prefetch(dim * batch_size)\r\n```\r\nto\r\n```python\r\ndatasets = datasets.prefetch(tf.data.experimental.AUTOTUNE)\r\n```\r\nand the memory leak problem is solved, hope it helps.", "> I encountered similar problems with keras.fit + tf.dataset, for me the problem is `prefetch`. I changed\r\n> \r\n> ```python\r\n> datasets = datasets.prefetch(dim * batch_size)\r\n> ```\r\n> \r\n> to\r\n> \r\n> ```python\r\n> datasets = datasets.prefetch(tf.data.experimental.AUTOTUNE)\r\n> ```\r\n> \r\n> and the memory leak problem is solved, hope it helps.\r\n\r\n@Sherry40931 hello, Im facing the same problem, and Im using tf.dataset.from_tensor_slices(), when training with keras.model.fit() distributedly with TF 2.4.1, memory usage keeps increasing. Below is the main procedure of my dataset part.\r\n```\r\nds = tf.data.Dataset.from_tensor_slices()\r\nds = ds.map().cache()\r\nds = ds.shuffle(reshuffle_each_iteration=True).batch()\r\nkeras.model.fit(ds)\r\n```\r\nCan u share some advice for me ?", "@SysuJayce The following code solved the problem for me. \r\n\r\n```\r\nimport gc\r\n\r\n...\r\n\r\ntf.keras.backend.clear_session()\r\ngc.collect()\r\n```", "> @SysuJayce The following code solved the problem for me.\r\n> \r\n> ```\r\n> import gc\r\n> \r\n> ...\r\n> \r\n> tf.keras.backend.clear_session()\r\n> gc.collect()\r\n> ```\r\n@krenerd Hi, thx for the reply. Where should I add this part in my code? ", "@SysuJayce In my code, the memory leakage happened after every iteration of reading images and training one epoch epoch on it. So I appended the code after every iteration. Although this is not a fundamental fix to this problem we experienced, it fixed my issue. \r\n```\r\nfor epoch in range(epochs):\r\n  data,labels=dataset.load_train(...)\r\n  self.model.fit(data,labels)\r\n  tf.keras.backend.clear_session()\r\n  gc.collect()\r\n```", "> @SysuJayce In my code, the memory leakage happened after every iteration of reading images and training one epoch epoch on it. So I appended the code after every iteration. Although this is not a fundamental fix to this problem we experienced, it fixed my issue.\r\n> \r\n> ```\r\n> for epoch in range(epochs):\r\n>   data,labels=dataset.load_train(...)\r\n>   self.model.fit(data,labels)\r\n>   tf.keras.backend.clear_session()\r\n>   gc.collect()\r\n> ```\r\n\r\n@krenerd unfortunately, I tried your code but memory leak still exists", "hi @SysuJayce, I think the order of your functions is correct, could you provide the parameters/functions you send to map, cache and batch? ", "> hi @SysuJayce, I think the order of your functions is correct, could you provide the parameters/functions you send to map, cache and batch?\r\n\r\n@Sherry40931  hi, actually, I did a dataset benchmark test, and found no memory leak. So maybe the problem is within the fit() / predict()?\r\n\r\nI use TextVectorization layer to convert text to vector in my map(), and cache() has no parameter, and batch(batch_size=32).\r\n```\r\ndef vectorize_text(text, label):\r\n    text = tf.expand_dims(text, -1)\r\n    return vectorize_layer(text), label\r\n```\r\n\r\nBelow is the dataset memory test snippet\r\n```\r\nfor it in ds.repeat():\r\n    used_mem = psutil.virtual_memory().used\r\n    print(f\"used memory: {used_mem / 1024 / 1024} Mb\")\r\n```", "https://fantashit.com/linearly-increasing-memory-with-use-multiprocessing-and-keras-sequence/#comment-254237", "> https://fantashit.com/linearly-increasing-memory-with-use-multiprocessing-and-keras-sequence/#comment-254237\r\n\r\nThis is the only thing that worked for me. I was experiencing a memory leak each epoch during the validation phase with a data generator (it didn't happened if I didn't validate). Apparently the fit() function doesn't handle well the data generator for validation when using multiprocessing.\r\n\r\nSpent weeks trying to solve it, tried with gc.collect(), K.clear_session(), among other; but no luck. By wrapping the generator with GeneratorEnqueuer (https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/utils/GeneratorEnqueuer) to handle multiprocessing before passing it to fit() (workers=1 and multiprocessing=False must be set in fit()), the memory leak was gone.\r\n\r\nI am using tf 2.3.2", "> > https://fantashit.com/linearly-increasing-memory-with-use-multiprocessing-and-keras-sequence/#comment-254237\r\n> \r\n> This is the only thing that worked for me. I was experiencing a memory leak each epoch during the validation phase with a data generator (it didn't happened if I didn't validate). Apparently the fit() function doesn't handle well the data generator for validation when using multiprocessing.\r\n> \r\n> Spent weeks trying to solve it, tried with gc.collect(), K.clear_session(), among other; but no luck. By wrapping the generator with GeneratorEnqueuer (https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/utils/GeneratorEnqueuer) to handle multiprocessing before passing it to fit() (workers=1 and multiprocessing=False must be set in fit()), the memory leak was gone.\r\n> \r\n> I am using tf 2.3.2\r\n\r\nWorked for me too. \r\nCan't believe that this problem still remains unsolved in 2021 on tf2.5", "I found the solution!\r\nNot use this piece of shit library written by fucking unprincipled dumbasses and morons. What they can do great is just do fucking plastic smiles, pass difficult useless interviews and lick ass to their bosses from the mose fascist organization of all times. Please die from cancer!\r\nPytorch forever", "This bug seems to be fixed in `tf.nightly` as of June 28th. \r\nhttps://github.com/tensorflow/tensorflow/issues/50379", "> Then I modified my generator_fun so that every time it's called it's not creating a new instance but reusing the same generator that has already been created, then the memory leak is gone.\r\n\r\n@ysyyork Thank you for this insight! May I know what modifications have you made to your generator function to reuse the same generator?", "> > Then I modified my generator_fun so that every time it's called it's not creating a new instance but reusing the same generator that has already been created, then the memory leak is gone.\r\n> \r\n> @ysyyork Thank you for this insight! May I know what modifications have you made to your generator function to reuse the same generator?\r\n\r\n@jiunyen-ching I used a singleton pattern to implement this generator. ", "Can anyone advise how I check since which version this bug is implemented in a stable release? Can't find any info on the releases page:\r\nhttps://github.com/tensorflow/tensorflow/releases?page=1", "@shir994 I think it was solved from tf version 2.3 -> 2.4"]}, {"number": 37504, "title": "Fit under TPU strategy fails on cardinality", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ran on colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: /\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.1.0 (Colab)\r\n- Python version: - 3.6.9\r\nversion (if compiling from source): /\r\n- GCC/Compiler version (if compiling from\r\nsource): /\r\n- CUDA/cuDNN version: - GPU model and memory: Using the TPU runtime\r\n\r\nWhen running the `fit` function on a model after compiling the model with the TPUStrategy as in [this post on the colab github](https://github.com/googlecolab/colabtools/issues/1056) the cardinality is implicitely calculated.\r\nA TFRecordDataset is used as Dataset for the fit function. \r\nThe error below is raised:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-7-db3ea26a4a48> in <module>()\r\n----> 1 mh.start_training()\r\n\r\n8 frames\r\n/content/modelling/NN/ModelHandler.py in start_training(self)\r\n     73                 steps_per_epoch=self.dataloaders[0].amount_of_batches,\r\n     74                 validation_steps=self.dataloaders[1].amount_of_batches,\r\n---> 75                 verbose=1\r\n     76                 )\r\n     77 \r\n\r\n/content/modelling/NN/Model.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    312                              validation_steps=validation_steps, validation_freq=validation_freq,\r\n    313                              max_queue_size=max_queue_size, workers=workers,\r\n--> 314                              use_multiprocessing=use_multiprocessing, **kwargs)\r\n    315         return out\r\n    316 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    258           steps_per_epoch,\r\n    259           steps_name='steps_per_epoch',\r\n--> 260           epochs=0)\r\n    261 \r\n    262       steps_per_epoch = (\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_utils.py in infer_steps_for_dataset(model, dataset, steps, epochs, steps_name)\r\n   1747     return None\r\n   1748 \r\n-> 1749   size = K.get_value(cardinality.cardinality(dataset))\r\n   1750   if size == cardinality.INFINITE and steps is None:\r\n   1751     raise ValueError('When passing an infinitely repeating dataset, you '\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/experimental/ops/cardinality.py in cardinality(dataset)\r\n     49   \"\"\"\r\n     50 \r\n---> 51   return ged_ops.dataset_cardinality(dataset._variant_tensor)  # pylint: disable=protected-access\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/gen_experimental_dataset_ops.py in dataset_cardinality(input_dataset, name)\r\n    663         pass  # Add nodes to the TensorFlow graph.\r\n    664     except _core._NotOkStatusException as e:\r\n--> 665       _ops.raise_from_not_ok_status(e, name)\r\n    666   # Add nodes to the TensorFlow graph.\r\n    667   _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6604   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6605   # pylint: disable=protected-access\r\n-> 6606   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6607   # pylint: enable=protected-access\r\n   6608 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Unable to parse tensor proto [Op:DatasetCardinality]\r\n```\r\n\r\nWhen calculating the cardinality without calling the TPUStrategy context as shown below, a value of `-2` is yielded:\r\n```\r\nimport tensorflow.keras.backend as K\r\nK.get_value(tf.data.experimental.cardinality(ds))\r\n```\r\n\r\n", "comments": ["@jonyvp \r\n\r\nCan you please help us with  colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@jonyvp \r\n\r\nAny update on this issue please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I'm getting a similar problem.  Running fit locally runs no problem, but running it on a TPU fails.  The error message is less informative, but it seems to be failing in the same spot: calculating cardinality.  See the below notebook.  Note the reference to  #https://github.com/googlecolab/colabtools/issues/1056\r\n\r\nhttps://colab.research.google.com/drive/13KUPcl8o03jYfIFj_UjAL3xPtlHH8umH?usp=sharing #"]}, {"number": 37503, "title": "[tflite] allow passing quantized ReLU to NNAPI", "body": "Standalone quantized ReLU is not delegated to NNAPI. Version check modified to allow delegating.", "comments": ["The changes looks good, approved.", "@gbaned I see \"import/copybara \u2014 An error happened while migrating the change\". But I can't see any details. Is there anything I can do?"]}, {"number": 37502, "title": "[tflite] add int8 input/output to label_image", "body": "More and more models, such as MobilenetV3's EdgeTPU ones, are using post-training full integer quantization. With this patch, I can get reasonable results.\r\n```\r\n./label_image_int8 -m mobilenet_edgetpu_224_1.0_int8.tflite\r\nLoaded model mobilenet_edgetpu_224_1.0_int8.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\ninvoked\r\naverage time: 15.363 ms\r\n0.867188: 653 military uniform\r\n0.0390625: 835 suit\r\n0.015625: 458 bow tie\r\n0.0078125: 907 Windsor tie\r\n0.00390625: 716 pickelhaube\r\n```\r\n\r\nThis PR is to reopen #34537, which was merged and reverted because of Google's internal sanity test.", "comments": []}, {"number": 37501, "title": "WARNING:tensorflow:Gradients do not exist for variables", "body": "While training BERT on TPU i am getting these warnings and my precision and recall is zero while accuracy is 100\r\n\r\n```\r\nTrain for 45205 steps, validate for 206 steps\r\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\r\n\r\n```\r\n\r\nHere is google colab file i am using\r\nhttps://colab.research.google.com/drive/1l0Eoram6vJRK5xQBmnCx-eU1e46higlo\r\n\r\nupdated colab file\r\nhttps://colab.research.google.com/drive/1D-eKgddRHROG39R2iyqE3qR7w0N0OHAB", "comments": ["@talhaanwarch,\r\nI was unable to reproduce the issue as the file `task_a_distant.tsv` is missing. Please find the gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/0768ebd3095d4a8410ba8407f45e595e/37501.ipynb).\r\nCould you please share all the supporting files required to run the given code? Thanks!", "@amahendrakar  please check this file, i have updated.\r\nhttps://colab.research.google.com/drive/1D-eKgddRHROG39R2iyqE3qR7w0N0OHAB", "@talhaanwarch,\r\nOn running the updated colab gist, I got the metrics `binary_accuracy: 0.6635 - precision: 0.3663 - recall: 0.0120`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5c2225faee2306e9fce0550bfc55aec1/37501-v2.ipynb). Thanks!", "but the warning is still there. `WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.`  I am thinking it should be an error instead of warning. becuase if gradient donot exist, how it will be updated?", "@talhaanwarch Please take a look at this [issue](https://github.com/huggingface/transformers/issues/2256) which is very similar to this issue.", "@talhaanwarch Can yo please respond to the above comment so that we can take the discussion further. Thanks!", "i requested at [issue ](https://github.com/huggingface/transformers/issues/2256) if he can update his colab notebook with solution, but for me i am unable to figure out the solution", "I'm getting same error. No solution found so far.", "same error founded. ", "same error for me", "@talhaanwarch Please post your issue [here](https://github.com/google-research/bert/issues). Is this happening only on TPUs ? or is it happening on GPUs too?", "I have a similar warning message using TF2.2 on Colab GPU:\r\nWARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss.\r\n\r\nI figured though that the Variables are still being updated.\r\nIs this warning ignorable?", "@gowthamkpr  i am getting this error on TPU only", "@talhaanwarch The warning occurs on GPU too. Its not specific to TPU. Please find the the gist [here](https://colab.research.google.com/gist/gowthamkpr/88af1c86ace8bdcaf82e5c7228d83b31/37501-v2.ipynb). As mentioned in this [issue comment](https://github.com/huggingface/transformers/issues/1727#issuecomment-568058704), this is not an bug. It means that these variables are not updated during training.\r\n\r\nIf you have more questions, please post the issue [here](https://github.com/google-research/bert/issues) or in stackoverflow. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37501\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37501\">No</a>\n", "I'm also getting this warning ! my embedding layers and attention layers are not updating and remains fixed as initialized. if anyone have solution please tell me. ", "```\r\n\r\nembd=Embedding(input_dim=len(vocab),output_dim=100,name=\"embd\")\r\nlstm1=Bidirectional(LSTM(units=100,return_sequences=True,name=\"lstm1\"),name=\"bd1\")\r\nlstm2=Bidirectional(LSTM(units=100,return_sequences=True,name=\"lstm2\"),name=\"bd2\")\r\nattention_layer=Attention_Model(21,200)\r\ndense1=Dense(units=80,name=\"dense1\",kernel_regularizer=\"l2\")\r\ndropout1=Dropout(0.5)\r\nact1=Activation('sigmoid')\r\n\r\ndense2=Dense(units=50,name=\"dense2\",kernel_regularizer=\"l2\")\r\ndropout2=Dropout(0.4)\r\nact2=Activation('sigmoid')\r\n\r\ndense3=Dense(units=30,name=\"dense3\",kernel_regularizer=\"l2\")\r\ndropout3=Dropout(0.3)\r\nact3=Activation('sigmoid')\r\n\r\ndense4=Dense(units=len(classes),name=\"dense4\")\r\ndropout4=Dropout(0.2)\r\noutput=Activation('softmax')\r\n\r\n```\r\nForward Pass : \r\n\r\n```\r\ndef forward_pass(X):\r\n  t=embd(X)\r\n \r\n  t=lstm1(t)\r\n  \r\n  t=lstm2(t)\r\n  \r\n\r\n \r\n  t=attention_layer(t)\r\n  \r\n  \r\n  t=dense1(t)\r\n  t=dropout1(t)\r\n  t=act1(t)\r\n\r\n  t=dense2(t)\r\n  t=dropout2(t)\r\n  t=act2(t)\r\n\r\n  t=dense3(t)\r\n  t=dropout3(t)\r\n  t=act3(t)\r\n  \r\n  t=dense4(t)\r\n  t=dropout4(t)\r\n  t=output(t)\r\n\r\n  return t\r\n\r\n\r\n```\r\n\r\nAttention Model : \r\n\r\n```\r\n\r\nclass Attention_Model():\r\n  def __init__(self,seq_length,units):\r\n    self.seq_length=seq_length\r\n    self.units=units\r\n    self.lstm=LSTM(units=units,return_sequences=True,return_state=True)\r\n    \r\n\r\n  def get_lstm_s(self,seq_no):\r\n    input_lstm=tf.expand_dims(tf.reduce_sum(self.X*(self.alphas[:,:,seq_no:seq_no+1]),axis=1),axis=1)\r\n    a,b,c=self.lstm(input_lstm)\r\n    self.output[:,seq_no,:]=a[:,0,:]\r\n\r\n    return b\r\n\r\n  def __call__(self,X):\r\n    self.X=X\r\n    self.output=np.zeros(shape=(self.X.shape[0],self.seq_length,self.units))\r\n    self.dense=Dense(units=self.seq_length)\r\n    self.softmax=Softmax(axis=1)\r\n    \r\n\r\n    for i in range(self.seq_length+1):\r\n      if i==0 :\r\n        s=np.zeros(shape=(self.X.shape[0],self.units))\r\n      else :\r\n        s=self.get_lstm_s(i-1)\r\n      if(i==self.seq_length):\r\n        break \r\n      \r\n      s=RepeatVector(self.X.shape[1])(s)\r\n      concate_X=np.concatenate([self.X,s],axis=-1)\r\n      \r\n      self.alphas=self.softmax(self.dense(concate_X))\r\n\r\n    return self.output\r\n      \r\n```\r\n\r\n\r\nis anything wrong with implementation or something else ?", "@amahendrakar  \r\nhow to remove these WARNING from output Info ???\r\n", "@SmileTM,\r\nTo disable the warning logs, try changing the log level using the below code.\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\n```\r\nIf you need further help, please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "https://stackoverflow.com/questions/57144586/tensorflow-gradienttape-gradients-does-not-exist-for-variables-intermittently\r\n````\r\noptimizer.apply_gradients(\r\n    (grad, var) \r\n    for (grad, var) in zip(gradients, model.trainable_variables) \r\n    if grad is not None\r\n)\r\n````", "For me, I just reduce the batch size to 5 and then ignored the waring "]}, {"number": 37500, "title": "\"WARNING:tensorflow:sample_weight ...\" when training a `keras.Model` with a `Sequence` object", "body": "\r\n**System information** \r\n- OS Platform and Distribution: macOS 10.14.5\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nCalling `tf.keras.Model.fit(...)` with any `tf.keras.utils.Sequence` object triggers the following warning:\r\n\r\n```\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\n```\r\n\r\nEven when not using `sample_weight`.\r\n\r\n**Describe the expected behavior**\r\n\r\nTraining keras models without sample weights specified or correctly specified should not trigger warnings about  `sample_weight`.\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass TestSequence(tf.keras.utils.Sequence):\r\n\r\n    def __init__(self, x, y, batch_size):\r\n        self.x, self.y = x, y\r\n        self.batch_size = batch_size\r\n\r\n    def __len__(self):\r\n        return len(self.x) // self.batch_size\r\n\r\n    def __getitem__(self, idx):\r\n        batch_x = self.x[idx * self.batch_size:(idx + 1) *\r\n        self.batch_size]\r\n        batch_y = self.y[idx * self.batch_size:(idx + 1) *\r\n        self.batch_size]\r\n\r\n        return batch_x, batch_y\r\n    \r\n\r\nseq = TestSequence(np.ones(10), np.ones(10), 2)\r\n\r\nx_in = tf.keras.layers.Input((1,))\r\nx_out = tf.keras.layers.Dense(1)(x_in)\r\nmodel = tf.keras.Model(x_in, x_out)\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\r\nmodel.fit(seq)\r\n```\r\n", "comments": ["@kieranricardo, I tried with latest Tf-nightly version, i don't get any warnings. \r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/a60f8fc2e229259a8f18665de35ac97b/untitled438.ipynb). Thanks!", "thanks @gadagashwini! I tested with `tf-nightly` on my machine and I'm no longer getting the warning", "@kieranricardo, Since the issue is fixed in latest TF version. Are you happy to close this issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37500\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37500\">No</a>\n"]}, {"number": 37499, "title": "[Intel MKL] Fix performance regression in DNNL 1 due to lack of primitive cache on reorder", "body": "", "comments": ["@penpornk Thanks for the comments, I have update my code based on it, please check, thank you!\r\nThis primitive cache will same the primitive create time. For some model, it will save about 30% time.", "@penpornk Could you please cherry pick this to TF2.2?  We found that 30% regression could impact models which don't amortize reorder primitive creation cost well.  ", "@Jianhui-Li rc0 was already released yesterday. I'll ask if there will be an rc1.", "@penpornk  Thanks. If there is rc1, could you please also tell us the possible time line? We are considering fixing the sgemm issue in DNNL. ", "@Jianhui-Li Yes, there will be an rc1. I've created a cherry-pick PR https://github.com/tensorflow/tensorflow/pull/37524. \r\nWill follow up about the timeline in the email."]}, {"number": 37498, "title": "Master branch failed to build the debug version", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: master, commit-id: b45adaf6efeeb8e4acf8517a01f7dc01bdf21db9\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): bazel 2.0.0\r\n- GCC/Compiler version (if compiling from source): 8.3\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe debug version build failed with the command:\r\n```\r\nbazel build -c dbg //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nError:\r\n```\r\nexternal/aws-checksums/source/intel/crc32c_sse42_asm.c: In function 's_crc32c_sse42_clmul_256':\r\nexternal/aws-checksums/source/intel/crc32c_sse42_asm.c:61:5: error: 'asm' operand has impossible constraints\r\n     asm volatile(\"enter_256_%=:\"\r\n     ^~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n", "comments": ["@Leslie-Fang \r\ncould you please downgrade the GCC/Compiler version to 7.x and build.", "Thanks @Saduf2019 Since I don't have GCC7.x available. I have tried GCC63 which gives the same error.\r\nIs GCC7.x specifically required to build the debug version?", "Hi @Saduf2019, I'm using `gcc==7.5.0` to build with the above command in debug mode, it raises the same issue. ", "i'm running into this on gcc 7.5.0 as well. i was able to work around this by adding a small change in here https://github.com/tensorflow/tensorflow/blob/master/third_party/aws/aws-checksums.bazel#L24. below the various options that are already there, i added\r\n```\r\n    defines = [                                                                                                                                                                                                    \r\n        \"DEBUG_BUILD\"                                                                                                                                                                                              \r\n    ]                                                                                                                                                                                                              \r\n```\r\nthat seemed to do the trick. the advice came from this comment https://github.com/awslabs/aws-checksums/issues/8#issuecomment-440027793.\r\n\r\nhope this helps someone else. i'm happy to help try to figure out or test a fix for this issue.", "I still experience this issue with fa0721cfbd93a1506d39735296a260a877354e6c (May 21). The real issue here is that the inline asm code in question isn't meant to compile with debug build flags (-c dbg), but only with -O3/-O2/-O1 specified. So, aws-checksums uses DEBUG_BUILD to avoid that inline asm and use an alternative slow implementation for debug builds. As such, for TensorFlow's debug builds, one should pass -DDEBUG_BUILD to aws-checksums so that the inline asm code path is avoided (it has hardcoded register constraints). For more information:\r\nhttps://github.com/awslabs/aws-checksums/issues/8\r\nThe workaround suggested in the post above works, but that would use the slow implementation even under release builds. Instead, DEBUG_BUILD for aws-checksums should be defined only under bazel debug build config. fastbuild and opt conf should be fine without that.\r\n", "I provided a fix in PR #42743 which adds DEBUG_BUILD only for linux debug builds", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37498\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37498\">No</a>\n"]}, {"number": 37497, "title": "Confusing behavior of GRUCell", "body": "```python\r\ncell = layers.GRUCell(4)\r\nx = tf.random.normal((2, 2, 3))\r\nprint(cell.get_initial_state(x)) # returns a single tensor\r\ncell(x[:, 0], [cell.get_initial_state(x)]) # requires and returns a list of tensors\r\n```\r\nWhy would `GRUCell.get_initial_state` return a Tensor, while calling `GRUCell` requires a list of Tensor as input? Is this a desired behavior? (code tested in TF2.1)\r\n\r\nFurthermore, while `Cell` returns output and state separately, RNN returns the output and states together in a single list.\r\n\r\n```python\r\ncell = layers.LSTMCell(4)\r\nx = tf.random.normal((2, 2, 3))\r\ns = cell.get_initial_state(x)\r\ncell_output, state = cell(inputs=x[:, 1], states=s)# works fine\r\nprint(cell_output)\r\nprint(state)\r\nrnn = layers.RNN(cell, return_state=True, return_sequences=True)\r\nrnn_output, state = rnn(x, initial_state=s)# error: rnn returns the output and states in a single list\r\n```\r\n\r\nI'm wondering why `rnn` and `cell` behave differently?", "comments": ["@xlnwel \r\nI have tried on colab with TF version 2.1.0 and and i am seeing the below error `ValueError: too many values to unpack (expected 2)`. Is this the expected behavior?. It will be helpful if you provide a colab link to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "A `Cell` processes a single time step, while a `RNN` processes all time steps. Setting `return_sequences = True` _and_ `return_state = True` in RNN returns three values, of shapes `[batch_size, time steps, output_shape], [batch_size, output_shape], [batch_size, output_shape]`.\r\n\r\nCheck [the example on the TF GRU page](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)", "Hi, @ravikyram. Yeah, the code provided by me will raise error `ValueError: too many values to unpack (expected 2)` at the last line. Here's the [colab link](https://colab.research.google.com/drive/1z58JoPdMDszfXV-rHBAOM6HUDd-W2yVb) I use for test. I humbly think that it will be easier to use if \r\n\r\n1. `cell.get_initial_state(x)` returns a list of tensor, where `cell` could be any RNN cell, including GRUCell, whose state is a single tensor.\r\n2. when RNN is parameterized by `return_state=True`, rnn(x) returns the output and RNN state, where RNN state is a list of tensors. That means for LSTM, `rnn(x)` returns `[output, [h, c]]`, and for GRU, `rnn(x)` returns `[output, [state]]`\r\n\r\nThat's my personal suggestion. But I believe you team designed the behavior for a reason, I'd like to hear that.", "@Susmit-A, thanks for point out the different behavior of `Cell` and `RNN` although I've already known them before. Please check this [colab link](https://colab.research.google.com/drive/1z58JoPdMDszfXV-rHBAOM6HUDd-W2yVb) and the comment above.", "@xlnwel GRU is a recurrent layer. GRUCell is an object (which happens to be a layer too) used by the GRU layer that contains the calculation logic for one step.\r\nA recurrent layer contains a cell object. The cell contains the core code for the calculations of each step, while the recurrent layer commands the cell and performs the actual recurrent calculations.\r\n\r\nAs this is not a bug, please post this issue in stackoverflow where there is a broader community to help. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37497\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37497\">No</a>\n", "Hi, @gowthamkpr. Thanks for your explanation. I know the difference between GRU and GRUCell, and I was not saying this is a bug. I just found their different styles of outputs are confusing and hard to use. Therefore, I'm wondering why you decided to adopt different behaviors for them? Please check my [colab link](https://colab.research.google.com/drive/1z58JoPdMDszfXV-rHBAOM6HUDd-W2yVb)."]}, {"number": 37496, "title": "C-API timeline for TensorFlow 2", "body": "On the Install TensorFlow for C page, it notes that \r\n \"There is no libtensorflow support for TensorFlow 2 yet. It is expected in a future release.\"\r\n\r\nIs there any update on when we can expect the c api for TF2?", "comments": ["I would be interested in it as well.", " Is **C api for TF2** coming soon?", "@av8ramit and @bmzhao are working on it", "[Here](https://storage.googleapis.com/libtensorflow-nightly) is a link in the meantime to a directory listing of builds we've uploaded recently.", "Thank you for quick reply. How can I see tensorflow version of these builds?\r\n", "They are all build from master, so they closely match tf-nightly", "Is C api for TF2 to be included on next release (2.3)?", "Yes we are planning to have libtensorflow releases for the 2.3 release.", "This is very good news. Thanks for the fast response.", "Doesn't look like the [Install TensorFlow for C](https://www.tensorflow.org/install/lang_c) webpage has been updated yet. Is there a link to get the official 2.3 libraries?", "It is now!\r\n\r\n", "@nio1814, [Install TensorFlow for C](https://www.tensorflow.org/install/lang_c) webpage is updated. Now you can download latest version of TensorFlow C library.\r\n\r\nLibtensorflow packages are built nightly and uploaded to GCS for all supported platforms. They are uploaded to the [libtensorflow-nightly GCS bucket ](https://storage.googleapis.com/libtensorflow-nightly)and are indexed by operating system and date built. For MacOS and Linux shared objects, we have a script that renames the .so files versioned to the current date copied into the directory with the artifacts.\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37496\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37496\">No</a>\n"]}, {"number": 37495, "title": "Custom Pearson Correlation Loss Taking Unfeasible Values During Training", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): **Windows 10**\r\n- TensorFlow installed from (source or\r\nbinary): **Source**\r\n- TensorFlow version (use command below): **TensorFlow 2.1.0**\r\n- Python version: **Python 3.6.10 (Anaconda Distribution)**\r\n- CUDA/cuDNN version: **CUDA 10.1; cuDNN 7.6.5 (Nov 5, 2019)**\r\n- GPU model and memory: **NVIDIA GeForce RTX 2070;  8GB VRAM**\r\n\r\n**Describe the current behavior**\r\nLoss values for a custom loss function that computes sample correlations takes on unfeasible values (i.e. 14-15) with certain model architectures; i.e. if hidden layers are too wide.\r\n\r\n**Describe the expected behavior**\r\nThe correlation function's implementation should bound it to within [-1,1], and the custom loss function takes the negative absolute so it should be within [-1,0].\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import regularizers\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout\r\nfrom tensorflow.keras.initializers import he_normal\r\nimport tensorflow_probability as tfp\r\n\r\n# Data\r\nnp.random.seed(42)\r\ndata_X = np.random.random_sample((100,6))\r\ndata_Y = np.random.random_sample((100,1))\r\n\r\n# Feed-forward NN\r\nmodel = Sequential()\r\n\r\n# Input Layer\r\nmodel.add(Dense(6, activation='linear', input_shape=(6,)))\r\n\r\n# Hidden Layers: Arbitrarily wide\r\nmodel.add(Dense(5000, \r\n                activation='relu', \r\n                kernel_initializer=he_normal(), \r\n                kernel_regularizer=regularizers.l1(10**-3)\r\n                ))\r\nmodel.add(Dropout(0.3))\r\n\r\n# Output Layer; Regression\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\n# Custom Loss Function\r\ndef MaxCorrelation(y_true,y_pred):\r\n    \"\"\"\r\n    Goal is to maximize correlation between y_pred, y_true. Same as minimizing the negative.\r\n    \"\"\"\r\n    return -tf.math.abs(tfp.stats.correlation(y_pred,y_true, sample_axis=None, event_axis=None))\r\n\r\n# Compilation\r\nmodel.compile(\r\n            optimizer='adam', \r\n            loss= MaxCorrelation,\r\n)\r\n\r\n# Train the model\r\nhistory = model.fit(data_X, data_Y,\r\n          epochs=5,\r\n          batch_size = 32,\r\n          verbose=1,\r\n         )\r\n```\r\n**Other info / logs** \r\n```\r\nOUT: Train on 100 samples\r\nEpoch 1/5\r\n100/100 [==============================] - 0s 2ms/sample - loss: 14.1344\r\nEpoch 2/5\r\n100/100 [==============================] - 0s 90us/sample - loss: 14.0880\r\nEpoch 3/5\r\n100/100 [==============================] - 0s 90us/sample - loss: 13.9630\r\n```\r\n\r\nTesting the loss function manually seems to work as intended:\r\n```\r\nimport tensorflow.keras.backend as K\r\nY_Pred = model.predict(data_X).astype(float)\r\nK.eval(MaxCorrelation(data_Y, Y_Pred))\r\n--\r\nOUT: -0.137205319813903\r\n```\r\n\r\nMy understanding is that for each `batch_size=32`, the model returns 32 samples of scalar values from the output layer which is squashed between (0,1) and the loss MaxCorrelation is evaluated on the 32 predictions vs 32 targets. Thus the losses should still be within the bounds unless some aggregation is happening per epoch (i.e. one of the batch losses tending to infinity?), but even when removing the negative sign the loss is still positive. This issue only seems to persist when the hidden layers are wide/deep enough.\r\n\r\nI have also tried replacing the loss function with some of the backend implementations [here](https://stackoverflow.com/questions/46619869/how-to-specify-the-correlation-coefficient-as-the-loss-function-in-keras?rq=1), but similar results.", "comments": ["I could reproduce the issue with Tf 2.1 on colab.\r\nPlease take a look at gist [here](https://colab.sandbox.google.com/gist/gadagashwini/a44f5f895fb6c07638a27b78fe7db355/untitled437.ipynb). Thanks", "I could reproduce the issue with TF 2.2-rc3 on colab.\r\nPlease, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/444e226ca2509e88fe293d9606f852b6/untitled845.ipynb).Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/7d061c446be18466d7ccbdf091b13040/untitled7.ipynb). Thanks!", "@adamw96 \r\nplease refer to the [latest execution](https://colab.research.google.com/gist/Saduf2019/f4cf2ba9296f4e7e3376d7eb35cb0f1e/untitled638.ipynb) on tf nightly and confirm if the issue is resolved", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37495\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37495\">No</a>\n"]}]