[{"number": 12918, "title": "Corrected description of what padding SAME does", "body": "The description read that the padding=SAME adds zeros to the output tensor. It should say that it pads the input tensor.", "comments": ["Can one of the admins verify this patch?", "@erlichson, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @sandersk and @isaprykin to be potential reviewers."]}, {"number": 12917, "title": "SVD on GPU: complex values, interface cleanup (Discussion)", "body": "Hi,\r\npull request #11878 brought an implementation of the SVD on the GPU. But at the moment, only real values are supported.\r\n\r\nThe current status when applying the SVD (M=USV') on a complex matrix M:\r\n- The python interface declares U,V as complex, S as real\r\n- The C++ kernel definition declares both U,V and S as complex\r\n   (This simplified the CPU implementation using Eigen)\r\n- The python code then immediately casts S to the reals\r\n- The GPU solver (cuSolver) would, however, output the singular values directly as reals\r\n\r\nThis leads to the following questions / ideas / suggestions\r\n(credit goes also to @rmlarsen for discussion this the first time with me)\r\n- Change the kernel definition: Add a new kernel (V2 suffix) that returns S as a real type\r\n- Implement the complex support on GPUs\r\n- Adopt the CPU code to also use the new kernel definition\r\n   OR\r\n    Keep both definitions and let the python wrapper to choose between the two versions based on the target device (CPU vs GPU)\r\n- ...?\r\n\r\nWhat do you think?", "comments": ["Nagging Assignee @rmlarsen: It has been 379 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "It would be great to have this available - it would be very handy for quantum physics applications! Any news?", "@shamanDevel Any chance of reopening this?"]}, {"number": 12916, "title": "inconsistent behavior in variable sharing ", "body": "hello , \r\n\r\nI have been struggling for few days with an issue that I'm facing in TensorFlow , I hesitated before opening this GitHub as an issue because I'm not sure if what I'm facing is a bug or a mere confusion on my side    . \r\n\r\nanyways it seems that I'm experiencing an inconsistent behavior in tensorFlow with variable name/reusing where I have a function that creates some cells and attentions objects : below is the function : \r\n\r\n```\r\ndef create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , beam_width ):\r\n\r\n    with tf.variable_scope(\"InnerScope\" , reuse=tf.AUTO_REUSE):\r\n        encoder_outputs_tr =tf.contrib.seq2seq.tile_batch(encoder_outputs_tr, multiplier=beam_width) \r\n        encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width) \r\n        batch_size =  batch_size * beam_width \r\n        dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\r\n\r\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs_tr ) \r\n\r\n        attn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size , output_attention=False)\r\n        attn_zero = attn_cell.zero_state(batch_size , tf.float32 )\r\n        attn_zero = attn_zero.clone(cell_state = encoder_state)\r\n    return attn_zero ,  attn_cell \r\n```\r\n\r\nand then I call the function with scope reusing as shown below : \r\n```\r\nwith tf.variable_scope('scope' ):\r\n    intial_train_state , train_cell = create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , 1  )\r\nwith tf.variable_scope('scope' ,reuse=True):\r\n    intial_infer_state , infer_cell = create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , beam_width  )\r\nprint(\"intial_train_state\" , intial_train_state)\r\nprint(\"intial_infer_state\" , intial_infer_state)\r\n```\r\n\r\nthe print commands parings the returned objects , first print outputs : \r\n`  ('intial_train_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_1:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_2:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_3:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_4:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_5:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_6:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_7:0' shape=(?, 512) dtype=float32>)), attention=<tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros_1:0' shape=(100, 512) dtype=float32>, time=<tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros_2:0' shape=(100, ?) dtype=float32>, alignment_history=()))`\r\n\r\n\r\nand the second print outputs : \r\n`  ('intial_infer_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_1:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_2:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_3:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_4:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_5:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_6:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_7:0' shape=(?, 512) dtype=float32>)), attention=<tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros_1:0' shape=(300, 512) dtype=float32>, time=<tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros_2:0' shape=(300, ?) dtype=float32>, alignment_history=()))`\r\n\r\nI was expecting that both output would be the same since I'm reusing the variables but as you can see that for example in the first variable the output has something like this \r\n**scope/InnerScope/tile_batch_1/Reshape_1:0**\r\n\r\nand in the second variable \r\n\r\n**scope_1/InnerScope/tile_batch_1/Reshape_1:0**\r\n\r\nI do not know why _1 is added to **scope** in the second call , and I'm a bit confused if the variable is being shared or not , further more when I set the reuse option to False , I get the below error in the second function call : \r\n`ValueError: Variable scope/memory_layer/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:`\r\n\r\nwhich leads me to think that the function was reusing the variables when the reuse flag was True , but I still do not understand why is tensorFlow adding _1 to the variable name **scope_1** , and does that mean it not being reused ? and how do I fix this . \r\n\r\nI'm using Tensorflow 1.3 \r\nON MacOs 10.12.4\r\n\r\nI have also opened a stackoverflow question : https://stackoverflow.com/questions/46081793/sharing-and-reusing-tensorflow-variables\r\n\r\nthank you ", "comments": ["Hi @malsulaimi , thanks for writing. You need to be careful to distinguish variables from all other nodes in a TensorFlow graph. E.g., when you write this name above:\r\n\r\nscope_1/InnerScope/tile_batch_1/Reshape_1:0\r\n\r\nThis is not a variable, this is just a node from the reshape op. Every node in the TF graph has its own unique name. When you reshape the same variable twice, for example, you'll get 2 different names for 2 different reshape nodes. It's just the variables that are shared. I hope this clarifies the confusion a bit!"]}, {"number": 12915, "title": "Load op library fix", "body": "This fixes the issue with external bindings to TensorFlow loading custom op libraries. It does so by exporting all the relevant symbols in `libtensorflow.so`. Note that these symbols were there but they were hidden and we could not link to them previously. Currently, with this fix, developers of custom op libraries can link at compile time using `libtensorflow.so`, thus avoiding the need for `-undefined -dynamic_lookup` on Macs (which also currently doesn't work as a solution on its own) and the need for loading with `RTLD_GLOBAL` (which also does not work on its own, given that `pywrap_tensorflow.so` needs to be loaded with that flag, given it's the own exporting the necessary symbols). I've tested this on my mac and it resolves the issue.\r\n\r\n@asimshankar I know this is not really a good solution given that you probably don't want to export everything in `libtensorflow.so`, but I think it offers a good temporary workaround to the issue, until  the fix @allenlavoie is working on, is published.\r\n\r\nP.S. This issue has been discussed a bit in #12895.", "comments": ["Can one of the admins verify this patch?", "Thanks @eaplatanios : but I'd really like to avoid this for now.  This restriction was introduced in https://github.com/tensorflow/tensorflow/pull/9059/commits/b0989a0280971034bd838fa176ae3f92b210e8dc for two reasons: (1) binary size and (2) clearly defining the API.\r\n\r\nThere is a reasonable chance that @allenlavoie 's more principled fix will make it to release 1.5 (if not earlier in 1.4). So paying this temporary technical debt doesn't seem worth it. (The workaround won't be helpful to any users of the release till the next release anyway, and those building from source can do this in their build)\r\n\r\n(Though, I'll sync with Allen - if I'm wrong about his fix being enough, then will revisit)", "@asimshankar That sounds reasonable! I agree that it's better to have a clearly defined API. In general, modularization of TensorFlow and it's binaries would probably be a good idea.\r\n\r\nAs for a temporary workaround, I only need this for my Scala API for defining an op equivalent to `py_func`. One way to go about this would be to make `pywrap_tensorflow.so` accessible through the nightly CI builds. Is that something that's easy to do? Then, I can simply also package it with `libtensorflow.so` for my current purposes, until a more elegant alternative is available.\r\n\r\nP.S. I refer to the CI builds because my packaging/release script right now, pulls the `libtensorflow.so` binaries from there.", "Great, thanks for your understanding (I'll go ahead and close this PR).\r\n\r\nYou could pull `pywrap_tensorflow.so` from the nightly builds by downloading the `whl` file and extracting it :)", "No problem! :)\r\n\r\nI checked out [http://ci.tensorflow.org/view/Nightly/](http://ci.tensorflow.org/view/Nightly/) for the nightly whl files but I can't find them. I only see one build for linux-python3 but that doesn't seem to include the pywrap shared object. Could you point me to a download link for one of those whl files?", "See: https://github.com/tensorflow/tensorflow#installation", "And the .so itself should be at something like tensorflow-1.3.0.data/purelib/tensorflow/python/_pywrap_tensorflow_internal.so once you unzip the .whl.", "Great! Thanks a lot! :)"]}, {"number": 12914, "title": "Fixed typo in tf.metrics docs.", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks @rasmi!"]}, {"number": 12913, "title": "Branch 167998450", "body": "", "comments": ["Thanks caisq@!", "Thanks @caisq!"]}, {"number": 12912, "title": "T-SNE Suddenly Not Working in Projector", "body": "Hello,\r\n\r\nI'm sorry this isn't a technical question, but just this week I've noticed that T-SNE in the tensorboard projector is no longer clustering data properly. I am running the same vectors I did a few weeks ago, with the same perplexity. While before I saw distinct clusters, now the points are all forming an indistinct ball. Has anyone else noticed this?", "comments": ["Hi HarryBaker,\r\n\r\nWithout a reproduction, this will be very hard to dig into. I recommend installing an older version of TensorFlow, and seeing if you can demonstrate that this is actually a bug in the TensorBoard projector. Since T-SNE is stochastic, you may have just gotten lucky the first time, or something else in the environment may have changed. \r\n\r\nAlso, this issue should be filed against http://github.com/tensorflow/tensorboard rather than TensorFlow, since TensorBoard is its own project now. Please file there once you have a repro :)", "Ok, thanks for the link. You should redo the link on the projector page, since it goes to this repo.\r\n\r\n"]}, {"number": 12911, "title": "tf.gather with int32 indices yields wrong result on gpu", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n$ cat /etc/issue\r\nUbuntu 16.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\nSource head\r\n- **TensorFlow version (use command below)**:\r\n1.3\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nbazel release 0.4.5\r\n- **CUDA/cuDNN version**:\r\n8.0, 7.0\r\n- **GPU model and memory**:\r\nGeForce GTX 980\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\n\r\ndef bug():\r\n  params = tf.convert_to_tensor([0., 1., 2., 3., 4., 5., 6., 7.], dtype=tf.float32)\r\n  indices_32 = tf.convert_to_tensor([2, 0, 2, 4], dtype=tf.int32)\r\n  indices_64 = tf.convert_to_tensor([2, 0, 2, 4], dtype=tf.int64)\r\n  test_op_32 = tf.gather(params, indices_32)\r\n  test_op_64 = tf.gather(params, indices_64)\r\n  with tf.Session() as sess:\r\n      res = sess.run([test_op_32, test_op_64])\r\n  print('test_op_32', res[0], 'test_op_64', res[1])\r\n\r\nbug()\r\nwith tf.device('/cpu:0'):\r\n  bug()\r\nbug()\r\n```\r\n\r\n### Describe the problem\r\nWhen using `int32` indices for `tf.gather` on GPU, wrong (random) numbers are returned.\r\n\r\nSee code above. It does not occur if using `int64` or running on CPU.\r\n\r\n### Output\r\n```\r\n2017-09-08 15:54:08.775830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:\r\nname: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.2785\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 3.94GiB freeMemory: 158.62MiB\r\n2017-09-08 15:54:08.775862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\ntest_op_32 [ 0.  0.  0.  0.] test_op_64 [ 2.  0.  2.  4.]\r\n2017-09-08 15:54:08.792745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\ntest_op_32 [ 2.  0.  2.  4.] test_op_64 [ 2.  0.  2.  4.]\r\n2017-09-08 15:54:08.800545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\ntest_op_32 [ 3.  0.  0.  0.] test_op_64 [ 2.  0.  2.  4.]\r\n```", "comments": ["Could you try on github head? I can't seem to be able to repro. Can you also try on a prebuilt pip package?", "I cant reproduce this on pip 1.3", "It might be related to the address the array is stored in:\r\n```\r\n2017-09-10 16:16:52.557622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:\r\nname: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.2785\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 3.94GiB freeMemory: 158.62MiB\r\n2017-09-10 16:16:52.557653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\ntest_op_32 [ 0.  0.  0.  0.] test_op_64 [ 2.  0.  2.  4.]\r\n2017-09-10 16:16:52.573418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\ntest_op_32 [ 2.  0.  2.  4.] test_op_64 [ 2.  0.  2.  4.]\r\n2017-09-10 16:16:52.580016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\ntest_op_32 [ 3.  0.  0.  0.] test_op_64 [ 2.  0.  2.  4.]\r\n\r\n2017-09-10 16:17:17.984597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:\r\nname: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.2785\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 3.94GiB freeMemory: 3.87GiB\r\n2017-09-10 16:17:17.984626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\ntest_op_32 [ 2.  0.  2.  4.] test_op_64 [ 2.  0.  2.  4.]\r\n2017-09-10 16:17:18.160432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\ntest_op_32 [ 2.  0.  2.  4.] test_op_64 [ 2.  0.  2.  4.]\r\n2017-09-10 16:17:18.167003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\ntest_op_32 [ 2.  0.  2.  4.] test_op_64 [ 2.  0.  2.  4.]\r\n```\r\nNotice that the bug appears when there is hardly any free memory while it doesn't when all memory is free. Possibly an int32 overflow somewhere?", "@jheymann85 that's interesting, thank you for pointing that out.\r\n\r\n@rmlarsen I'll take a look later, but if you get to this faster, that would be nice.", "More details for reproduction:\r\n\r\nI run tensorflow `32ffc5a81eee8c39bbe71536212a773b1ffd4eb2`. But the related tensorflow code does not seem to have changed very recently.\r\n\r\nWe only have access to cards with limited amount of memory (i.e. `totalMemory: 5.94GiB`). I have to consume a lot of memory in a different tensorflow session, to make this bug appear.\r\n\r\nTo be more precise: It works, when there is not enough allocated, compare `freeMemory`:\r\n```\r\nname: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.2785\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 3.94GiB freeMemory: 226.75MiB\r\n...\r\ntest_op_32 [ 2.  0.  2.  4.] test_op_64 [ 2.  0.  2.  4.]\r\n```\r\n\r\nIf I now allocate more (i.e. start yet another session):\r\n```\r\nname: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.2785\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 3.94GiB freeMemory: 149.75MiB\r\n...\r\ntest_op_32 [ -1.27668094e+38   2.91721193e-39   1.10202595e-38   3.42339542e+17] test_op_64 [ 2.  0.  2.  4.]\r\n```", "Sorry I didn't get around that, if you have a program that exhibits that behavior by allocating a lot of memory then performing the operation? That way I can try to repro and debug. Thank you!", "One way to reproduce is to open some other Tensorflow sessions on the same host. I.e.:\r\n\r\ntf_sess.py:\r\n```\r\nimport tensorflow as tf\r\nimport time\r\ns = tf.Session()\r\ntime.sleep(30)\r\n```\r\n\r\nand then execute\r\n```\r\npython tf_sess.py &\r\npython tf_sess.py &\r\npython example.py\r\n```\r\n\r\nyields\r\n```\r\n2017-09-20 08:01:36.029417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:\r\nname: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.2785\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 3.94GiB freeMemory: 149.75MiB\r\n2017-09-20 08:01:36.029446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\ntest_op_32 [ -6.38132574e+37   2.93414410e-38   2.77472790e-38   4.32417032e+17] test_op_64 [ 2.  0.  2.  4.]\r\n2017-09-20 08:01:36.044647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\ntest_op_32 [ 2.  0.  2.  4.] test_op_64 [ 2.  0.  2.  4.]\r\n2017-09-20 08:01:36.050076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\ntest_op_32 [  1.42191624e-13   2.90286264e-39              nan   4.26825146e-22] test_op_64 [ 2.  0.  2.  4.]\r\n```\r\n\r\nThe number of open sessions necessary to reproduce may vary depending on the amount of memory they consume. For me two worked, but one was not sufficient.\r\n", "@jheymann85, I can reproduce this issue on TensorFlow 1.3 but not TensorFlow 1.4, so I am going to assume it's fixed (although I'm curious what the issue was). Please reopen if you discover the issue still exists in some form.", "Has anyone had issues with this since? I'm having results reproducibility issues using `tf.gather` with TF2.2-stable. I have set all seeds and turned TF_DETERMINISTIC_OPS=1"]}, {"number": 12910, "title": "is_numeric_tensor on _ref variables", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\n>>> X = tf.Variable(np.random.rand(5,5))\r\n>>> X\r\n<tf.Variable 'Variable:0' shape=(5, 5) dtype=float64_ref>\r\n>>> tf.is_numeric_tensor(X)\r\nFalse\r\n```\r\n\r\n### Describe the problem\r\nI'm not sure if this is a bug or a feature, I feel it is a bug. I want to be able to test if an input to a function was a numpy style array or a tensor. I use the tf.is_numeric_tensor to check the input, however it doesn't pick up on _ref variables such as those initialized from a numpy array. \r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Why is it not a bug or feature request? Surely the is_numeric_tensor() should return true for _ref types, or at least explicitly explain why it doesn't in its help. ", "@langmore Can you take a look at this?", "How to treat ref types seems like a higher level design decision for wicke or someone like that.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@fbcotter I agree with you. Now a workaround is to use `tf.identity`, see:\r\n\r\n```python\r\nIn [8]: tf.is_numeric_tensor(tf.identity(X))\r\nOut[8]: True\r\n```\r\n\r\nor check [`X.dtype.base_dtype`](https://www.tensorflow.org/versions/master/api_docs/python/tf/DType#as_numpy_dtype) by youself.\r\n  ", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "So, essentially, it's working as intended. Variables are not tensors. I know it's confusing."]}, {"number": 12909, "title": "Maven Packaging for GPU-backed Java Bindings", "body": "### System information\r\nN/A\r\n\r\n### Describe the problem\r\nCurrently, the only `libtensorflow_jni` artifact which is deployed to Maven Central is compiled with CPU-only support. This means that (at least as far as my current understanding goes) any machine which I would like to deploy to and run GPU-backed code on has to build TensorFlow from source with GPU support. As one can imagine, this makes the logistics of such deployments much more difficult and time-consuming. \r\n\r\nIt would be wonderful if the `libtensorflow_jni` artifact either contained a GPU-compiled TensorFlow libraries and could use them automatically when possible _or_ if there was a separate artifact (perhaps with a `gpu` artifact classifier) that contained those libraries. The former streamlines build processes at the cost of bloating the artifact a bit, while the latter at least provides an easy means of enabling GPU support for users which desire it.\r\n\r\nI'm guessing that something like this is somewhere on the timeline (I know that the Java bindings are under active development), but I have been unable to find any documentation on the topic.\r\n\r\nThank you!\r\n\r\n### Source code / logs\r\nN/A", "comments": ["@asimshankar for Java distribution.", "(In the mean time, just wanted to point to: https://github.com/asimshankar/java-tensorflow/tree/master/gpu as a workaround for enabling GPU support when using the Java API)", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Overall, that sounds like a good idea. Marking as contributions welcome. It would be community supported.", "Actually, I think I should be able to make this happen for Linux with the 1.5.0 release, so removing \"contributions welcome\" and will update this issue with the commits that will help."]}, {"number": 12908, "title": "Subclasses of Estimator cannot override members of Estimator.", "body": "Any reason for not allowing subclasses of `Estimator` to override members other than `['_call_input_fn', '_create_global_step']`?", "comments": ["Could you explain your use case a bit and maybe we can help you figure out a solution to the problem that you're trying to solve?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "@rohan100jain In my case I need to override `_evaluate_run`, which calls a custom `_write_dict_to_summary` that treats binary data as image bytes by providing the data to [`summ.value[i].image.encoded_image_string`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L1852)\r\n\r\nCurrent workaround: mute the assert before calling parent's `__init__`, e.g.,\r\n\r\n```\r\nclass MyEstimator(tf.estimator.Estimator):\r\n    def __init__(self, *args, **kwargs):\r\n        tf.estimator.Estimator._assert_members_are_not_overridden = lambda self: None\r\n        super(MyEstimator, self).__init__(*args, **kwargs)\r\n```", "For anyone like me who has found themselves in this dark corner of the internet, I ended up (in TF 2.1.0) having to put in the subclassed `__init__` before any super `__init__`:\r\n\r\n`self.__class__._assert_members_are_not_overridden = lambda self: None`"]}, {"number": 12907, "title": "docs: Fix link to tf.estimator.DNNLinearCombinedRegressor", "body": "", "comments": ["Can one of the admins verify this patch?", "@drothlis, thanks for your PR! By analyzing the history of the files in this pull request, we identified @sandersk, @tensorflower-gardener and @vrv to be potential reviewers.", "Nice catch! Thanks @drothlis!"]}, {"number": 12906, "title": "Generators in graphs", "body": "I am wondering, is there any way to place a generator in a graph that would push values to lower parts of graph or asynchronous operations of this kind should be always placed outside of tensorflow graphs?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "You're right, thank you for the tip."]}, {"number": 12905, "title": "bazel error related to python ", "body": "bazel build tensorflow/examples/image_retraining:retrain\r\nERROR: /home/dile/tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n\tFile \"/home/dile/tensorflow/third_party/py/python_configure.bzl\", line 310\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/home/dile/tensorflow/third_party/py/python_configure.bzl\", line 268, in _create_local_python_repository\r\n\t\t_get_python_bin(repository_ctx)\r\n\tFile \"/home/dile/tensorflow/third_party/py/python_configure.bzl\", line 166, in _get_python_bin\r\n\t\t_get_env_var(repository_ctx, _PYTHON_BIN_PATH, No..., ...)\r\n\tFile \"/home/dile/tensorflow/third_party/py/python_configure.bzl\", line 49, in _get_env_var\r\n\t\t_python_configure_fail((\"'%s' environment variable is n...))\r\n\tFile \"/home/dile/tensorflow/third_party/py/python_configure.bzl\", line 37, in _python_configure_fail\r\n\t\tfail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: 'PYTHON_BIN_PATH' environment variable is not set\r\n and referenced by '//third_party/py/numpy:headers'\r\nERROR: Analysis of target '//tensorflow/examples/image_retraining:retrain' failed; build aborted\r\nINFO: Elapsed time: 4.555s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/core ... (3 packages)\r\n\r\n\r\n\r\nOperating system is ubuntu\r\nand i dont know how to run configure..what and all i have to select in that\r\n", "comments": ["As you point out, you must run configure prior to attempting to build; build [guidance is here.](https://www.tensorflow.org/install/install_sources)", "anaconda work bad even if run configure file."]}, {"number": 12904, "title": "make \"build_all_ios.sh\"  occur error", "body": "I try to build tensorflow support at Android and iOS by makefile [tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile\r\n) in current master branch 04c318b69c5b565436cfeeaab1cb7fd5419dde27\r\n\r\nWhen running the build_all_ios.sh script, the below error message show\r\n```\r\nUndefined symbols for architecture x86_64:\r\n  \"nsync::nsync_mu_init(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::mutex::mutex() in env.o\r\n      tensorflow::mutex::mutex() in random.o\r\n  \"nsync::nsync_mu_lock(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::mutex::lock() in env.o\r\n      tensorflow::mutex::lock() in random.o\r\n      tensorflow::mutex::lock() in histogram.o\r\n  \"nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::mutex::unlock() in env.o\r\n      tensorflow::mutex::unlock() in random.o\r\n      tensorflow::mutex::unlock() in histogram.o\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [/Users/CSL.Peter/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1\r\n+ '[' 2 -ne 0 ']'\r\n+ echo 'armv7 compilation failed.'\r\narmv7 compilation failed.\r\n+ exit 1\r\n```\r\nThe `download_dependencies.sh` and `compile_ios_protobuf.sh` run successfully but `compile_ios_tensorflow.sh` failed. I find same issues #3191 and #4252 and seem to be fixed at #4287, but this problem still happen.", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "I was able to get around this by linking\r\ntensorflow/contrib/makefile/downloads/nsync/builds/lipo.ios.c++11/nsync.a in the other_linker flags", "My platform is macOS Sierra Version 10.12.6\r\nTensorflow version over v1.3.0  at master branch 04c318b69c5b565436cfeeaab1cb7fd5419dde27 \r\nCompiler  information\r\n```bash\r\n$ gcc -v\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.12.sdk/usr/include/c++/4.2.1\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n```\r\n\r\nWhen I checkout to **v1.3.0** tag, building script is running successfully.\r\nI think this is building problem in latest version.\r\n", "@petewarden, have you hit this lately? Do you have a better fix?", "This seems to be some sort of Makefile / Bash issue on OSX where the $(HOST_NSYNC_LIB) env var in the Makefile is getting clobbered. \r\n\r\nI was able to build for a while without this issue and then started to hit the same issue. I ran each command in build_all_ios.sh by hand and then it seemed to work. \r\n\r\nHere is a simple patch to the Makefile which should fix the Undefined symbols error if you run into it.\r\n\r\ndiff --git a/tensorflow/contrib/makefile/Makefile b/tensorflow/contrib/makefile/Makefile\r\nindex 525cf2cd4..e4893dde2 100644\r\n--- a/tensorflow/contrib/makefile/Makefile\r\n+++ b/tensorflow/contrib/makefile/Makefile\r\n@@ -86,7 +86,7 @@ endif\r\n HOST_INCLUDES += -I/usr/local/include\r\n \r\n HOST_LIBS := \\\r\n-$(HOST_NSYNC_LIB) \\\r\n+$(MAKEFILE_DIR)/downloads/nsync/builds/default.macos.c++11/nsync.a \\\r\n -lstdc++ \\\r\n -lprotobuf \\\r\n -lpthread \\\r\n@@ -169,7 +169,7 @@ endif\r\n INCLUDES += -I/usr/local/include\r\n \r\n LIBS := \\\r\n-$(TARGET_NSYNC_LIB) \\\r\n+$(MAKEFILE_DIR)/downloads/nsync/builds/lipo.ios.c++11/nsync.a \\\r\n -lstdc++ \\\r\n -lprotobuf \\\r\n -lz \\\r\n\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I am getting the same error with tf1,3 on raspberry pi3\r\nI did following \r\n----------------------\r\nexport HOST_NSYNC_LIB=`tensorflow/contrib/makefile/compile_nsync.sh`\r\nexport TARGET_NSYNC_LIB=\"$HOST_NSYNC_LIB\"\r\nmake -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI  OPTFLAGS=\"-Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize\" CXX=g++-4.8\r\n\r\nsudo cp /home/pi/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public/nsync* /usr/include/.\r\nmake -f tensorflow/contrib/pi_examples/label_image/Makefile\r\n======== Now I start to get error-----\r\n\r\nsubprocess.cc:(.text+0xcd4): undefined reference to `nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)'\r\ncollect2: error: ld returned 1 exit status\r\n\r\n----------------------------\r\n\r\nHow do I address this error. I have tried all most all idea suggested on web .\r\nI am running Jessie version 8 .\r\n\r\n\r\n\r\n\r\n\r\n", "Posted in stackexchange too.", "@petewarden, PTAL", "I have not installed Bazel... do I need to ?\r\n", "@anilkumar1589 Pls try the patch above in my earlier comment to point to whereever your nsync.a is. My patch is for OSX.  You dont need Bazel when using the Makefile. \r\n\r\nFor iOS / OSX this should no longer be an issue since we explicitly pass the location of the nsync.a location to the compile scripts. ", "Thanks powerluv, I am using Raspberry pi.\r\n", "I really do not know how to use script/code suggested by PeteWarden. If it is possible then please let me know how to use it. I am novice .", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied."]}, {"number": 12903, "title": "Input streaming data to tensorflow", "body": "Can you please reccommend a way to input streaming data which comes in json format to tensorflow", "comments": ["If you have Python code for connecting to the stream and parsing the JSON into arrays, you could wrap that logic in a Python generator and use `Dataset.from_generator()` (currently only available in the nightly build).", "Thanks you so much  mrry .\r\nActually, I was quite stucked at inputting streaming data to tensorflow in my project. So, Thanks a lot."]}, {"number": 12902, "title": "Change TanhGrad() operation definition with respect to documentation", "body": "Hello,\r\n\r\nTanhGrad() documentation says: \"Specifically, `grad = dy * (1 - y*y)`, where `y = tanh(x)`, and `dy`\r\nis the corresponding input gradient.\" https://github.com/tensorflow/tensorflow/blob/bab2db47f60d59d843d661903a8d28227600dd60/tensorflow/core/ops/math_ops.cc#L323 which is correct and looks good.\r\n\r\nBut operation has following declaration of inputs:\r\n  Input(\"x: T\")                                                \r\n  .Input(\"y: T\")                                           \r\nhttps://github.com/tensorflow/tensorflow/blob/bab2db47f60d59d843d661903a8d28227600dd60/tensorflow/core/ops/math_ops.cc#L200\r\n\r\nwhat doesn't correlate with the documentated formula: grad = dy * (1 - y*y).\r\nCould you please rename inputs with respect to documentation like this:\r\n\r\n  Input(\"y: T\")                                                \r\n  .Input(\"dy: T\")   \r\n\r\nThanks.", "comments": ["This means modifying UNARY_GRADIENT_COMPLEX macro and changing x to y and y to dy for every op that uses it, are you sure that wouldn't break things/contradict documentation for those other ops? BTW, you can press \"y\" in github to generate a permanent link with line number reference", " @yaroslavvb, it doesn't contradict documentation for other operations which use UNARY_GRADIENT_COMPLEX macros, because the other ops use 'y' and 'dy' names in docs.\r\n\r\nThanks for your suggestion, I updated the issue with permanent links.", "@Dorokhov do you want to do a PR to fix it? If you do, make sure to run the tests locally first", "@yaroslavvb right, I will do"]}, {"number": 12900, "title": "Don't check patch command on Windows", "body": "Fix Windows build:\r\nhttp://ci.tensorflow.org/job/tf-master-win-bzl/\r\n@gunan @yifeif ", "comments": ["@meteorcloudy, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @tensorflower-gardener and @kirilg to be potential reviewers.", "Thanks Yun!"]}, {"number": 12899, "title": "Add batch layer normalization support in DNNClassifiers", "body": "", "comments": ["Can one of the admins verify this patch?", "@king821221, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jart and @ispirmustafa to be potential reviewers.", "Thanks for the PR @king821221! @ispirmustafa do you mind taking a look or reassign? Thanks!", "Thank you Jian. We're conservative about the interface of canned estimators. For every feature/argument we need an analysis of how it impacts training. in which conditions it works better. \r\nFor batchnorm, we recommend people to create their own model_fn by copying existing canned estimators model_fns. \r\nI'm closing this. Please re-open it if you feel otherwise.\r\nThanks again."]}, {"number": 12898, "title": "allocate output tensor for every operation before the first inference", "body": "### Describe the problem\r\nI was wondering if it's possible to allocate memory in advance, for internal output tensors of every operation in my inference workload. \r\nI notice TF will automatically allocate (and de-allocate) memory for every internal result in a graph. If my inference workload repeats many times, all these allocations/deallocations will also repeat. Why don't we allocate these internal output tensors just once before the first inference run, and delete them after the last?\r\n\r\nFor example, I have the following workload (which will run thousands of times):\r\n```\r\n    M = tf.MatMul(A, B)\r\n    D = tf.MatMul(M, C)\r\n````\r\nIs it possible to allocate M before the first run and use the memory for all inference? To avoid allocation of M in every single inference?\r\n", "comments": ["Similar thing is already being done with TensorFlow's caching allocator already. You may see `__LOG_MEMORY__` messages suggesting memory allocations, but those allocations are near instantaneous when they reuse memory that tensorflow allocated earlier", "Thanks for your reply, yaroslavvb.\r\n\r\nIf I understand correctly, (GPU)BFCAllocator does the caching stuff as you mentioned. And I do notice the same internal output tensors from different inference runs share the same \"true ptr\" addresses. \r\n\r\nHowever, when I changed code in \"framework/op_kernel.cc\" file, and timed the processes of \"allocate output\", I found these processes still cost 18-20 usec, which seems too long for me because the calculating time of my ops is only 30~50 usec on average.\r\n\r\n```\r\n2017-09-11 04:19:11.107759: I tensorflow/core/framework/op_kernel.cc:592] My log :: Start allocate output tensor\r\n2017-09-11 04:19:11.107772: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: 10 kernel_name: \"MatMul\" tensor { dtype: DT_FLOAT shape {} allocation_description {allocator_name: \"GPU_0_bfc\" } }\r\n2017-09-11 04:19:11.107778: I tensorflow/core/framework/op_kernel.cc:608] My log :: complete allocate output tensor\r\n```\r\n\r\nI was wondering if it's possible to walk around the invocation of \"allocate_output\" completely. ", "Hm 20 usec on cached allocation on GPU would be quite terrible. You can have 10's of thousands of temporary allocations during running of a large network, so that could mean half a second of overhead in session.run purely due to tensor allocation calls. \r\n\r\nI do wonder if this 20 usec delay between your print messages is an actual bottleneck that can be eliminated, or if its a side effect of things running asynchronously. cc @zheng-xq cc @reedwm ", "Since GPU allocations are run on the CPU during the OpKernel::Compute() call, they run in parallel with actual GPU computations, so reducing this allocation probably won't improve performance by much.\r\n\r\nStill, reducing allocation times always helps, especially when a GPU is not available. @zheng-xq, any thoughts?", "As a general comment -- it's preferable to to optimize existing allocators to be as fast as \"manual preallocation\" rather than give users a way to preallocate output tensor. This should be feasible in situation like yours where you run the same op thousands of times.\r\n \r\nAs an example of what \"preallocating\" does to code, here's an example of l-BFGS in [Torch](https://github.com/torch/optim/blob/master/lbfgs.lua), search for lines with \"table\" -- all that logic just complicates things", "Thanks for your replies, @yaroslavvb  and @reedwm .\r\nIf the allocator really takes 10s of usec, I guess the output-allocation process could still be a bottleneck even in GPU inference workload. \r\nFor example, we can consider a case that CPU side allocating process doesn't overlap with the previous launched GPU calculation.\r\n![20170912_120218](https://user-images.githubusercontent.com/8175586/30308136-ba738734-9772-11e7-95f7-909b113cb66d.jpg)\r\nI guess this could happen in inference (or training with small batch size) scenarios.\r\n  \r\n", "So I've talked to @zheng-xq who leads performance group and he said that they have ran experiments on real world networks and found that this delay was not a bottleneck. It was still not a delay when they artificially made this part 50% slower by adding sleep.", "@jiazhe0909 I am going to close this issue because as @yaroslavvb stated, the delay is currently not a bottleneck. As your diagram indicates, as GPUs get faster and the \"Compute for Op1\" box gets smaller, this could become a bottleneck in the future, but the solution will probably be to improve the allocator. Static allocation uses significantly more memory.\r\n\r\nAs you stated, for inference (or training) with small batches sizes, it is more likely the GPU compute time will be small relative to the CPU time, and the  increased memory usage from static allocation would not be a significant problem due to the small batch size. But as @yaroslavvb, this would complicate the code and take a lot of work. If this becomes a serious problem in the future, we can revisit this issue.\r\n\r\n"]}, {"number": 12897, "title": "ImportError: cannot import name 'c_einsum'", "body": "I updated tensorflow from 1.1.0 to 1.3.0 using the wheel file: tensorflow-1.3.0-cp35-cp35m-win_amd64.whl and in the line:\r\n\r\n`import tensorflow as tf`\r\n\r\nI got the following error:\r\n\r\n```\r\nfrom numpy.core.multiarray import c_einsum\r\nImportError: cannot import name 'c_einsum'\r\n```\r\n\r\nAny help?", "comments": ["Do you have the full stack trace? What's the numpy version?\r\n\r\nTo be clear, that's windows?", "hi drpngx,\r\n\r\ni have fixed this issue by updating numpy to version 1.13.1. \r\n\r\nthank you for a good question?\r\n\r\n-ernest"]}, {"number": 12896, "title": "tensorflow-master version  compile build_all_ios.sh  error ( armv7 compilation failed)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["It looks like you hit enter prematurely? Feel free to open a new bug if you're still having a problem. Or reopen with the description you intended to enter, please."]}, {"number": 12895, "title": "dlopen error when using TF_LoadLibrary", "body": "I'm trying to use `TF_LoadLibrary` on a mac and I keep getting errors such as:\r\n```\r\nCaused by: org.platanios.tensorflow.jni.TensorFlow$NativeException: dlopen(/var/folders/rw/lqrc8nk52kqcc4_zq3c2b6dh0000gn/T/tensorflow_scala_native_libraries7467192138547554485/libtensorflow_ops.dylib, 6): Symbol not found: __ZN10tensorflow15shape_inference12UnknownShapeEPNS0_16InferenceContextE\r\n  Referenced from: /var/folders/rw/lqrc8nk52kqcc4_zq3c2b6dh0000gn/T/tensorflow_scala_native_libraries7467192138547554485/libtensorflow_ops.dylib\r\n  Expected in: flat namespace\r\n in /var/folders/rw/lqrc8nk52kqcc4_zq3c2b6dh0000gn/T/tensorflow_scala_native_libraries7467192138547554485/libtensorflow_ops.dylib\r\n```\r\nThe same thing happens when I try to load the compiled dylib file through Python directly. It was compiled it using `-D_GLIBCXX_USE_CXX11_ABI=0` and `-undefined dynamic_lookup` for the linker. @asimshankar ", "comments": ["Unfortunately, loading custom op libraries from the C API isn't fully functional now\r\n(and worked around using [`RTLD_GLOBAL` in Python](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/pywrap_tensorflow.py#L40)).\r\n\r\nThe good news is that @allenlavoie is working on a fix. No immediate ETA, but long story short - loading custom op libraries might not work outside of Python yet, but we're on it.\r\n\r\n(Related issue #10454)", "@asimshankar That's interesting because I also tried to load it using RTLD_GLOBAL. However, on my mac, even `tf.load_op_library(...)` in Python is not working and is throwing the same error.\r\n\r\nUPDATE: I managed to load it in Python once I got rid of the C API dependencies. Weird thing is that I also use a JNI function that calls dlopen with RTLD_GLOBAL to load the main tensorflow shared object and my tensorflow JNI bindings in the Scala side but it doesn't work. Is there something else that's going on in Python other than that?", "@asimshankar I found out that it's not just RTLD_GLOBAL that matters. The symbols are actually exported through `_pywrap_tensorflow_internal.so` in the Python API. I was only loading `libtensorflow.so` with RTLD_GLOBAL, but that is not enough to make things work. Is there any easy workaround that you can see, until the fix comes out? Thanks! :)", "A temporary solution would be to replace `exported_symbols.lds` and `version_script.lds` in the `c` directory with `tf_exported_symbols.lds` and `tf_version_script.lds`. This would export all the necessary symbols in `libtensorflow.so`. Is there a reason not to do that for now, until the fix comes out?", "I just made a PR (#12915) with this proposed fix.", "@asimshankar @allenlavoie I just realized (after trying it) that pulling the pywrap so file doesn't really work for me because it depends on the Python libraries. Is there no other temporary solution we can use until the fix comes out? Maybe having a separate build target \"libtensorflow_all.so\", or something like that, that exports everything. Would that be acceptable? Aside from breaking the API, which I agree is not great, the binary increases in size by about 9MB (from ~98MB to ~107MB for Mac). Therefore, I believe that having a separate build target might not be too bad.\r\n\r\nI have a package I'm creating and releasing to Sonatype for my Scala API that can optionally include pre-compiled binaries. Given that I use some custom ops, I need to be able to distribute those too and that's why I'm trying to find a temporary solution for the next 1-2 months until the fix is out.", "As an alternative, I could also add my `ScalaCallback` and `ScalaCallbackStateless` ops with their kernel implementations (similar to that of `py_func`) to the main repository so they're included in the library by default. Would that be acceptable? I feel it interferes more than having a separate build target, but I'm not sure what your opinion is on this.", "This should be fixed by https://github.com/tensorflow/tensorflow/commit/5c7f9e316d8c7735308a217310350d416d7498cc. Would you mind taking a look? If you also need the C API symbols for your custom ops, you can link against libtensorflow.so (although that will make the custom ops non-portable to other languages).", "@allenlavoie That's actually great! Thanks! :) I get my code to compile with these changes. I only have one problem, which is that the JVM might invoke the op library loading function multiple times (even if I only do it from a synchronized static block). This is because the TensorFlow library might remain loaded in the background across multiple Java/Scala runs. This results in a fatal error where multiple kernels are registered for the same op. Is there a way to make `REGISTER_KERNEL_BUILDER` conditional on whether a kernel has already been registered for that op? Thanks a lot! :)", "@allenlavoie FYI, I'm currently doing the following which is a bit ugly:\r\n```\r\nnamespace tensorflow {\r\nnamespace kernel_factory {\r\n\r\nstruct KernelRegistration {\r\n  KernelRegistration(const KernelDef& d, StringPiece c,\r\n                     kernel_factory::OpKernelRegistrar::Factory f)\r\n      : def(d), kernel_class_name(c.ToString()), factory(f) {}\r\n  const KernelDef def;\r\n  const string kernel_class_name;\r\n  const kernel_factory::OpKernelRegistrar::Factory factory;\r\n};\r\n\r\nauto jvmCallbackOpInitializer = []{\r\n  auto* reg = reinterpret_cast<std::unordered_multimap<string, KernelRegistration>*>(GlobalKernelRegistry());\r\n  if (reg->find(strings::StrCat(\"JVMCallback:\", DeviceTypeString(DEVICE_CPU), \":\")) == reg->end()) {\r\n    REGISTER_KERNEL_BUILDER(Name(\"JVMCallback\").Device(DEVICE_CPU), JVMCallbackOp);\r\n    REGISTER_KERNEL_BUILDER(Name(\"JVMCallbackStateless\").Device(DEVICE_CPU), JVMCallbackOp);\r\n  }\r\n  return 0;\r\n}();\r\n\r\n}\r\n}\r\n```", "@allenlavoie Also, would it be easy to also export symbols such as `tensorflow::Graph::AddEdge`, `tensorflow::Graph::RemoveEdge`, `tensorflow::Graph::kControlSlot`, `tensorflow::Node::set_requested_device`, `tensorflow::Node::input_edge`, `nsync::nsync_mu_lock`, and `nsync::nsync_mu_unlock`, in the framework binary?", "Huh. I'm surprised the static initializers are called multiple times just because the op library is dlopen()ed multiple times, considering that we never dlclose() it. Maybe we could just add RTLD_NODELETE to TF_LoadLibrary? I'll take a look.\r\n\r\nAt least the tensorflow::Graph symbols should be there already (from //tensorflow/core:framework_internal_impl)?", "Yeah I was also surprised at that behavior, but I noticed online that other people have had similar experiences by calling TF_LoadLibrary multiple times.\r\n\r\nThey don't seem to be. I get this error when trying to compile [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/python_api.cc) from outside bazel (i.e., from within my library):\r\n```\r\nUndefined symbols for architecture x86_64:\r\n[error]   \"tensorflow::Node::set_requested_device(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\", referenced from:\r\n[error]       tensorflow::SetRequestedDevice(TF_Graph*, TF_Operation*, char const*) in tf_python_api.cc.o\r\n[error]   \"tensorflow::Graph::RemoveEdge(tensorflow::Edge const*)\", referenced from:\r\n[error]       tensorflow::UpdateInput(TF_Graph*, TF_Operation*, int, TF_Output) in tf_python_api.cc.o\r\n[error]       tensorflow::ClearControlInputs(TF_Graph*, TF_Operation*) in tf_python_api.cc.o\r\n[error]   \"tensorflow::Graph::kControlSlot\", referenced from:\r\n[error]       tensorflow::AddControlInput(TF_Graph*, TF_Operation*, TF_Operation*) in tf_python_api.cc.o\r\n[error]       tensorflow::ClearControlInputs(TF_Graph*, TF_Operation*) in tf_python_api.cc.o\r\n[error]   \"tensorflow::Graph::AddEdge(tensorflow::Node*, int, tensorflow::Node*, int)\", referenced from:\r\n[error]       tensorflow::UpdateInput(TF_Graph*, TF_Operation*, int, TF_Output) in tf_python_api.cc.o\r\n[error]       tensorflow::AddControlInput(TF_Graph*, TF_Operation*, TF_Operation*) in tf_python_api.cc.o\r\n[error]   \"nsync::nsync_mu_lock(nsync::nsync_mu_s_*)\", referenced from:\r\n[error]       tensorflow::UpdateInput(TF_Graph*, TF_Operation*, int, TF_Output) in tf_python_api.cc.o\r\n[error]       tensorflow::AddControlInput(TF_Graph*, TF_Operation*, TF_Operation*) in tf_python_api.cc.o\r\n[error]       tensorflow::ClearControlInputs(TF_Graph*, TF_Operation*) in tf_python_api.cc.o\r\n[error]       tensorflow::SetRequestedDevice(TF_Graph*, TF_Operation*, char const*) in tf_python_api.cc.o\r\n[error]   \"nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)\", referenced from:\r\n[error]       tensorflow::UpdateInput(TF_Graph*, TF_Operation*, int, TF_Output) in tf_python_api.cc.o\r\n[error]       tensorflow::AddControlInput(TF_Graph*, TF_Operation*, TF_Operation*) in tf_python_api.cc.o\r\n[error]       tensorflow::ClearControlInputs(TF_Graph*, TF_Operation*) in tf_python_api.cc.o\r\n[error]       tensorflow::SetRequestedDevice(TF_Graph*, TF_Operation*, char const*) in tf_python_api.cc.o\r\n[error]   \"tensorflow::Node::input_edge(int, tensorflow::Edge const**) const\", referenced from:\r\n[error]       tensorflow::UpdateInput(TF_Graph*, TF_Operation*, int, TF_Output) in tf_python_api.cc.o\r\n[error] ld: symbol(s) not found for architecture x86_64\r\n```", "@allenlavoie Never mind my last comment. I was doing something wrong with linking. The `TF_LoadLibrary` issue is the only problem I can see at this point. Thanks a lot for getting this out and letting me know! :)", "I can't reproduce duplicate registration errors, at least from Python (which is just a thin wrapper around TF_LoadLibrary). For example adding a bunch of extra tf.load_op_library calls from //tensorflow/user_ops:ackermann_test seems to work fine (on Linux). Is there something else I should be looking for?", "Did you try to duplicate this line? `REGISTER_KERNEL_BUILDER(Name(\"Ackermann\").Device(DEVICE_CPU), AckermannOp);` (i.e., the kernel registration)", "I'm familiar with the error, I'm trying to figure out why you're running into it. Is the name of the kernel library .so changing? "]}, {"number": 12894, "title": "Tensorflow gpu build error", "body": "I am getting the following error when building Cuda enabled Tensorflow.\r\n\r\n> ERROR: /home/ubuntu/tensorflow/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/                                                                      stream_executor:cuda_platform' failed (Exit 1)\r\n> tensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'cudnnStatus_t perftools::gputools::cuda::w                                                                      rap::WrapperShim__cudnnSetRNNDescriptor::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [wit                                                                      h Args = {cudnnRNNStruct*, int, int, cudnnDropoutStruct*, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRN                                                                      NMode_t, cudnnDataType_t}]':\r\n> tensorflow/stream_executor/cuda/cuda_dnn.cc:1017:50:   required from here\r\n> tensorflow/stream_executor/cuda/cuda_dnn.cc:139:46: **error**: cannot convert 'cudnnRNNStruct*' to 'cudnnHandle_                                                                      t {aka cudnnContext*}' for argument '1' to 'cudnnStatus_t cudnnSetRNNDescriptor(cudnnHandle_t, cudnnRNNDescr                                                                      iptor_t, int, int, cudnnDropoutDescriptor_t, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudn                                                                      nRNNAlgo_t, cudnnDataType_t)'\r\n>        cudnnStatus_t retval = ::__name(args...); \r\n\r\n------------------------\r\n\r\nI installed Cuda 8.0.61 (latest) with cudNN 7.0 (latest stable). It is my first time building tensorflow for the GPU so any explanation of the error would be great!\r\n\r\nEdit: Found the answer #12052", "comments": []}, {"number": 12893, "title": "Branch 167933991", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @benoitsteiner to be potential reviewers.", "Some of these commits seem to be on master already. ", "cc @nlopezgi this includes the java build fix."]}, {"number": 12892, "title": "ant(1234)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 12891, "title": "build fail (looser throw specifier in EnvWrapper)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nNo.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nLinux Fedora 17\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nTrying to build from source.\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\n1.3\r\n\r\n- **Python version**: \r\n\r\n2.7\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\n0.5.4\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\nDisabled.\r\n\r\n- **GPU model and memory**:\r\n\r\nDisabled.\r\n\r\n- **Exact command to reproduce**:\r\n\r\nbazel build\r\n\r\n### Describe the problem\r\n\r\nI'm trying to compile tensorflow from source, don't modify anything, simply following the instructions on the webpage:\r\n\r\nbuilt bazel 0.5.4 from source\r\n\r\ngit clone\r\ngit checkout origin/1.3\r\n./configure     # all answers are the defaults\r\n\r\nbazel build --verbose_failures --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nThe above fails with with a looser throw specifier error in core/platform/env.h\r\n\r\nThe versions of the various tools involved:\r\n\r\nopenjdk 1.8.0\r\nbazel 0.5.4\r\npython 2.7\r\ngcc 4.7.2\r\nlibstdc++ 4.7.2\r\n\r\nThe problem is reproducible on my box.\r\n\r\n### Source code / logs\r\n\r\nThe precise failure from the bazel build is this:\r\n\r\n```\r\n> \r\n> \r\n> ERROR: /home/fetch/tensorflow/tensorflow/core/BUILD:1244:1: C++ compilation of rule '//tensorflow/core:lib_internal' failed (Exit 1): gcc failed: error executing command \r\n>   (cd /home/fetch/.cache/bazel/_bazel_fetch/7ef242de6c5f89b75c729448096cd3bb/execroot/org_tensorflow && \\\r\n>   exec env - \\\r\n>     LD_LIBRARY_PATH=/usr/lib64:/usr/lib64/openmpi/lib \\\r\n>     PATH=/opt/obuildfactory/jdk-1.8.0-openjdk-x86_64/bin:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.25.x86_64/bin:/usr/lib64/openmpi/bin:/home/fetch/opt/bin:/home/fetch/bin:/usr/lib64/openmpi/bin:/home/fetch/opt/bin:/home/fetch/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin \\\r\n>     PWD=/proc/self/cwd \\\r\n>     PYTHON_BIN_PATH=/bin/python \\\r\n>     PYTHON_LIB_PATH=/usr/lib/python2.7/site-packages \\\r\n>     TF_NEED_CUDA=0 \\\r\n>     TF_NEED_OPENCL=0 \\\r\n>   /bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++0x' '-march=native' -MD -MF bazel-out/local-opt/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/platform/default/tracing.pic.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/platform/default/tracing.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DSNAPPY -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local-opt/genfiles/external/zlib_archive -iquote external/snappy -iquote bazel-out/local-opt/genfiles/external/snappy -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local-opt/genfiles/external/zlib_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions -msse3 -pthread -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/platform/default/tracing.cc -o bazel-out/local-opt/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/platform/default/tracing.pic.o).\r\n> In file included from ./tensorflow/core/lib/core/threadpool.h:21:0,\r\n>                  from ./tensorflow/core/platform/default/tracing_impl.h:25,\r\n>                  from ./tensorflow/core/platform/tracing.h:266,\r\n>                  from tensorflow/core/platform/default/tracing.cc:16:\r\n> ./tensorflow/core/platform/env.h:295:11: error: looser throw specifier for 'virtual tensorflow::EnvWrapper::~EnvWrapper()'\r\n> ./tensorflow/core/platform/env.h:50:11: error:   overriding 'virtual tensorflow::Env::~Env() noexcept (true)'\r\n> tensorflow/core/platform/default/tracing.cc:32:19: warning: 'tensorflow::port::dummy' defined but not used [-Wunused-variable]\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> INFO: Elapsed time: 418.721s, Critical Path: 19.37s\r\n> \r\n", "comments": ["I forgot to mention the glibc version so everything together again:\r\n\r\nopenjdk 1.8.0\r\nbazel 0.5.4\r\npython 2.7\r\ngcc 4.7.2\r\nlibstdc++ 4.7.2\r\nglibc 2.15\r\n", "I guess we're missing the `noexcept` in the `EnvWrapper`. Do you mind sending a PR?", "I'd be happy to help but I'm not familiar with C++ at all (only C) and it does look like to me to be a C++ specific error.", "Just to see if anything changed lately I tried compiling the current master (commit 32ffc5a81eee8c39bbe71536212a773b1ffd4eb2) and it still fails:\r\n\r\n```\r\ngcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/lib64/ccache -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++0x' '-march=native' -MD -MF bazel-out/local-opt/bin/external/grpc/_objs/grpc++_base_unsecure/external/grpc/src/cpp/client/generic_stub.pic.d '-frandom-seed=bazel-out/local-opt/bin/external/grpc/_objs/grpc++_base_unsecure/external/grpc/src/cpp/client/generic_stub.pic.o' -fPIC -iquote external/grpc -iquote bazel-out/local-opt/genfiles/external/grpc -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/zlib_archive -iquote bazel-out/local-opt/genfiles/external/zlib_archive -iquote external/boringssl -iquote bazel-out/local-opt/genfiles/external/boringssl -isystem external/grpc/include -isystem bazel-out/local-opt/genfiles/external/grpc/include -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/zlib_archive -isystem bazel-out/local-opt/genfiles/external/zlib_archive -isystem external/boringssl/src/include -isystem bazel-out/local-opt/genfiles/external/boringssl/src/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/grpc/src/cpp/client/generic_stub.cc -o bazel-out/local-opt/bin/external/grpc/_objs/grpc++_base_unsecure/external/grpc/src/cpp/client/generic_stub.pic.o).\r\nIn file included from external/grpc/include/grpc++/support/async_stream.h:22:0,\r\n                 from external/grpc/include/grpc++/generic/generic_stub.h:22,\r\n                 from external/grpc/src/cpp/client/generic_stub.cc:19:\r\nexternal/grpc/include/grpc++/impl/codegen/async_stream.h:271:30: error: '<::' cannot begin a template-argument list [-fpermissive]\r\nexternal/grpc/include/grpc++/impl/codegen/async_stream.h:271:30: note: '<:' is an alternate spelling for '['. Insert whitespace between '<' and '::'\r\nexternal/grpc/include/grpc++/impl/codegen/async_stream.h:271:30: note: (if you use '-fpermissive' G++ will accept your code)\r\nexternal/grpc/include/grpc++/impl/codegen/async_stream.h:275:30: error: '<::' cannot begin a template-argument list [-fpermissive]\r\nexternal/grpc/include/grpc++/impl/codegen/async_stream.h:275:30: note: '<:' is an alternate spelling for '['. Insert whitespace between '<' and '::'\r\nexternal/grpc/include/grpc++/impl/codegen/async_stream.h:277:30: error: '<::' cannot begin a template-argument list [-fpermissive]\r\nexternal/grpc/include/grpc++/impl/codegen/async_stream.h:277:30: note: '<:' is an alternate spelling for '['. Insert whitespace between '<' and '::'\r\nexternal/grpc/include/grpc++/impl/codegen/async_stream.h:280:30: error: '<::' cannot begin a template-argument list [-fpermissive]\r\nexternal/grpc/include/grpc++/impl/codegen/async_stream.h:280:30: note: '<:' is an alternate spelling for '['. Insert whitespace between '<' and '::'\r\n```\r\n\r\nThe setup is still the same (versions of libraries, etc) and I left all questions to ./configure the default.", "Is it about which version of the C++ standard is supported by the compiler?", "Well, let me ask a well-defined question: which version of the C++ standard do you guys write the code for? C++03, C++11, C+14?", "@gunan do you know what the problem might be? I cannot reproduce, but the gcc version @fetchinson is using is older then the one that comes with Ubuntu 14.04 (4.7.2 vs 4.8.4). Other tool versions might be different as well", "You need a compiler that supports c++11, anything earlier wont work.\r\nLooks like for full c++11, you need at least GCC 4.8.1:\r\nhttps://gcc.gnu.org/projects/cxx-status.html#cxx11"]}, {"number": 12890, "title": "MKL build broken / Proposed fix here", "body": "The CPU version of TensorFlow wasn't building because the MKL was broken; it was broken because a merge conflict somehow made it all the way through to here.  See diff (!).", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "I signed it\n\nOn 7 Sep 2017 11:41 pm, \"googlebot\" <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address. Check your\n>    existing CLA data <https://cla.developers.google.com/clas> and verify\n>    that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If your company signed a CLA, they designated a Point of Contact who\n>    decides which employees are authorized to participate. You may need to\n>    contact the Point of Contact for your company and ask to be added to the\n>    group of authorized contributors. If you don't know who your Point of\n>    Contact is, direct the project maintainer to go/cla#troubleshoot.\n>    - In order to pass this check, please resolve this problem and have\n>    the pull request author add another comment and the bot will run again.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12890#issuecomment-327946043>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AG6lr4SWwBWoG0EwcyomNHpBCnUIX8Aaks5sgHEigaJpZM4PQdzq>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 12889, "title": "Fix broken link in Estimators.", "body": "Move Estimators to beginning of Prog. Guide.\r\nChange title of Datasets unit.\r\n\r\nPiperOrigin-RevId: 167314186", "comments": ["Jenkins, test this please."]}, {"number": 12888, "title": "[feature] ONNX Support", "body": "Support exporting and loading models in ONNX format.\r\nSee: https://research.fb.com/facebook-and-microsoft-introduce-new-open-ecosystem-for-interchangeable-ai-frameworks/", "comments": ["This would be huge for using tensorflow in production for us (airbnb). We wouldn't even need to use the complicated tracing approach they used for pytorch since Tensorflow has a static graph representation already.", "Linking this issue to it's counterpart over at ONNX https://github.com/onnx/onnx/issues/3", "An experimental Tensorflow backend for onnx originally authored by Arpith Jacob, Tian Jin and Gheorghe-Teodor Bercea from IBM Research has been released here https://github.com/tjingrant/onnx-tf. Comments and contributions are very welcome.", "It looks like the code has been pushed to https://github.com/onnx/onnx-tensorflow. We're generally trying to move libraries in contrib to separate repositories, so the code living there seems like the best option. Should we close this issue?", "this issue also includes exporting from tf to onnx.", "Hi @jhseu, @cancan101 , in this case, do you think we should create a PR for the exporter then? Or would it be better if we just stick to our ONNX repository as for the importer? (Not that we have a fully functional one right now).", "It'd be better to keep both in the same repository in my opinion.", "Sorry to be a little bit rude but is TF officially supporting ONNX or it will be just a community effort cause there is still not an official policy on this topic?", "We do not have anyone actively working on it. I'm personally happy to see the community effort around it.", "But we will not see Google or TF logo in the footer of http://onnx.ai like AWS, FB and MS right?", ">We also have community contributed converters for other projects such as TensorFlow\r\nhttps://research.fb.com/onnx-v1-released/", "Seems a little bit marginal as a PR announcement.", "Also AMD, ARM, IBM, Intel, Huawei, NVIDIA, and Qualcomm logos appeared.", "I know this issue might not get that much attention from Google since they have their own interests, but for the AI research and development community having support for a standard format that is portable across frameworks and runtimes (like TensorRT) is HUGE, it should make tasks like reproducing results, deploying 3rd party models, transfer learning from existing 3rd part models, ect, way easier. I think its very healthy for the ecosystem and Tensorflow's team should embrace it.", "Acually seems that tensorflow onnx support is mainly maintained by IBM and Microsoft resources. ", "We totally agree. Our previous pull request (https://github.com/tensorflow/tensorflow/pull/14310) was turned down because and I quote:\r\n\r\n> we're happy to consider something like this as a separate repository in github.com/tensorflow, but would prefer to avoid moving it to contrib due to the onnx dependency\r\n\r\nAs much as I don't understand the fear of ONNX dependency, I'm willing to continue working on another pull request that isolates the **bad part** of the onnx dependency and only retain the part relevant to Tensorflow conversion.\r\n\r\nSince then our project has grown to support the entire workflow of Training in Tensorflow and Infer in other framework / runtimes AND Training in other frameworks and Infer in Tensorflow. We refer interested parties to our latest tutorial for evaluation and more information.\r\n\r\nExporting Tutorial: https://github.com/tjingrant/tutorials/blob/d79fb4f36108cb384efd14d3def7e4529d412ae8/tutorials/OnnxTensorflowExport.ipynb\r\n\r\nImporting Tutorial:\r\nhttps://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowImport.ipynb\r\n\r\nOur framework is far from perfect and you can get  a sense of operator coverage from our repository documentation about coverage status (https://github.com/onnx/onnx-tensorflow/blob/master/onnx_tf/doc/support_status.md) and API usage (https://github.com/onnx/onnx-tensorflow/blob/master/onnx_tf/doc/API.md).\r\n\r\nTo summarize, we are ready and willing to give this PR another try if the Tensorflow community desires so.", "@tjingrant I've been digging into the `onnx_tf` repo. Thanks a lot for this!!!\r\n\r\nI think that the `onnx` dependency should not be an issue for the following reasons: \r\n* PyTorch has it as a standard module, meaning another big company already trusts that code.\r\n* The format is getting good traction, Microsoft is adding native Windows [support](https://blogs.windows.com/buildingapps/2018/03/07/ai-platform-windows-developers) for it, MXNet Server added support to [serve OXXN](https://github.com/awslabs/mxnet-model-server) models, Nvidia already has [demos](https://www.youtube.com/watch?v=vAelubuwquE) supporting the format.\r\n* There is the [ONNX working groups](https://github.com/onnx/onnx/wiki/%5BAnnouncement%5D-ONNX-working-groups-established) to guide the development of the format so the TensorFlow team can join it they want to have a voice.\r\n\r\nThis pull requests is now 5+ months old and ONNX got 1.0 since then so I'd say that now is a good time to give it a try again. Seeing all of the commitment from other players I find it unacceptable that Tensorflow which is the leading framework is not part of this initiative, its as if the Chrome team missed out on web assembly. ", "Also, I think Google could favor from being able to serve ONNX models in ML Engine.", "cc @jhseu ", "So what is the issue here? Does TF prefer to move ONNX to another repo in order to prevent better interoperability? Are you guys doing the same thing that Microsoft did 10/20 years ago with Office and the `.doc`/`.docx` format (and that's only an example among many others)?\r\n\r\nApologize if the issue is only technicals but as a TF user I am really in favour of interoperability and that's what ONNX is all about.", "I understand @hadim position. I hope  that TF start to listen a little bit more users requests to let be a little bit community driven. For TF I suggest to play an active role on ONNX so they could try to let it to be owned by a more indipendent entity like:\r\nhttps://www.partnershiponai.org\r\nhttps://www.linuxfoundation.org/projects/deep-learning/\r\n\r\n ", "I'd really love to hear some voice coming from Google regarding this issue. It's been too long since the last Googler has commented on this thread. Silence really doesn't help resolve the issue.", "+1 for interoperability and open standards. ", "any update on this? I see there are converters, but still no official support, is this correct?", "This issue is more political than technical, I guess this will be\nimplemented in some years when innovation rate drops and integration\nbecomes a priority.\n\nOn Fri, May 24, 2019, 09:06 kartur <notifications@github.com> wrote:\n\n> any update on this? I see there are converters, but still no official\n> support, is this correct?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12888?email_source=notifications&email_token=ABMXGVDCR6IZ65DVBKLWHQ3PW7Y73A5CNFSM4D2BRBFKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWFOF5Q#issuecomment-495641334>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABMXGVBCPOZEJ7KTM2PEBIDPW7Y73ANCNFSM4D2BRBFA>\n> .\n>\n", "https://github.com/onnx/onnx/wiki/Expanded-ONNX-Steering-Committee-Announced!#expanded-onnx-steering-committee-announced", "Not sitting at the table I think it's a rearguard battle", "Tensorflow support for ONNX is being driven by Microsoft, this is very\nfunny.\n\nhttps://github.com/onnx/onnx-tensorflow\n\nOn Thu, May 30, 2019 at 6:54 PM bhack <notifications@github.com> wrote:\n\n> Not sitting at the table I think it's a rearguard battle\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12888?email_source=notifications&email_token=ABMXGVBTELJG7SYD2ZE3X7DPYBSLDA5CNFSM4D2BRBFKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWT2EFI#issuecomment-497525269>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABMXGVFAZXPDIZDSHYRIE23PYBSLDANCNFSM4D2BRBFA>\n> .\n>\n", "This is [2nd most voted Tensorflow issue](https://github.com/tensorflow/tensorflow/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc) after Opencl.", "> Tensorflow support for ONNX is being driven by Microsoft, this is very funny.\r\n\r\nThe linked repo is from IBM. They joined with Microsoft though:\r\n\r\n> We have joined force with Microsoft to co-develop ONNX Tensorflow frontend. For current onnx-tf frontend users, please migrate to use tf-onnx (https://github.com/onnx/tensorflow-onnx) where our code had been merged into.", "Onnx now It Is vendor neutral under the Linux Foundation https://lfai.foundation/projects/", "https://lfai.foundation/press-release/2019/11/14/lf-ai-welcomes-onnx/", "Check also https://github.com/onnx/onnx/issues/2190", "We are the team that created onnx-tensorflow, and we propose to support ONNX in MLIR!\r\nCheck out our contribution proposal https://github.com/onnx/onnx/issues/2499 .", "Hi @cancan101 ! We are checking to see whether you still need help in this issue. Inline with above comments , Have you checked these threads on Tensorflow to Onnx conversion yet . [Link1](https://github.com/onnx/tensorflow-onnx),[Link2](https://medium.com/analytics-vidhya/how-to-convert-your-keras-model-to-onnx-8d8b092c4e4f) ,[Link3 ](https://medium.com/nerd-for-tech/how-to-convert-tensorflow2-model-to-onnx-using-tf2onnx-when-there-is-custom-ops-6e703376ef20).Thanks!", "I think this issue can be closed. It was filed 4 years ago, long before any of those projects existed ;-)"]}]