[{"number": 40312, "title": "Using a FCN in a for loop", "body": "Hello,\r\nI have a question, if I want to create a Fully Connected Network and I would like to place it in a for loop, I want to tell Tensorflow that the weights of my FCN are shared between all iterations of this for loop, and prevent the creation of \"N\" instances of different FCNs.\r\nWhat should I do in this case?", "comments": ["@Antonio-git-lab,\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40311, "title": "How to use BlockLSTM and BlockLSTMGrad to build a LSTM network in C++?", "body": "Anyone have idea on how to use BlockLSTM and BlockLSTMGrad to build a LSTM network in C++? It take me quit a long on this problem, please give me a simple example, many thanks!", "comments": ["@cj741 \r\n\r\nPlease refer to this [link](https://tensorflow.google.cn/api_docs/python/tf/raw_ops/BlockLSTM?authuser=0&hl=Es) , [link1](http://maggus.com.co/TestLeo/Django/test34/lib/python3.4/site-packages/tensorflow/contrib/rnn/ops/gen_lstm_ops.py) \r\n\r\n\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as it has been inactive for more than 3 weeks. Please add additional comments for us to open the issue again. Thanks!"]}, {"number": 40310, "title": "Tried to export a function which references untracked object Tensor(\"5486:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS 10.15.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip \r\n- TensorFlow version (use command below):2.2\r\n- Python version:3.5/3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n1.  When I want to export saved model, I have met this crash \"Tried to export a function which references untracked object Tensor(\"5486:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\"\r\n\r\n2. When I want to predict model manually, the log logs some warning :\"\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0609 15:31:30.150616 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer.iter\r\nW0609 15:31:30.150791 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_1\r\nW0609 15:31:30.150847 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_2\r\nW0609 15:31:30.150895 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer.decay\r\nW0609 15:31:30.150939 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer.learning_rate\r\nW0609 15:31:30.150986 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\r\nW0609 15:31:30.151030 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\r\nW0609 15:31:30.151072 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\r\nW0609 15:31:30.151117 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).bidirectional_layer.forward_layer.cell.kernel\r\nW0609 15:31:30.151160 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).bidirectional_layer.forward_layer.cell.recurrent_kernel\r\nW0609 15:31:30.151202 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).bidirectional_layer.forward_layer.cell.bias\r\nW0609 15:31:30.151246 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).bidirectional_layer.backward_layer.cell.kernel\r\nW0609 15:31:30.151288 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).bidirectional_layer.backward_layer.cell.recurrent_kernel\r\nW0609 15:31:30.151330 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).bidirectional_layer.backward_layer.cell.bias\r\nW0609 15:31:30.151374 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\r\nW0609 15:31:30.151417 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\r\nW0609 15:31:30.151458 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\r\nW0609 15:31:30.151500 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).bidirectional_layer.forward_layer.cell.kernel\r\nW0609 15:31:30.151542 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).bidirectional_layer.forward_layer.cell.recurrent_kernel\r\nW0609 15:31:30.151585 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).bidirectional_layer.forward_layer.cell.bias\r\nW0609 15:31:30.151627 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).bidirectional_layer.backward_layer.cell.kernel\r\nW0609 15:31:30.151669 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).bidirectional_layer.backward_layer.cell.recurrent_kernel\r\nW0609 15:31:30.151710 140734750367168 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).bidirectional_layer.backward_layer.cell.bias\r\nW0609 15:31:30.151752 140734750367168 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n\"\r\nI don't know what's wrong on my customed model.\r\n**Describe the expected behavior**\r\nI want to predict from model and export saved model\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nCode can be download from baidu net drive by\r\nlink: https://pan.baidu.com/s/1ftizcVQsMXuAR91-X0Md4A \r\nkey: 6rns\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Goofy-G \r\n\r\nI am not seeing code in provided link ,also it is in different language.Request you to helps us with colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Hi\r\n   See the attach files\r\n[TestTagger1.zip](https://github.com/tensorflow/tensorflow/files/4750867/TestTagger1.zip)\r\n\r\n", "@Goofy-G \r\n\r\nI have downloaded the zip file and i am seeing multiple python files. Can you please provide the instructions or steps to reproduce the issue.Thanks!", "@ravikyram \r\n   You can directly run test/runner_test.py/test_predict method for predicting, test/runner_test.py/test_train method for training, test/runner_test.py/test_eval for evaluating and test/runner_test.py/test_export for exporting. \r\n   Model codes are in tagger/model.py, running codes are in tagger/runner.py and making dataset codes in tagger/data. Training source files are in data directory.", "@Goofy-G Please post this question in stackoverflow as its not related to bug/performance related issue. Thanks!", "@gowthamkpr \r\n  Firstly, I know first problem not related to bug/performance, the key is not tracing vacobulary file about tf.lookup op. But when I use tf 1.14 with estimator to build this model,  there has no question about tf.lookup op written in model_fn. So, I actually want to ask how to export serving model with tf.lookup op in model_fn by tf 2.x.\r\n  Secondly,  the second problem I think may be a bug. I use callback's modelcheckpoint to save model's weights, however when I loaded the checkpoint file to do model prediction there was no results but None I got. Additionally, I received a bunch of warning above. So I think model doesn't load the ckpt's saved weights successfully.", "**Related issue/trying to solve something similar:** \r\n- https://github.com/tensorflow/serving/issues/1719\r\n- https://github.com/tensorflow/tensorflow/issues/42325", "@Goofy-G Please follow the comment [here](https://github.com/tensorflow/serving/issues/1719#issuecomment-677831238), where the issue has been resolved. Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40310\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40310\">No</a>\n"]}, {"number": 40309, "title": "how do you get the string out of  Tensor(\"args_0:0\", shape=(), dtype=string) type?:", "body": "I tried to get the string out of \r\ntype  tensor(\"args_0:0\", shape=(), dtype=string). \r\n\r\nThe reason I tried to do is that I want to get the original image size \r\n\r\n```\r\n\r\n dataset = tf.data.Dataset.from_tensor_slices((images,boxes,labels))\r\n    run_train(dataset.map(resize_image_bbox, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(1).prefetch(tf.data.experimental.AUTOTUNE))\r\n\r\n```\r\n\r\nthis images is the list of image_path \r\nthen I passed it to the \r\n\r\n\r\n```\r\ndef resize_image_bbox(image,boxes,labels):\r\n\r\n    print('image->',image)\r\n    img = tf.io.read_file(image)\r\n    img = tf.image.decode_jpeg(img, channels=3)\r\n    img = tf.cast(img, tf.float32)\r\n    newSize = (300, 300)\r\n\r\n    new_img = tf.image.resize(img,newSize)\r\n    return new_img,boxes,labels\r\n```\r\nI also need to resize my boxes with original image size\r\nbut I have no idea to get the original image size from it \r\n\r\nso I tried to read it from  the path then get the original image size \r\n\r\nIf you have better idea let me know\r\n=============================================================\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 19.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): 2.0 \r\n- Python version: 2.0 \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: titan xp \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n", "comments": ["@SlowMonk,\r\nYou can use the `.numpy()` function to extract the string from the Tensor. Please check [this gist](https://colab.research.google.com/gist/amahendrakar/4645fa442bcbcf63365b8390ce75c575/40309.ipynb) for reference. \r\n\r\nAlso, if this issue is not a TensorFlow bug or feature request, it is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow). Since there is a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@SlowMonk  - Did you get a way to extract value?", "The same question", "Workaround https://stackoverflow.com/questions/58969880/how-to-use-map-with-tuples-in-a-tensorflow-2-dataset did not work.\r\n\r\nThe second workaround with its own generator works now without any map() call.", "Won't work because when we use the map function it passes a Tensor object and not EagerTensor which has no .numpy() function\r\n\r\nPlease provide recourse, thanks", "This issue is in no way resolved and should not be closed.", "same here... I try to parse file_path after executing `dataset.map()` to get the label value from the map, which is a float value and because of that I am unable to do so"]}, {"number": 40308, "title": "TypeError: Expected int64, got 1e-07 of type 'float' instead : FasterRCNN tensorflow 2.x", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linus Ubuntu 18.04.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.1.0\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: GTX1650\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nParsing annotation files\r\nTraining images per class:\r\n{'Platelets': 234, 'RBC': 2606, 'WBC': 231, 'bg': 0}\r\nNum classes (including bg) = 4\r\nConfig has been written to config.pickle, and can be loaded when testing to ensure correct results\r\nNum train samples 193\r\nNum val samples 31\r\nloading weights from pretrain/vgg16_weights_tf_dim_ordering_tf_kernels.h5\r\nCould not load pretrained model weights. Weights can be found in the keras application folder           https://github.com/fchollet/keras/tree/master/keras/applications\r\nno previous model was loaded\r\nStarting training\r\nEpoch 1/50\r\nException: in converted code:\r\n\r\n/home/reighns/ObjectDetection/blood_cells_detection/keras-frcnn-ken/fasterrcnn/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py:305 train_on_batch  *\r\n    outs, total_loss, output_losses, masks = (\r\n/home/reighns/ObjectDetection/blood_cells_detection/keras-frcnn-ken/fasterrcnn/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py:253 _process_single_batch\r\n    training=training))\r\n/home/reighns/ObjectDetection/blood_cells_detection/keras-frcnn-ken/fasterrcnn/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py:167 _model_loss\r\n    per_sample_losses = loss_fn.call(targets[i], outs[i])\r\n/home/reighns/ObjectDetection/blood_cells_detection/keras-frcnn-ken/fasterrcnn/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py:221 call\r\n    return self.fn(y_true, y_pred, **self._fn_kwargs)\r\n/home/reighns/ObjectDetection/blood_cells_detection/keras-frcnn-ken/keras_frcnn/losses.py:42 rpn_loss_cls_fixed_num\r\n    return lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * tf.keras.losses.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\r\n/home/reighns/ObjectDetection/blood_cells_detection/keras-frcnn-ken/fasterrcnn/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py:994 binary_crossentropy\r\n    K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\r\n/home/reighns/ObjectDetection/blood_cells_detection/keras-frcnn-ken/fasterrcnn/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:4602 binary_crossentropy\r\n    epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\r\n/home/reighns/ObjectDetection/blood_cells_detection/keras-frcnn-ken/fasterrcnn/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:678 _constant_to_tensor\r\n    return constant_op.constant(x, dtype=dtype)\r\n/home/reighns/ObjectDetection/blood_cells_detection/keras-frcnn-ken/fasterrcnn/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py:258 constant\r\n    allow_broadcast=True)\r\n/home/reighns/ObjectDetection/blood_cells_detection/keras-frcnn-ken/fasterrcnn/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py:297 _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n/home/reighns/ObjectDetection/blood_cells_detection/keras-frcnn-ken/fasterrcnn/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py:452 make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n/home/reighns/ObjectDetection/blood_cells_detection/keras-frcnn-ken/fasterrcnn/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py:332 _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\n\r\nTypeError: Expected int64, got 1e-07 of type 'float' instead.\r\n\r\n**Describe the expected behavior**\r\nExpected to train a faster rcnn model using https://github.com/kentaroy47/frcnn-from-scratch-with-keras repo; I want to use the code in tf2.x version but encounter this error. The error might be related to the loss function py file.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@ghnreigns \r\nPlease refer to this [comment](https://github.com/tensorflow/tensorflow/issues/33855#issuecomment-572176122) on similar issue and let us know if it helps, else share a simple stand alone code for us to replicate the issue faced or a colab gist with the error.", "@Saduf2019 Let me try it, thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40308\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40308\">No</a>\n"]}, {"number": 40307, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 home\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nwhile doing import tensorflow as tf, I'm getting error\r\n\r\n\r\n**Any other info / logs**\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nA:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59 \r\n\r\nA:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nA:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nA:\\anaconda\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\nA:\\anaconda\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\nA:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\nA:\\anaconda\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     48 import numpy as np\r\n     49 \r\n---> 50 from tensorflow.python import pywrap_tensorflow\r\n     51 \r\n     52 # Protocol buffers\r\n\r\nA:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     67 for some common reasons and solutions.  Include the entire stack trace\r\n     68 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 69   raise ImportError(msg)\r\n     70 \r\n     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"A:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"A:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"A:\\anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"A:\\anaconda\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"A:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["@abhinavsp0730 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40307\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40307\">No</a>\n"]}, {"number": 40306, "title": "None Type not Supported", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (or github SHA if from source): 1.14\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs*\r\n\r\nHi, when I trying to convert a .pb file to a .tflite model, my input tensor is of shape [None, None,None,4] . I am getting the None Type not supported error. \r\nHow ever if I replace it with integer values like, [200,200,200,4] it works. \r\nHow should I include the None Type support while convert .pb file to a .tflite file? ", "comments": ["@purva98,\r\nCould you please provide the complete code to reproduce the issue reported here or the `.pb` file you are trying to convert. \r\n\r\nAlso, please upgrade to TF 2.x as TF 1.x is not actively supported. Thanks!", "I believe the stable release of tensorflow do not support None type in input. Use nightly, this support has been added recently. I was able to convert he graphs successfully that has None shape tensors. ", "@hegman12, can you please share a small script or snippet of your code, it would be a great help!\r\nThanks ", "check this issue #40185 ", "> @purva98,\r\n> Could you please provide the complete code to reproduce the issue reported here or the `.pb` file you are trying to convert.\r\n> \r\n> Also, please upgrade to TF 2.x as TF 1.x is not actively supported. Thanks!\r\n\r\n@purva98,\r\nAny updates regarding this issue? Thanks!", "Yes @amahendrakar , the issue was resolved by using the tf-nightly version of tensorflow. "]}, {"number": 40305, "title": "[ROCm] Fixing and enabling //tensorflow/core/util:gpu_kernel_helper_test_gpu", "body": "This PR fixes and enables //tensorflow/core/util:gpu_kernel_helper_test_gpu for ROCm.", "comments": []}, {"number": 40304, "title": "[ROCm] Remove duplicate macros", "body": "This PR removes some macros which are now duplicated in  tensorflow/core/kernels/non_max_suppression_op.cu.cc and tensorflow/core/util/gpu_device_functions.h. This change was broken out of #39562 by reviewer's request. ", "comments": []}, {"number": 40303, "title": "Add support for the cuDNN NHWC depthwise conv", "body": "The PR further relaxes the conditions to support more cuDNN depthwise convolutions, since cuDNN v8 has improved its support for NHWC format.\r\n\r\n> Depthwise convolution extension\r\nWe\u2019ve extended the fprop and dgrad NHWC depthwise kernels to support more combinations (filter sizes/strides) such as 5x5/1x1, 5x5/2x2, 7x7/1x1, 7x7/2x2, which provides good performance.\r\n\r\n(https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_8.html#rel-800-Preview).\r\n\r\nFYI. @nluehr \r\n", "comments": []}, {"number": 40302, "title": "File system scheme 's3' not implemented", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\ntensorflow.python.framework.errors_impl.UnimplementedError: File system scheme 's3' not implemented\r\n\r\n**Describe the expected behavior**\r\nShould be able to connect the s3 filesystem.\r\n\r\n**Standalone code to reproduce the issue**\r\nfrom tensorflow.python.lib.io import file_io\r\nprint file_io.stat('s3://bucketname/path/')\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Can you try with the final 2.2 release please?", "Oh. You are using Windows. Windows does not yet support S3 filesystems, but there is a GSoC project to enable that.", "@mihaimaruseac will it be available to use in next tensorflow release? I find the PR #39124 to support the S3 filesystem for windows from master branch and tried to apply them to 2.2 release but got the same error. I am not sure if I am missing something. ", "#39124 will be included in the upcoming 2.3 release.\r\n\r\nModular filesystems (tensorflow/community#101) will probably come in 2.4", "Hi, I'm facing the exact same error with s3a://\r\nAnd when using s3://, I get: \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-42-6df8b1dd73c9> in <module>\r\n      1 from tensorflow.python.lib.io import file_io\r\n----> 2 print(file_io.stat(s3_base_path + \"/wine-quality.csv\"))\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/lib/io/file_io.py in stat(filename)\r\n    727     errors.OpError: If the operation fails.\r\n    728   \"\"\"\r\n--> 729   return stat_v2(filename)\r\n    730 \r\n    731 \r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/lib/io/file_io.py in stat_v2(path)\r\n    744   \"\"\"\r\n    745   file_statistics = pywrap_tensorflow.FileStatistics()\r\n--> 746   pywrap_tensorflow.Stat(compat.as_bytes(path), file_statistics)\r\n    747   return file_statistics\r\n    748 \r\n\r\nNotFoundError: Object s3://<path>/wine-quality.csv does not exist\r\n\r\n```\r\nI have set the aws creds per: https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/s3.md\r\nTF version: 2.1.0\r\nPython 3\r\nOS: Linux (7.6 (Maipo)\r\n\r\nPlease help ", "Also, I can read s3**a**://path/to/wine-quality.csv using Spark.", "@vaibhavsingh007 we only implement support for the `s3://` URI scheme. Didn't know there is a `s3a://` too. Do you know what are the differences?", "Yes, s3a is the latest protocol and is more performant than its predecessors s3/s3n: https://stackoverflow.com/questions/33356041/technically-what-is-the-difference-between-s3n-s3a-and-s3\r\n\r\nWould you know if or when we may expect TF to S3 functionality become available?  ", "There is funcionality for basic `s3://` scheme on Linux. Then there is work for tensorflow/community#101 to convert all these filesystem to a plugin based form so they can be used on all platforms, on demand.\r\n\r\nRegarding `s3a` and `s3n`, can you file a new issue, feature request type, and assign it to me/mention me? just to keep this issue only for the `s3` not implemented error.", "Done. Seems the bot auto assigned and I can't change the assignee.", "Thank you. Yes, the bot assigns oncall for triage.", "So @mihaimaruseac I wanted to draw our attention back to the original issue of not being able to write to S3 from TF using s3://.. \r\n\r\nI get the error _NotFoundError: Object s3://<path>/wine-quality.csv does not exist_ (from the post above).", "Hmm, what would be the output of `tf.io.gfile.exists` on that path?", "o/p of above is 'False'. \r\nAlso, I was able to get some backend logs for the stat call, in case it helps: \r\n\r\n![image](https://user-images.githubusercontent.com/4958034/87963548-7e303500-ca7e-11ea-88bb-10212cd34dcf.png)\r\n", "And I'm setting these auth vars:\r\n\r\n```\r\nos.environ[\"AWS_ACCESS_KEY_ID\"]=\"***\"\r\nos.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"***\"\r\nos.environ[\"AWS_REGION\"]=\"us-east-1\"\r\n```\r\n", "@mihaimaruseac any luck? ", "Sorry, didn't have time to look more into this. Can you manually issue the `curl` requests?", "Well like I said can access the said files using s3a:// via Spark. But let me try curling as well. ", "@mihaimaruseac I couldn't set up curl, but successfully read file using aws api via boto3. Here is a snapshot: \r\n\r\n![image](https://user-images.githubusercontent.com/4958034/88334162-f3a63a80-ccf6-11ea-8125-ba8cb2f7c2fa.png)\r\n", "Would you like to share a sample code to test read from private (secured) s3 repo using TF? I hope I am using the correct auth configuration (shared before). \r\n\r\n> And I'm setting these auth vars:\r\n> \r\n> ```\r\n> os.environ[\"AWS_ACCESS_KEY_ID\"]=\"***\"\r\n> os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"***\"\r\n> os.environ[\"AWS_REGION\"]=\"us-east-1\"\r\n> ```\r\n\r\n", "Current implementation uses curl to handle the requests to the S3 API. The modular filesystem approach that is currently in the works would use awscpp.\r\n\r\nBut until then, I think we need to differentiate between TF error and curl error.", "Hi, I am not sure I understand what you mean by 'differentiate between TF error and curl error'. Is there something else you want me to try since we still cannot read/write to S3 using TF.", "Can you intercept the requests TF makes during `tf.io.gfile.file_exists` on the path and then try playing them manually via `curl`? Looking at the provided logs it looks like there is a failure on the server / invalid credentials/cookies/arguments get sent.", "Would you be able to direct me how I can do that? ", "Hmm, I was wrong, it seems we are already using AWS library not issuing curl requests manually\r\n\r\nhttps://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/s3/s3_file_system.cc;l=707-766;drc=791f4f2b773208b21ece71e1d37204a91720346d\r\n\r\nYou could intercept the traffic using wireshark or if you can compile from source, I'd recommend adding some debug prints in the function I linked above.\r\n\r\nUnfortunately I currently don't have access to an S3 bucket and won't be able to look much into this issue until mid of August. Apologies for the delay", "Question: can you compile TF from source? Or parts of TF.\r\n\r\nWe have an experimental plugin for S3 support that might be easier to use than the existing non-modular approach. But at the moment it requires manual compile.", "Hi @mihaimaruseac sorry for the delay. \r\nI'm not sure we can do that but I may try. Can you share more details about the new API? ", "Hi.\r\n\r\nWe are planning to modularize filesystems in TF and only offer the local filesystem by default in the pip package, requiring every other filesystem to be loaded at runtime from a shared library. RFC is at https://github.com/tensorflow/community/blob/master/rfcs/20190506-filesystem-plugin-modular-tensorflow.md and currently the implementation is at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/c/experimental/filesystem\r\n\r\nI would suggest compiling the S3 plugin from the experimental filesystem and then using `tf.load_library` to load the shared object.", "can you please describe steps to do for implementing s3 filesystem on winows platform or it still require loading shared library at runtime?  ", "Compiling the plugin in https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/plugins/s3/;drc=75a455a9416630e4a33d23b4dc27bbbdb9c75f41 gives you a shared object that you then load at runtime.\r\n\r\nThis should work on all operating systems.", "The `s3` filesystem is available on windows by install `pip install tensorflow-io==0.17`. Could you give it a try ? Note that in order to avoid conflicts with current `s3` in TensorFlow, it is accessible by `s3e` uri, i.e `s3e://bucket/key`.\r\n\r\nExample : https://github.com/tensorflow/io/blob/master/tests/test_s3_eager.py", "@kumaradi08 @vaibhavsingh007  As per the mihai latest comment, it should work on all Operating systems.  Could you please confirm and if yes, Please go ahead and close the issue if you don't have any further queries.Thanks!", "@saikumarchalla @kumaradi08 @vaibhavsingh007 Actually there is one more issue https://github.com/tensorflow/tensorflow/issues/49515 which will be fixed by #49520.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "This is there in 2.6 version of tensorflow\r\n\r\nRunning in Docker image \"tensorflow/tensorflow:2.6.0\" (on Linux - Kuberenetes KIND Kubeflow)\r\n\r\n```\r\nFile \"/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\", line 2219, in set_model\r\n    self._write_keras_model_summary()\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\", line 2258, in _write_keras_model_summary\r\n    with self._train_writer.as_default():\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\", line 2228, in _train_writer\r\n    self._train_dir)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py\", line 554, in create_file_writer_v2\r\n    create_fn=create_fn, init_op_fn=init_op_fn)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py\", line 313, in __init__\r\n    self._init_op = init_op_fn(self._resource)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_summary_ops.py\", line 146, in create_summary_file_writer\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 6941, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnimplementedError: File system scheme 's3' not implemented (file: 's3://tfboard/logs/20210825-092239/train') [Op:CreateSummaryFileWriter]\r\n```\r\n", "I'm also getting the above error with Tensorflow 2.6\r\n\r\n```\r\nwriter = tf.io.TFRecordWriter(tf_record_path)\r\n```\r\n\r\n```\r\nException has occurred: UnimplementedError\r\nFile system scheme 's3' not implemented (file: 's3://ffda-poi/training_data/mauritania_by_areas/rural/od/large_tfrecords/train_mauritania_rural_od_large_training_data_001.tfrecords')\r\n  File \"/tfrecords/write_tfrecords_od.py\", line 219, in write_tfrecord_file\r\n    writer = tf.io.TFRecordWriter(tf_record_path)\r\n  File \"/tfrecords/write_tfrecords_od.py\", line 280, in write_tfrecord_od\r\n    write_tfrecord_file(train_df, tile_path, record_file_train, super_tile_size)\r\n  File \"/tfrecords/write_tfrecords_od.py\", line 318, in main\r\n    write_tfrecord_od(csv_file, output_dir, tile_path, super_tile_size, geojson_with_neg_path)\r\n  File \"/tfrecords/write_tfrecords_od.py\", line 322, in <module>\r\n    main()\r\n```", "I can confirm this error does not happen if I install tensorflow 2.3", "> The `s3` filesystem is available on windows by install `pip install tensorflow-io==0.17`. Could you give it a try ? Note that in order to avoid conflicts with current `s3` in TensorFlow, it is accessible by `s3e` uri, i.e `s3e://bucket/key`.\r\n> \r\n> Example : https://github.com/tensorflow/io/blob/master/tests/test_s3_eager.py\r\n\r\nThis should be the solution here. For cloud filesystems you have to also install the SIG IO wheel."]}, {"number": 40301, "title": "AutoGraph could not transform", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 2080TI 11gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThe following warning is printed\r\n```\r\nWARNING:tensorflow:AutoGraph could not transform <function parse_dataset.<locals>._parse at 0x7f3d8c126cb0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\n```\r\n**Describe the expected behavior**\r\nNo warning\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef bytes_feature(value):\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\n\r\ndef float_tensor_to_bytes_feature(value):\r\n    return bytes_feature(tf.io.serialize_tensor(tf.convert_to_tensor(value, dtype=tf.float32)).numpy())\r\n\r\n\r\ndef parse_dataset(dataset, feature_description, n_parallel_calls=None):\r\n    def _parse(example_proto):\r\n        deserialized_dict = tf.io.parse_single_example(example_proto, feature_description)\r\n        return deserialized_dict\r\n\r\n    parsed_dataset = dataset.map(_parse, num_parallel_calls=n_parallel_calls)\r\n    return parsed_dataset\r\n\r\n\r\nv = [1, 2, 3]\r\n\r\nfeatures = {\r\n    'x': float_tensor_to_bytes_feature(v)\r\n}\r\n\r\nexample_proto = tf.train.Example(features=tf.train.Features(feature=features))\r\nexample = example_proto.SerializeToString()\r\nserialized_tensors = [example]\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(serialized_tensors)\r\n\r\nfeatures_description = {\r\n    'x': tf.io.FixedLenFeature([], tf.string),\r\n}\r\nparsed_dataset = parse_dataset(dataset, features_description)\r\n\r\nprint(next(iter(parsed_dataset)))\r\n\r\n```", "comments": ["@PeterMitrano \r\n\r\nI tried in colab with TF 2.1, 2.2 and nightly versions and i am not seeing any warning message.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/3a75853c7c6841a4c8ad07afdb4206f9/untitled969.ipynb).Please, verify the issue and close the issue if you feel issue was resolved. Thanks!", "small mistake -- I'm actually using CUDA 10.2\r\nI also have `TF_CPP_MIN_LOG_LEVEL=3` in my environment. I also have `gast==0.3.3` instead of `gast==0.2.2` which I see from your colab notebook. perhaps one of these differences explains it?", "yes, downgrading gast to `0.2.2` fixes the problem. Not sure how my environment got this way.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40301\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40301\">No</a>\n"]}, {"number": 40300, "title": "Updated documentation for GRU & LSTM", "body": "Using cuDNN in GRU & LSTM require eager execution.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/35501", "comments": []}, {"number": 40299, "title": "TFLM: To support Himax WE1 EVB", "body": "In this pull request support Himax WiseEye Plus platform is added.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40299) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40299) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40299) for more info**.\n\n<!-- need_author_consent -->", " @googlebot I consent.", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40299) for more info**.\n\n<!-- ok -->"]}, {"number": 40297, "title": "Mask-RCNN conversion succeeds, but requires select-tf-ops despite selecting only TFLITE_BUILTINS", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\ntensorflow:2.2.0-gpu-jupyter docker image, so... binary?\r\n- TensorFlow version (or github SHA if from source):\r\n2.2.0\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nconverter = lite.TFLiteConverter.from_keras_model(model.keras_model)\r\nconverter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.experimental_new_converter = True\r\nconverter.allow_custom_ops = False\r\nconverter.representative_dataset = [(np.random.random((256,256,3))*255).astype(np.uint8)]\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n256614676\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nWhat is a good way to provide this?\r\nOriginal model parameters from matterport repo here: https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\nThe produced tflite model works on my android device, but it requires the tf-select-ops library, and fails to accept the provided GpuDelegate.\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@jtrammell-dla  Could you share which ops make it require tf-select-ops?", "Thanks for the fast response. Gladly! How do I figure that out?", "FYI, the GPU delegate supported op set is a subset of TFLite builtin ops and if the given model requires tf ops, which are not supported via TFLite Builtin op, those unsupported ops won't be supported via GPU delegate as well.\r\n\r\nYou can find the unsupported tf ops in TFLite builtin ops in the failed log when you convert without allowing custom ops and with `converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS]`.", "Ugh, my apologies... After a little more digging, it turns out that I had forgotten to actually install the new tflite file on the device. I'm still having problems, but they aren't tf-selec-ops related so I'll post them on a new ticket if I can't work them out. Thanks again for the fast response!", "@jtrammell-dla were you able to convert your model to tflite and run inference on it? I was able to convert it [[code](https://gist.github.com/bmabir17/754a6e0450ec4fd5e25e462af949cde6)] with tensorflow 1.13.1 but the converted model seems to have different node input as described [here](https://github.com/matterport/Mask_RCNN/issues/563#issuecomment-646546709). Would you be kind enough to share how you did this using tensorflow 2.2?", "I started with the matterport repo. I then switched all the keras imports to tf.keras, and then I updated the handful of function calls which no longer exist in TF2.2.0. In every case (and there were only a handful, maybe ten at most) it was a trivial issue of the function call being moved to a new path in TF2.2.0. It did not require generating any new code with one very minor exception, which was that the region proposals needed to have their shape specified so downstream ops would work.\r\n\r\nGetting it to work from there in TFLite was a little more complex, and will vary depending on your needs. I'm still in the process of adapting it, but yes, it can be done, with some caveats. It will not run with the TFLite GpuDelegate. It will not run without select-tf-ops. I've re-written the only function (tf.image.crop_and_resize) which actually requires the additional AAR, using TFLite supported code, and it compiles, but it fails with inexplicable \"dimension mismatch\" errors during inference.", "> using TFLite supported code, and it compiles, but it fails with inexplicable \"dimension mismatch\" errors during inference.\r\n\r\n@jtrammell-dla yeah, i am also stuck in there. it seems the converted model was not converted properly.\r\nPlease refer to this [comment](https://github.com/tensorflow/tensorflow/issues/39179#issuecomment-646544647)", "I suggest giving up on a full-on native TFLite implementation, and just stick with tf.image.crop_and_resize, and add tensorflow-lite-select-tf-ops to your build.", "> I suggest giving up on a full-on native TFLite implementation, and just stick with tf.image.crop_and_resize, and add tensorflow-lite-select-tf-ops to your build.\r\n\r\n@jtrammell-dla  how can you do that ?", "> > I suggest giving up on a full-on native TFLite implementation, and just stick with tf.image.crop_and_resize, and add tensorflow-lite-select-tf-ops to your build.\r\n> \r\n> @jtrammell-dla how can you do that ?\r\n\r\nThis guide here discusses including select-tf-ops:\r\n\r\nhttps://www.tensorflow.org/lite/guide/ops_select", "thanks @jtrammell-dla, will try it...", "@jtrammell-dla thanks a lot, it worked !! I converted it and called it:\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"../../logs/mask-rcnn-model2.tflite\")\r\nINFO: Initialized TensorFlow Lite runtime.\r\n\r\nwhere before got the error \r\nValueError: Didn't find custom op for name 'CropAndResize' with version 1\r\n\r\nnow I'm trying to continue with the inference which I'm trying to do in python to check if everything is fine... now I call:\r\n\r\ninterpreter.allocate_tensors() \r\n\r\nand get:\r\n\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you invoke the Flex delegate before inference.Node number 276 (Flex) failed to prepare.\r\n\r\nI'm following these instructions: https://www.tensorflow.org/lite/guide/inference, can you tell me if I have to do something different or maybe other link with other instructions, I'm sorry but this is my first time trying to use tflite. If you could I would greatly appreciate it.", "Yeah, unfortunately you have to deploy it to a mobile device to test it out. Only native TFLITE ops are supported by the Python implementation of Interpreter.", "Or, more specifically, it seems that the SELECT ops have support in the nightly build of the Python implementation, so if you are willing to go down that road, it might work for you...\r\n\r\nhttps://www.tensorflow.org/lite/guide/ops_select", "ok, thanks a lot @jtrammell-dla, will check it out. "]}, {"number": 40296, "title": "TFLM: To support Himax WE1 EVB", "body": "In this pull request support Himax WiseEye Plus platform is added. ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40296) for more info**.\n\n<!-- need_author_cla -->", "close this PR and use another CLA approved account to do PR again"]}, {"number": 40295, "title": "not tensorflow-GPU \"ImportError: DLL load failed: The specified module could not be found.\" inside Conda / Win10", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- Mobile device : none\r\n- TensorFlow installed from (source or binary): pip install tensorflow from conda terminal\r\n- TensorFlow version: not known\r\n- Python version: 3.7.6\r\n- Installed using : pip install tensorflow from conda terminal\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n*not using tensorflow-gpu* \r\nexecuting:\r\nimport tensorflow\r\n\r\nI get the following error:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"deep_versions.py\", line 5, in <module>\r\n    import tensorflow\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Programs\\Anaconda3\\envs\\MLKeras\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@manuelvalera \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "Hello, thanks for answering,\n\nMy processor is not that old and it does comply with the hardware\nrequirements, make/model is:\n\nIntel i7-670HQ @2.6Ghz, 2592 Mhz, 4 cores, 8 logical processors\nx64-based PC\n\nI will take a look to the previous tickets, but any ideas would be\nappreciated\n\n\nOn Mon, Jun 8, 2020 at 11:54 PM ravikyram <notifications@github.com> wrote:\n\n> @manuelvalera <https://github.com/manuelvalera>\n>\n> What is make/model of your cpu?\n> I suspect your cpu model does not support AVX instructions sets.See hardware\n> requirements\n> <https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements>\n> Make sure to download the latest microsoft visual c++ redistributable\n> from here.\n> <https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads>\n> .Also, please follow the instructions from to install from Tensorflow\n> website <https://www.tensorflow.org/install/source_windows>.\n>\n> Please, check Your CPU/Python is on 32 bits?Please, refer #36167\n> <https://github.com/tensorflow/tensorflow/issues/36167> and see if it\n> helps you.Please, refer similar issue #36167\n> <https://github.com/tensorflow/tensorflow/issues/36167> #36151\n> <https://github.com/tensorflow/tensorflow/issues/36151> #36138\n> <https://github.com/tensorflow/tensorflow/issues/36138> #36054\n> <https://github.com/tensorflow/tensorflow/issues/36054> #36045\n> <https://github.com/tensorflow/tensorflow/issues/36045> #36020\n> <https://github.com/tensorflow/tensorflow/issues/36020> #36003\n> <https://github.com/tensorflow/tensorflow/issues/36003> #35988\n> <https://github.com/tensorflow/tensorflow/issues/35988> #35903\n> <https://github.com/tensorflow/tensorflow/issues/35903> #35880\n> <https://github.com/tensorflow/tensorflow/issues/35880> #35865\n> <https://github.com/tensorflow/tensorflow/issues/35865> #35805\n> <https://github.com/tensorflow/tensorflow/issues/35805> #35789\n> <https://github.com/tensorflow/tensorflow/issues/35789> #35773\n> <https://github.com/tensorflow/tensorflow/issues/35773> #35772\n> <https://github.com/tensorflow/tensorflow/issues/35772> #35767\n> <https://github.com/tensorflow/tensorflow/issues/35767> #35766\n> <https://github.com/tensorflow/tensorflow/issues/35766> #35749\n> <https://github.com/tensorflow/tensorflow/issues/35749> #35721\n> <https://github.com/tensorflow/tensorflow/issues/35721> #35618\n> <https://github.com/tensorflow/tensorflow/issues/35618> #35204\n> <https://github.com/tensorflow/tensorflow/issues/35204>\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40295#issuecomment-641072633>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABSY7YINYFWSL65AL7IHFD3RVXMCZANCNFSM4NY52R5A>\n> .\n>\n", "@manuelvalera \r\n\r\nI suspect It may occur because you may not have [Visual C++ Redistributable for Windows.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)Please, install Installing C++ redistributables and see if it resolve your issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40295\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40295\">No</a>\n"]}, {"number": 40294, "title": "Fix tf.broadcast_to abort issue when given shape value overflows int32", "body": "This PR tries to address the issue raised in #40138 where\r\n`tf.broadcast_to(input, shape)` will cause abort core dump if `shape`\r\noverflows and `input` is `uint32` or `uint64`.\r\n\r\nThe issue is more about xla than `broadcast_to`. Since `BroadcastTo`\r\ndoes not have normal kernel for uint32 or uint64, it will fall back\r\nto XLA. In XLA, the `shape` will result in abort as no value check (>=0).\r\n\r\nThis PR adds shape dim value check for XLA so that an error will be returned\r\ngracefully (than a core dump).\r\n\r\nThis PR fixes #40138.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@joker-eph maybe you can take a look at the PR as it is related to XLA?", "@cheshire Can you please review this PR ? Thanks!", "@gbaned It looks like all tests have passed and change imported. Is there any update or anything else I can help?", "@yongtang Can you please resolve conflicts? Thanks!", "Thanks @gbaned. The PR has been rebased. Please take a look.", "> @gbaned It looks like all tests have passed and change imported. Is there any update or anything else I can help?\r\n\r\nThe problem is that internally your newly added test triggers a different error than the one you expect to see here. So the test fails internally.\r\n\"OP_REQUIRES failed at broadcast_to_op.cc:65 : Resource exhausted: OOM when allocating tensor with shape[1000000000000000000,2] and type uint32 on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\"\r\n\r\nGiven that, I also believe the fix should be done at a different level. You cannot (reliably) detect overflow by checking whether it overflowed to a negative value. It can also overflow to a positive value.\r\nAnd the test you wrote would be used both for XLA and for tensorflow, and it looks in tensorflow it actually has int64 for shape dimensions.", "@yongtang Can you please check @akuegel's comments and keep us posted ? Thanks!", "@yongtang, Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@yongtang, Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 40293, "title": "[TF-TRT] - Bazel Test does not use allow_growth in TF2", "body": "CC: @bixia1 I recently saw numerous of OOM when running python bazel tests:\r\n\r\n```bash\r\nbazel test --verbose_failures --cache_test_results=no --runs_per_test=10 //tensorflow/python/compiler/tensorrt/...\r\n```\r\n\r\nThis tends to generate OOM issues for the TF2 unittests. This PR fixes this. \r\n\r\nIn TF1 the configuration is applied properly, in TF2 mode this is skipped: https://github.com/tensorflow/tensorflow/blob/e5648ef49daa9500ebdebca603f3b5cc42c76a99/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py#L317-L320", "comments": ["Do you know why the \"TF1 way\" doesn't work for TF2? Is it because of [this code](https://github.com/tensorflow/tensorflow/blob/eb8f61f5f41c56b4cb267c549a192e8301eb7d30/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py#L782) for _MakeSavedModelV1 doesn't exist for _MakeSavedModelV2? Prefer to make it work the same way for TF1 and TF2.", "@bixia1 Your last comment is correct. This PR should fix that\r\nLet me know if you need anything else for this PR. I don't really see what to add", "Is there a way to fix it in _RunGraphV2 as _RunGraphV1 takes care of similar things, and RunTest is common  for V1 and V2.", "Not really sure ... TF1 path uses sessions (that takes a session config), not TF2 path. So this approach sounds safer to me", "Would simplify moving the code to inside _RunGraphV2 be a good solution?", "I'm not sure it could cover all paths. \r\nFor example this one would not be covered: https://github.com/tensorflow/tensorflow/blob/e5648ef49daa9500ebdebca603f3b5cc42c76a99/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py#L796-L798", "That code generates a saved model for V1. Your current fix doesn't cover it either. Right?", "Yeap correct. But again we miss the same `config` applied for the V2 path.\r\n\r\nIf I understand things correctly, my fix (in its current state) fix is applied at the beginning of the test execution and is therefore valid for the whole test execution scope. So it covers RunGraph and _MakeSavedModel. Hence I believe it is the best place to put this piece of code", "Each RunTest will call RunGraph twice. Are you concern that setting it twice at runtime through _RunGraphV2 is less efficient than setting it once in RunTest? I prefer to set it inside _RunGraphV2 to match the behavior of _RunGraphV1 and avoid special handling in RunTest. But if you insist, you may add it to RunTest with a comment to explain why this is only done for V2, to help readability. ", "@DEKHTIARJonathan Can you please check @bixia1's comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@DEKHTIARJonathan Any update on this PR? Please. Thanks!", "@DEKHTIARJonathan Any update on this PR? Please. Thanks!", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 40292, "title": "Update nasnet.py, Add `classifier_activation` arg", "body": "Add `classifier_activation` Arg to NASNetLarge, NASNetMobile classes\r\nUpdate docstring for the same\r\n\r\nThis PR will allow using the above classes as\r\n\r\n```python\r\ntf.keras.applications.NASNetLarge(\r\n    input_shape=None, include_top=True, weights='imagenet', input_tensor=None,\r\n    pooling=None, classes=1000, classifier_activation='softmax'\r\n)\r\n\r\ntf.keras.applications.NASNetMobile(\r\n    input_shape=None, include_top=True, weights='imagenet', input_tensor=None,\r\n    pooling=None, classes=1000, classifier_activation='softmax'\r\n)\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40292) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40292) for more info**.\n\n<!-- ok -->", "Thanks for the PR. The same feature has already been added as part of a separate change."]}, {"number": 40290, "title": "xrange() was removed from Python on 1/1/2020", "body": "Like #40241", "comments": []}, {"number": 40289, "title": "Keras: Allow setting constant initializers using Python literals", "body": "This PR adds support for setting Keras constant initializers using Python literals.\r\n\r\n## Motivation\r\nWhen implementing custom layers that use some variant of scaling factors that should be initialized with a non-zero and non-unit constants I often find myself needing to write some avoidable boilerplate since using a mutable `tf.keras.initializers.Constant` as a [default argument can lead to unexpected behaviour in Python](https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments).\r\n\r\nFor implementing such a custom layer I would usually do something like\r\n```python\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n    def __init__(self, scale_initializer=None, **kwargs):\r\n        if scale_initializer is None:\r\n            self.scale_initializer = tf.keras.initializers.Constant(0.5)\r\n        else:\r\n            self.scale_initializer = tf.keras.initializers.get(scale_initializer)\r\n        super().__init__(**kwargs)\r\n```\r\nwhich in my opinion is not optimial because the default value won't be obvious from the signature unless documented explicitely.\r\n\r\nThe second option would be to pass an initial value directly which breaks with the Keras naming convention of `{variable_name}_initializer`.\r\n```python\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n    def __init__(self, scale_initial_value=0.5, **kwargs):\r\n        self.scale_initializer = tf.keras.initializers.Constant(scale_initial_value)\r\n        super().__init__(**kwargs)\r\n```\r\n\r\nThis PR allows to set constant initializers directly via Python literals without needing to overwrite `tf.keras.initializers.get`:\r\n```python\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n    def __init__(self, scale_initializer=0.5, **kwargs):\r\n        self.scale_initializer = tf.keras.initializers.get(scale_initializer)\r\n        super().__init__(**kwargs)\r\n```\r\nFurthermore this simplifies the use of `Layer.add_weight` when initializing it with plain Python constants that might be computed from the `input_shape` inside `Layer.build`.", "comments": []}, {"number": 40288, "title": " metaclass conflict, ops.py, tensorflow.backend.py", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\nHello people, firstly...im new the last beginner and maybe the mess is big so...\r\nSome days ago i trained YOLOv3 (TrainYourOwnYOLO) everything was fine, however, when i've tried to test the algorithm the things went wrong. Issue after issue. i was looking in forums and after one issue came another one.  The last change was here:\r\n\r\nops.py\r\ncore_tf_types.Tensor = tuple()\r\n# Deprecated - do not use.\r\n# This API to avoid breaking estimator and tensorflow-mesh which depend on this\r\n# internal API. The stub should be safe to use after TF 2.3 is released.\r\ndef is_dense_tensor_like(t):\r\n  return isinstance(t, core_tf_types.Tensor)\r\n\r\ntensorflow_backend.py\r\nfrom tensorflow.python.types import core as core_tf_types \r\n\r\ndef is_tensor(x):\r\n    return isinstance(x, tf_ops.core_tf_types) or tf_ops.is_dense_tensor_like(x)\r\n\r\nWhen i try to run the detector -> Detector.py\r\nError is:\r\nTypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases\r\n\r\nCould give me an advice what to do?!?!", "comments": ["@Kottarashky,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40288\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40288\">No</a>\n"]}, {"number": 40287, "title": "Floating point exception occurred in tf.nn.fractional_max_pool", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nFloating point exception (arithmetic exception) occurs in c++ while executing `tf.nn.fractional_max_pool` when passing the first element of `pooling_ratio` > 2.0`. \r\n\r\n**Describe the expected behavior**\r\nAccording to the document (https://www.tensorflow.org/api_docs/python/tf/nn/fractional_max_pool), the first and the last elements of `pooling_ratio` must be 1.0. However, in some cases,  even though these elements are not 1.0 no exception occurs.  I think the way `fractional_max_pool` handles invalid inputs should be consistent at least. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntensor_4D = [[[[0, 1, 1], \r\n                      [2, 3, 3],\r\n                      [1, 3, 2]],\r\n                     [[1, 3, 2],\r\n                      [2, 4, 2],\r\n                      [0, 1, 1]]],\r\n                      [[[0, 3, 1], \r\n                      [2, 4, 1],\r\n                      [1, 3, 2]],\r\n                     [[1, 1, 1],\r\n                      [2, 3, 4],\r\n                      [1, 3, 2]]],\r\n                     [[[2, 2, 4], \r\n                      [2, 1, 3],\r\n                      [0, 4, 2]],\r\n                     [[2, 4, 1],\r\n                      [2, 3, 0],\r\n                      [1, 3, 3]]]]\r\npooling_ratio = [2.1]\r\n\r\ntf.nn.fractional_max_pool(tensor_4D, pooling_ratio)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@mjkim720 I cannot reproduce the issue. When I ran your code is colab, session was crashing. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/ba918ec54c38814664568e13027f3dc0/40287.ipynb). Thanks!\r\n\r\nCan you please a standalone code to reproduce the issue? Thanks!", "@jvishnuvardhan The attached code above is reproducible. It is expected the session crashes in Colab because the floating point exception occurs in C++, not in Python, causing it to core dump. ", "@mjkim720 I tested your code with `tf-nightly` and it throws a `ValueError ` as expected and the error trace is shown below. I don't see any session crashing as noticed earlier. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/539a582018d408de4a81da7d1308cea0/40287.ipynb).\r\n\r\nThe following is the error trace \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-335f3373d27b> in <module>()\r\n     21 pooling_ratio = [2.1]\r\n     22 \r\n---> 23 tf.nn.fractional_max_pool(tensor_4D, pooling_ratio)\r\n\r\n1 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py in fractional_max_pool_v2(value, pooling_ratio, pseudo_random, overlapping, seed, name)\r\n   5614     if (pooling_ratio[0] != 1.0 or pooling_ratio[-1] != 1.0):\r\n   5615       raise ValueError(\r\n-> 5616           \"The first and last elements of pooling ratio must be 1.0.\")\r\n   5617     for element in pooling_ratio:\r\n   5618       if element < 1.0:\r\n\r\nValueError: The first and last elements of pooling ratio must be 1.0.\r\n```\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40287\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40287\">No</a>\n"]}, {"number": 40286, "title": "List tf.function methods in tf.Module", "body": "Hi all!\r\n\r\nI use the dir(cls) build-in method to list member of a tf.Module but tf.function methods do not appear there. Is there a method that allow listing of tf.function in tf.Module?\r\n\r\nThanks\r\n", "comments": ["Can you provide some reproducible code here that demonstrates the issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40285, "title": "Support for .next() on tf.data.Dataset", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): **Yes** (If approved)\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently when working with https://www.tensorflow.org/api_docs/python/tf/data/Dataset\r\nYou can iterate a Dataset with a loop e.g. \r\n```python\r\nfor x,y in train:\r\n   tf.print(x,y)\r\n```\r\n\r\nHowever if you want to get a single batch, the only option would be:\r\n\r\n```python\r\ntrain.__iter__().next()\r\n```\r\n\r\nCurrently there is only support for creating a numpy iterator, Example\r\n```python\r\ntrain.as_numpy_iterator().next()\r\n```\r\n**and the output is not a tf object.** \r\n\r\nThe closest to the expected behavior of `.next()` would be `train.take(1)` which still needs to be converted to an iterator. \r\n\r\nThe current methods of getting a single batch are tedious and create bug prone code, multiple lines, and/or functions called. \r\n\r\n**Will this change the current api? How?**\r\n\r\nIt will create a .next() function for tf.data.Dataset e.g. \r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset#next\r\n\r\n**Who will benefit with this feature?**\r\nHopefully the entire community \r\n\r\n**Any Other info.**\r\n\r\nThe feature could be implemented by using an existing iterator for the dataset object and when calling .next() simply picking an element from that iterator rather than creating an iterator every time for performance benefits. \r\n", "comments": ["> However if you want to get a single batch, the only option would be `train.__iter__().next()`\r\n\r\nI think you should be able to call `next(iter(train))` similar to normal Python 3 `next(iter(range(3)))`.", "@lgeiger Thanks! I was not aware of that, however maybe it is tedious? in the use case of custom iteration over the data, wouldn't it make more sense for the iterator to be part of the dataset class/object. Such that the state of the iteration is saved and compatible with other methods such as take, rather than creating a new iterator every time?", "@lgeiger suggestion is idiomatic use of `tf.data` APIs.\r\n\r\n`tf.data.Dataset.next()` is problematic. What should happen when you call `next()` multiple times? I assume you would like different elements to be produced. But in that case `tf.data` would need to maintain internal iterator object to provide this functionality. If that's the case, how is the lifetime of this iterator object managed? When you create iterator explicitly via `iter`, the lifetime is tied to the lifetime of Python object which makes it possible to decide when the iterator object is no longer needed and can be destroyed. In contrast, if the iterator object was hidden inside of tf.data internals, then you would need an API for explicitly destroying it (since the iterator objects may be allocating a large amount of memory). There is tens of bugs opened for Keras memory leaks because of exactly this -- Keras backend session allocating memory that needs to be explicitly cleared -- and we will not repeat that mistake for tf.data."]}, {"number": 40284, "title": "[ROCm] Fix for ROCm CSB breakage - 200608", "body": "The test `//tensorflow/python/ops/numpy_ops:np_math_ops_test_gpu` started failing on Friday (200605), with the following error\r\n\r\n```\r\n=====================================================================\r\nFAIL: testExp (__main__.MathTest)\r\ntestExp (__main__.MathTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n...\r\n...\r\nAssertionError:\r\nArrays are not almost equal to 7 decimals\r\n\r\nMismatched elements: 1 / 2 (50%)\r\nMax absolute difference: 4.76837158e-07\r\nMax relative difference: 6.45328909e-08\r\n x: array([ 7.3890557, 20.085537 ])\r\n y: array([ 7.3890562, 20.085537 ])\r\n\r\n----------------------------------------------------------------------\r\nRan 19 tests in 24.663s\r\n\r\nFAILED (failures=1, skipped=1)\r\n================================================================================\r\n\r\n```\r\n\r\nThis is a new unit test introduced by the commit 84c796966b0b75a3561d4b076a8388f2091ff57d\r\n\r\nThe fix is to drop the relative tolerance down to 1e-6 (from the default of 1e-7). This should be okay since numerous other TF unit tests also use the same value for relative tolerance.\r\n\r\nNote that this commit also changes the check call from `assert_almost_equal` to `assert_allclose`, to be able to specifyt eh relative tolerance\r\n\r\n\r\n\r\n-------------------------------------\r\n\r\ncc @whchung @chsigg @cheshire @nvining-work ", "comments": []}, {"number": 40283, "title": "Annoying Documentation for Tensorflow 2.2.0", "body": "I saw the implementations of tf.reduce_mean, tf.reduce_sum but its not there in the documentation for tf 2.2.0. It is really annoying as it created a giant confusion in our code. Please keep documentation and implmentation in sync. ", "comments": ["Hi @y12uc231 \r\n[tf.reduce_mean](https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean) and [tf.reduce_sum ](https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum)are both included in the documentation. Can you please link to the implementations you are referring to, and provide more information on the discrepancy you are seeing between documentation and implementation? Thank you.\r\n", "Hi @nikitamaia  : If you check the links you posted, they are tf.math.reduce_sum not tf.reduce_sum. ", "If you click on the \"View aliases\" button at the top of the docs, you will see that tf.reduce_sum is an alias for tf.math.reduce_sum, so you can use either.", "Thanks! \r\nQuick feedback : Based on my personal experience, documentation of tensorflow is incredibly complicated compared any other frameworks I use like pytorch or mxnet. ", "Thanks for the feedback, @y12uc231. If you run into any confusion, or find something in the docs you think would benefit from some clarity, please do file an issue."]}, {"number": 40282, "title": "[TFLite 16x8] Documentation for an experimental release", "body": "This PR is a follow-up for this PR: https://github.com/tensorflow/tensorflow/pull/36251\r\n\r\nI added a section in the documentation on the new experimental option for TFLite converter for the quantization scheme:\r\nfull integer, activations - int16, weights - int8, bias - int64.", "comments": ["This is now part of the PR: https://github.com/tensorflow/tensorflow/pull/36251"]}, {"number": 40281, "title": "TF-Lite Micro: port Silicon Labs STK3701A to Micro Speech example", "body": "This ports the Silicon Labs STK3701A development board to the TF-Lite Micro Micro Speech example", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40281) for more info**.\n\n<!-- need_sender_cla -->", "@sldriedler Thank you for your contribution. Can you please sign CLA? Thanks!"]}]