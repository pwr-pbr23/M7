[{"number": 38672, "title": "How properly apply a tokenizer map function to a Tensorflow batched dataset?", "body": "Considering the following `batched_dataset`:\r\n\r\n```python3\r\nsamples =  ([{\"query\": \"this is a query 1\", \"doc\": \"this is one relevant document regarding query 1\"}, \r\n              {\"query\": \"this is a query 2\", \"doc\": \"this is one relevant document regarding query 2\"},\r\n              {\"query\": \"this is a query 3\", \"doc\": \"this is one relevant document regarding query 3\"},\r\n              {\"query\": \"this is a query 4\", \"doc\": \"this is one relevant document regarding query 4\"},\r\n              ])\r\ndataset = tf.data.Dataset.from_generator( \r\n    lambda: samples, {\"query\": tf.string, \"doc\": tf.string})\r\n\r\nbatched_dataset = dataset.batch(2)\r\n\r\n#{\r\n#'doc': <tf.Tensor: shape=(2,), dtype=string, numpy=array(\r\n#     [b'this is one relevant document regarding query 1',\r\n#      b'this is one relevant document regarding query 2'], dtype=object)>,\r\n# \r\n#'query': <tf.Tensor: shape=(2,), dtype=string, numpy=array(\r\n#     [b'this is a query 1', \r\n#      b'this is a query 2'], dtype=object)>\r\n#}\r\n\r\n```\r\nand a map function to tokenize this `batched_dataset`:\r\n\r\n```python3\r\ndef tokenize(sample):\r\n    tokenized_query = tokenizer.batch_encode_plus(sample[\"query\"].numpy().astype('str'), ...)\r\n    tokenized_doc = tokenizer.batch_encode_plus(sample[\"doc\"].numpy().astype('str'), ...)\r\n    return (tokenized_query, tokenized_doc) \r\n```\r\nI could tokenize the entire batched_dataset using a for-loop:\r\n\r\n```python3\r\nfor batch in batched_dataset:\r\n    tokenize(batch)\r\n# (\r\n# {'input_ids': <tf.Tensor: shape=(2, 8), dtype=int32, numpy=\r\n#   array([[  101,  2023,  2003,  1037, 23032,  1015,   102,     0],\r\n#          [  101,  2023,  2003,  1037, 23032,  1016,   102,     0]],\r\n#      dtype=int32)>, \r\n#  'attention_mask': <tf.Tensor: shape=(2, 8), dtype=int32, numpy=\r\n#   array([[1, 1, 1, 1, 1, 1, 1, 0],\r\n#          [1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}, \r\n\r\n# {'input_ids': <tf.Tensor: shape=(2, 8), #dtype=int32, numpy=\r\n#   array([[ 101, 2023, 2003, 2028, 7882, 6254, 4953,  102],\r\n#          [ 101, 2023, 2003, 2028, 7882, 6254, 4953,  102]], dtype=int32)>, \r\n#  'attention_mask': <tf.Tensor: shape=(2, 8), dtype=int32, numpy=\r\n#   array([[1, 1, 1, 1, 1, 1, 1, 1],\r\n#          [1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>})\r\n#  ...\r\n```\r\n \r\nHowever, when using [`tf.data.Dataset.map`][1] the following error arises:\r\n```python3\r\ntokenized_dataset = batched_dataset.map(tokenize)\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n\r\nThen, how properly apply a tokenizer map function to a batched dataset?\r\n\r\n**Note**: I published a working example on [`Google Colab`][2].\r\n\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#map\r\n  [2]: https://colab.research.google.com/drive/1TUbWwEgbgPHwY1QjgRLIqLpjin310pdh", "comments": ["I have tried on colab with TF version 2.2.0-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/e8f7ac6644b4cf76e88410e3ce5a7856/untitled788.ipynb).Thanks!", "@Ceceu Please take a look at similar issue [here](https://github.com/tensorflow/tensorflow/issues/36979) and [here](https://github.com/tensorflow/tensorflow/issues/30035) and let me know if it helps. Thanks!", "I think [tf.numpy_function](https://www.tensorflow.org/api_docs/python/tf/numpy_function?version=nightly) works for you.", "@Ceceu Can you please respond to the above comment to take this discussion further. Thanks!", "Hi everyone, see a related issue where this doesn't work on TPU here https://github.com/huggingface/transformers/issues/5066", "@gowthamkpr,\r\n\r\nCurrently I ended up refactoring my code to [torchtext](https://pytorch.org/text/examples.html) because it is much simpler to create an efficient data pipeline when you are using text datasets.\r\n\r\nUntil the Tensorflow v2.3, [tf.py_fuction](https://www.tensorflow.org/api_docs/python/tf/py_function) wrapper did not yet support dictionaries.", "@Ceceu  Can you o ahead and close the issue as it has been resolved. Thanks!"]}, {"number": 38671, "title": "[INTEL MKL] Upgrading DNNL to v1.4", "body": "DNNL  is the new name for MKL DNN.", "comments": []}, {"number": 38669, "title": "Fixed _save_model not working for batches in ModelCheckpoint Callback", "body": "Fixes #38668 .  \r\n@mihaimaruseac - Please review.", "comments": ["@mihaimaruseac , sorry. Had two spaces after comma instead of one. Pylint was failing. Please, can you approve it again?", "@mihaimaruseac , resolved merge conflict. Please approve.", "Let's wait for TF API owners to review the change in behavior", "(for tensorflow/api-owners)\r\n\r\nDeferring to @rchao to review the behavior change.", "@rchao and @mihaimaruseac , I have modified code as per the last review. Please can you review it?", "@rchao , I have tested pylint locally. Errors are resolved now.", "@rchao is api review done for this change ?", "Seems this failed some tests internally and failed to land. Testing again and will update.", "`//tensorflow/python/keras:callbacks_test` is failing\r\n\r\n```\r\n...\r\nFile \"tensorflow/python/keras/callbacks.py\", line 1347, in _get_file_path\r\n    epoch=epoch + 1, batch=batch + 1, **logs)\r\n```", "@mihaimaruseac , Please paste error stack here.", "Apologies, last line of stack trace (with the error) didn't copy above. Here's the full trace\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<embedded stdlib>/unittest/case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"<embedded stdlib>/unittest/case.py\", line 605, in run\r\n    testMethod()\r\n  File \".../absl/testing/parameterized.py\", line 284, in bound_param_test\r\n    return test_method(self, *testcase_params)\r\n  File \".../tensorflow/python/keras/keras_parameterized.py\", line 280, in decorated\r\n    _test_sequential_model_type(f, self, *args, **kwargs)\r\n  File \".../tensorflow/python/keras/keras_parameterized.py\", line 300, in _test_sequential_model_type\r\n    f(test_or_class, *args, **kwargs)\r\n  File \".../tensorflow/python/keras/callbacks_test.py\", line 633, in test_ModelCheckpoint\r\n    verbose=1)\r\n  File \".../tensorflow/python/keras/engine/training_v1.py\", line 807, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \".../tensorflow/python/keras/engine/training_arrays.py\", line 666, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \".../tensorflow/python/keras/engine/training_arrays.py\", line 397, in model_iteration\r\n    callbacks._call_batch_hook(mode, 'end', batch_index, batch_logs)\r\n  File \".../tensorflow/python/keras/callbacks.py\", line 284, in _call_batch_hook\r\n    self._call_batch_end_hook(mode, batch, logs)\r\n  File \".../tensorflow/python/keras/callbacks.py\", line 304, in _call_batch_end_hook\r\n    self._call_batch_hook_helper(hook_name, batch, logs)\r\n  File \".../tensorflow/python/keras/callbacks.py\", line 337, in _call_batch_hook_helper\r\n    hook(batch, logs)\r\n  File \".../tensorflow/python/keras/callbacks.py\", line 1243, in on_train_batch_end\r\n    self._save_model(epoch=self._current_epoch, batch=batch, logs=logs)\r\n  File \".../tensorflow/python/keras/callbacks.py\", line 1297, in _save_model\r\n    filepath = self._get_file_path(epoch, batch, logs)\r\n  File \".../tensorflow/python/keras/callbacks.py\", line 1347, in _get_file_path\r\n    epoch=epoch + 1, batch=batch + 1, **logs)\r\nTypeError: format() got multiple values for keyword argument 'batch'\r\n```", "For @tensorflow/api-owners: no need for API review.", "@ashutosh1919 Can you please resolve conflicts? Thanks!", "Ping on resolving conflicts and test failure?", "@ashutosh1919 gentle ping", "@mihaimaruseac and @rchao - Sorry for the delay. I was very much impacted by COVID-19 and had to stop contributions to tf due to it.\r\n\r\nI have resolved merge conflicts. Please run the tests.", "We are still seeing the `\r\nTypeError: format() got multiple values for keyword argument 'batch'` error mentioned before", "@ashutosh1919, @mihaimaruseac Any update on this PR? Please. Thanks!", "@ashutosh1919, @mihaimaruseac Any update on this PR? Please. Thanks!", "@ashutosh1919, @mihaimaruseac Any update on this PR? Please. Thanks!", "@ashutosh1919 Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 29 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 38668, "title": "In ModelCheckpoint, filepath is not accepting batch as formatting parameter.", "body": "In `ModelCheckpoint` callback, there is a parameter given named `save_freq` to save model. If `save_freq` is set to `epoch`, it will save model at the end of every epoch. (This works perfectly fine). But when `save_freq` is set to an integer let's say `N`, then the callback should save the model after `N` batches in every epoch. But the problem here is the callback doesn't accept the filepath as `file.batch{batch:02d}epoch{epoch:02d}.h5` and raises error as `batch` is invalid key.  \r\nThe problem in the code that I have noticed is that the `_save_model` function has access to `epoch` but it doesn't have access to `batch`. And that's why `_get_file_path()` has access to `epoch` but not `batch`.  The functionality should be changed little bit. I am raising PR to add access to `batch` param in both `_save_model` and `_get_file_path` variable.\r\nI noticed this error in tf code during the work on my PR [#1702](https://github.com/tensorflow/addons/pull/1702) in tensorflow/addons.  \r\n\r\ncc @gabrieldemarmiesse.\r\n", "comments": ["@ashutosh1919 would be willing to send a PR for this issue ?", "same problem here. It should support batch number formatting, especially when save_freq is not epoch but batch, only have epoch number would make previous weights files of same epoch been override.", "Same issue here. Is there actually no support for this yet?", "@aningineer , I have raised PR #49376 to fix this issue and you can see that it is approved as well. Let it get merged and you will be able to use this fix in `tf-nightly`", "The PR that fix this issue is merged to keras-team/keras as keras-team/keras@c567184. Closing this now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38668\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38668\">No</a>\n"]}, {"number": 38667, "title": "xla_hlo.compare support dynamic shape in materialize broadcast ", "body": "compare op support dynamic shape in materialize broadcast pass\r\nsupplement relevant case ", "comments": ["@sherhut please take a look again  ^ _ ^\r\nA further question, what's your plan for integrate this materialize_broadcast into  hlo transformation(now it is a optional ) because I think this pass provide the capacity to support broadcast in some dynamic shape scenarios. \r\nI can collaborate more  if I know the plan you have  ^_^\r\n", "Hi OuYang (@qqsun8819), I'm doing work on this right now. There are two versions of this pass floating around between IREE and XLA and neither is complete enough for dynamic shapes beyond being a POC -- but I think we've learned what we need from them and are trying to do them properly now.\r\n\r\nThe current plan is to do the following:\r\n\r\n* Define broadcasting versions of the various BinaryOp()s in the new chlo dialect (client-HLO, mirroring the API of the XlaBuilder).\r\n* Migrate the conversion patterns from this old MaterializeBroadcasts pass into an overall set of conversion patterns to lower from chlo -> hlo. This will involve materializing the broadcasts and relevant checks/asserts (which are omitted now) as well as some corner cases that the current patterns do not implement correctly.\r\n* Change the binary ops in xla_hlo to not accept broadcast_dims nor allow degenerate broadcasting: they will require either static shapes that are equal or dynamic shapes that have been checked by the caller/producer.\r\n* Implement missing ops/patterns in the shape dialect, and do a lot of grunge work on folders, etc to make it all work.\r\n* There are also some issues we need to work out so that such lowerings do not violate the invariants of LinAlg ops, which they currently can do.\r\n\r\nOnce all of that is done, we'll enable the new CHLO->HLO conversion patterns (which will include refactored versions of what is here) in the standard flow. We've got this minimally working in a prototype, so are reasonably confident that it is just work to get it landed.\r\n\r\nI'm totally fine going forward with this PR. It'll just be some copy-paste in the next step as we move things around. My changes had missed the implicit broadcast on the compare op -- thanks for surfacing!\r\n\r\nHere is the first change in my series (just landed): https://github.com/tensorflow/tensorflow/commit/0ede71e954c5c3bd5ee3e89ffb816d39ab796648", "@sherhut  thanks for your review! Please take a look again ^_^ I've updated the pr according to your comments", "@sherhut  format fixed"]}, {"number": 38666, "title": "Fix a bug related to build TF Lite on RPI Zero.", "body": "Why:\r\n\r\n* Enable to build TF Lite on RPI Zero.\r\n\r\nThis change addresses the need by:\r\n\r\n* Changing compiler from arm-linux-gnueabi- to arm-linux-gnueabihf-.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38666) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38666) for more info**.\n\n<!-- ok -->", "Building TF Lite on RPI Zero was successful with arm-linux-gnueabi- prefix to me.\r\nDid you install crossbuild-essential-armel toolchain as guided at https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_raspberry_pi ?\r\n\r\nIf arm-linux-gnueabi- is not successful even with crossbuild-essential-armel package installed, could you give me the details on your host environment?", "Hi @hajuho,\r\n\r\nI was building the package on the target device (Rasbian buster) and not cross-compiling on the host device.\r\n\r\nBest Regards\r\n\r\nMarcin Sielski", "Hi Terry,\r\nCould you take a look at this?\r\nLook like arm-linux-gnueabi- is not working for native compiling on RPI zero. Is there any problem using arm-linux-gnueabihf- ?", "Hi,\r\n\r\narm-linux-gnueabihf- cross compiler available on Ubuntu will not work because it is tight to armv7. The official cross-compiler for RPI Zero is available at https://github.com/raspberrypi/tools/tree/master/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf.\r\n\r\nHope it helps.\r\n\r\nBest Regards", "Raspberry pi zero w (armv6) user here. Running raspbian stretch 9.11 and arm-linux-gnueabi- was not preinstalled but arm-linux-gnueabihf- was. So switching to it worked.", "The code was added by https://github.com/tensorflow/tensorflow/issues/36445#issuecomment-585738062 and it's needed for cross compiling.\r\nCould you update your PR to select TARGET_TOOLCHAIN_PREFIX  by checking $(HOST_ARCH) to figure out if it's cross build or native build?\r\n\r\n\r\n", "If you're using native build, could you try the following?\r\n\r\n```bash\r\nmake -f tensorflow/lite/tools/make/Makefile\r\n```\r\n\r\nWe don't need to use build script for the native build.\r\n                                                                                \r\n", "@terryheo \r\nCross compiling with arm-linux-gnueabi- (armel) will work but is not efficient. The binaries that are being produced are incompatible with Rasbian binaries. The difference is in support of hard float (supported by FPU - faster) vs. soft float (emulated on CPU - slower).", "@terryheo  Any update on this PR? Thanks!", "@marcin-sielski could you confirm if this patch works for cross compiling?\r\nAlso https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_rpi.md should be updated.\r\n", "@terryheo ,\r\nThis patch should work with official Raspberry Pi cross compiler. I will double confirm that. I agree that the instruction should be updated along with this change. I was not aware that the instruction is also maintained in this repository.", "@marcin-sielski, cool. Could you update PR with the doc change?", "@terryheo \r\nI have updated this PR with instruction updates.\r\n\r\nBest Regards\r\n\r\nMarcin Sielski", "@terryheo \r\n\r\nI think I have fixed all the issues"]}, {"number": 38665, "title": "deviceMemoryBandwidth: -1B/s with tensorflow-rocm", "body": "**Describe the problem**\r\nI have a fresh install of Ubuntu (18.04.4) and just followed  [this instruction](https://www.videogames.ai/Install-ROCM-Machine-Learning-AMD-GPU) to install rocm and tensorflow. After importing tensorflow and executing \"tf.config.list_physical_devices('GPU')\", I get this output:\r\n```\r\n2020-04-18 10:17:47.965008: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libhip_hcc.so\r\n2020-04-18 10:17:48.016723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1573] Found device 0 with properties: \r\npciBusID: 0000:29:00.0 name: Ellesmere [Radeon RX 470/480/570/570X/580/580X]     ROCm AMD GPU ISA: gfx803\r\ncoreClock: 1.35GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: -1B/s\r\n2020-04-18 10:17:48.051748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library librocblas.so\r\n2020-04-18 10:17:48.053144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libMIOpen.so\r\n2020-04-18 10:17:48.055482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library librocfft.so\r\n2020-04-18 10:17:48.055669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library librocrand.so\r\n2020-04-18 10:17:48.055769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n```\r\nThe problem is that the deviceMemoryBandwidth seems to not be found and set to -1B/s. This results in tensorflow only allocating memory, but not using the GPU.\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04.4\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.1.1\r\n- Python version: 3.6.9\r\n- GPU model and memory: AMD RX 580 8GB ", "comments": []}, {"number": 38664, "title": "WARNING: 3 cannot be handled by this delegate. Only the first 0 ops will run on the GPU, and the remaining 17 on the CPU.", "body": "\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\r\n    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:Iphone XR\r\n    TensorFlow installed from (source or binary):binary\r\n    TensorFlow version: 1.14 (gpu)\r\n    Python version:3.6\r\n    Installed using virtualenv? pip? conda?:pip\r\n    Bazel version (if compiling from source):\r\n    GCC/Compiler version (if compiling from source):\r\n    CUDA/cuDNN version: 10\r\n    GPU model and memory: 1070Ti - 8GB\r\n\r\nI have converted .h5 to .tflite. It's working perfectly with CPU but not with GPU. \r\nIt's showing following error:\r\n'WARNING: 3 cannot be handled by this delegate. Only the first 0 ops will run on the GPU, and the remaining 17 on the CPU.'\r\nHere is my model: \r\nhttps://drive.google.com/file/d/1osGt-pr9hh9qvslrKY2TaxOLkwWsEA-2/view?usp=sharing", "comments": ["@nazmulasha \r\n\r\nRequest you to share simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\nI have changed the tensorflow version and now it's fine. But I got a new error now. I have used PRelu and it's giving me the following error.\r\n'Dimensions are not BHWC'.", "@nazmulasha Can you please give more details on the error or share full trace of the error.  I looked into the the tflite model that you shared, everything looks fine to me.\r\n1) Are you testing any classification model?\r\n2) What were the steps you followed before noticing this error?\r\n3) Can you share labels.txt, so that I will try to check with an android device?\r\n\r\nPlease give as many details as possible to find root-cause of the issue. Thanks!\r\n\r\nAnother note, please update the title of this issue so that it represents actual problem you are facing. Thanks!", "@jvishnuvardhan I have already fixed it.", "Closing this as this was resolved already. Thanks!"]}, {"number": 38663, "title": "Fix TypeError while importing tensorflow caused by site.USER_SITE is set to None", "body": "\r\nThis PR tries to address the issue raised in #34643 where the following may cause\r\nimport error:\r\n```\r\n$ python3 -S\r\n>>> import sys\r\n>>> sys.path=['', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages']\r\n>>> import tensorflow\r\n2020-04-18 02:53:44.825113: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-04-18 02:53:44.825163: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-04-18 02:53:44.825191: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (7d29dc61d5ee): /proc/driver/nvidia/version does not exist\r\n2020-04-18 02:53:44.825441: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-18 02:53:44.849778: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199995000 Hz\r\n2020-04-18 02:53:44.850142: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc308000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-18 02:53:44.850178: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 417, in <module>\r\n    if _running_from_pip_package():\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 415, in _running_from_pip_package\r\n    _current_file_location.startswith(dir_) for dir_ in _site_packages_dirs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 415, in <genexpr>\r\n    _current_file_location.startswith(dir_) for dir_ in _site_packages_dirs)\r\nTypeError: startswith first arg must be str or a tuple of str, not NoneType\r\n```\r\n\r\nThe reason was that `site.USER_SITE` could be None.\r\n\r\nThis PR removes None with:\r\n```\r\n_site_packages_dirs += [] if _site.USER_SITE is None else [_site.USER_SITE]\r\n```\r\n\r\nThis PR fixes #34643\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 38662, "title": "TFLiteConverter ConverterError with tf.float16 layers", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- TensorFlow installed from (source or binary): colab\r\n- TensorFlow version (or github SHA if from source): 2.2.0-rc3\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\nColab:\r\nhttps://colab.research.google.com/drive/1yLEzIz2O1opjQDHUTaZJGRSgg8DKHfeW\r\n\r\nTo get Error1:\r\n```\r\nimport tensorflow as tf\r\n\r\ninput_layer = tf.keras.layers.Input(shape=(224,224,3), dtype=tf.float16)\r\nfinal = tf.keras.layers.Dense(10, dtype=tf.float32)(input_layer)\r\nmodel = tf.keras.models.Model(input_layer, final)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nquantized_tflite_model = converter.convert()\r\n```\r\n\r\nTo get Error2:\r\n```\r\nimport tensorflow as tf\r\n\r\ninput_layer = tf.keras.layers.Input(shape=(224,224,3))\r\nfinal = tf.keras.layers.Dense(10, dtype=tf.float16)(input_layer)\r\nmodel = tf.keras.models.Model(input_layer, final)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nquantized_tflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\nError1:\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-3-cfb1369fd448> in <module>()\r\n      2 converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n      3 \r\n----> 4 quantized_tflite_model = converter.convert()\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    225       stdout = _try_convert_to_unicode(stdout)\r\n    226       stderr = _try_convert_to_unicode(stderr)\r\n--> 227       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    228   finally:\r\n    229     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: Use '' if want to use the type from graph.\r\n\r\n```\r\n\r\nError2:\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-5-cfb1369fd448> in <module>()\r\n      2 converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n      3 \r\n----> 4 quantized_tflite_model = converter.convert()\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    225       stdout = _try_convert_to_unicode(stdout)\r\n    226       stderr = _try_convert_to_unicode(stderr)\r\n--> 227       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    228   finally:\r\n    229     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2020-04-18 02:25:05.438485: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:89] Ignored output_format.\r\n2020-04-18 02:25:05.438531: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:95] Ignored drop_control_dependency.\r\n2020-04-18 02:25:05.460283: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\nloc(callsite(\"model_1/dense_1/Cast\"(\"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\":865:0) at callsite(\"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\":959:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\":435:0 at callsite(\"<ipython-input-5-cfb1369fd448>\":1:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\":2882:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\":2822:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\":2718:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\":537:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\":208:0 at \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\":399:0)))))))))): error: 'tfl.cast' op result #0 must be tensor of 32-bit float or 1-bit integer or 32-bit integer or 64-bit integer values, but got 'tensor<1x224x224x3xf16>'\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:865:9: error: 'tfl.cast' op result #0 must be tensor of 32-bit float or 1-bit integer or 32-bit integer or 64-bit integer values, but got 'tensor<1x224x224x3xf16>'\r\n        self._initialize(args, kwargs, add_initializers_to=initializers)\r\n        ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:959:5: note: called from\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n    ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py:435:5: note: called from\r\n    concrete_func = func.get_concrete_function()\r\n    ^\r\n<ipython-input-5-cfb1369fd448>: note: called from\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882:17: note: called from\r\n                exec(code_obj, self.user_global_ns, self.user_ns)\r\n                ^\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822:17: note: called from\r\n                if self.run_code(code, result):\r\n                ^\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718:20: note: called from\r\n                   interactivity=interactivity, compiler=compiler, result=result)\r\n                   ^\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py:537:9: note: called from\r\n        return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n        ^\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py:208:13: note: called from\r\n            res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n            ^\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py:399:41: note: called from\r\n                                        user_expressions, allow_stdin)\r\n                                        ^\r\n\r\n\r\n\r\n\r\n```\r\n\r\n**Failure details**\r\nTFLiteConverter throws a ConverterError if a layer is build with dtype=tf.float16\r\n", "comments": ["@HauserA,\r\nI was able to reproduce the issue with [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/f224587803b856ee70613173638b04f1/38662.ipynb#scrollTo=6MVh8zlXA2DX). However, running the code with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/92107cf4dac1453d9c630b7d2e878788/38662-tf-nightly.ipynb) and adding the below two lines of code seems to work.\r\n\r\n```\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops =[tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] \r\n```\r\n\r\nPlease find the attached gist. Thanks!", "Thank you. I will use tf-nightly."]}, {"number": 38661, "title": "Using variable value in tf.name_scope raises ValueError in graph mode", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: macOS 10.15.5 Beta\r\n- Mobile device: n/a\r\n- TensorFlow installed from binary (`pip`)\r\n- TensorFlow version: \r\n    - `tf.version.GIT_VERSION`: v2.1.0-rc2-17-ge5bf8de410\r\n    - `tf.version.VERSION`: 2.1.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a (currently developing on CPU)\r\n\r\n\r\n**Describe the current behavior**\r\nExample code:\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass MyModule(tf.Module):\r\n    def __init__(self, name):\r\n        super(MyModule, self).__init__(name=name)\r\n\r\n@tf.function\r\ndef graphcry():\r\n    mod_inst1 = MyModule(name='inst1')\r\n    mod_inst2 = MyModule(name='inst2')\r\n    myscalar = tf.constant(83.2)\r\n    with tf.name_scope('scalaragain'):\r\n        tf.summary.scalar('scalaragain', data=myscalar)\r\n    with tf.name_scope(mod_inst1.name + ' and ' + mod_inst2.name):\r\n        tf.summary.scalar('myscalar', data=myscalar)\r\n\r\ngraphcry()\r\n```\r\nresults in this error:\r\n```\r\n with tf.name_scope(mod_inst1.name + ' and ' + mod_inst2.name):\r\n    .../site-packages/tensorflow_core/python/framework/ops.py:6392 __enter__\r\n        scope_name = scope.__enter__()\r\n    /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py:81 __enter__\r\n        return next(self.gen)\r\n   .../site-packages/tensorflow_core/python/framework/ops.py:4024 name_scope\r\n        raise ValueError(\"'%s' is not a valid scope name\" % name)\r\n\r\n    ValueError: 'inst1 and inst2' is not a valid scope name\r\n```\r\nI may be misunderstanding something about Graph mode, but at the very least, I think a better error should be shown (the docs say `ValueError` is raised when the argument to `tf.name_scope` is `None` or not a string). \r\n\r\nAnd excuse my poor variable naming and nonadherence to PEP 8\u2014I wrote this in nano. \r\n\r\n**Describe the expected behavior**\r\nThe scalar should be logged to TensorBoard without any errors, and under the name scope `inst1` and `inst2`. \r\n\r\n**Standalone code to reproduce the issue**\r\nA minimal reproducible example is in the current behavior section.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Here's a smaller reproducible example:\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef graphcry():\r\n    mod_inst1 = tf.Module(name='inst1')\r\n    mod_inst2 = tf.Module(name='inst2')\r\n    myscalar = tf.constant(83.2)  # just a random number\r\n    with tf.name_scope('scalaragain'):\r\n        tf.summary.scalar('scalaragain', data=myscalar)  # this works!\r\n    with tf.name_scope(mod_inst1.name + ' and ' + mod_inst2.name):\r\n        tf.summary.scalar('myscalar', data=myscalar)  # this returns the error above\r\n\r\ngraphcry()\r\n```", "@sumanthratna \r\ncan you please refer to these lonks and let us know if it helps [link1](https://stackoverflow.com/questions/51876460/getting-tensorflow-s-is-not-valid-scope-name-error-while-i-am-trying-to-creat) [link2](https://github.com/oswaldoludwig/Seq2seq-Chatbot-for-Keras/issues/12)\r\n", "For the first link: the string `'inst1 and inst2'` should pass the regex test\u2014it only contains alpha characters. \r\n\r\nI'm not using Keras, so the second link doesn't apply to my case. ", "i ran the code shared and face the same error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/2542174318deb80fd957706d6ca4e24f/38661.ipynb)", "Interestingly, it looks like `tf.Module` isn't specifically the problem. The following returns the same error:\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef graphcry():\r\n    inst1 = 'inst1'\r\n    inst2 = 'inst2'\r\n    myscalar = tf.constant(83.2)  # just a random number\r\n    with tf.name_scope('scalaragain_scope'):\r\n        tf.summary.scalar('scalaragain', data=myscalar)  # this works!\r\n    with tf.name_scope(inst1 + ' and ' + inst2):\r\n        tf.summary.scalar('myscalar', data=myscalar)  # this returns the error above\r\n\r\ngraphcry()\r\n```\r\n\r\n**EDIT**: and this doesn't work either:\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ninst1 = 'inst1'\r\ninst2 = 'inst2'\r\n\r\n@tf.function\r\ndef graphcry():\r\n    myscalar = tf.constant(83.2)  # just a random number\r\n    with tf.name_scope('scalaragain_scope'):\r\n        tf.summary.scalar('scalaragain', data=myscalar)  # this works!\r\n    with tf.name_scope(inst1 + ' and ' + inst2):\r\n        tf.summary.scalar('myscalar', data=myscalar)  # this returns the error above\r\n\r\ngraphcry()\r\n```", "And the following also fails:\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef graphcry():\r\n    inst1 = 'inst1'\r\n    inst2 = 'inst2'\r\n    myscalar = tf.constant(83.2)  # just a random number\r\n    with tf.name_scope('scalaragain_scope'):\r\n        tf.summary.scalar('scalaragain', data=myscalar)  # this works!\r\n    with tf.name_scope(inst1 + ' and ' + inst2):\r\n        tf.summary.scalar('myscalar', data=myscalar)  # this returns the error above\r\n\r\ntf.function(graphcry)()\r\n```", "@sumanthratna There are some constraints on the `name_scope`. If you remove the spaces in your name_scope as in the example below, then everything works as expected. Pleas take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/3b971c997fd4961f2b317fec8798b6fe/38661.ipynb).\r\n\r\nReplace this line\r\n`tf.name_scope(mod_inst1.name + ' and ' + mod_inst2.name)`\r\n\r\nwith this line\r\n`tf.name_scope(mod_inst1.name + '_and_' + mod_inst2.name)`\r\n\r\nCheck the [source here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L4149) and related stackoverflow [question here](https://stackoverflow.com/a/44664366/9936228). Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks! ", "Thanks for your reply! You're correct, removing spaces does fix the issue. However, I still see this as a bug\u2014in eager mode (without the `tf.function`), spaces are allowed. On the other hand, spaces aren't allowed with autograph. \r\n\r\nOne would expect the `name_scope` constraints to be the same for both modes of execution. ", "@sumanthratna For the benefit of users, can you please close the issue and open a new issue (use the original code block as it without tf.function) mentioning this error happens with \"eager\" mode only. It will be easy for the users to follow the issue better. You can ping me in that issue. \r\n\r\nPlease let us know what you think. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38661\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38661\">No</a>\n"]}, {"number": 38660, "title": "Build from sources: An error occurred during the fetch of repository 'local_config_cuda'", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 3.0.0 \r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 2.1.0\r\n- GCC/Compiler version (if compiling from source):7.5.0\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: NVIDIA QUADRO K600\r\n\r\n\r\n\r\n**Problem**\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 70: ordinal not in range(128)\r\n\r\n\r\n**Sequence of commands**\r\nbazel build  //tensorflow/tools/pip_package:build_pip_packageDD\r\n\r\n\r\n**logs**\r\n\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/andrey/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1213\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"/home/andrey/tensorflow/third_party/gpus/cuda_configure.bzl\", line 935, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/andrey/tensorflow/third_party/gpus/cuda_configure.bzl\", line 578, in _find_libs\r\n\t\t_check_cuda_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/andrey/tensorflow/third_party/gpus/cuda_configure.bzl\", line 480, in _check_cuda_libs\r\n\t\texecute(repository_ctx, <1 more arguments>)\r\n\tFile \"/home/andrey/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nRepository command failed\r\nTraceback (most recent call last):\r\n  File \"script.py\", line 88, in <module>\r\n    main()\r\n  File \"script.py\", line 77, in main\r\n    check_cuda_lib(path, check_soname=args[i + 1] == \"True\")\r\n  File \"script.py\", line 62, in check_cuda_lib\r\n    output = subprocess.check_output([objdump, \"-p\", path]).decode(\"ascii\")\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 70: ordinal not in range(128)\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/andrey/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1213\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"/home/andrey/tensorflow/third_party/gpus/cuda_configure.bzl\", line 935, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/andrey/tensorflow/third_party/gpus/cuda_configure.bzl\", line 578, in _find_libs\r\n\t\t_check_cuda_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/andrey/tensorflow/third_party/gpus/cuda_configure.bzl\", line 480, in _check_cuda_libs\r\n\t\texecute(repository_ctx, <1 more arguments>)\r\n\tFile \"/home/andrey/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nRepository command failed\r\nTraceback (most recent call last):\r\n  File \"script.py\", line 88, in <module>\r\n    main()\r\n  File \"script.py\", line 77, in main\r\n    check_cuda_lib(path, check_soname=args[i + 1] == \"True\")\r\n  File \"script.py\", line 62, in check_cuda_lib\r\n    output = subprocess.check_output([objdump, \"-p\", path]).decode(\"ascii\")\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 70: ordinal not in range(128)\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/andrey/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1213\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"/home/andrey/tensorflow/third_party/gpus/cuda_configure.bzl\", line 935, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/andrey/tensorflow/third_party/gpus/cuda_configure.bzl\", line 578, in _find_libs\r\n\t\t_check_cuda_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/andrey/tensorflow/third_party/gpus/cuda_configure.bzl\", line 480, in _check_cuda_libs\r\n\t\texecute(repository_ctx, <1 more arguments>)\r\n\tFile \"/home/andrey/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nRepository command failed\r\nTraceback (most recent call last):\r\n  File \"script.py\", line 88, in <module>\r\n    main()\r\n  File \"script.py\", line 77, in main\r\n    check_cuda_lib(path, check_soname=args[i + 1] == \"True\")\r\n  File \"script.py\", line 62, in check_cuda_lib\r\n    output = subprocess.check_output([objdump, \"-p\", path]).decode(\"ascii\")\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 70: ordinal not in range(128)\r\nINFO: Elapsed time: 9.036s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n", "comments": ["@AndreyM0 \r\nplease refer to these issues with similar error and let us know if it helps:\r\n\r\n#37748 #37995 #34552 #33879 #36132 #32363 #34146", "This small patch fix the issue on my side: #38940", "@AndreyM0\r\nplease update as per above comment", "This small patch work.\r\nBut when I compile\r\nbazel build //tensorflow/tools/pip_package:build_pip_package --jobs=1\r\n...\r\n[14,447 / 15,968] Comiling .../core/kernels/pad_op_gpu.cu.cc; 10s local\r\ncomputer freezes", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38660\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38660\">No</a>\n"]}, {"number": 38659, "title": "tf2+yolov3 Convert to tflite quession", "body": "o use: AVX2\r\nModel: \"yolov3\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\ninput (InputLayer)              [(None, None, None,  0\r\n__________________________________________________________________________________________________\r\nyolo_darknet (Model)            ((None, None, None,  40620640    input[0][0]\r\n__________________________________________________________________________________________________\r\nyolo_conv_0 (Model)             (None, None, None, 5 11024384    yolo_darknet[1][2]\r\n__________________________________________________________________________________________________\r\nyolo_conv_1 (Model)             (None, None, None, 2 2957312     yolo_conv_0[1][0]\r\n                                                                 yolo_darknet[1][1]\r\n__________________________________________________________________________________________________\r\nyolo_conv_2 (Model)             (None, None, None, 1 741376      yolo_conv_1[1][0]\r\n                                                                 yolo_darknet[1][0]\r\n__________________________________________________________________________________________________\r\nyolo_output_0 (Model)           (None, None, None, 3 4984063     yolo_conv_0[1][0]\r\n__________________________________________________________________________________________________\r\nyolo_output_1 (Model)           (None, None, None, 3 1312511     yolo_conv_1[1][0]\r\n__________________________________________________________________________________________________\r\nyolo_output_2 (Model)           (None, None, None, 3 361471      yolo_conv_2[1][0]\r\n__________________________________________________________________________________________________\r\nyolo_boxes_0 (Lambda)           ((None, None, None,  0           yolo_output_0[1][0]\r\n__________________________________________________________________________________________________\r\nyolo_boxes_1 (Lambda)           ((None, None, None,  0           yolo_output_1[1][0]\r\n__________________________________________________________________________________________________\r\nyolo_boxes_2 (Lambda)           ((None, None, None,  0           yolo_output_2[1][0]\r\n__________________________________________________________________________________________________\r\nyolo_nms (Lambda)               ((None, 100, 4), (No 0           yolo_boxes_0[0][0]\r\n                                                                 yolo_boxes_0[0][1]\r\n                                                                 yolo_boxes_0[0][2]\r\n                                                                 yolo_boxes_1[0][0]\r\n                                                                 yolo_boxes_1[0][1]\r\n                                                                 yolo_boxes_1[0][2]\r\n                                                                 yolo_boxes_2[0][0]\r\n                                                                 yolo_boxes_2[0][1]\r\n                                                                 yolo_boxes_2[0][2]\r\n==================================================================================================\r\nTotal params: 62,001,757\r\nTrainable params: 61,949,149\r\nNon-trainable params: 52,608\r\n__________________________________________________________________________________________________\r\nI0418 07:57:30.913888 54352 convert.py:19] model created\r\nI0418 07:57:30.922950 54352 utils.py:45] yolo_darknet/conv2d bn\r\nI0418 07:57:30.924918 54352 utils.py:45] yolo_darknet/conv2d_1 bn\r\nI0418 07:57:30.926922 54352 utils.py:45] yolo_darknet/conv2d_2 bn\r\nI0418 07:57:30.928931 54352 utils.py:45] yolo_darknet/conv2d_3 bn\r\nI0418 07:57:30.933943 54352 utils.py:45] yolo_darknet/conv2d_4 bn\r\nI0418 07:57:30.936951 54352 utils.py:45] yolo_darknet/conv2d_5 bn\r\nI0418 07:57:30.939996 54352 utils.py:45] yolo_darknet/conv2d_6 bn\r\nI0418 07:57:30.944971 54352 utils.py:45] yolo_darknet/conv2d_7 bn\r\nI0418 07:57:30.947012 54352 utils.py:45] yolo_darknet/conv2d_8 bn\r\nI0418 07:57:30.948982 54352 utils.py:45] yolo_darknet/conv2d_9 bn\r\nI0418 07:57:30.956000 54352 utils.py:45] yolo_darknet/conv2d_10 bn\r\nI0418 07:57:30.960010 54352 utils.py:45] yolo_darknet/conv2d_11 bn\r\nI0418 07:57:30.966026 54352 utils.py:45] yolo_darknet/conv2d_12 bn\r\nI0418 07:57:30.970039 54352 utils.py:45] yolo_darknet/conv2d_13 bn\r\nI0418 07:57:30.973078 54352 utils.py:45] yolo_darknet/conv2d_14 bn\r\nI0418 07:57:30.976053 54352 utils.py:45] yolo_darknet/conv2d_15 bn\r\nI0418 07:57:30.981098 54352 utils.py:45] yolo_darknet/conv2d_16 bn\r\nI0418 07:57:30.983071 54352 utils.py:45] yolo_darknet/conv2d_17 bn\r\nI0418 07:57:30.989118 54352 utils.py:45] yolo_darknet/conv2d_18 bn\r\nI0418 07:57:30.991132 54352 utils.py:45] yolo_darknet/conv2d_19 bn\r\nI0418 07:57:30.996108 54352 utils.py:45] yolo_darknet/conv2d_20 bn\r\nI0418 07:57:30.998111 54352 utils.py:45] yolo_darknet/conv2d_21 bn\r\nI0418 07:57:31.002122 54352 utils.py:45] yolo_darknet/conv2d_22 bn\r\nI0418 07:57:31.004158 54352 utils.py:45] yolo_darknet/conv2d_23 bn\r\nI0418 07:57:31.007134 54352 utils.py:45] yolo_darknet/conv2d_24 bn\r\nI0418 07:57:31.010144 54352 utils.py:45] yolo_darknet/conv2d_25 bn\r\nI0418 07:57:31.014154 54352 utils.py:45] yolo_darknet/conv2d_26 bn\r\nI0418 07:57:31.026219 54352 utils.py:45] yolo_darknet/conv2d_27 bn\r\nI0418 07:57:31.029193 54352 utils.py:45] yolo_darknet/conv2d_28 bn\r\nI0418 07:57:31.041259 54352 utils.py:45] yolo_darknet/conv2d_29 bn\r\nI0418 07:57:31.044235 54352 utils.py:45] yolo_darknet/conv2d_30 bn\r\nI0418 07:57:31.055304 54352 utils.py:45] yolo_darknet/conv2d_31 bn\r\nI0418 07:57:31.058273 54352 utils.py:45] yolo_darknet/conv2d_32 bn\r\nI0418 07:57:31.068298 54352 utils.py:45] yolo_darknet/conv2d_33 bn\r\nI0418 07:57:31.071306 54352 utils.py:45] yolo_darknet/conv2d_34 bn\r\nI0418 07:57:31.085376 54352 utils.py:45] yolo_darknet/conv2d_35 bn\r\nI0418 07:57:31.088377 54352 utils.py:45] yolo_darknet/conv2d_36 bn\r\nI0418 07:57:31.100429 54352 utils.py:45] yolo_darknet/conv2d_37 bn\r\nI0418 07:57:31.103426 54352 utils.py:45] yolo_darknet/conv2d_38 bn\r\nI0418 07:57:31.116426 54352 utils.py:45] yolo_darknet/conv2d_39 bn\r\nI0418 07:57:31.119471 54352 utils.py:45] yolo_darknet/conv2d_40 bn\r\nI0418 07:57:31.130496 54352 utils.py:45] yolo_darknet/conv2d_41 bn\r\nI0418 07:57:31.133472 54352 utils.py:45] yolo_darknet/conv2d_42 bn\r\nI0418 07:57:31.147543 54352 utils.py:45] yolo_darknet/conv2d_43 bn\r\nI0418 07:57:31.193630 54352 utils.py:45] yolo_darknet/conv2d_44 bn\r\nI0418 07:57:31.200688 54352 utils.py:45] yolo_darknet/conv2d_45 bn\r\nI0418 07:57:31.245767 54352 utils.py:45] yolo_darknet/conv2d_46 bn\r\nI0418 07:57:31.252820 54352 utils.py:45] yolo_darknet/conv2d_47 bn\r\nI0418 07:57:31.299911 54352 utils.py:45] yolo_darknet/conv2d_48 bn\r\nI0418 07:57:31.307933 54352 utils.py:45] yolo_darknet/conv2d_49 bn\r\nI0418 07:57:31.353052 54352 utils.py:45] yolo_darknet/conv2d_50 bn\r\nI0418 07:57:31.361075 54352 utils.py:45] yolo_darknet/conv2d_51 bn\r\nI0418 07:57:31.404188 54352 utils.py:45] yolo_conv_0/conv2d_52 bn\r\nI0418 07:57:31.411207 54352 utils.py:45] yolo_conv_0/conv2d_53 bn\r\nI0418 07:57:31.459374 54352 utils.py:45] yolo_conv_0/conv2d_54 bn\r\nI0418 07:57:31.466355 54352 utils.py:45] yolo_conv_0/conv2d_55 bn\r\nI0418 07:57:31.511475 54352 utils.py:45] yolo_conv_0/conv2d_56 bn\r\nI0418 07:57:31.518527 54352 utils.py:45] yolo_output_0/conv2d_57 bn\r\nI0418 07:57:31.562608 54352 utils.py:45] yolo_output_0/conv2d_58 bias\r\nI0418 07:57:31.566619 54352 utils.py:45] yolo_conv_1/conv2d_59 bn\r\nI0418 07:57:31.568628 54352 utils.py:45] yolo_conv_1/conv2d_60 bn\r\nI0418 07:57:31.570631 54352 utils.py:45] yolo_conv_1/conv2d_61 bn\r\nI0418 07:57:31.579694 54352 utils.py:45] yolo_conv_1/conv2d_62 bn\r\nI0418 07:57:31.582699 54352 utils.py:45] yolo_conv_1/conv2d_63 bn\r\nI0418 07:57:31.591723 54352 utils.py:45] yolo_conv_1/conv2d_64 bn\r\nI0418 07:57:31.593731 54352 utils.py:45] yolo_output_1/conv2d_65 bn\r\nI0418 07:57:31.602754 54352 utils.py:45] yolo_output_1/conv2d_66 bias\r\nI0418 07:57:31.604759 54352 utils.py:45] yolo_conv_2/conv2d_67 bn\r\nI0418 07:57:31.605724 54352 utils.py:45] yolo_conv_2/conv2d_68 bn\r\nI0418 07:57:31.606727 54352 utils.py:45] yolo_conv_2/conv2d_69 bn\r\nI0418 07:57:31.609774 54352 utils.py:45] yolo_conv_2/conv2d_70 bn\r\nI0418 07:57:31.610738 54352 utils.py:45] yolo_conv_2/conv2d_71 bn\r\nI0418 07:57:31.613745 54352 utils.py:45] yolo_conv_2/conv2d_72 bn\r\nI0418 07:57:31.614750 54352 utils.py:45] yolo_output_2/conv2d_73 bn\r\nI0418 07:57:31.617794 54352 utils.py:45] yolo_output_2/conv2d_74 bias\r\nI0418 07:57:31.618758 54352 convert.py:22] weights loaded\r\nI0418 07:57:32.081988 54352 convert.py:26] sanity check passed\r\nI0418 07:57:32.725983 54352 convert.py:29] weights saved\r\n\r\n(yolov3-tf2-cpu) C:\\Users\\EZSHIPE\\Downloads\\yolov3-tf2-peng\\yolov3-tf2-master>\r\n(yolov3-tf2-cpu) C:\\Users\\EZSHIPE\\Downloads\\yolov3-tf2-peng\\yolov3-tf2-master>python tools/export_tflite.py\r\n2020-04-18 07:58:00.586491: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not fo\r\nund\r\n2020-04-18 07:58:00.591208: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-04-18 07:58:08.994699: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n2020-04-18 07:58:08.999120: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-04-18 07:58:09.008186: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: CN-00012968\r\n2020-04-18 07:58:09.012520: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: CN-00012968\r\n2020-04-18 07:58:09.015456: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nI0418 07:58:14.900625 14620 export_tflite.py:34] weights loaded\r\n2020-04-18 07:58:15.624135: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-04-18 07:58:15.628949: I tensorflow/core/grappler/clusters/single2020-04-18 07:58:15.676348: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization res\r\nults for grappler item: graph_to_optimize\r\n2020-04-18 07:58:15.680926: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.005ms.\r\n2020-04-18 07:58:15.685573: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-04-18 07:58:22.022971: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-04-18 07:58:22.027511: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-18 07:58:26.747778: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-04-18 07:58:26.751954: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 993 nodes (-366), 2545 edges (-366), time = 3012\r\n.25903ms.\r\n2020-04-18 07:58:26.755753: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 993 nodes (0), 2545 edges (0), time = 833.124ms.\r\n\r\nTraceback (most recent call last):\r\n  File \"tools/export_tflite.py\", line 65, in <module>\r\n    app.run(main)\r\n  File \"C:\\Users\\EZSHIPE\\.conda\\envs\\yolov3-tf2-cpu\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\EZSHIPE\\.conda\\envs\\yolov3-tf2-cpu\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"tools/export_tflite.py\", line 37, in main\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\EZSHIPE\\.conda\\envs\\yolov3-tf2-cpu\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\EZSHIPE\\.conda\\envs\\yolov3-tf2-cpu\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\", line 457, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"C:\\Users\\EZSHIPE\\.conda\\envs\\yolov3-tf2-cpu\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\", line 203, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-04-18 07:58:29.920472: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not fo\r\nund\r\n2020-04-18 07:58:29.920864: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-04-18 07:58:39.215598: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-04-18 07:58:39.215864: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-04-18 07:58:39.216436: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-04-18 07:58:39.216683: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-04-18 07:58:39.217119: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-04-18 07:58:39.217325: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-04-18 07:58:39.217665: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: CombinedNonMaxSuppression\r\n2020-04-18 07:58:39.276378: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 735 operators, 1368 arrays (0 quantized)\r\n2020-04-18 07:58:39.295435: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 735 operators, 1368 arrays (0 quant\r\nized)\r\n2020-04-18 07:58:39.987076: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 284 operators, 533 arrays (0\r\nquantized)\r\n2020-04-18 07:58:39.993843: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 284 operators, 533 arrays (0\r\nquantized)\r\n2020-04-18 07:58:40.000453: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 284 operators, 533 arrays (\r\n0 quantized)\r\n2020-04-18 07:58:40.005543: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 284 operators, 533 arrays (0\r\n quantized)\r\n2020-04-18 07:58:40.009657: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Identify nearest upsample.: 284 operators, 533 arrays (0 quantized\r\n)\r\n2020-04-18 07:58:40.020140: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 66560128 bytes, theoretical optimal value: 44408960\r\nbytes.\r\n2020-04-18 07:58:40.021499: I tensorflow/lite/toco/toco_tooling.cc:471] Number of parameters: 61923215\r\n2020-04-18 07:58:40.029275: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpfu\r\nl if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended r\r\nuntime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implem\r\nentation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin ope\r\nrators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MUL, PACK, PAD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, S\r\nTRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: CombinedNonMaxSuppression, Size.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\ezshipe\\.conda\\envs\\yolov3-tf2-cpu\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\ezshipe\\.conda\\envs\\yolov3-tf2-cpu\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\EZSHIPE\\.conda\\envs\\yolov3-tf2-cpu\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\ezshipe\\.conda\\envs\\yolov3-tf2-cpu\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\ezshipe\\.conda\\envs\\yolov3-tf2-cpu\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\users\\ezshipe\\.conda\\envs\\yolov3-tf2-cpu\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"c:\\users\\ezshipe\\.conda\\envs\\yolov3-tf2-cpu\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\ezshipe\\.conda\\envs\\yolov3-tf2-cpu\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by openi\r\nng a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended r\r\nuntime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implem\r\nentation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin ope\r\nrators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MUL, PACK, PAD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, S\r\nTRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: CombinedNonMaxSuppression, Size.\r\n\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Please, fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).Make sure you also include the code snippet to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@teijeong \r\nCan you link to the tflite-supported yolo model?", "I don't have model file available for now - I've fixed 5D tensor usage in the model (https://github.com/YunYang1994/tensorflow-yolov3/pull/312), but it seems the problem here is some unsupported ops:\r\n\"CombinedNonMaxSuppression, Size\"", "I think this was resolved with the above comment. Please test with latest stable TF 2.6 version and open a new issue if problem still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38659\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38659\">No</a>\n"]}, {"number": 38657, "title": "Make LossScaleOpimtizer wrapper compatible with decoupled weight decay", "body": "This allows for us to use decoupled weight decay optimizers with mixed precision.\r\n\r\nhttps://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/weight_decay_optimizers/DecoupledWeightDecayExtension", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38657) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38657) for more info**.\n\n<!-- ok -->", "Right now, we intentionally do not expose attributes in the wrapped optimizer, so I'm closing this PR However, LossScaleOptimizer should still be able to wrap a DecoupledWeightDecayExtension optimizer. Let me know if there is an incompatibility between LossScaleOptimizer and DecoupledWeightDecayExtension.", "@reedwm \r\n\r\nThe problem doesn't arise from wrapping the optimizer directly, but if we need to dynamically change the weight_decay like we do with lr in a callback like ReduceLROnPlateau then we run into problems.\r\n\r\nWe can get and set the lr in the callback\r\n\r\n```\r\nK.get_value(self.model.optimizer.lr)\r\nK.set_value(self.model.optimizer.lr, new_lr)\r\n```\r\n\r\nBut we cannot do the same for weight decay (which is required when using decoupled weight decay)\r\n\r\n```\r\nK.get_value(self.model.optimizer.weight_decay)\r\nK.set_value(self.model.optimizer.weight_decay, new_wd)\r\n```\r\nI believe we should be able to decay lr/wd in the same manner.", "We did special case `lr` since almost every optimizer has a learning rate. I agree you should able to set the weight decay as well, but it's not feasible to manually define every attribute that an optimizer may define.\r\n\r\nI think the solution here is to either modify `__getattribute__` to delegate arbitrary attributes to the inner optimizer or to create a dynamic class which subclasses both the wrapped optimizer and LossScaleOptimizer. The latter is [what DecoupledWeightDecayExtension does](https://github.com/tensorflow/addons/blob/1af92905ed03f05fcf6f4918783c3d151a8b8350/tensorflow_addons/optimizers/weight_decay_optimizers.py#L183). I will work on implementing one of these and try to get it into TF 2.3.\r\n\r\nAs a workaround, for now you can access the weight decay with\r\n```\r\nif isinstance(optimizer, tf.keras.mixed_precision.experimental.LossScaleOptimizer):\r\n    weight_decay = optimizer._optimizer.weight_decay\r\nelse:\r\n    weight_decay = optimizer.weight_decay\r\n```\r\n\r\nNote accessing `optimizer._optimizer` may break in future versions of TensorFlow."]}, {"number": 38656, "title": "[Intel MKL] Fix yet another mkl_eager_op build failure", "body": "This commit fixes yet another change in Eager op that causes build failure in mkl_eager_op.", "comments": []}, {"number": 38655, "title": "[2.2rc3] Keras validation data doesn't respect cache in MirroredStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0rc3\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1 / 7.6.5.32\r\n- GPU model and memory: 4 x NVIDIA V100 on GCP\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running the code below with cached training and validations datasets in a multi-GPU environment (I am using a GCP VM with 312GB of memory and 4 NVIDIA V100s) with `tf.distribute.MirroredStrategy()` the validation dataset isn't correctly cached and examples are still read from GCS during validation.\r\n\r\nThe memory usage suggests that the validation dataset is cached, but during the Keras validation loop it looks like that data is still read from GCS instead of from the cache which can be observed by the very high network usage during validation. I would expect no network usage after the first epoch.\r\n\r\nIn the example below I intentionally use an very large validation set to make this issue very obvious and easy to detect through monitoring network usage. This behaviour can also be observed with other datasets, but the unexpected network access will be less noticible on smaller datsets.\r\n\r\n**In which cases can this issue not be observed?**\r\nTo narrow down the possible causes for this I found two cases where this issue doesn't exist:\r\n\r\n1. When running on a single GPU without `MirroredStrategy` the validation data is correctly read from the cache and after the start of the second epoch no additional network traffic reading from GCS can be observed.\r\n\r\n2. When not using a validation dataset at all the network usage is zero after the first epoch so caching of the training set works as expected.\r\n\r\nThis seems to be a complicated interaction between `tf.data`, `tf.keras` and `tf.distribute`, do you have an idea what could cause this behaviour? Please let me know what additional information I could provide.\r\n\r\n**Describe the expected behavior**\r\n\r\nNetwork usage should be zero after the start of the second epoch since both datasets are cached  in memory and no additional reads from GCS should be required.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\n\r\nbatch_size = 1024\r\ndecoders = {\"image\": tfds.decode.SkipDecoding()}\r\n\r\ndataset = tfds.load(\r\n    \"imagenet2012:5.0.0\",\r\n    decoders=decoders,\r\n    split=\"validation\",\r\n    data_dir=\"gs://my-data-bucket\",\r\n)\r\n\r\nval_dataset = tfds.load(\r\n    \"imagenet2012:5.0.0\",\r\n    decoders=decoders,\r\n    split=\"train\",\r\n    data_dir=\"gs://my-data-bucket\",\r\n)\r\n\r\n\r\ndef _decode_and_center_crop(image_bytes):\r\n    \"\"\"Crops to center of image with padding then scales image_size.\"\"\"\r\n    shape = tf.image.extract_jpeg_shape(image_bytes)\r\n    image_height = shape[0]\r\n    image_width = shape[1]\r\n    image_size = 224\r\n\r\n    padded_center_crop_size = tf.cast(\r\n        (\r\n            (image_size / (image_size + 32))\r\n            * tf.cast(tf.minimum(image_height, image_width), tf.float32)\r\n        ),\r\n        tf.int32,\r\n    )\r\n\r\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\r\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\r\n    crop_window = tf.stack(\r\n        [offset_height, offset_width, padded_center_crop_size, padded_center_crop_size]\r\n    )\r\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\r\n    return tf.image.resize(image, [image_size, image_size], method=\"bicubic\")\r\n\r\n\r\ndef preprocessing(data):\r\n    return tf.cast(_decode_and_center_crop(data[\"image\"]), tf.float32), data[\"label\"]\r\n\r\n\r\ndef apply_preprocessing(dataset):\r\n    return (\r\n        dataset.cache()\r\n        .map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n        .batch(batch_size)\r\n        .prefetch(1)\r\n    )\r\n\r\n\r\ndataset = apply_preprocessing(dataset)\r\nval_dataset = apply_preprocessing(val_dataset)\r\n\r\nwith tf.distribute.MirroredStrategy().scope():\r\n    model = tf.keras.models.Sequential(\r\n        [\r\n            tf.keras.layers.GlobalMaxPool2D(input_shape=(224, 224, 3)),\r\n            tf.keras.layers.Dense(1000, activation=\"softmax\",),\r\n        ]\r\n    )\r\n\r\n    model.compile(\r\n        optimizer=\"adam\",\r\n        loss=\"sparse_categorical_crossentropy\",\r\n        metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"],\r\n    )\r\n\r\nmodel.fit(\r\n    dataset, epochs=5, validation_data=val_dataset,\r\n)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nTo monitor the network usage over time tools like [`ytop`](https://github.com/cjbassi/ytop/) can be used.", "comments": ["Thank you for filing, it seems like this is known issue and the team is looking into it.  I can suggest a workaround with tf-nightly if this is blocking you. ", "@lgeiger \r\nplease update as per above comment ", "> Thank you for filing, it seems like this is known issue and the team is looking into it. I can suggest a workaround with tf-nightly if this is blocking you.\r\n\r\nIt'd be great to have a workaround, in case the fix cannot make into v2.2.", "Actually @jsimsa just submitted a fix for this yesterday (https://github.com/tensorflow/tensorflow/commit/7ebbab819e736319ec35b48e31f4d62fbad6626b). Assuming that has made it to the nightly - could @lgeiger could you try the nightly and see if this has been fixed for your use case? \r\nWe will try to have this fix in 2.2 as well. \r\n\r\n", "@guptapriya @jsimsa Thank you for the fast fix \ud83c\udf89 \r\n\r\nI can confirm that this issue doesn't exist on the latest nightly.\r\n\r\nShould I open a PR to cherry-pick 7ebbab819e736319ec35b48e31f4d62fbad6626b onto the release branch?\r\n", "Has been cherrypicked as #38807 ", "Thanks for the fast fix \ud83d\udc4d ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38655\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38655\">No</a>\n", "Hi, I was training my model using Multi Workers Mirrored Strategy and still get this issue.\r\nTime by time, the memory usage is still increasing until I got OOM.", "@alimhanif Could you check if your issue is related to #38617?"]}, {"number": 38654, "title": "[ROCm] Updating Dockerfile.rocm / CI scripts to use ROCm 3.3", "body": "/cc @whchung @cheshire @chsigg @nvining-work ", "comments": []}, {"number": 38653, "title": "Update version numbers for TensorFlow 2.2.0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 2 -> 2\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.2.0-rc3\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.2.0rc3\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 38652, "title": "TFLite: use the int8 type for no_float quantization", "body": "use the int8 type for inference input and output when quantizing with int8_no_float mode", "comments": ["Suharsh,\r\nCould you review this pull request?", "The model signature (input/output) needs to be opted into directly via the inference_input and inference_output flags. I believe there is work in progress to introduce these flags into the V2 converter.\r\n\r\nThe reason for this is that users who are converting to a float model, will suddenly get a model that expects a different input type and output type than what is obvious, so the converter expects the input type and output type provided explicitly.\r\n\r\nso this is working as intended for now. ", "Here is the issue tracking that https://github.com/tensorflow/tensorflow/issues/38285\r\n"]}, {"number": 38651, "title": "Cross-posting from Keras Memory leaks hang on GPU; no PID to kill (not sudo user so cannot install nvtop)", "body": "https://github.com/keras-team/keras/issues/13975", "comments": ["For this use case, we would suggest that you write a custom Callback to handle the extra processing you want after each step or batch. You can read more about callbacks here: https://www.tensorflow.org/guide/keras/custom_callback", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38651\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38651\">No</a>\n"]}, {"number": 38650, "title": "LossScaleOptimizer compatible with DecoupledWeightDecayExtension", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe features allows for use decoupled weight decay optimizers\r\n\r\nhttps://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/weight_decay_optimizers/DecoupledWeightDecayExtension\r\n\r\nwith mixed precision using\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer\r\n\r\n**Will this change the current api? How?**\r\n\r\nDon't believe so\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThose used decoupled weight decay and mixed precision.\r\n\r\n**Any Other info.**\r\n\r\nThat's it.\r\n", "comments": ["@ben-arnao \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "@ravikyram \r\n\r\nThe use case would be doing mixed precision with weight decay decoupled optimizer and then using a callback like ReduceLrOnPlateua where we need access to modify weight decay dynamically. \r\n\r\nHowever it looks like this is not something TF wants to change at this time but maybe will be improved in future releases.\r\n\r\nI know it's a narrow use case but for anyone that runs into the same issue, modifying the LossScaleOptimizer class to expose weight_decay works and you can decay LR and WD in the same manner.\r\n\r\n#38657"]}, {"number": 38649, "title": "Update RELEASE.md ", "body": "remove the macos py3.8 caveat as we are able to release py3.8 binary for macos. ", "comments": []}, {"number": 38648, "title": "save_best_only does not work", "body": "**System information**\r\nWindows 7\r\nPython 3.8\r\nTensorflow 2.1\r\n\r\n**Describe the current behavior**\r\n\r\n```python\r\nfilepath = \"RNN_Final-{epoch:02d}-{val_accuracy:.3f}\"\r\n\r\ncheckpoint = ModelCheckpoint(\"models\\{}.hdf5\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))\r\n```\r\n\r\n```python\r\nhistory = model.fit(\r\n    train_x, train_y,\r\n    batch_size=BATCH_SIZE,\r\n    epochs=EPOCHS,\r\n    validation_data=(validation_x, validation_y),\r\n    callbacks=[checkpoint],\r\n)\r\n```\r\n\r\nI train the model with  _save_best_only_  enabled. It saves **all** the models in the directory, even when the val_accuracy reduces. I tried also switching to val_loss mode='min' but same behavior occurs.\r\n\r\n**Describe the expected behavior**\r\n\r\nI was expecting it to save only the models that had higher val / lower loss (According to definitions)\r\n![tensorflow](https://user-images.githubusercontent.com/4284362/79604094-634bce80-80e5-11ea-8d5b-fe69262205b6.png)\r\n", "comments": ["@acegilz,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "this was related with _val_acc_ vs _val_accuracy_\r\n\r\nFixed after changing all variables to _val_accuracy_", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38648\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38648\">No</a>\n"]}, {"number": 38647, "title": "[2.2 cherrypick] Img preproc layers fix", "body": "Remove RandomZoom layer from the API.\r\nFix meaning of \"horizontal\" vs \"vertical\" flip in RandomFlip layer.\r\nRestrict \"fill_mode\" argument to \"constant\" value.", "comments": []}, {"number": 38646, "title": "Facing issue with installation of witwidget (google what-if too) on Windows in Anaconda", "body": " I am facing issue with installation of What-If tool.\r\n\r\nStep 1 : pip install tensorflow (This completed successfully)\r\n\r\nStep 2 : pip install witwidget (this completed successfully)\r\n\r\nStep 3 : jupyter nbextension install --py --symlink --sys-prefix witwidget (this failed and the error log is given below)\r\n\r\nEnvironment : Windows 10 Python version : 3.7.0\r\n\r\n(base) C:\\Users\\ankitagarwal5>jupyter nbextension install --py --symlink --sys-prefix witwidget Traceback (most recent call last): File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in _pywrap_tensorflow_internal = swig_import_helper() File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 243, in load_module return load_dynamic(name, filename, file) File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 343, in load_dynamic return _load(spec) ImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last): File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\Scripts\\jupyter-nbextension-script.py\", line 10, in sys.exit(main()) File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\jupyter_core\\application.py\", line 266, in launch_instance return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs) File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance app.start() File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\notebook\\nbextensions.py\", line 988, in start super(NBExtensionApp, self).start() File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\jupyter_core\\application.py\", line 255, in start self.subapp.start() File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\notebook\\nbextensions.py\", line 716, in start self.install_extensions() File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\notebook\\nbextensions.py\", line 695, in install_extensions **kwargs File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\notebook\\nbextensions.py\", line 211, in install_nbextension_python m, nbexts = _get_nbextension_metadata(module) File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\notebook\\nbextensions.py\", line 1122, in _get_nbextension_metadata m = import_item(module) File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\traitlets\\utils\\importstring.py\", line 42, in import_item return import(parts[0]) File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\witwidget__init__.py\", line 15, in from witwidget.notebook.visualization import * File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\witwidget\\notebook\\visualization.py\", line 17, in import tensorflow as tf File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow__init__.py\", line 101, in from tensorflow_core import * File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core__init__.py\", line 40, in from tensorflow.python.tools import module_util as _module_util File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow__init__.py\", line 50, in getattr module = self._load() File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow__init__.py\", line 44, in _load module = _importlib.import_module(self.name) File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib__init__.py\", line 127, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python__init__.py\", line 49, in from tensorflow.python import pywrap_tensorflow File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in raise ImportError(msg) ImportError: Traceback (most recent call last): File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in _pywrap_tensorflow_internal = swig_import_helper() File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 243, in load_module return load_dynamic(name, filename, file) File \"C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 343, in load_dynamic return _load(spec) ImportError: DLL load failed: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions. Include the entire stack trace above this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "This issue is more suitable on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues) repo since its related to TF installation with Anaconda. \r\nPlease post it on  [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues).\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38646\">No</a>\n"]}, {"number": 38645, "title": "Issue with Tensorshape and Datasets", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\nYes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\nLinux Ubuntu 18.04\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n\r\nN/A\r\n\r\n- TensorFlow installed from (source or binary):\r\n\r\nbinary (pip)\r\n\r\n- TensorFlow version (use command below):\r\n\r\nv2.1.0-rc2-17-ge5bf8de\r\n2.1.0\r\n\r\n- Python version:\r\n\r\n3.7.7\r\n\r\n- Bazel version (if compiling from source):\r\n\r\nN/A\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n\r\nN/A\r\n\r\n- CUDA/cuDNN version:\r\n\r\n10.1\r\n\r\n- GPU model and memory:\r\n\r\nGTX 1080 Ti, 11 GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to use tf.Dataset with generators, I am getting the following very strange error trying to build tf.TensorShape:\r\n\r\n```\r\nTypeError: in converted code:\r\n\r\n    <ipython-input-86-05d6b5f23a20>:11 None  *\r\n        ds = ds.interleave(lambda gen_idx: tf.data.Dataset.from_generator(gen_wrapper,\r\n    /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py:744 from_generator\r\n        output_types, tensor_shape.as_shape, output_shapes)\r\n    /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py:471 map_structure_up_to\r\n        results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n    /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py:471 <listcomp>\r\n        results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n    /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:1211 as_shape\r\n        return TensorShape(shape)\r\n    /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:771 __init__\r\n        self._dims = [as_dimension(d) for d in dims_iter]\r\n    /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:771 <listcomp>\r\n        self._dims = [as_dimension(d) for d in dims_iter]\r\n    /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:716 as_dimension\r\n        return Dimension(value)\r\n    /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:200 __init__\r\n        None)\r\n    <string>:3 raise_from\r\n        \r\n\r\n    TypeError: Dimension value must be integer or None or have an __index__ method, got TensorShape([1])\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThe ints I pass into TensorShape should be recognized as ints and not throw an error.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\ndef gen(): \r\n  for i in itertools.count(1): \r\n    yield (i, [1] * i) \r\nds = tf.data.Dataset.from_tensor_slices(list(range(24)))\r\nds = ds.interleave(lambda gen_idx: tf.data.Dataset.from_generator(gen,\r\n                                                                  output_types=(tf.float32),\r\n                                                                  args=(gen_idx,),\r\n                                                                  output_shapes=(tf.TensorShape([]), tf.TensorShape([None]))),\r\n                   cycle_length=24,\r\n                   block_length=1,\r\n                   num_parallel_calls=24)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Still not sure why this doesn't work, but I worked around it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38645\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38645\">No</a>\n", "@jostheim How did you work around the problem? I'm seeing the same thing in one (but not the other) of a dataset from generator", "@timothyjlaurent I made a callback where I run evaluate using a Sequence and manually stick the metrics into tensorboard\r\n\r\n```\r\nclass ValidationCallback(keras.callbacks.Callback):\r\n\r\n    def __init__(self, validation_gen, prefix, file_writer):\r\n        super(ValidationCallback, self).__init__()\r\n        config = get_config()\r\n        self.config = config\r\n        self.validation_gen = validation_gen\r\n        self.prefix = prefix\r\n        self.file_writer = file_writer\r\n\r\n    def on_batch_end(self, batch, logs=None):\r\n        if batch % 1000 == 0 and batch != 0:\r\n            logger.info('')\r\n            logger.info(f'learning_rate {keras.backend.eval(self.model.optimizer._decayed_lr(tf.float32))}')\r\n            logger.info('')\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        # with a Sequential model\r\n        metrics = self.model.evaluate(self.validation_gen,\r\n                                      verbose=1,\r\n                                      use_multiprocessing=False if self.config['train'].as_bool('use_dataset') else True,\r\n                                      workers=int(self.config['train']['num_workers']) if not self.config['train'].as_bool('use_dataset') else 1)\r\n        with self.file_writer.as_default():\r\n            for name, metric in zip(self.model.metrics_names, metrics):\r\n                logger.info('metric name {} prefix {}'.format(name, self.prefix))\r\n                tf.summary.scalar('{}_{}'.format(self.prefix, name), metric, step=epoch)\r\n                logs['{}_{}'.format(self.prefix, name)] = metric\r\n        logger.info('flushing file writer')\r\n```\r\n\r\nSome of that stuff is for my particular use case, but that is the basic gist. For file_writer\r\n\r\n```\r\nfile_writer = tf.summary.create_file_writer(tensorboard_directory + \"/metrics\")\r\n```\r\nI hope this helps!"]}, {"number": 38644, "title": "The output of BatchNormalization may contain Nan under certain parameters", "body": "**System information**  \r\n- Have I written custom code (as opposed to using example directory):  \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Linux Ubuntu 18.04\r\n- TensorFlow backend (yes / no):  yes\r\n- TensorFlow version:  2.1.0-CPU\r\n- Python version:  3.6.9\r\n- CUDA/cuDNN version:  -\r\n- GPU model and memory:  -\r\n\r\n**Describe the current behavior**  \r\nI found that the output of  `BatchNormalization` in `tensorflow.keras` may contain \"Nan\" under certain parameters. As an operation for normalization calculations, this is quite abnormal for `BatchNormalization` with nan output. This may lead to horrible results for the models with `BatchNormalization`, for example, accidental termination in training.  Detailed reproduction can be found in the following codes.\r\n\r\n![image](https://user-images.githubusercontent.com/46860123/79631824-f5a7ad00-818d-11ea-8474-c72228a6cf65.png)\r\n\r\n\r\n## Code to reproduce the issue\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nimport pickle\r\nimport argparse\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as L\r\nfrom tensorflow.keras.models import load_model\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='reproduce')\r\n    parser.add_argument('-path', '-p', type=str,help='Path')\r\n    args = parser.parse_args('-p ./Your Path/nan'.split( ))# Your path is where you unzip nan.zip\r\n    root_path = os.path.abspath(args.path)\r\n    with open(os.path.join(root_path, 'input.pkl'), 'rb') as f:#input,bug type,params\r\n        meta = pickle.load(f)\r\n    input1 = meta['input']\r\n    input=input1.astype(np.float32)\r\n    model_path = os.path.join(root_path, 'model.h5')\r\n    model = load_model(model_path)\r\n    output = model.predict(input)\r\n    nanresult=np.isnan(output).any()\r\n    print(nanresult)\r\n\r\n```\r\n[nan.zip](https://github.com/tensorflow/tensorflow/files/4494054/nan.zip)\r\n\r\nUnzip the nan.zip, you can find the model.h5 and input.pkl in it.", "comments": ["@shiningrain \r\n\r\nI tried to reproduce the issue with TF version 2.1, 2.2-rc3. I am seeing this error message (`SystemExit: 2`). Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/b5532e13bc2eb5aa048eee145dada450/untitled790.ipynb). Is this the expected behavior?Thanks!", "> @shiningrain\r\n> \r\n> I tried to reproduce the issue with TF version 2.1, 2.2-rc3. I am seeing this error message (`SystemExit: 2`). Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/b5532e13bc2eb5aa048eee145dada450/untitled790.ipynb). Is this the expected behavior?Thanks!\r\n\r\nThanks for your reply!\r\nAs shown in the following figure, I this there is something wrong with your path for the unzipped nan.zip. It seem like colab can not find your path.\r\n![image](https://user-images.githubusercontent.com/46860123/79747171-d3ed2800-833d-11ea-800c-0b4822a12f52.png)\r\nI think the reason caused this problem may be the `split( )` in my code to split the string. Could you try to use a path without a  blank space? like `./content/drive/MyDrive/nan`, or may be you can use the following codes:\r\n```\r\nimport os\r\nimport numpy as np\r\nimport pickle\r\nimport argparse\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as L\r\nfrom tensorflow.keras.models import load_model\r\ntensorflowVersion=(tf.__version__)\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='reproduce')\r\n    parser.add_argument('-path', '-p', type=str,help='Path')\r\n    args = parser.parse_args('-p$$/YourPath/nan'.split(\"$$\"))\r\n    root_path = os.path.abspath(args.path)\r\n    with open(os.path.join(root_path, 'input.pkl'), 'rb') as f:#input,bug type,params\r\n        meta = pickle.load(f)\r\n    input1 = meta['input']\r\n    input=input1.astype(np.float32)\r\n    model_path = os.path.join(root_path, 'model.h5')\r\n    model = load_model(model_path)\r\n    output = model.predict(input)\r\n    nanresult=np.isnan(output).any()\r\n    print(nanresult)\r\n```", "@shiningrain \r\n\r\nI tried to reproduce the issue with TF version 2.1.0. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2602f1f2a94a08ccc6dc072198d19b18/untitled801.ipynb).Is this the expected behavior?Thanks!", "> @shiningrain\r\n> \r\n> I tried to reproduce the issue with TF version 2.1.0. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2602f1f2a94a08ccc6dc072198d19b18/untitled801.ipynb).Is this the expected behavior?Thanks!\r\n\r\nThis is exactly the issue I want to show! Under such a model, `BatchNormalization` will return the results containing 'Nan', which will have horrible impact on actual use. I think that there should be some code defects in the implementation of `BatchNormalization`, because the calculation of `BatchNormalization` should be no possibility of overflow.\r\nI hope you can confirm the root cause of this issue.\r\nThank you very much!", "I have tried on colab with TF version 2.1.0, 2.2-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/63e5fdd38d1737e8b98142e0d4b9b984/untitled808.ipynb).Thanks!", "> I have tried on colab with TF version 2.1.0, 2.2-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/63e5fdd38d1737e8b98142e0d4b9b984/untitled808.ipynb).Thanks!\r\n\r\nYes, that is it.\r\n I further find that when conducting `BatchNormalization`, Tensorflow will invoke the **sqrt** related operations. As shown in the following figure.\r\n![image](https://user-images.githubusercontent.com/46860123/80238482-19d52380-8691-11ea-8856-34b0b9cc0b54.png)\r\n\r\n The **sqrt** returns NAN when taking negative values as inputs.  I conduct a confirmation testing with the following example. \r\n![image](https://user-images.githubusercontent.com/46860123/80238273-bba84080-8690-11ea-8c19-d4ae8922a7a8.png)\r\n\r\nI think this should be the reason for the NAN output of `BatchNormalization` in the above model.\r\nTensorflow may need a check for such negative input when using sqrt to avoid possible nan output results that interfere with subsequent use.\r\n\r\n", "I tried this test case with \r\n`input = np.absolute(input)`, which makes sure all the values in the `input` are positive.\r\nBut it still gives `nan`. \r\n\r\n~~~python\r\nimport os\r\nimport numpy as np\r\nimport pickle\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as L\r\nfrom tensorflow.keras.models import load_model\r\n\r\nwith open('./input.pkl', 'rb') as f:#input,bug type,params\r\n        meta = pickle.load(f)\r\n\r\ninput1 = meta['input']\r\ninput2 = input1.astype(np.float32)\r\ninput3 = np.absolute(input2)\r\n\r\nprint(np.amin(input3))   # 0.001712285\r\n\r\nmodel = load_model('./model.h5')\r\noutput = model.predict(input3)\r\nnanresult=np.isnan(output).any()\r\n~~~\r\n", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210525, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/aa9b10583ae04263001472f8fb173f9e/38644.ipynb). Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38644\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38644\">No</a>\n"]}, {"number": 38641, "title": "Consistent capitalization of \"Python\" in array_ops.py", "body": "", "comments": []}, {"number": 38640, "title": "K.cast_to_floatx() will convert \"None\" to \"Nan\" and lead the ReLU to Nan output.", "body": "**System information**  \r\n- Have I written custom code (as opposed to using example directory):  \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Linux Ubuntu 18.04\r\n- TensorFlow backend (yes / no):  yes\r\n- TensorFlow version:  1.14.0-cpu\r\n- Python version:  3.6.9\r\n- CUDA/cuDNN version:  -\r\n- GPU model and memory:  -\r\n\r\n**Describe the current behavior**  \r\n\r\nI found that if I used `ReLU(threshold = None)` in `tensorflow.keras`, without any errors or warnings, Tensorflow will return a matrix with Nan.  (Detailed configuration and codes for reproduction can be found in the following part) .\r\n\r\nFor this reason, I did some investigations and found that when the parameters in `ReLU` are passed to `/tensorflow/python/keras/layers/advanced_activations.py line near line 311`, `K.cast_to_floatx()`  will incorrectly convert the \"None\" parameter to \"Nan\" and pass it to the backend for calculation (refer to Figure 1 and Figure 2).  \r\n\r\n\"Nan\" and \"None\" should have different meanings, but `K.cast_to_floatx` did not distinguish between \"Nan\" and \"None\" during the calculation, which led to the usage of a \"Nan\" parameter in the tensorflow calculation. This further affects the final output result and makes the output with  \"Nan\". This operation may confuse the users.  \r\n\r\n**Is there a difference in meaning between None and Nan in the implementation of `K.cast_to_floatx`?  Judging from the current results, their meanings are different.**   This issue n**ot only affect ReLU, but also affect ThresholdReLU, LeakyReLU and other operations using `K.cast_to_floatx()`** to convert the parameters.\r\n\r\n## Code to reproduce the issue\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as L\r\nfrom tensorflow.keras.models import load_model\r\n\r\n\r\nroot_path = \"./Your Path\"\r\nlayer_name=\"ReLU\"\r\nkwargs={'max_value': 0.5761369157060329, 'negative_slope': 0.7845179761191806, 'threshold': None}\r\ninput= (10 * np.random.randn(1,32,32,16)).astype(np.float32)\r\nfrom tensorflow.keras import Model, Input\r\nlayer_cls = getattr(L, layer_name)\r\nlayer = layer_cls(**kwargs)\r\nx = Input(batch_shape=input.shape)\r\ny = layer(x)\r\nbk_model =Model(x, y)\r\nmodel_path = os.path.join(root_path, 'model.h5')\r\nbk_model.save(model_path, bk_model)\r\nmodel = load_model(model_path)     \r\noutput = model.predict(input)\r\nnanresult=np.isnan(output).any()\r\nprint(nanresult)\r\n```", "comments": ["@shiningrain \r\ni ran the code shared and face [this error](https://colab.sandbox.google.com/gist/Saduf2019/673d2c21fcf02091f8243c5ac51f72a9/38640.ipynb)", "> @shiningrain\r\n> i ran the code shared and face [this error](https://colab.sandbox.google.com/gist/Saduf2019/673d2c21fcf02091f8243c5ac51f72a9/38640.ipynb)\r\n\r\nHello! Thanks for your reply!\r\nThe \"root path\" in my code is \"./ Your Path\", this means that you can set any path you want to save the following model... You can also delete it and just save the model in the current dir. You can also use this code:\r\n```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as L\r\nfrom tensorflow.keras.models import load_model\r\n\r\n\r\nlayer_name=\"ReLU\"\r\nkwargs={'max_value': 0.5761369157060329, 'negative_slope': 0.7845179761191806, 'threshold': None}\r\ninput= (10 * np.random.randn(1,32,32,16)).astype(np.float32)\r\nfrom tensorflow.keras import Model, Input\r\nlayer_cls = getattr(L, layer_name)\r\nlayer = layer_cls(**kwargs)\r\nx = Input(batch_shape=input.shape)\r\ny = layer(x)\r\nbk_model =Model(x, y)\r\nbk_model.save(\"model.h5\", bk_model)\r\nmodel = load_model(\"model.h5\")     \r\noutput = model.predict(input)\r\nnanresult=np.isnan(output).any()\r\nprint(nanresult)\r\n```\r\nAs the following picture shown, the `nanresult` is True, which means there is nan in `output`\r\n\r\n![image](https://user-images.githubusercontent.com/46860123/79586408-a452db80-8103-11ea-85f7-9b1e0310ef3f.png)\r\n", "Hello, @Saduf2019  I can also reproduce this issue in Tensorflow 2.1.0-cpu version, shown in the following figure. You can use the code in my reply to reproduce it.\r\n![image](https://user-images.githubusercontent.com/46860123/79631518-6ea60500-818c-11ea-9493-221ccd343132.png)\r\n", "i am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/bd0e794e3005cee01640dca7926e946f/untitled141.ipynb)", "> i am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/bd0e794e3005cee01640dca7926e946f/untitled141.ipynb)\r\n\r\nThanks for your reply! That is exactly what I want to show. \r\n`K.cast_to_floatx()` will convert None to nan, which are not the same things. And then it leads to nan output.", "The `none` to `nan` conversion is due to numpy behavior.\r\nhttps://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/backend.py#L183\r\n```python\r\nimport numpy as np\r\nnp.asarray(None, dtype=float)\r\n```\r\noutput:\r\n```python\r\narray(nan)\r\n```\r\n\r\nHowever the documentation says `threshold: Float` . Perhaps we can add check to pass a float value to threshold argument. \r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU#arguments_2", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38640\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38640\">No</a>\n", "> The `none` to `nan` conversion is due to numpy behavior.\r\n> https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/backend.py#L183\r\n> \r\n> ```python\r\n> import numpy as np\r\n> np.asarray(None, dtype=float)\r\n> ```\r\n> \r\n> output:\r\n> \r\n> ```python\r\n> array(nan)\r\n> ```\r\n> \r\n> However the documentation says `threshold: Float` . Perhaps we can add check to pass a float value to threshold argument.\r\n> https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU#arguments_2\r\n\r\nThat's a good idea! Thanks for your fix."]}, {"number": 38639, "title": "TFLu: add stm32f4 binary building to CI script", "body": "At least build binaries in regression until Renode is available, therefore\r\nmoving TODO.\r\nAlso merge changes to cmsis-nn wrappers", "comments": []}]