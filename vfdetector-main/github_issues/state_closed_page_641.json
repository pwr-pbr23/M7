[{"number": 34393, "title": "TFLite SIGILL on Invoke()", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I am using the C++ API on Android\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 10, Android 9\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 3, Samsung Galaxy S8\r\n- TensorFlow installed from (source or binary): Built from tag (build included in repro repo)\r\n- TensorFlow version (use command below): v2.0.0\r\n- Python version:\r\n- Bazel version (if compiling from source): 0.28.0\r\n- GCC/Compiler version (if compiling from source): NDK 18\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behaviour**\r\nWhile debugging C++ on Android, SIGILL is triggered on the interpreter->Invoke() call.  If you step past inference, TFLite still successfully infers and produces the correct result, but SIGILL shouldn't happen.\r\n\r\n**Describe the expected behavior**\r\nSIGILL shouldn't happen\r\n\r\n**Code to reproduce the issue**\r\nThis repro reproduces this issue:\r\ngit@github.com:infil00p/TFLiteSigILL.git\r\n\r\n**Other info / logs**\r\nDebug frame for SIGILL\r\n<img width=\"1286\" alt=\"Screen Shot 2019-11-18 at 12 56 57 PM\" src=\"https://user-images.githubusercontent.com/2313/69093285-026b7d80-0a03-11ea-809b-1e73d1537bf4.png\">\r\n", "comments": ["@srjoglekar246 could you take a look?", "Hey @infil00p,\r\nDoes the SIGILL also occur when you do inference without any debug tooling? There are some known issues with JNI code & gdb tooling, so I was wondering if its related.", "@srjoglekar246 It doesn't.  Even when using debug tooling, the inference continues and I get an output from it.  I'm trying to investigate some other issues and this is appearing on v2.0.0 where v1.14.0 was working fine.\r\n", "@infil00p,\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34393\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34393\">No</a>\n"]}, {"number": 34392, "title": "Training stucking and strange GPU usage", "body": "Hi there,\r\nIm recently trying to train my model using configs from zoo.\r\nBut sometimes my training is stucking.\r\nfull Issue is described here \r\nhttps://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/issues/385\r\n\r\nAnd the GPU usage looks like here - im uploading you a graph from MSI Afterburner :\r\n\r\nhttps://drive.google.com/file/d/1Z6q6uQnOfMn2VGDlpcWAHAwYW10qI1jA/view?usp=sharing\r\n\r\nThanks for help.\r\n", "comments": ["@Clouxie, Will it be possible to provide the complete code to reproduce reported issue. Thanks!", "So , first of all, I've downloaded models repository from here : \r\nhttps://github.com/tensorflow/models/tree/r1.13.0 , I've also downloaded lastest models with same result. Then I've downloaded faster_rcnn_inception_v2_coco from models zoo and make some changes inside labelmap.pbtxt and config file which im postin you below.\r\n\r\nmodel {\r\n  faster_rcnn {\r\n    num_classes: 6\r\n    image_resizer {\r\n      keep_aspect_ratio_resizer {\r\n        min_dimension: 600\r\n        max_dimension: 1024\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'faster_rcnn_inception_v2'\r\n      first_stage_features_stride: 16\r\n    }\r\n    first_stage_anchor_generator {\r\n      grid_anchor_generator {\r\n        scales: [0.25, 0.5, 1.0, 2.0]\r\n        aspect_ratios: [0.5, 1.0, 2.0]\r\n        height_stride: 16\r\n        width_stride: 16\r\n      }\r\n    }\r\n    first_stage_box_predictor_conv_hyperparams {\r\n      op: CONV\r\n      regularizer {\r\n        l2_regularizer {\r\n          weight: 0.0\r\n        }\r\n      }\r\n      initializer {\r\n        truncated_normal_initializer {\r\n          stddev: 0.01\r\n        }\r\n      }\r\n    }\r\n    first_stage_nms_score_threshold: 0.0\r\n    first_stage_nms_iou_threshold: 0.7\r\n    first_stage_max_proposals: 300\r\n    first_stage_localization_loss_weight: 2.0\r\n    first_stage_objectness_loss_weight: 1.0\r\n    initial_crop_size: 14\r\n    maxpool_kernel_size: 2\r\n    maxpool_stride: 2\r\n    second_stage_box_predictor {\r\n      mask_rcnn_box_predictor {\r\n        use_dropout: false\r\n        dropout_keep_probability: 1.0\r\n        fc_hyperparams {\r\n          op: FC\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            variance_scaling_initializer {\r\n              factor: 1.0\r\n              uniform: true\r\n              mode: FAN_AVG\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    second_stage_post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.0\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 300\r\n      }\r\n      score_converter: SOFTMAX\r\n    }\r\n    second_stage_localization_loss_weight: 2.0\r\n    second_stage_classification_loss_weight: 1.0\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  batch_size: 1\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        manual_step_learning_rate {\r\n          initial_learning_rate: 0.0002\r\n          schedule {\r\n            step: 1\r\n            learning_rate: .0002\r\n          }\r\n          schedule {\r\n            step: 900000\r\n            learning_rate: .00002\r\n          }\r\n          schedule {\r\n            step: 1200000\r\n            learning_rate: .000002\r\n          }\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  gradient_clipping_by_norm: 10.0\r\n  fine_tune_checkpoint: \"C:/Users/holox/models-r1.13.0/research/object_detection/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt\"\r\n  from_detection_checkpoint: true\r\n  num_steps: 200000\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n}\r\n\r\n\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"C:/Users/holox/models-r1.13.0/research/object_detection/train.record\"\r\n  }\r\n  label_map_path: \"C:/Users/holox/models-r1.13.0/research/object_detection/training/labelmap.pbtxt\"\r\n}\r\n\r\neval_config: {\r\n  num_examples: 15\r\n  max_evals: 3\r\n}\r\n\r\neval_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"C:/Users/holox/models-r1.13.0/research/object_detection/test.record\"\r\n  }\r\n  label_map_path: \"C:/Users/holox/models-r1.13.0/research/object_detection/training/labelmap.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n}\r\n\r\n", "Do you see the same problem when you train with just a CPU?", "@Clouxie I'm running into the exact same issue, training stops randomly with 0 GPU usage. Just wondering if you've found a workaround yet?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 34391, "title": "TPUStrategy broken in TF2 Keras", "body": "**System information**\r\n- Have I written custom code: YES\r\n- OS Platform and Distribution: Google Colab\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below):  ('v2.0.0-rc2-26-g64c3d38', '2.0.0')\r\n- GPU model and memory:  Colab TPU\r\n\r\n**Describe the current behavior**\r\nTPU in collab cannot be used with TF 2, trying to use `TFRecordDataset` as an input to a Keras `Model.fit()` generates different exceptions depending on the whether `TPUStrategy.experimental_distribute_dataset()` is used or not.\r\n\r\n\r\n1. when `TPUStrategy.experimental_distribute_dataset()` is **not** used or used but not within an `/job:worker` context:\r\n```\r\nInternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:worker/replica:0/task:0/device:CPU:0 in order to run AutoShardDataset: Unable to parse tensor proto\r\nAdditional GRPC error information:\r\n{\"created\":\"@1574107145.043685979\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to parse tensor proto\",\"grpc_status\":3} [Op:AutoShardDataset]\r\n```\r\n\r\n2. using a `/job:worker` context without `experimental_distribute_dataset` crashes the Colab session with:\r\n```\r\nNov 18, 2019, 9:17:39 PM\tWARNING\t2019-11-18 20:17:39.591672: E tensorflow/core/framework/variant.cc:102] Could not decode variant with type_name: \"tensorflow::DatasetVariantWrapper\". Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?\r\nNov 18, 2019, 9:09:53 PM\tWARNING\t2019-11-18 20:09:53.396513: E tensorflow/core/framework/dataset.cc:76] The Encode() method is not implemented for DatasetVariantWrapper objects.\r\nNov 18, 2019, 9:08:19 PM\tWARNING\t2019-11-18 20:08:19.842178: E tensorflow/core/framework/dataset.cc:76] The Encode() method is not implemented for DatasetVariantWrapper objects.\r\n```\r\n\r\n3. and 4. when `experimental_distribute_dataset` is used in a `tf.device(\"/job:worker\")` context:\r\n```\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py in _distribution_standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, validation_split, shuffle, epochs, allow_partial_batch)\r\n   2313         x = ds.batch(batch_size, drop_remainder=drop_remainder)\r\n   2314       else:\r\n-> 2315         assert isinstance(x, dataset_ops.DatasetV2)\r\n   2316         training_utils.validate_dataset_input(x, y, sample_weight,\r\n   2317                                               validation_split)\r\n```\r\n\r\n**Describe the expected behavior**\r\nThere should be at least one way of making the example bellow work with TF2 (it works with TF1).\r\n\r\n**Code to reproduce the issue**\r\ncould also be checked here https://colab.research.google.com/gist/kpe/22340866c1dd3208d9177d2c8a9322e3/tpu-emb.ipynb\r\n\r\n```python\r\n%tensorflow_version 2.x\r\nimport tensorflow as tf\r\nprint(\"TF version:\", tf.__version__)\r\n\r\nimport os\r\n\r\ntfrec_path = \"gs://kpe-pub/pub/tpu-strategy-ds-issue/test.tfrecords\"\r\n\r\ndef parse_example(proto):\r\n    return tf.io.parse_single_example(proto, {\r\n        \"feature\": tf.io.VarLenFeature(tf.float32), \r\n        \"label\":   tf.io.VarLenFeature(tf.int64)\r\n    })\r\n\r\ndef from_tfrecords_file(tfrec_path):\r\n    ds = tf.data.TFRecordDataset([tfrec_path], compression_type=\"GZIP\")\r\n    ds = ds.map(parse_example)\r\n    def to_dense(example):\r\n        feature = tf.cast(tf.sparse.to_dense(example[\"feature\"]), tf.float32)\r\n        label   = tf.cast(tf.sparse.to_dense(example[\"label\"]), tf.int32)\r\n        return feature, tf.squeeze(label, -1)\r\n    ds = ds.map(to_dense)\r\n    return ds\r\n\r\ntry:\r\n    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\n    tf.config.experimental_connect_to_host(TPU_WORKER)\r\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\nexcept:\r\n    strategy = tf.distribute.get_strategy()\r\nprint(strategy)\r\n\r\n\r\ndef test_strategy(use_case):\r\n    assert use_case in [0,1,2,3,4]\r\n\r\n    if tf.__version__.startswith(\"1.\"): # in TF1 no need to distribute the dataset\r\n        ds = from_tfrecords_file(tfrec_path).repeat().batch(32)\r\n    else:\r\n        if use_case == 0:\r\n            ds = from_tfrecords_file(tfrec_path).repeat().batch(32)\r\n        if use_case == 1:\r\n            ds = from_tfrecords_file(tfrec_path).repeat().batch(32)\r\n            ds = strategy.experimental_distribute_dataset(ds)\r\n        if use_case == 2:            \r\n            with tf.device(\"/job:worker\"):\r\n                ds = from_tfrecords_file(tfrec_path).repeat().batch(32)\r\n        if use_case == 3:\r\n            with tf.device(\"/job:worker\"):\r\n                ds = from_tfrecords_file(tfrec_path).repeat().batch(32)\r\n                ds = strategy.experimental_distribute_dataset(ds)\r\n        if use_case == 4:\r\n            with strategy.scope(), tf.device(\"/job:worker\"):\r\n                ds = from_tfrecords_file(tfrec_path).repeat().batch(32)\r\n                ds = strategy.experimental_distribute_dataset(ds)\r\n\r\n    with strategy.scope():\r\n        model = tf.keras.models.Sequential([\r\n            tf.keras.layers.InputLayer(input_shape=(128,)),\r\n            tf.keras.layers.Dense(2)\r\n        ])\r\n        model.build()\r\n        model.compile(\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\r\n        model.fit(ds, steps_per_epoch=4)\r\n\r\ntest_strategy(0) # try 0 ot 4\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Just noted, that disabling eager execution, makes the test cases 0 and 2 above work again, i.e. without `experimental_distribute_dataset()` it works fine, just `eager_execution` needs to be disabled.", "If I don't disable eager execution in Colab now tf.config.experimental_connect_to_cluster(cluster_resolver)\r\ndose not return", "Using the nightly build (Tensorflow version 2.1.0-dev20191203) it returns but then tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\nProduces:\r\n\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.68.77.162:8470\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.68.77.162:8470\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Clearing out eager caches\r\n ---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-46-fbbe9ad0029d> in <module>()\r\n----> 1 tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: '__inference__tpu_init_fn_105' is neither a type of a primitive operation nor a name of a function registered in binary running on n-050d2992-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n\r\nIf we disable eager execution then we get:\r\n<ipython-input-92-e1cd6679a88c> in <module>()\r\n----> 1 tf.config.experimental_connect_to_cluster(cluster_resolver)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/remote.py in connect_to_cluster(cluster_spec_or_resolver, job_name, task_index, protocol, make_master_device_default)\r\n    108   if not context.executing_eagerly():\r\n    109     raise ValueError(\r\n--> 110         \"`tf.config.experimental_connect_to_cluster` can only be called in \"\r\n    111         \"eager mode.\"\r\n    112     )\r\n\r\nValueError: `tf.config.experimental_connect_to_cluster` can only be called in eager mode.", "That error indicates that the backend is old relative to the Python version. You'll have to wait until the TF 2.1 release for this to work, or use a nightly TPU backend.", "The error has not been solved in nightly builds yet. Any idea about when will a solution be available?", "Issue persists in TF2.1.0 and nightly (12/2) releases.", "i am also looking for a solution of training CRNN model with TPU strategy but shows \r\n\r\n> InvalidArgumentError: {{function_node __inference_distributed_function_65800}} Compilation failure: Detected unsupported operations when trying to compile graph cluster_distributed_function_5228616458413911404[] on XLA_TPU_JIT: CTCLoss (No registered 'CTCLoss' OpKernel for XLA_TPU_JIT devices compatible with node {{node model_1/lambda_1/CTCLoss}}\r\n\t.  Registered:  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n){{node model_1/lambda_1/CTCLoss}}\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_10567693008944060997/_8]]\r\n\r\nLink to the issue [here](https://github.com/tensorflow/tpu/issues/692)", "I run into this problem when I try to train the huggingface's distilbert model on google cloud. Any update on this issue?", "having same problem, any idea about the solution?", "The original issue should have been resolved in TF2.1 and nightly. Now users don't need to manage the device scope themselves.\r\n\r\nThe issue @puneetjindal reported is actually because CTCLoss is not supported in TPU, but other than that, your job should have been initialized and run, it just encounters a compilation error.\r\n\r\n@srinjay-paul, @EthanPhan, @auwal84, can you post more details on your error? It is likely the errors are caused by different causes.", "I believe I have found a way to fix this issue. See https://github.com/huan/tensorflow-handbook-tpu/issues/1#issuecomment-606189444", " @kpe It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Can you please execute your code using Latest stable Version(2.5) and let us know if the issue still persists?Please refer to this [issue comment ](https://github.com/huan/tensorflow-handbook-tpu/issues/1#issuecomment-606189444)  and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34391\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34391\">No</a>\n"]}, {"number": 34390, "title": "[TF2.0] tf.reduce_mean crashes Python (Floating point exception) if the count becomes zero due to overflow", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: binary, source tested as well\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the script:\r\n```python\r\nimport tensorflow as tf\r\ndata = tf.zeros(256, dtype=tf.uint8)\r\nprint(tf.reduce_mean(data))\r\n```\r\nCrashes the Python interpreter (e.g. `Floating point exception (core dumped)`).\r\n(Likely as 256 overflows in `uint8` to 0, leading to an uncaught division by zero.\r\n\r\n**Describe the expected behavior**\r\nA result, possibly incorrect (due to too small dtype), or some other way to deal with the issue, e.g. assertion errors or other exceptions, however no crashing of Python.\r\n*Ideally*, `tf.reduce_mean` could yield correct results for non-floating-point dtypes as well.\r\n\r\n**Code to reproduce the issue**\r\nSee above.\r\n", "comments": ["Issue replicating for TF-2.0 and tf-nightly, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/dcb73f2c1dea1d095f2acaf69df575f0/34390.ipynb) of colab.Thanks!", "from the call stack, the core dump happens in the eigen lib\r\n```\r\n#0  0x00007fb3605bdfb1 in Eigen::internal::scalar_quotient_op<unsigned char const, unsigned char const>::operator() (\r\n    this=0x7ffd3048cb80, a=@0x7ffd3048caef: 0 '\\000', b=@0x7ffd3048cb80: 0 '\\000')\r\n    at external/eigen_archive/Eigen/src/Core/functors/BinaryFunctors.h:361\r\n#1  0x00007fb3605a2ef7 in Eigen::internal::bind2nd_op<Eigen::internal::scalar_quotient_op<unsigned char const, unsigned char const> >::operator() (this=0x7ffd3048cb80, a=@0x7ffd3048caef: 0 '\\000')\r\n    at external/eigen_archive/Eigen/src/Core/functors/BinaryFunctors.h:460\r\n```\r\n@csachs division by zero do happens which cause the core dump", "@csachs Looks like this was resolved. I couldn't reproduce the issue. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/1f50643112231016fa21d70942febf93/untitled838.ipynb) is the gist.\r\n\r\nI am closing this issue as it was resolved. Please feel free to open if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34390\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34390\">No</a>\n", "Looks good in nightly."]}, {"number": 34389, "title": "Fixes #34194 - tf.size() documentation", "body": "Copied definition from v1 size API reference", "comments": []}, {"number": 34388, "title": "Update save.py", "body": "Fix #34348 .\r\nNotes:\r\n- Documentation needs to be changed (in multiple places) after final changes in code.\r\n- Changed code for deciding whether to save file as h5 or tf.\r\n- Removed the unncessary _HDF5_EXTENSIONS list. Will have to make sure it wasn't used elsewhere.\r\n- Added 4 new ValueError raises.", "comments": ["@k-w-w , Please see the new changes:\r\n- Added new function `validate_save_format` as requested by @k-w-w inside `network.py`.\r\n- Using `validate_save_format` for validating save_format in `save.save_model` and `network.save_weights`\r\n\r\nA few updates are required in `save_weights`:\r\n- `validate_save_format` is designed to work with path as well as h5py.File objects. This works with `save.save_model` but not with `network.save_weights` which accepts only String as the path.\r\n- Does it make sense to add functionality to save_weights to save to h5py.File objects?", "@k-w-w gentle ping to review new changes ?", "@nikochiko Could you please check failed build errors? Thanks!", "@gbaned Looking into it now. Sorry for the delay,", "@gbaned Fixed.", "@gbaned @k-w-w Should I add tests for the new function in the same PR or create another?\r\nAlso, can you point me to where the tests should be written.", "@nikochiko please add the test cases in `tensorflow/python/keras/engine/network_test.py`", "@rthadur I have added the tests. :)", "@nikochiko can you please fix sanity build failures.", "@rthadur done! ", "@nikochiko can you please resolve the conflicts as well ?", "@rthadur :+1: Done!", "Oops, another change is needed to push this. \r\n\r\nCan you modify the line `from tensorflow.python.keras.engine import network` in `save.py` to use the LazyLoader? An example is here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/saved_model/save_impl.py#L45", "Thanks for the example! \r\nI've added the changes :+1:", "Oops I had missed a trivial typo \ud83d\ude05 \r\nSorry for requesting review again. ", "@nikochiko can you please fix build failures ?", "@rthadur Done!", "@nikochiko Could you please address Ubuntu Sanity errors? Thanks!", "@gbaned Done :+1:", "> `Traceback (most recent call last):\r\n  File \"/build/work//google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/build/work//google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"/build/work//google3/runfiles/google3/learning/brain/contrib/distribute/python/keras_example_test_tpu.py\", line 39, in test_fit_eval_predict\r\n    callbacks=[tb_callback, checkpoint_cb])\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/keras/engine/training_v1.py\", line 792, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/keras/engine/training_distributed.py\", line 670, in fit\r\n    validation_freq=validation_freq)\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/keras/engine/training_distributed.py\", line 280, in experimental_tpu_fit_loop\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/keras/callbacks.py\", line 302, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/keras/callbacks.py\", line 1025, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/keras/callbacks.py\", line 1073, in _save_model\r\n    self.model.save(filepath, overwrite=True)\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/keras/engine/network.py\", line 1005, in save\r\n    signatures, options)\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/keras/saving/save.py\", line 117, in save_model\r\n    save_format = network.validate_save_format(filepath, save_format)\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"<embedded stdlib>/importlib/__init__.py\", line 37, in import_module\r\n    __import__(name)\r\nImportError: No module named python.keras.engine.network`", "@nikochiko sorry for the delay , this PR has been failing for above internal error, can you please fix this ?\r\n\r\ncc @k-w-w @goldiegadde ", "@rthadur @k-w-w I am unsure as to what is causing the error. I will try to troubleshoot it on my own but please let me know if it is a known issue.\r\nShould the local name be `network_lib` instead of `network`?"]}, {"number": 34387, "title": "MKL DNN:  Fixing mkl compilation failure for commit e154f44", "body": "The commit e154f4431e8283272a3b67c5ad59a2c6d50a506f in Nov 15, 2019 causes a failure in the Intel MKL build. We fixed it here. ", "comments": []}, {"number": 34386, "title": "Updated saved_model documentation", "body": "Fixed grammatical errors in documentation for files in /python/saved_model", "comments": []}, {"number": 34385, "title": "GPU device not found in Colab", "body": "My code worked well with GPU in Colab yesterday. But this morning it became very slow. So I suspect that CPU is used despite hardware accelerator is set to GPU in \u201cchange runtime type\u201d explicitly. So I check the availability of GPU following the tutorial:\r\nhttps://colab.research.google.com/notebooks/gpu.ipynb\r\n\r\ncode chunk:\r\n```\r\n%tensorflow_version 2.x\r\nimport tensorflow as tf\r\ndevice_name = tf.test.gpu_device_name()\r\nif device_name != '/device:GPU:0':\r\n  raise SystemError('GPU device not found')\r\nprint('Found GPU at: {}'.format(device_name))\r\n```\r\n\r\nIndeed, I got SystemError: GPU device not found. I tried this with different sessions or google user accounts, the same results. You should be able to replicate this in a new Colab session.\r\nMaybe some tensorflow updates are involved? It could be that Colab session provide GPU, but tensorflow can\u2019t see it, e.g. tf.test.gpu_device_name() or tf.test.is_gpu_available(). \r\nPlease help, thank you!", "comments": ["The best available hardware is prioritized for users who use Colaboratory interactively rather than for long-running computations. Users who use Colaboratory for long-running computations may be temporarily restricted in the type of hardware made available to them, and/or the duration that the hardware can be used for.\r\nSee [Why are hardware resources such as T4 GPUs not available to me?](https://research.google.com/colaboratory/faq.html)\r\nYou may try shut down current notebook and start a new one.", "Thanks for the response. I only use Colaboratory interactively, and never for long-running computations. It is not likely that I am temporarily restricted because I tried the GPU test in different sessions and different user accounts (mentioned above). That\u2019s way I think it is more likely related to tensorflow update.\r\nCan you try the code chunk above in Colab (of course with hardware accelerator is set to GPU in \u201cchange runtime type\u201d first) to see whether you can replicate the problem? thanks.", "Thanks. I was able to reproduce the behavior.\r\nAs a workaround you may try installing tf-nightly-gpu version;\r\n```pip install tf-nightly-gpu```\r\nYou can also comment out ```%tensorflow_version 2.x``` lines from the colab.\r\nCan you please point me to the webpage where you found the tutorial? Thanks!", "I have the same issue. Yesterday everything worked fine. I also didn't use it for long-running computations.", "@ymodak \r\ninstalling tf-nightly-gpu does not help. Commenting out %tensorflow_version 2.x lines works. Unfortunately, I need to work with tensorflow 2. It's even more likely that this is due to some recent update in tensorflow 2. You can find the tutorial in my original question above. Thanks.\r\n@qo4on \r\nglad that someone replicates my problem more precisely! Hope the tensorflow team can quicly noticed this and reverse the problematic update.", "I'm also having the same problem and in this morning worked fine.", "Same here. `pip install tf-nightly-gpu` and commenting `%tensorflow_version 2.x` doesn't work for me", "Hello @bigmw, I got the same issue.  tf 2.0 GPU worked yesterday and today it is not working.\r\nTf 1.15 still can find GPU but same like tf 2.0 can't find GPU (include K80, T4 and P100 in google colab).", "Downgrading cuda from 10.1 to 10.0 may solve the problem. [stackoverflow](https://stackoverflow.com/questions/58926378/unable-to-run-even-basic-example-with-tf-2-0-and-gpu-support-in-colab)", "Thank @ymodak, tensorflow 2.1.0-dev20191119 worked for me.\r\n\r\n```\r\n!pip install tf-nightly-gpu\r\ntry:\r\n  import tensorflow.compat.v2 as tf\r\nexcept Exception:\r\n  pass\r\ntf.enable_v2_behavior()\r\nprint(tf.__version__)\r\n```\r\n`2.1.0-dev20191119`\r\n\r\nComment: %tensorflow_version 2.x still get tf 2.0 so that wont work.\r\n", "Thanks @QuangHa97 \r\ninstalling tf-nightly-gpu following your code above does solve the GPU error. However, GPU under tf-nightly-gpu (TF 2.1.0-dev20191119) seems to be much slower than the original TF 2.0 (before this bug occurred). My code used to take 5.6 seconds per epoch, now it\u2019s 8.6 seconds, about ~54% slower than before. There should still be some issue there with tf-nightly-gpu. Hope the TF team would notice and work on this.\r\n", "UPDATE:\r\nGPU is working with TF 2.0 now in Colab. Thanks for all your comments/suggestions.", "Awesome. Closing this issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34385\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34385\">No</a>\n", "> UPDATE:\r\n> GPU is working with TF 2.0 now in Colab. Thanks for all your comments/suggestions.\r\n\r\nI still got the same error; why is that?", "I still got the same error after\r\n```\r\npip install tf-nightly-gpu\r\n\r\n%tensorflow_version 2.x  \r\nimport tensorflow as tf  \r\ndevice_name = tf.test.gpu_device_name()  \r\nif device_name != '/device:GPU:0':\r\n   raise SystemError('GPU device not found')\r\nprint('Found GPU at: {}'.format(device_name))\r\n```\r\nSystemError                               Traceback (most recent call last)\r\n\r\n<ipython-input-10-7c0b5e3718da> in <module>()\r\n      3 device_name = tf.test.gpu_device_name()\r\n      4 if device_name != '/device:GPU:0':\r\n----> 5    raise SystemError('GPU device not found')\r\n      6 print('Found GPU at: {}'.format(device_name))\r\n\r\nSystemError: GPU device not found\r\n\r\nbut \r\n`!nvidia-smi`\r\nget\r\nNVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2  ", "Open google Colab -> Right top corner find RAM/Disk -> Dropdown ->View resources - >change runtime -> hardware accelerator -> select GPU -> save .This solves the issue", "> \r\n> \r\n> Open google Colab -> Right top corner find RAM/Disk -> Dropdown ->View resources - >change runtime -> hardware accelerator -> select GPU -> save .This solves the issue\r\n\r\nthat actually solved my errors in colab, cheers", "\r\nafter giving command in colab:\r\n\r\n!pip install tf-nightly-gpu\r\n%tensorflow_version 2.x  \r\nimport tensorflow as tf  \r\ndevice_name = tf.test.gpu_device_name()  \r\nif device_name != '/device:GPU:0':\r\n   raise SystemError('GPU device not found')\r\nprint('Found GPU at: {}'.format(device_name))\r\n\r\n\r\n\r\ni am getting error as:\r\n\r\n\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\nRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nImportError: numpy.core.multiarray failed to import\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nSystemError                               Traceback (most recent call last)\r\nSystemError: <built-in method __contains__ of dict object at 0x7f367d7bee60> returned a result with an error set\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nImportError                               Traceback (most recent call last)\r\n[<ipython-input-9-f287495fe26c>](https://localhost:8080/#) in <module>()\r\n      2 \r\n      3 get_ipython().magic('tensorflow_version 2.x')\r\n----> 4 import tensorflow as tf\r\n      5 device_name = tf.test.gpu_device_name()\r\n      6 if device_name != '/device:GPU:0':\r\n\r\n3 frames\r\n[/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/pywrap_tf_session.py](https://localhost:8080/#) in <module>()\r\n     17 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n     18 from tensorflow.python import pywrap_tensorflow\r\n---> 19 from tensorflow.python.client._pywrap_tf_session import *\r\n     20 from tensorflow.python.client._pywrap_tf_session import _TF_SetTarget\r\n     21 from tensorflow.python.client._pywrap_tf_session import _TF_SetConfig\r\n\r\nImportError: initialization failed"]}, {"number": 34384, "title": "Re-enable GPUNMSv3 and v4 for small box count users", "body": "Reopening #34331 for master, to be CP'ed to 2.1. @mihaimaruseac @aaroey \r\n\r\nThis PR enables GPU implementation for NonMaxSuppresion ops v3 and v4 which was disabled due to high memory utilization at extremely large input counts (~8GB for 256000 boxes). Op has modest memory requirements for most models having about O(10k) inputs.", "comments": ["Leaving @aaroey to review", "@mihaimaruseac There is no reason to leave this op out of 2.1 even though google internal models which seem to be using >=256k boxes, most object detection/segmentation networks use O(10k) boxes where memory implications are negligible. Google internal models keep using CPU implementation with a device scope trivially if they don't want to use this op. I don't believe punishing other users because of a trivially solvable issue in Google internal models is right approach.", "I agree with you. However, there are 2 other aspects we need to consider:\r\n\r\n1. getting this on `master` will cause issues internally which we need to prevent/fix before this can land on master\r\n1. getting this only on `r2.1` would punish users of TF2.2, TF2.3, etc. unless someone keeps cherry-picking this on the other release branches.\r\n\r\nThat's why I removed my approval and I'm just waiting for the internal talk about the future of this to conclude. Sorry this causes significant delays.", "As Mihai says, I'm hesitant to take this into 2.1 unless we have a path forward to enable this on master.  The internal model that this breaks is RetinaNet and we want well known models like RetinaNet to keep working out of the box.", "@samikama - Do you know how much performance is enabled by this optimization?", "@tatianashp it depends on the input parameters but it is in the order of 10x to several 100x.", "@samikama  - thank you for the data.", "@tatianashp Any update on this PR? Please. Thanks!", "@sanjoy - Can you give the latest status on this PR?", "> @sanjoy - Can you give the latest status on this PR?\r\n\r\nNothing new to share so far.  Last time we tried merging this we had some google-internal models run out of memory.", "@sanjoy, Any update on this PR? Please. Thanks!", "> @sanjoy, Any update on this PR? Please. Thanks!\r\n\r\nThese kernels are now enabled at head and so this PR can be closed.", "CC @pkanwar23 "]}, {"number": 34383, "title": "Improve tf.batch_to_space documentation. Refer issue #34005", "body": "Made minor changes to make code more suitable for GCI challenge. Refer #34005 \r\nThe changes have been made in order to make the code suitable for a Google Code-In Documentation and Research challenge.\r\nThe task now is to clean the documentation so that it is more clear. There are too many code blocks in-between the text.\r\nSome examples are placed in the args section for illustration. A better way to handle that would be to add those examples in a separate \"Examples\" section (consistent with other docs) and add pointers in the \"Args\" section.\r\nSimilarly, some other code elements could be eliminated from the text by adding pointers.\r\nClarify the crops part in the \"Args\" section further.", "comments": ["@mihaimaruseac Sorry, didn't notice the r2.x branch.", "@mihaimaruseac I have a bad internet connection here. Sorry, didn't mean to re-request review after you assigned another reviewer.", "@nikochiko thank you , can you please check the sanity errors [here](https://source.cloud.google.com/results/invocations/5c77a330-068d-403f-9d59-2264e293e670/log) and fix them."]}, {"number": 34382, "title": "Made minor changes to make code more suitable for GCI challenge. Refer #34005 .", "body": "The changes have been made in order to make the code suitable for a Google Code-In Documentation and Research challenge.\r\nThe task now is to clean the documentation so that it is more clear. There are too many code blocks in-between the text.\r\nSome examples are placed in the args section for illustration. A better way to handle that would be to add those examples in a separate \"Examples\" section (consistent with other docs) and add pointers in the \"Args\" section.\r\nSimilarly, some other code elements could be eliminated from the text by adding pointers.\r\nClarify the crops part in the \"Args\" section further.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34382) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 34381, "title": "Segmentation fault (core dumped) tf 1.12.0-gpu-py3 image (works with cuda:9.0-cudnn7-devel-ubuntu16.04 and manual tf install)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no (code from https://keras.io/examples/mnist_cnn/)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu Pop!_OS 19.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): docker image 1.12.0-gpu-py\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: from Tensorflow image\r\n- GPU model and memory: GeForce RTX 2070 | 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen running the aforementioned example inside the Tensorflow `1.12.0-gpu-py3` container I get the following output:\r\n\r\n```bash\r\ndocker run -it tensorflow/tensorflow:1.12.0-gpu-py3 bash\r\nroot@da57df222465:/notebooks# cd /export/home/bartosz.miselis/code/repos/cicd/\r\nroot@da57df222465:/export/home/bartosz.miselis/code/repos/cicd# python mnist.py\r\n2019-11-18 16:57:56.234513: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-18 16:57:56.335561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-18 16:57:56.335832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.71\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.79GiB freeMemory: 7.34GiB\r\n2019-11-18 16:57:56.335847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-11-18 16:57:56.519252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-18 16:57:56.519282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2019-11-18 16:57:56.519287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2019-11-18 16:57:56.519367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7056 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\nTrain on 60000 samples, validate on 10000 samples\r\nEpoch 1/12\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe training should run smoothly.\r\n\r\n**Code to reproduce the issue**\r\nAs stated above, the code is directly accessible through the URL.\r\n\r\n**Other info / logs**\r\n`nvidia-smi` output:\r\n\r\n```bash\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.31       Driver Version: 440.31       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 2070    Off  | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   41C    P8    19W / 185W |    363MiB /  7981MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\nAdditionally, running the code inside `cuda:9.0-cudnn7-devel-ubuntu16.04` container with manual `tensorflow-gpu==1.12.0` package via pip makes it run properly.\r\n\r\nEven more strangely, the code runs perfectly fine when using `1.12.3-gpu-py3` container. I'm limited to using `1.12.0` currently (strict project requirements).", "comments": ["Thanks for your report. I'm afraid this falls outside of our official support matrix, especially because the issue appears to be fixed in the `1.12.3` container.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34381\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34381\">No</a>\n", "I met `Segmentation fault (core dumped)` too with `GeForce RTX 2070 | 8GB` and docker image `tensorflow/tensorflow:1.12.0-gpu-py3`. When I move to docker image `tensorflow/tensorflow:1.12.3-gpu-py3`, it seems failed to use the GPU (`0 Mib` usage in the output of `nvidia-smi`).\r\nBut in another machine, with `TITAN Xp` and `tensorflow/tensorflow:1.12.0-gpu-py3`, it works well.\r\n(Both running the SAME codes, which can be found [here](https://github.com/lelan-li/SSAH))", "If this issue only occurs in the official 1.12.0 TF image, then it would be nice to have updated to fix this issue... I don't think this is happening... I managed to make it work using the 1.14.0-gpu-py3 image. Thankfully I can still go back to running the final model that was trained using the previous version, since the device that I'm using is limited to the cuda version 9, and so I have to use this older TF... "]}, {"number": 34380, "title": "Why is autoencoder with tensorflow 2.0 is performing very bad compared to the same code in keras?", "body": "I am training an autoencoder on the mnist data With keras the validation loss is great (starting from 0.2687 to .1) While with tensorflow(version 2.0).keras validation loss is stuck at (.6) Even though I am using the same code.\r\n\r\nBelow is the code with keras ([you can test it in colab](https://colab.research.google.com/drive/1AD5Z_pXsaM6Ubn6Yfg0uD230J9o8AfRS)) and followed by the code with tf.keras ([you can test it in colab](https://colab.research.google.com/drive/1Hj3-_Bjfkp3-dmd38-1CmIR4VLOV98hB))\r\n\r\n```\r\nfrom keras.layers import Input, Dense\r\nfrom keras.models import Model, Sequential\r\nfrom keras.datasets import mnist\r\nimport numpy as np\r\n\r\n#Import the MNIST data, only take the images since we don't need the targets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n#Normalize and reshape images to vectors\r\nx_train = x_train.astype('float32') / 255.\r\nx_test = x_test.astype('float32') / 255.\r\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\r\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\r\nprint (x_train.shape)\r\nprint (x_test.shape)\r\n\r\n\r\ninput = Input(shape=(784,))\r\ny = Dense(64, activation='relu')(input)\r\nz = Dense(784, activation='sigmoid')(y)\r\nae = Model(input, z)\r\nencoder = Model(input, y)\r\ninput_decoder = Input(shape = (64,))\r\ndecoder_layer = ae.layers[-1]\r\ndecoder = Model(input_decoder, decoder_layer(input_decoder))\r\nae.compile(optimizer='adadelta', loss = 'binary_crossentropy')\r\nae.fit(x_train, x_train, epochs = 50, batch_size=256, shuffle=False, validation_data=(x_test, x_test))\r\n```\r\n\r\n60000/60000 [==============================] - 5s 88us/step - loss: 0.3494 - val_loss: 0.2688 Epoch 2/50 60000/60000 [==============================] - 4s 74us/step - loss: 0.2578 - val_loss: 0.2445 Epoch 3/50\r\n\r\n```\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model, Sequential\r\nfrom tensorflow.keras.datasets import mnist\r\nimport numpy as np\r\n\r\n#Import the MNIST data, only take the images since we don't need the targets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n#Normalize and reshape images to vectors\r\nx_train = x_train.astype('float32') / 255.\r\nx_test = x_test.astype('float32') / 255.\r\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\r\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\r\nprint (x_train.shape)\r\nprint (x_test.shape)\r\n\r\n\r\ninput = Input(shape=(784,))\r\ny = Dense(64, activation='relu')(input)\r\nz = Dense(784, activation='sigmoid')(y)\r\nae = Model(input, z)\r\nencoder = Model(input, y)\r\ninput_decoder = Input(shape = (64,))\r\ndecoder_layer = ae.layers[-1]\r\ndecoder = Model(input_decoder, decoder_layer(input_decoder))\r\nae.compile(optimizer='adadelta', loss = 'binary_crossentropy')\r\nae.fit(x_train, x_train, epochs = 50, batch_size=256, shuffle=False, validation_data=(x_test, x_test))\r\n\r\n```\r\nTrain on 60000 samples, validate on 10000 samples Epoch 1/50 60000/60000 [==============================] - 4s 59us/sample - loss: 0.6941 - val_loss: 0.6939 Epoch 2/50 60000/60000 [==============================] - 3s 47us/sample - loss: 0.6937 - val_loss: 0.6936\r\n", "comments": ["It turns out it's a behaviour from the optimizer. Could it be a bug? or just different initialization parameters \r\n\r\nwith tf.keras it works very well and reaches val_loss less than 1 with adam optimizer\r\n```\r\n\r\n#from keras import backend as K\r\nfrom tensorflow.keras.layers import Input, Dense, Dropout\r\nfrom tensorflow.keras.utils import plot_model, to_categorical\r\nfrom tensorflow.keras.callbacks import EarlyStopping\r\nfrom tensorflow.keras.models import Model, Sequential\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras import regularizers\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\n#Import the MNIST data, only take the images since we don't need the targets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n#Normalize and reshape images to vectors\r\nx_train = x_train.astype('float32') / 255.\r\nx_test = x_test.astype('float32') / 255.\r\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\r\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\r\nprint (x_train.shape)\r\nprint (x_test.shape)\r\n\r\n\r\ninput = Input(shape=(784,))\r\ny = Dense(64, activation='relu')(input)\r\nz = Dense(784, activation='sigmoid')(y)\r\nae = Model(input, z)\r\nencoder = Model(input, y)\r\ninput_decoder = Input(shape = (encoding_size,))\r\ndecoder_layer = ae.layers[-1]\r\ndecoder = Model(input_decoder, decoder_layer(input_decoder))\r\nae.compile(optimizer='adam', loss = 'binary_crossentropy')\r\nae.fit(x_train, x_train, epochs = 50, batch_size=256, shuffle=False, validation_data=(x_test, x_test))\r\n\r\n```\r\n\r\nTrain on 60000 samples, validate on 10000 samples\r\nEpoch 1/50\r\n60000/60000 [==============================] - 2s 28us/sample - loss: 0.2540 - val_loss: 0.1771\r\nEpoch 2/50\r\n60000/60000 [==============================] - 1s 20us/sample - loss: 0.1578 - val_loss: 0.1418", "@ahmedelmahy \r\nI tried the code with keras using TF 1.15 and validation loss is converging  (starting from 0.2687 to .1). However in TF 2.0 using keras gives the below error.\r\n`RuntimeError: It looks like you are trying to use a version of multi-backend Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14`.\r\nI tried the given code using tf.keras using TF 2.0 against adadelta and adam and yes 'adadelta' has much worse performance compared to adam (as you can see [here](https://colab.sandbox.google.com/gist/ravikyram/abf3f37d69beb90346d2aa7a05f88f70/ae-in-tf-keras.ipynb))and this can be explained [here](https://www.reddit.com/r/MachineLearning/comments/3y84hr/how_does_adam_compare_to_adadelta/).Please correct me if you think otherwise.Thanks!", "@ahmedelmahy \r\n\r\nAny update on this issue please?. Thanks!", "@ravikyram  thanks for the mention. That seems correct that adam could be faster than adadelta\r\n[reddit](https://www.reddit.com/r/MachineLearning/comments/3y84hr/how_does_adam_compare_to_adadelta/)\r\n\r\n\r\nBut I still kind of don't understand why \"adadelta\" is behaving in a different way in tf.keras as compared to the separate keras  package.\r\nFor now I am using Adam and things are working fine.", "@ahmedelmahy I think this is probably due to we changed the default learning rate (unfortunately) from 1.0 to 0.001. Can you try it and let us know if it fixes for you?", "That works like a charm @tanzhenyu  . [Setting the learning rate ](https://colab.research.google.com/drive/1Hj3-_Bjfkp3-dmd38-1CmIR4VLOV98hB) to 1.0 produces the same results as with keras. \r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34380\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34380\">No</a>\n"]}, {"number": 34379, "title": "Memory leak in custom tensors", "body": "Hello everyone,\r\n\r\ncc: @alextp \r\n\r\nThere is a memory leak for custom tensors from https://github.com/tensorflow/tensorflow/issues/24865. You can find an MWE here: https://colab.research.google.com/drive/1v-PEv3-qubszfK0gFS2RLhcP6qbDUvIK. Long story short. In GPflow we define parameters of the model as custom tensors (variables) https://github.com/GPflow/GPflow/blob/develop/gpflow/base.py#L36. The parameter is a `tf.Module` container with single tf.Variable and TFP bijector inside. The custom tensor behaves as a standard TensorFlow tensor, **plus** it applies transformation on internal `tf.Variable` and returns forward transformed Tensor of the variable every-time when someone decides to read it (user, tensorflow function).\r\n\r\nWhen I use a model with Parameter class and compute gradients w.r.t. the loss, I observe constant memory growth. And when I use the model with same variables but do transformation of the variables manually the memory stays the same.\r\n\r\nIf I do forward mode only, there are no issues. It happens only when `GradientTape` is involved.\r\n\r\nPS: memory profiler doesn't work in colab, you will have to run it on your computer.\r\n\r\n- macOS 10.14.6\r\n- Python 3.7.0\r\n- TensorFlow version:\r\n```bash\r\n\u2192 python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\nv2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n```\r\n\r\nMiddle of the training:\r\n\r\n```python\r\nCompute gradient\r\nIteration 56 has passed with loss = 494609555730402.0\r\nFilename: mwe2.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n   118    484.5 MiB    484.5 MiB   @profile\r\n   119                             def train_step(compute_grads: bool = True):\r\n   120    484.6 MiB      0.1 MiB       data = next(dataset_it)\r\n   121    484.6 MiB      0.0 MiB       variables = model.trainable_variables\r\n   122    484.6 MiB      0.0 MiB       with tf.GradientTape(watch_accessed_variables=False) as tape:\r\n   123    484.6 MiB      0.0 MiB           tape.watch(variables)\r\n   124    490.6 MiB      6.1 MiB           loss = loss_fn(data)\r\n   125\r\n   126    490.6 MiB      0.0 MiB       if compute_grads:\r\n   127    490.6 MiB      0.0 MiB           tf.print(f\"Compute gradient\")\r\n   128    491.2 MiB      0.6 MiB           grads = tape.gradient(loss, variables)\r\n   129    491.2 MiB      0.0 MiB           grads_and_vals = zip(grads, variables)\r\n   130    491.2 MiB      0.0 MiB           opt.apply_gradients(grads_and_vals)\r\n   131\r\n   132    491.2 MiB      0.0 MiB       tf.print(f\"Iteration {opt.iterations.numpy()} has passed with loss = {loss.numpy()}\")\r\n```\r\n\r\nIn the end:\r\n```python\r\nCompute gradient\r\nIteration 100 has passed with loss = 273947779893557.2\r\nFilename: mwe2.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n   118    514.8 MiB    514.8 MiB   @profile\r\n   119                             def train_step(compute_grads: bool = True):\r\n   120    514.9 MiB      0.1 MiB       data = next(dataset_it)\r\n   121    514.9 MiB      0.0 MiB       variables = model.trainable_variables\r\n   122    514.9 MiB      0.0 MiB       with tf.GradientTape(watch_accessed_variables=False) as tape:\r\n   123    514.9 MiB      0.0 MiB           tape.watch(variables)\r\n   124    520.9 MiB      6.1 MiB           loss = loss_fn(data)\r\n   125\r\n   126    520.9 MiB      0.0 MiB       if compute_grads:\r\n   127    520.9 MiB      0.0 MiB           tf.print(f\"Compute gradient\")\r\n   128    521.5 MiB      0.6 MiB           grads = tape.gradient(loss, variables)\r\n   129    521.5 MiB      0.0 MiB           grads_and_vals = zip(grads, variables)\r\n   130    521.5 MiB      0.0 MiB           opt.apply_gradients(grads_and_vals)\r\n   131\r\n   132    521.5 MiB      0.0 MiB       tf.print(f\"Iteration {opt.iterations.numpy()} has passed with loss = {loss.numpy()}\")\r\n```", "comments": ["@kkimdev Do you know if this is related to the leak you've been investigating?", "It seems like it's using tf.data? There were some recent tf.data memory leak fixes https://github.com/tensorflow/tensorflow/commit/082415b2ff49bfb8890f7d5361585bac04749add https://github.com/tensorflow/tensorflow/commit/c2fc448fe253bc59d3f0417d7d08e16d53f2a856 so I encourage to try on tf-nightly or tf-nightly-gpu .", "@alextp, @kkimdev I replaced data with tensors and installed `tf-nightly` - still the issue is present.", "Looks like it's leaking through tensorflow_probability's FillTriangular class and bijector.  I'm not sure if I'll have time to dig this deeper soon.  I attached the script I used to profile and it's outputs below.\r\n\r\nhttps://colab.research.google.com/gist/kkimdev/f69960cb84d05a8d865eaadf44f47967/memory-leak-in-custom-tensor.ipynb\r\n\r\n```\r\n=======================================================================\r\nType                     Old_ids  Current_ids      New_ids Count_Deltas\r\n=======================================================================\r\nHashableWeakRef              565          604          +39          +39\r\nDimension                    601          639          +38          +38\r\ntuple                      38841        38870          +30          +29\r\nlist                       20449        20470          +23          +21\r\ncell                       14187        14209          +22          +22\r\nfunction                   76122        76142          +20          +20\r\ndict                       52279        52298          +19          +19\r\n_Mapping                     269          288          +19          +19\r\nTensorShape                  338          357          +19          +19\r\nEagerTensor                  322          341          +19          +19\r\nframe                        126          136          +10          +10\r\nmethod                      1397         1400           +3           +3\r\nStringIO                       3            3           +1           +0\r\nNullContext                    1            2           +1           +1\r\nzipf_gen                       1            1           +0           +0\r\nyulesimon_gen                  1            1           +0           +0\r\nwrapper_descriptor          3583         3583           +0           +0\r\nwrapcauchy_gen                 1            1           +0           +0\r\nwords                         12           12           +0           +0\r\nwishart_gen                    1            1           +0           +0\r\nweibull_min_gen                1            1           +0           +0\r\nweibull_max_gen                1            1           +0           +0\r\nweekday                       14           14           +0           +0\r\nweakref                    19679        19679           +0           +0\r\nweakproxy                     12           12           +0           +0\r\nwald_gen                       1            1           +0           +0\r\nvonmises_gen                   2            2           +0           +0\r\nvectorize                    454          454           +0           +0\r\nvalidate_nseq_int              1            1           +0           +0\r\nvalidate_nseq_float            5            5           +0           +0\r\n=======================================================================\r\n```\r\n![image](https://user-images.githubusercontent.com/503414/69096361-1dd98700-0a09-11ea-9602-d254be7f0d41.png)\r\n", "@jvdillon have you seen this leak before?", "We definitely used to leak memory but switched to weakref dict. Given the intrinsic complexity here, my guess is we have a bug. Ill see if someone on the team is willing to dig in.", "@alextp , @jvdillon , @kkimdev Thanks everyone for quick responses. Welcome @csuter ;)", "This is a TFP issue, which we're presently resolving. Closing this (please follow https://github.com/tensorflow/probability/issues/647 for TFP updates)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34379\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34379\">No</a>\n"]}, {"number": 34378, "title": "Identifying activating neurons for specific classes", "body": "Hi all, hope this question is ok placed here! any help would be much appreciated!\r\n\r\nI have a question i am hoping somebody might be able to help with. I have trained a sequential model (keras) that attempts to understand multiple labels in a data set. There are hundreds of samples (more samples to follow), and each individual sample will have multiple labels (an organ, an age and sex).\r\n\r\nthe model performs really well and is able to understand every concept above (age sex and organ), with really high performance. I would like to know which nodes in the network interact with one another, especially, which samples in the network interact with one another (i.e, are the features that activate for example, a brain and lung if your male at a specific age).\r\n\r\nFor this reason, I have taken the activation's of every node in the hidden layers and I attach a UMAP to this post. There is a slight problem because the most common activation are actually organ specific. This makes sense as the concept of organ is very strong, so it not surprising that this is the easiest class to pick apart. The problem is is that when I use dimensionality reduction on the node activation, the nodes that separate most closely cluster according to organ, again because this is the class that has an overwhelming signature.\r\n\r\nThis is even the case in the penultimate layer (image attached)...\r\n\r\nMy question is, is there a way of identifying and removing the neurons that lead specifically to organ decisions so that i can then repeat the dimensionality reduction in the intermediate layer to look for relationships in the data?\r\n\r\nIf i plot a normal heatmap i can see where activation's are shared and which samples where they are shared, but i would also like to use dimensionality reduction to view which neurons activate in a similar manner and what samples share features beyond just the overwhelming organ signature.\r\n\r\ni am happy to provide code if required!\r\n\r\nmany thanks!\r\n![umap](https://user-images.githubusercontent.com/33659783/69063806-0902f080-0a15-11ea-9692-9ca361b583dc.png)\r\n", "comments": ["@amjass12 Please post these questions in stackoverflow as github is meant for build/install, bug/performance, doc and feature request related issues. Thanks!"]}, {"number": 34377, "title": "TFLite: Added support for ZeroPadding3D, AveragePooling3D & MaxPooling3D in OpenCL", "body": "A step towards solving [this](https://github.com/tensorflow/tensorflow/issues/34010) issue.\r\nThis pull request was sent to get early feedback from the dev team.", "comments": ["I have just realized that my changes to pad.cc and pad_test.cc are not required and better be reverted. However, I originally made them because when you enable the GPU delegate, it first prepares the CPU kernels [here](https://github.com/guydavid/tensorflow/blob/lite5d/tensorflow/lite/core/subgraph.cc#L1144). Can you explain this step?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34377) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34377) for more info**.\n\n<!-- ok -->", "@guydavid Can you please resolve conflicts? Thanks!", "I have completely resolved the original issue linked in the description here with full OpenCL support. However, I don't see it going through.\r\nThere are some adjustments to be made to keep compatibility with older TF-Lite schema's as the new one has completely changed. Seeing the zero progress with other one-liner pull requests that I have made (this pull request has 5000+ changed lines) does not add incentives so I will be dropping this for now.", "@guydavid can you link to the other PRs? We're in the process of restructuring how we assign/prioritize PRs, so apologies if anything has fallen through the cracks. Happy to help push those through.\r\n\r\nFor this specific case, it's not that we don't want this, but adding a new op is fairly non-trivial at the moment (particularly as we're working on a new converter), and generally prefer having the conversion/CPU path in place before we extend it to accelerators."]}, {"number": 34376, "title": "RunTimeError: DST Tensor Not initialised - while saving checkpoint, not while training.", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: \r\nnvidia-driver-418\r\ncuda-10-0\r\nlibcudnn7=7.6.2.24-1+cuda10.0\r\nlibcudnn7-dev=7.6.2.24-1+cuda10.0\r\nlibnvinfer5=5.1.5-1+cuda10.0\r\nlibnvinfer-dev=5.1.5-1+cuda10.0\r\n\r\n- GPU model and memory: T4 16GB\r\n\r\n**Describe the current behavior**\r\n\r\nThe GPU memory is only used for 58%, detected by using limit_growth option:\r\nGPU Total memory:  15843721216\r\nGPU Free memory:  6628966400\r\nGPU Used memory:  9214754816\r\nGPU Memory Percentage Used:  58\r\n\r\n```\r\n    if gpus:\r\n      try:\r\n        # Currently, memory growth needs to be the same across GPUs\r\n        for gpu in gpus:\r\n          tf.config.experimental.set_memory_growth(gpu, True)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n      except RuntimeError as e:\r\n        # Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\n```\r\n\r\nTraining runs fine, but only when saving the model as a checkpoint, it crashes:\r\n```\r\n\r\n        def createCheckpointManager(self):\r\n\r\n            if self.model_weight_path:\r\n                checkpoint_dir = self.model_weight_path + '/' + self.training_session_id + '_checkpoint/ckpt'\r\n\r\n                print(\"Store Checkpoint: \",checkpoint_dir)\r\n\r\n                self.checkpoint = tf.train.Checkpoint(optimizer=self.optimizer,\r\n                                                 encode_network=self.encode_network,\r\n                                                 decode_network=self.decode_network)\r\n\r\n                self.manager = tf.train.CheckpointManager(self.checkpoint,checkpoint_dir, max_to_keep=3)\r\n            else:\r\n                print(\"Model checkpoint path not found, not saving checkpoint.\")\r\n\r\nCheckpointManager.save()\r\n```\r\n\r\nLog:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/[]/[]/[]/v1/[]/[].py\", line 107, in runTrainingEpoch\r\n    [].storeCheckpoint()\r\n  File \"/home/[]/[]/[]/v1/[]/[].py\", line 179, in storeCheckpoint\r\n    self.manager.save()\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/checkpoint_management.py\", line 720, in save\r\n    save_path = self._checkpoint.write(prefix)\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1819, in write\r\n    output = self._saver.save(file_prefix=file_prefix)\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1155, in save\r\n    file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1103, in _save_cached_when_graph_building\r\n    save_op = saver.save(file_prefix)  \r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/saving/functional_saver.py\", line 230, in save\r\n    sharded_saves.append(saver.save(shard_prefix))\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/saving/functional_saver.py\", line 69, in save\r\n    tensors.append(spec.tensor)\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object.py\", line 52, in tensor\r\n    return self._tensor() if callable(self._tensor) else self._tensor\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 94, in f\r\n    return array_ops.identity(x)\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\", line 209, in identity\r\n    copied = input._copy()  # pylint: disable=protected-access\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1015, in _copy\r\n    new_tensor = self._copy_nograd(ctx, device_name)\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1008, in _copy_nograd\r\n    new_tensor = self._copy_to_device(device_name)\r\nRuntimeError: Dst tensor is not initialized.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThe model is the exact implementation of the CVAE tutorial from the Tensorflow website:\r\n```\r\nself.optimizer = tf.keras.optimizers.Adam(1e-4)\r\n\r\n       self.encode_network = tf.keras.Sequential(\r\n              [\r\n                  tf.keras.layers.InputLayer(input_shape=(self.width, self.height, 3)),\r\n                  tf.keras.layers.Conv2D(filters=32, kernel_size=4, strides=(2, 2), activation='relu'),\r\n                  tf.keras.layers.Conv2D(filters=64, kernel_size=2, strides=(2, 2), activation='relu'),\r\n                  tf.keras.layers.Flatten(),\r\n                  tf.keras.layers.Dense(self.latent_dim + self.latent_dim),\r\n              ]\r\n            )\r\n\r\n            # Deconvolution 2D Transpose Formula:\r\n            # Output Shape = Input Shape * stride\r\n\r\n            self.decode_network = tf.keras.Sequential(\r\n                [\r\n                  tf.keras.layers.InputLayer(input_shape=(self.latent_dim,)),\r\n                  tf.keras.layers.Dense(units=64 * 64 * 128, activation=tf.nn.relu),\r\n                  tf.keras.layers.Reshape(target_shape=(64, 64, 128)),\r\n                  tf.keras.layers.Conv2DTranspose(\r\n                      filters=64,\r\n                      kernel_size=3,\r\n                      strides=(2, 2),\r\n                      padding=\"SAME\",\r\n                      activation='relu'),\r\n                  tf.keras.layers.Conv2DTranspose(\r\n                      filters=32,\r\n                      kernel_size=3,\r\n                      strides=(2, 2),\r\n                      padding=\"SAME\",\r\n                      activation='relu'),\r\n                  tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=2, strides=(1, 1), padding=\"SAME\")\r\n                ]\r\n            )\r\n```\r\n\r\nBecause the layer size is increased, I suspect it has something to do with memory, because earlier issues refer the 'DST tensor' issue to a memory problem with the GPU.\r\nHowever this is not possible because only 58% is utilised and training runs fine, therefore it seems there is an issue with saving the checkpoint.\r\n", "comments": ["@igorhoogerwoord Can you share a github gist of the issue preferably on colab. Thanks!", "@gowthamkpr thanks for getting back on this, what would you need me to share specifically in the gist or colab?", "@igorhoogerwoord Please try Reproducing your issue [here](https://colab.sandbox.google.com/notebooks/welcome.ipynb). May be its related to RAM issue. Thanks!", "Haven't had time to reproduce it in colab, but one pointer to what made this issue disappear may have been that I had too little RAM on the workstation. After testing the same code on a server with more RAM the issue wasn't there.", "Great good to know that. I am gonna close this issue as it has been resolved. Please add additional comments and we can open this issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34376\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34376\">No</a>\n", "Is it related to RAM however?\r\nIs the runtime error the right way to indicate a memory issue?\r\nWhy does it appear during saving the checkpoint and not during training?"]}, {"number": 34375, "title": "Run multi-worker with nccl error: NET/IB : collective mismatch error", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `tensorflow/models`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  `ubuntu 18.04.3`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  `tensorflow:2.0.0-gpu` (docker image)\r\n- TensorFlow version (use command below):  `2.0.0`\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: `10.1/7.6.0.64`\r\n- NCCL version: `2.5.4`\r\n- GPU model and memory:  Nvidia P40\r\n\r\n**Describe the current behavior**\r\n\r\n* worker A\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=0 NCCL_DEBUG=INFO  NCCL_IB_GID_INDEX=3 NCCL_IB_HCA=mlx5_1:1 NCCL_IB_SL=3 NCCL_SOCKET_IFNAME=eth1 python -m resnet_imagenet_main --model_dir=/tmp/model_dir/resnet  --num_gpus=1   --batch_size=32  --use_synthetic_data=true --worker_hosts=A:2222,B:2222  --task_index=0 --distribution_strategy=multi_worker_mirrored --all_reduce_alg=nccl\r\n```\r\n* worker B\r\n```\r\nCUDA_VISIBLE_DEVICES=0 NCCL_DEBUG=INFO  NCCL_IB_GID_INDEX=3 NCCL_IB_HCA=mlx5_1:1 NCCL_IB_SL=3 NCCL_SOCKET_IFNAME=eth1 python -m resnet_imagenet_main --model_dir=/tmp/model_dir/resnet  --num_gpus=1   --batch_size=32  --use_synthetic_data=true --worker_hosts=A:2222,B:2222 --task_index=1 --distribution_strategy=multi_worker_mirrored --all_reduce_alg=nccl\r\n```\r\n\r\nThe error as follows:\r\n\r\n```\r\n...\r\n2019-11-18 10:26:43.012421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n:499:967 [0] NCCL INFO NET/Socket : Using [0]eth1:100.x.x.x<0>\r\n:499:967 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n2019-11-18 10:26:43.248455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n:499:967 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE ; OOB eth1:100.x.x.x<0>\r\n:499:978 [0] NCCL INFO Setting affinity for GPU 0 to 03,fffff000,003fffff\r\n:499:978 [0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  PHB\r\n:499:978 [0] NCCL INFO Ring 00 : 0 -> 1 [receive] via NET/IB/0\r\n:499:978 [0] NCCL INFO Ring 00 : 1 -> 0 [send] via NET/IB/0\r\n:499:978 [0] NCCL INFO NCCL_IB_GID_INDEX set by environment to 3.\r\n:499:978 [0] NCCL INFO NCCL_IB_SL set by environment to 3.\r\n:499:978 [0] NCCL INFO comm 0x7f3e640025b0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\n\r\n:499:979 [0] external/nccl_archive/src/transport/net_ib.cc:651 NCCL WARN NET/IB : collective mismatch error local size 1048576 remote 32768 addr 7f7563811000 rkey 103480 seq 2/2\r\n:499:979 [0] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:31 -> 3\r\n:499:979 [0] NCCL INFO external/nccl_archive/src/transport/net.cc:470 -> 3\r\n:499:979 [0] NCCL INFO external/nccl_archive/src/transport.cc:163 -> 3 [Proxy Thread]\r\n...\r\n```", "comments": ["@hustcat ,\r\nHi, Can you please provide more info on the issue reported here? Thanks!", "@oanush Of course, what infomation do you need? I'v run horovod  successfully with NCCL/RDMA success on the same hardware environment.", "I doubt 2.5.4 is your actual NCCL version. AFAIK TF bundled NCCL is still 2.4.7: https://github.com/tensorflow/tensorflow/blob/master/third_party/nccl/archive.patch ", "@byronyi Yeah, you are right, It seems that TF use the NCCL 2.4.7 actually:\r\n\r\n```\r\n2019-11-18 10:27:33.545262: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\nNCCL version 2.4.7+cudaCUDA_MAJOR.CUDA_MINOR\r\n...\r\n```\r\n\r\nBut I'v installed the NCCL 2.5.4 base on the docker image `tensorflow/tensorflow:2.0.0-gpu`:\r\n\r\n```\r\n# dpkg -l | grep nccl\r\nii  libnccl-dev                      2.5.4-1+cuda10.1                        amd64        NVIDIA Collectives Communication Library (NCCL) Development Files\r\nii  libnccl2                         2.5.4-1+cuda10.1                        amd64        NVIDIA Collectives Communication Library (NCCL) Runtime\r\n```", "Only libnccl.so 2.5.4 exist in the container:\r\n\r\n```\r\n# find / -name \"libnccl.so*\"\r\n/usr/lib/x86_64-linux-gnu/libnccl.so\r\n/usr/lib/x86_64-linux-gnu/libnccl.so.2\r\n/usr/lib/x86_64-linux-gnu/libnccl.so.2.5.4\r\n\r\n# ls /usr/lib/x86_64-linux-gnu/libnccl.so* -l\r\nlrwxrwxrwx 1 root root       12 Oct 19 01:33 /usr/lib/x86_64-linux-gnu/libnccl.so -> libnccl.so.2\r\nlrwxrwxrwx 1 root root       16 Oct 19 01:33 /usr/lib/x86_64-linux-gnu/libnccl.so.2 -> libnccl.so.2.5.4\r\n-rw-r--r-- 1 root root 70714672 Oct 19 01:33 /usr/lib/x86_64-linux-gnu/libnccl.so.2.5.4\r\n```", "should I install the NCCL 2.4.7 library with TF? I don't understand why output the log `NCCL version 2.4.7+cudaCUDA_MAJOR.CUDA_MINOR`?", "I remove the NCCL 2.5.4  package, and no IB error reported. But the test program  seems  hangup after NCCL initial, and nothing to output:\r\n```\r\n...\r\n2019-11-20 11:43:26.985771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\nNCCL version 2.4.7+cudaCUDA_MAJOR.CUDA_MINOR\r\n2019-11-20 11:43:27.369942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n:5202:5683 [0] NCCL INFO Setting affinity for GPU 0 to 03,fffff000,003fffff\r\n:5202:5683 [0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  PHB\r\n:5202:5683 [0] NCCL INFO Channel 00 :    0   1\r\n:5202:5683 [0] NCCL INFO Ring 00 : 1 -> 0 [receive] via NET/IB/0\r\n:5202:5683 [0] NCCL INFO Ring 00 : 0 -> 1 [send] via NET/IB/0\r\n:5202:5683 [0] NCCL INFO NCCL_IB_GID_INDEX set by environment to 3.\r\n:5202:5683 [0] NCCL INFO NCCL_IB_SL set by environment to 3.\r\n:5202:5683 [0] NCCL INFO Using 256 threads, Min Comp Cap 6, Trees disabled\r\n:5202:5683 [0] NCCL INFO comm 0x7f94740025b0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\n:5202:5682 [0] NCCL INFO Launch mode Parallel\r\n```\r\n\r\nno more log to output.", "It's bundled in TF, i.e. you do not need to install a separate NCCL library on your system path.\r\n\r\nThe program hang may caused by other issues in your environment. Could you try cuda-gdb attaching to your process and send us a stack trace of your threads?", "@hustcat could you try NCCL_IB_DISABLE=1 to see if it's a TF-side issue? \r\n\r\nPing @dubey; not sure if you have IB available for testing. I could help if we pin down the issue to RDMA.", "I don't have access to IB.  @hustcat it would be good to understand if setting `NCCL_IB_DISABLE=1` as @byronyi suggests fixes the issue.  If you can also run with `TF_CPP_VMODULE=\"nccl_manager=2\"` you will get additional logs from TF that can help debug this issue.", "@byronyi @dubey Yeah\uff0c with `NCCL_IB_DISABLE=1`\uff0c I can run the test successfully.\r\n\r\n```\r\n...\r\n2019-11-25 03:14:43.785261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-25 03:14:44.148612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\nINFO:tensorflow:BenchmarkMetric: {'global step':100, 'time_taken': 30.711297,'examples_per_second': 104.196185}\r\nI1125 03:15:05.992312 140480118101824 keras_utils.py:85] BenchmarkMetric: {'global step':100, 'time_taken': 30.711297,'examples_per_second': 104.196185}\r\nINFO:tensorflow:BenchmarkMetric: {'global step':200, 'time_taken': 20.167266,'examples_per_second': 158.672971}\r\nI1125 03:15:26.159527 140480118101824 keras_utils.py:85] BenchmarkMetric: {'global step':200, 'time_taken': 20.167266,'examples_per_second': 158.672971}\r\n...\r\n```", "@dubey  With `TF_CPP_VMODULE=\"nccl_manager=2\"`,  return error `NCCL WARN NET/IB : Got completion with error 12, opcode 1, len 0, vendor err 129`:\r\n```\r\n...\r\n2019-11-25 03:31:18.766128: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\nNCCL version 2.4.7+cudaCUDA_MAJOR.CUDA_MINOR\r\n2019-11-25 03:31:19.004075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n:9099:9580 [0] NCCL INFO Setting affinity for GPU 0 to 03,fffff000,003fffff\r\n:9099:9580 [0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  PHB\r\n:9099:9580 [0] NCCL INFO Channel 00 :    0   1\r\n:9099:9580 [0] NCCL INFO Ring 00 : 1 -> 0 [receive] via NET/IB/0\r\n:9099:9580 [0] NCCL INFO Ring 00 : 0 -> 1 [send] via NET/IB/0\r\n:9099:9580 [0] NCCL INFO Using 256 threads, Min Comp Cap 6, Trees disabled\r\n:9099:9580 [0] NCCL INFO comm 0x7f51300025b0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\n2019-11-25 03:31:19.432910: I tensorflow/core/nccl/nccl_manager.cc:603] call NcclAllReduce collective_key 537:0:0:0 participant 0 sendbuff 0x7f519a2b4600 recvbuff 0x7f519ce49100 nccl_comm 0x7f51300025b0 comm_stream 0x7f51580c3160 cuda_stream 0x7f51580c3140\r\n:9099:9579 [0] NCCL INFO Launch mode Parallel\r\n\r\n:9099:9581 [0] external/nccl_archive/src/transport/net_ib.cc:789 NCCL WARN NET/IB : Got completion with error 12, opcode 1, len 0, vendor err 129\r\n:9099:9581 [0] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:34 -> 2\r\n:9099:9581 [0] NCCL INFO external/nccl_archive/src/transport/net.cc:533 -> 2\r\n:9099:9581 [0] NCCL INFO external/nccl_archive/src/transport.cc:163 -> 2 [Proxy Thread]\r\n```", "@hustcat That appears strictly a networking issue.\r\n\r\nAdd my WeChat (same ID as on GitHub) so we can discuss offline.", "Is this still an issue?", "I'm having a similar issue. The training stalled after this message shows up on one of nodes (it's somewhat random which node it shows up on):\r\n\r\n```\r\nhost2:14508:15205 [0] external/nccl_archive/src/transport/net_ib.cc:651 NCCL WARN NET/IB : collective mismatch error local size 348160 remote 32768 addr 7fb47b011000 rkey baa8 seq 5a/5a\r\n2020-01-11 00:35:35.443989: I <redacted>/tensorflow/core/nccl/nccl_manager.cc:670] done Nccl kernel collective_key 118:0:0:0 participant 0 ncclResult 0\r\n2020-01-11 00:35:35.443996: I <redacted>/tensorflow/core/nccl/nccl_manager.cc:670] done Nccl kernel collective_key 118:0:0:0 participant 1 ncclResult 0\r\nhost2:14508:15205 [0] NCCL INFO <redacted>/external/nccl_archive/_virtual_includes/include_hdrs/net.h:31 -> 3\r\nhost2:14508:15205 [0] NCCL INFO external/nccl_archive/src/transport/net.cc:470 -> 3\r\nhost2:14508:15205 [0] NCCL INFO external/nccl_archive/src/transport.cc:163 -> 3 [Proxy Thread]\r\n2020-01-11 00:35:35.444166: I <redacted>/tensorflow/core/nccl/nccl_manager.cc:603] call NcclAllReduce collective_key 119:0:0:0 participant 1 sendbuff 0x7f29dc153700 recvbuff 0x7f29dc564300 nccl_comm 0x7f2948000e70 comm_stream 0x7f2eec009ce0 cuda_stream 0x7f2eec009d90\r\n2020-01-11 00:35:35.444177: I <redacted>/tensorflow/core/nccl/nccl_manager.cc:603] call NcclAllReduce collective_key 119:0:0:0 participant 0 sendbuff 0x7f2c70153d00 recvbuff 0x7f2c706ea800 nccl_comm 0x7f29440018c0 comm_stream 0x7f2eec000d10 cuda_stream 0x7f2eec000dc0\r\n```\r\n\r\nFull logs are at\r\nhost1: https://gist.github.com/guoshimin/fe3bf21d79ff97203d6d7bd3d3b922da\r\nhost2: https://gist.github.com/guoshimin/45635b0562eb162174b2f868c00033df\r\n\r\nI'm using tensorflow 2.0.0, with NCCL that's bundled with TF.", "I suggest you to take a look to the `NCCL_IB_GID_INDEX` environment variable if you would like to use RDMA. Otherwise, try `NCCL_IB_DISABLE=1`.", "This is the output of show_gids on one host. The other host has similar output.\r\n```\r\n$ show_gids\r\nDEV\tPORT\tINDEX\tGID\t\t\t\t\tIPv4  \t\tVER\tDEV\r\n---\t----\t-----\t---\t\t\t\t\t------------  \t---\t---\r\nmlx5_0\t1\t0\tfe80:0000:0000:0000:9a03:9bff:fe90:be32\t\t\tv1\teth0\r\nmlx5_0\t1\t1\tfe80:0000:0000:0000:9a03:9bff:fe90:be32\t\t\tv2\teth0\r\nmlx5_0\t1\t2\t0000:0000:0000:0000:0000:ffff:0a41:06b6\t10.65.6.182  \tv1\teth0\r\nmlx5_0\t1\t3\t0000:0000:0000:0000:0000:ffff:0a41:06b6\t10.65.6.182  \tv2\teth0\r\nmlx5_1\t1\t0\tfe80:0000:0000:0000:9a03:9bff:fe90:be33\t\t\tv1\teth1\r\nmlx5_1\t1\t1\tfe80:0000:0000:0000:9a03:9bff:fe90:be33\t\t\tv2\teth1\r\nn_gids_found=6\r\n```\r\nI tried setting `NCCL_IB_GID_INDEX` to 2 and then 3, same result.\r\n\r\nI also tried setting `NCCL_IB_DISABLE=1`, and got a slightly different log message before training stalled:\r\n```\r\nhost2:18503:19204 [0] external/nccl_archive/src/transport/net_socket.cc:200 NCCL WARN NET/Socket : message truncated : receiving 348160 bytes instead of 32768\r\nhost2:18503:19204 [0] NCCL INFO <redacted>/external/nccl_archive/_virtual_includes/include_hdrs/net.h:34 -> 3\r\nhost2:18503:19204 [0] NCCL INFO external/nccl_archive/src/transport/net.cc:533 -> 3\r\nhost2:18503:19204 [0] NCCL INFO external/nccl_archive/src/transport.cc:163 -> 3 [Proxy Thread]\r\n```", "Your network env may not function properly. Please consult your vendor Mellanox for further information.", "We've been using MultiWorkerMirroredStrategy with NCCL and IB successfully with tf.estimator with TF 1.14. This problem only shows up when we use tf.keras with TF 2.0. (The code I'm running is from the official tutorial https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) Does it make it less likely to be a problem with the network environment?", "I will suggest you to test with nightly or 2.1 first. Last time I checked it was running fine. Let me know if you could reproduce the issue with 2.1 or nightly. Thanks!", "Thanks. I will try 2.1.\n\nOn Sat, Jan 11, 2020, 7:04 AM Bairen Yi <notifications@github.com> wrote:\n\n> I will suggest you to test with nightly or 2.1 first. Last time I checked\n> it was running fine. Let me know if you could reproduce the issue with 2.1\n> or nightly. Thanks!\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34375?email_source=notifications&email_token=ACZ5YHNWQMOQYUN54KOVHDLQ5HNYFA5CNFSM4JOSCXC2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEIWDZEQ#issuecomment-573324434>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACZ5YHKGIZ5UNZ75L2GLBGTQ5HNYFANCNFSM4JOSCXCQ>\n> .\n>\n", "Keras + MultiWorkerMirroredStrategy + NCCL + IB works fine in 2.1. Looks like there is a bug in 2.0."]}, {"number": 34374, "title": "TFLite android example custom model error: java.lang.ArrayIndexOutOfBoundsException: length=10; index=-2147483646", "body": "Hi @achowdhery ,\r\n\r\nI'm getting this error while placing my custom model in tensorflow lite android example. I've gone through the post which resolves the same \r\nint labelOffset = 1;\r\n      recognitions.add(\r\n          new Recognition(\r\n              \"\" + i,\r\n              labels.get((int) outputClasses[0][i] + labelOffset),\r\n              outputScores[0][i],\r\n              detection));\r\n\r\nthis is the part which is throwing the error. But i'm not getting how to resolve it.\r\n\r\nI annotated images using labelimg tool with class names as parallel_parking and perpendicular parking. Then while creating tf.record I did this:\r\n\r\nitem {\r\n  id: 1\r\n  name: 'parallel_parking'\r\n}\r\nitem {\r\n  id: 2\r\n  name: 'perpendicular_parking'\r\n}\r\n\r\nand followed the steps mentioned here https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193.\r\n\r\nnow, while running the tflite app, i'm getting the above mentioned error. Need help.", "comments": ["@shauryad15 Sorry for late response. Is it still an issue? Can you check whether this [resource](https://github.com/tensorflow/tensorflow/issues/22106#issuecomment-428409506) will help you in resolving your issue? If it is not resolving, please share a standalone code to reproduce your issue. Thanks!\r\n", "@shauryad15 Is it still an issue? Please check the resource in my last response. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@jvishnuvardhan   I'm also facing this problem while running custom model. \r\n\r\nI'm integrating Custom Nail Detection Segmentation model with android app.\r\n\r\n[Model tflite file link:](https://github.com/makeml-app/MakeML-Nails/tree/master/Segmentation%20Nails/Resources)\r\n\r\n[Android Code which I'm using](https://github.com/sercant/android-segmentation-app) \r\n\r\nAbove android code link works well with the model which is give but whenever I use MakeML nail detection model app crash with the error message.\r\n\r\n`java.lang.ArrayIndexOutOfBoundsException: length=66049; index=66049` \r\n\r\nError in bellow while loop.\r\n`while (segmentedImage.hasRemaining()){\r\n                outFrame[i++] = segmentedImage.int\r\n        }` \r\n\r\n\r\nI have print the value of outframe use `outframe.size` and this value is 66049.\r\n \r\nAlready spend full week on this issue but no luck. Please guide me how to resolve this issue ?", "@shauryad15 Can you please create a new issue with the details you mentioned above and any standalone code to reproduce the issue? Thanks!", "I have created separate issue on [this](https://github.com/tensorflow/tensorflow/issues/36956) link.\r\n\r\nI have upload the stand alone code on [this](https://drive.google.com/open?id=1WQ9LmAqJpxMwZIHtzMxRP2j6nq1Ga2lD) link which produce this error.\r\n\r\nThis code is working fine with other models which come with the code.", "Hi @Ehtasha/ @jvishnuvardhan  ,\r\n\r\nThis issue generally occurs when you're trying to read the index which isn't not present. TO solve this, please check your labels file and see whether you're accessing the correct index of the label or not. If you try to access the memory which isn't allocated, you'll get the error.\r\n\r\nFor ex: label 0 is Cat, label 1 is Dog. If access label 2 which isn't there we'll get the same error. ", "@shauryad15  Thanks for reply.\r\nYou are right this is because of the index which don't exist. But in this case I don't have label file.\r\n\r\nThis model is doing segmentation on nails(Placing colors on nails ). There is only single tflite file which I'm using. \r\n\r\nThis model is working fine with the IoS app and label file is also not present there. You can see [this](https://github.com/makeml-app/MakeML-Nails/tree/master/Segmentation%20Nails/Resources) link.\r\n\r\nOther segmentation models which are working fine in android app also don't have label file. ", "But can you please share that how to get the labels.txt file in while converting the keras model to tflite?"]}, {"number": 34373, "title": "tf.concate() #34365", "body": "Concatenates the list of tensors values along dimension axis. If values[i].shape = [D0, D1, ... Daxis(i), ...Dn], the concatenated result has shape\r\n[D0, D1, ... Raxis, ...Dn]\r\nwhere\r\nRaxis = sum(Daxis(i))\r\n\r\nThat is, the data from the input tensors is joined along the axis dimension.\r\n\r\nThe number of dimensions of the input tensors must match, and all dimensions except axis must be equal.\r\n\r\nFor example:\r\nt1 = [[1, 2, 3], [4, 5, 6]]\r\nt2 = [[7, 8, 9], [10, 11, 12]]\r\ntf.concat([t1, t2], 0)  # [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\r\ntf.concat([t1, t2], 1)  # [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]\r\n\r\n# tensor t3 with shape [2, 3]\r\n# tensor t4 with shape [2, 3]\r\ntf.shape(tf.concat([t3, t4], 0))  # [4, 3]\r\ntf.shape(tf.concat([t3, t4], 1))  # [2, 6]\r\n\r\nAs in Python, the axis could also be negative numbers. Negative axis are interpreted as counting from the end of the rank, i.e., axis + rank(values)-th dimension.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34373) for more info**.\n\n<!-- need_author_consent -->", "This doesn't look like a real PR"]}, {"number": 34372, "title": "[WIP] Allow to specify the type of the tf.data.Dataset.range function #33414 ", "body": "#33414", "comments": []}, {"number": 34370, "title": "Fixes #34194 - tf.size() documentation", "body": "Copied definition from v1 size API reference ", "comments": ["Please open PR against `master` branch. We don't update release branches (`r...`) after the final release of the corresponding TF version (i.e., we won't update `r2.1` after TF2.1 is released).\r\n\r\nThe only time release branches are updated is when we make a patch release to fix a security vulnerability. As that requires expediency, we only accept minimal changes even then.", "Thanks @mihaimaruseac I will send a new PR in some time. Appreciate your detailed feedback!", "Please add me as reviewer to expedite process", "@mihaimaruseac I have sent a PR for master branch here: https://github.com/tensorflow/tensorflow/pull/34389 but I am unable to see the option to add you as a reviewer. Could you help me here, please?"]}, {"number": 34369, "title": "TF2 Warning :  Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.", "body": "When i compute grads by tf.GradientTape, console presents  a **warning : Sets are not currently considered sequences, but this may change in future, so consider avoiding using them.** My code is like this \r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport logging\r\n\r\nlogging.basicConfig(format='%(message)s', level=logging.INFO)\r\n\r\ninput = np.array([[0.1, 0], [0, 0.1]], np.float32)\r\ntarget = np.array([1, 0], np.float32)\r\n\r\n\r\nclass layer(tf.Module):\r\n    def __init__(self):\r\n        super(layer, self).__init__()\r\n        with self.name_scope:\r\n            self.w1 = tf.Variable(tf.ones([2, 2]), name='w1')\r\n\r\n    @tf.Module.with_name_scope\r\n    def __call__(self, x):\r\n        return tf.nn.tanh(x * self.w1)\r\n\r\n\r\ndef get_loss(pred, y_):\r\n    return tf.losses.binary_crossentropy(pred, y_)\r\n\r\n\r\ndef get_opti():\r\n    return tf.optimizers.SGD()\r\n\r\n\r\nmy_layer = layer()\r\n\r\noptimizer = get_opti()\r\n\r\nfor _ in range(2):\r\n    for x_, y_ in zip(input, target):\r\n        with tf.GradientTape() as tape:\r\n            pred = my_layer(x_)\r\n\r\n            loss = get_loss(pred, y_)\r\n        grads = tape.gradient(loss, my_layer.trainable_variables )\r\n        optimizer.apply_gradients(zip(grads, my_layer.trainable_variables ))\r\n\r\n\r\nprint('-----------')\r\nprint(my_layer.w1.numpy())\r\n```\r\n![image](https://user-images.githubusercontent.com/47944452/69391681-8a58cc80-0d0e-11ea-9b73-f35742116387.png)\r\n", "comments": ["@songs18, Please provide the complete code and the Tensorflow version. Thanks!", "Duplicate of #\r\nThe code is very simple, it contains just single hidden layer. In the following code, train_x refers input x, ground_truth is the correct label. I am doing a classification task. \r\n\r\n` with tf.GradientTape() as tape:\r\n            pred,w = model(train_x)\r\n            loss_pred = compute_loss(ground_truth, pred)\r\n\r\n        grads = tape.gradient(loss_pred, model.trainable_variables)\r\n\r\n         optimizer.apply_gradients(zip(grads, model.trainable_variables))`\r\n\r\nThe version of tensorflow  is v2.0.0.\r\nNote: When i use v2.0.0 release version, there is no warning, but i update it to the lastest version, it appears the W:\r\n2019-11-19 15:55:58.350407: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nI looked the util.cc in tensorflow source code, it is likely the parameter of the tape.gradient() input could change in future. But i don't know how to write the compute gradient code is the best way.\r\nThanks!\r\n       \r\n", "@songs18, Just to verify, Did you update the latest version using \r\n`pip install -U tensorflow` ?", "my tensorflow is actually the lastest version, i tried -U, the warning remains.", "@songs18, Can you add the complete code which generates this warning? It's hard to say whether it's important without knowing what you were trying to do. It will indeed help us to move faster. Thanks!", "To run any tf2 script will reproduce the warning, so i do not provide the whole code but just the exact code snippet, if you need the whole code, i can provide a simple verison.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport logging\r\n\r\nlogging.basicConfig(format='%(message)s', level=logging.INFO)\r\n\r\ninput = np.array([[0.1, 0], [0, 0.1]], np.float32)\r\ntarget = np.array([1, 0], np.float32)\r\n\r\n\r\nclass layer(tf.Module):\r\n    def __init__(self):\r\n        super(layer, self).__init__()\r\n        with self.name_scope:\r\n            self.w1 = tf.Variable(tf.ones([2, 2]), name='w1')\r\n\r\n    @tf.Module.with_name_scope\r\n    def __call__(self, x):\r\n        return tf.nn.tanh(x * self.w1)\r\n\r\n\r\ndef get_loss(pred, y_):\r\n    return tf.losses.binary_crossentropy(pred, y_)\r\n\r\n\r\ndef get_opti():\r\n    return tf.optimizers.SGD()\r\n\r\n\r\nmy_layer = layer()\r\n\r\noptimizer = get_opti()\r\n\r\nfor _ in range(2):\r\n    for x_, y_ in zip(input, target):\r\n        with tf.GradientTape() as tape:\r\n            pred = my_layer(x_)\r\n\r\n            loss = get_loss(pred, y_)\r\n        grads = tape.gradient(loss, my_layer.trainable_variables )\r\n        optimizer.apply_gradients(zip(grads, my_layer.trainable_variables ))\r\n\r\n\r\nprint('-----------')\r\nprint(my_layer.w1.numpy())\r\n```\r\n![image](https://user-images.githubusercontent.com/47944452/69391681-8a58cc80-0d0e-11ea-9b73-f35742116387.png)\r\n", "From the warning info, the code can work now, but cannot promise in future, i want to write right code which meet the tf recommends, unfortunately, i do not get help from tf website, i want to know how to write code to eliminate the warning.\r\nThanks!", "@songs18, Thanks for the code, I tried replicating the reported issue but looks like code is not intended correctly. Please take a look at [gist](https://colab.sandbox.google.com/gist/gadagashwini/08de511173260cc94e849e9a0ea1e5bc/untitled273.ipynb) and modify accordingly. Thanks!", "Thanks for your patient, unfortunately, i can not open the gist link as i am in China and i cannot use Google any service, i post my snippet in [Gitee](https://gitee.com/songhaohao2018/codes/01nbm4guh5dtlx8opy7fi31) which is one webiset like github in China.", "@songs18, The warnings are about Tensorflow support for AVX instruction sets. \r\n```\r\nTensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\n``` \r\nFor more read [here](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements). Your code is good to go, warnings can be ignored. Thanks!\r\n", "Thanks. I read the page, unfortunately, i do not understand how AVX causes the warning, could you provide a detailed explain? What is the relationship between AVX and the warning? ", "@songs18, If you use pip version of tensorflow, which means it's already compiled and you are just installing it. When you download it from repository and trying to build, you should build it with CPU AVX support. If you ignore it, you will get the warning every time when you run on cpu.\r\nIf you don't want to get these warnings, You can turn it off by setting os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'. ", "Thanks.\r\nI know that AVX could cause the warning ' Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA',  and my question is not this one.\r\n'Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them'  occured during computing derivative and applying gradient, how AVX cuase this warning? Is there any relationship between AVX and computing derivative and applying gradient? \r\nThanks for your long time patient and help !!", " @songs18 \u89e3\u51b3\u4e86\u5417?", "@ymodak any explanation on why the warning is there??", "I can trace the warning down to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py#L338 where we pass in a set() to nest.flatten which triggers the warning. I don't think this is something you need to really worry about. Assigning to Tom who wrote tf.Module", "I'm seeing the same error in TF2.2 in Python 3.7 on MacOS. \r\n```\r\n2020-06-21 23:12:15.321826: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From /Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n\r\n```\r\nError depends on this line.  The error dissappears when this line is commented.\r\n`model.save(os.path.join(run_dir, f'model_{run_dirname}_keras_best'), save_format='tf')\r\n`\r\nI agree that the error is probably not harmful, but it is annoying.\r\n", "@songs18  \r\nI ran the code on tf nightly and do not see nay warnings,please have a look at the [gist here](https://colab.research.google.com/gist/Saduf2019/99b833665239d84bc2568930506699ec/untitled268.ipynb).\r\nCould you please confirm and let us know.", "@Saduf2019 \r\nThis issue occurred at tensorflow  v2.0.0.\r\nI see you installed tf-nightly-2.4.0.dev20200714 in the given gist.", "I am still seeing it on 2.3.0-rc0 and on a recent 2.4-nightly as well.\r\nHere is the full error message from a run last night with 2.3.0-rc0.\r\nPersonally, I can't tell if this is one error or two that overlap.\r\n```\r\nWARNING:tensorflow:From /Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-07-12 22:57:22.189856: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From /Users/.../anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n\r\n```\r\nI will note that it is impossible for me to tell where this error message is coming from.  It does seem to show up as soon as I execute `model.fit(...)`.\r\n", "@songs18\r\nI have verified on colab for 2.0 as well, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/85d4d135c772ddcdb7d01a0537dbcad2/untitled265.ipynb)\r\n\r\n@kevinashaw \r\nCan you please let us know where are you seeing these warnings.", "@Saduf2019 \r\nAs best I can tell if occurs during the `model.fit()` operation.  \r\nIn particular, it occurs between the start of the operation and before the first `Epoch 1/90` statement.\r\nTensorflow graph messages are always cryptic since it is almost impossible to tell where in the graph the error occurred.", "@Saduf2019 , @kevinashaw \r\nThanks.\r\nAfter a few months, I reviewed the code again and found the problem. \r\nThis warning occurs when the shape of the model output does not match with the ground_truth labels. Although it can be broadcasted.\r\nHowever, the warning given is obscure and it can be better described.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34369\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34369\">No</a>\n", "@songs18: Thank you for the hint about broadcasting.\r\n\r\nInterestingly, I think this has to do with my use of `tf.squeeze()` at the end of my model.  I wanted to remove the channels-dimension after some 1x1 CNNs, such that `(None, 155, 1)` would be reduced to `(None, 155)` and I choose (reasonably) to use `tf.squeeze()` however it apparently returns a shape of `[None]`.  While the model would still train, this caused much confusion and i think this was forcing TF to broadcast into the target vector, since it did not know the true shape of the output tensor.\r\n\r\n```\r\n__________________________________________________________________________________________________\r\nL08_1x1CNN (Conv1D)             (None, 155, 1)       601         L07_Dropout[0][0]                \r\n__________________________________________________________________________________________________\r\ntf_op_layer_L09_Squeeze (Tensor [None]               0           L08_1x1CNN[0][0]                 \r\n__________________________________________________________________________________________________\r\ntf_op_layer_QrsTarget (TensorFl [None]               0           tf_op_layer_L09_Squeeze[0][0]    \r\n==================================================================================================\r\n```\r\n", "Does anyone else think it's weird that @Saduf2019 is on here asking people to post their entire codebase, has no code or repos in their repo, etc..? Is there any way to block these kinds of scammers from being able to post on these kinds of repos. Luckily the OP is smart enough to know that this person was trying to get their codebase, but others may not be so wary and post things that are not necessary to fix issues."]}, {"number": 34368, "title": "MemcpyHtoD blocks gpu computing when using StagingArea", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.4\r\n- GPU model and memory: v100 32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI used StagingArea to make data transferring and coputing as a pipeline. But it turned out that MemcpyHtoD was not overlapped with computing. \r\n![image](https://user-images.githubusercontent.com/12964346/69027289-62652280-0a09-11ea-9f4d-721ba29f8442.png)\r\n\r\n**Describe the expected behavior**\r\nData transferring overlaps with computing in order to speed up training.\r\n**Code to reproduce the issue**\r\n```\r\nwith tf.device(\"/gpu:0\"):\r\n    cpu_to_gpu_stage = data_flow_ops.StagingArea(dtypes=_dtypes(data), shapes=_shapes(data))\r\n    cpu_to_gpu_op = cpu_to_gpu_stage.put(data)\r\n\r\n    data2 = cpu_to_gpu_stage.get()\r\n    train_op = model.build_graph(data2)\r\n\r\nsess.run([cpu_to_gpu_op])\r\nwhile 1:\r\n    res = sess.run([train_op, cpu_to_gpu_op])[0]\r\n\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["i've got the same problem on tf1.14, rtx2080ti", "Code you provided is incomplete. Please provide minimal reproducible test case. Thanks!", "> Code you provided is incomplete. Please provide minimal reproducible test case. Thanks!\r\n\r\nThanks for answering!\r\n\r\nI use tf_cnn_benchmarks to see if the problem remains.\r\n[https://github.com/tensorflow/benchmarks](url)\r\n\r\nThe command is \r\n`mpirun --allow-run-as-root -np 1 python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model alexnet --batch_size 1024 --variable_update horovod --trace_file=/learner/benchmarks/timeline.json --data_name cifar10 --data_dir /learner/benchmarks/cifar-10-batches-py`.\r\n\r\nThe problem remains. MemcpyHtoD is not overlapped with computing.\r\n![image](https://user-images.githubusercontent.com/12964346/69699495-072ae280-1123-11ea-9cdf-5c46675117e1.png)\r\n\r\n", "Hi, everyone!\r\nI wrote an easy case to reproduce the problem. Here is the code:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import data_flow_ops\r\nfrom tensorflow.python.client import timeline\r\n\r\ngpu = \"/gpu:0\"\r\nsize = 10000000\r\ndata = np.random.rand(size).astype(np.float32)\r\ndef generate_data():\r\n    return data\r\n\r\ndef compute(inp):\r\n    out = tf.zeros_like(inp, dtype=np.float32)\r\n    for i in range(1000):\r\n        out = tf.math.add(inp, out)\r\n    return out\r\n\r\ndef build_graph(inp):\r\n    with tf.device(gpu):\r\n        cpu_to_gpu_stage = data_flow_ops.StagingArea(dtypes=[np.float32], shapes=[size])\r\n        copy_op = cpu_to_gpu_stage.put(inp)\r\n        \r\n        inp = cpu_to_gpu_stage.get()\r\n        compute_op = compute(inp)\r\n\r\n        return copy_op, compute_op\r\n\r\ninp = tf.py_func(generate_data, [], [np.float32])\r\ncopy_op, compute_op = build_graph(inp)\r\n\r\nsess = tf.Session()\r\n\r\ndef run_with_tl(sess, ops, output_path):\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    sess.run(ops, options=run_options, run_metadata=run_metadata)\r\n    tl = timeline.Timeline(run_metadata.step_stats)\r\n    ctf = tl.generate_chrome_trace_format()\r\n    with open(output_path, \"w\") as fp:\r\n        fp.write(ctf)\r\n\r\nrun_with_tl(sess, [copy_op], \"timeline.json\")\r\nfor i in range(20):\r\n    print(i)\r\n    run_with_tl(sess, [copy_op, compute_op], \"timeline_{}.json\".format(i))\r\n```\r\nHere is the timeline:\r\n![image](https://user-images.githubusercontent.com/12964346/69911433-66a42d80-1456-11ea-8877-67557aabb1a9.png)\r\n\r\nIt still took extra time to do the MemcpyHToD.", "@ymodak \r\nHi! Could you tell what was wrong in the case above?\r\nThanks!!", "I figured it out. It's about the pinned memory.", "\r\n\r\n\r\n> I figured it out. It's about the pinned memory.\r\n\r\n@zhm9484 how to fix it ? ", "> > I figured it out. It's about the pinned memory.\r\n> \r\n> @zhm9484 how to fix it ?\r\n\r\nModify the case above.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import data_flow_ops\r\nfrom tensorflow.python.client import timeline\r\nimport pycuda.driver as cuda\r\nimport pycuda.autoinit\r\n\r\ngpu = \"/gpu:0\"\r\nsize = 10000000\r\ndata = np.random.rand(size).astype(np.float32)\r\npinned_data = cuda.pagelocked_empty_like(data, mem_flags=cuda.host_alloc_flags.DEVICEMAP)\r\npinned_data[:] = data\r\ndef generate_data():\r\n    return pinned_data\r\n\r\ndef compute(inp):\r\n    out = tf.zeros_like(inp, dtype=np.float32)\r\n    for i in range(100):\r\n        out = tf.math.add(inp, out)\r\n    return out\r\n\r\ndef build_graph(inp):\r\n    with tf.device(gpu):\r\n        cpu_to_gpu_stage = data_flow_ops.StagingArea(dtypes=[np.float32], shapes=[size])\r\n        copy_op = cpu_to_gpu_stage.put(inp)\r\n        \r\n        inp = cpu_to_gpu_stage.get()\r\n        compute_op = compute(inp)\r\n\r\n        return copy_op, compute_op\r\n\r\ninp = tf.py_func(generate_data, [], [np.float32])\r\ncopy_op, compute_op = build_graph(inp)\r\n\r\nsess = tf.Session()\r\n\r\ndef run_with_tl(sess, ops, output_path):\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    sess.run(ops, options=run_options, run_metadata=run_metadata)\r\n    tl = timeline.Timeline(run_metadata.step_stats)\r\n    ctf = tl.generate_chrome_trace_format()\r\n    with open(output_path, \"w\") as fp:\r\n        fp.write(ctf)\r\n\r\nrun_with_tl(sess, [copy_op], \"timeline.json\")\r\nfor i in range(20):\r\n    print(i)\r\n    run_with_tl(sess, [copy_op, compute_op], \"timeline_{}.json\".format(i))\r\n\r\n```"]}, {"number": 34367, "title": "Implement Tell method of SnappyInputBuffer class.", "body": "Finish the [TODO](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/snappy/snappy_inputbuffer.cc#L51) in `snappy_inputbuffer.cc`.\r\n```cpp\r\nint64 SnappyInputBuffer::Tell() const {\r\n  // TODO(srbs): Implement this.\r\n  return -1;\r\n}\r\n```\r\nThen by involving `SnappyInputBuffer` like `ZlibInputStream`, Dataset operators can easily support snappy-compressed input.", "comments": ["@jhseu @gbaned One week past, is there any progress or update? Thanks a lot!"]}, {"number": 34366, "title": "with tf.GradientTape() tf.image.non_max_suppression returns  unexpected result", "body": "  with tf.GradientTape() as tape:\r\n    predictions = model(images)\r\n    vol = predictions.shape[1]\r\n    col = predictions.shape[2]\r\n    num_boxes = vol*col*anchor_len\r\n    predictions = tf.reshape(predictions,[num_boxes,5+classes])\r\n    \r\n    scores = predictions[:,0]\r\n    boxes = predictions[:,1:5]\r\n    num_labels = len(labels)\r\n    print(num_labels,'num_labels')\r\n    nms_index = tf.image.non_max_suppression(boxes,scores,max_output_size=num_labels,score_threshold=0.1)\r\n\r\nerrors:\r\nTensor(\"non_max_suppression/NonMaxSuppressionV3:0\", shape=(None,), dtype=int32)\r\n\r\nthe same input ,when eval, the nms_index will be normal. but in training, it returns nothing.\r\n\r\nwhy? is  tf2.0's eagermode only supported in some steps? \r\nwhere can i find the document about eagermode vs traditional ways?", "comments": ["tf.function makes this happens. but the document in the tutorial does not mention it.\r\n\r\ncarefully use tf 2.0 tutorial"]}, {"number": 34365, "title": "Why tf.concat not support multiply types", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI have 2 tensor with shape(1,4)\r\nbut one tensor(a) is string type.\r\nAnother tensor(b) is int32 type.\r\nWhen I try to concat them\r\ntf.concat([a,b], 0)\r\nI get the error message:\r\n_tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute ConcatV2 as input #1(zero-based) was expected to be a string tensor but is a int32 tensor [Op:ConcatV2] name: concat_\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\ntf.concat users\r\n\r\n**Any Other info.**\r\n", "comments": ["Concatenates the list of tensors values along dimension axis. If values[i].shape = [D0, D1, ... Daxis(i), ...Dn], the concatenated result has shape\r\n[D0, D1, ... Raxis, ...Dn]\r\nwhere\r\nRaxis = sum(Daxis(i))\r\n\r\nThat is, the data from the input tensors is joined along the axis dimension.\r\n\r\nThe number of dimensions of the input tensors must match, and all dimensions except axis must be equal.\r\n\r\nFor example:\r\nt1 = [[1, 2, 3], [4, 5, 6]]\r\nt2 = [[7, 8, 9], [10, 11, 12]]\r\ntf.concat([t1, t2], 0) # [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\r\ntf.concat([t1, t2], 1) # [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]\r\n\r\ntensor t3 with shape [2, 3]\r\ntensor t4 with shape [2, 3]\r\ntf.shape(tf.concat([t3, t4], 0)) # [4, 3]\r\ntf.shape(tf.concat([t3, t4], 1)) # [2, 6]\r\n\r\nAs in Python, the axis could also be negative numbers. Negative axis are interpreted as counting from the end of the rank, i.e., axis + rank(values)-th dimension", "@sgpritam My input tensors definitely has the same dimension but one is tf.string and another one is tf.int32. ", "@Leslie-Fang As you can see from the definition of values in the function, the two tensors should be of same type and this function doesn't cast the two types into the same type. This is the cause of the error.\r\n\r\n```\r\ndef concat_v2(values, axis, name=None):\r\n  r\"\"\"Concatenates tensors along one dimension.\r\n\r\n  Args:\r\n    values: A list of at least 2 `Tensor` objects with the same type.\r\n      List of `N` Tensors to concatenate. Their ranks and types must match,\r\n      and their sizes must match in all dimensions except `concat_dim`.\r\n    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\r\n      0-D.  The dimension along which to concatenate.  Must be in the\r\n      range [-rank(values), rank(values)).\r\n    name: A name for the operation (optional).\r\n\r\n```", "@gowthamkpr Thanks for the sharing. Is there any special consideration to design the input tensors must be the same type? My question is that whether this API could support the input tensors with different types. ", "@Leslie-Fang Can't promise anything but I will keep you updated on this feature request. ", "@Leslie-Fang,\r\nSorry for the delayed response. **`Concatenation`** of a **`String Tensor`** and an **`Integer Tensor`** no more results in an **`Error`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/97e681b1c190d1599f75f203c277ac21/gh_34365.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34364, "title": "build failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04.3 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.1.0-rc.0\r\n- Python version: 3.7.5 x64\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.29.1 x64\r\n- GCC/Compiler version (if compiling from source):  7.4.0\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: GTX1080Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n bazel build\r\n \r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: /home/wmind/repo/tensorflow/tensorflow/python/BUILD:2591:1: Linking of rule '//tensorflow/python:gen_sparse_ops_py_wrappers_cc' failed (Exit 1)\r\nbazel-out/host/bin/tensorflow/core/libop_gen_lib.a(op_gen_lib.o): In function `google::protobuf::internal::ArenaStringPtr::CreateInstance(google::protobuf::Arena*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const*)':\r\n```\r\n", "comments": ["Duplicate of #34117."]}, {"number": 34363, "title": "random.uniform((),minval,maxval) returns array instead of scalar tensor when min or maxval is not a scalar tensor", "body": "```\r\n>>> import tensorflow as tf\r\n>>> scalar = tf.zeros(shape=())\r\n>>> array = tf.zeros(shape=(1,))\r\n\r\n>>> tf.random.uniform(shape=(),minval = scalar)\r\n<tf.Tensor: id=25, shape=(), dtype=float32, numpy=0.021499991>\r\n\r\n>>> tf.random.uniform(shape=(),minval = array)\r\n<tf.Tensor: id=31, shape=(1,), dtype=float32, numpy=array([0.9388697], dtype=float32)>\r\n```\r\nExpected behavior is to either trow an error or treat single element tensor as scalar and return a scalar.\r\n\r\n-win10, tf2, cuda", "comments": ["@Pixel-Therapy,\r\nCorrect me if I am wrong but, From this [TF Link](https://www.tensorflow.org/api_docs/python/tf/random/uniform) and [The Source Code](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/random_ops.py#L186-L252),  it is mentioned that `random.uniform()` returns a Tensor but it is not mentioned that it will be a Scalar Tensor.\r\n\r\nCan you please provide the reference where it mentions `random.uniform` returns a Scalar Tensor. Thanks!", "> @Pixel-Therapy,\r\n> Correct me if I am wrong but, From this [TF Link](https://www.tensorflow.org/api_docs/python/tf/random/uniform) and [The Source Code](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/random_ops.py#L186-L252), it is mentioned that `random.uniform()` returns a Tensor but it is not mentioned that it will be a Scalar Tensor.\r\n> \r\n> Can you please provide the reference where it mentions `random.uniform` returns a Scalar Tensor. Thanks!\r\n\r\nThe first argument in random.uniform asks for shape, ill edit my example as shape = ().\r\nIt seems that random.uniform always follows this shape except in the above case.", "The issue is that in one code path, math_ops.add was called which implicitly broadcast.\r\n\r\nCreated a PR #34399 for the fix.", "This issue is still being worked on. PR #34399 depends on PR #38544. Once PR #38544 is merged, PR #34399 will be reopened and this issue will be fixed by then.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34363\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34363\">No</a>\n", "PR #38544 has been merged. However, the issue is not fixed yet. Will re-submit the PR  of PR #34399 to eventually fix this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34363\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34363\">No</a>\n"]}]