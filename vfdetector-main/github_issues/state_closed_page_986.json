[{"number": 23824, "title": "Load SavedModel for Estimator API", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.10+\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nTensorFlow current touts several high level APIs including:\r\n\r\n> - Estimators, a high-level API that provides fully-packaged models ready for large-scale training and production.\r\n\r\nFocusing on what Estimator's encapsulate:\r\n\r\n> Estimators encapsulate the following actions:\r\n> - training\r\n> - evaluation\r\n> - prediction\r\n> - export for serving\r\n\r\n\r\nthere seems to be a glaring omission that straddles prediction and serving.\r\n\r\nWhere is the import?\r\n\r\nPreparing a custom estimator for serving is an arduous journey, so once one has a `saved_model`, it would be nice to be able to load the saved model via the estimator class, rather than the `contrib` module:\r\n\r\n```\r\nfrom tensorflow.contrib import predictor\r\npredict_fn = predictor.from_saved_model('<model-dir>')\r\n```\r\n\r\ne.g.\r\n\r\n```\r\nest = tf.estimator.Estimator.from_saved_model(model_dir)\r\n\r\n# retrain new dataset or whatever, using estimator api rather than SavedModelPredictor\r\n```\r\n\r\nperhaps this was left out as it was the goal to only used exported models in the serving context. \r\n\r\nIf that is the case, however, then the serving documentation needs critical updates. In addition it (the interface to serving) would benefit from a higher level api centered around estimators rather than any saved model.   \r\n\r\nIf I have a saved model and it is this easy to predict with it:\r\n\r\n```\r\nfrom tensorflow.contrib import predictor\r\npredict_fn = predictor.from_saved_model('<model-dir>')\r\npredict_fn(<pred_features>)\r\n```\r\n\r\nthen it should be this easy to get a restful api to my model\r\n\r\n```\r\n> tf-serve <model-dir> --host=localhost --port=8000\r\n```\r\n\r\nso that I can \"GET\" my prediction\r\n\r\n\r\nNow, I could be misinformed. It is very likely I do not understand the tensorflow/serving.git to the fullest and it might very well be that easy. If that is the case, then the documentation needs to be updated to clarify how one can just serve their model. \r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nAdd a method to the `tf.estimator.Estimator` class `from_saved_model` or `import_saved_model` which allows one to recover the estimator. \r\n\r\n\r\nAlso if the Estimator API is the high level API that the TF documentation leads me to believe it to be, perhaps make it directly compatible with TF.JS\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople using the estimator api.\r\n\r\nPeople confused by the obtuse protocol to serve a custom estimator\r\n\r\n**Any Other info.**\r\n", "comments": ["please check out `tf.contrib.estimator.SavedModelEstimator`", "Now that `tf.contrib` no longer exist, there is no way of evaluating an estimator from a SavedModel easily. \r\n\r\n`estimator.evaluate()` only takes a `checkpoint_path` argument. However, Estimator does not provide a way to manually save checkpoints. ", "> Now that `tf.contrib` no longer exist, there is no way of evaluating an estimator from a SavedModel easily.\r\n> \r\n> `estimator.evaluate()` only takes a `checkpoint_path` argument. However, Estimator does not provide a way to manually save checkpoints.\r\n\r\nAny updates here? To my understanding, currently any saved estimator model relying on using `tf.contrib.estimator.SavedModelEstimator()` previously is now left out in the cold.", "+1 \r\n\r\nAny update here would be appreciated! Our models in production are breaking because we're using this in our workflow and recently updated to TF v2"]}, {"number": 23823, "title": " tf.estimator.export.ServingInputReceiver very rigid implementation", "body": "**System information**\r\n- I have written custom code.\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from:  binary\r\n- TensorFlow version: tensorflow-1.12.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: GeForce MX150 2GB\r\n\r\n**Describe the current behavior**\r\n\r\nVery rigid implementation for tf.estimator.export.ServingInputReceiver. I use nested dictionaries for train and eval features, such as: features={}; features['static'] = tf.Tensor(); features['dynamic']['a'] = tf.Tensor(); features['dynamic']['b'] = tf.Tensor(), which works fine for train and eval input fns but it is not supported for tf.estimator.export.ServingInputReceiver. It raises value_error at _check_tensor:\r\n\r\nValueError: feature dynamic must be a Tensor or SparseTensor.\r\n\r\nSince the function _wrap_and_check_input_tensors only allows plain dictionaries containing string keys and tensor values. \r\n\r\n**Describe the expected behavior**\r\n\r\nAt least one should be able to disable the option check_tensors. Actually, tf.estimator is expected to provide a more flexible implementation.\r\n\r\n**Other info / logs**\r\n\r\n  File \"/home/edu/.local/lib/python3.6/site-packages/tensorflow/python/estimator/export/export.py\", line 137, in __new__ \r\n    features = _wrap_and_check_input_tensors(features, 'feature')\r\n  File \"/home/edu/.local/lib/python3.6/site-packages/tensorflow/python/estimator/export/export.py\", line 73, in _wrap_and_check_input_tensors\r\n    _check_tensor(tensor, name, error_label=field_name)\r\n  File \"/home/edu/.local/lib/python3.6/site-packages/tensorflow/python/estimator/export/export.py\", line 97, in _check_tensor\r\n    raise value_error\r\nValueError: feature dynamic must be a Tensor or SparseTensor.\r\n", "comments": ["ServingInputReceivers have strict requirements to comply with serving infrastructure (ie tf-serving). As a workaround, you could unroll your dictionaries and key each separately.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It is not, this is the expected behaviour as @karmel pointed out. I unrolled my dictionary as suggested with no further issues."]}, {"number": 23822, "title": "Event files created by estimator are not closed after training/evaluation", "body": "I'm using the estimator API and a custom implementation of several hyperparameter search methods (random search, hyperband, etc.) to train multiple models and select the best hyperparameters. At the end of the algorithm I would like to delete all the files created during the trainings/evaluations (e.g., the checkpoint files, event files, etc.). I'm trying this with shutil.rmtree; however, I am not able to delete the event files as it appears they are still in use.\r\n\r\nHere is an example code - mostly pulled from the Iris example scripts - along with the error to illustrate the issue. The actual code uses a custom estimator implementation instead of the DNNClassifier, but this has the same problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport os\r\nimport shutil\r\n\r\ndef load_data(y_name='Species'):\r\n    train_url = \"http://download.tensorflow.org/data/iris_training.csv\"\r\n    test_url = \"http://download.tensorflow.org/data/iris_test.csv\"\r\n\r\n    train_data = pd.read_csv(train_url)\r\n    train_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\r\n    test_data = pd.read_csv(test_url)\r\n    test_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\r\n    \r\n    train_x = train_data.iloc[:, 0:4]\r\n    train_y = train_data.iloc[:, 4]\r\n    \r\n    test_x = test_data.iloc[:, 0:4]\r\n    test_y = test_data.iloc[:, 4]\r\n    \r\n    return (train_x, train_y), (test_x, test_y)\r\n\r\ndef train_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for training\"\"\"\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\r\n\r\n    # Shuffle, repeat, and batch the examples.\r\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\r\ndef eval_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for evaluation or prediction\"\"\"\r\n    features=dict(features)\r\n    if labels is None:\r\n        # No labels, use only features.\r\n        inputs = features\r\n    else:\r\n        inputs = (features, labels)\r\n\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\r\n\r\n    # Batch the examples\r\n    assert batch_size is not None, \"batch_size must not be None\"\r\n    dataset = dataset.batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\r\nmodel_dir = os.path.join(os.getcwd(), *['estimator_test', 'test_1'])\r\n\r\n(train_x, train_y), (test_x, test_y) = load_data()\r\n\r\n# Feature columns describe how to use the input.\r\nmy_feature_columns = []\r\nfor key in train_x.keys():\r\n    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\r\n\r\n# Build 2 hidden layer DNN with 10, 10 units respectively.\r\nclassifier = tf.estimator.DNNClassifier(\r\n    feature_columns=my_feature_columns,\r\n    # Two hidden layers of 10 nodes each.\r\n    hidden_units=[10, 10],\r\n    # The model must choose between 3 classes.\r\n    n_classes=3,\r\n    model_dir = model_dir)\r\n\r\n# Train the Model.\r\nclassifier.train(\r\n    input_fn=lambda: train_input_fn(train_x, train_y, 100),\r\n    steps=100)\r\n\r\n# Evaluate the model.\r\neval_result = classifier.evaluate(\r\n    input_fn=lambda: eval_input_fn(test_x, test_y, 100))\r\n\r\nprint('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\r\n\r\nshutil.rmtree(model_dir)\r\n```\r\nHere is the error:\r\n```OSError                                   Traceback (most recent call last)\r\n<ipython-input-40-beccd7cbb56e> in <module>()\r\n----> 1 shutil.rmtree(os.path.join(os.getcwd(), *['estimator_test', 'delete_test_1']))\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py in rmtree(path, ignore_errors, onerror)\r\n    492             os.close(fd)\r\n    493     else:\r\n--> 494         return _rmtree_unsafe(path, onerror)\r\n    495 \r\n    496 # Allow introspection of whether or not the hardening against symlink\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py in _rmtree_unsafe(path, onerror)\r\n    391         os.rmdir(path)\r\n    392     except OSError:\r\n--> 393         onerror(os.rmdir, path, sys.exc_info())\r\n    394 \r\n    395 # Version using fd-based APIs to protect against races\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py in _rmtree_unsafe(path, onerror)\r\n    389                 onerror(os.unlink, fullname, sys.exc_info())\r\n    390     try:\r\n--> 391         os.rmdir(path)\r\n    392     except OSError:\r\n    393         onerror(os.rmdir, path, sys.exc_info())\r\n\r\nOSError: [WinError 145] The directory is not empty:\r\n```\r\nI looked at all the files that were open by Python processes, and it's only the event files that are still in use.  I would expect that all files would be closed after training/evaluation are complete. Unfortunately, I cannot find a way to manually close these after training.\r\n\r\nOn a related note (and this is more of a feature request), it would be great to be able to disable writing summaries in the estimator configuration as the files are pretty large. This would remove the need to delete them after training altogether. \r\n\r\nSystem information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.11 (CPU version)\r\n- Python version: 3.6\r\n", "comments": ["The error above seems to be that you cannot call rmdir on a non-empty directory; model_dir is not empty after training. See, for example, https://stackoverflow.com/questions/6996603/delete-a-file-or-folder\r\n\r\nClosing this issue, as it appears to be an issue in the using code. Please reopen if I have misinterpreted.", "@karmel That's the error, but that's not the underlying issue. rmtree deletes both files and the directory; I believe rmtree first tries to remove all files then deletes the directory. However, the event files are still open, so they cannot be deleted by rmtree. If you check out the files open by the python process, you can see the event files are still open (I did this with psutils).\r\n\r\nYou can also try to just delete the event file after training in the same script by replacing the last line with os.remove for a specific event file. When I try to delete one of the event files after training, I get this error, which (I believe) is more in line with the underlying issue:\r\nPermissionError: [WinError 5] Access is denied:\r\n\r\nPlease reopen. Thanks.", "Re: the feature request in the note above, set both save_summaries_steps and save_summaries_secs to None: \r\n\r\n```\r\n    save_summaries_steps: The frequency, in number of global steps, that the\r\n      summaries are written to disk using a default summary saver. If both\r\n      `save_summaries_steps` and `save_summaries_secs` are set to `None`, then\r\n      the default summary saver isn't used. Default 100.\r\n```\r\n\r\nAssigning to @rchao for the file writing issue; presumably the hook is not properly closing the file handles? Not sure if this would be an issue with the summary writer or the hook itself.", "\r\nI'm facing a similar issue...\r\n\r\nI executed a similar script @phil510 posted on linux and everything ran smoothly.\r\n\r\nThis seems to be a windows issue. Furthermore, if you close the python process in windows, the event file is gone assuming that you attempted to delete it and got `PermissionError: [WinError 5] Access is denied:`. ", "I found a way around this awhile ago, so I thought I'd share in case this is happening to anyone else. Once you are done writing all the summaries, you can clear the file writer cache.\r\n\r\n```\r\ncache = tf.summary.FileWriterCache()\r\ncache.clear()\r\n```\r\nAfter that, I'm able to delete event files and the directory.", "Hi @phil510 , The above code in Template is  running with out Error in TF 2.6 through Colab .Attaching [GIST](https://colab.research.google.com/gist/mohantym/3e14578557cadcc2a5c90041d65361e4/github_23822.ipynb) for reference. Closing this issue as it seems to be resolved in latest version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23822\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23822\">No</a>\n"]}, {"number": 23821, "title": "Pull", "body": "", "comments": []}, {"number": 23820, "title": "When tf.Session(), got error cuDevicePrimaryCtxRetain: CUDA_ERROR_NOT_SUPPORTED", "body": "When try to create session by tf.Session(), it gave error as below. Not sure how to solve it.\r\n\r\nPython 3.6.5 (default, Apr  1 2018, 05:46:30) \r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.Session()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/eliyart/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1511, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/home/eliyart/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 634, in __init__\r\n    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\r\ntensorflow.python.framework.errors_impl.InternalError: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_NOT_SUPPORTED: operation not supported\r\n\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|=====+======================+======================|\r\n|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   27C    P8    27W / 149W |     16MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|==============================================|\r\n|    0      2075      G   /usr/lib/xorg/Xorg                             9MiB |\r\n|    0      2128      G   /usr/bin/gnome-shell                           6MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\nAny suggestion, thanks. \r\nalex\r\n", "comments": ["Could you please check if [this ](https://stackoverflow.com/questions/41965187/nvidia-device-error-in-tensorflow)helps. Also please post the CUDA/cuDNN versions, tensorflow version used as asked in the new issue template.", "Have you actually installed CUDA/cuDNN following these guides ([1](https://www.tensorflow.org/install/pip), [2](https://www.tensorflow.org/install/gpu))? Also, make sure to use CUDA 9.0 and cuDNN 7.1.4 or cuDNN 7.3.1 for it to work properly...", "Closing as this issue is in \"awaiting response\" status for more than 7 days and did not hear back from the user. Please post your comments if any, we will reopen. Thanks !"]}, {"number": 23819, "title": "Broken link in \"A Tool Developer's Guide...\" for graph_run_run2.pbtxt", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- Doc Link:\r\n   i.  https://www.tensorflow.org/guide/extend/model_files\r\n   ii. https://github.com/tensorflow/docs/blob/master/site/en/guide/extend/model_files.md\r\n\r\n\r\n**Describe the documentation issue**\r\nAs in the previously closed #5978 and #12042, the link for `graph_run_run2.pbtxt` is currently broken.", "comments": ["@francescomilano172 It seems to be working correctly. You can close the issue now \ud83d\ude42 ", "> @francescomilano172 It seems to be working correctly. You can close the issue now slightly_smiling_face\r\n\r\nYou're right, thanks."]}, {"number": 23818, "title": "update mklml version to make it consistent with mkldnn.", "body": "We found mkldnn version was updated in https://github.com/tensorflow/tensorflow/commit/13440cc25460113c4828f4355bcc7544a613daa1 but did not update the mklml version.\r\nThis PR update the mklml to latest version to make it consistent with the mkldnn version.", "comments": ["@penpornk It\u2019s important to use latest mklml especially in GEMM function.", "@guizili0 Thank you for the info!\r\n@wt-huang Could you please help pull the PR? (Windows Bazel GPU test doesn't really work right now and I think we can ignore its failure.) Thank you!", "@penpornk Just pulled the PR, thank you!"]}, {"number": 23817, "title": "Build from source r1.12 : tensorflow/tensorflow/contrib/kafka/BUILD:36:1: no such package '@kafka//':", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.12\r\n- Python version: python2.7.14\r\n- Installed using virtualenv? pip? conda?: source\r\n- Bazel version (if compiling from source): 0.18.0\r\n- GCC/Compiler version (if compiling from source): gcc4.8.5\r\n- CUDA/cuDNN version: cuda9.0\r\n- GPU model and memory: p40 20g\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nAfter bazel build:\r\n```\r\nWARNING: Duplicate rc file: /home/adamzhangchao/bin/tensorflow/tools/bazel.rc is read multiple times, most recently imported from /home/adamzhangchao/bin/tensorflow/.bazelrc\r\nWARNING: Processed legacy workspace file /home/adamzhangchao/bin/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nDEBUG: /home/adamzhangchao/.cache/bazel/_bazel_adamzhangchao/6492ced5c6ed3f5961e75357254e4c4a/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nERROR: /home/adamzhangchao/bin/tensorflow/tensorflow/contrib/kafka/BUILD:36:1: no such package '@kafka//': Traceback (most recent call last):\r\n\tFile \"/home/adamzhangchao/bin/tensorflow/third_party/repo.bzl\", line 106\r\n\t\t_apply_patch(ctx, ctx.attr.patch_file)\r\n\tFile \"/home/adamzhangchao/bin/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n\t\tfail(\"patch command is not found, ple...\")\r\npatch command is not found, please install it and referenced by '//tensorflow/contrib/kafka:dataset_kernels'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@kafka//': Traceback (most recent call last):\r\n\tFile \"/home/adamzhangchao/bin/tensorflow/third_party/repo.bzl\", line 106\r\n\t\t_apply_patch(ctx, ctx.attr.patch_file)\r\n\tFile \"/home/adamzhangchao/bin/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n\t\tfail(\"patch command is not found, ple...\")\r\npatch command is not found, please install it\r\nINFO: Elapsed time: 7.860s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (251 packages loaded)\r\n    currently loading: tensorflow/core/kernels\r\n```", "comments": ["Had the same problem on **exactly** the same platform: `centos7`\r\nIn my case: \r\n- `Python 3.6.5`\r\n- **NO CUDA**\r\n- `bazel 0.19`", "I finally solved this by installing `patch`:\r\n\r\n```\r\nsudo yum install patch\r\n```", "> I finally solved this by installing `patch`:\r\n> \r\n> ```\r\n> sudo yum install patch\r\n> ```\r\n\r\nThanks for your advice,  have you met  this error `/home/adamzhangchao/bin/tensorflow/tensorflow/contrib/kafka/BUILD:36:1: no such package '@kafka//': Traceback (most recent call last): ` ?", "@peterzhang2029 that error message is the result of missing `patch` command. Please read the full error message you shared:\r\n```\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@kafka//': Traceback (most recent call last):\r\n\tFile \"/home/adamzhangchao/bin/tensorflow/third_party/repo.bzl\", line 106\r\n\t\t_apply_patch(ctx, ctx.attr.patch_file)\r\n\tFile \"/home/adamzhangchao/bin/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n\t\tfail(\"patch command is not found, ple...\")\r\npatch command is not found, please install it\r\n```\r\nIt cannot build kafka, because it needs the `patch` command."]}, {"number": 23816, "title": "entry point not found ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["didn't mean to open a request I was going through the whole get-up thing\nand try to understand it and I was clicking on things didn't know what I\nwas doing but I did it didn't understand it log out of it and let it go all\nthat is just to much for a mind like mines\n\nOn Nov 16, 2018 8:50 PM, \"thpatty\" <notifications@github.com> wrote:\n\nPlease go to Stack Overflow for help and support:\n\nhttps://stackoverflow.com/questions/tagged/tensorflow\n\nIf you open a GitHub issue, here is our policy:\n\n   1. It must be a bug, a feature request, or a significant problem with\n   documentation (for small docs fixes please send a PR instead).\n   2. The form below must be filled out.\n   3. It shouldn't be a TensorBoard issue. Those go here\n   <https://github.com/tensorflow/tensorboard/issues>.\n\n*Here's why we have that policy*: TensorFlow developers respond to issues.\nWe want to focus on work that benefits the whole community, e.g., fixing\nbugs and adding features. Support only helps individuals. GitHub also\nnotifies thousands of people when issues are filed. We want them to see you\ncommunicating an interesting problem, rather than being redirected to Stack\nOverflow.\n------------------------------\nSystem information\n\n   - *Have I written custom code (as opposed to using a stock example\n   script provided in TensorFlow)*:\n   - *OS Platform and Distribution (e.g., Linux Ubuntu 16.04)*:\n   - *Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\n   happens on mobile device*:\n   - *TensorFlow installed from (source or binary)*:\n   - *TensorFlow version (use command below)*:\n   - *Python version*:\n   - *Bazel version (if compiling from source)*:\n   - *GCC/Compiler version (if compiling from source)*:\n   - *CUDA/cuDNN version*:\n   - *GPU model and memory*:\n   - *Exact command to reproduce*:\n\nYou can collect some of this information using our environment capture\nscript:\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\n\nYou can obtain the TensorFlow version with:\n\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\n\nDescribe the problem\n\nDescribe the problem clearly here. Be sure to convey here why it's a bug in\nTensorFlow or a feature request.\nSource code / logs\n\nInclude any logs or source code that would be helpful to diagnose the\nproblem. If including tracebacks, please include the full traceback. Large\nlogs and files should be attached. Try to provide a reproducible test case\nthat is the bare minimum necessary to generate the problem.\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/23816>, or mute the thread\n<https://github.com/notifications/unsubscribe-auth/ArBbKtAdhamC88ynCBD5CjMJ-qUphNxQks5uv3mQgaJpZM4YnPU1>\n.\n", "If there is an issue with tensorflow, request you to give a clear explanation about the problem by filling the new issue template. Thank you !"]}, {"number": 23815, "title": "TensorFlow restore model Key Variable not found in checkpoint", "body": "OSX10.14.1  py3.6  tf1.8\r\n\r\nI'm a tensorflow beginner.I try to train a model for mnist. When I restore the model I get Error.\r\n\r\n    from datetime import datetime\r\n    \r\n    import tensorflow as tf\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n    \r\n    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\n    save_model_path = 'mnist_model/model.ckpt'\r\n    \r\n    \r\n    def train():\r\n        learning_rate = 0.05\r\n        batch_size = 100\r\n        max_epochs = 100\r\n        num_of_batch = int(mnist.train.num_examples / batch_size)\r\n        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\r\n    \r\n        X = tf.placeholder(tf.float32, shape=[None, 784], name='X')\r\n        y = tf.placeholder(tf.float32, shape=[None, 10], name='y')\r\n        print(X.name, y.name)\r\n    \r\n        W = tf.get_variable(shape=[784, 10], name='weight')\r\n        b = tf.get_variable(initializer=tf.zeros([10]), name='bais')\r\n        tf.summary.histogram(\"weights\", W)\r\n        tf.summary.histogram(\"biases\", b)\r\n    \r\n        with tf.name_scope('pred'):\r\n            y_pred = tf.nn.softmax(tf.matmul(X, W) + b, name='predict')\r\n            print(y_pred.name)\r\n    \r\n        with tf.name_scope('loss'):\r\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_pred))\r\n            tf.summary.scalar('loss', loss)\r\n            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\r\n    \r\n        with tf.name_scope('acc'):\r\n            correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\r\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='acc')\r\n            print(accuracy.name)\r\n    \r\n        merged_summary_op = tf.summary.merge_all()\r\n        init_op = tf.global_variables_initializer()\r\n    \r\n        saver = tf.train.Saver()\r\n    \r\n        with tf.Session() as sess:\r\n            sess.run(init_op)\r\n    \r\n            loss_avg = 0\r\n            writer = tf.summary.FileWriter('mnist/{}'.format(now), sess.graph)\r\n            for epoch in range(max_epochs):\r\n                for i in range(num_of_batch):\r\n                    batch_x, batch_y = mnist.train.next_batch(batch_size)\r\n                    summary_str, _, l = sess.run([merged_summary_op, optimizer, loss], feed_dict={X: batch_x, y: batch_y})\r\n                    loss_avg += l\r\n                    global_step = epoch * num_of_batch + i\r\n                    writer.add_summary(summary_str, global_step)\r\n    \r\n                    if global_step % 100 == 0:\r\n                        print('Epoch {}: {} save model'.format(epoch, i))\r\n                        # save model in halfway\r\n                        saver.save(sess, save_model_path, global_step=global_step)\r\n    \r\n                loss_avg /= num_of_batch\r\n                print('Epoch {}: Loss {}'.format(epoch, loss_avg))\r\n    \r\n            print(sess.run(accuracy, feed_dict={X: mnist.test.images, y: mnist.test.labels}))\r\n            saver.save(sess, save_model_path)\r\n    \r\n    \r\n    def predict(import_from_meta=False):\r\n        if import_from_meta:\r\n            meta_path = 'mnist_model/model.ckpt.meta'\r\n            checkpoint_path = 'mnist_model'\r\n        else:\r\n            # stupid var WTF ValueError: No variables to save\r\n            _ = tf.Variable(0)\r\n            saver = tf.train.Saver()\r\n    \r\n        with tf.Session() as sess:\r\n            sess.run(tf.global_variables_initializer())\r\n            if import_from_meta:\r\n                saver = tf.train.import_meta_graph(meta_path)\r\n                saver.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\r\n            else:\r\n                saver.restore(sess, save_model_path)\r\n            graph = tf.get_default_graph()\r\n            X = graph.get_tensor_by_name('X:0')\r\n            y = graph.get_tensor_by_name('y:0')\r\n            accuracy = graph.get_tensor_by_name('acc/acc:0')\r\n            print(sess.run(accuracy, feed_dict={X: mnist.test.images, y: mnist.test.labels}))\r\n    \r\n            pred = graph.get_tensor_by_name('pred/predict:0')\r\n            import matplotlib.pyplot as plt\r\n            i = 90\r\n            img_orign = mnist.train.images[i]\r\n            img = img_orign.reshape((28, 28))\r\n            plt.imshow(img, cmap='gray')\r\n            plt.title(mnist.train.labels[i])\r\n            plt.show()\r\n            a = sess.run(pred, feed_dict={X: img_orign.reshape(-1, 784)})\r\n            print(a.shape)\r\n            import numpy as np\r\n            print(np.argmax(a))\r\n    \r\n    \r\n    def check_ckpt():\r\n        from tensorflow.python.tools import inspect_checkpoint as chkp\r\n        chkp.print_tensors_in_checkpoint_file(save_model_path, tensor_name='', all_tensors=True)\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        # train()\r\n        predict(import_from_meta=False)\r\n        # check_ckpt()\r\n\r\n\r\nuse `predict(import_from_meta=False)`\r\n\r\nError:\r\n\r\n\r\n    WARNING:tensorflow:From /Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n    Instructions for updating:\r\n    Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\n    2018-11-08 16:53:40.482921: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key Variable not found in checkpoint\r\n    Traceback (most recent call last):\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n        return fn(*args)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n        options, feed_dict, fetch_list, target_list, run_metadata)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n        run_metadata)\r\n    tensorflow.python.framework.errors_impl.NotFoundError: Key Variable not found in checkpoint\r\n    \t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"/Users/wyx/project/learn-sktf/tf/mnist_clf.py\", line 115, in <module>\r\n        predict(import_from_meta=False)\r\n      File \"/Users/wyx/project/learn-sktf/tf/mnist_clf.py\", line 92, in predict\r\n        saver.restore(sess, save_model_path)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1802, in restore\r\n        {self.saver_def.filename_tensor_name: save_path})\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n        run_metadata_ptr)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n        feed_dict_tensor, options, run_metadata)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n        run_metadata)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n        raise type(e)(node_def, op, message)\r\n    tensorflow.python.framework.errors_impl.NotFoundError: Key Variable not found in checkpoint\r\n    \t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n    \r\n    Caused by op 'save/RestoreV2', defined at:\r\n      File \"/Users/wyx/project/learn-sktf/tf/mnist_clf.py\", line 115, in <module>\r\n        predict(import_from_meta=False)\r\n      File \"/Users/wyx/project/learn-sktf/tf/mnist_clf.py\", line 84, in predict\r\n        saver = tf.train.Saver()\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\r\n        self.build()\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\r\n        self._build(self._filename, build_save=True, build_restore=True)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\r\n        build_save=build_save, build_restore=build_restore)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 835, in _build_internal\r\n        restore_sequentially, reshape)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 472, in _AddRestoreOps\r\n        restore_sequentially)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 886, in bulk_restore\r\n        return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\r\n        shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n        op_def=op_def)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\r\n        op_def=op_def)\r\n      File \"/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\r\n        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n    \r\n    NotFoundError (see above for traceback): Key Variable not found in checkpoint\r\n    \t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nIt's very strange that when I use `predict(import_from_meta=True)` I can get a **right answer**.\r\n\r\nThen I try `check_ckpt()` to [inspect variables in a checkpoint](https://www.tensorflow.org/guide/saved_model#inspect_variables_in_a_checkpoint). I can't find any right tensor_name . That's so funny and `name_scope` just like a joke.\r\n\r\n    tensor_name:  bais\r\n    [-24.933702    1.7660792   3.697866  -14.221888    8.967291   42.149403\r\n      -3.2693458  23.876926  -30.643892   -3.7861202]\r\n    tensor_name:  bais/Adam\r\n    [ 3.1726879e-07 -5.2043208e-07  3.4227469e-05  2.5119303e-07\r\n     -2.0110610e-04  1.8493415e-04 -3.6275055e-06 -1.4343520e-04\r\n     -7.2765622e-05  2.0172486e-04]\r\n    tensor_name:  bais/Adam_1\r\n    [5.2586905e-08 8.9204484e-08 1.5440051e-07 2.9412612e-07 2.4380788e-07\r\n     3.4676964e-07 8.7062219e-08 1.8839150e-07 4.3878950e-07 4.2466107e-07]\r\n    tensor_name:  loss/beta1_power\r\n    0.0\r\n    tensor_name:  loss/beta2_power\r\n    1.2639432e-24\r\n    tensor_name:  weight\r\n    [[-0.03386476  0.03485525 -0.03267809 ... -0.08548199  0.00565728\r\n      -0.01887459]\r\n     [ 0.00370622  0.08523928  0.05811391 ... -0.07838921  0.05987743\r\n       0.074329  ]\r\n     [ 0.0180116   0.04400793 -0.0260816  ...  0.00807328  0.06537797\r\n      -0.07446742]\r\n     ...\r\n     [-0.00665552 -0.03390152 -0.03889231 ... -0.01871967 -0.05968629\r\n       0.07207178]\r\n     [ 0.01317277  0.03459686 -0.03268962 ...  0.07082433  0.03290742\r\n       0.03172391]\r\n     [-0.04514085 -0.03013236  0.01006595 ...  0.01906221  0.02611361\r\n       0.04348358]]\r\n    tensor_name:  weight/Adam\r\n    [[0. 0. 0. ... 0. 0. 0.]\r\n     [0. 0. 0. ... 0. 0. 0.]\r\n     [0. 0. 0. ... 0. 0. 0.]\r\n     ...\r\n     [0. 0. 0. ... 0. 0. 0.]\r\n     [0. 0. 0. ... 0. 0. 0.]\r\n     [0. 0. 0. ... 0. 0. 0.]]\r\n    tensor_name:  weight/Adam_1\r\n    [[0. 0. 0. ... 0. 0. 0.]\r\n     [0. 0. 0. ... 0. 0. 0.]\r\n     [0. 0. 0. ... 0. 0. 0.]\r\n     ...\r\n     [0. 0. 0. ... 0. 0. 0.]\r\n     [0. 0. 0. ... 0. 0. 0.]\r\n     [0. 0. 0. ... 0. 0. 0.]]\r\n    \r\n    Process finished with exit code 0\r\n\r\n\r\nSo what's wrong in my code?  and why must create variables before `tf.train.Saver` when we just want to restore a model?", "comments": ["name_scope applies to the graph operations, but not to variable names, and if variable names do not match, the checkpoint cannot load. For further help in debugging, this question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Have you solved it ? i also meet same problem", "I met the same problem, and i have solved it now. I think it is caused by the implicit use of default graph. So, to solve it, just change the line of \"tf.get_default_graph()\" to \"tf.Graph()\" . What to say, it really works for me. It may help to solve yours.", "@WeifaGan \r\nYou need to change the training directory and it will work then. Because the program reload from the old training directory.", "> @WeifaGan\r\n> You need to change the training directory and it will work then. Because the program reload from the old training directory.\r\n\r\nthanks a lot. It  works!!!\r\nReload from old training directory and then output to a new directory"]}, {"number": 23813, "title": "deleted", "body": "deleted", "comments": []}, {"number": 23812, "title": "Fix incorrect link to version compatibility", "body": "While looking into `tensorflow/java/README.md`,\r\nthe `API stability guarantees` retured 404. This\r\nfix made the change\r\n`version_semantics` -> `version_compat` to fix the issue.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 23811, "title": "Enable while_loop to support timeout", "body": "This PR fixes #22217. \r\n\r\nThe current implementation of `WhileLoop` does not support `Timeout` yet. As a result, even when setting a timeout for a session, the infinite while_loop could not stop as expected. This PR solves this issue by adding `CancellationManager` to `LoopCondOp` to check if the current session is cancelled. If cancelled, then stop the while_loop.   ", "comments": ["LGTM but I'm not very familiar with cancellation, @mrry can you take a look?", "@mrry Thanks for your suggestion. The PR has been revised to raise an error when the loop execution is canceled. Could you have a look at the change?", "@mrry Thanks for reviewing this PR. Do we need to run the test?", "I just set the tests running again. @wt-huang Is anything else needed to move this forward?", "@mrry Some of the tests fail. I'm investigating them and will keep you updated.", "@mrry The test failures are caused by that `CancellationManager` is `nullptr` in the eager mode. This [commit](https://github.com/tensorflow/tensorflow/pull/23811/commits/5bb0553a1b3bd580bd1502ab6b339ca3a1f0b5df) fixes this issue. Do you think the fix is proper? Could you review the changes and trigger the test?", "Yes, that should work.", "@mrry All the tests passed except two windows bazel tests, which seem to be unrelated. If yes, could you approve the change?", "It's already approved as far as I am concerned. @wt-huang Is there something else that needs to be done before starting the pull?", "> It's already approved as far as I am concerned. @wt-huang Is there something else that needs to be done before starting the pull?\r\n\r\nwt-huang is OOO.  I'll help this PR getting merged. But before that, could you please approve it so that I can proceed with next steps. I don't see any approved mark yet(so, cross verifying with you).", "@mrry @harshini-gadige The `Windows Bazel` failure seems to be unrelated. I could not access the logs for `Windows Bazel GPU` and `feedback/copybara`. Are they related to this PR?", "> @mrry @harshini-gadige The `Windows Bazel` failure seems to be unrelated. I could not access the logs for `Windows Bazel GPU` and `feedback/copybara`. Are they related to this PR?\r\n\r\nHey, do not worry. We are taking care of this and it will get merged soon :)  We will let you know if there is any thing to be done from your end. Will keep you posted. ", "@mrry Can you PTAL at the CL created by copybara ?  Looks like the CL is approved but it is not getting merged."]}, {"number": 23810, "title": "tfjs-node, can't compile using tsc / Missing type definitions. ", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 16.04 / Docker\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: N/A\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\nCompiling a index.ts file with something simple as  :\r\n\r\n```typescript\r\nimport * as tf from '@tensorflow/tfjs';\r\n// Load the binding\r\nimport '@tensorflow/tfjs-node';\r\n```\r\nraise this : \r\n```\r\n> tsc\r\n\r\nnode_modules/@tensorflow/tfjs-converter/dist/src/executor/frozen_model.d.ts:16:78 - error TS2304: Cannot find name 'RequestInit'.\r\n\r\n16     constructor(modelUrl: string, weightManifestUrl: string, requestOption?: RequestInit);\r\n                                                                                ~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-converter/dist/src/executor/frozen_model.d.ts:27:103 - error TS2304: Cannot find name 'RequestInit'.\r\n\r\n27 export declare function loadFrozenModel(modelUrl: string, weightsManifestUrl: string, requestOption?: RequestInit): Promise<FrozenModel>;\r\n                                                                                                         ~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/engine.d.ts:94:24 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n94     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;\r\n                          ~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/engine.d.ts:94:36 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n94     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;\r\n                                      ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/engine.d.ts:94:75 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n94     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;\r\n                                                                             ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/io/browser_files.d.ts:14:45 - error TS2304: Cannot find name 'File'.\r\n\r\n14 export declare function browserFiles(files: File[]): IOHandler;\r\n                                               ~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/io/browser_http.d.ts:6:37 - error TS2304: Cannot find name 'RequestInit'.\r\n\r\n6     protected readonly requestInit: RequestInit;\r\n                                      ~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/io/browser_http.d.ts:9:56 - error TS2304: Cannot find name 'RequestInit'.\r\n\r\n9     constructor(path: string | string[], requestInit?: RequestInit, weightPathPrefix?: string);\r\n                                                         ~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/io/browser_http.d.ts:19:83 - error TS2304: Cannot find name 'RequestInit'.\r\n\r\n19 export declare function browserHTTPRequest(path: string | string[], requestInit?: RequestInit, weightPathPrefix?: string): IOHandler;\r\n                                                                                     ~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/io/indexed_db.d.ts:5:35 - error TS2304: Cannot find name 'IDBFactory'.\r\n\r\n5     protected readonly indexedDB: IDBFactory;\r\n                                    ~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/io/local_storage.d.ts:5:28 - error TS2304: Cannot find name 'Storage'.\r\n\r\n5     protected readonly LS: Storage;\r\n                             ~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/io/types.d.ts:24:17 - error TS2304: Cannot find name 'Response'.\r\n\r\n24     responses?: Response[];\r\n                   ~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/io/weights_loader.d.ts:3:88 - error TS2304: Cannot find name 'RequestInit'.\r\n\r\n3 export declare function loadWeightsAsArrayBuffer(fetchURLs: string[], requestOptions?: RequestInit): Promise<ArrayBuffer[]>;\r\n                                                                                         ~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/io/weights_loader.d.ts:4:136 - error TS2304: Cannot find name 'RequestInit'.\r\n\r\n4 export declare function loadWeights(manifest: WeightsManifestConfig, filePathPrefix?: string, weightNames?: string[], requestOptions?: RequestInit): Promise<NamedTensorMap>;\r\n                                                                                                                                         ~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/backend.d.ts:13:24 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n13     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;\r\n                          ~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/backend.d.ts:13:36 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n13     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;\r\n                                      ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/backend.d.ts:13:75 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n13     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;\r\n                                                                             ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/backend.d.ts:40:24 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n40     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor<Rank.R3>;\r\n                          ~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/backend.d.ts:40:36 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n40     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor<Rank.R3>;\r\n                                      ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/backend.d.ts:40:75 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n40     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor<Rank.R3>;\r\n                                                                             ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/backend_cpu.d.ts:14:24 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n14     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;\r\n                          ~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/backend_cpu.d.ts:14:36 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n14     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;\r\n                                      ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/backend_cpu.d.ts:14:75 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n14     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;\r\n                                                                             ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/backend_webgl.d.ts:51:24 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n51     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;\r\n                          ~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/backend_webgl.d.ts:51:36 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n51     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;\r\n                                      ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/backend_webgl.d.ts:51:75 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n51     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;\r\n                                                                             ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_context.d.ts:8:9 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n8     gl: WebGLRenderingContext;\r\n          ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_context.d.ts:24:22 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n24     constructor(gl?: WebGLRenderingContext);\r\n                        ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_context.d.ts:30:61 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n30     uploadPixelDataToTexture(texture: WebGLTexture, pixels: ImageData | HTMLImageElement | HTMLCanvasElement): void;\r\n                                                               ~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_context.d.ts:30:73 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n30     uploadPixelDataToTexture(texture: WebGLTexture, pixels: ImageData | HTMLImageElement | HTMLCanvasElement): void;\r\n                                                                           ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:11:48 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n11 export declare function createVertexShader(gl: WebGLRenderingContext): WebGLShader;\r\n                                                  ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:12:48 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n12 export declare function createVertexBuffer(gl: WebGLRenderingContext): WebGLBuffer;\r\n                                                  ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:13:47 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n13 export declare function createIndexBuffer(gl: WebGLRenderingContext): WebGLBuffer;\r\n                                                 ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:14:46 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n14 export declare function getTextureConfig(gl: WebGLRenderingContext, textureHalfFloatExtension?: any): TextureConfig;\r\n                                                ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:15:56 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n15 export declare function createFloat32MatrixTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): WebGLTexture;\r\n                                                          ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:16:56 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n16 export declare function createFloat16MatrixTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): WebGLTexture;\r\n                                                          ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:17:62 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n17 export declare function createUnsignedBytesMatrixTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): WebGLTexture;\r\n                                                                ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:18:55 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n18 export declare function createPackedMatrixTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): WebGLTexture;\r\n                                                         ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:19:62 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n19 export declare function createFloat16PackedMatrixTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): WebGLTexture;\r\n                                                                ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:20:63 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n20 export declare function bindVertexProgramAttributeStreams(gl: WebGLRenderingContext, program: WebGLProgram, vertexBuffer: WebGLBuffer): boolean;\r\n                                                                 ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:21:54 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n21 export declare function uploadPixelDataToTexture(gl: WebGLRenderingContext, texture: WebGLTexture, pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void;\r\n                                                        ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:21:108 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n21 export declare function uploadPixelDataToTexture(gl: WebGLRenderingContext, texture: WebGLTexture, pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void;\r\n                                                                                                              ~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:21:120 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n21 export declare function uploadPixelDataToTexture(gl: WebGLRenderingContext, texture: WebGLTexture, pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void;\r\n                                                                                                                          ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:21:159 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n21 export declare function uploadPixelDataToTexture(gl: WebGLRenderingContext, texture: WebGLTexture, pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void;\r\n                                                                                                                                                                 ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:22:51 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n22 export declare function uploadMatrixToTexture(gl: WebGLRenderingContext, texture: WebGLTexture, rows: number, columns: number, matrix: Float32Array, numChannels: number, textureConfig: TextureConfig): void;\r\n                                                     ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:23:57 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n23 export declare function uploadMatrixToPackedTexture(gl: WebGLRenderingContext, texture: WebGLTexture, batch: number, rows: number, columns: number, matrix: Float32Array, textureConfig: TextureConfig): void;\r\n                                                           ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:24:64 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n24 export declare function maybeCreateBufferFromOutputTexture(gl: WebGLRenderingContext, texture: WebGLTexture, rows: number, columns: number, textureConfig: TextureConfig): WebGLBuffer | WebGLTexture;\r\n                                                                  ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:25:61 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n25 export declare function downloadFloat32MatrixFromBuffer(gl: WebGLRenderingContext, buffer: WebGLBuffer, rows: number, columns: number, textureConfig: TextureConfig): Float32Array;\r\n                                                               ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:26:68 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n26 export declare function downloadFloat32MatrixFromOutputTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): Float32Array;\r\n                                                                      ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:27:77 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n27 export declare function downloadByteEncodedFloatMatrixFromOutputTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): Float32Array;\r\n                                                                               ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:28:67 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n28 export declare function downloadMatrixFromPackedOutputTexture(gl: WebGLRenderingContext, batch: number, rows: number, cols: number, physicalRows: number, physicalCols: number, textureConfig: TextureConfig): Float32Array;\r\n                                                                     ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:1:45 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n1 export declare function callAndCheck<T>(gl: WebGLRenderingContext, func: () => T): T;\r\n                                              ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:3:45 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n3 export declare function checkWebGLError(gl: WebGLRenderingContext): void;\r\n                                              ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:4:50 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n4 export declare function getWebGLErrorMessage(gl: WebGLRenderingContext, status: number): string;\r\n                                                   ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:5:49 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n5 export declare function getExtensionOrThrow(gl: WebGLRenderingContext, extensionName: string): {};\r\n                                                  ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:6:48 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n6 export declare function createVertexShader(gl: WebGLRenderingContext, vertexShaderSource: string): WebGLShader;\r\n                                                 ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:7:50 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n7 export declare function createFragmentShader(gl: WebGLRenderingContext, fragmentShaderSource: string): WebGLShader;\r\n                                                   ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:8:43 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n8 export declare function createProgram(gl: WebGLRenderingContext): WebGLProgram;\r\n                                            ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:9:41 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n9 export declare function linkProgram(gl: WebGLRenderingContext, program: WebGLProgram): void;\r\n                                          ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:10:45 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n10 export declare function validateProgram(gl: WebGLRenderingContext, program: WebGLProgram): void;\r\n                                               ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:11:54 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n11 export declare function createStaticVertexBuffer(gl: WebGLRenderingContext, data: Float32Array): WebGLBuffer;\r\n                                                        ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:12:53 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n12 export declare function createStaticIndexBuffer(gl: WebGLRenderingContext, data: Uint16Array): WebGLBuffer;\r\n                                                       ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:14:43 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n14 export declare function createTexture(gl: WebGLRenderingContext): WebGLTexture;\r\n                                             ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:16:47 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n16 export declare function createFramebuffer(gl: WebGLRenderingContext): WebGLFramebuffer;\r\n                                                 ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:17:64 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n17 export declare function bindVertexBufferToProgramAttribute(gl: WebGLRenderingContext, program: WebGLProgram, attribute: string, buffer: WebGLBuffer, arrayEntriesPerItem: number, itemStrideInBytes: number, itemOffsetInBytes: number): boolean;\r\n                                                                  ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:18:45 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n18 export declare function bindTextureUnit(gl: WebGLRenderingContext, texture: WebGLTexture, textureUnit: number): void;\r\n                                               ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:19:47 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n19 export declare function unbindTextureUnit(gl: WebGLRenderingContext, textureUnit: number): void;\r\n                                                 ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:20:62 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n20 export declare function getProgramUniformLocationOrThrow(gl: WebGLRenderingContext, program: WebGLProgram, uniformName: string): WebGLUniformLocation;\r\n                                                                ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:21:55 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n21 export declare function getProgramUniformLocation(gl: WebGLRenderingContext, program: WebGLProgram, uniformName: string): WebGLUniformLocation;\r\n                                                         ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:22:64 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n22 export declare function bindTextureToProgramUniformSampler(gl: WebGLRenderingContext, program: WebGLProgram, texture: WebGLTexture, uniformSamplerLocation: WebGLUniformLocation, textureUnit: number): void;\r\n                                                                  ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:23:53 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n23 export declare function bindCanvasToFramebuffer(gl: WebGLRenderingContext): void;\r\n                                                       ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:24:59 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n24 export declare function bindColorTextureToFramebuffer(gl: WebGLRenderingContext, texture: WebGLTexture, framebuffer: WebGLFramebuffer): void;\r\n                                                             ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:25:63 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n25 export declare function unbindColorTextureFromFramebuffer(gl: WebGLRenderingContext, framebuffer: WebGLFramebuffer): void;\r\n                                                                 ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:26:49 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n26 export declare function validateFramebuffer(gl: WebGLRenderingContext): void;\r\n                                                   ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:27:56 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n27 export declare function getFramebufferErrorMessage(gl: WebGLRenderingContext, status: number): string;\r\n                                                          ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/ops/array_ops.d.ts:11:38 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n11 declare function fromPixels_(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels?: number): Tensor3D;\r\n                                        ~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/ops/array_ops.d.ts:11:50 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n11 declare function fromPixels_(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels?: number): Tensor3D;\r\n                                                    ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@tensorflow/tfjs-core/dist/ops/array_ops.d.ts:11:89 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n11 declare function fromPixels_(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels?: number): Tensor3D;\r\n                                                                                           ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/gl-texture2d/index.d.ts:11:18 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n11 type InputType = ImageData | HTMLCanvasElement | HTMLImageElement | HTMLVideoElement;\r\n                    ~~~~~~~~~\r\n\r\nnode_modules/@types/gl-texture2d/index.d.ts:11:50 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n11 type InputType = ImageData | HTMLCanvasElement | HTMLImageElement | HTMLVideoElement;\r\n                                                    ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/gl-texture2d/index.d.ts:11:69 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n11 type InputType = ImageData | HTMLCanvasElement | HTMLImageElement | HTMLVideoElement;\r\n                                                                       ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/gl-texture2d/index.d.ts:26:18 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n26     readonly gl: WebGLRenderingContext;\r\n                    ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/gl-texture2d/index.d.ts:37:32 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n37 declare function texture2d(gl: WebGLRenderingContext, array: ndarray): Texture;\r\n                                  ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/gl-texture2d/index.d.ts:40:9 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n40     gl: WebGLRenderingContext,\r\n           ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl-ext/index.d.ts:11:76 - error TS2304: Cannot find name 'WebGLContextAttributes'.\r\n\r\n11  getContext(contextId: \"webgl\" | \"experimental-webgl\", contextAttributes?: WebGLContextAttributes): (WebGLRenderingContext & WebGL1Extensions) | null;\r\n                                                                              ~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl-ext/index.d.ts:11:102 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n11  getContext(contextId: \"webgl\" | \"experimental-webgl\", contextAttributes?: WebGLContextAttributes): (WebGLRenderingContext & WebGL1Extensions) | null;\r\n                                                                                                        ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl-ext/index.d.ts:22:63 - error TS2304: Cannot find name 'EXT_texture_filter_anisotropic'.\r\n\r\n22  getExtension(name: \"WEBKIT_EXT_texture_filter_anisotropic\"): EXT_texture_filter_anisotropic; // Chrome\r\n                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl-ext/index.d.ts:25:62 - error TS2304: Cannot find name 'WEBGL_compressed_texture_s3tc'.\r\n\r\n25  getExtension(name: \"WEBKIT_WEBGL_compressed_texture_s3tc\"): WEBGL_compressed_texture_s3tc; // Chrome\r\n                                                                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl-ext/index.d.ts:26:52 - error TS2304: Cannot find name 'WEBGL_depth_texture'.\r\n\r\n26  getExtension(name: \"WEBKIT_WEBGL_depth_texture\"): WEBGL_depth_texture; // Chrome\r\n                                                      ~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl-ext/index.d.ts:27:51 - error TS2304: Cannot find name 'WEBGL_lose_context'.\r\n\r\n27  getExtension(name: \"WEBKIT_WEBGL_lose_context\"): WEBGL_lose_context; // Chrome\r\n                                                     ~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:7:37 - error TS2304: Cannot find name 'HTMLElement'.\r\n\r\n7 interface HTMLCanvasElement extends HTMLElement {\r\n                                      ~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:8:81 - error TS2304: Cannot find name 'WebGLContextAttributes'.\r\n\r\n8     getContext(contextId: \"webgl2\" | \"experimental-webgl2\", contextAttributes?: WebGLContextAttributes): WebGL2RenderingContext | null;\r\n                                                                                  ~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:17:42 - error TS2304: Cannot find name 'WebGLRenderingContext'.\r\n\r\n17 interface WebGL2RenderingContext extends WebGLRenderingContext {\r\n                                            ~~~~~~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:337:47 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n337         format: number, type: number, source: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                  ~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:337:59 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n337         format: number, type: number, source: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                              ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:337:98 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n337         format: number, type: number, source: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                                                                     ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:339:61 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n339         format: number, type: number, source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                                ~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:339:73 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n339         format: number, type: number, source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                                            ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:339:112 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n339         format: number, type: number, source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                                                                                   ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:345:47 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n345         format: number, type: number, source: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                  ~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:345:59 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n345         format: number, type: number, source: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                              ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:345:98 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n345         format: number, type: number, source: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                                                                     ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:347:61 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n347         format: number, type: number, source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                                ~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:347:73 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n347         format: number, type: number, source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                                            ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:347:112 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n347         format: number, type: number, source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                                                                                   ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:354:31 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n354         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                  ~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:354:43 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n354         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                              ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:354:82 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n354         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                                                     ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:363:31 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n363         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                  ~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:363:43 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n363         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                              ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:363:82 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n363         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                                                     ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:374:31 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n374         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                  ~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:374:43 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n374         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                              ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:374:82 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n374         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                                                     ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:384:31 - error TS2304: Cannot find name 'ImageData'.\r\n\r\n384         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                  ~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:384:43 - error TS2304: Cannot find name 'HTMLImageElement'.\r\n\r\n384         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                              ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:384:82 - error TS2304: Cannot find name 'HTMLVideoElement'.\r\n\r\n384         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException\r\n                                                                                     ~~~~~~~~~~~~~~~~\r\n\r\nnode_modules/@types/webgl2/index.d.ts:560:72 - error TS2304: Cannot find name 'WebGLActiveInfo'.\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nMy tsconfig : \r\n```\r\n{\r\n  \"compilerOptions\": {\r\n    \"strict\": true,\r\n    \"noImplicitAny\": false,\r\n    \"target\": \"es2017\",\r\n    \"module\": \"commonjs\",\r\n    \"sourceMap\": true,\r\n    \"skipLibCheck\": true,\r\n    \"lib\": [\r\n      \"esnext\"\r\n    ],\r\n    \"moduleResolution\": \"node\",\r\n    \"outDir\": \"dist\",\r\n    \"baseUrl\": \"dist\",\r\n    \"noEmitOnError\": true\r\n  },\r\n  \"include\": [\r\n    \"src\"\r\n  ],\r\n  \"exclude\": [\r\n    \"node_modules\",\r\n    \"dist\",\r\n    \"eth-contracts\"\r\n  ]\r\n}\r\n```\r\n\r\nAny advices ? \r\n", "comments": ["@harshini-gadige Unfortunately, I don't know anything about TFJS.\r\n\r\n@dsmilkov, do you know who should take a look at this? ", "Thanks @angersson, someone can at least try to reproduce this issue ? ", "Finally found. Added missing lib in tsconfig (\"es2015\", \"dom\"). "]}, {"number": 23809, "title": "Cannot deploy trained model: google.protobuf.message.DecodeError: Error parsing message", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS High Sierra \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.18\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: No GPU\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen I try to deploy my trained model using this function:\r\ndef load_graph(frozen_graph_filename):\r\n\twith tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n\t\tgraph_def = tf.GraphDef()\r\n\t\tgraph_def.ParseFromString(f.read())\r\n\r\n\twith tf.Graph().as_default() as graph:\r\n\t\ttf.import_graph_def(graph_def, name='prefix')\r\n\treturn graph\r\nI get this error: google.protobuf.message.DecodeError: Error parsing message\r\nat the line that goes:\r\ngraph_def.ParseFromString(f.read())\r\n\r\n**Describe the expected behavior**\r\nFor all my other trained models, this error doesn't pop up, and this method does work. However, this model is one I trained myself. I got it from the export folder the TF Object Detection API made in my folder.\r\n\r\n**Code to reproduce the issue**\r\ndef load_graph(frozen_graph_filename):\r\n\twith tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n\t\tgraph_def = tf.GraphDef()\r\n\t\tgraph_def.ParseFromString(f.read())\r\n\r\n\twith tf.Graph().as_default() as graph:\r\n\t\ttf.import_graph_def(graph_def, name='prefix')\r\n\treturn graph\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/spencerkraisler/Desktop/raccoon_tutorial/model_deploy.py\", line 27, in <module>\r\n    detection_graph = load_graph(GRAPH_PATH)\r\n  File \"/Users/spencerkraisler/Desktop/raccoon_tutorial/model_deploy.py\", line 10, in load_graph\r\n    graph_def.ParseFromString(f.read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message", "comments": ["Personally, I use this kind of shenanigans to make it work:\r\n\r\ndef openGraph():\r\ngraph = tf.Graph()\r\ngraphDef = tf.GraphDef()\r\nwith open([path], \"rb\") as graphFile:\r\ngraphDef.ParseFromString(graphFile.read())\r\n\r\nwith graph.as_default():\r\ntf.import_graph_def(graphDef)\r\n\r\nreturn graph\r\n\r\n\r\nAnd then I use this function like this:\r\n\r\ngraph = openGraph()\r\n\r\nwith tf.Session(graph=graph) as sess:\r\nblahblahblah\r\n\r\n\r\n\r\n\r\nNot much different from you, but maybe creating the graph inside of the function could work? If not, what code are you using to save the graph? It looks like it's litterally an error inside the graph file...", "I found this method could load pb file [link](https://gitlab.lyle.work/lkozloff/clipper/commit/b2b5d8d38094297c973ae7840f2f9ce001edd097)", "This also breaks the [Tensorboard import tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py) when I try to use it on a model that was trained with [RLlib](https://ray.readthedocs.io/en/latest/rllib.html).", "I was stuck with a similar error but then I was able to resolve it. Also answered something similar on [stackoverflow](https://stackoverflow.com/questions/49117938/unable-to-use-trained-tensorflow-model/55981972#55981972)", "Here is a solution load the graph. It is working fine.\r\n\r\n```\r\ndef load_model():\r\n    with tf.gfile.GFile(path, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(graph_def, name=\"\")\r\n    return graph\r\n\r\nif __name__=='__main__':\r\n    path = D:/path/to/your/dot pb file\r\n    graph = load_model()\r\n    with tf.Session(graph=graph) as sess:\r\n        for op in graph.get_operations():\r\n            print(op.name)\r\n```\r\nHope it helps.", "@spencerkraisler Did you try the solution provided by @mohapatras. Let us know if that helps. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23809\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23809\">No</a>\n", "detection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n    od_graph_def = tf.compat.v1.GraphDef()\r\n    with tf.io.gfile.GFile(PATH_TO_CRYPT, 'rb') as fid:\r\n        serialized_graph = fid.read()\r\n        od_graph_def.ParseFromString(serialized_graph)\r\n        tf.import_graph_def(od_graph_def, name='')\r\n\r\nHi, I used the same code. But I am getting the following error. I am using TF2.0\r\n\r\nDecodeError                               Traceback (most recent call last)\r\n<ipython-input-7-6918b89350a6> in <module>()\r\n     25     with tf.io.gfile.GFile(PATH_TO_CRYPT, 'rb') as fid:\r\n     26         serialized_graph = fid.read()\r\n---> 27         od_graph_def.ParseFromString(serialized_graph)\r\n     28         tf.import_graph_def(od_graph_def, name='')\r\n     29 \r\n\r\nDecodeError: Error parsing message\r\n\r\nCould anybody help me on this?", "Why there are so many downvotes here? I can not find anything helpful :(", " i was using the model from saved_model folder and had the same issue. Get your model from trained_inference_graph folder. it should be sth like: frozen_inference_graph.pb", "In my case, the protobuffer data that saved by SavedModel API and frozen_graph is different binary formats. You should be careful to use the different save model APIs provided by Tensorflow. According to your descirption, maybe frozen_graph is more suitable way.\r\nYou can use two different ways to save [froze_graph()[]:](https://leimao.github.io/blog/Save-Load-Inference-From-TF-Frozen-Graph/)\r\n`# Save check point for graph frozen later\r\n        ckpt_filepath = self.save(directory=directory, filename=filename)\r\n        pbtxt_filename = filename + '.pbtxt'\r\n        pbtxt_filepath = os.path.join(directory, pbtxt_filename)\r\n        pb_filepath = os.path.join(directory, filename + '.pb')\r\n        # This will only save the graph but the variables will not be saved.\r\n        # You have to freeze your model first.\r\n        tf.train.write_graph(graph_or_graph_def=self.sess.graph_def, logdir=directory, name=pbtxt_filename, as_text=True)\r\n\r\n        # Freeze graph\r\n        # Method 1\r\n        freeze_graph.freeze_graph(input_graph=pbtxt_filepath, input_saver='', input_binary=False, input_checkpoint=ckpt_filepath, output_node_names='cnn/output', restore_op_name='save/restore_all', filename_tensor_name='save/Const:0', output_graph=pb_filepath, clear_devices=True, initializer_nodes='')\r\n        \r\n        # Method 2\r\n        '''\r\n        graph = tf.get_default_graph()\r\n        input_graph_def = graph.as_graph_def()\r\n        output_node_names = ['cnn/output']\r\n\r\n        output_graph_def = graph_util.convert_variables_to_constants(self.sess, input_graph_def, output_node_names)\r\n        # For some models, we would like to remove training nodes\r\n        # output_graph_def = graph_util.remove_training_nodes(output_graph_def, protected_nodes=None)\r\n\r\n        with tf.gfile.GFile(pb_filepath, 'wb') as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n        '''`\r\nif you use:\r\n`builder = tf.saved_model.builder.SavedModelBuilder(dump_filepath)` \r\nto save model, and `graph_def.ParseFromString()` to parse saved model, you will get `google.protobuf.message.DecodeError:Error parsing message`  this Error.", "if your .pb model came from `tf.export_graph` (which may be converted by onnx), which may be not a frozen_graph,\r\ntry:\r\n1\uff09use saved_model.loader.load to load **metagraph** \r\n2\uff09get input\\output tensor from **signature_def** \r\n3\uff09finally sess.run \r\n\r\n```python\r\nsess=tf.compat.v1.Session()\r\nmetagraph=tf.compat.v1.saved_model.loader.load(sess,['serve'],'/path_to_pb_project') \r\n# tag here may be different ,first time you can run this code with blank tag [ ] ,error report will tell you the tag of metagraph from the path.\r\nsig=metagraph.signature_def['serving_default']\r\ninput_dict=dict(sig.inputs)\r\noutput_dict=dict(sig.outputs)\r\nprint(input_dict,'\\n',output_dict)\r\ninput_obs_label_0=input_dict['input'].name\r\noutput_stochastic_label_0=output_dict['output'].name\r\n\r\nactual_input={input_obs_label_0:np.random.randn(6,150,224,224)} # fill here with your actual input \r\nout=sess.run(output_stochastic_label_0,feed_dict=actual_input)\r\nprint(out)\r\n```\r\nhere I use tensorflow2.X\r\nalso can take the latest saved_model api  <https://www.tensorflow.org/api_docs/python/tf/saved_model/load>\r\nif u use tensorflow1.X, the `saved_model.load()` should be `tf.saved_model.loader.load()`", "> detection_graph = tf.Graph() with detection_graph.as_default(): od_graph_def = tf.compat.v1.GraphDef() with tf.io.gfile.GFile(PATH_TO_CRYPT, 'rb') as fid: serialized_graph = fid.read() od_graph_def.ParseFromString(serialized_graph) tf.import_graph_def(od_graph_def, name='')\r\n> \r\n> Hi, I used the same code. But I am getting the following error. I am using TF2.0\r\n> \r\n> DecodeError Traceback (most recent call last) in () 25 with tf.io.gfile.GFile(PATH_TO_CRYPT, 'rb') as fid: 26 serialized_graph = fid.read() ---> 27 od_graph_def.ParseFromString(serialized_graph) 28 tf.import_graph_def(od_graph_def, name='') 29\r\n> \r\n> DecodeError: Error parsing message\r\n> \r\n> Could anybody help me on this?\r\n\r\nDid you solve this error?\r\ni too had this error in AWS....but works in kaggle with no issues...\r\n\r\nhow to solve this?????"]}, {"number": 23808, "title": "Add option to disable nccl", "body": "In platforms like embedded systems, nccl is not supported and doesn't make sense. This PR adds a bazel config flag to disable nccl portions of TF for such platforms.", "comments": ["@gunan It looks like it is a buildifier issue. If + sign is the last non-white character on the line it is indenting. If I add the select afterwards with opening braces, it is same as the old one.", "Either way is OK with me, as long as we make the tests pass. Let's target merging this change today.", "I believe the current failures are due to CI rather than the PR. Feel free to restart the tests or merge.", "We had to roll this back internally due to some dependencies.  We'll fix the dependencies and get this resubmitted.", "@nnorwitz when can we have this in?", "I have prepared a rollforward with a fix. It is pending review right now,\nand should be submitted by the end of tomorrow the latest.\n\nOn Mon, Nov 26, 2018 at 9:49 AM Sami Kama <notifications@github.com> wrote:\n\n> @nnorwitz <https://github.com/nnorwitz> when can we have this in?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/23808#issuecomment-441731640>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCORX8Thm79NTd8b4H1YerJYnoZaIjks5uzCmRgaJpZM4YnFf9>\n> .\n>\n"]}, {"number": 23807, "title": "Convert pb model to tflite model failed in docker environment", "body": "I m using the docker image pulled from here:\r\nhttps://hub.docker.com/r/tensorflow/tensorflow/tags\r\nthe tag is nightly-devel-py3.\r\nAnd then I upgrade the tf-nightly to  tf-nightly 1.13.0.dev20181116 using pip.\r\n\r\n**System information**\r\n- i7-8650U+16G+ 1060\r\n- Windows Pro Docker Environment(CPU mode)\r\n- tf-nightly 1.13.0.dev20181116\r\n- Python version:\r\n\r\nIt failed when I trying to convert pb model to tflite model.\r\nThe error info says that some operators doesn't supported in the TFLiteConverter.\r\nCan anyone help me to fix it?\r\nThanks.\r\n\r\n**My Command Line**\r\ntflite_convert --output_file='/data/fast-neural-style-train&test/models/result.tflite' --graph_def_file='/data/fast-neural-style-train&test/models/test_model.pb' --input_arrays=input --output_arrays=output_new --input_shapes=256,256,3\r\n\r\n**Error Info**\r\n2018-11-16 03:45:33.142433: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 421, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 417, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 170, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/lite.py\", line 456, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/convert.py\", line 397, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/convert.py\", line 172, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2018-11-16 03:45:34.088574: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: Round\r\n2018-11-16 03:45:34.095390: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: Round\r\n2018-11-16 03:45:34.095586: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2018-11-16 03:45:34.095621: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2018-11-16 03:45:34.095652: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2018-11-16 03:45:34.095684: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2018-11-16 03:45:34.095801: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2018-11-16 03:45:34.095848: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2018-11-16 03:45:34.095925: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.096229: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.096281: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference\r\n2018-11-16 03:45:34.096329: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.096374: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference\r\n2018-11-16 03:45:34.096455: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.096501: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference\r\n2018-11-16 03:45:34.096631: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.096783: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.096957: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.097208: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.097403: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.097626: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.097862: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.098174: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.098459: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.098733: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.098940: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.098991: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference\r\n2018-11-16 03:45:34.099099: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.099149: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference\r\n2018-11-16 03:45:34.099194: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad\r\n2018-11-16 03:45:34.099242: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference\r\n2018-11-16 03:45:34.100307: F tensorflow/lite/toco/tooling_util.cc:1020] Check failed: array->has_shape()\r\nAborted\r\n", "comments": ["Errors are for:  convert the model into tflite compatible operations. \r\n\r\nMy Docker file for this looks like this for comparison.\r\n\r\nFROM python:3.6-slim\r\nARG UID\r\nARG TAG_NAME\r\nENV TERM linux\r\nENV DEBIAN_FRONTEND noninteractive\r\nARG proxy\r\nENV http_proxy $proxy\r\nENV https_proxy $proxy\r\nCOPY ZscalerRootCA.crt /usr/local/share/ca-certificates/\r\nRUN chmod 644 /usr/local/share/ca-certificates/ZscalerRootCA.crt\r\nRUN echo \"check_certificate = off\" >> ~/.wgetrc\r\n#RUN apt-get install --allow-unauthenticated -y ca-certificates\r\n#RUN update-ca-certificates \r\nRUN pip install --trusted-host pypi.python.org --trusted-host pypi.org --trusted-host files.pythonhosted.org tf_nightly\r\nRUN apt-get update && apt-get -y install ffmpeg && rm -r /var/lib/apt/lists\r\n\r\nADD scripts/ /usr/local/bin/scripts\r\n\r\nRUN chmod +x /usr/local/bin/scripts/convert_quantized_tflite.py\r\nRUN chmod +x /usr/local/bin/scripts/count_ops.py\r\nRUN chmod +x /usr/local/bin/scripts/dump_operations.py\r\nRUN chmod +x /usr/local/bin/scripts/evaluate.py\r\nRUN chmod +x /usr/local/bin/scripts/graph_pb2tb.py\r\nRUN chmod +x /usr/local/bin/scripts/hello.py\r\nRUN chmod +x /usr/local/bin/scripts/inspect.py\r\nRUN chmod +x /usr/local/bin/scripts/label_image.py\r\nRUN chmod +x /usr/local/bin/scripts/post_quantize_lite.py\r\nRUN chmod +x /usr/local/bin/scripts/quantize_graph.py\r\nRUN chmod +x /usr/local/bin/scripts/retrain.py\r\nRUN chmod +x /usr/local/bin/scripts/show_image.py\r\nRUN chmod +x /usr/local/bin/scripts/__init__.py\r\nRUN chmod +x /usr/local/bin/scripts/__init__.pyc\r\nRUN chmod +x /usr/local/bin/scripts/retrainV2.py\r\n\r\nWORKDIR /usr/local/bin", "Marking issue as resolved due to inactivity. Feel free to re-open this if it's unresolved or file a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose)"]}, {"number": 23806, "title": "Register.h not found", "body": "For the Xcode iOS project, I follow the steps, however during the build for device process, Xcode gives me an error stating that it cannot find the register.h file.\r\n\r\n**System information**\r\n- OS Platform and Distribution: macOS Mojave 10.14.2 and High Sierra 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): clones repo, followed instructions on website\r\n- TensorFlow version: 1.12\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Steps ( sorry for not posting in first post ):\r\n\r\nFollow steps exactly as stated on TensorFlow website for the iOS demo project. Also in terminal, checked to make sure python was installed, brew, automate and lib tool, cocoa pods.", "I suspect this may be caused by commit 61c6c84964b4aec80aeace187aab8cb2c3e55a72.", "@michael-stram I can confirm that it works if you checkout 61c6c84964b4aec80aeace187aab8cb2c3e55a72~ and move into tensorflow/contrib/lite/examples/ios/camera", "Thank you Jay!!! umm, I'm about to ask something stupid..what do you mean by check out. Do I change the file path in Xcode  of the register.h file? Thanks!!", "@michael-stram Try this: close xcode, do `git checkout  61c6c84~` in terminal, and then follow the original instructions from the guide using `tensorflow/contrib/lite/examples/ios/camera` instead of `tensorflow/lite/examples/ios/camera`. Do the same pod install command and then open the xcworkspace file.", "Ohhhh...Thank you!", "Thanks Jay, I follow your steps as well for downloading the models? \r\n", "I am trying to do the check out but I am getting an error saying its not a repository ?\r\n\r\nEDIT: I had to CD to the directory \r\n", "What I\u2019ve been doing is either editing the pod file to specify the\nXcodeproj or just removing the second project.\n\nOn Sun, Nov 18, 2018 at 4:13 PM michael-stram <notifications@github.com>\nwrote:\n\n> Hey Jay there's always two Xcode projects in that folder, do I remove one\n> of them for the pod install?\n>\n> Thanks!\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23806#issuecomment-439738982>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AB9DW8T1W09faB-piOevfXeT5RVTtyTGks5uwffDgaJpZM4Ymq5w>\n> .\n>\n", "I have the same issue. And it is independent from a concrete device:\r\n\r\n- OS Platform and Distribution: OSX High Sierra 10.13.6\r\n- Mobile device: iPhone 7, iOS 12.x\r\n- TensorFlow installed from (source or binary):\r\nhttps://www.tensorflow.org/lite/demo_ios\r\n- TensorFlow version: 1.12.0\r\nOther options have no influence because the project is not even compiled due to incorrect paths to files.", "@gerchicov-bp: Does following instructions https://github.com/tensorflow/tensorflow/issues/23806#issuecomment-439554347 not help.\r\n", "Can you please follow the updated instructions here \r\nhttps://www.tensorflow.org/lite/demo_ios\r\nand let me know if it is not working.\r\n\r\nThanks", "Sorry folks it is still not working.. somethings seams to be wrong with the paths to the framework.\r\n![screen shot 2019-02-16 at 20 27 59](https://user-images.githubusercontent.com/16985927/52904227-5e616580-3229-11e9-8ea7-50090afc8d3f.png)\r\n", "We are sorry for the breakage.\r\nWill be sending a fix. You can fix it manually by removing the prefix  \"third_party/\".\r\n\r\nThanks and sorry for the breakage.", "Thx! I found out meanwhile\ud83e\udd17\n\nVon meinem iPhone gesendet\n\n> Am 17.02.2019 um 07:19 schrieb karimnosseir <notifications@github.com>:\n> \n> We are sorry for the breakage.\n> Will be sending a fix. You can fix it manually by removing the prefix \"third_party/\".\n> \n> Thanks and sorry for the breakage.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "Fix submitted already. closing the bug.\r\nPlease file new /reopen if you have any problems/questions.\r\n\r\nThanks"]}, {"number": 23805, "title": "Add hello", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Closing this pull request."]}, {"number": 23804, "title": "remove op Save also in remove_training_nodes method", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): master branch\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn current `remove_training_nodes` method, we just remove `CheckNumerics` and `Identity` (conditional).\r\nAs `Save` is a training nodes, I think we should remove it also.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\n`remove_training_nodes` users.\r\n", "comments": ["I think `remove_training_nodes` is being removed in 2.0, so it might not make sense to update it. @martinwicke do you know what's going on with this?", "Yeah, let's not touch this. In any case, this would be a breaking change, so we'd need a new option, and I don't think this is worth doing."]}, {"number": 23803, "title": "TF1.12 building w/ py3.6.4 on HPC", "body": "Hello, \r\nI am trying to (re-)build Tensorflow on the HPC I use. I already managed to build once TF 1.9 but with Python 3.4 and probably another compiler. For the rest I tried to keep everything the same, such as bazel with the suggested version. Unfortunately I was able to build only GCC 5.5.0 instead of 4.8.0, but I don't think it is the problem here as I am using the flag --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\". \r\n\r\nIn this case I am trying to build TF 1.12 with Py3.6.4, but I am experiencing problems (a bit different) with TF 1.9 as well. I am not sure of the path to the Python Library that I give at the beginning.\r\nThanks.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 3.16.51-3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 1.5.0\r\n- GCC/Compiler version (if compiling from source): 5.5.0\r\n- CUDA/cuDNN version: 9.1.85/7.1.2\r\n- GPU model and memory: K80\r\n\r\n**Problem Description**\r\nERROR: /mnt/gaiagpfs/users/homedirs/ccimarelli/.cache/bazel/_bazel_ccimarelli/9669602cb65c53050fa19def8832a149/external/nasm/BUILD.bazel:8:1: undeclared\r\ninclusion(s) in rule '@nasm//:nasm':\r\nthis rule is missing dependency declarations for the following files included by 'external/nasm/x86/regflags.c':\r\n  '/home/users/ccimarelli/opt/gcc/GCC-5.5.0/lib/gcc/x86_64-unknown-linux-gnu/5.5.0/include-fixed/limits.h'\r\n  '/home/users/ccimarelli/opt/gcc/GCC-5.5.0/lib/gcc/x86_64-unknown-linux-gnu/5.5.0/include-fixed/syslimits.h'\r\n  '/home/users/ccimarelli/opt/gcc/GCC-5.5.0/lib/gcc/x86_64-unknown-linux-gnu/5.5.0/include/stddef.h'\r\n  '/home/users/ccimarelli/opt/gcc/GCC-5.5.0/lib/gcc/x86_64-unknown-linux-gnu/5.5.0/include/stdarg.h'\r\n\r\n**Sequence of commands / steps executed before running into the problem**\r\n$ ./cofigure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.0 installed.\r\nPlease specify the location of python. [Default is /home/users/.../virtualenvs/ml-full/bin/python]:\r\nPlease input the desired Python library path to use.  Default is [/home/users/.../virtualenvs/ml-full/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with Apache Ignite support? [Y/n]: n\r\nNo Apache Ignite support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.1\r\n\r\nPlease specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /opt/apps/resif/data/production/v1.1-20180718/default/software/system/CUDA/9.1.85\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.1\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /opt/apps/resif/data/production/v1.1-20180718/default/software/system/CUDA/9.1.85]: /opt/apps/resif/data/production/v1.1-20180718/default/software/numlib/cuDNN/7.1.2-CUDA-9.1.85\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]:\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may\r\nhave worse performance with multiple GPUs. [Default is 2.2]: 1.3\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.7,3.7,3.7,3.7,3.7,3.7,3.7,3.7]: 3.5,3.7,7.0\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]:\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /home/users/ccimarelli/opt/gcc/GCC-5.5.0/bin/gcc]:\r\n\r\n$ bazel build --config=opt --config=cuda --spawn_strategy=standalone --action_env=TMP=/home/users/ccimarelli/tmp --verbose_failures --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n\r\n", "comments": ["It looks like you configured with one GCC version before, so bazel workspace is corrupted.\r\nCould you try this:\r\n```\r\nbazel clean --expunge_async\r\n```\r\nThen reconfigure and rebuild?", "Yes, sorry I had no time this two weeks to test the solution yet. I will try asap. Thanks for your help!!", "I tried again using the command to clean before reconfiguring. \r\n I tried both with gcc 6.4.0, 5.5.0 and 4.8.0. \r\nWith gcc 5.5.0 I got this error:\r\n\r\n> ERROR: /mnt/gaiagpfs/users/homedirs/ccimarelli/.cache/bazel/_bazel_ccimarelli/9669602cb65c53050fa19def8832a149/external/double_conversion/BUILD.bazel:12:1: undeclared inclusion(s) in rule '@double_conversion//:double-conversion':\r\n> this rule is missing dependency declarations for the following files included by 'external/double_conversion/double-conversion/diy-fp.cc':\r\n>   '/home/users/ccimarelli/opt/gcc/GCC-5.5.0/lib/gcc/x86_64-unknown-linux-gnu/5.5.0/include/stddef.h'\r\n>   '/home/users/ccimarelli/opt/gcc/GCC-5.5.0/lib/gcc/x86_64-unknown-linux-gnu/5.5.0/include/stdint.h'\r\n\r\nwith gcc 6.4.0:\r\n\r\n> ERROR: \r\n>  /mnt/gaiagpfs/users/homedirs/ccimarelli/.cache/bazel/_bazel_ccimarelli/9669602cb65c53050fa19def8832a149/external/nasm/BUILD.bazel:8:1: undeclared inclusion(s) in rule '@nasm//:nasm':\r\n> this rule is missing dependency declarations for the following files included by 'external/nasm/x86/insnsn.c':\r\n>   '/opt/apps/resif/data/production/v1.1-20180718/default/software/compiler/GCCcore/6.4.0/lib/gcc/x86_64-pc-linux-gnu/6.4.0/include-fixed/limits.h'\r\n>   '/opt/apps/resif/data/production/v1.1-20180718/default/software/compiler/GCCcore/6.4.0/lib/gcc/x86_64-pc-linux-gnu/6.4.0/include-fixed/syslimits.h'\r\n>   '/opt/apps/resif/data/production/v1.1-20180718/default/software/compiler/GCCcore/6.4.0/lib/gcc/x86_64-pc-linux-gnu/6.4.0/include/stddef.h'\r\n>   '/opt/apps/resif/data/production/v1.1-20180718/default/software/compiler/GCCcore/6.4.0/lib/gcc/x86_64-pc-linux-gnu/6.4.0/include/stdarg.h'\r\n\r\n'\r\n\r\nand with gcc 4.8.0: \r\n\r\n> gcc-4.8: error trying to exec 'cc1plus': execvp: No such file or directory\r\n\r\nI am using this command alway to compile for the compatibility of the compiler:\r\n`bazel build --config=opt --config=cuda --action_env=TMP=/home/users/ccimarelli/tmp --verbose_failures --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --spawn_strategy=standalone //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nalways using python 3.6.4 and bazel 0.15.0...\r\n\r\nLast thing I alway receive this WARNINGS:\r\n```\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: /mnt/gaiagpfs/users/homedirs/ccimarelli/.cache/bazel/_bazel_ccimarelli/9669602cb65c53050fa19def8832a149/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /mnt/gaiagpfs/users/homedirs/ccimarelli/.cache/bazel/_bazel_ccimarelli/9669602cb65c53050fa19def8832a149/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /mnt/gaiagpfs/users/homedirs/ccimarelli/.cache/bazel/_bazel_ccimarelli/9669602cb65c53050fa19def8832a149/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /mnt/gaiagpfs/users/homedirs/ccimarelli/.cache/bazel/_bazel_ccimarelli/9669602cb65c53050fa19def8832a149/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /mnt/gaiagpfs/users/homedirs/ccimarelli/.cache/bazel/_bazel_ccimarelli/9669602cb65c53050fa19def8832a149/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /mnt/gaiagpfs/users/homedirs/ccimarelli/.cache/bazel/_bazel_ccimarelli/9669602cb65c53050fa19def8832a149/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /mnt/gaiagpfs/users/workdirs/ccimarelli/git/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /mnt/gaiagpfs/users/workdirs/ccimarelli/git/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /mnt/gaiagpfs/users/workdirs/ccimarelli/git/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /mnt/gaiagpfs/users/workdirs/ccimarelli/git/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /mnt/gaiagpfs/users/workdirs/ccimarelli/git/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /mnt/gaiagpfs/users/workdirs/ccimarelli/git/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /mnt/gaiagpfs/users/workdirs/ccimarelli/git/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /mnt/gaiagpfs/users/workdirs/ccimarelli/git/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\n```\r\n\r\nI really hope you can help. Thanks in advance.\r\n", "tried with gcc 4.9.0 as well..\r\n\r\n> ERROR: /mnt/gaiagpfs/users/homedirs/ccimarelli/.cache/bazel/_bazel_ccimarelli/9669602cb65c53050fa19def8832a149/external/gif_archive/BUILD.bazel:8:1: undeclared inclusion(s) in rule '@gif_archive//:gif':\r\nthis rule is missing dependency declarations for the following files included by 'external/gif_archive/lib/gif_font.c':\r\n  '/home/users/ccimarelli/bin/gcc-4.9/lib/gcc/x86_64-unknown-linux-gnu/4.9.0/include/stddef.h'\r\n  '/home/users/ccimarelli/bin/gcc-4.9/lib/gcc/x86_64-unknown-linux-gnu/4.9.0/include/stdbool.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build", "Ok, I am advancing finally.\r\n I discovered that the global path I was using on the HPC was not good for finding dependencies by bazel. Hence, I used the full path (starting with `/mnt/gaiagpfs` in my case) instead of the mounted directory ( I got this hint by luck and trying if something changed ). \r\nAfter that, I had to follow this guide http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html because the linker that bazel was using was not the correct version. \r\nNow I am experiencing a different problem similar to the issue #20592 and related with Google Cloud integration:\r\n\r\n> external/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/bigtable/internal/rowreaderiterator.h:60:44: error: ambiguous overload for 'operator*' (operand type is 'std::remove_reference<const google::cloud::v0::internal::optional<google::cloud::bigtable::v0::Row>&>::type {aka const google::cloud::v0::internal::optional<google::cloud::bigtable::v0::Row>}')\r\n   Row const&& operator*() const&& { return *std::move(row_); }\r\n                                            ^\r\nexternal/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/bigtable/internal/rowreaderiterator.h:60:44: note: candidates are:\r\nIn file included from external/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/bigtable/internal/rowreaderiterator.h:19:0,\r\n                 from external/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/bigtable/internal/rowreaderiterator.cc:15:\r\nexternal/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/internal/optional.h:142:22: note: constexpr const T& google::cloud::v0::internal::optional<T>::operator*() const & [with T = google::cloud::bigtable::v0::Row]\r\n   constexpr T const& operator*() const& {\r\n                      ^\r\nexternal/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/internal/optional.h:147:23: note: constexpr const T&& google::cloud::v0::internal::optional<T>::operator*() const && [with T = google::cloud::bigtable::v0::Row]\r\n   constexpr T const&& operator*() const&& {\r\n                       ^", "@gunan,  I applied the fix you suggest in the issue #23771 with the commit [3437098](https://github.com/tensorflow/tensorflow/commit/3437098ba5b111817ef6ac5906d86934168704b7)  changing all the files, but I am not able to avoid compiling also GCP : / . Do you have any suggestion?", "Completely missed this issue under the mountain of emails.\r\nLooks like initial issue was due to bazel not being able to auto detect the toolchain.\r\n\r\nFor the latter problem. I remember other reports of the same issue.\r\nI think they were resolved, but I may be mistaken. if not, @mihaimaruseac is working to move all cloud filesystem support out of the main package, which should help with this.\r\n\r\n@mihaimaruseac could you close this issue once gcs support is modularized and migrated out of TF?", "Taking ownership of this and will get to solve it in time", "Hi @ClaudioCimarelli ! \r\nYou are using older versions(1.x versions) of Tensorflow which is not supported any more. Have you checked this [thread ](https://its.tntech.edu/display/MON/Installing+TensorFlow+in+Your+HPC+Account)on using Tensorflow on HPC cluster though? ", "Hey @mohantym, thank you for your reply. I moved on from this nightmare and found an alternative solution at the time. "]}, {"number": 23802, "title": "Reduced redundancy in Estimator api", "body": "\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.10+\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe TensorFlow estimator api, while a welcomed addition, seems to add a lot of redundancy to the workflow as well as unnecessary complexity. While a prominent goal of the estimator API is to decouple the input pipelines from the model, unfortunately this is not fully realized due to this redundancy and complexity.\r\n\r\n# Example\r\n\r\nSuppose we wish to make a custom estimator that has an `input_fn` which reads from TF `Record` files via `tf.data.TFRecordDataset(files)` and further, we wish to save our trained model so we can use it later (i.e. we need a `serving_input_reciever_fn`).  \r\n\r\nFor simplicity lets assume for a single \"example\" that after our input pipeline we are left with a tensor with shape [2,3], perhaps:\r\n\r\n    [[0,0,1],[1,0,0]]\r\n\r\nand let us say that contextually we will refer to this feature as \"main_input\".\r\n\r\nThen we now need to wrap this into a `(Sequence)Example` (*NOTE*: see this [S.O. post](https://stackoverflow.com/questions/52035692/tensorflow-v1-10-store-images-as-byte-strings-or-per-channel) for the lack of clarity the documentation provides on how to best do this)\r\n\r\n\r\n\r\n```\r\n# all of input pipeline here\r\n\r\n# processed data\r\nmain_input = np.array([\r\n    [0, 0, 1],\r\n    [1, 0, 0],\r\n])\r\n\r\n# wrap each channel in Feature then wrap all in FeatureList\r\nmain_input_feat_list = tf.train.FeatureList(\r\n    feature=[\r\n        tf.train.Feature(float_list=tf.train.FloatList(value=value)) \r\n        for value in main_input\r\n    ]\r\n)\r\n\r\n# wrap in SequenceExample\r\nexample = tf.train.SequenceExample(\r\n    context={\r\n        # other stuff could go here\r\n    }, \r\n    feature_lists=tf.train.FeatureLists(feature_list={\r\n        'main_input': main_input_feat_list,\r\n        # other stuff could go here\r\n    })\r\n)\r\n\r\n\r\n# write to record\r\nwith tf.python_io.TFRecordWriter('demo_example.tfrecord') as writer:\r\n    writer.write(example.SerializeToString())\r\n```\r\n\r\nfrom here, we now need to unwrap this from TF `Record` in our `Estimator`'s `input_fn` using a _different_ api:\r\n```\r\n\r\ndef parse_record(record):\r\n    return tf.parse_single_sequence_example(\r\n        record, \r\n        context_features={\r\n            # get context features here\r\n        }, \r\n        sequence_features={\r\n            'main_input': tf.FixedLenSequenceFeature((3, ), tf.float32), # <---- REDUNDANT\r\n            # get other sequence features here\r\n        }\r\n    )\r\n\r\n\r\ndef input_fn(files:list=['demo_example.tfrecord'], params:dict):\r\n    dataset = tf.data.TFRecordDataset(files).map(lambda record: parse_record(record))\r\n    # reformat dataset to return feature, label pairs\r\n    return dataset\r\n\r\n\r\n```\r\n\r\nNow if we want to use `tf.estimator.BestExporter` we need a `serving_input_receiver_fn` so lets go ahead and get that written:\r\n\r\n\r\n```\r\ndef serving_input_receiver_fn():\r\n    batch_size = None\r\n    main_input = tf.placeholder(tf.float32, (batch_size, 2, 3), name='main_input') # <---- REDUNDANT\r\n\r\n   \r\n    features = {'main_input': main_input}\r\n    \r\n    # unclear what the difference is between features and receiver tensors\r\n    receiver_tensors = features\r\n    return  tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\r\n\r\n\r\nexporter = tf.estimator.BestExporter(\r\n    name='best_exporter', \r\n    serving_input_receiver_fn= serving_input_receiver_fn,\r\n    exports_to_keep=3\r\n)\r\n```\r\n\r\nSo while the input pipeline should be decoupled from the model, the model's first layer at the very least is defined by the input. Therefore I should not have to three different times, in three different ways tell TensorFlow what my input features are.\r\n\r\n```\r\n\r\n        # 1st to write to TF Records \r\n        feature_lists=tf.train.FeatureLists(feature_list={\r\n            'main_input':  tf.train.FeatureList(\r\n                feature=[\r\n                    tf.train.Feature(float_list=tf.train.FloatList(value=value)) \r\n                    for value in main_input\r\n                ]\r\n            ),\r\n            # other stuff could go here\r\n        })\r\n\r\n\r\n        # then to get it out of TF Records with an asymmetric api\r\n        #...\r\n        'main_input': tf.FixedLenSequenceFeature((3, ), tf.float32)\r\n        #...\r\n\r\n        # and once more I have to define placeholders for the features in the serving_input_receiver_fn\r\n        main_input = tf.placeholder(tf.float32, (batch_size, 2, 3), name='main_input') # <---- REDUNDANT\r\n        \r\n\r\n```\r\n\r\n# Proposal to solve\r\n\r\nIn this [colab](https://colab.research.google.com/drive/1HrSYF1I7rBGaNQ7388Ss3epWPLTloEC6) I demonstrate the fledgling idea of how to solve this, named [\"FIO\" (Feature Input / Output)](https://pypi.org/project/fio/) whereby instead of having to redefine everything many times over with different apis depending on the case, you can define everything once in a data \"schema\" e.g.\r\n\r\n\r\n```\r\n# features here are of an actual example (not batched)\r\nfeatures = {\r\n    'my-feature': 'hi',\r\n    'seq': np.array([\r\n        # ch1, ch2, ch3\r\n        [   1,   1,  1], # element 1\r\n        [   2,   2,  2], # element 2\r\n        [   3,   3,  3], # element 3\r\n        [   4,   5,  6]  # element 4\r\n    ])\r\n}\r\n\r\n\r\n# here we specify what is needed to encode and decode from `(Sequence)Example` and `TF Records`\r\n\r\nSCHEMA = {\r\n    'my-feature': {'length': 'fixed', 'dtype': tf.string,  'shape': []},\r\n    'seq': {\r\n        'length': 'fixed',\r\n        'dtype': tf.int64,\r\n        'shape': [4, 3],\r\n        'encode': 'channels',\r\n        'channel_names': ['A', 'B', 'C'],\r\n        'data_format': 'channels_last'\r\n    }\r\n}\r\n```\r\n\r\nthen\r\n```\r\n\r\n# define our converter\r\nfio = FIO(SCHEMA, etype='example')\r\n\r\n# write to example\r\nfile = 'example.tfrecord'\r\n\r\nexample = fio.to_example(features)\r\nwith tf.python_io.TFRecordWriter(file) as writer:\r\n    writer.write(example.SerializeToString())\r\n\r\n# use a sequence example instead\r\nfio = FIO(SCHEMA, etype='sequence_example', sequence_features=['seq'])\r\nfile = 'sequence_example.tfrecord'\r\nwith tf.python_io.TFRecordWriter(file) as writer:\r\n    writer.write(example.SerializeToString())\r\n\r\n```\r\n\r\nwhere in both cases we can read from records with\r\n\r\n```\r\ntf.data.TFRecordDataset(DATASET_FILENAMES).map(lambda r: fio.from_record(r))\r\n```\r\n\r\nThus I propose that TensorFlow add's a new component to the estimator class, which would be the bride between the input pipelines / serving input functions and the `model_fn`. This would be something like the data schema. \r\n\r\nA user can define a data schema instance once and import it into the input pipeline to write their inputs to TF Record, import the same schema into their `Estimator`'s `input_fn` to read from TF `Record`s (or change the TF Record file format slightly so that you do not have define how it should be read, like how I do not have to define the keys in a json file to read a json file) and then once more it can be passed to (or automatically used by the `Estimator`) for the `serving_input_receiver_fn`.  \r\n\r\nI believe letting users declaratively, upfront, define what the `Estimator` accepts in terms of data and then not forcing them to, at least trice, re-write what the data is, would be a great simplifier and clean up the `Estimator` api\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nTechnically, everything \"under the hood\" could remain the same. The changes would be made to `tf.data` to introduce a `Schema` class which would work as a unifier across the multiple ways of encoding / decoding `tf`/`np` tensors to / from TF Records. In addition, the `Estimator` class would be updated to accept a `tf.data.Schema` instance, from which it would automate the redundancy described above.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who uses the `Estimator` api, anyone who uses `TF Records`, anyone who acknowledges that TensorFlow's documentation is far from complete and examples are needed to bridge how the \"high\" level apis quite often interface directly with lower (or even the lowest) level apis (e.g. `Estimators` which deal with `Records` or users who want to have an exporter which requires a `ServingInputReceiver`)\r\n\r\n**Any Other info.**\r\nPlease look at [FIO](https://pypi.org/project/fio/) and generalize this to work as described above.\r\n\r\n\r\nSee these S.O. Posts and their linked Colab's to follow my journey in trying to decipher what should be a high level api, but it turns out to be fairly tethered to lower features.\r\n\r\n\r\n[optimal way to store tensors at TF Records](https://stackoverflow.com/questions/52035692/tensorflow-v1-10-store-images-as-byte-strings-or-per-channel)\r\n\r\n[recovering TF Records](https://stackoverflow.com/questions/52064866/tensorflow-1-10-tfrecorddataset-recovering-tfrecords)\r\n\r\n[early stopping](https://stackoverflow.com/questions/52641737/tensorflow-1-10-custom-estimator-early-stopping-with-train-and-evaluate)\r\n\r\n[what are serving_input_receiver_fn](https://stackoverflow.com/questions/52874647/tensorflow-v1-10-why-is-an-input-serving-receiver-function-needed-when-checkpoi)\r\n\r\n[using an estimator after training](https://stackoverflow.com/questions/53307954/tensorflow-custom-estimator-predict-throwing-value-error)\r\n\r\n[defining estimator spec throws error](https://stackoverflow.com/questions/53317235/tensorflow-custom-estimators-defining-estimator-spec-triggers-error)", "comments": ["@wt-huang thoughts?", "@karmel thoughts?", "In TensorFlow 2.0, we have consolidated and simplified saving/serving logic. Please see https://github.com/tensorflow/community/blob/master/rfcs/20190509-keras-saved-model.md , and specifically [the section on signatures](https://github.com/tensorflow/community/blob/master/rfcs/20190509-keras-saved-model.md#savedmodel-signatures).", "@karmel I do not see how that is relevant to the redundancy of converting data to Examples, writing to Records, reading from Records, and parsing Records discussed in this issue. ", "It seems that one of the primary endpoints is the ServingInputReceiver, which is being de-emphasized. More broadly, in 2.0, many of these usability issues are resolved using the Keras framework, so we are reticent to change the existing behavior of the Estimator framework. For your purposes, I would recommend hosting the suggested class in your lib, with some added functionality to unwrap and rewrap for passing through estimator.", "@karmel I understand that the emphasis is to move to TF 2.0..\r\n\r\nHowever, I hope you can also acknowledge why that may not be a sufficient answer.\r\nTo my knowledge (and I may very well be wrong), TF 2.0 is still in beta. The key benefits of estimators (e.g. serving and scalable performance), are not fully mirrored in this major release. Further, this is about standardizing the interface of moving data into and out of tf records, which the keras api does not solve.  \r\n\r\nSome high level experimental classes like CsvDataset will help in some cases, but there will still be a need for many to work at the example / record level and fixing the inconsistency or at least bolstering the documentation would be in everyone's best interest. \r\n\r\nConsider this StackOverflow question:  https://stackoverflow.com/questions/52035692/tensorflow-v1-10-store-images-as-byte-strings-or-per-channel\r\n\r\nThere is a multitude of ways of encoding the same tensor into an (Sequence)Example and no where is it stated which way is most optimal for performance, storage, or recovery. Further still, SequenceExample seems to be a superset of the Example functionality (give \"context_feature\" and \"sequence_features\"). So it could even be argued to drop Examples all together.\r\n\r\nUnless you (TF as an organization) plans on closing of TF Records from the user, then the major release of 2.0 will not solve this issue.\r\n\r\nI think, perhaps, this issue could be renamed as \"Reduced redundancy in TF (Sequence)Example, TF Records, and parsing thereof\".  I initially stated it as relating to the Estimator API as that was the context in which I was using tf records.\r\n\r\n\r\n\r\n\r\n\r\n", "@SumNeuron,\r\nSorry for the delayed response. In the **`Tensorflow Version 2.x`**, since we use [TF Keras](https://www.tensorflow.org/api_docs/python/tf/keras) predominantly and don't use [Estimators](https://www.tensorflow.org/guide/estimator) much, can you please let us know if this Feature is still relevant? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23801, "title": "How to train my dataset(audio samples) in tensorflow ( simple audio recognition).", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No , i am using simple audio recognition example \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): https://www.tensorflow.org\r\n- TensorFlow version (use command below):r1.12\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):N.A\r\n- GCC/Compiler version (if compiling from source):N.A\r\n- CUDA/cuDNN version:N.A\r\n- GPU model and memory:N.A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen i want to train my own dataset (without url). i already commented --data_url  but i got positional argument error.\r\n\r\n\r\n**Code to reproduce the issue**\r\n'''\r\n(base) H:\\tensorflow-r1.10>python tensorflow/examples/speech_commands/train.py --data_dir=tensorflow/examples/speech_commands/dataset\r\nH:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of\r\nthe second argument of issubdtype from `float` to `np.floating` is deprecated. I\r\nn future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/speech_commands/train.py\", line 463, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"H:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 1\r\n25, in run\r\n    _sys.exit(main(argv))\r\n  File \"tensorflow/examples/speech_commands/train.py\", line 107, in main\r\n    FLAGS.testing_percentage, model_settings, FLAGS.summaries_dir)\r\nTypeError: __init__() missing 1 required positional argument: 'summaries_dir'\r\n'''\r\nWhere i was wrong and what i needs to do?", "comments": ["The [Simple Audio Recognition](https://www.tensorflow.org/tutorials/sequences/audio_recognition) example  on TensorFlow website works as expected. This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23800, "title": "TFLite:  numeric precision issue in Conv int8 implementation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, but dump the runtime data\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Null\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.11.0 and 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): 5.5.0\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nWhen running mobiletnet int8 model, starting from the third Conv(`MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6`), TFLite's output mismatches with another framework.\r\n\r\nI looked into it and found that [GetQuantizedConvolutionMultipler()](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/lite/kernels/kernel_util.cc#L25) generates unexpected results with the calculation `multiplier = input_scale * weight_scale / output_scale`. By *unexpected* I mean the math result is wrong, on my machine at least.\r\n\r\nLog example of parameter of `MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6` is as below. Since `input_scale` and `output_scale` are the same, the `real_mulpliter` shall be same as `filter_scale` - more even, these parameters shall be the same for most Conv ops in quantized MobileNetV1.\r\n```conv pre computed args --------------------------\r\n            input scale : 0.023528477177023888\r\n            filter scale : 0.015148180536925793\r\n            output scale : 0.023528477177023888\r\n            real_multiplier: 0.015148180864416702\r\n            output_multiplier: 2081950125\r\n```\r\n\r\n**Describe the expected behavior**\r\nWell, the expected behavior is that generates correct result.\r\n\r\n**Code to reproduce the issue**\r\nTo generates the log in r1.11, try https://github.com/jackwish/tensorflow/tree/d/lite/qconv-r1.11 . Note that, I have already used **[another code style](https://github.com/jackwish/tensorflow/commit/f3677c512ebe06a2dd0dda129ba9ef71de639d87#diff-ca0f46c80fd3cf1f2040be6147a2d8cfR47) such that the output multiplier is correct**.\r\nYou can also try [r1.12](https://github.com/jackwish/tensorflow/tree/d/lite/qconv-r1.12) (there is bug in dumping conv outputs here, but the `Prepare()` still has multiplier log).\r\n\r\n**Other info / logs**\r\nI was running quantized MobileNetV1  with TFLite on a x86 server. By chance, i noticed that some output of Conv int8 is unexpected. TFLite was invoked through python interfaces, the model comes from [TF model zoo](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md) (the TFLite model inside [MobileNet_v1_1.0_224_quant](http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224_quant.tgz)). Nothing else special.\r\n\r\nNote sure if there is similar behavior on other machine/devices.", "comments": ["I have looked into this a bit further. Far beyond expected, the *original* multiplier calculation code style and mine generate in/correct result for different Conv ops - listed as bellow.\r\n* the indexing of conv starts from 1. Depthwise Conv is ignored (no verification of their result)\r\n* *correct* means same multiplier value as a test( or use gdb cli).\r\nNot sure whether the classification accuracy of the network is impacted as I am focus on arithmetic result only.\r\n\r\n| which conv | original impl. correct | new impl. correct |\r\n| -------- | -------- | -------- |\r\n| 1     |    yes     |    no     |\r\n| 2     |     no     |    no     |\r\n| 3     |     no     |    yes    |\r\n| 4     |     no     |    yes    |\r\n| 5    |       no     |    no    |\r\n| 6     |     no     |    yes    |\r\n| 7     |     no     |    yes    |\r\n| 8     |     no     |    no     |\r\n| 9     |     no     |    yes    |\r\n| 10    |     no     |    yes    |\r\n| 11    |     no     |    yes    |\r\n| 12   |       no     |    no    |\r\n| 13    |     no     |    yes    |\r\n| 14    |     no     |    yes    |\r\n| 15    |     no     |    no     |\r\n\r\nFYI: debug code for [r1.12](https://github.com/jackwish/tensorflow/tree/d/lite/qconv-r1.12) has updated", "I think i know what's going on for this issue. Will verify and provide a fix.", "PR #24036 addresses Conv's issue. We will look into other precision issue later.", "Thanks for the detailed debugging! Apologies for the delay, can you describe what you mean by unexpected? What was the result you were getting and what was expected and why? Specifically, what are you comparing against when you say incorrect/", "@suharshs \r\n> Thanks for the detailed debugging! Apologies for the delay, can you describe what you mean by unexpected? What was the result you were getting and what was expected and why? Specifically, what are you comparing against when you say incorrect/\r\n\r\n**expected and unexpected** has been addressed initially in this issue.\r\n\r\n**root cause and code**: Check [this code segment](https://github.com/tensorflow/tensorflow/pull/24036/files#diff-ca0f46c80fd3cf1f2040be6147a2d8cfL31) of PR #24036 and you will get it (as we are C++ programmers...).\r\n\r\nThe code i was looking into is `double product = a * b`, of which `a` and `b` are `float` - the multiplication is in single precision, and casted to double precision.\r\n\r\nI assumed that we were expecting a *multiplication in double precision* - as the quantized multiplier is essential.  I was thinking `double product = static_cast<double>(a) * static_cast<double>(b)` is what we want.\r\n\r\n**others**\r\nWe are working on an internal project which takes TFLite as reference. Sorry that we cannot tell more according to the NDA. Still, the arithmetic issue can be easily reproduced in a tiny  C++ code. And, i have provided test in that PR.\r\n\r\nThe example of input/output of quantized multiplier function has been given in the **Describe the current behavior** section above and should have been well-described.\r\n\r\nWell, since this issue was raised about a month ago, and we have fixed it (pr #24036 ) and turned to some other direction,  we have lost some more-detailed data. Sorry :(\r\n\r\nThank you.\r\n", "Thanks! I guess I am curious how much of a different in accuracy this was generating for you in your use case if you can share :) Thanks for finding this! Will approve the PR.", "> Thanks! I guess I am curious how much of a different in accuracy this was generating for you in your use case if you can share :) Thanks for finding this! Will approve the PR.\r\n\r\nYou mean the end2end network accuracy? Well, we have not come to that yet. We are basically focus on arithmetic precision currently (though I think our team needs to put more resource on end2end verification... but that's out of my control...).\r\n\r\nOne interesting part of ML I think is, a software system with better **arithmetic precision** doesn't necessarily mean a better **predication accuracy**. I think you know what i mean... haha :)", "Hi there, this numeric precision issue had been fixed by #25302 before. However I notice the fix was reverted in https://github.com/tensorflow/tensorflow/commit/55211a958cc5bb1cf0739c9582f97e0783bf37b4 . May I know why to revert it as there is obvious numeric precision here? If it is because that the numeric eventually impact end2end predicate accuracy, or it is not consistent with TF's implementation (which may also lead to accuracy problem)?", "Hi @jackwish , I am so sorry I actually wasn't informed of this getting rolled back internally :(\r\n\r\nI dug around and it appears this changed the output values of many model tests, (unclear if it actually impacted accuracy negatively). But since FakeQuant in TF uses float, this change may have caused a meaningful difference, and I think we need more rigorous end2end model accuracy analysis before we move forward. And if we do move forward I think we should resolve this by changing the TFLiteQuantizationParams value to double once and for all rather than performing these localized promotions. \r\n\r\nIn particular if we could get the accuracy impact these makes on the released quantized mobilenet v1's that could be a useful data point.\r\n\r\nWhat do you think? \r\n\r\nAnd again apologies for this getting silently rolled back!", "Thank you @suharshs for your reply. I am totally agree with your consideration about the end2end accuracy, which I have not verified yet. And, I think your proposal of *float/double alignment* in TFLite and TF is absolutely the decent solution. All we need is some data. I can work on this, but maybe not that fast - we are lack of some infrastructure :(\r\n\r\nBtw, I have a question about the quantization-aware training which I fail to find the answer when looking into detail. As we know, the `contrib.quanize` rewrites the graph by inserting `FakeQuant` ops to where it is needed, and the output of `FakeQuant` is float which has been narrowed to $$[min, max]$$ from float input. This `FakeQuant` float output flows into typical Conv2D (for example) op which is computing in float. So, is the *simulated quantization* forwarding is actually performed in float which has been narrowed to a range that can be represented in INT8?\r\n\r\nThanks.", "Yes, the simulated convolution is performed in float with each input and output being effectively \"quantized and dequantized\".\r\n\r\nFakeQuant(x) == Quantize(Dequantize(x))", "> Yes, the simulated convolution is performed in float with each input and output being effectively \"quantized and dequantized\".\r\n> \r\n> FakeQuant(x) == Quantize(Dequantize(x))\r\n\r\nThank you for the knowledge!"]}, {"number": 23799, "title": "TFLite can not set input shape and input_stats with 1.0/128", "body": "```\r\nconverter = tf.contrib.lite.TFLiteConverter.from_saved_model(\r\n  saved_model_dir=sys.argv[1],\r\n  input_shapes={'input' :[5, 3000, 40, 3]}\r\n  )\r\nconverter.dump_graphviz_dir='./lite_dump'\r\nconverter.inference_type=tf.contrib.lite.constants.QUANTIZED_UINT8\r\nconverter.quantized_input_stats={'inputs': (127, 1.0/128)}\r\nconverter.default_ranges_stats=(0, 6)\r\ntflite_model = converter.convert()\r\n```\r\n```\r\ninputs\r\nType: Uint8 [1\u00d73000\u00d740\u00d73] \r\nMinMax: [-16256, 16384]\r\nQuantization:\r\n128 * (x - 127)\r\n```\r\n\r\nThe batch_size is 1 , not 5.\r\nthe input var is 128 not 1.0/128\r\n\r\n```\r\ninterpreter.resize_tensor_input(input_details[0]['index'], [2, 3000, 40, 3])\r\ninterpreter.allocate_tensors()\r\ninterpreter.invoke()\r\n```\r\n```\r\nInputs [{'name': 'inputs', 'index': 17, 'shape': array([   1, 3000,   40,    3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\nOutputs [{'name': 'softmax', 'index': 24, 'shape': array([1, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\nInputs [{'name': 'inputs', 'index': 17, 'shape': array([   2, 3000,   40,    3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\nTraceback (most recent call last):\r\n  File \"tflite_run.py\", line 15, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/nfs/project/tools/anaconda3/envs/tf1.12_py3.5/lib/python3.6/site-packages/tensorflow/contrib/lite/python/interpreter.py\", line 71, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/nfs/project/tools/anaconda3/envs/tf1.12_py3.5/lib/python3.6/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/contrib/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (1500 != 750)Node number 14 (RESHAPE) failed to prepare.\r\n```", "comments": ["Hi @zh794390558  would you be able to try again with a super recent version? I change was submitted yesterday to help check for common shape mistakes. Is your input tensor called 'input' or 'inputs'?", "@andrehentz Input tensor called 'inputs', and I will try.", "Note that in your first comment you had:\r\n\r\ninput_shapes={'**input**' :[5, 3000, 40, 3]}\r\n\r\nmaybe change that to **inputs**?", "@andrehentz That's my fault, thx. \r\n\r\n```\r\nInputs [{'name': 'inputs', 'index': 17, 'shape': array([   5, 2000,   40,    3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\nOutputs [{'name': 'softmax_output', 'index': 22, 'shape': array([5, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\nInputs [{'name': 'inputs', 'index': 17, 'shape': array([   2, 2000,   40,    3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\nTraceback (most recent call last):\r\n  File \"tflite_run.py\", line 15, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/nfs/project/tools/anaconda3/envs/tf1.12_py3.5/lib/python3.6/site-packages/tensorflow/contrib/lite/python/interpreter.py\", line 71, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/nfs/project/tools/anaconda3/envs/tf1.12_py3.5/lib/python3.6/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/contrib/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (786000 != 1965000)Node number 7 (RESHAPE) failed to prepare.\r\n```", "@andrehentz What's the meaning of MinMax and how to set Quantization to `1.0/128(x-127.0)`\r\n\r\n```\r\ninputs\r\nType: Uint8\r\n[5\u00d72000\u00d740\u00d73]\r\nMinMax: [-16256, 16384]\r\nQuantization:\r\n128 * (x - 127)\r\n```", "They are the estimated mininum and maximum value of that tensor, usually during training. Quantization tells you how to convert from float to uint8. Note however, that you converted the model using:\r\n  converter.quantized_input_stats={'inputs': (127, 1.0/128)}\r\n  converter.default_ranges_stats=(0, 6)\r\n\r\nUsing default ranges is unlikely to produce an accurate model.", "What does `converter.default_ranges_stats` mean?"]}, {"number": 23798, "title": "Docs Needed", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.10+\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/estimator/export/ServingInputReceiver\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nIt is unclear:\r\n\r\n1. what a `serving_input_receiver_fn` is\r\n2. why one is needed\r\n3. how one should be written (e.g. what arguments are required in the function definition)\r\n4. what should be returned \r\n5. what the difference is between the `features` argument of `tf.estimator.export.ServingInputReceiver` and the `receiver_tensors` argument is and why they are needed.\r\n\r\n\r\nSee this [S.O. post](https://stackoverflow.com/questions/52874647/tensorflow-v1-10-why-is-an-input-serving-receiver-function-needed-when-checkpoi) for an example of why it is unclear\r\n\r\n", "comments": ["Nagging Assignees @karmel, @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I wanted to start contributing here, this seems like a good place to start, so I can be assigned this?", "Thanks, @shivam13juna -- sure, please update the docs for the relevant classes/methods above as necessary, and it would be great if you also came up with an end-to-end example that could be published as a tutorial/Jupyter notebook. Thanks--", "Is this issue not assigned. Can i work on this!?", "Yeah go ahead pal, I'm not working on this. ", "Ok thanks @shivam13juna .. can i get assigned to this @karmel ", "By the way i was looking in the docs and this particular  `tf.estimator.export.ServingInputReceiver`\r\nis deprecated in tensorflow v2.0 do we need to work on this issue any more or this issue still open?", "\ud83d\ude01 Thanks for your interest in helping improve the docs! And you're right -  `tf.estimator.export.ServingInputReceiver` will be deprecated in TF 2.0. Closing out this issue.", "ServingInputReceivers will still be in 2.0; there will be other options for exporting (see https://github.com/tensorflow/community/pull/34 ), but ServingInputReceivers will still be necessary for Estimators. If you would like to help with documentation/examples, that would be much appreciated.", "If the issue is open, Can I work on this?\ud83d\ude80", "@adithyaakrishna nobody's currently working on this, If you want to contribute to these docs you're welcome to. They're generated from the docstrings in the [tensorflow/estimator repository](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/export/export.py#L109).\r\n\r\n", "@SumNeuron Can you please assign this to me, if this issue is still open ?", "Hi @siddharth25pandey,\r\n\r\nWhile it's true that this thing is under-documented,  `ServingInputReceiver` is now only available in the legacy `tf.compat.v1` package. So any effort may be better-spent elsewhere.\r\n\r\nIf it's important to you, you can always send us a documentation update (but note that we do have a \"too small to accept\" threshold, we don't accept single-character PRs.)."]}, {"number": 23797, "title": "tf lite armeabi version is much slower than the nightly aar ", "body": "      At first I was using  'org.tensorflow:tensorflow-lite:0.0.0-nightly'  aar in my own project. The tf lite model costs 900ms.\r\n      But in my company project, the ndk abiFilters is armeabi.  I changed abiFilters from armeabi-v7a to armeabi. However,  I found the aar not support armeabi.\r\n![image](https://user-images.githubusercontent.com/10142633/48612953-5f35f180-e9c5-11e8-9fde-9faddba7884e.png)\r\n      I compile the armeabi version with the following command, and import so and jar file, but my model costs 4000ms. Is there some wrong with the command ??\r\nbazel build --cxxopt='--std=c++11' //tensorflow/lite/java:tensorflowlite \\\r\n--crosstool_top=//external:android/crosstool \\\r\n--host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n--cpu=armeabi\r\n     ", "comments": ["@shashishekhar , could you ptal?", "@zjmirving : You should use -c opt for optimized code.", "> @zjmirving : You should use -c opt for optimized code.\r\n\r\nI found an answer in stackoverflow. Is this the reason why the armeabi tf lite  is slow ?\r\nhttps://stackoverflow.com/questions/7080525/why-use-armeabi-v7a-code-over-armeabi-code", "@zjmirving : From the original question I gathered that you were using armeabi with Java wrapper and still getting better performance than what you built yourself, in that case right build flags are important.\r\narmeabi-v7a will be better than armeabi", "@zjmirving Hi. I'm trying to build tensorflowlite with option --cpu=armeabi, but always fail due to \"clang: error: clang frontend command failed with exit code 70\". Could you tell me the version of your android sdk and ndk?", "@shashishekhar After the original question, i found the 'org.tensorflow:tensorflow-lite:0.0.0-nightly' arr only contains armeabi-v7a.  Obviously,  it's faster than armeabi", "@FrisaSZ ndk is 14, sdk is 26.  I think there is no need to build tensorflowlite with option --cpu=armeabi. Now almost all of the mobiles support armeabi-v7a, you can just assess it.\r\n       String[] abis = new String[]{};\r\n        if(Build.VERSION.SDK_INT>=Build.VERSION_CODES.LOLLIPOP)\r\n        {\r\n            abis = Build.SUPPORTED_ABIS;\r\n        } else {\r\n            abis = new String[]{Build.CPU_ABI,Build.CPU_ABI2};\r\n        }\r\n        for(String abi:abis)\r\n        {\r\n            if(abi.equals(\"XXX\")){\r\n             }\r\n        }\r\n"]}, {"number": 23796, "title": "Improve using of graph transform tool ", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12.0\r\n- Doc Link: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms\r\n\r\n\r\n**Describe the documentation issue**\r\nAs newbie to Tensorflow it quite confusing seeing that you need to `bazel build` something, all I had to do before this point is `pip install`. To be more specific, what is bazel? Why do I need it to use GTT? Why is GTT not available from pip installation? Why when I'm going to _Build from source page_ it encourages me that you providing pre-built packages, where is GTT then?\r\n", "comments": ["Hi @andrhua, we tend to make binaries available for things with stable api's, that we want to encourage the use of. There's no binary for this and it's never mentioned on tensorflow.org, which likely puts it in the category of internal details that regular users shouldn't rely-on.\r\n\r\nHey @suharshs, you're more familiar with the graph-transformation landscape. Do you know what the future holds for this tool? ", "Hi, super sorry for the delay. \r\nGTT is a legacy tool and its not being directly supported anymore. Grappler should be used instead. That is the reason its not exposed via pip.\r\n\r\nThanks!"]}, {"number": 23795, "title": "Windows 10 Bazel Build failed ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.10\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): not compiled from source(bazel version: 0.18.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0/7.0\r\n- GPU model and memory: Zotac Gtx 1080ti mini\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuild fails with 1 error\r\ninitially i tried with cmake, it was giving few errors so started with bazel..no luck since many days! \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ni followed all the steps mentioned [here](https://www.tensorflow.org/install/source) \r\n\r\n`bazel build --copt=-nvcc_options=disable-warnings --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nThe reason i'm trying to build from source is because ssd_mobilnet retraining is taking longer than expected (3seconds per step)\r\nresizing of image is happening on cpu and training on gpu.\r\nif both happens on gpu then it should take around half second.\r\n\r\ni have attached full build output \r\n[Build output.txt](https://github.com/tensorflow/tensorflow/files/2588467/Build.output.txt)\r\n\r\ni've notived too many `warning LNK4044` in build process\r\n\r\nbelow is the gist of build \r\n\r\n> build --copt=-nvcc_options=disable-warnings --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n> WARNING: Processed legacy workspace file e:\\git_projects\\tensorflow-experimental\\tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\r\n> WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\n> .\r\n> .\r\n> .\r\n> .\r\n> \r\n>    Creating library bazel-out/host/bin/tensorflow/cc/ops/image_ops_gen_cc.lib and object bazel-out/host/bin/tensorflow/cc/ops/image_ops_gen_cc.exp\r\n> ERROR: E:/git_projects/tensorflow-experimental/tensorflow/tensorflow/core/kernels/BUILD:2123:1: C++ compilation of rule '//tensorflow/core/kernels:colorspace_op_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n>   cd C:/users/neuroflares/_bazel_neuroflares/twu4fovw/execroot/org_tensorflow\r\n> .\r\n> .\r\n> .\r\n> .\r\n> \r\n> 1 error detected in the compilation of \"C:/Users/user~1/AppData/Local/Temp/nvcc_inter_files_tmp_dir/colorspace_op_gpu.cu.cpp1.ii\".\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> INFO: Elapsed time: 1530.120s, Critical Path: 558.89s\r\n> INFO: 878 processes: 878 local.\r\n> FAILED: Build did NOT complete successfully\r\n\r\n\r\nWhat could have gone wrong?\r\nThanks & Regards\r\n", "comments": ["Try bazel 15.0.0 as it [stated](https://www.tensorflow.org/install/source_windows#gpu) to be tested build configuration or cmake. Maybe `bazel clean` before any bazel builds will help. Or, you should clone from r1.12 branch, not master", "@andrhua, Thanks for your reply.\r\nunfortunately build failed with bazel version 0.15.0 as well!   \r\ninfact it failed much early. below is the build output\r\n\r\n\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\n...........................................\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nDEBUG: C:/users/user/_bazel_user/twu4fovw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/user/_bazel_user/twu4fovw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/user/_bazel_user/twu4fovw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nDEBUG: C:/users/user/_bazel_user/twu4fovw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/user/_bazel_user/twu4fovw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/user/_bazel_user/twu4fovw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nDEBUG: C:/users/user/_bazel_user/twu4fovw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/user/_bazel_user/twu4fovw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/user/_bazel_user/twu4fovw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nERROR: E:/git_projects/tensorflow-experimental/tensorflow/tensorflow/tools/pip_package/BUILD:123:1: no such package '@libxsmm_archive//': C:/users/user/_bazel_user/twu4fovw/external/libxsmm_archive/documentation (Directory not empty) and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@libxsmm_archive//': C:/users/user/_bazel_user/twu4fovw/external/libxsmm_archive/documentation (Directory not empty)\r\nINFO: Elapsed time: 784.562s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (74 packages loaded)\r\n    currently loading: @protobuf_archive// ... (2 packages) \r\n```\r\n\r\ni tried msys as well.\r\nit failed with same message as command prompt.", "Yeah I'm having tough times to build it for windows too. Considering all gathered information from these several days I think the only way to succeed is to start revert commits and build, as you can see [here](https://github.com/tensorflow/tensorflow#continuous-build-status) currently it successfully builds only for Linux and Android", "Finally Build succeeded with tensorflow r1.11  (i tried r1.5, 1.9    but they failed)\r\nthis time r1.11 build succeeded after 4.5 hours of compilation time! \r\nbefore building, when i ran `py config.py` i had turned off `ngraph `support and didn't build with intel `mkl `library\r\n\r\nit showed too many warnings while building.. after building the library it showed error when i execute below command \r\n`bazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/tmp/tensorflow_pkg`\r\n\r\nfirst i deleted `simple_console_for_windows.zip`  which was 0 bytes then followed steps mentioned in answer [here ](https://stackoverflow.com/questions/52394305/creating-pip-package-for-tensorflow-with-gpu-support-results-in-0-byte-simple-co/52398168)   [error i was getting is same as asked in question in that link]\r\nfinally i got tensorflow .whl file.\r\n\r\ni removed the tensorflow which was installed via `pip install tensorflow-gpu `and installed the one that i built.\r\n\r\nnow again the main problem remains same as asked [here](https://github.com/tensorflow/models/issues/1942)\r\n\r\n\r\nincluding intel `mkl ` or  `ngraph `make image resizing happen on gpu while training?  so that load on cpu will be reduced..  \r\ni remember some error by ngraph while building, so removed it while building..\r\nhave been building tensorflow with different versions to make entire training happen on GPU.\r\n", "[someone ](https://stackoverflow.com/questions/52394305/creating-pip-package-for-tensorflow-with-gpu-support-results-in-0-byte-simple-co/52398168?noredirect=1#comment93617530_52398168 )succeeded in building latest release on windows.\r\nso i gave a try with r1.12, started with below command \r\n\r\n`bazel build --define=no_tensorflow_py_deps=true --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nyet build failed with below message\r\n\r\n\r\n\r\n```\r\nERROR: E:/git_projects/tensorflow/tensorflow/compiler/tf2xla/BUILD:97:1: C++ compilation of rule '//tensorflow/compiler/tf2xla:cpu_function_runtime' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/user/_bazel_user/q6fatijb/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.16299.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.16299.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/user/AppData/Local/Programs/Python/Python35/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/user/AppData/Local/Programs/Python/Python35/lib/site-packages\r\n    SET TEMP=C:\\Users\\user~1\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n    SET TF_CUDA_VERSION=9.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\user~1\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/tf2xla/_objs/cpu_function_runtime/cpu_function_runtime.o /c tensorflow/compiler/tf2xla/cpu_function_runtime.cc\r\n.\\tensorflow/compiler/tf2xla/cpu_function_runtime.h(71): error C2338:\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 4035.211s, Critical Path: 188.99s\r\nINFO: 3218 processes: 3218 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "Release 1.12.0\r\nbazel 0.16.1\r\nturn off winodws defender , bazel clean and stricltly follow these choices during the config \r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\n\r\nGrab a drink and watch! :)", "Build succeeded with TF 1.12 and bzel  0.18.1. But probably i'm missing some support library while building, because when i installed the TF that i built, main problem remains same as asked [here](https://github.com/tensorflow/models/issues/1942)", "I will close this issue since the build/install related issue is solved which is the subject for this issue. Thanks!", "@Prakash19921206  did you build it with XLA JIT or not?\r\n@ymodak if I understand it correctly, then the compilation still does not work when you want to compile with XLA JIT (which, for me, was the main reason to upgrade to 1.12), so I don't the issue is solved. ", "After struggling with a ton of issues.. some takeaways:\r\n- Targeting the 1.14 branch...\r\n- Needed to install VS 2017 (community at least) - I had 2019 installed... wasn't good enough. Used procmon to figure this out.\r\n- I couldn't get things to build with the XLA JIT option enabled - after disabling it, the build is still running!!!\r\n- Had to revert to CUDA 10.0 (rather than 10.1) it seems\r\n\r\nGoogle needs to update their build instructions in a SERIOUS way. They are out of date, and given the number of users with NVIDIA cards on WIndows (pretty common), they should put some emphasis on steps needed to build TF with common Intel CPU features enabled (i7's - 8000 series in my case)\r\n\r\n**Scratch that... build still failed:**\r\n\r\nERROR: C:/data/tensorflow/tensorflow/python/keras/api/BUILD:28:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/username/_bazel_username/fa4varly/execroot/org_tensorflow"]}, {"number": 23794, "title": "RuntimeError : about tf.Gradient() in eager mode", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):  pip installed\r\n- TensorFlow version (use command below):   1.10.0\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: cuda-9.0, cudnn-7.2\r\n- GPU model and memory:   GTX 1080 ti - 11G\r\n\r\n**Describe the current behavior**\r\nI have a huge model with large parameter, before train code, cost GPU memory is 7859MB, and  my local memory unused is 234M free(total is 32G)\r\n![image](https://user-images.githubusercontent.com/10268274/48605840-dca73500-e9b8-11e8-84ac-8a7a8bb617ba.png)\r\n![image](https://user-images.githubusercontent.com/10268274/48605858-ea5cba80-e9b8-11e8-92b9-3bc184829bba.png)\r\n\r\nwhen I bug the code ,occur this warning:\r\n```\r\n2018-11-16 15:52:30.386249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-11-16 15:52:30.571530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-16 15:52:30.571559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0\r\n2018-11-16 15:52:30.571565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N\r\n2018-11-16 15:52:30.571742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9772 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-11-16 15:52:34.410871: W tensorflow/core/framework/allocator.cc:108] Allocation of 423405000 exceeds 10% of system memory.\r\n2018-11-16 15:52:34.602313: W tensorflow/core/framework/allocator.cc:108] Allocation of 372480000 exceeds 10% of system memory.\r\n\r\n```\r\nI will update my discriminator network so I compute gradient first and in eager mode, I used tf.GradientTape() for it.\r\nBut Exception here: I already used tf.GradientTape() for gradients but, it also warning tf.gradients is not supported when eager execution is enabled.  I did not use tf.gradient anywhere!!!!\r\n![image](https://user-images.githubusercontent.com/10268274/48606113-a9b17100-e9b9-11e8-856b-e88a5bbed34e.png)\r\n\r\nand I test demo code like line:1508-1511 in pictures, it worked well. But line:1557 it did not work with same code!!!!  I am sure that I did not use tf.gradients() in my code!!\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should be work \r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n![image](https://user-images.githubusercontent.com/10268274/48606265-30664e00-e9ba-11e8-9688-553d27c0ab45.png)\r\n", "comments": ["I have a fix for this which should be merged soon."]}]