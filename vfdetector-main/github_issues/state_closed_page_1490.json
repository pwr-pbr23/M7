[{"number": 8240, "title": "ImportError: No module named tensorflow", "body": "Hello,\r\nI am using  [TensorFlow For Poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0) guides for retraining  model. I followed first 3 step successfully. While I run following command at tensorflow root directory.\r\n\r\npython tensorflow/examples/image_retraining/retrain.py \\\r\n--bottleneck_dir=/tf_files/bottlenecks \\\r\n--how_many_training_steps 500 \\\r\n--model_dir=/tf_files/inception \\\r\n--output_graph=/tf_files/retrained_graph.pb \\\r\n--output_labels=/tf_files/retrained_labels.txt \\\r\n--image_dir /tf_files/flower_photos\r\n\r\nI get error   **File \"tensorflow/examples/image_retraining/retrain.py\", line 79, in <module>\r\n                  import tensorflow as tf\r\n                  ImportError: No module named tensorflow**\r\n\r\nSystem : Ubantu 14.04 LTS\r\nPython Version 3.4.3\r\n\r\n ", "comments": ["Running from the home directory may cause this issue. It may cause python to search for the package within this directory. (Especially you have compiled from source)\r\n\r\nCould you try cd-ing into the \"tensorflow/examples/...\" folder and try running it from there?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Hey kinDSa, have you figured out where the issue is for this? Thanks."]}, {"number": 8239, "title": "Why still using two gpus even if I have set gpu_device to the second gpu?", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nI want to use the second gpu, and I use the following code to do this:\r\n        with tf.Session() as sess:\r\n            with tf.device(\"/gpu:1\"):\r\n\r\nbut why the nvidis-smi results still shows that tensorflow is using both of the two gpus?\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: Ubuntu14.04 ,Tensorflow 1.0.0rc2\r\n\r\nInstalled version of CUDA and cuDNN:  cuda7.5, cudnn5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["The tensorflow runtime by default enumerates all GPUs and initializes them.  (including pre-allocating a bunch of GPU memory to reduce runtime overheads).\r\n\r\nAll that `with tf.device(\"/gpu:1\")` does is request to place a _particular_ op on a specific GPU.  Other ops without device constraints may be automatically assigned to any device.\r\n\r\nYou can restrict the set of device used by TensorFlow by setting the `CUDA_VISIBLE_DEVICES` environment variable. \r\n\r\nYou can also remap the numbering of GPUs via by passing `ConfigProto.gpu_options.visible_devices_list` argument to `tf.Session()`.\r\n\r\nClosing since this is not a bug/issue, but rather a general usage question (which should really be directed to StackOverflow)"]}, {"number": 8238, "title": "OS/X compile error with tensorflow/compiler/xla/service/allocation_tracker.cc", "body": "There is a length/signed/unsigned mismatch in an inline vector initialization.\r\n\r\n```\r\ntensorflow/compiler/xla/service/allocation_tracker.cc:178:54: error: non-constant-expression cannot be narrowed from type 'std::vector<se::DeviceMemoryBase>::size_type' (aka 'unsigned long') to 'long long' in initializer list [-Wc++11-narrowing]\r\n        ShapeUtil::GetSubshape(allocation->shape(), {i}),\r\n                                                     ^\r\ntensorflow/compiler/xla/service/allocation_tracker.cc:178:54: note: insert an explicit cast to silence this issue\r\n        ShapeUtil::GetSubshape(allocation->shape(), {i}),\r\n                                                     ^\r\n                                                     static_cast<long long>( )\r\n1 error generated.\r\n```\r\n\r\ndiff:\r\n\r\n```\r\n--- a/tensorflow/compiler/xla/service/allocation_tracker.cc\r\n+++ b/tensorflow/compiler/xla/service/allocation_tracker.cc\r\n@@ -175,7 +175,7 @@ StatusOr<std::vector<GlobalDataHandle>> AllocationTracker::DeconstructTuple(\r\n        i < element_bases.size(); ++i) {\r\n     element_handles.push_back(RegisterInternal(\r\n         allocation->backend(), allocation->device_ordinal(), element_bases[i],\r\n-        ShapeUtil::GetSubshape(allocation->shape(), {i}),\r\n+        ShapeUtil::GetSubshape(allocation->shape(), {static_cast<long long>(i)}),\r\n         tensorflow::strings::StrCat(allocation->tag(), \".element_\", i),\r\n         /*initial_ref_count=*/2));\r\n   }\r\n```\r\n", "comments": ["Can confirm.", "@DavidNorman Please feel free to submit a pull request!", "will be fixed by https://github.com/tensorflow/tensorflow/pull/8276\r\n"]}, {"number": 8237, "title": "Fix documentation in dynamic_bidirectional_rnn", "body": "docstring for `dtype` was defined twice.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Signed it.", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 8236, "title": "c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["+1\r\nc:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu"]}, {"number": 8235, "title": "RuntimeError: Graph is finalized and cannot be modified. when I use tf.train.Supervisor", "body": "In [1]: import tensorflow as tf\r\n\r\nIn [2]: g = tf.Variable(0, name=\"g\")\r\n\r\nIn [3]: def pre_load(sess):\r\n   ...:     print(\"load\")\r\n   ...:     sess.run(g.assign(1))\r\n   ...:\r\n\r\nIn [4]: sv = tf.train.Supervisor(logdir=\"/tmp/2/\", init_fn=pre_load)\r\n\r\nIn [5]: with sv.managed_session() as sess:\r\n   ...:     sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\r\n   ...:     print(sess.run(g))\r\n\r\nwhen I run this code in ipython,\r\nI got an execption below\r\nINFO:tensorflow:Error reported to Coordinator: <type 'exceptions.RuntimeError'>, Graph is finalized and cannot be modified.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-5-a3cd6e10e8d0> in <module>()\r\n----> 1 with sv.managed_session() as sess:\r\n      2     sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\r\n      3     print(sess.run(g))\r\n      4\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/contextlib.pyc in __enter__(self)\r\n     15     def __enter__(self):\r\n     16         try:\r\n---> 17             return self.gen.next()\r\n     18         except StopIteration:\r\n     19             raise RuntimeError(\"generator didn't yield\")\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.pyc in managed_session(self, master, config, start_standard_services, close_summary_writer)\r\n    971         # threads which are not checking for `should_stop()`.  They\r\n    972         # will be stopped when we close the session further down.\r\n--> 973         self.stop(close_summary_writer=close_summary_writer)\r\n    974       finally:\r\n    975         # Close the session to finish up all pending calls.  We do not care\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.pyc in stop(self, threads, close_summary_writer)\r\n    799       # reported.\r\n    800       self._coord.join(threads,\r\n--> 801                        stop_grace_period_secs=self._stop_grace_secs)\r\n    802     finally:\r\n    803       # Close the writer last, in case one of the running threads was using it.\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.pyc in join(self, threads, stop_grace_period_secs)\r\n    384       self._registered_threads = set()\r\n    385       if self._exc_info_to_raise:\r\n--> 386         six.reraise(*self._exc_info_to_raise)\r\n    387       elif stragglers:\r\n    388         raise RuntimeError(\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.pyc in managed_session(self, master, config, start_standard_services, close_summary_writer)\r\n    960       sess = self.prepare_or_wait_for_session(\r\n    961           master=master, config=config,\r\n--> 962           start_standard_services=start_standard_services)\r\n    963       yield sess\r\n    964     except Exception as e:\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.pyc in prepare_or_wait_for_session(self, master, config, wait_for_checkpoint, max_wait_secs, start_standard_services)\r\n    717           checkpoint_dir=self._logdir, wait_for_checkpoint=wait_for_checkpoint,\r\n    718           max_wait_secs=max_wait_secs, config=config,\r\n--> 719           init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n    720       self._write_graph()\r\n    721       if start_standard_services:\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/session_manager.pyc in prepare_session(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\r\n    262         sess.run(init_op, feed_dict=init_feed_dict)\r\n    263       if init_fn:\r\n--> 264         init_fn(sess)\r\n    265\r\n    266     local_init_success, msg = self._try_run_local_init_op(sess)\r\n\r\n<ipython-input-3-65858289aa25> in pre_load(sess)\r\n      1 def pre_load(sess):\r\n      2     print(\"load\")\r\n----> 3     sess.run(g.assign(1))\r\n      4\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/ops/variables.pyc in assign(self, value, use_locking)\r\n    549       the assignment has completed.\r\n    550     \"\"\"\r\n--> 551     return state_ops.assign(self._variable, value, use_locking=use_locking)\r\n    552\r\n    553   def assign_add(self, delta, use_locking=False):\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.pyc in assign(ref, value, validate_shape, use_locking, name)\r\n     45   result = _op_def_lib.apply_op(\"Assign\", ref=ref, value=value,\r\n     46                                 validate_shape=validate_shape,\r\n---> 47                                 use_locking=use_locking, name=name)\r\n     48   return result\r\n     49\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\r\n    489                 dtype=dtype,\r\n    490                 as_ref=input_arg.is_ref,\r\n--> 491                 preferred_dtype=default_dtype)\r\n    492           except TypeError as err:\r\n    493             if dtype is None:\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)\r\n    714\r\n    715         if ret is None:\r\n--> 716           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    717\r\n    718         if ret is NotImplemented:\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    174                                          as_ref=False):\r\n    175   _ = as_ref\r\n--> 176   return constant(v, dtype=dtype, name=name)\r\n    177\r\n    178\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in constant(value, dtype, shape, name, verify_shape)\r\n    167   const_tensor = g.create_op(\r\n    168       \"Const\", [], [dtype_value.type],\r\n--> 169       attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\r\n    170   return const_tensor\r\n    171\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n   2352\r\n   2353     \"\"\"\r\n-> 2354     self._check_not_finalized()\r\n   2355     for idx, a in enumerate(inputs):\r\n   2356       if not isinstance(a, Tensor):\r\n\r\n/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _check_not_finalized(self)\r\n   2075     \"\"\"\r\n   2076     if self._finalized:\r\n-> 2077       raise RuntimeError(\"Graph is finalized and cannot be modified.\")\r\n   2078\r\n   2079   def _add_op(self, op):\r\n\r\nRuntimeError: Graph is finalized and cannot be modified.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "It seems the assign operation cannot be put after the sess.run operation. "]}, {"number": 8234, "title": "When I try to run a python program after importing Tensorflow library I get this message", "body": "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n", "comments": ["@prajjwalin You're using the CPU version of TensorFlow, right? If you use the GPU version, you'll have a different output message. You'll have one like this:\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n```\r\n\r\nYou may ignore it, but if you want faster computation, use the GPU, and you'll have an output message like the one I posted above.", "These warnings are harmless, and can safely be ignored. \r\n\r\nThey are there as a gentle reminder to people who are benchmarking TensorFlow that it is possible to generate faster CPU code if various processor specific build options are enabled.  (In generic binary builds are unable to turn on these options.)\r\n\r\nClosing since this is not a bug/issue and really a general usage question. (which are best sent to StackOverflow for future reference!)"]}, {"number": 8233, "title": "Android: java.lang.NullPointerException: Attempt to invoke virtual method 'org.tensorflow.Output org.tensorflow.Operation.output(int)' o", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nStackOverflow: [Why am I getting error: Initializing Tensorflow?](http://stackoverflow.com/questions/42386650/why-am-i-getting-error-initializing-tensorflow)\r\n\r\n### Environment info\r\nOperating System: Mac OS 10.11.6\r\nTensorflow version: v0.12.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI have successfully built the Android demo from command line using bazel. After that, I replaced the inception model with my custom model. I have changed the parameters in `ClassifierActivity` with the following:\r\n```\r\nprivate static final int INPUT_SIZE = 299;\r\n  private static final int IMAGE_MEAN = 128;\r\n  private static final float IMAGE_STD = 128;\r\n  private static final String INPUT_NAME = \"Mul:0\";\r\n  private static final String OUTPUT_NAME = \"final_result:0\";\r\n```\r\nThen I build again with bazel, when I try to run the app, the app crashed. Logs are attached below.\r\n\r\n### What other attempted solutions have you tried?\r\nUsing the same model with memory mapping works fine on iOS.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n> FATAL EXCEPTION: main\r\n>                                                                      Process: org.tensorflow.demo, PID: 23532\r\n>                                                                      java.lang.RuntimeException: Error initializing TensorFlow!\r\n>                                                                          at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:131)\r\n>                                                                          at org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:158)\r\n>                                                                          at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:394)\r\n>                                                                          at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:411)\r\n>                                                                          at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:63)\r\n>                                                                          at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:94)\r\n>                                                                          at android.view.TextureView.getHardwareLayer(TextureView.java:370)\r\n>                                                                          at android.view.View.updateDisplayListIfDirty(View.java:14115)\r\n>                                                                          at android.view.View.getDisplayList(View.java:14161)\r\n>                                                                          at android.view.View.draw(View.java:14928)\r\n>                                                                          at android.view.ViewGroup.drawChild(ViewGroup.java:3405)\r\n>                                                                          at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3199)\r\n>                                                                          at android.view.View.updateDisplayListIfDirty(View.java:14133)\r\n>                                                                          at android.view.View.getDisplayList(View.java:14161)\r\n>                                                                          at android.view.View.draw(View.java:14928)\r\n>                                                                          at android.view.ViewGroup.drawChild(ViewGroup.java:3405)\r\n>                                                                          at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3199)\r\n>                                                                          at android.view.View.draw(View.java:15207)\r\n>                                                                          at android.widget.FrameLayout.draw(FrameLayout.java:592)\r\n>                                                                          at android.view.View.updateDisplayListIfDirty(View.java:14138)\r\n>                                                                          at android.view.View.getDisplayList(View.java:14161)\r\n>                                                                          at android.view.View.draw(View.java:14928)\r\n>                                                                          at android.view.ViewGroup.drawChild(ViewGroup.java:3405)\r\n>                                                                          at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3199)\r\n>                                                                          at android.view.View.updateDisplayListIfDirty(View.java:14133)\r\n>                                                                          at android.view.View.getDisplayList(View.java:14161)\r\n>                                                                          at android.view.View.draw(View.java:14928)\r\n>                                                                          at android.view.ViewGroup.drawChild(ViewGroup.java:3405)\r\n>                                                                          at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3199)\r\n>                                                                          at android.view.View.updateDisplayListIfDirty(View.java:14133)\r\n>                                                                          at android.view.View.getDisplayList(View.java:14161)\r\n>                                                                          at android.view.View.draw(View.java:14928)\r\n>                                                                          at android.view.ViewGroup.drawChild(ViewGroup.java:3405)\r\n>                                                                          at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3199)\r\n>                                                                          at android.view.View.draw(View.java:15207)\r\n>                                                                          at android.widget.FrameLayout.draw(FrameLayout.java:592)\r\n>                                                                          at com.android.internal.policy.impl.PhoneWindow$DecorView.draw(PhoneWindow.java:2599)\r\n>                                                                          at android.view.View.updateDisplayListIfDirty(View.java:14138)\r\n>                                                                          at android.view.View.getDisplayList(View.java:14161)\r\n>                                                                          at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:268)\r\n>                                                                          at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:274)\r\n>                                                                          at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:313)\r\n>                                                                          at android.view.ViewRootImpl.draw(ViewRootImpl.java:2503)\r\n>                                                                          at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2341)\r\n>                                                                          at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:1972)\r\n>                                                                          at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1058)\r\n>                                                                          at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:5796)\r\n>                                                                          at android.view.Choreographer$CallbackRecord.run(Choreographer.java:767)\r\n>                                                                          at android.view.Choreographer.doCallbacks(Choreographer.java:580)\r\n>                                                                          at android.view.Choreographer.doFrame(Choreographer.java:550)\r\n>                                                                          at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:753)\r\n>                                                                          at android.os.Handler.handleCallback(Handler.java:739)\r\n>                                                                          at android.os.Handler.dispatchMessage(Handler.java:95)\r\n>                                                                          at android.os.Looper.loop(Looper.java:211)\r\n>                                                                          at android.app.ActivityThread.main(ActivityThread.java:5317)\r\n>                                                                          at java.lang.reflect.Method.invoke(Native Method)\r\n>                                                                          at java.lang.reflect.Method.invoke(Method.java:372)\r\n>                                                                          at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1016)\r\n>                                                                          at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:811)\r\n>                                                                       Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'org.tensorflow.Output org.tensorflow.Operation.output(int)' o", "comments": ["@chengsam This is the error typically seen when the output node is not found when the classifier tries to check the number of output classes.\r\n\r\nDoes this work if you remove the \":0\"s from the end of the node names?", "@andrewharp It worked! But why the doc said the label names need the \":0\"?", "@chengsam It used to accept them before we switched to using the Java API internally. Looks like the comment just didn't get updated, so I'll make a fix.", "To clarify, specifying ports on the node names passed to fillNode* and readNode* in TensorFlowInferenceInterface should still work.\r\n\r\nThe crash here was likely happening in TensorFlowImageClassifier where it tries to get the size of the output node prior to inference:\r\n`final Operation operation = c.inferenceInterface.graph().operation(outputName);`", "I use keras change to tensorflow but when i use it on Android It says can't find the input layer but in keras my input layer is bidirectional_3 but I use keras.input_names result is sequential_3_input ,when I try both ,None of them how can I do?? I use TensorflowInference.feed()"]}, {"number": 8232, "title": "Optimize Array initializer_list constructor.", "body": "std::vector reserve + emplace could be more efficient than\r\n  - resize + copy\r\n      or\r\n  - construct default + copy", "comments": ["Can one of the admins verify this patch?", "@hawkinsp please review.", "@hawkinsp Thanks for the review. When we \"reserve\" a capacity for vector, we avoid creating the elements. On the other hand, the \"resize\" option or the \"vector constructor + copy \" option, the elements are created in the vector. So when we call the \"[]\"  operator to get a reference to the object at that index, we again copy the element to the already default constructed element.  So we have a creation + copy cost. In the reserve case, we incur just the creation cost. Also, emplace_back wont resize the vector every time because of the 'reserve'.", "I think there are two ways forward here: I can close this PR, or we can look at benchmarks. You can find other micro-benchmarks throughout the code, and writing one of those for this piece of code would be a nice thing to have anyway.\r\n\r\nLet me know.", "Closing due to inactivity."]}, {"number": 8231, "title": "memory leak when training complex neural networks", "body": "When training some complex (seq-to-seq) neural networks, the memory cost of my program will keep growing, and this only happens on GPU...(On CPU, everything is OK)\r\nI used to report this problem in this issue: https://github.com/tensorflow/tensorflow/issues/6599\r\nAfter that, I solved this problem by encapsulating my encoding method as an RNN cell, so this issue was closed, though no one knows the reason...\r\nBut now, this problem happens again, because I changed the structure of my network...\r\nI do not think this is due to the bugs in my program, because it runs very well on CPU...", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!\r\n\r\nIn your previous issue report you did not describe how you measured or diagnosed the 'memory leak'.   What tool or statistic are you using?\r\n\r\nAlso, if you believe there is a specific GPU operation which is somehow leaking tensors the this would be difficult to triage without a repro - either code or a GraphDef - which would allow us to indentify which ops you are running. Are you able to provide either of these?", "@carltonwang you can use https://github.com/yaroslavvb/memory_util to get a timeline of all tensor allocations/deallocations, determine which tensors are getting allocated that are taking so much space, and then decide if this is intentional (requested by your program), or not", "Look at this code:\r\n`result = tf.contrib.layers.bias_add(inputs=tf.matmul(a=left, b=right), activation_fn=tf.nn.softmax)`\r\nIf I use \"softmax\" as the activation function, there will be a memory leak, but if I change the activation function to \"relu\" or \"softplus\" or \"sigmoid\", the memory leak will disappear.\r\nBy the way, this \"softmax\" is not the one used at the last layer for classification.\r\n\r\nPlatform: CentOS 7, x86_64\r\nTensorFlow version: 1.0_gpu\r\n\r\n", "@carltonwang, did you try @yaroslavvb's memory analyzer. Perhaps with and without using tf.nn.softmax to get a baseline.", "After I update from TF 1.0.0 to TF 1.0.1, this problem disappear...", "Thanks, closing since it seems to be resolved in newer version of software.", "I also find the same problem. After update TF from 1.0.0 to 1.0.1, the memory leak still exists. \r\nSo we implement the softmax mannally use other TF operators such as tf.exp , etc. The the memory leak disappear. ", "we have submit a new open issue here https://github.com/tensorflow/tensorflow/issues/9779"]}, {"number": 8230, "title": "Added missing SVM in init file", "body": "cc: @martinwicke ", "comments": []}, {"number": 8229, "title": "Bug in contrib/tensor_forest/python/tensor_forest_test.py", "body": "My environment is Win7 x64, python 3.5, tensorflow r1.0 GPU version.\r\nI download contrib/tensor_forest/python/tensor_forest_test.py and run 'python test.py' in console.  Then the following error information shows:\r\n\r\n```\r\n..E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\co\r\nre\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"')\r\nfor unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_t\r\nype: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"')\r\n for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for\r\nunknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_ty\r\npe: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"')\r\nfor unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"'\r\n) for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') fo\r\nr unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') fo\r\nr unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"\r\n') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"C\r\nPU\"') for unknown op: UpdateFertileSlots\r\n.EEEEE.\r\n======================================================================\r\nERROR: testInferenceConstruction (__main__.TensorForestTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \".\\test.py\", line 90, in testInferenceConstruction\r\n    graph = graph_builder.inference_graph(input_data)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\t\r\nensor_forest.py\", line 459, in inference_graph\r\n    **inference_args))\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\t\r\nensor_forest.py\", line 958, in inference_graph\r\n    valid_leaf_threshold=self.params.valid_leaf_threshold)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\o\r\nps\\gen_tensor_forest_ops.py\", line 662, in tree_predictions\r\n    name=name)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_librar\r\ny.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 2397, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 1757, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 1707, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes\r\n.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes\r\n.py\", line 670, in _call_cpp_shape_fn_impl\r\n    status)\r\n  File \"F:\\Anaconda3\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.p\r\ny\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'T\r\nreePredictions'\r\n\r\n======================================================================\r\nERROR: testInferenceConstructionSparse (__main__.TensorForestTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \".\\test.py\", line 143, in testInferenceConstructionSparse\r\n    graph = graph_builder.inference_graph(input_data)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\t\r\nensor_forest.py\", line 459, in inference_graph\r\n    **inference_args))\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\t\r\nensor_forest.py\", line 958, in inference_graph\r\n    valid_leaf_threshold=self.params.valid_leaf_threshold)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\o\r\nps\\gen_tensor_forest_ops.py\", line 662, in tree_predictions\r\n    name=name)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_librar\r\ny.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 2397, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 1757, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 1707, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes\r\n.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes\r\n.py\", line 670, in _call_cpp_shape_fn_impl\r\n    status)\r\n  File \"F:\\Anaconda3\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.p\r\ny\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'T\r\nreePredictions'\r\n\r\n======================================================================\r\nERROR: testTrainingConstructionClassification (__main__.TensorForestTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \".\\test.py\", line 58, in testTrainingConstructionClassification\r\n    graph = graph_builder.training_graph(input_data, input_labels)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\t\r\nensor_forest.py\", line 425, in training_graph\r\n    **tree_kwargs))\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\t\r\nensor_forest.py\", line 705, in training_graph\r\n    regression=self.params.regression))\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\o\r\nps\\gen_tensor_forest_ops.py\", line 224, in count_extremely_random_stats\r\n    regression=regression, name=name)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_librar\r\ny.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 2397, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 1757, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 1707, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes\r\n.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes\r\n.py\", line 670, in _call_cpp_shape_fn_impl\r\n    status)\r\n  File \"F:\\Anaconda3\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.p\r\ny\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'C\r\nountExtremelyRandomStats'\r\n\r\n======================================================================\r\nERROR: testTrainingConstructionClassificationSparse (__main__.TensorForestTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \".\\test.py\", line 120, in testTrainingConstructionClassificationSparse\r\n    graph = graph_builder.training_graph(input_data, input_labels)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\t\r\nensor_forest.py\", line 425, in training_graph\r\n    **tree_kwargs))\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\t\r\nensor_forest.py\", line 705, in training_graph\r\n    regression=self.params.regression))\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\o\r\nps\\gen_tensor_forest_ops.py\", line 224, in count_extremely_random_stats\r\n    regression=regression, name=name)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_librar\r\ny.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 2397, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 1757, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 1707, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes\r\n.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes\r\n.py\", line 670, in _call_cpp_shape_fn_impl\r\n    status)\r\n  File \"F:\\Anaconda3\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.p\r\ny\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'C\r\nountExtremelyRandomStats'\r\n\r\n======================================================================\r\nERROR: testTrainingConstructionRegression (__main__.TensorForestTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \".\\test.py\", line 75, in testTrainingConstructionRegression\r\n    graph = graph_builder.training_graph(input_data, input_labels)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\t\r\nensor_forest.py\", line 425, in training_graph\r\n    **tree_kwargs))\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\t\r\nensor_forest.py\", line 705, in training_graph\r\n    regression=self.params.regression))\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\o\r\nps\\gen_tensor_forest_ops.py\", line 224, in count_extremely_random_stats\r\n    regression=regression, name=name)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_librar\r\ny.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 2397, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 1757, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line\r\n 1707, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes\r\n.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes\r\n.py\", line 670, in _call_cpp_shape_fn_impl\r\n    status)\r\n  File \"F:\\Anaconda3\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.p\r\ny\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'C\r\nountExtremelyRandomStats'\r\n\r\n----------------------------------------------------------------------\r\nRan 9 tests in 2.945s\r\n\r\nFAILED (errors=5)\r\n```", "comments": ["Support for the `tf.contrib.tensor_forest` submodule was not included in TensorFlow 1.0, but it has been fixed at HEAD. Can you try upgrading to a nightly build of TensorFlow and see if the problem persists?", "@mrry  well, it is OK using nightly version.\r\nIt shows:\r\n DeprecationWarning: Please use assertEqual instead. self.assertEquals(2, hparams.num_classes)\r\nRan 9 tests in 7.760s\r\n\r\nOK\r\n\r\nHowever this nightly version also show these warning info when I do some other work:\r\n\r\n2017-03-09 12:45:53.258939: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use SSE instructions, but these are available on your m\r\nachine and could speed up CPU computations.\r\n2017-03-09 12:45:53.259939: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use SSE2 instructions, but these are available on your\r\nmachine and could speed up CPU computations.\r\n2017-03-09 12:45:53.260939: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use SSE3 instructions, but these are available on your\r\nmachine and could speed up CPU computations.\r\n2017-03-09 12:45:53.261939: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use SSE4.1 instructions, but these are available on you\r\nr machine and could speed up CPU computations.\r\n2017-03-09 12:45:53.262939: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use SSE4.2 instructions, but these are available on you\r\nr machine and could speed up CPU computations.\r\n2017-03-09 12:45:53.263939: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use AVX instructions, but these are available on your m\r\nachine and could speed up CPU computations.\r\n2017-03-09 12:45:53.263939: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use AVX2 instructions, but these are available on your\r\nmachine and could speed up CPU computations.\r\n2017-03-09 12:45:53.264940: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use FMA instructions, but these are available on your m\r\nachine and could speed up CPU computations.\r\n\r\nSo I continue to use tf r1.0 and wait for your good news.", "It sounds like the issue has been fixed in that case. You can safely ignore the warnings about SSE and AVX in the nightly build... "]}, {"number": 8228, "title": "Sorry I got question again\u3002\u3002\u3002", "body": "Can TF use ram as swap space for GPU memory\uff1f Cause  I ran a model with batch size 100 on GPU which will be out of memory on caffe. But on TF it works and just show pool_allocator. As I saw in other issues that's just a log. If it can swap why does it crashes sometimes? Is there any limitation? ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8226, "title": "Unable to uninstall tensorflow", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["*sudo pip uninstall does not work,  says Cannot uninstall requirement tensorflow, not installed.\r\n* Installed CUDA version is 8.0, cudnn 5.1\r\n* python -c \"import tensorflow; print(tensorflow.__version__)\" output is\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.1\r\n\r\n", "got around to solving it, you need to mention the gpu version of tensorflow which is tensorflow-gpu when uninstalling, silly mistake :)", "Thank you @bharathbs93 your method suits me!"]}, {"number": 8225, "title": "[feature] Define Op Polymorphic on Fully Defined vs not Fully Defined Shape", "body": "Repost from [SO](http://stackoverflow.com/questions/42655141/tensorflow-define-op-polymorphic-on-fully-defined-vs-not-fully-defined-shape/42683994#42683994):\r\n\r\nWhen defining an Op in Tensorflow, make it possible to provide two Kernels for the op that are polymorphic on whether the shape for the inputs are fully defined.\r\n\r\nFor example, you can then optimize when shape is fully known / defined by constructing certain structures once at Kernel construction.\r\n\r\n/CC @mrry ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8224, "title": "Feature request: numeric type promotion", "body": "TensorFlow does some numeric type promotion.\r\nIt should do more of it.\r\n\r\nExamples:\r\n```\r\n# this works, 2 is int32, gets promoted to float32\r\ntf.pow(2.,2)\r\n\r\n# this fails, apply_op promotion logic is not smart enough\r\ntf.pow(2,2.)\r\n\r\n# this fails, [2,] is converted to int32 but needs to be int64\r\ntf.sparse_placeholder(tf.float32, [2,])\r\n\r\n# this works, numpy arrays are int64 by default\r\ntf.sparse_placeholder(tf.float32, np.array([2,]))\r\n```\r\n\r\nThis came up in:\r\nhttps://github.com/tensorflow/tensorflow/issues/7483\r\nhttps://github.com/tensorflow/tensorflow/issues/7220\r\nhttps://github.com/tensorflow/tensorflow/issues/7170\r\n\r\ncc @josh11b who wrote type promotion logic in OpDefLibrary.apply_op\r\ncc @suharshs who changed the default to treat Python integer as int32", "comments": ["@suharshs @josh11b Could you comment on whether anybody is actively working on this?  ", "@saxenasaurabh ", "Is this about the TODO here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/op_def_library.py#L356\r\n?  That issue is on my radar, something I hope to fix by moving more logic into the generated code.", "@josh11b yes, it's related.\r\n\r\nAlthough It feels like numeric promotion is a generic feature of computation framework that shouldn't be Python-specific", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8223, "title": "R1.0 to master", "body": "Merging r1.0 back into master.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@cwhipkey @jhseu Could you take another look at this PR?", "@gunan any other issues before I merge? CLA?", "CLA bot does this during merge PRs. All CLAs are signed, thats all we need."]}, {"number": 8222, "title": "Merging r1.0 to master", "body": "Merging r1.0 back into master to include all the commits that were needed for 1.0.1.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "This likely won't work, and none of the changes seem that important (most are only relevant to 1.0). Let's not merge back in?", "IMO, only the doc changes are useful. The others may not compile and conflict with what's in master. @caisq can comment on whether the `stepper_test` changes are useful.", "+1 to what Jonathan said.", "We have to merge a few changes:\r\n-changing the ci_build Dockerfiles to use ubuntu 14.04\r\n-fixes to mac_gpu tests.\r\n\r\nWe can pick master version for most changes, but we should merge the branch back into master. ", "Sorry reverted to the wrong version of cwhipkey's changes, fixing now.", "Messed up a few things, had some confusion on what should go in. I'll just create a new PR."]}, {"number": 8221, "title": "Fixing quotes in documentation. See https://github.com/tensorflow/ten\u2026", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/8207", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!\r\n\r\nIt is under my company Knuedge", "Looks like CLABot isn't satisfied. Can you verify that the commit email address matches an email that you've signed a CLA under? (see the links at the bottom of CLABot's last message)", "They are having the same problem here https://github.com/google/protobuf/pull/2807#issuecomment-286154817. I don't know how to check which email is my  CLA one, but if I remember correctly, it was the amacdonald@knuedge.com one. ", "@willnorris Is there a way I can check whether someone else has a proper CLA? It would be useful and I wouldn't have to bother you.", "I was told by @acozzette in https://github.com/google/protobuf/pull/2807#issuecomment-286154817 that it was because I might not be in the Google Group for my company. I am working it out now", "> @willnorris Is there a way I can check whether someone else has a proper CLA? It would be useful and I wouldn't have to bother you.\r\n\r\ngo/cla-signers", "@willnorris Thanks!", "Is it also possible that your commit email is not your company email (I did not check it, but this is a common cause).\r\nYou can git commit --amend to edit the commit author", "I signed it!", "Is the commit email the right one?", "I think I fixed the CLA. I had to amend a couple commits to get the emails right", "@Staberinde are you ok with your commit ending up in TensorFlow?", "a83cc3f should not be part of this PR -- can you rebase to head?", "@martinwicke  I think its gone.", "Thanks! Just waiting for @Staberinde for CLA consent.\r\n\r\nJenkins, test this please?", "Jenkins, test this please?", "Jenkins, test this please.\r\n\r\nSorry, must be tensorflow member to trigger.", "I signed it!", "So... the CLA is still not registered as signed. We definitely need this.", "I signed it!", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@staberinde, are you ok with this commit ending up in TensorFlow?", "Ok, sorry @aidan-plenert-macdonald. This is very annoying, but since we don't have explicit consent from @Staberinde I cannot merge this PR. Can you make a new one that has only your commits?\r\n\r\nI will close this one."]}, {"number": 8220, "title": "Session hang issue with python multiprocessing", "body": "### Issue summary\r\n\r\nI am having trouble allocating GPU devices for a multiprocessing pool. Please see the short code reproduction below. I would like to understand why I am getting the CUDA_ERROR_NOT_INITIALIZED error in case 4. For this case, the program hangs, and I have to stop my docker container to exit.\r\n\r\n### Minimal reproducible example \r\n\r\ncore code:\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef run_session(device):\r\n    gpu_options = tf.GPUOptions(allow_growth=True, visible_device_list=device)\r\n    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n    print('Using device #%s' % device)\r\n    a = tf.placeholder(tf.int16, name='a')\r\n    y = tf.identity(a, name='y')\r\n    print sess.run(y, feed_dict={a: 3})\r\n    sess.close()\r\n    print('Done.')\r\n```\r\nCase 1 (this works fine):\r\n```python\r\nrun_session('0')\r\n```\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:08:00.0\r\nTotal memory: 5.97GiB\r\nFree memory: 5.86GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:08:00.0)\r\nUsing device #0\r\n3\r\nDone.\r\n```\r\nCase 2 (this works fine):\r\n```python\r\nrun_session('0')\r\nrun_session('1')\r\n```\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:08:00.0\r\nTotal memory: 5.97GiB\r\nFree memory: 5.86GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:08:00.0)\r\nUsing device #0\r\n3\r\nDone.\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x24cbbe0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:84:00.0\r\nTotal memory: 5.97GiB\r\nFree memory: 5.86GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 1, name: GeForce GTX 980 Ti, pci bus id: 0000:84:00.0)\r\nUsing device #1\r\n3\r\nDone.\r\n```\r\nCase 3 (this works fine):\r\n```python\r\nimport multiprocessing as mp\r\n\r\np = mp.Pool(2)\r\np.map(run_session, ['0', '1'])\r\np.close()\r\np.join()\r\n```\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:84:00.0\r\nTotal memory: 5.97GiB\r\nFree memory: 5.86GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 1, name: GeForce GTX 980 Ti, pci bus id: 0000:84:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:08:00.0\r\nTotal memory: 5.97GiB\r\nFree memory: 5.86GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:08:00.0)\r\nUsing device #1\r\nUsing device #0\r\n3\r\nDone.\r\n3\r\nDone.\r\n```\r\nCase 4 (here, the program hangs):\r\n```python\r\nimport multiprocessing as mp\r\n\r\nrun_session('0')\r\np = mp.Pool(2)\r\np.map(run_session, ['0', '1'])\r\np.close()\r\np.join()\r\n```\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:08:00.0\r\nTotal memory: 5.97GiB\r\nFree memory: 5.86GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:08:00.0)\r\nUsing device #0\r\n3\r\nDone.\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1368] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED\r\nUsing device #0\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1368] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED\r\nUsing device #1\r\n```\r\n\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.4 LTS (GNU/Linux 3.19.0-25-generic x86_64)\r\nDocker container: gcr.io/tensorflow/tensorflow:latest-devel-gpu\r\nCUDA version: 8.0.61\r\ncuDNN version: 5.1.10\r\n\r\n### Related GitHub issues\r\n#1578\r\n", "comments": ["@suharshs This seems like it may be down to device creation in DirectSession.  Could you please comment?", "@prb12 @suharshs @zheng-xq Have any of you taken a look at this one yet? I am getting the same issue for case 4. Any ideas for a temporary fix?", "Apologies for the delay, I am taking a look into this soon.", "Update: I have looked into this a bit more, and have a couple more interesting repro cases :)\r\nWorks:\r\n```\r\nrun_session('0')\r\nrun_session('0')\r\n```\r\n\r\nHangs:\r\n```\r\n  run_session('0')\r\n  p = mp.Process(target=run_session, args=('0'))\r\n  p.start()\r\n  p.join()\r\n```\r\n\r\nIt looks like there is some shared python tensorflow state that interferes when a new python process is created (multiprocessing creates new python process whose state separation i am not to clear on). I plan to look into it very soon, but just wanted to provide an update in case that gives you any workarounds.", "The python multiprocessing package seems to just call fork when creating a child process. This cannot work when the child process calls async code (i.e TensorFlow is multithreaded). From the posix [spec](http://pubs.opengroup.org/onlinepubs/009695399/functions/fork.html) for fork:\r\n\r\n>  If a multi-threaded process calls fork(), the new process shall contain a replica of the calling thread and its entire address space, possibly including the states of mutexes and other resources. Consequently, to avoid errors, the child process may only execute async-signal-safe operations until such time as one of the exec functions is called.\r\n\r\nSo long story short, don't use python multiprocessing for anything non-trivial and expect it to work :)\r\n", "Hi I had the same issue today, but this problem can be resolved by putting `import tensorflow as tf` inside your worker function (and the result is well parallelised). ", "@suharshs Python multiprocessing works fine with tensorflow. The only thing should be noticed is that tensorflow must be imported independently inside each process (must use multiprocessing instead of multithreading since tensorflow will take over the entire process). Below is how I achieved multi-GPU and multiprocessing inferencing and I hope it helps:\r\n\r\n\r\n```python\r\nimport os\r\nimport multiprocessing\r\n\r\n\r\nclass Predictor(multiprocessing.Process):\r\n    def __init__(self, input_queue, gpu_id):\r\n        multiprocessing.Process.__init__(self)\r\n        self.input_queue = input_queue\r\n        self.gpu_id = gpu_id\r\n    def run(self):\r\n        #set GPU id before importing tensorflow!!!!!!!!!!!!!\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(self.gpu_id)\r\n        #import tensorflow here\r\n        import tensorflow as tf\r\n        sess = tf.Session()\r\n        print('Using device #%s' % self.gpu_id)\r\n        a = tf.placeholder(tf.int16, name='a')\r\n        y = tf.identity(a, name='y')\r\n        while True:\r\n            input = self.input_queue.get()\r\n            if input is None:\r\n                self.input_queue.task_done()\r\n                print(\"Exiting Process %d\" % self.gpu_id)\r\n                break\r\n            else:\r\n                print sess.run(y, feed_dict={a: input})\r\n                self.input_queue.task_done()\r\n        sess.close()\r\n        return\r\n\r\nif __name__ == \"__main__\":\r\n    jobs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n    num_gpus = 2\r\n    p_list = []\r\n    input_queue = multiprocessing.JoinableQueue()\r\n\r\n    for i in range(num_gpus):\r\n        p = Predictor(input_queue, i)\r\n        p_list.append(p)\r\n    for p in p_list:\r\n        p.start()\r\n    for job in jobs:\r\n        input_queue.put(job)\r\n    for i in range(num_gpus):\r\n        input_queue.put(None)\r\n\r\n    input_queue.join()\r\n    for p in p_list:\r\n        p.join()\r\n```", "I am also running into this issue. Multiprocessing works  **unless** I first run a session in the parent thread. I've tried moving the \"import tensorflow\" statement to the function as @Lancerchiang suggested with no luck. Below is my minimal repro with 4 test cases. \r\n\r\n```python\r\nimport os\r\nimport tensorflow\r\nfrom multiprocessing.pool import Pool\r\n\r\ndef runInSubprocess(somearg):\r\n    print('Training model on process id {}.'.format(os.getpid()))\r\n    with tensorflow.Session() as sess:\r\n        sess.run(tensorflow.global_variables_initializer())\r\n\r\n# This Hangs:\r\nrunInSubprocess(2)\r\nPool(processes=2).map(runInSubprocess, [1,2])\r\n\r\n# This works:\r\nrunInSubprocess(2)\r\nrunInSubprocess(2)\r\n\r\n# This works:\r\nPool(processes=2).map(runInSubprocess, [1,2])\r\nPool(processes=2).map(runInSubprocess, [1,2])\r\n\r\n# This works:\r\nPool(processes=2).map(runInSubprocess, [1,2])\r\nrunInSubprocess(2)\r\n```", "@breckuh If you really need to run a `tensorflow` session in your parent process, my advice is that launching explicit child processes like I did above instead of using pool mapping, and import `tensorflow` in your parent process after you have done that in your child processes.\r\n\r\n\r\n```python\r\nimport os\r\nimport multiprocessing\r\nimport time\r\n\r\nclass Predictor(multiprocessing.Process):\r\n    def __init__(self, input_queue, gpu_id):\r\n        multiprocessing.Process.__init__(self)\r\n        self.input_queue = input_queue\r\n        self.gpu_id = gpu_id\r\n    def run(self):\r\n        #set GPU id before importing tensorflow!!\r\n        #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(self.gpu_id)\r\n        import tensorflow as tf\r\n        sess = tf.Session()\r\n        print('Using device #%s' % self.gpu_id)\r\n        a = tf.placeholder(tf.int16, name='a')\r\n        y = tf.identity(a, name='y')\r\n        while True:\r\n            input = self.input_queue.get()\r\n            if input is None:\r\n                self.input_queue.task_done()\r\n                print(\"Exiting Process %d\" % self.gpu_id)\r\n                break\r\n            else:\r\n                print sess.run(y, feed_dict={a: input})\r\n                self.input_queue.task_done()\r\n        sess.close()\r\n        return\r\n\r\nif __name__ == \"__main__\":\r\n    works = [4,5]\r\n    num_gpus = 2\r\n    p_list = []\r\n    input_queue = multiprocessing.JoinableQueue()\r\n\r\n    for i in range(num_gpus):\r\n        p = Predictor(input_queue, i)\r\n        p_list.append(p)\r\n    for p in p_list:\r\n        p.start()\r\n\r\n    time.sleep(2)\r\n\r\n    import tensorflow as tf\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n    for work in works:\r\n        input_queue.put(work)\r\n    for i in range(num_gpus):\r\n        input_queue.put(None)\r\n\r\n    input_queue.join()\r\n    for p in p_list:\r\n        p.join()\r\n```\r\n\r\nIt would give:\r\n\r\n```\r\n2018-05-11 11:01:57.844637: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n2018-05-11 11:01:57.844638: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\nUsing device #1\r\nUsing device #0\r\n2018-05-11 11:01:59.207167: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n4\r\n5\r\nExiting Process 1\r\nExiting Process 0\r\n```\r\n\r\nYou can see the three tensorflow sessions finished successfully.", "Thanks @Lancerchiang that makes sense. I don't actually know if we'll ever have this use case in practice, it only came up because our test suite was failing when certain tests were run in different orders. Then we fell down a rabbit hole isolating this :). In the end we just had the workaround where we specifically arranged our suite to run the tests in the child processes first, and then the tests in the parent processes after. Not ideal but good enough for now. What I would like to do is add a line or two to check if this hang might hit and then Throw/Alert the user, so no one is left hanging.", "@mrry .\r\nI am facing the problem - \"could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED\" due to memory issue while using django celery.\r\nIs there any work around for python multiprocessing or celery in case of distributed tensorflow with gpu. \r\n", "> import numpy as np\r\nimport tensorflow as tf\r\nfrom multiprocessing import Process, Pool\r\nimport os\r\nimport time\r\n\r\n>def run_proc(name, session):\r\n        \r\n\r\n- import tensorflow as tf\r\n-         process_session = session\r\n-         process_input = process_session.graph.get_tensor_by_name('input:0')\r\n-         process_output = process_session.graph.get_tensor_by_name('output:0')\r\n-         res = process_session.run(process_input, feed_dict={process_output: np.ones((10, 2))})\r\n- \r\n\r\n>if __name__ == '__main__':\r\n        import tensorflow as tf\r\n        session = tf.Session()\r\n        with session.as_default():\r\n            input = tf.placeholder(dtype=tf.float32, shape=[None, 2], name='input')\r\n            tmp = tf.ones(shape=[10, 2])\r\n            add_output = tf.add(x=input, y=tmp, name='output')\r\n        print('Parent process %s.' % os.getpid())\r\n        p = Process(\r\n            target=run_proc,\r\n            args=('test', session))\r\n        print('Process will start.')\r\n        p.start()\r\n    p.join()\r\n    print('Process end.')\r\n>it stucks when new process  start to run(```, feed_dict={```})", "> @mrry .\r\n> I am facing the problem - \"could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED\" due to memory issue while using django celery.\r\n> Is there any work around for python multiprocessing or celery in case of distributed tensorflow with gpu.\r\n\r\nI just hit the same issue when using celery worker to run tensorflow gpu. I this issue solved?", "I miss the problem:PicklingError: Can't pickle <type 'module'>: attribute lookup __builtin__.module failed @rfeinman @", "> @suharshs Python multiprocessing works fine with tensorflow. The only thing should be noticed is that tensorflow must be imported independently inside each process (must use multiprocessing instead of multithreading since tensorflow will take over the entire process). Below is how I achieved multi-GPU and multiprocessing inferencing and I hope it helps:\r\n> \r\n> ```python\r\n> import os\r\n> import multiprocessing\r\n> \r\n> \r\n> class Predictor(multiprocessing.Process):\r\n>     def __init__(self, input_queue, gpu_id):\r\n>         multiprocessing.Process.__init__(self)\r\n>         self.input_queue = input_queue\r\n>         self.gpu_id = gpu_id\r\n>     def run(self):\r\n>         #set GPU id before importing tensorflow!!!!!!!!!!!!!\r\n>         os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(self.gpu_id)\r\n>         #import tensorflow here\r\n>         import tensorflow as tf\r\n>         sess = tf.Session()\r\n>         print('Using device #%s' % self.gpu_id)\r\n>         a = tf.placeholder(tf.int16, name='a')\r\n>         y = tf.identity(a, name='y')\r\n>         while True:\r\n>             input = self.input_queue.get()\r\n>             if input is None:\r\n>                 self.input_queue.task_done()\r\n>                 print(\"Exiting Process %d\" % self.gpu_id)\r\n>                 break\r\n>             else:\r\n>                 print sess.run(y, feed_dict={a: input})\r\n>                 self.input_queue.task_done()\r\n>         sess.close()\r\n>         return\r\n> \r\n> if __name__ == \"__main__\":\r\n>     jobs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n>     num_gpus = 2\r\n>     p_list = []\r\n>     input_queue = multiprocessing.JoinableQueue()\r\n> \r\n>     for i in range(num_gpus):\r\n>         p = Predictor(input_queue, i)\r\n>         p_list.append(p)\r\n>     for p in p_list:\r\n>         p.start()\r\n>     for job in jobs:\r\n>         input_queue.put(job)\r\n>     for i in range(num_gpus):\r\n>         input_queue.put(None)\r\n> \r\n>     input_queue.join()\r\n>     for p in p_list:\r\n>         p.join()\r\n> ```\r\n\r\nI wonder if I want get a return value from every processing, how should I do? return in method 'run'? and how can I get this return value?", "> I wonder if I want get a return value from every processing, how should I do? return in method 'run'? and how can I get this return value?\r\n\r\n```python\r\nimport os\r\nimport multiprocessing\r\n\r\n\r\nclass Predictor(multiprocessing.Process):\r\n    def __init__(self, input_queue, output_queue, gpu_id):\r\n        multiprocessing.Process.__init__(self)\r\n        self.input_queue = input_queue\r\n        self.output_queue = output_queue\r\n        self.gpu_id = gpu_id\r\n\r\n    def run(self):\r\n        #set GPU id before importing tensorflow!!!!!!!!!!!!!\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(self.gpu_id)\r\n        #import tensorflow here\r\n        import tensorflow as tf\r\n        sess = tf.Session()\r\n        print('Using device #%s' % self.gpu_id)\r\n        a = tf.placeholder(tf.int16, name='a')\r\n        y = tf.identity(a, name='y')\r\n        while True:\r\n            input = self.input_queue.get()\r\n            if input is None:\r\n                self.input_queue.task_done()\r\n                print(\"Exiting Process %d\" % self.gpu_id)\r\n                break\r\n            else:\r\n                res = sess.run(y, feed_dict={a: input})\r\n                self.input_queue.task_done()\r\n                self.output_queue.put(res)\r\n        sess.close()\r\n        return\r\n\r\nif __name__ == \"__main__\":\r\n    jobs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n    num_gpus = 2\r\n    p_list = []\r\n    input_queue = multiprocessing.JoinableQueue()\r\n    output_queue = multiprocessing.Queue()\r\n    for i in range(num_gpus):\r\n        p = Predictor(input_queue, output_queue, i)\r\n        p_list.append(p)\r\n\r\n    for p in p_list:\r\n        p.start()\r\n\r\n    for job in jobs:\r\n        input_queue.put(job)\r\n\r\n    for i in range(num_gpus):\r\n        input_queue.put(None)\r\n\r\n    for i in range(num_gpus):\r\n        print(output_queue.get())\r\n\r\n    input_queue.join()\r\n    \r\n    for p in p_list:\r\n        p.join()\r\n```", "For the code, I'm confusing that why \r\n```\r\nfor i in range(num_gpus):\r\n        input_queue.put(None)\r\n```\r\nWhat is this part means?\r\n@Lancerchiang ", "> For the code, I'm confusing that why\r\n> \r\n> ```\r\n> for i in range(num_gpus):\r\n>         input_queue.put(None)\r\n> ```\r\n> What is this part means?\r\n> @Lancerchiang\r\n\r\n@zhangjinyangnwpu  The workers won't know the tasks are done if this signal is not broadcasted", "> @breckuh If you really need to run a `tensorflow` session in your parent process, my advice is that launching explicit child processes like I did above instead of using pool mapping, and import `tensorflow` in your parent process after you have done that in your child processes.\r\n> \r\n> ```python\r\n> import os\r\n> import multiprocessing\r\n> import time\r\n> \r\n> class Predictor(multiprocessing.Process):\r\n>     def __init__(self, input_queue, gpu_id):\r\n>         multiprocessing.Process.__init__(self)\r\n>         self.input_queue = input_queue\r\n>         self.gpu_id = gpu_id\r\n>     def run(self):\r\n>         #set GPU id before importing tensorflow!!\r\n>         #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(self.gpu_id)\r\n>         import tensorflow as tf\r\n>         sess = tf.Session()\r\n>         print('Using device #%s' % self.gpu_id)\r\n>         a = tf.placeholder(tf.int16, name='a')\r\n>         y = tf.identity(a, name='y')\r\n>         while True:\r\n>             input = self.input_queue.get()\r\n>             if input is None:\r\n>                 self.input_queue.task_done()\r\n>                 print(\"Exiting Process %d\" % self.gpu_id)\r\n>                 break\r\n>             else:\r\n>                 print sess.run(y, feed_dict={a: input})\r\n>                 self.input_queue.task_done()\r\n>         sess.close()\r\n>         return\r\n> \r\n> if __name__ == \"__main__\":\r\n>     works = [4,5]\r\n>     num_gpus = 2\r\n>     p_list = []\r\n>     input_queue = multiprocessing.JoinableQueue()\r\n> \r\n>     for i in range(num_gpus):\r\n>         p = Predictor(input_queue, i)\r\n>         p_list.append(p)\r\n>     for p in p_list:\r\n>         p.start()\r\n> \r\n>     time.sleep(2)\r\n> \r\n>     import tensorflow as tf\r\n> \r\n>     with tf.Session() as sess:\r\n>         sess.run(tf.global_variables_initializer())\r\n> \r\n>     for work in works:\r\n>         input_queue.put(work)\r\n>     for i in range(num_gpus):\r\n>         input_queue.put(None)\r\n> \r\n>     input_queue.join()\r\n>     for p in p_list:\r\n>         p.join()\r\n> ```\r\n> \r\n> It would give:\r\n> \r\n> ```\r\n> 2018-05-11 11:01:57.844637: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n> 2018-05-11 11:01:57.844638: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n> Using device #1\r\n> Using device #0\r\n> 2018-05-11 11:01:59.207167: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n> 4\r\n> 5\r\n> Exiting Process 1\r\n> Exiting Process 0\r\n> ```\r\n> You can see the three tensorflow sessions finished successfully.\r\n\r\n\r\nI really need to run a session(write in c++ called by python3) in the parent process, the program hangs before multi-process evaluation using GPU.   As far as I know, GPU memory is process related which means only kill process can release memory.  Does this mean the GPU used in the parent process session cannot be used for the next multi-process evaluation?\r\nWhen launching explicit child processes like you said(without input or output queue)  the same problem occurred as using pool.apply_async(func, args).\r\nBesides I am writing some tools on the tensorflow source code, there is no `import tensorflow as tf`.\r\n Any ideas to fix this?  @Lancerchiang", "> > @breckuh If you really need to run a `tensorflow` session in your parent process, my advice is that launching explicit child processes like I did above instead of using pool mapping, and import `tensorflow` in your parent process after you have done that in your child processes.\r\n> > ```python\r\n> > import os\r\n> > import multiprocessing\r\n> > import time\r\n> > \r\n> > class Predictor(multiprocessing.Process):\r\n> >     def __init__(self, input_queue, gpu_id):\r\n> >         multiprocessing.Process.__init__(self)\r\n> >         self.input_queue = input_queue\r\n> >         self.gpu_id = gpu_id\r\n> >     def run(self):\r\n> >         #set GPU id before importing tensorflow!!\r\n> >         #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(self.gpu_id)\r\n> >         import tensorflow as tf\r\n> >         sess = tf.Session()\r\n> >         print('Using device #%s' % self.gpu_id)\r\n> >         a = tf.placeholder(tf.int16, name='a')\r\n> >         y = tf.identity(a, name='y')\r\n> >         while True:\r\n> >             input = self.input_queue.get()\r\n> >             if input is None:\r\n> >                 self.input_queue.task_done()\r\n> >                 print(\"Exiting Process %d\" % self.gpu_id)\r\n> >                 break\r\n> >             else:\r\n> >                 print sess.run(y, feed_dict={a: input})\r\n> >                 self.input_queue.task_done()\r\n> >         sess.close()\r\n> >         return\r\n> > \r\n> > if __name__ == \"__main__\":\r\n> >     works = [4,5]\r\n> >     num_gpus = 2\r\n> >     p_list = []\r\n> >     input_queue = multiprocessing.JoinableQueue()\r\n> > \r\n> >     for i in range(num_gpus):\r\n> >         p = Predictor(input_queue, i)\r\n> >         p_list.append(p)\r\n> >     for p in p_list:\r\n> >         p.start()\r\n> > \r\n> >     time.sleep(2)\r\n> > \r\n> >     import tensorflow as tf\r\n> > \r\n> >     with tf.Session() as sess:\r\n> >         sess.run(tf.global_variables_initializer())\r\n> > \r\n> >     for work in works:\r\n> >         input_queue.put(work)\r\n> >     for i in range(num_gpus):\r\n> >         input_queue.put(None)\r\n> > \r\n> >     input_queue.join()\r\n> >     for p in p_list:\r\n> >         p.join()\r\n> > ```\r\n> > \r\n> > \r\n> > It would give:\r\n> > ```\r\n> > 2018-05-11 11:01:57.844637: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n> > 2018-05-11 11:01:57.844638: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n> > Using device #1\r\n> > Using device #0\r\n> > 2018-05-11 11:01:59.207167: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n> > 4\r\n> > 5\r\n> > Exiting Process 1\r\n> > Exiting Process 0\r\n> > ```\r\n> > You can see the three tensorflow sessions finished successfully.\r\n> \r\n> I really need to run a session(write in c++ called by python3) in the parent process, the program hangs before multi-process evaluation using GPU. As far as I know, GPU memory is process related which means only kill process can release memory. Does this mean the GPU used in the parent process session cannot be used for the next multi-process evaluation?\r\n> When launching explicit child processes like you said(without input or output queue) the same problem occurred as using pool.apply_async(func, args).\r\n> Besides I am writing some tools on the tensorflow source code, there is no `import tensorflow as tf`.\r\n> Any ideas to fix this? @Lancerchiang\r\n\r\nI currently don't know how to make different processes sharing the identical GPU memory at Python level. But for the session hang issue, how about you try to reload your customized module in the child processes? The folk() function is called when Python starts a new process and the parent process's module info will be copied as well. I guess reimporting the module would solve the problem.", "> @suharshs Python multiprocessing works fine with tensorflow. The only thing should be noticed is that tensorflow must be imported independently inside each process (must use multiprocessing instead of multithreading since tensorflow will take over the entire process). Below is how I achieved multi-GPU and multiprocessing inferencing and I hope it helps:\r\n> \r\n> ```python\r\n> import os\r\n> import multiprocessing\r\n> \r\n> \r\n> class Predictor(multiprocessing.Process):\r\n>     def __init__(self, input_queue, gpu_id):\r\n>         multiprocessing.Process.__init__(self)\r\n>         self.input_queue = input_queue\r\n>         self.gpu_id = gpu_id\r\n>     def run(self):\r\n>         #set GPU id before importing tensorflow!!!!!!!!!!!!!\r\n>         os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(self.gpu_id)\r\n>         #import tensorflow here\r\n>         import tensorflow as tf\r\n>         sess = tf.Session()\r\n>         print('Using device #%s' % self.gpu_id)\r\n>         a = tf.placeholder(tf.int16, name='a')\r\n>         y = tf.identity(a, name='y')\r\n>         while True:\r\n>             input = self.input_queue.get()\r\n>             if input is None:\r\n>                 self.input_queue.task_done()\r\n>                 print(\"Exiting Process %d\" % self.gpu_id)\r\n>                 break\r\n>             else:\r\n>                 print sess.run(y, feed_dict={a: input})\r\n>                 self.input_queue.task_done()\r\n>         sess.close()\r\n>         return\r\n> \r\n> if __name__ == \"__main__\":\r\n>     jobs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n>     num_gpus = 2\r\n>     p_list = []\r\n>     input_queue = multiprocessing.JoinableQueue()\r\n> \r\n>     for i in range(num_gpus):\r\n>         p = Predictor(input_queue, i)\r\n>         p_list.append(p)\r\n>     for p in p_list:\r\n>         p.start()\r\n>     for job in jobs:\r\n>         input_queue.put(job)\r\n>     for i in range(num_gpus):\r\n>         input_queue.put(None)\r\n> \r\n>     input_queue.join()\r\n>     for p in p_list:\r\n>         p.join()\r\n> ```\r\n\r\nThat would be very slow when inference, since import TensorFlow and load model consume seconds\r\n", "> > @suharshs Python multiprocessing works fine with tensorflow. The only thing should be noticed is that tensorflow must be imported independently inside each process (must use multiprocessing instead of multithreading since tensorflow will take over the entire process). Below is how I achieved multi-GPU and multiprocessing inferencing and I hope it helps:\r\n> > ```python\r\n> > import os\r\n> > import multiprocessing\r\n> > \r\n> > \r\n> > class Predictor(multiprocessing.Process):\r\n> >     def __init__(self, input_queue, gpu_id):\r\n> >         multiprocessing.Process.__init__(self)\r\n> >         self.input_queue = input_queue\r\n> >         self.gpu_id = gpu_id\r\n> >     def run(self):\r\n> >         #set GPU id before importing tensorflow!!!!!!!!!!!!!\r\n> >         os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(self.gpu_id)\r\n> >         #import tensorflow here\r\n> >         import tensorflow as tf\r\n> >         sess = tf.Session()\r\n> >         print('Using device #%s' % self.gpu_id)\r\n> >         a = tf.placeholder(tf.int16, name='a')\r\n> >         y = tf.identity(a, name='y')\r\n> >         while True:\r\n> >             input = self.input_queue.get()\r\n> >             if input is None:\r\n> >                 self.input_queue.task_done()\r\n> >                 print(\"Exiting Process %d\" % self.gpu_id)\r\n> >                 break\r\n> >             else:\r\n> >                 print sess.run(y, feed_dict={a: input})\r\n> >                 self.input_queue.task_done()\r\n> >         sess.close()\r\n> >         return\r\n> > \r\n> > if __name__ == \"__main__\":\r\n> >     jobs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n> >     num_gpus = 2\r\n> >     p_list = []\r\n> >     input_queue = multiprocessing.JoinableQueue()\r\n> > \r\n> >     for i in range(num_gpus):\r\n> >         p = Predictor(input_queue, i)\r\n> >         p_list.append(p)\r\n> >     for p in p_list:\r\n> >         p.start()\r\n> >     for job in jobs:\r\n> >         input_queue.put(job)\r\n> >     for i in range(num_gpus):\r\n> >         input_queue.put(None)\r\n> > \r\n> >     input_queue.join()\r\n> >     for p in p_list:\r\n> >         p.join()\r\n> > ```\r\n> \r\n> I wonder if I want get a return value from every processing, how should I do? return in method 'run'? and how can I get this return value?\r\n\r\nThis way works!", "> Hi I had the same issue today, but this problem can be resolved by putting `import tensorflow as tf` inside your worker function (and the result is well parallelised).\r\n\r\nThank you! My script with multiprocessing was working on windows, but it hung up on `tf.image.decode_image` in linux. Importing tf only inside worker fixed it.\r\n_Python 3.6.9, tensorflow 2.2.0_"]}, {"number": 8219, "title": "Make binary ops delegate to rhs when tensor conversion fails", "body": "Fixes #8051.\r\n\r\nThe goal is to allow users to choose whether `tensor + something` should call `tensor`'s `__add__()` method or `something`'s `__radd__()` method.\r\n\r\nExample usage:\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> tensor = tf.constant(3.0)\r\n>>> class MagicMatrix(object):\r\n...   def __radd__(self, lhs):\r\n...     print(\"MagicMatrix.__radd__\", lhs)\r\n... \r\n>>> tensor + MagicMatrix()\r\nMagicMatrix.__radd__ Tensor(\"Const:0\", shape=(), dtype=float32)\r\n```\r\n\r\nIf the RHS's type does not define the reverse method (in this example `__radd__()`) then the usual behavior applies:\r\n\r\n```\r\n>>> class MagicMatrix(object):\r\n...   pass\r\n... \r\n>>> tensor + MagicMatrix()\r\nTraceback (most recent call last):\r\n[...]\r\nTypeError: Expected float32, got <__main__.MagicMatrix object at 0x7f14e32aa978> of type 'MagicMatrix' instead.\r\n```\r\n", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "@tensorflow-jenkins TEst this please", "Jenkins, test this please.", "@ebrevdo @aselle ping for a review?", "@ebrevdo @aselle any chance to look at this?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please."]}, {"number": 8218, "title": "Use .bazelrc to configure optional dependencies", "body": "Rather than having ./configure mutate one of our .bzl files, which dirties the git repository, this change has ./configure put --define options in .bazelrc for jemalloc, GCS, HDFS, and XLA inside a .bazelrc at the root of the TensorFlow repository. This file is listed in .gitignore.\r\n\r\nTherefore, running ./configure will no longer cause the git repository to be in a modified state.\r\n\r\nFixes #8202 \r\nCC: @girving ", "comments": ["I like how the change looks. I will wait on the approval until Damien and Martin takes a look, and raise any concerns if there is any.", "Shiny! The only thing left now is python config, right?", "After re-reading the change: all those define will have to go in an external repository to make the import TF as a workspace works nicely, unfortunately there is no vardef so you probably wants to write a bzl file with those variable defined with an external repository (either as a follow-up or as part of this change).", "@jart Do you want to merge this, or add in the change damien suggested?\r\n@tensorflow-jenkins Test this please", "Jenkins, test this please", "@dandelionmane I wrote this change under the assumption that dependent projects might choose to write their own ./configure scripts, that define the .bazelrc in their root directory.\r\n\r\n@damienmg How robust can we make Bazel's `.bazelrc` discovery? Assume we have three .bazelrc files:\r\n\r\n- ~/.bazelrc\r\n- ~/tf_serving/.bazelrc\r\n- ~/tf_serving/tensorflow/.bazelrc\r\n\r\nIf we build something inside ~/tf_serving/tensorflow/... then should all three bazelrc's be loaded? And if so, having the latter ones take priority?", "~/.bazelrc will be loaded when building tf_serving or tensorflow, .bazelrc\nfrom tf_serving can import the .bazelrc from tensorflow it should just work.\n\nThere is no way to load ~/tf_serving/.bazelrc when building from\n~/tf_serving/tensorflow.\n\nAs said previously, to make that use case works nicely, it would be better\nto write a remote repository that write those configuration in a BUILD file\n/ skylark file.\n\nOn Wed, Mar 15, 2017 at 1:59 AM Justine Tunney <notifications@github.com>\nwrote:\n\n@dandelionmane <https://github.com/dandelionmane> I wrote this change under\nthe assumption that dependent projects might choose to write their own\n./configure scripts, that define the .bazelrc in their root directory.\n\n@damienmg <https://github.com/damienmg> How robust can we make Bazel's\n.bazelrc discovery? Assume we have three .bazelrc files:\n\n   - ~/.bazelrc\n   - ~/tf_serving/.bazelrc\n   - ~/tf_serving/tensorflow/.bazelrc\n\nIf we build something inside ~/tf_serving/tensorflow/... then should all\nthree bazelrc's be loaded? And if so, having the latter ones take priority?\n\n\u2014\nYou are receiving this because you were mentioned.\n\n\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/8218#issuecomment-286609051>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/ADjHf09eu1qPa6K42rshW1-69uXidcwIks5rlzgHgaJpZM4MXWqz>\n.\n"]}, {"number": 8217, "title": "Windows loadlibrary", "body": "windows:\r\n- support for tf.load_library()\r\n- support for linking against tensorflow.dll to access c/c++ api\r\n- switched contrib/rnn to use .so's\r\n- enabled all tests in contrib/rnn\r\n", "comments": ["Can one of the admins verify this patch?", "this should enable https://github.com/tensorflow/models/issues/1103", "@guschmue am I assuming that this PR might also address the gap seemingly underlying #6184, i.e. see specifically [this issue comment](https://github.com/tensorflow/tensorflow/issues/6184#issuecomment-267374882)\r\n\r\nOr am I overinterpreting here? Thx to all for the hard work on TensorFlow!", "For now I added only the .so's in contrib/rnn to keep this pr reasonable small (fyi @mrry).\r\nBut now it should be real easy to add a .so for windows  and we can make a pass over what should be included.", "TF_EXPORT sounds good!\n\nOn Mar 8, 2017 4:40 PM, \"guschmue\" <notifications@github.com> wrote:\n\n> *@guschmue* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/core/framework/types.h\n> <https://github.com/tensorflow/tensorflow/pull/8217#discussion_r105061992>\n> :\n>\n> > @@ -68,9 +68,17 @@ class DeviceType {\n>  std::ostream& operator<<(std::ostream& os, const DeviceType& d);\n>\n>  // Convenient constants that can be passed to a DeviceType constructor\n> -extern const char* const DEVICE_CPU;   // \"CPU\"\n> -extern const char* const DEVICE_GPU;   // \"GPU\"\n> -extern const char* const DEVICE_SYCL;  // \"SYCL\"\n> +#if !defined(TF_EXTERN)\n>\n> Like it. Want to call it TF_DLLEXPORT or TF_EXPORT ? Later might be nicer\n> if you want to control visibility on linux.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8217#discussion_r105061992>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbcUFjNUYjirlXtZ97Q_-jLhrPdsgks5rj0pygaJpZM4MXVGg>\n> .\n>\n", "Jenkins, test this please.", "@tensorflow-jenkins test this please.", "soory, just saw that there was a ')' missing for the Linux path", "Jenkins, test this please.", "@tensorflow-jenkins test this please.", "It looks like the `check_futures_test` is unhappy because the new Python script doesn't have the appropriate copyright and `from future import ...` lines at the top of the file. Can you add the following copyright comment above the module comment:\r\n\r\n```python\r\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n```\r\n\r\n...and the following imports after the module comment:\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n```", "Jenkins, test this please.", "just added copyright and futures", "Jenkins, test this please.", "Yay!", "Super. As always you guys are a pleasure to work with.", "Hi, I'm currently using Tensorflow version 1.0.0 on Windows 10, but I'm still having this problem: that is, the libraries are not being loaded in Windows...This causes an error when trying to use crossed_columns or bucketize_columns for wide and deep learning. Can you please help me with this?", "This fix is the master but not in the release branch yet (there was no release since the fix).\r\nYou could build from source or fetch a master build from ci.\r\n(I think its this one http://ci.tensorflow.org/view/Nightly/job/nightly-win/122/DEVICE=cpu,OS=windows/)", "@techscientist and @guschmue I just pulled down the last stable nightly build for Windows/GPU (114) (the later builds up to 122 have failed for GPU) and the Windows/CPU build (122). I can confirm that, the original issue underlying #6184 still persists.\r\n\r\nBy implication `feature_column`, `crossed_column` etc. still fail on Windows. I will update the referenced issue with  a new comment. Would be great, if this could be addressed in a follow-up to this PR as it seems fundamental.", "ok, I can take a look at this.", "@guschmue Thx a lot. FYI I pinged @mrry on my recent update to #6184 , as I saw his handle mentioned in the diagnosis and a TODO in the code."]}, {"number": 8216, "title": "Branch 149567254", "body": "", "comments": ["@tensorflow-jenkins Test this please", "@tensorflow-jenkins test this please"]}, {"number": 8215, "title": "Make configure not create -e files on Mac", "body": "There is no cross platform way to use sed -i without backup files that works in both GNU sed and Darwin sed. So a helper function has been added to the configure script.\r\n\r\nCloses #7978\r\nSee #8202\r\n\r\nCC: @Lewuathe", "comments": []}, {"number": 8214, "title": "[WIP] Added support for erfcinv.", "body": "As requested in #8089. My machine is having errors running the tests, so this isn't tested; I would be very grateful if one of the maintainers can run the tests in Travis and I'll fix the results.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Still working on this- sorry. Should have more commits soon!", "Has the `erfinv` op been implemented in Tensorflow anywhere? I can't seem to find it, so I'm planning on implementing my own version (following this Stack Overflow [answer](https://stackoverflow.com/questions/42381244/pure-python-inverse-error-function)).", "I added [WIP] to the description. Remove that once it's ready to be looked at. Thanks!", "Closing to keep the set of open PRs manageable. Feel free to reopen once ready."]}, {"number": 8213, "title": "Update gmock from 1.7.0 to 1.8.0", "body": "Update gmock from 1.7.0 to 1.8.0", "comments": ["Can one of the admins verify this patch?", "Why does the BUILD file change so much?", "@tensorflow-jenkins Test this please", "@dandelionmane Re BUILD: the packaging changed slightly.\r\nWhat used to be \"./\" and \"gmock\" became \"googletest\" and \"googlemock\"", "@gunan  thank you for a quick response! Added the mirror link (also fixed the tab issue). PTAL", "@tensorflow-jenkins Test this please", "Jenkins, test this please."]}, {"number": 8212, "title": "Update installation documentation for v1.0.1.", "body": "", "comments": ["Looks good."]}, {"number": 8211, "title": "[feature] Support Building for iOS Using Bazel", "body": "Would be great to be able to build for ios using Bazel rather than make. This would allow more rapid development of ops that can only run on iOS (eg: [Metal Performance Shaders](https://github.com/tensorflow/tensorflow/issues/7958)).\r\n\r\nCarry over from: https://github.com/tensorflow/tensorflow/issues/5360#issuecomment-283557890\r\n\r\nIt looks like there might be some progress here: https://github.com/tensorflow/tensorflow/commit/78c9dec5a62e74389608c709027fb8eabdf2bef0  ?\r\n/CC @petewarden @aselle", "comments": ["This is still an open issue, we'd like to see this too, but no recent progress has been made.", "Any progress here ?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "There's been no progress, and we're more focused on TF Lite for the mobile side, so I don't think we're likely to see this happen soon. Closing so that the status is clearer."]}, {"number": 8210, "title": "Fixing built jar location in readme", "body": "JAR location in readme is incorrect.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@jart ptal", "@tensorflow-jenkins test this please\r\nthen i will merge it either way since it just a readme change...."]}]