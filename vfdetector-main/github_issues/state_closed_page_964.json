[{"number": 24506, "title": "failed to use toco to convert quantized pb file to tflite file", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  NO\r\n- TensorFlow installed from (source or binary):  pip install tensorflow-gpu==1.9\r\n- TensorFlow version (use command below):  1.9 gpu\r\n- Python version:\r\n- Bazel version (if compiling from source):   NO\r\n- GCC/Compiler version (if compiling from source):  gcc  5.4.0 \r\n- CUDA/cuDNN version: cuda 9.0 cudnn 7.1\r\n- GPU model and memory: gtx 1070\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n       I used tf.contrib.quantize.create_eval_graph() to quantized trainning a cnn network,  , and used \r\nfreeze_graph.py to generate frozen pb file  successfully\r\n      but when I used toco to convert the pb to tflite  ,,error occured    shown below \r\n\r\n      Array Slice, which is an input to the Conv operator producing the output array conv1/Relu, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results\r\n\r\nIt seems that the toco can not support tf.slice op ???\r\n\r\n**Code to reproduce the issue**\r\n\r\nmy network is  shown below  \r\n\r\ndef model(x, y, is_training, is_quantize):\r\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n                        activation_fn=tf.nn.relu,\r\n                        biases_initializer = tf.constant_initializer(0.0),\t\t\t\t\t\t\r\n                        normalizer_fn=slim.batch_norm,\r\n                        normalizer_params={'is_training': is_training, 'epsilon':1e-5, 'scale': True, 'updates_collections': tf.GraphKeys.UPDATE_OPS}):\r\n        print('x',x.get_shape())\t\t\r\n        data_slice  = tf.slice(x, [ 0, 0, 0, 0], [ train_batch, 64, 60, 3])\r\n        conv1 = slim.conv2d(data_slice,  16,    [3, 3], scope='conv1')\r\n\r\n**Other info / logs**\r\n$ toco  --output_file=quant.tflite --graph_def_file=quant.pb --input_arrays=input  --output_arrays=fc2/act_quant/FakeQuantWithMinMaxVars  --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --mean_values=0 --std_dev_values=1\r\n2018-12-21 20:50:10.927248: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-12-21 20:50:11.011200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-12-21 20:50:11.011620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.77GiB\r\n2018-12-21 20:50:11.011633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-12-21 20:50:11.194928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-12-21 20:50:11.194963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 \r\n2018-12-21 20:50:11.194970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N \r\n2018-12-21 20:50:11.195190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7498 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/home/icare/.local/bin/toco\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 320, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 316, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 121, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 309, in convert\r\n    allow_custom_ops=self.allow_custom_ops)\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 225, in toco_convert\r\n    input_data.SerializeToString())\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 107, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\n2018-12-21 20:50:12.108082: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 113 operators, 173 arrays (0 quantized)\r\n2018-12-21 20:50:12.108759: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 113 operators, 173 arrays (0 quantized)\r\n2018-12-21 20:50:12.112645: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 20 operators, 37 arrays (1 quantized)\r\n2018-12-21 20:50:12.112836: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 19 operators, 35 arrays (1 quantized)\r\n2018-12-21 20:50:12.112978: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 18 operators, 33 arrays (1 quantized)\r\n2018-12-21 20:50:12.113106: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 18 operators, 33 arrays (1 quantized)\r\n2018-12-21 20:50:12.113179: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 12 operators, 27 arrays (1 quantized)\r\n2018-12-21 20:50:12.113252: F tensorflow/contrib/lite/toco/tooling_util.cc:1589] Array Slice, which is an input to the Conv operator producing the output array conv1/Relu, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\nAborted (core dumped)\r\n\r\nNone\r\n\r\n\r\n\r\n\r\nwhat should I do to solve the problem ???\r\n\r\n", "comments": ["I see that you are using lower version of TF, likely a reason for TOCO converter failure. Can you please test it with TF-nightly version?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24505, "title": "Is there any plan to integrate NCCL 2.0 or horovod into CollectiveAllReduceStrategy?", "body": "**System information**\r\n- TensorFlow version (you are using): master\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`Horovod` brings significant performance to distribution execution especially when using `NCCL` to do `all_reduce`. Is there any plan to do integration  or simply support NCCL 2.0 in `CollectiveAllReduceStrategy`? I am willing to contribute for it!\r\n\r\n**Will this change the current api? How?**\r\nNo\uff0cit will be another `cross_device_ops` for candidate.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to use `CollectiveAllReduceStrategy` to improve performance.\r\n\r\n\r\n**Any Other info.**\r\nCurrently I am working for distribution optimization and I hope to have a discussion. Thanks.\r\n\r\n@yuefengz \r\n", "comments": ["Thank Siyu for submitting this issue.\r\n\r\n@yuefengz  We are willing to contribute to Distribution Strategy to make it greater and brings more benefits to end-users. Currently, we found Horovod is a nice backend to be used to support CollectiveAllreduceStrategy and we are willing to work on it. Any suggestions or feedback from your side are highly welcome.\r\n\r\nThanks", "I think @dubey is working on this, and you could take a look at 681f6a6ac9344425549c8a38fcbfae854e7deddf. \r\n\r\nIt seems to be rolled back in current master though. I expect it to come back later on.", "@yangjunpro Sorry I missed this thread from my inbox somehow. @dubey is working on the nccl implementation, we hope to have it soon. It bypasses grpc and should give `CollectiveAllReduceStrategy` speed-up. \r\n\r\nIf you are interested in contributing, it is very welcomed. We can discuss what you want to add to Distribution Strategy and we can work on a RFC together.", "@yuefengz @byronyi Thanks for reply ! I will take a look at this commit soon and then have a discussion with you.", "@yuefengz Thanks for the sharing.  As to adding nccl support for CollectiveAllReduceStrategy, I think it will surely bring performance benefit and we also have made some internal benchmark. In addition to nccl, I think there are also some other high-level optimization based on nccl which is already implemented in Horovod, such as Tensor Fusion, Hierarchy Allreduce etc. And we have already started working on integrating Horovod as a backbone for CollectiveAllreduceStrategy. \r\nI do think we can start a discussion as to how to collaborate to make CollectiveAllReduceStrategy more convenient and faster.\r\n\r\nThanks", "Tensor fusion is implemented with `ScopedAllocator` in 380a032e64222db73766ef18ea51130e3df4c15f, and hierarchical all-reduce is implemented with hierarchical `Broadcaster` in d4bb99250a2f8d15985823baa5ccfbc55998f7bb. Those are with native TF collective ops and may not related to NCCL though. @dubey might have some ideas.", "> Tensor fusion is implemented with `ScopedAllocator` in [380a032](https://github.com/tensorflow/tensorflow/commit/380a032e64222db73766ef18ea51130e3df4c15f), and hierarchical all-reduce is implemented with hierarchical `Broadcaster` in [d4bb992](https://github.com/tensorflow/tensorflow/commit/d4bb99250a2f8d15985823baa5ccfbc55998f7bb). Those are with native TF collective ops and may not related to NCCL though. @dubey might have some ideas.\r\n\r\n@byronyi \r\n\r\nThanks for the sharing. I have gone through the ScopedAllocator implementation and it looks that it currently plays as a bridge between a large tensor and its corresponding smaller slices.  To a certain degree it can be viewed as \"Tensor Fusion\". However from distributed optimization perspective, it may not be complete, since during the execution of distribution communication flow, it may not be regarded as a fixed \"fusion pattern\" in which tensors should be fused together. Background workloads, currently data pattern may both impact the Tensor fusion behavior.  \r\nAlso is there any place in TF community's branch code already using the ScopedAllocator? I would like to take a look at its concrete use cases.\r\n\r\nThanks.", "You could take a look at 0b522fd22b986704d1056254961cc7988ae182eb, enabled by 282750fee5e2df502436ca9ef6a95283f8adab34 and 09e529ff5adb916e40481563698dee72e8a15162. It is enabled by [this toggle](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/core/protobuf/rewriter_config.proto#L77).", "> You could take a look at [0b522fd](https://github.com/tensorflow/tensorflow/commit/0b522fd22b986704d1056254961cc7988ae182eb), enabled by [282750f](https://github.com/tensorflow/tensorflow/commit/282750fee5e2df502436ca9ef6a95283f8adab34) and [09e529f](https://github.com/tensorflow/tensorflow/commit/09e529ff5adb916e40481563698dee72e8a15162). It is enabled by [this toggle](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/core/protobuf/rewriter_config.proto#L77).\r\n\r\nThanks for sharing the details. I will take a look at them.", "Thanks for the quick responses, @byronyi!  As Yuefeng said, we welcome contributions and I'd love to hear more about your plans.  Please find my thoughts below on the discussion until now.\r\n\r\n- After [`f6b81f4`](https://github.com/tensorflow/tensorflow/commit/f6b81f458da1f7b68e06c6a4501c5c380d33d590), the key pieces for enabling NCCL with CollectiveAllReduceStrategy are now in TensorFlow.  We're running some final tests, and we will soon provide a way for you to test this out.  It would be great if you can try it out.\r\n\r\n- The goal of `ScopedAllocator` is similar to Tensor Fusion: convert many small all reduces into fewer, larger all reduces.  `ScopedAllocator` is already a part of `CollectiveOps` implementation, so those benefits should carry over to the NCCL implementation as well.\r\n\r\n- We do indeed have a hierarchical broadcast implementation, and it would be possible to build something that combines the ideas in [`d4bb992`](https://github.com/tensorflow/tensorflow/commit/d4bb99250a2f8d15985823baa5ccfbc55998f7bb) and [`f6b81f4`](https://github.com/tensorflow/tensorflow/commit/f6b81f458da1f7b68e06c6a4501c5c380d33d590) to get a hierarchical NCCL-based all reduce implementation.  I've thought about this in the past but haven't gotten around to doing it yet.\r\n\r\n> However from distributed optimization perspective, it may not be complete, since during the execution of distribution communication flow, it may not be regarded as a fixed \"fusion pattern\" in which tensors should be fused together. Background workloads, currently data pattern may both impact the Tensor fusion behavior.\r\n\r\nI'd like to understand more what you mean by this.  Can you elaborate, or perhaps provide an example?  \r\n", "> Thanks for the quick responses, @byronyi! As Yuefeng said, we welcome contributions and I'd love to hear more about your plans. Please find my thoughts below on the discussion until now.\r\n> \r\n> * After [`f6b81f4`](https://github.com/tensorflow/tensorflow/commit/f6b81f458da1f7b68e06c6a4501c5c380d33d590), the key pieces for enabling NCCL with CollectiveAllReduceStrategy are now in TensorFlow.  We're running some final tests, and we will soon provide a way for you to test this out.  It would be great if you can try it out.\r\n> * The goal of `ScopedAllocator` is similar to Tensor Fusion: convert many small all reduces into fewer, larger all reduces.  `ScopedAllocator` is already a part of `CollectiveOps` implementation, so those benefits should carry over to the NCCL implementation as well.\r\n> * We do indeed have a hierarchical broadcast implementation, and it would be possible to build something that combines the ideas in [`d4bb992`](https://github.com/tensorflow/tensorflow/commit/d4bb99250a2f8d15985823baa5ccfbc55998f7bb) and [`f6b81f4`](https://github.com/tensorflow/tensorflow/commit/f6b81f458da1f7b68e06c6a4501c5c380d33d590) to get a hierarchical NCCL-based all reduce implementation.  I've thought about this in the past but haven't gotten around to doing it yet.\r\n> \r\n> > However from distributed optimization perspective, it may not be complete, since during the execution of distribution communication flow, it may not be regarded as a fixed \"fusion pattern\" in which tensors should be fused together. Background workloads, currently data pattern may both impact the Tensor fusion behavior.\r\n> \r\n> I'd like to understand more what you mean by this. Can you elaborate, or perhaps provide an example?\r\n\r\nI think his mean that the tensor fusion buffer size cannot be auto tuned online in every training loop. Is that correct ? @yangjunpro ", "> > Thanks for the quick responses, @byronyi! As Yuefeng said, we welcome contributions and I'd love to hear more about your plans. Please find my thoughts below on the discussion until now.\r\n> > \r\n> > * After [`f6b81f4`](https://github.com/tensorflow/tensorflow/commit/f6b81f458da1f7b68e06c6a4501c5c380d33d590), the key pieces for enabling NCCL with CollectiveAllReduceStrategy are now in TensorFlow.  We're running some final tests, and we will soon provide a way for you to test this out.  It would be great if you can try it out.\r\n> > * The goal of `ScopedAllocator` is similar to Tensor Fusion: convert many small all reduces into fewer, larger all reduces.  `ScopedAllocator` is already a part of `CollectiveOps` implementation, so those benefits should carry over to the NCCL implementation as well.\r\n> > * We do indeed have a hierarchical broadcast implementation, and it would be possible to build something that combines the ideas in [`d4bb992`](https://github.com/tensorflow/tensorflow/commit/d4bb99250a2f8d15985823baa5ccfbc55998f7bb) and [`f6b81f4`](https://github.com/tensorflow/tensorflow/commit/f6b81f458da1f7b68e06c6a4501c5c380d33d590) to get a hierarchical NCCL-based all reduce implementation.  I've thought about this in the past but haven't gotten around to doing it yet.\r\n> > \r\n> > > However from distributed optimization perspective, it may not be complete, since during the execution of distribution communication flow, it may not be regarded as a fixed \"fusion pattern\" in which tensors should be fused together. Background workloads, currently data pattern may both impact the Tensor fusion behavior.\r\n> > \r\n> > \r\n> > I'd like to understand more what you mean by this. Can you elaborate, or perhaps provide an example?\r\n> \r\n> I think his mean that the tensor fusion buffer size cannot be auto tuned online in every training loop. Is that correct ? @yangjunpro\r\n\r\nYep, this is what I mean. For different training iteration, due to the background workload or other production environment variation, the best tensor fusion buffer size may also change dynamically.", "@yangjunpro That would be very interesting. Do you have any further reference/experiment to illustrate the potential gain? AFAIK, one could use `contrib/all_reduce` for prototyping when tuning the best merge/split strategy, but you may have different choices.\r\n\r\nSee https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/allreduce.py#L448 as an example.", "You can now use `tf.distribute.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL`)` to try out our nccl implementation. Nccl is faster on some networks than our previous implementation but allocates extra memory buffers.", "Horovod's TensorFusion is a dynamic technique: During execution nodes report on the tensor reductions that are ready to go, then a master determines the next reduction set based on an intersection of those reported ready.  I assume there is next a compacting data copy at each node before the collective op begins (which is an opportunity for compaction or precision reduction).  All of this requires some out of band communication to a master which potentially introduces extra latency or high in-cast at large scale, plus there's also extra latency for data copies.  Depending on the computation it may be possible to overlap much of this latency with other useful work.\r\n\r\nScopedAllocator is a static technique: upstream operations compute into adjacent regions of a pre-allocated tensor.  The collective op can begin as soon as the final inbound edge completes, there is no out-of-band communication necessary.  As you point out, it's not clear that static analysis can find the best strategy, or that the best strategy is consistent from step to step.\r\n\r\nIt seems an empirical question which technique will work best for a particular graph in a particular distribution.  The TF team is interested in ways of improving the application of ScopedAllocator, perhaps by periodically examining runtime performance to modify the merge strategy.\r\n\r\n", "@poxvoculi So is there any plan to do the pre-analysis based on cost model in static tensor fusion technique or other tuning method?", "@wangsiyu I've had some casual conversations with colleagues about the feasibility of doing something like that, but there's no concrete plan to do anything yet.  I think it would be interesting to do some detailed performance comparisons of the dynamic Horovod approach with the static ScopedAllocator approach and see where each looks stronger.  That might give some insight into the most worthwhile next project in this area.", "@poxvoculi \r\n\r\nThanks for the feedback.\r\n\r\n@yuefengz \r\n\r\nInternally we have already integrated Horovod(we have gotten rid of MPI to ensure as less extra dependencies to be introduced as possible ) into CollectiveAllReduceStrategy and benchmarked with some of our in-house models. The performance also looks promising. For a Transformer NLP model, the rough number is as following: for 2node 16 GPUs, around 14X speed-up against single GPU version; for 3node 24 GPUs, around 18X speed-up, with only 10Gbps ethernet device. Further performance improvement is expected if better hardware is utilized, such as RDMA or IB, etc. \r\n\r\nWe are considering whether to make this Horovod integration a separate PR into distribution strategy. Any feedback from your folks are highly welcome. \r\n\r\nThanks.", "@yangjunpro We are having this discussion on collective communication and networking API in general within the networking SIG. If you are interested, feel free to send us a design draft to networking@tensorflow.org. We are also having a conference call this Tuesday night (11pm GMT+8), and it is a great place to escalate the discussion.", "@byronyi \r\n\r\nSure, we will join the conference this time.\r\n\r\nIs there any dial-in number or Hangout invitations?\r\n\r\nThanks", "You should be able to see the invitation once you add yourself to https://groups.google.com/a/tensorflow.org/forum/m/?pli=1#!forum/networking", "Closing issue; NCCL 2.0 is now integrated into MultiWorkerMirroredStrategy."]}, {"number": 24504, "title": "Add parameter for exporting TensorFlow models to override existed files", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): TensorFlow 1.8.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nNow we have `saved_model.simple_save()` and `saved_model.builder` APIs to export the TensorFlow models. However, there is no parameter to override the model files if we want to do that. Using extra HDFS APIs to delete remote files requires more dependencies for user's Python scripts. And we can do that with the TensorFlow FileSystem APIs.\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes. Add the new parameter for `saved_model.simple_save()` function. It could not override the files by default so it may be compatible with the older versions.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThe TensorFlow end users.\r\n\r\n**Any Other info.**\r\n\r\nNo.", "comments": ["Here is the source code of Python API to export model if we want to support this, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/builder_impl.py .", "Reassigning to Allen since he works on saving models.", "I'd rather not add an option. The 2.x SavedModel APIs will overwrite without complaining. We can have that behavior by default in the 1.x APIs if someone wants to implement it, as it's a backward compatible change."]}, {"number": 24503, "title": "Support overriding model files when exporting model with extra parameter ", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): TensorFlow 1.8.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nNow we have `saved_model.simple_save()` and `saved_model.builder` APIs to export the TensorFlow models. However, there is no parameter to override the model files if we want to do that. Using extra HDFS APIs to delete remote files requires more dependencies for user's Python scripts. And we can do that with the TensorFlow FileSystem APIs.\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes. Add the new parameter for `saved_model.simple_save()` function. It could not override the files by default so it may be compatible with the older versions.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThe TensorFlow end users.\r\n\r\n**Any Other info.**\r\n\r\nNo.", "comments": ["Thanks, @tobegit3hub , for the request. I am having trouble understanding exactly what you want, though. Can you clarify what the \"model files\" are here? Do you mean the checkpoint? Why do you want to override them?", "Thanks @karmel for the reply. The \"model files\" means the SavedModel files. We may export the SavedModel multiple times with the same model version and only keep the latest one. Now we get exception if we try to export the SavedModel with the same model path.", "Simple save is designed to be explicitly simple, and I do not think we should add more arguments to that function. Closing this for now. See also new, 2.0 saved model APIs: https://github.com/tensorflow/community/pull/34"]}, {"number": 24502, "title": "copy_graph.copy_variable_to_graph fix shape", "body": "This pull request fixes #24442\r\nAdd flag `validate_shape=True` to the Variable initializer. In this way the resulting variable has the same shape as the original variable.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Signed CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@EmanueleGhelfi gentle ping", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 24501, "title": "lite: tiny typo-like update to ResizeBilinear opt op", "body": "", "comments": ["@jackwish Thanks for the PR , Here we can process only merge comit to tensorflow:master , please update it .", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "oops, pinged too many people, sorry... will open another.", "> @jackwish Thanks for the PR , Here we can process only merge comit to tensorflow:master , please update it .\r\n\r\n@rthadur check #24514 instead please."]}, {"number": 24500, "title": "while install tensorflow 1.12 with cudnn 9  GETTING ERROR AS DLL:SPECIFIED PROCEDURE COUDN'T FIND", "body": "TENSORFLOW ISSUE GETTNG SPECIFIED PROCEDURE COUDN'T FIND \r\nWHIL COMUTER IS HAVING\r\n WINDOWS 7 OS , \r\nGRAPHIC IS NVIDIA GEFORCE 7025 \r\nINSTALLED TENSORFLOW 1.12 WITH CUDNN 9\r\nIN ANACONDA 1.9.6 LATEST 2018 NOV VERSION\r\n\r\nGET ME HELP ..... \r\n(SRI) C:\\Users\\SRIKANTH>python -c \"import tensorflow\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\imp.py\", line 243, in load_modu\r\nle\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\imp.py\", line 343, in load_dyna\r\nmic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\__init\r\n__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\imp.py\", line 243, in load_modu\r\nle\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\imp.py\", line 343, in load_dyna\r\nmic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["Have you installed cuDNN7 correctly?", "thanks @rootkitchao  for replying \r\nyes installed cudnn7 with cuda 9 through anaconda \r\ninstalled cudnn7. with cuda 9.0 \r\nwhen i reinstall only tensorflow version i can import it \r\nbut with tenosrflow -gpu version its getting error AS DLL : THE SPECIFIED PROCEDURE COUDN'T FOUND\r\nmy system is windows 7  , gpu version is nvidia geforce 7025  model \r\n", "> thanks @rootkitchao for replying\r\n> yes installed cudnn7 with cuda 9 through anaconda\r\n> installed cudnn7. with cuda 9.0\r\n> when i reinstall only tensorflow version i can import it\r\n> but with tenosrflow -gpu version its getting error AS DLL : THE SPECIFIED PROCEDURE COUDN'T FOUND\r\n> my system is windows 7 , gpu version is nvidia geforce 7025 model\r\n\r\nI would like to ask \"gpu version is nvidia geforce 7025 model\" means your GPU model is Nvidia Geforce 7025?NVIDIA Geforce 7025 is an integrated graphics processor introduced by NVIDIA in 2007. It is very old.This graphics processor was released even earlier than the original CUDA version.\r\nCurrent version of Tensorflow-GPU requirements NVIDIA GPU card with CUDACompute Capability 3.5 or higher(see https://www.tensorflow.org/install/gpu).Obviously your hardware can't meet the requirements and there is no solution.", "Closing this issue since @rootkitchao is correct."]}, {"number": 24499, "title": "Bazel C++ example link error on Windows 10 with Tensorflow 1.12 CPU", "body": "<em>C++ example build error on Windows 10 with Tensorflow 1.12</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution Windows 10:\r\n- TensorFlow installed from source:\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6\r\n- Installed using conda:\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2015 tools\r\n- CUDA/cuDNN version: without GPU\r\n- GPU model and memory: without GPU\r\n\r\n**I cloned the Tensorflow Git repository and got the latest version from the master branch (commit  [fe84b75] from  Thursday, December 20, 2018 7:37:28 AM. I installed Bazel and I tried to compile the C++ example provided on the web site https://www.tensorflow.org/guide/extend/cc following the instructions. The required libraries from Tensorflow were compiled successfully but the example.cpp has a link error:**\r\n\r\n**C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /OUT:bazel-out/x64_windows-opt/bin/tensorflow/cc/example/example.exe /SUBSYSTEM:CONSOLE -DEFAULTLIB:advapi32.lib /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/cc/example/example.exe-2.params /OPT:ICF /OPT:REF\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/cc/example/example.lib and object bazel-out/x64_windows-opt/bin/tensorflow/cc/example/example.exp\r\npin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nutils.lib(utils.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nbatch_kernels.lo.lib(batch_kernels.obj) : warning LNK4217: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported in function \"void __cdecl tensorflow::`dynamic initializer for 'registrar__body__0__object''(void)\" (??__Eregistrar__body__0__object@tensorflow@@YAXXZ)\r\ncaptured_function.lib(captured_function.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\narithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nmemory_optimizer.lib(memory_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nunicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ??3UMemory@icu_62@@SAXPEAX@Z (public: static void __cdecl icu_62::UMemory::operator delete(void *)) imported in function \"public: virtual void * __cdecl icu_62::StringByteSink<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::`scalar deleting destructor'(unsigned int)\" (??_G?$StringByteSink@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@icu_62@@UEAAPEAXI@Z)\r\nunicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ??0UnicodeStringAppendable@icu_62@@QEAA@AEAVUnicodeString@1@@Z (public: __cdecl icu_62::UnicodeStringAppendable::UnicodeStringAppendable(class icu_62::UnicodeString &)) imported in function \"public: virtual void __cdecl tensorflow::UnicodeEncodeOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeEncodeOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nunicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ??1UnicodeStringAppendable@icu_62@@UEAA@XZ (public: virtual __cdecl icu_62::UnicodeStringAppendable::~UnicodeStringAppendable(void)) imported in function \"public: virtual void __cdecl tensorflow::UnicodeEncodeOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeEncodeOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nunicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ?appendCodePoint@UnicodeStringAppendable@icu_62@@UEAACH@Z (public: virtual signed char __cdecl icu_62::UnicodeStringAppendable::appendCodePoint(int)) imported in function \"public: virtual void __cdecl tensorflow::UnicodeEncodeOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeEncodeOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nunicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ??1ByteSink@icu_62@@UEAA@XZ (public: virtual __cdecl icu_62::ByteSink::~ByteSink(void)) imported in function \"public: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > & __cdecl icu_62::UnicodeString::toUTF8String<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > &)const \" (??$toUTF8String@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@UnicodeString@icu_62@@QEBAAEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEAV23@@Z)\r\nunicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ?toUTF8@UnicodeString@icu_62@@QEBAXAEAVByteSink@2@@Z (public: void __cdecl icu_62::UnicodeString::toUTF8(class icu_62::ByteSink &)const ) imported in function \"public: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > & __cdecl icu_62::UnicodeString::toUTF8String<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > &)const \" (??$toUTF8String@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@UnicodeString@icu_62@@QEBAAEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEAV23@@Z)\r\nunicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ?countChar32@UnicodeString@icu_62@@QEBAHHH@Z (public: int __cdecl icu_62::UnicodeString::countChar32(int,int)const ) imported in function \"void __cdecl tensorflow::`anonymous namespace'::Encode(enum tensorflow::UnicodeEncoding,class icu_62::UnicodeString const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)\" (?Encode@?A0xb9816742@tensorflow@@YAXW4UnicodeEncoding@2@AEBVUnicodeString@icu_62@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\r\nunicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ?append@UnicodeString@icu_62@@QEAAAEAV12@H@Z (public: class icu_62::UnicodeString & __cdecl icu_62::UnicodeString::append(int)) imported in function \"private: void __cdecl tensorflow::UnicodeTranscodeOp::TranslateCodepoints(class icu_62::UnicodeString *,bool *,int,int,bool)\" (?TranslateCodepoints@UnicodeTranscodeOp@tensorflow@@AEAAXPEAVUnicodeString@icu_62@@PEA_NHH_N@Z)\r\nunicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ??0UnicodeString@icu_62@@QEAA@XZ (public: __cdecl icu_62::UnicodeString::UnicodeString(void)) imported in function \"public: virtual void __cdecl tensorflow::UnicodeEncodeOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeEncodeOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nunicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ??1UnicodeString@icu_62@@UEAA@XZ (public: virtual __cdecl icu_62::UnicodeString::~UnicodeString(void)) imported in function \"public: virtual void __cdecl tensorflow::UnicodeEncodeOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeEncodeOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\narithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4217: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported in function \"private: bool __cdecl tensorflow::grappler::`anonymous namespace'::ReorderCastLikeAndValuePreserving::NodeIsOnCpuOrGpu(class tensorflow::NodeDef const *)const \" (?NodeIsOnCpuOrGpu@ReorderCastLikeAndValuePreserving@?A0x956ba610@grappler@tensorflow@@AEBA_NPEBVNodeDef@4@@Z)\r\nlayout_optimizer.lib(layout_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\nmemory_optimizer.lib(memory_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\npin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\nunicode_ops.lo.lib(unicode_ops.obj) : error LNK2019: unresolved external symbol \"__declspec(dllimport) public: virtual int __cdecl icu_62::UCharCharacterIterator::next32PostInc(void)\" (__imp_?next32PostInc@UCharCharacterIterator@icu_62@@UEAAHXZ) referenced in function \"void __cdecl tensorflow::`anonymous namespace'::Encode(enum tensorflow::UnicodeEncoding,class icu_62::UnicodeString const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)\" (?Encode@?A0xb9816742@tensorflow@@YAXW4UnicodeEncoding@2@AEBVUnicodeString@icu_62@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\r\nunicode_ops.lo.lib(unicode_ops.obj) : error LNK2019: unresolved external symbol \"__declspec(dllimport) public: virtual signed char __cdecl icu_62::UCharCharacterIterator::hasNext(void)\" (__imp_?hasNext@UCharCharacterIterator@icu_62@@UEAACXZ) referenced in function \"void __cdecl tensorflow::`anonymous namespace'::Encode(enum tensorflow::UnicodeEncoding,class icu_62::UnicodeString const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)\" (?Encode@?A0xb9816742@tensorflow@@YAXW4UnicodeEncoding@2@AEBVUnicodeString@icu_62@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\r\nunicode_ops.lo.lib(unicode_ops.obj) : error LNK2019: unresolved external symbol \"__declspec(dllimport) public: __cdecl icu_62::StringCharacterIterator::StringCharacterIterator(class icu_62::UnicodeString const &)\" (__imp_??0StringCharacterIterator@icu_62@@QEAA@AEBVUnicodeString@1@@Z) referenced in function \"void __cdecl tensorflow::`anonymous namespace'::Encode(enum tensorflow::UnicodeEncoding,class icu_62::UnicodeString const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)\" (?Encode@?A0xb9816742@tensorflow@@YAXW4UnicodeEncoding@2@AEBVUnicodeString@icu_62@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\r\nunicode_ops.lo.lib(unicode_ops.obj) : error LNK2019: unresolved external symbol \"__declspec(dllimport) public: virtual __cdecl icu_62::StringCharacterIterator::~StringCharacterIterator(void)\" (__imp_??1StringCharacterIterator@icu_62@@UEAA@XZ) referenced in function \"void __cdecl tensorflow::`anonymous namespace'::Encode(enum tensorflow::UnicodeEncoding,class icu_62::UnicodeString const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)\" (?Encode@?A0xb9816742@tensorflow@@YAXW4UnicodeEncoding@2@AEBVUnicodeString@icu_62@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\r\nunicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol \"__declspec(dllimport) public: virtual __cdecl icu_62::ErrorCode::~ErrorCode(void)\" (__imp_??1ErrorCode@icu_62@@UEAA@XZ) referenced in function \"public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nunicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol \"__declspec(dllimport) public: signed char __cdecl icu_62::ErrorCode::isSuccess(void)const \" (__imp_?isSuccess@ErrorCode@icu_62@@QEBACXZ) referenced in function \"public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nunicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol \"__declspec(dllimport) public: enum UErrorCode __cdecl icu_62::ErrorCode::reset(void)\" (__imp_?reset@ErrorCode@icu_62@@QEAA?AW4UErrorCode@@XZ) referenced in function \"public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nunicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol \"__declspec(dllimport) const icu_62::ErrorCode::`vftable'\" (__imp_??_7ErrorCode@icu_62@@6B@) referenced in function \"public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nicuuc.lib(udata.obj) : error LNK2019: unresolved external symbol __imp_icudt62_dat referenced in function \"struct UDataMemory * __cdecl openCommonData(char const *,int,enum UErrorCode *)\" (?openCommonData@@YAPEAUUDataMemory@@PEBDHPEAW4UErrorCode@@@Z)\r\nbazel-out/x64_windows-opt/bin/tensorflow/cc/example/example.exe : fatal error LNK1120: 9 unresolved externals\r\nTarget //tensorflow/cc/example:example failed to build\r\nINFO: Elapsed time: 19976.467s, Critical Path: 13898.60s\r\nINFO: 1461 processes: 1461 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n**\r\n\r\nThank you for help!", "comments": ["Can the current version of the Tensorflow C++ API work under Windows?", "For myself, for now, I am content with just not linking the Unicode ops as a workaround.\r\n\r\nhttps://gist.github.com/matth79/224962f167f01cd41ba21039eab51fbc\r\n\r\nThe correct solution is more along these lines:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/23655\r\n", "Thanks,\r\nThe solution from https://gist.github.com/matth79/224962f167f01cd41ba21039eab51fbc worked", "Closing this issue since its resolved. Feel free to reopen if have any further problems. Thanks! ", "I solved the problem by compiling icu4c,adding the path of lib64.", "![image](https://user-images.githubusercontent.com/34018741/59147434-21b2b780-8a2e-11e9-809e-57668e8b70ec.png)\r\n![image](https://user-images.githubusercontent.com/34018741/59147442-355e1e00-8a2e-11e9-87c9-003349bd90ce.png)\r\n", "@gmt710 hi,can you teach me how to build tf on vs?", "@jesen8 hello,I wrote the installation instructions.May you follow this:[windows+bazel+tensorflow-v1.12.0(GPU)](https://blog.csdn.net/qq_35975447/article/details/91986142).", "thanks!,can you give me your wchart id?", "@gmt710 ", "@jesen8 hi,Can you email me, OK?"]}, {"number": 24498, "title": "Make estimator part of pip install", "body": "PiperOrigin-RevId: 226422176", "comments": []}, {"number": 24497, "title": "this graph contains an operator of type SquaredDifference for which the quantized form is not yet implemented", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu14.04\r\n- TensorFlow installed from (source or binary):pip tf-nightly\r\n- TensorFlow version (or github SHA if from source): tf-nightly1.13.0.dev20181216\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nI convert pb to lite use the following code:\r\n`import tensorflow as tf\r\nconverter = tf.contrib.lite.TFLiteConverter.from_frozen_graph('tflite_graph.pb',[\"input_image\"],[\"result\"], input_shapes={\"input_image\":[1,626,360,3]})\r\nconverter.allow_custom_ops = True\r\nconverter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8\r\nconverter.quantized_input_stats = {\"input_image\" : (0., 2.)}\r\nconverter.default_ranges_stats=(0, 6)\r\ntflite_quantized_model=converter.convert()\r\nopen(\"model.tflite\", \"wb\").write(tflite_quantized_model)`\r\n\r\nI get the following error:\r\n2018-12-21 11:26:06.351171: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-12-21 11:26:06.354986: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3392410000 Hz\r\n2018-12-21 11:26:06.355300: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x53e0ee0 executing computations on platform Host. Devices:\r\n2018-12-21 11:26:06.355325: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 25, in <module>\r\n    tflite_quantized_model=converter.convert()\r\n  File \"/home/zhoushaohuang/Virtualenv/python3.4/lib/python3.4/site-packages/tensorflow/lite/python/lite.py\", line 455, in convert\r\n    **converter_kwargs)\r\n  File \"/home/zhoushaohuang/Virtualenv/python3.4/lib/python3.4/site-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/zhoushaohuang/Virtualenv/python3.4/lib/python3.4/site-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2018-12-21 11:26:07.312638: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 168 operators, 271 arrays (0 quantized)\r\n2018-12-21 11:26:07.314127: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 168 operators, 271 arrays (0 quantized)\r\n2018-12-21 11:26:07.323240: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 102 operators, 183 arrays (1 quantized)\r\n2018-12-21 11:26:07.324611: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 96 operators, 171 arrays (1 quantized)\r\n2018-12-21 11:26:07.325812: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 96 operators, 171 arrays (1 quantized)\r\n2018-12-21 11:26:07.326413: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 90 operators, 165 arrays (1 quantized)\r\n2018-12-21 11:26:07.327324: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 90 operators, 165 arrays (1 quantized)\r\n2018-12-21 11:26:07.327972: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 90 operators, 165 arrays (1 quantized)\r\n2018-12-21 11:26:07.328720: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 90 operators, 165 arrays (1 quantized)\r\n2018-12-21 11:26:07.328791: W tensorflow/lite/toco/graph_transformations/quantize.cc:127] Constant array conv1/conv/weight lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.\r\n2018-12-21 11:26:07.328936: F tensorflow/lite/toco/graph_transformations/quantize.cc:491] Unimplemented: this graph contains an operator of type SquaredDifference for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nAborted (core dumped)\r\n\r\n```\r\n\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nI use SquaredDifference in following code:\r\n`def instance_norm(x):\r\n    epsilon = 1e-9\r\n\r\n    mean = tf.reduce_mean(x,[1,2])\r\n    mean = tf.expand_dims(mean,[1])\r\n    mean = tf.expand_dims(mean,[1])\r\n    s = x.get_shape()\r\n    var = tf.reduce_sum(tf.squared_difference(x, mean), [1,2], keep_dims=True)/(s[1].value*s[2].value)\r\n    result = tf.div(tf.subtract(x, mean), tf.sqrt(tf.add(var, epsilon)))\r\n    return result`\r\n![2018-12-21 15 18 10](https://user-images.githubusercontent.com/28701781/50329687-49bb6680-0533-11e9-9672-68c7d451a888.png)\r\n\r\n**Any other info / logs**\r\nI generate pb use the following code:\r\n`input_saver_def = saver.as_saver_def()\r\n frozen_graph_def = freeze_graph.freeze_graph_with_def_protos(input_graph_def=tf.get_default_graph().as_graph_def(),input_saver_def=input_saver_def,input_checkpoint = FLAGS.model_file,output_node_names='result',restore_op_name='save/restore_all', filename_tensor_name='save/Const:0',clear_devices=True,output_graph='',initializer_nodes='')\r\nbinary_graph = 'tflite_graph.pb'\r\nwith tf.gfile.GFile(binary_graph, 'wb') as f:\r\n       f.write(frozen_graph_def.SerializeToString())`\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi, we met SquaredDifference is not supporting operator as well and check from Tensorflow 1.9 to 1.12, all have this problem.\r\nCould you let us know if this is a bug? If yes, could you let us know do you have planning to fix this?\r\nThanks.", "@MrCary, @samuallin  : Will it be possible for you to share sample code to reproduce the issue?\r\nI believe your sample code is incomplete.\r\nOtherwise you can share the model itself.\r\nThanks!", "Marking issue as resolved due to inactivity. Feel free to re-open this if it's unresolved or file a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose)"]}, {"number": 24496, "title": "Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and No (described below)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tf-nightly-gpu (Dec 19, r1.13)\r\n- TensorFlow version (use command below): 1.13.0-dev20181219\r\n- Python version: 3.7.1\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10 with cuDNN 7.4.1\r\n- GPU model and memory: RTX 2070 8GB\r\n\r\n**Describe the current behavior**\r\nI'm running the CNN model on MNIST. When I'm running with the GPU, I am encountering\r\n```2018-12-20 20:09:13.644176: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR```\r\n\r\nI did some digging and realized that it is a memory issue (which shouldn't be the case as I have 32GB of RAM and 64GB of swap. I ran htop when running the model and I have 20+GB free, which is more than enough to fit the 8GB vRAM mappings. \r\n\r\nUsing the `gpu_options.allow_growth = True` gets the model to work properly, and setting `os.environ['CUDA_VISIBLE_DEVICES'] = '-1'` also works. This means that I AM facing a memory issue, but I don't see how. \r\n\r\nAlso, using `gpu_options.allow_growth = True` does not fix the same issue when trying to run tensorflow/models/official/mnist/ model, which should have a similar behavior with my code. \r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nimport math\r\nimport time\r\n# Killing optional CPU driver warnings\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\n\r\n\r\nclass Model:\r\n\r\n    def __init__(self, image, label):\r\n        \"\"\"\r\n        A Model class contains a computational graph that classifies images\r\n        to predictions. Each of its methods builds part of the graph\r\n        on Model initialization. Do not modify the constructor, as doing so\r\n        would break the autograder. You may, however, add class variables\r\n        to use in your graph-building. e.g. learning rate, \r\n\r\n        image: the input image to the computational graph as a tensor\r\n        label: the correct label of an image as a tensor\r\n        prediction: the output prediction of the computational graph,\r\n                    produced by self.forward_pass()\r\n        optimize: the model's optimizing tensor produced by self.optimizer()\r\n        loss: the model's loss produced by computing self.loss_function()\r\n        accuracy: the model's prediction accuracy\r\n        \"\"\"\r\n        self.image = image\r\n        self.label = label\r\n\r\n        # TO-DO: Add any class variables you want to use.\r\n\r\n        self.prediction = self.forward_pass()\r\n        self.loss = self.loss_function()\r\n        self.optimize = self.optimizer()\r\n        self.accuracy = self.accuracy_function()\r\n\r\n    def forward_pass(self):\r\n        \"\"\"\r\n        Predicts a label given an image using convolution layers\r\n\r\n        :return: the prediction as a tensor\r\n        \"\"\"\r\n        filter_1 = tf.Variable(tf.truncated_normal([3, 3, 1, 8], stddev=0.1))\r\n        conv_1 = tf.nn.conv2d(self.image, filter_1, [1, 1, 1, 1], \"SAME\")\r\n\r\n        reshaped = tf.reshape(conv_1, shape=[50, -1])\r\n\r\n        L1 = reshaped.shape[1].value\r\n        L2 = 500\r\n        W1 = tf.Variable(tf.random_normal([L1, L2], mean=0, stddev=0.01))\r\n        b1 = tf.Variable(tf.random_normal([L2], mean=0, stddev=0.01))\r\n        relu_1 = tf.nn.relu(tf.matmul(reshaped, W1) + b1)\r\n\r\n        W2 = tf.Variable(tf.random_normal([L2, 10], mean=0, stddev=0.01))\r\n        b2 = tf.Variable(tf.random_normal([10], mean=0, stddev=0.01))\r\n        logits = tf.nn.relu(tf.matmul(relu_1, W2) + b2)\r\n        return logits\r\n\r\n    def loss_function(self):\r\n        \"\"\"\r\n        Calculates the model cross-entropy loss\r\n\r\n        :return: the loss of the model as a tensor\r\n        \"\"\"\r\n        loss = tf.losses.softmax_cross_entropy(onehot_labels=self.label, logits=self.prediction)\r\n        return loss\r\n\r\n    def optimizer(self):\r\n        \"\"\"\r\n        Optimizes the model loss using an Adam Optimizer\r\n\r\n        :return: the optimizer as a tensor\r\n        \"\"\"\r\n        learning_rate = 0.1\r\n        sgd = tf.train.GradientDescentOptimizer(learning_rate)\r\n        train = sgd.minimize(self.loss)\r\n        return train\r\n\r\n    def accuracy_function(self):\r\n        \"\"\"\r\n        Calculates the model's prediction accuracy by comparing\r\n        predictions to correct labels \u2013 no need to modify this\r\n\r\n        :return: the accuracy of the model as a tensor\r\n        \"\"\"\r\n        correct_prediction = tf.equal(tf.argmax(self.prediction, 1),\r\n                                      tf.argmax(self.label, 1))\r\n        return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n\r\ndef main():\r\n    t_start = time.time()\r\n\r\n    mnist = input_data.read_data_sets(\"data/mnist/\", one_hot=True)\r\n    batch_sz = 50\r\n    batch = 2000\r\n\r\n    inputs = tf.placeholder(shape=[batch_sz, 28, 28, 1], dtype=tf.float32)\r\n    labels = tf.placeholder(shape=[batch_sz, 10], dtype=tf.float32)\r\n\r\n    model = Model(inputs, labels)\r\n\r\n    session_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\r\n    sess = tf.Session(config=session_config)\r\n\r\n    # sess = tf.Session()\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n    for i in range(batch):\r\n        next_image, next_label = mnist.train.next_batch(batch_sz)\r\n        next_image = next_image.reshape((batch_sz, 28, 28, 1))\r\n        sess.run(model.optimize, feed_dict={inputs: next_image, labels: next_label})\r\n\r\n    acc, test_images, test_labels = 0, mnist.test.images, mnist.test.labels\r\n    test_batch = math.ceil(len(test_images) / batch_sz)\r\n    for i in range(test_batch):\r\n        batch_images = test_images[i * batch_sz: (i + 1) * batch_sz]\r\n        batch_images = batch_images.reshape((batch_sz, 28, 28, 1))\r\n        batch_labes = test_labels[i * batch_sz: (i + 1) * batch_sz]\r\n        acc += sess.run(model.accuracy, feed_dict={inputs: batch_images, labels: batch_labes})\r\n    acc /= test_batch\r\n    print(acc)\r\n\r\n    print(time.time() - t_start, 'seconds')\r\n\r\n    return\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n", "comments": ["I've been running into the same issue with the same GPU: \"CUDNN_STATUS_INTERNAL_ERROR\".\r\n\r\nRTX 2070 GPU\r\nCUDA 10\r\ncuDNN 7.4.2\r\nUbuntu 18.04\r\ntf-nightly-gpu (r1.13, Jan 13)\r\nPython 3.6.7\r\n\r\n```\r\n2019-01-15 05:01:03.503415: I tensorflow/stream_executor/platform/default/dso_loader.cc:154] successfully opened CUDA li\r\nbrary libcublas.so.10.0 locally\r\n2019-01-15 05:01:03.752563: I tensorflow/stream_executor/platform/default/dso_loader.cc:154] successfully opened CUDA li\r\nbrary libcudnn.so.7 locally\r\n2019-01-15 05:01:04.905618: E tensorflow/stream_executor/cuda/cuda_dnn.cc:493] Could not create cudnn handle: CUDNN_STAT\r\nUS_INTERNAL_ERROR\r\n2019-01-15 05:01:04.908147: E tensorflow/stream_executor/cuda/cuda_dnn.cc:493] Could not create cudnn handle: CUDNN_STAT\r\nUS_INTERNAL_ERROR\r\n2019-01-15 05:01:04.908191: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops_fused.cc:801 :\r\n Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to se\r\ne if a warning log message was printed above.\r\n```", "I've the same problem running on \r\n\r\nRTX2080 GPU \r\nCUDA 10 \r\ncudnn 7.4.2\r\n\r\nI tried the following tf Versions tf-nightly-gpu and a self compiled Version from master (060b6e32ad). \r\nI found out that its possible to set the following ENVIRONMENT Variables to get further Debug Info. \r\n\r\nCUDNN_LOGINFO_DBG=1;\r\nCUDNN_LOGDEST_DBG=stdout\r\n\r\nThen i get the following error:\r\n\r\nI0117 14:11:24.441819 140433563125568 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /tmp/mnist/model.ckpt.\r\n2019-01-17 14:11:25.916269: I tensorflow/stream_executor/platform/default/dso_loader.cc:154] successfully opened CUDA library libcublas.so.10.0 locally\r\n\r\nI! CuDNN (v7402) function cudnnCreate() called:\r\ni! Time: 2019-01-17T14:11:26.079184 (0d+0h+0m+0s since start)\r\ni! Process=29255; Thread=29356; GPU=NULL; Handle=NULL; StreamId=NULL.\r\n\r\n2019-01-17 14:11:26.079151: I tensorflow/stream_executor/platform/default/dso_loader.cc:154] successfully opened CUDA library libcudnn.so.7 locally\r\n\r\nI! CuDNN (v7402) function cudnnCreate() called:\r\ni! Time: 2019-01-17T14:11:26.571897 (0d+0h+0m+0s since start)\r\ni! Process=29255; Thread=29356; GPU=NULL; Handle=NULL; StreamId=NULL.\r\n\r\n2019-01-17 14:11:26.571858: E tensorflow/stream_executor/cuda/cuda_dnn.cc:493] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-01-17 14:11:26.579375: E tensorflow/stream_executor/cuda/cuda_dnn.cc:493] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\nI! CuDNN (v7402) function cudnnCreate() called:\r\ni! Time: 2019-01-17T14:11:26.579803 (0d+0h+0m+0s since start)\r\ni! Process=29255; Thread=29356; GPU=NULL; Handle=NULL; StreamId=NULL.\r\n\r\n2019-01-17 14:11:26.585818: E tensorflow/stream_executor/cuda/cuda_dnn.cc:493] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-01-17 14:11:26.585850: W ./tensorflow/stream_executor/stream.h:2109] attempting to perform DNN operation using StreamExecutor without DNN support\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1320, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1408, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node Discriminator_1/Conv/Conv2D}}]]\r\n\t [[train/discriminator_train/train_op/control_dependency/_569]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dj/projects/gan/tf_models/research/gan/mnist/train.py\", line 151, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/dj/projects/gan/tf_models/research/gan/mnist/train.py\", line 147, in main\r\n    get_hooks_fn=tfgan.get_joint_train_hooks())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/gan/python/train.py\", line 1200, in gan_train\r\n    config=config)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/training/python/training/training.py\", line 546, in train\r\n    loss = session.run(train_op, run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 693, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1188, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1287, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1272, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1336, in run\r\n    feed_dict, options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1362, in _call_hook_before_run\r\n    request = hook.before_run(run_context)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/gan/python/train.py\", line 1061, in before_run\r\n    run_context.session.run(self._train_ops)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 930, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1153, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1329, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1349, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node Discriminator_1/Conv/Conv2D (defined at home/dj/projects/gan/tf_models/research/gan/mnist/networks.py:152) ]]\r\n\t [[train/discriminator_train/train_op/control_dependency/_569]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node Discriminator_1/Conv/Conv2D:\r\n inputs/batch/n (defined at home/dj/projects/gan/tf_models/research/gan/mnist/data_provider.py:67)\r\n\r\nOriginal stack trace for 'Discriminator_1/Conv/Conv2D':\r\n  File \"home/dj/projects/gan/tf_models/research/gan/mnist/train.py\", line 151, in <module>\r\n    tf.app.run()\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"home/dj/projects/gan/tf_models/research/gan/mnist/train.py\", line 87, in main\r\n    [FLAGS.batch_size, FLAGS.noise_dims]))\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/contrib/gan/python/train.py\", line 118, in gan_model\r\n    discriminator_real_outputs = discriminator_fn(real_data, generator_inputs)\r\n  File \"home/dj/projects/gan/tf_models/research/gan/mnist/networks.py\", line 176, in unconditional_discriminator\r\n    net = _discriminator_helper(img, False, None, weight_decay)\r\n  File \"home/dj/projects/gan/tf_models/research/gan/mnist/networks.py\", line 152, in _discriminator_helper\r\n    net = layers.conv2d(img, 64, [4, 4], stride=2)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 182, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1155, in convolution2d\r\n    conv_dims=2)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 182, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1058, in convolution\r\n    outputs = layer.apply(inputs)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1228, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\", line 531, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 564, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 196, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 966, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 591, in __call__\r\n    return self.call(inp, filter)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 208, in __call__\r\n    name=self.name)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1578, in conv2d\r\n    name=name)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1040, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 501, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nAny ideas somebody? I am just before reinstalling my complete environement :-(", "Try to compile r1.13 from source. It would take a long time, but it should fix your problem. At least it fixed mine. ", "I did try compiling from source, but ran into the same issue. I was finally able to fix my problem was setting `config.gpu_options.allow_growth = True`.", "I've been having the same issue (on an RTX 2060, Ubuntu 18.04, Python 3.6.7, CUDA 10.0.130, cuDNN 7.4.2, Tensorflow 1.13.0-rc0 from source). Thanks to @va-andrew's suggestion I have it working with the `allow_growth` option set.\r\n\r\nFWIW, in the course of searching for solutions to this it seems that this issue is a common problem with the RTX series (although it might be a general problem with CUDA 10.0, since the new cards don't support the older versions). It would be great if the defaults could get updated in the release of 1.13 so that special options don't need to be set for these cards.", "Chiming in to say I also experienced this under the following configuration:\r\n\r\n- Running tf benchmarks from https://github.com/tensorflow/benchmarks\r\n- RTX 2080\r\n- Ubuntu 18.04\r\n- CUDA 10.0\r\n- Nvidia Drivers 415.27\r\n- Tensorflow 1.13.0-dev20190125\r\n- CuDNN 7.4.2\r\n- Python 3\r\n\r\nTensorflow Docker GPU containers with stable releases of everything don't work either (they straight up segfault rather than report CUDNN_STATUS_INTERNAL_ERROR).\r\n\r\nCuriously, things work fine on Windows 10 with Tensorflow v1.12!\r\n\r\nAnd has others have reported, setting allow_growth allows things to run properly.", "Same problem here. \r\n\r\n- RTX 2070\r\n- Ubuntu 18.04\r\n- CudNN 7.4.2 (but I have tried compiling with other older versions with no luck)\r\n- Tensorflow 1.13.0-dev20190125 (also tried Tensorflow 1.12 compiled with Cuda 10)\r\n\r\nAnd as others have reported, setting allow_growth=TRUE allows things to run.\r\n", "Closing this issue since its resolved. Thanks!", "@ymodak Can you please reference the PR that fixed this bug?", "I have a [similar issue](https://github.com/tensorflow/benchmarks/issues/300) with `tf-nightly-gpu-2.0-preview` on the RTX 2080 ", "Same issue with an RTX2080, spent two days recompiling and bug hunting until I found this fix.\r\n(the allow_growth=true thing fixed it)\r\n\r\nYou made my day", "How do you actually set allow_growth=true? I have tf-nightly-gpu-2.0-preview and tried:\r\n\r\nimport tensorflow as tf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config, ...)\r\n\r\n\r\nbut get this error:\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-14-b4f9929bf252> in <module>()\r\n      1 import tensorflow as tf\r\n----> 2 config = tf.ConfigProto()\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'ConfigProto'\r\n\r\n\r\nHow can I set allow_growth in tensorflow 2.0?\r\n", "ok, made it work in tf-nightly-gpu-2.0-preview and ipython notebook adding this to my code:\r\n\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)", "same issue, with gpu_options.allow_growth = True the issue fixed.\r\n\r\n", "@newhouseb how/where did you set that true for all benchmarks? Was it an easy change? ", "Is blanket allow growth a solution ?\r\n\r\nIt is turned off by default for a reason see\r\nhttps://www.tensorflow.org/guide/using_gpu#allowing_gpu_memory_growth\r\n\r\nIn my program memory management is important \r\n\r\nI would like to limit the amount of GPU used by TF because in my graphics application the GPU memory will be used for other things and putting it into a limited space is important to prevent out of memory errors \r\n", "I am working in C++ under Windows\r\n\r\nAdding the allow growth option results in an OOM error.\r\n\r\nWithout this line of code the model runs fine on the same machine with the same card.\r\n\r\nWith OOM error\r\n```\r\noptions.config.mutable_gpu_options()->set_allow_growth(true);\r\noptions.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(fraction);\r\n```\r\n\r\nWithout OOM error\r\n```\r\n//options.config.mutable_gpu_options()->set_allow_growth(true);\r\noptions.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(fraction);\r\n```\r\n\r\nSo to solve this problem with set allow growth results in a segfault.", "@ymodak This bug is not fixed. Arguably, using any sort of convnet should work in the default configuration. Either allow_growth should be true by default, it should be fixed so this works, or there should be a better error than `CUDNN_STATUS_INTERNAL_ERROR`.", "@ymodak It looks like this issue was closed prematurely. While there is a work-around for this issue it involves changing application code. As a result the example code does not work _out of the box_ on RTX cards and most recipes on line will also need modification. ", "@samhodge can't you prevent OOM by using `config.gpu_options.per_process_gpu_memory_fraction = 0.4` as suggested on the tensorflow [documentation page](https://www.tensorflow.org/guide/using_gpu#allowing_gpu_memory_growth) you posted yourself ?\r\n\r\nI'm confused by this boolean hack to enable tensorflow-gpu on my RTX 2080: will this `allow_growth = True` be an issue if I use my GPU solely for one tensorflow script/jupyter notebook at a time ? (in addition to standard GPU usage for the screen etc)\r\n\r\nI intend to set a static ML stack on a computer and would like to know whether this will end up in a mess at some point (big gridsearch, models with lots of parameters etc). I didn't figure out yet whether I definitely need to build from sources to try to avoid this internal error or just change this boolean.", "Ok I think I found the source of my issues before I create my session I measure the GPU RAM free so if I am on a 8Gb card and 6Gb are free I use a fraction of 0.75 and occasionally that ends in an OOM but recently I have been experimenting with 0.95*0.75 and I have yet to have an OOM. So if you push the space for allocation of Tensorflow to the limit it sometimes clashes. Obviously if you inputs and outputs to an individual Op don\u2019t fit it will OOM,  but I measure against this an will use GPU or CPU depending on which fits.", "@samhodge great, so in the end the `allow_growth` boolean hack does provide a solution if no major GPU operation is launched in parallel and if what is processed _at a time_ by tensorflow (batch size would be critical) doesn't overflow the memory provided by the GPU... ? ", "Everything uses the GPU even your browser", "Running into the same issue on a GTX 1050 using tensorflow-gpu 1.13.1 from pip with CUDA 10.0/cuDNN 7.4.2.24/Nvidia driver 410/Ubuntu 16.04.", "Still having the same issue here but \"config.gpu_options.allow_growth = True\" doesn't fix the problem.  Happens on both TF-gpu 1.14.1 and TF-gpu 2.0.  RTX1070, CUDA 10.0, Ubuntu 18.04, Nvidia driver 430.09.\r\n", "The descriptions of the problems you are seeing makes me believe that (particular version of) cuDNN tries to allocate GPU memory when creating the handle. If TensorFlow already took all the memory (either because config.gpu_options.allow_growth = false, or per_process_gpu_memory_fraction close to 1.0) there is no memory left to allocate for cuDNN.\r\n\r\nYou could confirm this by running TensorFlow through nvprof and generate an API trace to inspect the failing cuMemAlloc call.\r\n\r\nIssue  #6698 seems to discuss the same problem. Some people noticed that they had accidentally used a cuDNN release that doesn't match their CUDA version. Could you please verify that you are using cuDNN for CUDA 10 when running with CUDA 10?", "Turns out I didn't have cuDNN installed correctly because I am a great fool.  Got it in, reinstalled TF2-nightly, added the lines to allow the growth, and all is good.", "How to delete cudatoolkit and cudnn from Conda?\r\n\r\nSince Anaconda-included(or embedded) cudnn has the error as follows, I want to remove conda-installed cudatoolkit and cudnn, and install independent CUDA and cudnn from the Nvidia's website.\r\n\r\nError : Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\r\nHowever, while I use the commands as follows but can not remove them, I can not remove them.\r\nconda remove --name cuda --all\r\nconda remove --name cudnn --all\r\n\r\nI see that two documents including cudatoolkit-10.0.130-0 and cudnn-7.3.1-cuda10.0.0_0 in the path as\r\nfollows.\r\n\r\n/home/anaconda3/pkgs/cudatoolkit-10.0.130-0\r\n/home/anaconda3/pkgs/cudnn-7.3.1-cuda10.0.0_0\r\n\r\nHow can I delete(or remove) cuda and cudnn that included(or embedded) in Anaconda.\r\n\r\nThanks in advance,\r\n\r\nMike", "@mikechen66 What is the output of conda? It may be because other packages depend on cuda and cudnn. Why would you want to delete them in the first place? If you want to get a custom environment, use miniconda rather than anaconda. Miniconda only comes with conda, and you need to install all the packages you need manually. ", "Hi tydlwav: \r\n\r\nThanks for your feedback. After checking the version compatibility and release date of the core libraries, I installed the related dev environments, run the simple MNIST test code and got the outputs as follows. \r\n\r\nI think that Anaconda3 can not even support the core libraries of cudnn and TensorFlow. So it is a big problem of Anaconda3. So I want to delete the lightweight cudnn libraries from Anaconda and use the independent and powerful Nvidia cuda and cudnn libraries to run the test code.  Please help give some suggestions. \r\n\r\n1. Installation Environments\r\n\r\nNvidia GeForce RTX 2060 \r\nGraphics Driver: NVIDIA-Linux-x86_64-415.27 (Jan 15, 2019)\r\n                            1st version that supports RTX 2060 \r\nAnaconda3: Anaconda3-2019.03-Linux-x86_64.sh (2019.04-04)\r\n   -- cudatoolkit-10.0.130-0\r\n   -- cudnn-7.3.1-cuda10.0.0_0\r\n   -- TensorFlow 13.1\r\n   -- Juputer Notebook and ipykernel \r\n       --defaulted by Ananconda3\r\n\r\n2. MNIST Test Code: \r\n\r\nimport keras\r\nfrom keras.datasets import mnist\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout\r\nfrom keras.layers import Flatten,  MaxPooling2D, Conv2D\r\nfrom keras.callbacks import TensorBoard\r\n\r\n(X_train,y_train), (X_test, y_test) = mnist.load_data()\r\n\r\nX_train = X_train.reshape(60000,28,28,1).astype('float32')\r\nX_test = X_test.reshape(10000,28,28,1).astype('float32')\r\n\r\nX_train /= 255\r\nX_test /= 255\r\n\r\nn_classes = 10\r\ny_train = keras.utils.to_categorical(y_train, n_classes)\r\ny_test = keras.utils.to_categorical(y_test, n_classes)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)) )\r\nmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2,2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())          \r\nmodel.add(Dense(128, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(n_classes, activation='softmax'))\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\ntensor_board = TensorBoard('./logs/LeNet-MNIST-1')\r\n\r\nmodel.fit(X_train, y_train, batch_size=128, epochs=15, verbose=1,\r\n          validation_data=(X_test,y_test), callbacks=[tensor_board])\r\n\r\n3. Outputs: \r\n\r\nUsing TensorFlow backend.\r\n\r\nWARNING:tensorflow:From /home/mike/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From /home/mike/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nWARNING:tensorflow:From /home/mike/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nTrain on 60000 samples, validate on 10000 samples\r\nEpoch 1/15\r\n\r\n\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-1-c7f0e9a016e9> in <module>\r\n     34\r\n     35 model.fit(X_train, y_train, batch_size=128, epochs=15, verbose=1,\r\n---> 36           validation_data=(X_test,y_test), callbacks=[tensor_board])\r\n\r\n~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1037                                         initial_epoch=initial_epoch,\r\n   1038                                         steps_per_epoch=steps_per_epoch,\r\n-> 1039                                         validation_steps=validation_steps)\r\n   1040\r\n   1041     def evaluate(self, x=None, y=None,\r\n\r\n~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n    197                     ins_batch[i] = ins_batch[i].toarray()\r\n    198\r\n--> 199                 outs = f(ins_batch)\r\n    200                 outs = to_list(outs)\r\n    201                 for l, o in zip(out_labels, outs):\r\n\r\n~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)\r\n   2713                 return self._legacy_call(inputs)\r\n   2714\r\n-> 2715             return self._call(inputs)\r\n   2716         else:\r\n   2717             if py_any(is_tensor(x) for x in inputs):\r\n\r\n~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)\r\n   2673             fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)\r\n   2674         else:\r\n-> 2675             fetched = self._callable_fn(*array_vals)\r\n   2676         return fetched[:len(self.outputs)]\r\n   2677\r\n\r\n~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)\r\n   1437           ret = tf_session.TF_SessionRunCallable(\r\n   1438               self._session._session, self._handle, args, status,\r\n-> 1439               run_metadata_ptr)\r\n   1440         if run_metadata:\r\n   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    526             None, None,\r\n    527             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 528             c_api.TF_GetCode(self.status.status))\r\n    529     # Delete the underlying status object from memory otherwise it stays alive\r\n    530     # as there is a reference to status from this from the traceback due to\r\n\r\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d_1/convolution}}]]\r\n\t [[{{node metrics/acc/Mean}}]]\r\n\r\n", "Hi tydlwav:\r\n\r\nI use the following command to uninstall both cuda and cudnn.  However, both the libraries are still located in Anaconda3 even though they do not work right now. I guess that Anaconda3 intends to protect the core libraries not to be removed. It might be the core capability of Continuum even thought it has bugs. I  will try to use either Independent Nvdia cuda(uncluding nvcc) and cudnn or find the new cuda or cudnn with conda to be installed. \r\n\r\nUninstall Command: \r\n\r\nconda uninstall cudatoolkit\r\n\r\nCollecting package metadata: done\r\nSolving environment: done\r\n\r\n## Package Plan ##\r\n\r\n  environment location: /home/mike/anaconda3/envs/tf-gpu\r\n\r\n  removed specs:\r\n    - cudatoolkit\r\n\r\nThe following packages will be REMOVED:\r\n\r\n  cudatoolkit-10.0.130-0\r\n  cudnn-7.3.1-cuda10.0_0\r\n  cupti-10.0.130-0\r\n  keras-2.2.4-0\r\n  tensorflow-1.13.1-gpu_py37hc158e3b_0\r\n  tensorflow-base-1.13.1-gpu_py37h8d69cac_0\r\n  tensorflow-gpu-1.13.1-h0d30ee6_0\r\n\r\nProceed ([y]/n)? y\r\n\r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\n\r\nNotes: \r\n\r\nAfter I uninstalled both of them, Jupyter Notebook shown \"No mudule named \"tensorflow\". That means that uninsallation is successful. However, both cudatoolkit and cudnn are still found in Anaconda3. I think that Continuum defaults them not to not delete although both of them does not work. \r\n\r\n/home/anaconda3/pkgs/cudatoolkit-10.0.130-0 \r\n/home/anaconda3/pkgs/cudnn-7.3.1-cuda10.0.0_0", "You have already removed them. The files in `pkgs` are for installation. These are downloaded cache for the installation. Also, this is not the place to discuss conda environment issues. It is not relevant to this issue. You may want to try stack overflow. ", "I'm a little confused by the state of this issue. I am using an RTX 2080, cuda 10.1, cudnn v7.5.1.10 and tensorflow 1.14. \r\n\r\nUsing the allow growth work around works, but maybe I have a different version mismatch?\r\n\r\nWill there be a fix for this in tensorflow 1.14?\r\n\r\nThank you\r\n\r\n", "Thanks. I see the compatibility issue among RTX 20XX Turing series, TensorFlow and Anaconda. It is obvious that RTX 20XX series Supports cudnn 7.5.0, TensorFlow only supports cudnn 7.4, but Anaconda includes a streamlined 7.3.1, it is a total mismatch among the three vendors. In addition, RTX 20XX series has a big compatibility problem with Ubuntu 16.04 LTS. Sometimes, the Ubuntu 16.04 crashed. I had to bring two bootable USB stick to reinstall the OS. Therefore, I upgraded two PCs to Ubuntu 18.04 LTS and installed Miniconda. Then I will try a higher version Tensorflow.\r\n\r\nNotes:\r\n\r\nNvidia has its own custom Ubuntu 18.04 LTS for its Jetson TX1/TX2 and Jetson Nano Mobile GPU platform. Nvidia seems to determine its new products such as RTX 20XX series in compatibility with Ubuntu 18.04 LTS rather than the lower version Ubuntu 16.04. However, I do not know whether Continuum has its upgrade plan for Nvidia RTX 20XX Turing series. ", "RTX series are well supported as of right now. I have used tf with RTX 2070 through a conda environment on non-ubuntu distribution. This should be the worst case scenario, and it's still working fine. Cuda and cudnn are backwards compatible, and it should not be an issue if you use the newer versions. You should simply create a new Python 3.6 environment with `conda create -n tf python==3.6.8` and run `conda install tensorflow-gpu`. ", "That is great I have compiled from source and have had clients work with Tensorflow 1.12.0 CUDA 10.0 and CUDNN 7.4.2.24 on most hardware but I have had issues with a handful of clients with RTX cards with a CNN with cudnn on the GPU. I may have accidentally packaged the wrong CUDNN for CUDA 9.0 the files are identically named.\r\n\r\nCan anyone confirm that these versions work on RTX2080 and other Turing based cards?", "Hi tydlwav: \r\n\r\nI installed Miniconda and related python and tensorflow environment according to your suggestion. It still has the error: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize.......\r\nPlease help find a solution.\r\n\r\nPlease see the steps I operated. \r\n\r\n1. Instal python 3.6.8 according to your guideline. \r\nconda create -n tf python==3.6.8\r\n\r\n2. activate tf\r\nconda activate tf\r\n\r\n3. install tensorflow-gpu in the tf environment according to your guideline. \r\n conda install tensorflow-gpu\r\n\r\nThe installed package includes cudatoolkit and cudnn as follows. \r\n....................................................................................................\r\ncudatoolkit        pkgs/main/linux-64::cudatoolkit-10.0.130-0\r\ncudnn              pkgs/main/linux-64::cudnn-7.3.1-cuda10.0_0\r\n....................................................................................................\r\n\r\n4. install jupyter notebook, ipykernel and related environment the webpage. \r\n\r\n1). install jupyter notebook\r\nconda install jupyter notebook\r\n\r\n2). install ipykernel based on jupyter notebook\r\nconda install ipykernel jupyter\r\n\r\n3). create TensorFlow-GPU in the webpage of jupyter notebook\r\npython -m ipykernel install --user --name tf-gpu --display-name \"TensorFlow-GPU\"\r\n\r\n5. Open jupyter notebook \r\n1). command into jupyter notebook webpage \r\njupyter notebook\r\n\r\n2). Click TensorFlow-GPU \r\nWhile clikcing in the menu \"new\" and selecting \"TensorFlow-GPU\" in the wepage, the cell shows in the webpage of jupyter notebook.  The webpage is listed as follows. \r\nhttp://localhost:8888/notebooks/Untitled3.ipynb?kernel_name=tf-gpu\r\n\r\n6. Paste Run the simple MNIST test code\r\n\r\nimport keras\r\nfrom keras.datasets import mnist\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout\r\nfrom keras.layers import Flatten,  MaxPooling2D, Conv2D\r\nfrom keras.callbacks import TensorBoard\r\n\r\n(X_train,y_train), (X_test, y_test) = mnist.load_data()\r\n\r\nX_train = X_train.reshape(60000,28,28,1).astype('float32')\r\nX_test = X_test.reshape(10000,28,28,1).astype('float32')\r\n\r\nX_train /= 255\r\nX_test /= 255\r\n\r\nn_classes = 10\r\ny_train = keras.utils.to_categorical(y_train, n_classes)\r\ny_test = keras.utils.to_categorical(y_test, n_classes)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)) )\r\nmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2,2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())          \r\nmodel.add(Dense(128, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(n_classes, activation='softmax'))\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\ntensor_board = TensorBoard('./logs/LeNet-MNIST-1')\r\n\r\n\r\nmodel.fit(X_train, y_train, batch_size=128, epochs=15, verbose=1,\r\n          validation_data=(X_test,y_test), callbacks=[tensor_board])\r\n\r\n7. Errors as same as the last-mentioned message: \r\n\r\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d_1/convolution}}]]\r\n\t [[{{node metrics/acc/Mean}}]]\r\n\r\nThanks,\r\n\r\nMike \r\n", "HI tydlwav:\r\n\r\nBy the way, I also installed keras with the following command. \r\nconda install keras-gpu\r\n\r\nSince ever installation is correct, I have got the error. So I assume that it is the version compatibility issue between Miniconda and RTX20XX Turing series. The error is as same as Anaconda. I get to know that the cudnn and cuda version in Miniconda and Anaconda are same.  \r\n\r\n", "That's fairly interesting. I got cuda 10 and cudnn7.3 working with conda about a month and a half ago. I haven't used tensorflow since then. If it doesn't work for you, you can build from source. That always works for me. If you're just starting, I'd recommend using pytorch. You'd have a much easier time installing and getting things working. ", "Hi tydlwav:\r\n\r\nI will try the other method such as pytorch. Now that Google releases tensorflow-gpu 1.14 , can I use the Miniconda to install independent tensorflow-gpu 1.14 at the Google Tensorflow website as follows.\r\n\r\nGoogle tensorflow: https://www.tensorflow.org/install/source\r\n\r\nNotes:\r\n\r\nConda has only tensorflow-gpu builds from 1.0.1 to 1.13.1 as follows. The builds are so old that the builds could not catch up with the official Google TensorFlow and the official Nvidia GeForce RTX 20XX (2060~2080) Truing series.\r\n\r\nCommand:\r\nconda search tensorflow-gpu\r\n\r\nLoading channels: done\r\n\r\nName Version Build Channel\r\ntensorflow-gpu 1.0.1 py27_4 pkgs/free\r\ntensorflow-gpu 1.0.1 py35_4 pkgs/free\r\ntensorflow-gpu 1.0.1 py36_4 pkgs/free\r\ntensorflow-gpu 1.1.0 np111py27_0 pkgs/free\r\ntensorflow-gpu 1.1.0 np111py35_0 pkgs/free\r\ntensorflow-gpu 1.1.0 np111py36_0 pkgs/free\r\ntensorflow-gpu 1.1.0 np112py27_0 pkgs/free\r\ntensorflow-gpu 1.1.0 np112py35_0 pkgs/free\r\ntensorflow-gpu 1.1.0 np112py36_0 pkgs/free\r\ntensorflow-gpu 1.2.1 py27cuda7.5cudnn5.1_0 pkgs/free\r\ntensorflow-gpu 1.2.1 py27cuda7.5cudnn6.0_0 pkgs/free\r\ntensorflow-gpu 1.2.1 py27cuda8.0cudnn5.1_0 pkgs/free\r\ntensorflow-gpu 1.2.1 py27cuda8.0cudnn6.0_0 pkgs/free\r\ntensorflow-gpu 1.2.1 py35cuda7.5cudnn5.1_0 pkgs/free\r\ntensorflow-gpu 1.2.1 py35cuda7.5cudnn6.0_0 pkgs/free\r\ntensorflow-gpu 1.2.1 py35cuda8.0cudnn5.1_0 pkgs/free\r\ntensorflow-gpu 1.2.1 py35cuda8.0cudnn6.0_0 pkgs/free\r\ntensorflow-gpu 1.2.1 py36cuda7.5cudnn5.1_0 pkgs/free\r\ntensorflow-gpu 1.2.1 py36cuda7.5cudnn6.0_0 pkgs/free\r\ntensorflow-gpu 1.2.1 py36cuda8.0cudnn5.1_0 pkgs/free\r\ntensorflow-gpu 1.2.1 py36cuda8.0cudnn6.0_0 pkgs/free\r\ntensorflow-gpu 1.3.0 0 pkgs/free\r\ntensorflow-gpu 1.4.1 0 pkgs/main\r\ntensorflow-gpu 1.5.0 0 pkgs/main\r\ntensorflow-gpu 1.6.0 0 pkgs/main\r\ntensorflow-gpu 1.7.0 0 pkgs/main\r\ntensorflow-gpu 1.8.0 h7b35bdc_0 pkgs/main\r\ntensorflow-gpu 1.9.0 hf154084_0 pkgs/main\r\ntensorflow-gpu 1.10.0 hf154084_0 pkgs/main\r\ntensorflow-gpu 1.11.0 h0d30ee6_0 pkgs/main\r\ntensorflow-gpu 1.12.0 h0d30ee6_0 pkgs/main\r\ntensorflow-gpu 1.13.1 h0d30ee6_0 pkgs/main", "They are not old, as I've used conda's release of tf 1.12 with RTX 2070. New hardware are usually backward compatible, and RTX is no different. It is most likely there are some weird environment issue at play. I don't have access to an RTX machine until July so I can't help with testing right now. Building from source should solve your problem. I've never failed to run convnets from tf built from source (assuming you have the correct configurations during build). \r\n\r\nOnce again, this is not the right place to discuss the distribution issue of tensorflow. You can make a post on stack overflow or reddit and link it here. More people will be able to see it and help you this way. \r\n\r\nYour issue is not a bug, and it is definitely not what this issue is discussing. ", "@chsigg you're diagnosis that this is a problem w/ CUDNN attempting to allocate GPU memory resources that tensorflow has already allocated seems correct to me.  Simply setting `per_process_gpu_memory_fraction=0.9` instead of `0.95` was sufficient to resolve my issues.", "I was also facing this issue. Fixed it by updating cuDNN to 7.6 version. \r\n\r\n```\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above\r\n```\r\n\r\nTensorflow-gpu: 1.13.1\r\nCuda: 10.0\r\nCuDNN: 7.3.1\r\n\r\nAlso, tensorflow and CuDNN was installed by Conda. \r\n`conda list cudnn`\r\n```\r\ncudnn                     7.3.1                cuda10.0_0    anaconda\r\n```\r\n\r\nThings I did:\r\n1. Uninstalled conda tensorflow.\r\n`conda remove tensorflow`\r\n2. Uninstall conda cuDNN\r\n`conda remove cudnn`\r\n3. Install tensorflow with pip\r\n`pip install tensorflow`\r\n4. Download corresponding cuDNN 7.6 runtime deb file from https://developer.nvidia.com/cudnn\r\n5. Install it with `sudo dpkg -i libcudnn_xxxxx_amd64.deb`\r\n", "@nluehr any comments? Can we make [MinSystemMemory()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L899) cuda/cudnn version aware?", "It is legit a memory error, if using tf.keras then do the following at the top of your file\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\ntf.keras.backend.set_session(tf.Session(config=config))", "I ran into this issue as well, and was able to solve it by using @va-andrew 's solution, and specifically, I used @colinsteidtmann 's implementation, since I use some of the tensorflow.keras functions in my code. I spent a long time trying to debug this problem, so thank you both for your contributions.\r\n\r\nEDIT: I was just looking at tensorflow documentation (https://www.tensorflow.org/guide/using_gpu), and you can also tell it to allow memory growth by setting the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true. It also says that this configuration is platform specific, so YMMV (works for me with Ubuntu 18.04).\r\n\r\nFor reference, I am running:\r\nUbuntu 18.04.2 LTS, Gigabyte GeForce RTX 2080 Turbo, NVIDIA driver 430.26, CUDA 10.0.130, cuDNN 7.4.2.24, tensorflow-gpu 1.13.1, python 3.6. I run tensorflow from within a virtual environment, using spyder 3.3.4.\r\n\r\nI have a 2nd computer with the exact same hardware, and I set it up following the same set of instructions, used the same files to do the install, and had this issue on that machine as well. No surprise there.\r\n\r\nI have a 3rd computer with the exact same hardware, except that it has a 2080 Ti instead of the 2080, and I set it up following the same set of instructions, and again used the same files to do the install. But this time, there was no issue.\r\n\r\nSo, I'm led to believe it's not related to some conflict of CUDA, cuDNN, and driver version; it's not an incorrectly done installation, etc. Rather, it's related to the model of video card; I've only seen mention of this issue with RTX 2060, 2070, and 2080.\r\n\r\nFortunately, it's not a big inconvenience to use the workaround.\r\n\r\n", "> \r\n> \r\n> I was also facing this issue. Fixed it by updating cuDNN to 7.6 version.\r\n> \r\n> ```\r\n> tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above\r\n> ```\r\n> \r\n> Tensorflow: 1.13.1\r\n> Cuda: 10.0\r\n> CuDNN: 7.3.1\r\n> \r\n> Also, tensorflow and CuDNN was installed by Conda.\r\n> `conda list cudnn`\r\n> \r\n> ```\r\n> cudnn                     7.3.1                cuda10.0_0    anaconda\r\n> ```\r\n> \r\n> Things I did:\r\n> \r\n>     1. Uninstalled conda tensorflow.\r\n>        `conda remove tensorflow`\r\n> \r\n>     2. Uninstall conda cuDNN\r\n>        `conda remove cudnn`\r\n> \r\n>     3. Install tensorflow with pip\r\n>        `pip install tensorflow`\r\n> \r\n>     4. Download corresponding cuDNN 7.6 runtime deb file from https://developer.nvidia.com/cudnn\r\n> \r\n>     5. Install it with `sudo dpkg -i libcudnn7_-1+cuda9.0_amd64.deb`\r\n\r\n\r\n@alexforever86  After you did your update, are you sure that you are running on your GPU, and not the CPU? It seems you are using the GPU before you did your update (due to the error message referencing cuDNN), but I wonder about after. You use \"pip install tensorflow\", but it should be \"pip install tensorflow-gpu\", no? Also, you said you are using CUDA 10, but the cuDNN deb file you listed is for cuda9.0, so that shouldn't work. \r\n\r\nSo, I think it might be the case that you aren't actually using the GPU, and thus is not proof that updating to cuDNN 7.6 resolves the issue.", "@synapse8 You are absolutely right about tensorflow-gpu and cuDNN version. I'm also very much confused by my comment now, and I don't remember the details anymore. Anyways, given below are the current versions in my system.\r\n\r\n`pip show tensorflow-gpu`\r\nName: tensorflow-gpu\r\nVersion: 1.13.1\r\n\r\n`nvidia-smi`\r\nNVIDIA-SMI 430.26       Driver Version: 430.26       CUDA Version: 10.2     \r\n\r\n`sudo apt search cudnn | grep installed`\r\nlibcudnn7/now 7.6.0.64-1+cuda10.0 amd64\r\n", "@alexforever86 with the configuration you mentioned now do you still see this problem? (I assume it works for you). I recently installed a system with cuda10, 410 driver, 7.6 cudnn and TF-gpu 1.14 (pip install) and have not seen the issue.", "@robzor92 I've been using tensorflow-gpu 1.13, and out of curiosity, I just installed 1.14 to test if this resolved the issue (for me). I'm still getting the error, and still have to do the 'allow growth' workaround (again, not that big a deal). \r\n\r\nWhat video card are you using?", "@synapse8 Tried it with a GTX 1070. ", "@synapse8 I also tried the sample code provided by this thread creator just now, it worked without a problem. I would however not claim it is only a problem of the RTX line as I saw the same problem on a  GTX 1050Ti with TF 1.13.1. Using the same driver/cuda/cudnn combination I posted before.", "@robzor92 I doubt the 1050Ti's problem is with the small VRAM size. The RTX cards would encounter this on the basic CNN MNIST models. I doubt it's NVIDIA's tweaking of VRAM allocation on RTX cards somehow messed things up. ", "I have the same error on tensorflow 1.14.0 and RTX2080. But in my case, this error occurs only when I use convolution layer.\r\n\r\n```\r\n2019-07-14 21:48:13.041683: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-14 21:48:13.064262: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz\r\n2019-07-14 21:48:13.064955: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55abe99bcd30 executing computations on platform Host. Devices:\r\n2019-07-14 21:48:13.064967: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-07-14 21:48:13.066219: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-07-14 21:48:13.153748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-14 21:48:13.154195: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55abebb44f00 executing computations on platform CUDA. Devices:\r\n2019-07-14 21:48:13.154207: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5\r\n2019-07-14 21:48:13.154317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-14 21:48:13.154707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.71\r\npciBusID: 0000:01:00.0\r\n2019-07-14 21:48:13.154845: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-07-14 21:48:13.155504: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-07-14 21:48:13.156112: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-07-14 21:48:13.156265: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-07-14 21:48:13.157040: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-07-14 21:48:13.157646: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-07-14 21:48:13.159661: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-14 21:48:13.159730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-14 21:48:13.160165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-14 21:48:13.160542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-14 21:48:13.160559: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-07-14 21:48:13.161120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-14 21:48:13.161129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-14 21:48:13.161133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-14 21:48:13.161331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-14 21:48:13.161730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-14 21:48:13.162120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6794 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-07-14 21:48:13.497639: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-14 21:48:14.077729: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-07-14 21:48:14.080055: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 16, in <module>\r\n    print(model.predict(test_inputs))\r\n  File \"/home/yudai/.local/share/virtualenvs/pipenv_practice-DKmRVcs4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1078, in predict\r\n    callbacks=callbacks)\r\n  File \"/home/yudai/.local/share/virtualenvs/pipenv_practice-DKmRVcs4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 363, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/home/yudai/.local/share/virtualenvs/pipenv_practice-DKmRVcs4/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3292, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/yudai/.local/share/virtualenvs/pipenv_practice-DKmRVcs4/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d/Conv2D}}]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d/Conv2D}}]]\r\n\t [[flatten/Reshape/_7]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\nI tried `config.gpu_options.allow_growth = True`, but it does not solve this error.\r\n\r\nI want someone to help me.\r\n\r\nThank you.", "Same issue with RTX 2070", "I've made an interesting observation concerning this, that might help track down this error or find a viable solution:\r\nI get also the error `Failed to get convolution algorithm` with reference to `Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR`.\r\nSystem: laptop machine with the Nvidia Quadro P2000, Ubuntu 18.04, tf 1.13.1, cuda10, cudnn 7.4.2\r\nAs mentioned, I can run the program smoothly using `allow_growth`, so thanks for that, good enough for me.\r\n\r\n*Interesting:* I get this error only when using `tf.layers.conv...` but switching to `tf.keras.layers....` allows the program to run without `allow_growth`, so something in the keras code seems to work better than in the tf code. Maybe somebody can use this information to track down a solution from keras.\r\nI am sticking to the tf.layers for now, as they provide an easy weight sharing through variable scopes, which are not supported by keras sadly.", "@DavidS3141 It's interesting. In my case, the only convolution layer does not work in both tf.layers and tf.keras.layers...\r\n\r\nWhen I use pytorch, `torch.cuda.is_available` is True and can use convolution layer without any trouble, so I believe the cause is the tensorflow, but I do not know what is wrong.", "I agree with @Hayashi-Yudai: The same is true about MXNet. Identical configuration works fine when Tensorflow fails.\r\n\r\nEnvironment:\r\nRTX2080\r\nUbuntu 18.10\r\nDriver 430.26\r\nCUDA 10.0 (also 10.1, which isn't yet supported by TF)\r\ncuDNN 7.6.1\r\nmxnet-cu100 1.4.1\r\ntensorflow-gpu 1.14.0", "Hey guys, I am using the weights from the pre-trained model with ResNet50 backbone on COCO dataset to train on my CSV dataset. I am getting this error : *Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning \u2502 log message was printed above.*I am running the following command in a virtual environment on Ubuntu 16.0 to for training.: keras-retinanet/keras_retinanet/bin/train.py --weights resnet50_coco_best_v2.1.0.h5\r\n--batch-size 7 --steps 9 --epochs 4\r\n--snapshot-path snapshots --tensorboard-dir tensorboard\r\ncsv dataset/train.csv dataset/classes.csvI tried to resolve the problem by the following script in command line in the virtual environment:\r\npython\r\n\r\nimport tensorflow\r\n\r\n>> from tensorflow.compat.v1 import ConfigProto\r\n>> from tensorflow.compat.v1 import InteractiveSession\r\n>> config = ConfigProto()\r\n>> config.gpu_options.allow_growth = True\r\n>> session = InteractiveSession(config=config)\r\n\r\nas well as\r\nimport tensorflow as tf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config)but it did not resolve my error.:\r\n\r\nI am using:-\r\nUbuntu 16.0\r\nCuda: 10.0\r\nTensorflow 1.14.0\r\n\r\nError:\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found. \u2502| No running processes found |\r\n(0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning \u2502+-----------------------------------------------------------------------------+\r\nlog message was printed above. \u2502\r\n[[{{node conv1/convolution}}]] \u2502\r\n[[loss/add/_2377]] \u2502\r\n(1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning \u2502\r\nlog message was printed above. \u2502\r\n[[{{node conv1/convolution}}]] \u2502\r\n0 successful operations. \u2502\r\n0 derived errors ignored. \u2502\r\nterminate called without an active exception \u2502\r\nAborted (core dumped)\r\nAny help would be appreciated.", "Same problem here. Allow_growth workaround works. Otherwise I get this error on the most basic MNIST tensorflow dataset.\r\n\r\nRTX2060 mobile here.\r\n\r\nIssue occurs with compiled tensorflow from r2.0 branch as well as TF 1.4 installed via conda with conda ( tensorflow-gpu).\r\n\r\n", "@Hayashi-Yudai \r\n\r\n> I tried config.gpu_options.allow_growth = True, but it does not solve this error.\r\n\r\nWhat were the exact commands you added to your code? Try the following instead if it's different ...\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\ntf.keras.backend.set_session(tf.Session(config=config))", "@synapse8 Thank you for your comment. I tried but the result was the same.\r\n\r\nBy the way, I tried nvidia-docker and went well except for that the python version is 3.5.\r\nhttps://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/running.html#running", "An additional information, if you do not mind using python 3.6.8 and tensorflow-gpu 1.12.0, you can use anaconda.\r\n\r\n```\r\nconda create -n <virtual env name> python=3.6.8\r\nconda install tensorflow-gpu==1.12.0\r\nconda install cudnn==7.3.1    # By default, cudnn7.6 is installed but it causes the error\r\n```\r\n", "I tested building tf-2.0.0-beta1 from sources with CUDA-10.1 and CUDNN-7.6.2.4 and the error doesn't manifest.\r\n\r\nYou can find docker images for [building](https://github.com/edowson/docker-tensorflow/tree/master/ubuntu/xenial/tf-build) a tf-gpu package and a [tf-base](https://github.com/edowson/docker-tensorflow/tree/master/ubuntu/xenial/tf-base) package here:\r\nhttps://github.com/edowson/docker-tensorflow\r\n\r\nThe anaconda channel doesn't have `cudnn==7.6.2` at the time of writing this comment.", "Windows 7, bashed my head against the wall over `Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR` for quite a while trying to get a new machine up. \r\n\r\nReinstalls, lots of other things in this and other threads didn't fix it. \r\n\r\nWhile testing that not having `cudnn64_7.dll` would cause a different error than the `CUDNN_STATUS_INTERNAL_ERROR` I renamed the dll. Confirming the error was instead a `CUDNN NOT INSTALLED` type error, I undid the file name change. \r\n\r\nMagically, everything started working. \r\n\r\nNo idea why or how, but it does. Hopefully this helps someone else. If not, it only takes a few seconds to try. ", "I found this issue was caused by me erroneously making two calls to tf.Session \r\n```\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n\r\n# several lines of code later...\r\n\r\nsess = tf.Session(config=config)\r\n```\r\n\r\nProbably not the root cause for most folks but it might be worth looking out for.", "Just to share \"allow_growth = True\" solves the issue for my system below\r\nrtx 2080ti, ubuntu18.04, cuda9.0, cudnn7, tf1.9\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config)", "It has to do with the memory fraction available to load GPU resources to create cudnn handle, also known as `per_process_gpu_memory_fraction`.\r\nReducing this memory fraction by yourself will solve the error.\r\n\r\n\r\n    > sess_config = tf.ConfigProto(gpu_options =\r\n    > tf.GPUOptions(per_process_gpu_memory_fraction=0.7),\r\n    > allow_soft_placement = True)\r\n    > \r\n    > with tf.Session(config=sess_config) as sess:\r\n    >      sess.run([whatever])\r\n\r\nUse as small fraction as could fit in your memory. (In the code, I use 0.7, you can start with 0.3 or even smaller, then increase until you get the same error, that's your limit.)\r\nPass it to your `tf.Session()` or `tf.train.MonitoredTrainingSession()` or Supervisor's `sv.managed_session()` as config.\r\n\r\nThis should allow your GPU create a cudnn handle for your TensorFlow code.", "As explained [here](https://www.tensorflow.org/beta/guide/using_gpu), the new approach in TF 2.0 for setting `config.gpu_options.allow_growth = True` is:\r\n\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  # Currently, memory growth needs to be the same across GPUs\r\n  try:\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n  except RuntimeError as e:\r\n    print(e)\r\n```\r\n\r\nWith this code snippet and TF 2.0 RC1, the error no longer appears.\r\nHowever, due to the number of people that have a 20XX Nvidia GPU, I think that it would be a good idea to address this problem natively before the final version of TF 2.0 is released.", "I had the same issue with 1080Ti & TitanX on TF1.4 and the suggestions from @va-andrew and @oscarlinux saved the day! Which reminds me in the first place why I switched to pytorch and never coming back. Unfortunately there are still ppl using TF.... so I still have to go through this pain whenever I use their codebase... maybe it's time to play a bit with ONNX.", "For anyone else finding this after upgrading to tensorflow 2.0, the API and the code are slightly different.\r\n\r\nUbuntu 18\r\nTensorflow 2.0\r\nTensorflow-gpu 2.0\r\nGeForce RTX 2070\r\n\r\nUpdated code for this system.\r\n```\r\nimport tensorflow as tf\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.compat.v1.Session(config=config)\r\n```", "[This solution](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/config/experimental/set_memory_growth#for_example) worked for me.  (TF-GPU 2.0, Windows 10, GeForce RTX 2070)\r\n\r\n```\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nassert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```", "Add an additional datapoint.:\r\nrtx 2080ti, ubuntu18.04, cuda10.0, cudnn7\r\nIn my case it does not work with either tf1.14 and 1.15rc3", "@w4nderlust, for 1.14 and 1.15 you will want to continue to set the session config option `config.gpu_options.allow_growth = True`. Is that what you are reporting does not work, or just the `tf.config.experimental` mechanism?", "> @w4nderlust, for 1.14 and 1.15 you will want to continue to set the session config option `config.gpu_options.allow_growth = True`. Is that what you are reporting does not work, or just the `tf.config.experimental` mechanism?\r\n\r\nSorry should have been more precise, I'm reporting that without `config.gpu_options.allow_growth = True` it still doesn't work in my configuration with both 1.14 and 1.15rc3.", "I think I found a better workaround than the `config.gpu_options.allow_growth = True`.\r\n\r\nFor my setup (_RTX 2070_, docker image _tensorflow:1.15.0-gpu-py3_), setting config as shown below avoids the _CUDNN_STATUS_INTERNAL_ERROR_ while **still allocating the whole GPU memory**.\r\nThis is very useful for large models that would not fit into memory in `allow_growth` mode but just fits when the whole memory is allocated.\r\n\r\n**To allocate the whole memory on RTX:** \r\n`config.gpu_options.per_process_gpu_memory_fraction = 1.0`", "> **To allocate the whole memory on RTX:**\r\n> `config.gpu_options.per_process_gpu_memory_fraction = 1.0`\r\n\r\n@PoloShock \r\nI tried this with TF 2.0 and it does not seem to work.\r\nUbuntu18.04, RTX 2080, CUDA10, cudnn 7.6.\r\n", "For TF 2.0 the API for limiting GPU memory usage has changed.\r\n\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\n\r\ntf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]\r\n```", "@nluehr do you understand why this issue only shows up on RTX?  Could it be because we have other applications using it as a display GPU concurrently with TensorFlow?\r\n\r\nIt is difficult for me to debug this directly because I don't have access to an RTX GPU.", "@sanjoy I am running display on integrated gpu. No other apps on my single RTX gpu while running TensorFlow.", "I tried using that for tensorflow 2.0: \r\n```\r\n    config = tf.compat.v1.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    session = tf.compat.v1.Session(config=config)\r\n```\r\n\r\nIt fixes cudnn error on my rtx2080, but the training is as fast as my 1050Ti on my laptop!\r\nWhile training a CNN: \r\n\r\n```\r\nTue Nov 12 19:22:35 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.26       Driver Version: 440.26       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 2080    Off  | 00000000:2D:00.0 Off |                  N/A |\r\n|  0%   37C    P2    75W / 265W |   2904MiB /  7979MiB |     27%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1026      G   /usr/lib/Xorg                                200MiB |\r\n|    0      6420      G   cinnamon                                      43MiB |\r\n|    0     21073      C   /home/clementpoiret/anaconda3/bin/python    2647MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nAdding \r\n\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7000)]\r\n```\r\nDidn't solve the issue, without allow_growth I'm getting the cudnn error, and anyway my RTX is only using something like 3Gb or memory.\r\n\r\nAny idea ?\r\n\r\nI tried \r\n```\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    tf.config.experimental.set_memory_growth(gpus[0], True)\r\n    tf.config.experimental.set_virtual_device_configuration(\r\n        gpus[0],\r\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7900)])\r\n```\r\nbut cudnn is still throwing an error", "I also get this error working in the tensorflow 1.15.0-py3-gpu Docker image (Ubuntu 18.04) with two Titan V GPUs  (@sanjoy)  - not RTXs.  However, this error only seems to occur for me on my GPU0 which has Xorg and gnome-shell using GPU0 memory while GPU1 only has python using GPU mem and does not throw this error.  The error is also unfortunately intermittent -- sometimes I will be able to remove the docker container, recreate it with the same settings and same code, then then the error will go away.  Or not.\r\n\r\nI was able to fix it using the Keras backend interface with:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nallow_growth_session = tf.Session(config=config)\r\ntf.keras.backend.set_session(allow_growth_session)\r\n```\r\n\r\nFollowing is my nvidia-smi on both GPUs\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.26       Driver Version: 440.26       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN V             Off  | 00000000:01:00.0  On |                  N/A |\r\n| 46%   63C    P2    51W / 250W |   7936MiB / 12065MiB |     31%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN V             Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 52%   70C    P2   131W / 250W |  12014MiB / 12066MiB |     60%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1564      G   /usr/lib/xorg/Xorg                            56MiB |\r\n|    0      1607      G   /usr/bin/gnome-shell                          58MiB |\r\n|    0      2428      G   /usr/lib/xorg/Xorg                           442MiB |\r\n|    0      2574      G   /usr/bin/gnome-shell                         289MiB |\r\n|    0      3292      G   ...p/pycharm-professional/167/jbr/bin/java    12MiB |\r\n|    0      6794      G   anki                                          60MiB |\r\n|    0     10336      G   /usr/lib/firefox/firefox                       6MiB |\r\n|    0     16986      C   python                                      6981MiB |\r\n|    1      4057      C   python                                     12001MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n", "I'm having the same issue as @clementpoiret with TF 2.0 installed via conda. By using the `allow_growth` flag the issue disappears but that also makes the training very very slow, slower than what I had on TF 1.x... Eager first uh?", "@clementpoiret and @EKami , does it speed up your training if you replace  `config.gpu_options.allow_growth = True` with `config.gpu_options.per_process_gpu_memory_fraction = 0.8`? You can experiment to see what fraction makes the most use of your gpu.\r\n", "@synapse8 I don't see something equivalent in tensorflow 2.0's documentation, any way to do so with tf.config.experimental ?\r\n\r\nEdit: I'm gonna try to set memory this way, to see if it's solving the issue:\r\n\r\n```\r\nimport subprocess\r\nimport tensorflow as tf\r\n\r\n\r\ndef get_gpus_memory():\r\n    \"\"\"Get the max gpu memory.\r\n\r\n    Returns\r\n    -------\r\n    usage: list\r\n        Returns a list of total memory for each gpus.\r\n    \"\"\"\r\n    result = subprocess.check_output([\r\n        \"nvidia-smi\", \"--query-gpu=memory.total\",\r\n        \"--format=csv,nounits,noheader\"\r\n    ]).decode(\"utf-8\")\r\n\r\n    gpus_memory = [int(x) for x in result.strip().split(\"\\n\")]\r\n    return gpus_memory\r\n\r\n\r\ndef setup_gpus(allow_growth=True, memory_fraction=.9):\r\n    \"\"\"Setup GPUs.\r\n    \r\n    Parameters:\r\n    allow_growth (Boolean)\r\n    memory_fraction (Float): Set maximum memory usage, with 1 using\r\n        maximum memory\r\n    \"\"\"\r\n    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\n    if gpus:\r\n        try:\r\n            # Currently, memory growth needs to be the same across GPUs\r\n            for i, gpu in enumerate(gpus):\r\n                memory = get_gpus_memory()[i]\r\n\r\n                tf.config.experimental.set_memory_growth(gpu, allow_growth)\r\n\r\n                # Setting memory limit to max*fraction\r\n                tf.config.experimental.set_virtual_device_configuration(\r\n                    gpu, [\r\n                        tf.config.experimental.VirtualDeviceConfiguration(\r\n                            memory_limit=memory * memory_fraction)\r\n                    ])\r\n\r\n                logical_gpus = tf.config.experimental.list_logical_devices(\r\n                    \"GPU\")\r\n                print(len(gpus), \"Physical GPUs,\", len(logical_gpus),\r\n                      \"Logical GPUs\")\r\n        except RuntimeError as e:\r\n            # Memory growth must be set before GPUs have been initialized\r\n            print(e)\r\n\r\n```\r\n\r\nThis way we can conveniently just call `setup_gpus(True, .9)`", "@clementpoiret: Please note that the `tf.config.experimental.set_memory_growth` call is unnecessary since `tf.config.experimental.set_virtual_device_configuration` overrides that flag since it slices up the GPU memory and pre-allocates the allocated memory.", "This issue isn't limited to the RTX.  Or TF 2.0.  \r\n\r\nAdding:\r\n_from tensorflow.compat.v1 import ConfigProto\r\n from tensorflow.compat.v1 import InteractiveSession\r\n config = ConfigProto()\r\n config.gpu_options.allow_growth = True\r\n session = InteractiveSession(config=config)_\r\n\r\nSolves the \"Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\" issue with environment as follows:\r\n\r\n`nvidia-smi  \r\n| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |\r\n|   0  GeForce GT 1030  \r\n| 49%   67C    P0    N/A /  30W |   1957MiB /  2000MiB |     94%  \r\n`\r\n\r\n`python -c 'import tensorflow as tf; print(tf.__version__)'  \r\n1.14.0\r\n`\r\nCould this be a maximum contiguous block allocation issue with the NVIDIA drivers?  Where it's ok to allocate the same total amount of memory but in smaller blocks?\r\n", "Hi,\r\n\r\nI cannot reproduce this on my machine so I'll need some help root-causing this.  Do we have someone here who can reproduce the problem and is willing to do some hands-on debugging?\r\n\r\nAs a starting point I'd like to understand why [`MinSystemMemory`](https://github.com/tensorflow/tensorflow/blob/4596b3bc1ea85ea1bfdb6c0779b90eaab65ae252/tensorflow/core/common_runtime/gpu/gpu_device.cc#L804) does not preserve enough memory for cuDNN.  If someone with a setup that reproduces this issue can add some logging (as a local patch) to discover out the amount of memory returned by `MinSystemMemory` that would be great.  And does increasing the magic `0.05` number in `MinSystemMemory` help the situation?", "@sanjoy I have a version that exhibits this problem. How would I go about accessing MinSystemMemory or \"setting the magic 0.05 number\"? I have reverted to using cuda 9.1 for the most part, but I don't mind trying a few things.", "@odinsbane you'll have to build TensorFlow from source to do what I suggest below.\r\n\r\nFirst step is to add `LOG(INFO)` or `std::cerr` lines to [`MinSystemMemory`](https://github.com/tensorflow/tensorflow/blob/4596b3bc1ea85ea1bfdb6c0779b90eaab65ae252/tensorflow/core/common_runtime/gpu/gpu_device.cc#L804) to print out `available_memory` and the return value from `MinSystemMemory`.  Does `available_memory` agree with what `nvidia-smi` prints?  How much memory are we leaving for the system?\r\n\r\nSecondly, does increasing the [`0.05` magic number](https://github.com/tensorflow/tensorflow/blob/4596b3bc1ea85ea1bfdb6c0779b90eaab65ae252/tensorflow/core/common_runtime/gpu/gpu_device.cc#L818) to, say, `0.07` help at all?", "This one works! Thank you guys!\r\n```\r\nfrom keras.backend.tensorflow_backend import set_session\r\n$ import tensorflow as tf\r\n$ config = tf.ConfigProto()\r\n$ config.gpu_options.allow_growth = True\r\n$ config.log_device_placement = True\r\n$ sess = tf.Session(config=config)\r\n$ set_session(sess)\r\n```", "we are facing a similar issue on our RTX 2070 (Ubuntu 18.04, TF2) We tried different combinations of CUDA 10.0 and libcudnn7.x.x.x versions, but the error keeps showing up again. \r\nOn another machine we have a GTX 1080ti and this one runs without issue. \r\nThe nvidia-driver is 430.50 in both cases. ", "It is not caused by `tf.keras.utils.plot_model`, I remove it and this error still appears, but less frequently.\r\n~~Update: I find this only happens when I use `tf.keras.utils.plot_model`. I'm not sure whether this is a coincident. I'll keep trying.~~\r\n\r\n============\r\n\r\nI have a similar issue with RTX 2080 Ti on Ubuntu 18.04.3 LTS, tf 1.15, cuda 10.0.\r\n\r\nWhat is weird in my case is that **this only happens very occasionally, and once it happens, it will last for minuets to hours and then just disappear itself**.\r\n\r\nI tried all the above solutions and none can fix it immediately. I tried to do nothing and just wait, it will finally disappear.\r\n\r\nWhat I also tried and is not mentioned above:\r\n1. Remove `~/.nv` directory\r\n2. Simply reboot\r\n\r\nFYI, error logs\r\n```\r\n2019-12-21 14:47:30.785233: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-21 14:47:30.959825: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-21 14:47:31.722238: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-12-21 14:47:31.749524: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"train_cifar.py\", line 204, in <module>\r\n    main()\r\n  File \"train_cifar.py\", line 133, in main\r\n    validation_data=(x_test, output_test), callbacks=callbacks, verbose=0)\r\n  File \"/home/xxx/anaconda3/envs/tf-1-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 727, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/xxx/anaconda3/envs/tf-1-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\", line 603, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/xxx/anaconda3/envs/tf-1-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\", line 265, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n  File \"/home/xxx/anaconda3/envs/tf-1-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1017, in train_on_batch\r\n    outputs = self.train_function(ins)  # pylint: disable=not-callable\r\n  File \"/home/xxx/anaconda3/envs/tf-1-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\", line 3476, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/xxx/anaconda3/envs/tf-1-gpu/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1472, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node stem_layer/conv2d/Conv2D}}]]\r\n\t [[metrics/classifier_acc/Identity/_1749]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node stem_layer/conv2d/Conv2D}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```", "We are facing  relevant issues \r\n\r\nSystem specifications\r\n---\r\n\r\n- Ubuntu 18.04.3 LTS\r\n- RTX 2070\r\n- python 3.7.1\r\n-  tf-gpu 2.0.0\r\n-  V10.0.130 CUDA\r\n- libcudnn7 7.6.2 \r\n\r\nThe error is triggered when I try to use LSTM, GRU, RNN etc.\r\n\r\n Actual error \r\n---\r\n`2019-12-23 16:09:00.912238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-23 16:09:01.408990: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-12-23 16:09:01.409043: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1491 : Unknown: Fail to find the dnn implementation.`\r\n\r\n`  File \"/home/alex/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\", line 961, in call\r\n    **cudnn_lstm_kwargs)\r\n  File \"/home/alex/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\", line 1174, in cudnn_lstm\r\n    rnn_mode='lstm')\r\n  File \"/home/alex/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py\", line 109, in cudnn_rnn\r\n    ctx=_ctx)\r\n  File \"/home/alex/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py\", line 198, in cudnn_rnn_eager_fallback\r\n    attrs=_attrs, ctx=_ctx, name=name)\r\n  File \"/home/alex/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnknownError: Fail to find the dnn implementation. [Op:CudnnRNN]`\r\n\r\nApparent problem\r\n---\r\nAs it seems all my memory is eaten out pretty fast. The problems seems to come up only in gpu mode, the same code works fine with cpu \r\n\r\nTrials\r\n---\r\n\r\n* allow memory growth\r\n* create virtual device with limited memory \r\n\r\nBoth tries produce the same error.\r\n\r\n\r\n\r\n\r\nAny ideas?\r\n", "I can't make progress on this issue because I cannot reproduce it. If you're able to reliably reproduce this on your machine you can help; here's how: https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-560963770, https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-561366750", "> I can't make progress on this issue because I cannot reproduce it. If you're able to reliably reproduce this on your machine you can help; here's how: [#24496 (comment)](https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-560963770), [#24496 (comment)](https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-561366750)\r\n\r\nHi @sanjoy , I am very willing to help, but unfortunately I might cannot build tf from source because I am using my university's properties to do my experiments and my personal laptop is not equipped with a GPU. Is there any other ways to obtain the log we need? \r\n\r\nI found the following code on [stack overflow](https://stackoverflow.com/questions/36123740/is-there-a-way-of-determining-how-much-gpu-memory-is-in-use-by-tensorflow), could it help?\r\n\r\n```python\r\nfrom tensorflow.contrib.memory_stats.python.ops.memory_stats_ops import BytesInUse\r\nwith tf.device('/device:GPU:0'):  # Replace with device you are interested in\r\n  bytes_in_use = BytesInUse()\r\nwith tf.Session() as sess:\r\n  print(sess.run(bytes_in_use))\r\n```", "> Is there any other ways to obtain the log we need?\r\n\r\nI'll check in a `VLOG` statement to get this information.  Once that is done, will you be able to install and reproduce this with tf-nightly (with some extra flags, I'll let you know exactly which ones)?", "Surely, I can install a package on that computer if it is available on `pip` or `conda` and I use a virtual environment. I'll try to reproduce the error.", "> Surely, I can install a package on that computer if it is available on `pip` or `conda` and I use a virtual environment. I'll try to reproduce the error.\r\n\r\nCan you please install tf-nightly (so that it picks up the [commit](https://github.com/tensorflow/tensorflow/commit/497d9538e97ca00b045f59c8be93d2492b3e436c) that adds logging) and run with the enviroment variable `TF_CPP_VMODULE` set to `gpu_device=5`?  That should print out two lines like\r\n\r\n```\r\n2019-12-26 12:07:37.196206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:837] available_memory = 12319588352                                             \r\n2019-12-26 12:07:37.196221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] min_system_memory = 615979417                                              \r\n```\r\n\r\nCan you please report these numbers here?", "Sorry, my current code is not compatible with tf 2.0 (I use 1.15), I am trying to update it. Please give me some time.", "This problem seems related with my RTX2080, I have a desktop GTX1080, everything seems ok, then i use conda clone the conda enviroment to my RTX2080 notebook, I use tensorflow2.0.0-gpu . once application code use Conv2d, LSTM, GRU then this trouble come.\r\nbefore I use the following codes to solve this problem:\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n# Currently, memory growth needs to be the same across GPUs\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n    except RuntimeError as e:\r\n# Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\n\r\nbut since several days ago, the above method does not work any more\r\n", "I am having the same problem with gtx 960m", "Hi @sanjoy , I just got this output:\r\n```\r\n2019-12-30 17:38:23.824323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:837] available_memory = 10840309760\r\n2019-12-30 17:38:23.824328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] min_system_memory = 542015488\r\n```", "> Hi @sanjoy , I just got this output:\r\n> \r\n> ```\r\n> 2019-12-30 17:38:23.824323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:837] available_memory = 10840309760\r\n> 2019-12-30 17:38:23.824328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] min_system_memory = 542015488\r\n> ```\r\n\r\nThanks!\r\n\r\nUnfortunately this didn't help as much as I thought.  If I clamp `MinSystemMemory` on a local build to `542015488` (i.e. `min_system_memory = std::min(min_system_memory, 542015488ll)`) resnet (for instance) seems to work just fine, and I don't get any errors from cuDNN.", "@sanjoy I'm able to (mostly consistently) reproduce the issue on my end.\r\n\r\nRelevant messages from the latest nightly:\r\n\r\n### With memory growth explicitly allowed\r\n```\r\n2019-12-30 22:51:06.846774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nWARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .\r\n2019-12-30 22:51:08.851660: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-12-30 22:51:08.877811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2019-12-30 22:51:08.887672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2019-12-30 22:51:08.895277: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2019-12-30 22:51:08.906016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2019-12-30 22:51:08.913767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2019-12-30 22:51:08.921329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2019-12-30 22:51:08.930208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2019-12-30 22:51:08.941818: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-30 22:51:08.945713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\nTF GPU device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\r\n\r\n\r\n\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\r\nTensorflow Version: 2.1.0-dev20191230\r\nTensorflow_addons Version: 0.7.0-dev\r\n\r\n\r\n\r\nPreparing data\r\nLoading dataset\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 80/80 [00:03<00:00, 21.61it/s] \r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 68/68 [00:00<00:00, 447.32it/s] \r\nPerforming NLP\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 80/80 [00:00<00:00, 13332.71it/s] \r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 68/68 [00:00<?, ?it/s] \r\nTransforming dataset\r\nGenerating primitives and constructing vocabulary\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 80/80 [00:00<00:00, 139.11it/s] \r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 68/68 [00:00<00:00, 4249.86it/s] \r\nEncoding primitives\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16654/16654 [00:00<00:00, 33640.74it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 805/805 [00:00<00:00, 33538.43it/s] \r\n2019-12-30 22:51:22.970554: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-12-30 22:51:22.977228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2019-12-30 22:51:22.983571: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2019-12-30 22:51:22.986832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2019-12-30 22:51:22.990667: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2019-12-30 22:51:22.993801: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2019-12-30 22:51:22.996967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2019-12-30 22:51:23.002629: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2019-12-30 22:51:23.006072: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-30 22:51:23.010482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2019-12-30 22:51:23.557556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1087] TensorFlow compiled with CUDA 10.1 and cuDNN 7.6.5\r\n2019-12-30 22:51:23.560870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-30 22:51:23.564144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 \r\n2019-12-30 22:51:23.569159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N\r\n2019-12-30 22:51:23.571310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:837] available_memory = 7038160076\r\n2019-12-30 22:51:23.573861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] min_system_memory = 351908003\r\n2019-12-30 22:51:23.576728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1370] GPUDevice PlatformGpuId 0 TfGpuId 0 on bus 1 numa: 0 pci: 0000:08:00.0 DeviceLocality: bus_id: 1\r\nlinks {\r\n}\r\n\r\n2019-12-30 22:51:23.583814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6376 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2019-12-30 22:51:23.590034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:249] Created stream[0] = 000002093BAB9860\r\n2019-12-30 22:51:23.594885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:268] Created host_to_device_stream[0] = 000002093BAB9360\r\n2019-12-30 22:51:23.597951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:273] Created device_to_host_stream[0] = 000002093BABA960\r\n2019-12-30 22:51:23.600920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:289] Created device_to_device_stream[0] = 000002093BAB8EE0\r\n```\r\n\r\n### Without any changes to the GPU device's config\r\n```\r\n2019-12-30 22:54:47.762913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nWARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .\r\n2019-12-30 22:54:50.073199: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-12-30 22:54:50.100339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2019-12-30 22:54:50.105836: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2019-12-30 22:54:50.115940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2019-12-30 22:54:50.127341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2019-12-30 22:54:50.131871: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2019-12-30 22:54:50.139786: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2019-12-30 22:54:50.144940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2019-12-30 22:54:50.159197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-30 22:54:50.162685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\nTF GPU device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\r\n\r\n\r\n\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\r\nTensorflow Version: 2.1.0-dev20191230\r\nTensorflow_addons Version: 0.7.0-dev\r\n\r\n\r\n\r\nPreparing data\r\nLoading dataset\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 80/80 [00:03<00:00, 21.71it/s] \r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 68/68 [00:00<00:00, 433.07it/s] \r\nPerforming NLP\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 80/80 [00:00<00:00, 13332.18it/s] \r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 68/68 [00:00<?, ?it/s] \r\nTransforming dataset\r\nGenerating primitives and constructing vocabulary\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 80/80 [00:00<00:00, 140.34it/s] \r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 68/68 [00:00<00:00, 4249.55it/s] \r\nEncoding primitives\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16654/16654 [00:00<00:00, 33039.93it/s] \r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 805/805 [00:00<00:00, 33537.43it/s] \r\n2019-12-30 22:55:04.084880: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-12-30 22:55:04.088867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2019-12-30 22:55:04.094516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2019-12-30 22:55:04.097049: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2019-12-30 22:55:04.099754: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2019-12-30 22:55:04.102329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2019-12-30 22:55:04.105131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2019-12-30 22:55:04.108029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2019-12-30 22:55:04.110629: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-30 22:55:04.114339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2019-12-30 22:55:04.655119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1087] TensorFlow compiled with CUDA 10.1 and cuDNN 7.6.5\r\n2019-12-30 22:55:04.658124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-30 22:55:04.660826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0\r\n2019-12-30 22:55:04.662403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N\r\n2019-12-30 22:55:04.664213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:837] available_memory = 7038160076\r\n2019-12-30 22:55:04.666185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] min_system_memory = 351908003\r\n2019-12-30 22:55:04.668490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1370] GPUDevice PlatformGpuId 0 TfGpuId 0 on bus 1 numa: 0 pci: 0000:08:00.0 DeviceLocality: bus_id: 1\r\nlinks {\r\n}\r\n\r\n2019-12-30 22:55:04.672820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6376 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2019-12-30 22:55:04.677690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:249] Created stream[0] = 0000021EC0CF5840\r\n2019-12-30 22:55:04.679747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:268] Created host_to_device_stream[0] = 0000021EC0CF58C0\r\n2019-12-30 22:55:04.682343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:273] Created device_to_host_stream[0] = 0000021EC0CF5940\r\n2019-12-30 22:55:04.685266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:289] Created device_to_device_stream[0] = 0000021EC0CF59C0\r\n```\r\n\r\nEDIT: Model information, if it helps.\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\nFeature_1 (InputLayer)          [(None, 150)]        0\r\n__________________________________________________________________________________________________\r\nFeature_2 (InputLayer)          [(None, 150)]        0\r\n__________________________________________________________________________________________________\r\nembedding (Embedding)           (None, 150, 64)      5632        Feature_1[0][0]\r\n__________________________________________________________________________________________________\r\nembedding_1 (Embedding)         (None, 150, 64)      2944        Feature_2[0][0]\r\n__________________________________________________________________________________________________\r\nbidirectional (Bidirectional)   (None, 150, 128)     66048       embedding[0][0]\r\n__________________________________________________________________________________________________\r\nbidirectional_1 (Bidirectional) (None, 150, 128)     66048       embedding_1[0][0]\r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 150, 256)     0           bidirectional[0][0]\r\n                                                                 bidirectional_1[0][0]\r\n__________________________________________________________________________________________________\r\nbidirectional_2 (Bidirectional) (None, 64)           73984       concatenate[0][0]\r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 32)           2080        bidirectional_2[0][0]\r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 1)            33          dense[0][0]\r\n==================================================================================================\r\nTotal params: 216,769\r\nTrainable params: 216,769\r\nNon-trainable params: 0\r\n```", "A minimal example using TF 1.15, and I get this error. On RTX 2070 and NVIDIA 440.44 and CUDA version 10.2.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras.applications as applications\r\nimport tensorflow.keras.utils as utils\r\nimport numpy as np\r\n\r\nnum_samples = 1000\r\nheight = 224\r\nwidth = 224\r\nnum_classes = 1000\r\n\r\nmodel = applications.ResNet50(weights=None, input_shape=(height, width, 3), classes=num_classes)\r\n\r\nparallel_model = utils.multi_gpu_model(model, gpus=2, cpu_relocation=True)\r\nparallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\r\n\r\nx = np.random.random((num_samples, height, width, 3))\r\ny = np.random.random((num_samples, num_classes))\r\n\r\nparallel_model.fit(x, y, epochs=20, batch_size=256)\r\n\r\nprint('all done')\r\n```\r\n\r\n```\r\nTrain on 1000 samples\r\nEpoch 1/20\r\n2020-02-06 15:06:40.524918: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-06 15:06:41.291528: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-06 15:06:41.329183: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 822083584 exceeds 10% of system memory.\r\n2020-02-06 15:06:42.082319: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 851705856 exceeds 10% of system memory.\r\n2020-02-06 15:06:42.293092: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 822083584 exceeds 10% of system memory.\r\n2020-02-06 15:06:43.173764: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 822083584 exceeds 10% of system memory.\r\n2020-02-06 15:06:43.820074: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-02-06 15:06:44.390897: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 822083584 exceeds 10% of system memory.\r\n2020-02-06 15:06:45.839525: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-02-06 15:06:45.856793: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-02-06 15:06:45.883423: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"./test_tf.py\", line 19, in <module>\r\n    parallel_model.fit(x, y, epochs=20, batch_size=256)\r\n  File \"/nix/store/520352w3m8lyj2zgv647qfqrws5q798n-python3.7-tensorflow-gpu-1.15.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 727, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/nix/store/520352w3m8lyj2zgv647qfqrws5q798n-python3.7-tensorflow-gpu-1.15.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 675, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/nix/store/520352w3m8lyj2zgv647qfqrws5q798n-python3.7-tensorflow-gpu-1.15.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 394, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/nix/store/520352w3m8lyj2zgv647qfqrws5q798n-python3.7-tensorflow-gpu-1.15.0/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\", line 3476, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/nix/store/520352w3m8lyj2zgv647qfqrws5q798n-python3.7-tensorflow-gpu-1.15.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1472, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t[[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t[[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]\r\n\t[[training/RMSprop/gradients/gradients/Switch_482/_3893]]\r\n0 successful operations.\r\n1 derived errors ignored.\r\n```", "I want to point out in a separate issue https://github.com/tensorflow/tensorflow/issues/36501 that while using those options enables the code to run, observing the actual memory usage of the GPUs shows that it is not even really doing incremental memory usage. So the option above fixes the error, but it doesn't actually do what it claims to be doing. I used to use the same model back in older TF versions like 1.2... etc and those did actual incremental memory allocation.", "I have the same problems as everyone here! After having installed tf 2.1 I couldn't get a simple MNIST example to run without adding memory growth to the GPU. I use a **2080 ti**. \r\n\r\nThe major problem I face is that I cannot run tensorflow-probability together with tf 2.1 without getting the cursed CUDNN internal error, even with memory growth added to the code. I have tried installing tf 2.0, CUDA 10.0 and CUDA 10.1, different CUDNN versions. I managed to fix the simple MNIST example to work without the growth after completely reinstalling my ubuntu but not the tensorflow probability example. I finally tried using a tensorflow official nightly docker and still got the same error when using tensorflow probability (tf 2.2 inside container). Everything runs fine on CPU. I have also tried running the same docker on a machine with 1080 ti and that **worked...** There is definitely something wrong with the **RTX series** I feel.\r\n\r\n\r\nerror with tf docker and tensorflow-probability example and extra cudnn debug info:\r\n```\r\nTF VERSION: 2.2.0-dev20200208\r\n2020-02-11 08:51:05.891560: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-02-11 08:51:05.912465: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3696000000 Hz\r\n2020-02-11 08:51:05.913040: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x57b1fd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-02-11 08:51:05.913052: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-02-11 08:51:05.914414: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-02-11 08:51:05.975016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.975364: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5679220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-02-11 08:51:05.975376: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-02-11 08:51:05.975477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.975744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-02-11 08:51:05.975865: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-11 08:51:05.976745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-11 08:51:05.977582: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-11 08:51:05.977722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-11 08:51:05.978636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-11 08:51:05.979165: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-11 08:51:05.981150: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-11 08:51:05.981216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.981528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.981792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-02-11 08:51:05.981812: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-11 08:51:05.982323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-11 08:51:05.982331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 \r\n2020-02-11 08:51:05.982335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N \r\n2020-02-11 08:51:05.982395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.982687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.982959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/device:GPU:0 with 9604 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-02-11 08:51:05.983594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.983864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-02-11 08:51:05.983881: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-11 08:51:05.983889: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-11 08:51:05.983896: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-11 08:51:05.983904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-11 08:51:05.983912: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-11 08:51:05.983920: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-11 08:51:05.983928: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-11 08:51:05.983961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.984238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.984497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-02-11 08:51:05.984508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-11 08:51:05.984512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 \r\n2020-02-11 08:51:05.984516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N \r\n2020-02-11 08:51:05.984563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.984842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.985099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/device:GPU:0 with 9604 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nSUCCESS: Found GPU: /device:GPU:0\r\n2020-02-11 08:51:05.989382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.989649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-02-11 08:51:05.989663: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-11 08:51:05.989671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-11 08:51:05.989678: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-11 08:51:05.989684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-11 08:51:05.989691: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-11 08:51:05.989700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-11 08:51:05.989709: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-11 08:51:05.989744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.990021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.990347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-02-11 08:51:05.990544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.990807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-02-11 08:51:05.990820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-11 08:51:05.990828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-11 08:51:05.990834: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-11 08:51:05.990841: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-11 08:51:05.990848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-11 08:51:05.990854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-11 08:51:05.990861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-11 08:51:05.990892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.991171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.991426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-02-11 08:51:05.991437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-11 08:51:05.991441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 \r\n2020-02-11 08:51:05.991444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N \r\n2020-02-11 08:51:05.991486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.991763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-11 08:51:05.992022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9604 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/linalg/linear_operator_lower_triangular.py:158: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDo not pass `graph_parents`.  They will  no longer be used.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/linalg/linear_operator_lower_triangular.py:158: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDo not pass `graph_parents`.  They will  no longer be used.\r\n2020-02-11 08:51:06.822991: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nEpoch 1/15\r\n2020-02-11 08:51:07.907445: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-11 08:51:09.832694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n\r\nI! CuDNN (v7604) function cudnnCreate() called:\r\ni! Time: 2020-02-11T08:51:09.832722 (0d+0h+0m+4s since start)\r\ni! Process=205; Thread=269; GPU=NULL; Handle=NULL; StreamId=NULL.\r\n\r\n2020-02-11 08:51:10.409902: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\nI! CuDNN (v7604) function cudnnCreate() called:\r\ni! Time: 2020-02-11T08:51:10.410012 (0d+0h+0m+5s since start)\r\ni! Process=205; Thread=269; GPU=NULL; Handle=NULL; StreamId=NULL.\r\n\r\n2020-02-11 08:51:10.417952: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n      1/Unknown - 4s 4s/stepTraceback (most recent call last):\r\n  File \"VAE_MNIST_tfp.py\", line 150, in <module>\r\n    validation_data=eval_dataset)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 718, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 341, in fit\r\n    total_epochs=epochs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 576, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 640, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 2414, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1660, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1741, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node model/conv2d/Conv2D (defined at VAE_MNIST_tfp.py:150) ]] [Op:__inference_distributed_function_4291]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node model/conv2d/Conv2D:\r\n model/lambda/sub (defined at VAE_MNIST_tfp.py:98)\r\n\r\nFunction call stack:\r\ndistributed_function\r\n```\r\n\r\n\r\n\r\n\r\n", "@sanjoy I have the same issue with RTX 2080 and can build from source if needed.", "> @odinsbane you'll have to build TensorFlow from source to do what I suggest below.\r\n> \r\n> First step is to add `LOG(INFO)` or `std::cerr` lines to [`MinSystemMemory`](https://github.com/tensorflow/tensorflow/blob/4596b3bc1ea85ea1bfdb6c0779b90eaab65ae252/tensorflow/core/common_runtime/gpu/gpu_device.cc#L804) to print out `available_memory` and the return value from `MinSystemMemory`. Does `available_memory` agree with what `nvidia-smi` prints? How much memory are we leaving for the system?\r\n> \r\n> Secondly, does increasing the [`0.05` magic number](https://github.com/tensorflow/tensorflow/blob/4596b3bc1ea85ea1bfdb6c0779b90eaab65ae252/tensorflow/core/common_runtime/gpu/gpu_device.cc#L818) to, say, `0.07` help at all?\r\n\r\ncan confirm that building from source with changing the magic number  [`0.05` magic number](https://github.com/tensorflow/tensorflow/blob/4596b3bc1ea85ea1bfdb6c0779b90eaab65ae252/tensorflow/core/common_runtime/gpu/gpu_device.cc#L818) to `0.1` seems to fix the issue (at least for 1.15.2)!\r\n\r\n", "In an ocean of noisy post the minimum system memory magic number totally seems logical. Thanks for sharing!", "@chsigg Any suggestions?  Maybe we can try to initialize cuDNN, cuBLAS and other NVIDIA libraries _before_ we reserve all of the GPU memory?\r\n\r\nWe can also try to enable `allow_growth` by default, but that's going to take time.", "> This problem seems related with my RTX2080, I have a desktop GTX1080, everything seems ok, then i use conda clone the conda enviroment to my RTX2080 notebook, I use tensorflow2.0.0-gpu . once application code use Conv2d, LSTM, GRU then this trouble come.\r\n> before I use the following codes to solve this problem:\r\n> gpus = tf.config.experimental.list_physical_devices('GPU')\r\n> if gpus:\r\n> try:\r\n> \r\n> # Currently, memory growth needs to be the same across GPUs\r\n> ```\r\n>     for gpu in gpus:\r\n>         tf.config.experimental.set_memory_growth(gpu, True)\r\n>         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n>         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n> except RuntimeError as e:\r\n> ```\r\n> \r\n> # Memory growth must be set before GPUs have been initialized\r\n> ```\r\n>     print(e)\r\n> ```\r\n> \r\n> but since several days ago, the above method does not work any more\r\n\r\nHave been trying to run the [lambda Tensorflow2-tutorial basic-image-classification](https://github.com/lambdal/TensorFlow2-tutorial) code for days and getting the same cudnn handle error until I tried your solution. It is finally running now on RTX 2070 Max Q and using minimal GPU memory.\r\n\r\n", "I also meet this problem\r\nanacondacloud install tensorflow-gpu2.0\r\n\r\nrtx2070s\r\ntensorflow-gpu.2.0.0\r\ncuda 10.0.13\r\ncudnn 7.6.5\r\nCould not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nFailed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\r\n", "> I also meet this problem\r\n> anacondacloud install tensorflow-gpu2.0\r\n> \r\n> rtx2070s\r\n> tensorflow-gpu.2.0.0\r\n> cuda 10.0.13\r\n> cudnn 7.6.5\r\n> Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\r\nDid you insert:\r\n\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\ntry:\r\n    for gpu in gpus:\r\n        tf.config.experimental.set_memory_growth(gpu, True)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\nexcept RuntimeError as e:\r\n    print(e)\r\n```\r\nat the top of your entry code?", "After quite some time experimenting with an apparently different problem failing with tf.signal.stft\r\nI finally came across this thread and tried the solution allowing the memory growth. It solved my problem as well.\r\nI have installed tensorflow-gpu=2.1 with cudatoolkit=10.1 from anaconda, but tried as well installing \r\ntensorflow-gpu via pip with exactly the same result. I can reproduce this under linux-ubuntu 18.04 and debian 9.12 with the cards\r\n\r\n\r\n       GeForce GTX 1050 Ti with Max-Q Design   \r\n       GeForce GTX 1050 Ti\r\n       GeForce RTX 2080 Ti\r\n\r\nI also tried two other cards in our lab   \r\n\r\n      GeForce GTX 1080 Ti\r\n      TITAN Xp COLLECTORS EDITION\r\n\r\nwhere the code runs fine with and without allowing memory growth\r\n\r\nMy minimal problem is below. Interestingly the problem is not conv2d. I can change the order of these three commands and it is always the third that one fails.\r\n\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus and len(sys.argv)> 1 and sys.argv[1].startswith(\"-a\"):\r\n    print(\"allowing growth\")\r\n    growth = True\r\nelse:\r\n    print(\"nogrowth\")\r\n    growth = False\r\n\r\ntry:\r\n    for gpu in gpus:\r\n        tf.config.experimental.set_memory_growth(gpu, growth)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\nexcept RuntimeError as e:\r\n    print(e)\r\n    \r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.matmul(tf.zeros((2,2,2)), tf.zeros((2,2,2)))\r\ntf.nn.conv2d(tf.zeros((2,20,20,20), dtype=tf.float32),\r\n                                         filters=tf.zeros((2,2,20,20), dtype=tf.float32),\r\n            strides=(1,1,1,1), padding=\"VALID\")\r\nprint(\"done\")\r\n```\r\n\r\n", "> > \u6211\u4e5f\u9047\u5230\u8fd9\u4e2a\u95ee\u9898\r\n> > anacondacloud install tensorflow-gpu2.0\r\n> > rtx2070s \r\n> > tensorflow-gpu.2.0.0 \r\n> > cuda 10.0.13 \r\n> > cudnn 7.6.5 \r\n> > \u65e0\u6cd5\u521b\u5efacudnn\u53e5\u67c4\uff1aCUDNN_STATUS_INTERNAL_ERROR \r\n> > \u65e0\u6cd5\u83b7\u53d6\u5377\u79ef\u7b97\u6cd5\u3002\u8fd9\u53ef\u80fd\u662f\u56e0\u4e3acuDNN\u521d\u59cb\u5316\u5931\u8d25\uff0c\u6240\u4ee5\u8bf7\u5c1d\u8bd5\u67e5\u770b\u4e0a\u9762\u662f\u5426\u6253\u5370\u4e86\u8b66\u544a\u65e5\u5fd7\u6d88\u606f\u3002\r\n> \r\n> \u60a8\u662f\u5426\u63d2\u5165\uff1a\r\n> \r\n> ```\r\n> gpus = tf.config.experimental.list_physical_devices('GPU')\r\n> if gpus:\r\n> try:\r\n>     for gpu in gpus:\r\n>         tf.config.experimental.set_memory_growth(gpu, True)\r\n>         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n>         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n> except RuntimeError as e:\r\n>     print(e)\r\n> ```\r\n> \r\n> \u5728\u60a8\u8f93\u5165\u4ee3\u7801\u7684\u9876\u90e8\uff1f\r\n\r\nyeah,I solved this problem like this way.Thanks!!", "I had the same problem and `allow_growth = True` was the solution. BUT, for TensorFlow 2, in order to do that you need to add the following lines:\r\n\r\n`gpu_devices = tf.config.experimental.list_physical_devices('GPU')\r\nfor device in gpu_devices:\r\n    tf.config.experimental.set_memory_growth(device, True)`\r\n\r\nThanks to user  @opcecco in this issue: https://github.com/tensorflow/tensorflow/issues/25446", "> Interestingly the problem is not conv2d. I can change the order of these three commands and it is always the third that one fails.\r\n\r\n@roebel Can you please attach logs for a few different six permutations?\r\n\r\nAnd what happens if you change the program to (say):\r\n\r\n```\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.matmul(tf.zeros((2,2,2)), tf.zeros((2,2,2)))\r\ntf.nn.conv2d(tf.zeros((2,20,20,20), dtype=tf.float32),\r\n                                         filters=tf.zeros((2,2,20,20), dtype=tf.float32),\r\n            strides=(1,1,1,1), padding=\"VALID\")\r\n```\r\n\r\nDoes the failure still happen at the `conv2d` or does it happen at the third `stft`?", "@sanjoy sure here three variations of the script above changing the order of commands and  a fourth variant that starts with 4 stft and ends with conv2d\r\n\r\nThe four different logs use the script from\r\nhttps://github.com/tensorflow/tensorflow/issues/24496#issuecomment-593098386\r\nreplacing the last four lines.\r\n\r\nIn short the results depending on the order:\r\n\r\n stft->blas->conv2d fails when executing conv2d\r\n conv2d->stft->blas fails when executing stft (so not the third, but blas seems to be loaded already for conv2d\r\nmatmul-> conv2d-> stft fails when executing STFT\r\nstft->-stft->-stft->stft->matmul-> conv2d fails when conv2d is executed. Please see the logs below.\r\n\r\nDon't mind asking for other variants if needed.\r\n\r\n\r\n\r\nconv2d last:\r\n```\r\ntf.matmul(tf.zeros((2,2,2)), tf.zeros((2,2,2)))\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.nn.conv2d(tf.zeros((2,20,20,20), dtype=tf.float32),\r\n                                         filters=tf.zeros((2,2,20,20), dtype=tf.float32),\r\n            strides=(1,1,1,1), padding=\"VALID\")\r\nprint(\"done\")\r\n```\r\n\r\n[log.conv2d.last.txt](https://github.com/tensorflow/tensorflow/files/4299225/log.conv2d.last.txt)\r\n\r\nmatmul last\r\n```\r\ntf.nn.conv2d(tf.zeros((2,20,20,20), dtype=tf.float32),\r\n                                         filters=tf.zeros((2,2,20,20), dtype=tf.float32),\r\n            strides=(1,1,1,1), padding=\"VALID\")\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.matmul(tf.zeros((2,2,2)), tf.zeros((2,2,2)))\r\nprint(\"done\")\r\n```\r\n[log.matmul.last.txt](https://github.com/tensorflow/tensorflow/files/4299231/log.matmul.last.txt)\r\n\r\nstft last\r\n```\r\ntf.matmul(tf.zeros((2,2,2)), tf.zeros((2,2,2)))\r\ntf.nn.conv2d(tf.zeros((2,20,20,20), dtype=tf.float32),\r\n                                         filters=tf.zeros((2,2,20,20), dtype=tf.float32),\r\n            strides=(1,1,1,1), padding=\"VALID\")\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\nprint(\"done\")\r\n\r\n```\r\n[log.stft.last.txt](https://github.com/tensorflow/tensorflow/files/4299243/log.stft.last.txt)\r\n\r\n\r\n4 stft first conv2d last:\r\n```\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.matmul(tf.zeros((2,2,2)), tf.zeros((2,2,2)))\r\ntf.nn.conv2d(tf.zeros((2,20,20,20), dtype=tf.float32),\r\n                                         filters=tf.zeros((2,2,20,20), dtype=tf.float32),\r\n            strides=(1,1,1,1), padding=\"VALID\")\r\nprint(\"done\")\r\n```\r\n[log.multi_stft.first.txt](https://github.com/tensorflow/tensorflow/files/4299241/log.multi_stft.first.txt)\r\n\r\nMany thanks\r\n", "I got the same problem with following configuration:\r\nTensorFlow installed from (source or binary): r1.13.1,r.1.13.2,r1.14\r\nPython version: 3.6.1\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: CUDA 10 with cuDNN 7.4.1\r\nGPU model and memory: RTX 2070 8GB.\r\n\r\nI sovled this problem with:\r\nTensorFlow installed from (source or binary): r1.12.0\r\nPython version: 3.6.9\r\nGCC/Compiler version: 4.8\r\nCUDA/cuDNN version: CUDA 9.0 with cuDNN 7.1.4\r\nGPU model and memory: RTX 2070 8GB.\r\nHope helpful to you", "I've also faced such a problem, which was solved by adding an environment variable TF_FORCE_GPU_ALLOW_GROWTH=true.\r\n\r\nThe configuration is the following:\r\nWindows 10\r\nTensorflow compiled from source r2.0\r\nBazel: 0.26.1\r\nC++ compiler: MSVC 2017\r\nCUDA: 10\r\ncuDNN: 7.6.5", "intel4930 cpu, nvidia titan XP pascal\r\nUbuntu 18.04.4 , miniconda latest, \r\n`!conda list | grep \"cud\" gives\r\n```\r\n    cudatoolkit               10.1.243             h6bb024c_0  \r\n    cudnn                     7.6.5                cuda10.1_0  \r\n\r\n```\r\n`!conda list | grep \"tensor\"`` gives\r\n\r\n```\r\ntensorboard               2.1.0                     py3_0  \r\ntensorflow                2.1.0           gpu_py37h7a4bb67_0  \r\ntensorflow-base           2.1.0           gpu_py37h6c5654b_0  \r\ntensorflow-estimator      2.1.0              pyhd54b08b_0  \r\ntensorflow-gpu            2.1.0                h0d30ee6_0  \r\n```\r\nfirst cell in jupyter notebook is:\r\n\r\n```\r\nimport tensorflow as tf\r\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\r\nfor device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)\r\n\r\n```\r\nmodel is a variational autoencoder with Total params: 112,269\r\nx_train.shape, y_train.shape, x_test.shape, y_test.shape gives\r\n`    ((106496, 32, 32, 1), (106496,), (12288, 32, 32, 1), (12288,))`\r\n\r\ncode includes:\r\n```\r\nbatch_size=64\r\nvar_auto_encoder.fit(x_train, x_train, verbose=1, \r\n                 batch_size=batch_size, epochs=100,\r\n                 validation_data=(x_test, x_test))\r\n\r\n```\r\n\r\nand it fails. console shows\r\n\r\n```\r\n2020-03-18 15:46:03.019451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-03-18 15:46:03.179472: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-03-18 15:46:03.566267: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-03-18 15:46:03.569842: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-03-18 15:46:03.569907: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d/Conv2D}}]]\r\n2020-03-18 15:46:03.573206: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\n```\r\n\r\nI f instead of the first cell as noted above , I use\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.2\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\n```\r\n\r\nthen I get this error\r\n\r\n```\r\n\r\n2020-03-18 15:55:43.050094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-03-18 15:55:43.050123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-03-18 15:55:43.050150: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-03-18 15:55:43.050177: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-03-18 15:55:43.050209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-03-18 15:55:43.050246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-03-18 15:55:43.050273: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-03-18 15:55:43.050337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-03-18 15:55:43.050720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-03-18 15:55:43.051063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-03-18 15:55:43.051097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-03-18 15:55:43.051108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-03-18 15:55:43.051116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-03-18 15:55:43.051201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-03-18 15:55:43.051573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-03-18 15:55:43.051915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 16 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-03-18 15:56:07.877181: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-03-18 15:56:07.882424: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-03-18 15:56:07.886148: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-03-18 15:56:07.889830: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\n\r\nWhy am I having the problem if I allow memory growth? Do I need to reboot to reinitialize the gpu?\r\n```", "Interestingly, during my struggles, I got a message from a red 'no entry' sign in my menubar that said 'error broken count you have unmet dependenceis'\r\nI ran software update and it wants to remove libcudnn7-dev and libcudnn7-doc\r\nas well as upgrade 57 other libraries having to do with linux \r\n\r\n\r\nEDIT: After reboot the model seems to train successfully using this:\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.2\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\n```\r\nor this:\r\n\r\n```\r\nimport tensorflow as tf\r\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\r\nfor device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)\r\n```\r\nmemory utilization on the gpu is <700 MB with batch size 16 and\r\n~1 gigabyte with batch size 256 (which trains 3x faster)\r\n", "> I did try compiling from source, but ran into the same issue. I was finally able to fix my problem was setting `config.gpu_options.allow_growth = True`.\r\n\r\nBut if i met this issue in command line, how to add these codes ?", "> > I also meet this problem\r\n> > anacondacloud install tensorflow-gpu2.0\r\n> > rtx2070s\r\n> > tensorflow-gpu.2.0.0\r\n> > cuda 10.0.13\r\n> > cudnn 7.6.5\r\n> > Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> > Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n> \r\n> Did you insert:\r\n> \r\n> ```\r\n> gpus = tf.config.experimental.list_physical_devices('GPU')\r\n> if gpus:\r\n> try:\r\n>     for gpu in gpus:\r\n>         tf.config.experimental.set_memory_growth(gpu, True)\r\n>         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n>         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n> except RuntimeError as e:\r\n>     print(e)\r\n> ```\r\n> \r\n> at the top of your entry code?\r\n\r\nI had the exact same problem as above. `Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR`\r\n\r\nThe solution from @robosmith fix my problem completely!\r\n\r\nMy specs:\r\nRTX 2070\r\nUbuntu 18.04 LTE\r\nTensorflow 2.1.0\r\nKeras 2.3.0\r\ncudnn 7.6.5\r\ncuda10.1.0\r\nconda 4.8.3\r\npython 3.7.7\r\n\r\nBuilt via `conda install tensorflow-gpu keras`\r\n\r\nThank you so much! This is the **first** time that I've gotten TF-2 to work at all! And TF-1 stopped working altogether, which is why I decided to upgrade and 'see what happens'!\r\n\r\nThank you!", "> config.gpu_options.allow_growth = True\r\n\r\nwhen you use tensorflow 2.0 , you can use\r\n`tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)`\r\nthis code is after `import tensorflow as tf`  but before your code. ", "> I did try compiling from source, but ran into the same issue. I was finally able to fix my problem was setting `config.gpu_options.allow_growth = True`.\r\n\r\nThis code is shared to make it faster available for both tensorflow and keras users.\r\nsource from [here](https://kobkrit.com/using-allow-growth-memory-option-in-tensorflow-and-keras-dc8c8081bc96)\r\n\r\n    # Tensorflow\r\n    import tensorflow as tf\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    session = tf.Session(config=config, ...)\r\n\r\n \r\n    #And for Keras\r\n    from keras.callbacks import ModelCheckpoint\r\n    from keras.models import Model, load_model, save_model, Sequential\r\n    from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\r\n    from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\r\n    from keras.optimizers import Adam\r\n    from keras.backend.tensorflow_backend import set_session\r\n    import tensorflow as tf\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\n    config.log_device_placement = True  # to log device placement (on which device the operation ran)\r\n    sess = tf.Session(config=config)\r\n    set_session(sess)  # set this TensorFlow session as the default session for Keras\r\n\r\n\r\n", "Just wanted to chime in and say that the problem is still there;\r\n\r\nMy specs: \r\nUbuntu 20.04\r\nNVIDIA RTX 2070\r\nNvidia_driver 440.64\r\nTensorflow-gpu 2.0.1 (Installed through conda, which automatically installs Cudatoolkit and CuDNN in same env)\r\ncudatoolkit 10.1.243\r\ncudnn 7.6.5\r\n\r\nProblem is solved by `tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)`\r\n\r\nHowever this seems more like a work-around than an actual fix, and a lot of people have 20XX cards these days. Probably there should be an update in which this issue is addressed. \r\n\r\nUpdate: Since I'm dual-booting, I tried to check for windows as well. Problem persists there.\r\nWindows 10\r\nNvidia-driver 445.87\r\nOther than that everything is similar\r\n\r\n\r\n", "Installing the latest driver (445.87) for my RTX 2080 solved this issue for me.", "@NBouman That is interesting but for me on Ubuntu 18.04 with GeForce GTX 1050 TI, I just updated to the last available driver 440.82. Still allowing memory growth is required to make it work.", "> Installing the latest driver (445.87) for my RTX 2080 solved this issue for me.\r\n\r\n@NBouman What OS are you using? I'm on Ubuntu 20.40, and the latest available driver I could find is 440.82, and, like @roebel, the problem persists.", "@roebel @eduardoscsouza I am on Windows 10 with the machine that earlier had this issue.", "> Just wanted to chime in and say that the problem is still there;\r\n> \r\n> My specs:\r\n> Ubuntu 20.04\r\n> NVIDIA RTX 2070\r\n> Nvidia_driver 440.64\r\n> Tensorflow-gpu 2.0.1 (Installed through conda, which automatically installs Cudatoolkit and CuDNN in same env)\r\n> cudatoolkit 10.1.243\r\n> cudnn 7.6.5\r\n> \r\n> Problem is solved by `tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)`\r\n> \r\n> However this seems more like a work-around than an actual fix, and a lot of people have 20XX cards these days. Probably there should be an update in which this issue is addressed.\r\n> \r\n> Update: Since I'm dual-booting, I tried to check for windows as well. Problem persists there.\r\n> Windows 10\r\n> Nvidia-driver 445.87\r\n> Other than that everything is similar\r\n\r\nFor tensorflow 2.0.0 worked with:\r\n```tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0],True)```\r\n\r\nThank you!!! thousands of thank you!!!!!!!!!!!!!!!", "OS: ubuntu 18.04 lts\r\n\r\nDriver Version: 435.21\r\n\r\nCUDA: cudatoolkit 10.1\r\n\r\nCUDNN: cudnn-7.6.5-cuda10.1_0\r\n\r\nI used anaconda install tensorflow\r\n\r\n```\r\nconda create -n tf-gpu tensorflow-gpu\r\n```\r\n\r\n**the cudatoolkit and cudnn are auto-install by anaconda through the command before.**\r\n\r\nI have the same question, The error:\r\n\r\n```\r\ncoreClock: 1.5315GHz coreCount: 3 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-05-12 17:58:44.119679: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-05-12 17:58:44.119694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-05-12 17:58:44.119707: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-05-12 17:58:44.119719: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-05-12 17:58:44.119732: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-05-12 17:58:44.119744: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-05-12 17:58:44.119756: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-05-12 17:58:44.119819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-12 17:58:44.120069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-12 17:58:44.120277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-05-12 17:58:44.120308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-05-12 17:58:44.174976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-05-12 17:58:44.175003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-05-12 17:58:44.175012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-05-12 17:58:44.175136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-12 17:58:44.175392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-12 17:58:44.175624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-12 17:58:44.175844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1439 MB memory) -> physical GPU (device: 0, name: GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-05-12 17:58:44.177113: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55abc3d20b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-05-12 17:58:44.177129: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX150, Compute Capability 6.1\r\n2020-05-12 17:58:44.177749: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 376320000 exceeds 10% of system memory.\r\n2020-05-12 17:58:44.787493: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 376320000 exceeds 10% of system memory.\r\nWARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\n2020-05-12 17:58:45.311821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-05-12 17:58:45.467966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-05-12 17:58:45.904025: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-05-12 17:58:45.913861: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-05-12 17:58:45.913978: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node my_model/conv2d/Conv2D}}]]\r\n\r\n```\r\n\r\n", "So we have here a problem that is unsolved (besides a workaround that is against official recommendations to not use memory growth for more efficient memory handling). There has not been much feedback by the dev team. I wonder why?\r\n\r\nThis bug seems to affect quite a variety of tensorflow versions (1.13, 2.0, 2.1), If I saw correctly all problems are reported to happen with cuda 10. The code runs fine on many cards but not on others. \r\nCould somebody of the dev team tell us whether this hints towards a problem in the cuda driver more than in the tensorflow layer ? In that case it would certainly be helpful to transmit the bug report to the NVIDIA support pages. Wouldn't it?\r\n\r\nCould somebody from the tensorflow dev team comment on how they see this bug ? Is there anybody looking into this ?\r\n", "Have people been checking if there are two CuDNN 7 shared libraries on the path or LD library path. There are no minor or patch numbers in this library but version mismatches can lead to this error message.", "I opened a bug report at NVIDIA, I'll let you know what comes out of that.", "@samhodge \r\nIndeed there are many versions of libcudnn installed, each anaconda env has its own version.\r\nNormally anaconda installs with rpath properly set up so it is rather difficult to not get the right libraries.\r\n\r\nI have made an strace and grepped the libraries that are opened when it fails\r\nThey consistently come from the anaconda env dir that hosts the tensorflow package (see below). \r\nBesides libcuda that is version 440.82 and that I have  compiled with the NVIDIA installer. \r\n\r\nI can set the LD_LIBRARY_PATH to one of the other anaconda env lib dirs with different cudatoolkits and different libcudnn, the trace remains the same. \r\nNote as well that it is not lbcudnn that poses the problem. It is always the third libcuxyz library that is \r\nused and this only on specifc GPUs (I have used the same install script on different machines with different GPUs, some do work some don't) and they work all if memory growth is enabled.\r\n\r\n```\r\n(tf2.1) m3088.roebel: (test_sd) 510> grep open trace.log  | grep libcu | grep -v -- -1\r\nopenat(AT_FDCWD, \"/usr/lib/x86_64-linux-gnu/libcuda.so.1\", O_RDONLY|O_CLOEXEC) = 4\r\nopenat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcudart.so.10.1\", O_RDONLY|O_CLOEXEC) = 11\r\nopenat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcublas.so.10\", O_RDONLY|O_CLOEXEC) = 11\r\nopenat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../.././libcublasLt.so.10\", O_RDONLY|O_CLOEXEC) = 11\r\nopenat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcufft.so.10\", O_RDONLY|O_CLOEXEC) = 11\r\nopenat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcurand.so.10\", O_RDONLY|O_CLOEXEC) = 11\r\nopenat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcusolver.so.10\", O_RDONLY|O_CLOEXEC) = 11\r\nopenat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcusparse.so.10\", O_RDONLY|O_CLOEXEC) = 11\r\nopenat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcudnn.so.7\", O_RDONLY|O_CLOEXEC) = 11\r\n```", "I got the same problem on Ubuntu 20.04 with a GeForce RTX 2060 SUPER. A NN with dense layers works well. But with CNN layers I'm getting `Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.`\r\nAdding `tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)` makes no difference to the error.\r\nI followed the installation according to [https://www.tensorflow.org/install/gpu](url) and `nvidia-smi` shows:\r\n`Driver Version: 440.64.00\r\nCUDA Version: 10.2`\r\nMy conda env has:\r\n```\r\ncudatoolkit               10.1.243             h6bb024c_0  \r\ncudnn                     7.6.5                cuda10.1_0  \r\ntensorflow-gpu            2.1.0                h0d30ee6_0\r\n```\r\nIn a conda env with tf 1.15 I am getting the same error. It would be great if this could be fixed.\r\n\r\n## Update\r\nAfter using `export TF_FORCE_GPU_ALLOW_GROWTH=true` it all works. I was of the impression that the `tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)` would to the same thing, but that's not the case. I think this should be clearly stated on the TensorFlow GPU support webpage.", "> @samhodge\r\n> Indeed there are many versions of libcudnn installed, each anaconda env has its own version.\r\n> Normally anaconda installs with rpath properly set up so it is rather difficult to not get the right libraries.\r\n> \r\n> I have made an strace and grepped the libraries that are opened when it fails\r\n> They consistently come from the anaconda env dir that hosts the tensorflow package (see below).\r\n> Besides libcuda that is version 440.82 and that I have compiled with the NVIDIA installer.\r\n> \r\n> I can set the LD_LIBRARY_PATH to one of the other anaconda env lib dirs with different cudatoolkits and different libcudnn, the trace remains the same.\r\n> Note as well that it is not lbcudnn that poses the problem. It is always the third libcuxyz library that is\r\n> used and this only on specifc GPUs (I have used the same install script on different machines with different GPUs, some do work some don't) and they work all if memory growth is enabled.\r\n> \r\n> ```\r\n> (tf2.1) m3088.roebel: (test_sd) 510> grep open trace.log  | grep libcu | grep -v -- -1\r\n> openat(AT_FDCWD, \"/usr/lib/x86_64-linux-gnu/libcuda.so.1\", O_RDONLY|O_CLOEXEC) = 4\r\n> openat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcudart.so.10.1\", O_RDONLY|O_CLOEXEC) = 11\r\n> openat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcublas.so.10\", O_RDONLY|O_CLOEXEC) = 11\r\n> openat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../.././libcublasLt.so.10\", O_RDONLY|O_CLOEXEC) = 11\r\n> openat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcufft.so.10\", O_RDONLY|O_CLOEXEC) = 11\r\n> openat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcurand.so.10\", O_RDONLY|O_CLOEXEC) = 11\r\n> openat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcusolver.so.10\", O_RDONLY|O_CLOEXEC) = 11\r\n> openat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcusparse.so.10\", O_RDONLY|O_CLOEXEC) = 11\r\n> openat(AT_FDCWD, \"/data/anasynth/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/../../../../libcudnn.so.7\", O_RDONLY|O_CLOEXEC) = 11\r\n> ```\r\n\r\nSo you are sort of illustrating my point `libcudnn.so.7` doesn't say `7.XXX.YYY` on top of that `7.XXX.YYY` has a further dependancy on CUDA `10.2` `10.1` `10.0` `9.2` `9.1` `9.0` etc\r\n\r\nI have not seen the error since I started managing the path well and managing the amount of memory available before initialising a graph of a known size and making sure that the targeted GPU only used enough memory for the graph and enough memory to query how much CUDA memory is available.\r\n\r\nI think it is a resources problem. How much memory is available when you start the process and how much memory does your graph use?", "@kognat-docs\r\n\r\n> So you are sort of illustrating my point libcudnn.so.7 doesn't say 7.XXX.YYY on top of that 7.XXX.YYY has a further dependancy on CUDA 10.2 10.1 10.0 9.2 9.1 9.0 etc\r\n\r\nThe question you posed was \"Have people been checking if there are two CuDNN 7 shared libraries on the path or LD library path\". And my answer was: I have checked this, there is only one. \r\nI've sent you the trace.\r\n\r\n> I have not seen the error since I started managing the path\r\n\r\nWhat do you mean by managing the path?\r\nI always manage my paths ! I have installed a conda environment which I verified to be consistent! Everything is as it has been packaged by anaconda, I verified this.\r\n\r\nAnyway you may believe I am too stupid, to set up anaconda. Well \r\nI now have downloaded the official docker image\r\n\r\ntensorflow/tensorflow:2.1.0-gpu-py3\r\n\r\nand run my script in there. It crashes if I don't have\r\n\r\nexport TF_FORCE_GPU_ALLOW_GROWTH=true\r\n\r\nCan I manage paths any better ?\r\n\r\n> and managing the amount of memory available before initialising a graph of a known size and making sure that the targeted GPU only used enough memory for the graph and enough memory to query how much CUDA memory is available.\r\n\r\n>I think it is a resources problem. How much memory is available when you start the process and how much memory does your graph use?\r\n\r\nAs I wrote above in my report there is no graph (or better say there is hardly a graph) ! I just run these four lines\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.matmul(tf.zeros((2,2,2)), tf.zeros((2,2,2)))\r\ntf.nn.conv2d(tf.zeros((2,20,20,20), dtype=tf.float32), filters=tf.zeros((2,2,20,20), dtype=tf.float32), strides=(1,1,1,1), padding=\"VALID\")\r\n```\r\n\r\nand it crashes. If I change the order of the three lines it always crashes after these three operations (I had explained this in my bug report).\r\n\r\nJust for the fun of it I counted the bytes:  there is <83kB of data  memory required. The GPU is empty, I don't use it for graphics, and there are no other processes running on it.  On the various systems there are 4GB or 11GB available! Besides I know how to run nvidia-smi! So the card is empty still I cannot run these 4 lines that require 84kB!\r\n  \r\nJust for your information, an error due to memory exhausted looks quite differently, I have these as well. For my real graphs, I am very well able to detect these and react accordingly.\r\n\r\nThanks for your efforts anyway.", "@roebel Did you see @sanjoy s comment about debugging from the cpp https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-561366750 ? \r\n\r\nI haven't gotten around to recompiling tensorflow and trying it out. Their versions move so fast it would take me a bit to setup and compile everything. Plus, 1.15 dropped support for the gcc version I use, and 1.13 doesn't receive any updates so it was somewhat pointless for me to debug this anyway.", "@roebel I did not recall what triggered the problem for you.\r\n\r\nsee this https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-480549043\r\n\r\nWhich is why I thought it was memory related, this issue has not effected me for some time, nor the users of my software on a variety of platforms.", "@samhodge \r\n\r\nYes, I understand if there is a bug it does seem to be triggered by a rather particular situation only.\r\n\r\n@odinsbane \r\n\r\nthanks, no I had not noticed that. I will see whether manage to compile the most recent version tf2.2.0. \r\n\r\nIn fact I tried the docker  with tensorflow 2.2, it uses the same version of cuda 10.1 and has the same problem.\r\n\r\n", "Thought this was a windows only problem so I installed an ubuntu environment from scratch, only to find out it's my graphics card (RTX 2080) that is the issue.  Unfortunately I think I'm going to select a different machine learning platform due to this issue, since this seems to have been a problem since 2018.", "> @kognat-docs\r\n> > and managing the amount of memory available before initialising a graph of a known size and making sure that the targeted GPU only used enough memory for the graph and enough memory to query how much CUDA memory is available.\r\n> \r\n> > I think it is a resources problem. How much memory is available when you start the process and how much memory does your graph use?\r\n> \r\n> As I wrote above in my report there is no graph (or better say there is hardly a graph) ! I just run these four lines\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> tf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\n> tf.matmul(tf.zeros((2,2,2)), tf.zeros((2,2,2)))\r\n> tf.nn.conv2d(tf.zeros((2,20,20,20), dtype=tf.float32), filters=tf.zeros((2,2,20,20), dtype=tf.float32), strides=(1,1,1,1), padding=\"VALID\")\r\n> ```\r\n> \r\n> and it crashes. If I change the order of the three lines it always crashes after these three operations (I had explained this in my bug report).\r\n> \r\n> Just for the fun of it I counted the bytes: there is <83kB of data memory required. The GPU is empty, I don't use it for graphics, and there are no other processes running on it. On the various systems there are 4GB or 11GB available! Besides I know how to run nvidia-smi! So the card is empty still I cannot run these 4 lines that require 84kB!\r\n\r\nDid you observe how much memory was in use by using watch on nvidia-smi while you process was running with an interval of 50 ms?\r\n\r\nSee this fix that worked for other people\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/24496#issuecomment-497202806", "here is a related post from 4 years ago.\r\n\r\nhttps://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory", "Or you can read the friendly manual:\r\nhttps://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth", "So you can do the patch without touching the code just by altering your runtime environment.\r\n\r\n`Another way to enable this option is to set the environmental variable TF_FORCE_GPU_ALLOW_GROWTH to true. This configuration is platform specific.`", "@sanjoy  @odinsbane\r\n\r\nGood news!\r\nFollowing\r\nhttps://github.com/tensorflow/tensorflow/issues/24496#issuecomment-561366750\r\n\r\nI rebuilt the version 2.1 using the anaconda tensorflow recipe from here\r\nhttps://github.com/AnacondaRecipes/tensorflow_recipes\r\n\r\nI added two prints in MinSystemMemory showing available_memory and min_system_memory.\r\nOn my system with  `GeForce GTX 1050 Ti` disabling the TF standard log\r\nI have got this\r\n\r\n```\r\nTF_CPP_MIN_LOG_LEVEL=2 python run_cuda.py \r\n=========================================================\r\nMinSystemMemory: available_memory::4163764224\r\nMinSystemMemory: min_system_memory::314572800\r\n=========================================================\r\n1 Physical GPUs, 1 Logical GPUs\r\n2020-05-21 09:44:32.143642: E tensorflow/stream_executor/cuda/cuda_fft.cc:223] failed to make cuFFT batched plan:5\r\n2020-05-21 09:44:32.143671: E tensorflow/stream_executor/cuda/cuda_fft.cc:426] Initialize Params: rank: 1 elem_count: 512 input_embed: 512 input_stride: 1 input_distance: 512 output_embed: 257 output_stride: 1 output_distance: 257 batch_count: 20\r\n2020-05-21 09:44:32.143677: F tensorflow/stream_executor/cuda/cuda_fft.cc:435] failed to initialize batched cufft plan with customized allocator: Failed to make cuFFT batched plan.\r\nAborted\r\n```\r\n\r\nnvidia-smi reports the  GPU has 4040MiB, on this system there is X running on the card which has 13MiB so the numbers seem fine.\r\n\r\nmin_system_memory is set like this\r\n```\r\n    min_system_memory =                                                                                                                        \r\n        std::max(int64{314572800}, static_cast<int64>(available_memory * 0.05));                                                               \r\n```\r\nSo the maximum amount o memory is chosen anyway. Instead I added a mechanism to force the min_system_memory via environment variable TF_FORCE_MIN_SYSTEM_MEMORY_MB.\r\nThen running\r\n\r\n```\r\nTF_FORCE_MIN_SYSTEM_MEMORY_MB=310 TF_CPP_MIN_LOG_LEVEL=2 python run_cuda.py \r\n=========================================================\r\nMinSystemMemory: available_memory::4163764224\r\nMinSystemMemory: min_system_memory::314572800\r\nMinSystemMemory: forced min_system_memory::325058560\r\n=========================================================\r\n1 Physical GPUs, 1 Logical GPUs\r\ndone\r\n```\r\n\r\nthe problem is solved!\r\n\r\nUnfortunately I don't currently have a system with a working RTX card and I am not sure when those will be back working. If anybody would be willing to test this on such a card I could provide the pip package and the content of the conda environment for ubuntu linux that needs to be installed to have it run. \r\n", "Nice one @roebel !\r\n\r\nMight be worth suggesting that as a pull request and add to the docs.", "@samhodge @sanjoy @odinsbane \r\n\r\n> Might be worth suggesting that as a pull request and add to the docs.\r\n\r\nSure, but the problem is that the solution will probably not work for the other cards.\r\nFor my GTX 1050 the total memory is 4GB and the default system memory retained \r\nby tensorflow is max(300MB,4GB*0.05). So for GTX1050 this will be 300MB which apparently is too small. As mentioned above I need to increase to 310MB.\r\n \r\nNow for the RTX2080 the total memory is 11GB which with max(300MB,11GB*0.05)\r\nwill select system memory to be 550MB, which according to the findings on the 1050 \r\nshould normally be enough.\r\n\r\nI will have access to the RTX2080 GPUs again by the end of the week and will see \r\nwhat I get there.\r\n", "@samhodge @sanjoy @odinsbane\r\n\r\nFinally I have been able to run the patched library on the rtx 2080 cards.\r\nAs expected the patched version does not pass. Here again the script\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.signal.stft(tf.zeros(3000, dtype=tf.float32), 512, 128)\r\ntf.matmul(tf.zeros((2,2,2)), tf.zeros((2,2,2)))\r\ntf.nn.conv2d(tf.zeros((2,20,20,20), dtype=tf.float32), filters=tf.zeros((2,2,20,20), dtype=tf.float32), strides=(1,1,1,1), padding=\"VALID\")\r\n```\r\n\r\nAnd here the matrix of `available memory` reported from gpu_device.cc, \r\ndefault value of `Min_system_memory` as selected in gpu_device.cc  and the\r\n`min value of the min_system_memory` I need to select for the script to not abort:\r\n\r\nCard |  AvailMem | Def MinSysMem | Required MinSysMem \r\n:-------|:-----------|:----------|:-----------------------\r\n1050 TI | 4163764224 | 314572800 | 325058560 \r\n1080 TI | 11567431680 | 578371584 | 335544320 \r\n2080 TI | 11381964800 | 569098240 | 618659840 \r\n\r\nSo while 1050 and 1080 run the script with about the same memory size\r\n the RTX2080 requires nearly twice as much memory. This does not sound good \r\nto me.\r\n\r\nAny suggestions what to try to get this to a comparable value?\r\n\r\n", "@roebel \r\n\r\nI have struggled with this in my C++ application for a number of iterations.\r\n\r\nWhat is came down to in the end was the following.\r\n\r\nOnly run models on the GPU when enough memory is available to run the model.\r\n\r\nSo the amount of memory that the model will require is quantifiable.\r\n\r\nSo you need to have a GPU memory as a percentage which will fit that model.\r\n\r\nThen you also need to know about how much memory is available on the card exactly before allocating the memory, which is subject to race conditions, because you don't know what else is using CUDA memory at the same time on the operating system.\r\n\r\nBut the race condition aside, you also need to measure the memory free.\r\n\r\nThis is done by using `cudaMemInfo`, which in itself uses memory.\r\n\r\nSo on the provision that you have enough memory to run `cudaMemInfo` once to measure and you need to make sure that enough memory is free to fit the model and run `cudaMemInfo` one more time, then and only then you can allocate enough of the percentage of available VRAM on that card for running the model.\r\n\r\nAnyway the take home from my random babbling is that `cudaMemInfo` is required to poll the amount of memory available to allocate which in itself also uses some of that available memory.\r\n\r\nMaybe somehow the amount of memory used by `cudaMemInfo` is different on a Turing based card compared at a Pascal based card, I can get someone from NVIDIA to have a look if you wish. ", "Yeah I cannot find reference to the `cudaMemInfo` at all but that seems like the kind of footprint which would be the max of 300Mb and 5% of the card's memory.\r\n\r\nhaving a look at:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.2/tensorflow/core/common_runtime/gpu/gpu_process_state.cc\r\n\r\nIt doesn't seem like it is using this per se.", "I don't think we should be playing cat-and-mouse with the amount of memory we need to reserve for system libraries -- as you've observed, there is no systematic way to get this right.\r\n\r\nInstead IMO we should try to initialize the system libraries before BFC allocator has had a chance to allocate the rest of the GPU's memory.\r\n\r\nCC @chsigg ", "Probably one should do this only if allow memory growth is off.  Otherwise you will always need about 580MB for the 2080 even if you don't need all the operators.\r\n\r\nI made a few more test concerning the minimum system memory requirements for running combinations of the three operations from my test case. I compare only the 1080 and 2080 cards. You dont find conv2d alone because it initializes blas in any case. Out comes\r\n\r\nGPU|   MatMul | STFT | Conv2D+MatMUL  | MatMul+STFT | MATMUL+STFT+Conv2D|\r\n:---|:---|:---|:---|:---|:---\r\n1080 | 140MB | 130MB | 290MB | 170MB | 320MB\r\n2080 | 190MB | 190MB | 520MB | 250MB |580MB\r\n\r\nOne can see that on the 2080 cuda requires an overhead for each operation, and that this overhead increases when using more libraries. In most cases the overhead is `<100MB` but it becomes `>220MB` once Conv2D is involved..\r\n\r\nIf @samhodge has contact to NVIDIA I would personnally find it interesting to hear whether this is intended.", "Hello everyone! \r\nI have solved similar problem with limiting memory growth and you can try.\r\n\r\nYou can find code in section [Limit memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)\r\n\r\n(This is my first comment in GitHub)", "I had a similar issue before. limiting GPU memory manually helped. https://github.com/tensorflow/tensorflow/issues/25160#issuecomment-643703167", "> I got the same problem on Ubuntu 20.04 with a GeForce RTX 2060 SUPER. A NN with dense layers works well. But with CNN layers I'm getting `Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.`\r\n> Adding `tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)` makes no difference to the error.\r\n> I followed the installation according to [https://www.tensorflow.org/install/gpu](url) and `nvidia-smi` shows:\r\n> `Driver Version: 440.64.00 CUDA Version: 10.2`\r\n> My conda env has:\r\n> \r\n> ```\r\n> cudatoolkit               10.1.243             h6bb024c_0  \r\n> cudnn                     7.6.5                cuda10.1_0  \r\n> tensorflow-gpu            2.1.0                h0d30ee6_0\r\n> ```\r\n> \r\n> In a conda env with tf 1.15 I am getting the same error. It would be great if this could be fixed.\r\n> \r\n> ## Update\r\n> After using `export TF_FORCE_GPU_ALLOW_GROWTH=true` it all works. I was of the impression that the `tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)` would to the same thing, but that's not the case. I think this should be clearly stated on the TensorFlow GPU support webpage.\r\n\r\nDude, your solution saves my life.", "Nvidia just released the 440.100 and 450.51(Beta) Linux display drivers.\r\nI tried out the 440.100, and it didn't fix the issue. Has anyone tried out the beta 450.51?", "@eduardoscsouza\r\n> Nvidia just released the 440.100 and 450.51(Beta) Linux display drivers.\r\n> I tried out the 440.100, and it didn't fix the issue. Has anyone tried out the beta 450.51?\r\n\r\nI tried 450.36.06. check https://github.com/tensorflow/tensorflow/issues/25160#issuecomment-643703167.", "the code that worked for me:\r\n\r\nimport tensorflow as tf\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.compat.v1.InteractiveSession(config=config)", "> _Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template_\r\n> \r\n> **System information**\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and No (described below)\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n> * TensorFlow installed from (source or binary): tf-nightly-gpu (Dec 19, r1.13)\r\n> * TensorFlow version (use command below): 1.13.0-dev20181219\r\n> * Python version: 3.7.1\r\n> * Bazel version (if compiling from source):\r\n> * GCC/Compiler version (if compiling from source):\r\n> * CUDA/cuDNN version: CUDA 10 with cuDNN 7.4.1\r\n> * GPU model and memory: RTX 2070 8GB\r\n> \r\n> **Describe the current behavior**\r\n> I'm running the CNN model on MNIST. When I'm running with the GPU, I am encountering\r\n> `2018-12-20 20:09:13.644176: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR`\r\n> \r\n> I did some digging and realized that it is a memory issue (which shouldn't be the case as I have 32GB of RAM and 64GB of swap. I ran htop when running the model and I have 20+GB free, which is more than enough to fit the 8GB vRAM mappings.\r\n> \r\n> Using the `gpu_options.allow_growth = True` gets the model to work properly, and setting `os.environ['CUDA_VISIBLE_DEVICES'] = '-1'` also works. This means that I AM facing a memory issue, but I don't see how.\r\n> \r\n> Also, using `gpu_options.allow_growth = True` does not fix the same issue when trying to run tensorflow/models/official/mnist/ model, which should have a similar behavior with my code.\r\n> \r\n> **Code to reproduce the issue**\r\n> \r\n> ```\r\n> import os\r\n> import tensorflow as tf\r\n> from tensorflow.examples.tutorials.mnist import input_data\r\n> import math\r\n> import time\r\n> # Killing optional CPU driver warnings\r\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n> # os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n> tf.logging.set_verbosity(tf.logging.ERROR)\r\n> \r\n> \r\n> class Model:\r\n> \r\n>     def __init__(self, image, label):\r\n>         \"\"\"\r\n>         A Model class contains a computational graph that classifies images\r\n>         to predictions. Each of its methods builds part of the graph\r\n>         on Model initialization. Do not modify the constructor, as doing so\r\n>         would break the autograder. You may, however, add class variables\r\n>         to use in your graph-building. e.g. learning rate, \r\n> \r\n>         image: the input image to the computational graph as a tensor\r\n>         label: the correct label of an image as a tensor\r\n>         prediction: the output prediction of the computational graph,\r\n>                     produced by self.forward_pass()\r\n>         optimize: the model's optimizing tensor produced by self.optimizer()\r\n>         loss: the model's loss produced by computing self.loss_function()\r\n>         accuracy: the model's prediction accuracy\r\n>         \"\"\"\r\n>         self.image = image\r\n>         self.label = label\r\n> \r\n>         # TO-DO: Add any class variables you want to use.\r\n> \r\n>         self.prediction = self.forward_pass()\r\n>         self.loss = self.loss_function()\r\n>         self.optimize = self.optimizer()\r\n>         self.accuracy = self.accuracy_function()\r\n> \r\n>     def forward_pass(self):\r\n>         \"\"\"\r\n>         Predicts a label given an image using convolution layers\r\n> \r\n>         :return: the prediction as a tensor\r\n>         \"\"\"\r\n>         filter_1 = tf.Variable(tf.truncated_normal([3, 3, 1, 8], stddev=0.1))\r\n>         conv_1 = tf.nn.conv2d(self.image, filter_1, [1, 1, 1, 1], \"SAME\")\r\n> \r\n>         reshaped = tf.reshape(conv_1, shape=[50, -1])\r\n> \r\n>         L1 = reshaped.shape[1].value\r\n>         L2 = 500\r\n>         W1 = tf.Variable(tf.random_normal([L1, L2], mean=0, stddev=0.01))\r\n>         b1 = tf.Variable(tf.random_normal([L2], mean=0, stddev=0.01))\r\n>         relu_1 = tf.nn.relu(tf.matmul(reshaped, W1) + b1)\r\n> \r\n>         W2 = tf.Variable(tf.random_normal([L2, 10], mean=0, stddev=0.01))\r\n>         b2 = tf.Variable(tf.random_normal([10], mean=0, stddev=0.01))\r\n>         logits = tf.nn.relu(tf.matmul(relu_1, W2) + b2)\r\n>         return logits\r\n> \r\n>     def loss_function(self):\r\n>         \"\"\"\r\n>         Calculates the model cross-entropy loss\r\n> \r\n>         :return: the loss of the model as a tensor\r\n>         \"\"\"\r\n>         loss = tf.losses.softmax_cross_entropy(onehot_labels=self.label, logits=self.prediction)\r\n>         return loss\r\n> \r\n>     def optimizer(self):\r\n>         \"\"\"\r\n>         Optimizes the model loss using an Adam Optimizer\r\n> \r\n>         :return: the optimizer as a tensor\r\n>         \"\"\"\r\n>         learning_rate = 0.1\r\n>         sgd = tf.train.GradientDescentOptimizer(learning_rate)\r\n>         train = sgd.minimize(self.loss)\r\n>         return train\r\n> \r\n>     def accuracy_function(self):\r\n>         \"\"\"\r\n>         Calculates the model's prediction accuracy by comparing\r\n>         predictions to correct labels \u2013 no need to modify this\r\n> \r\n>         :return: the accuracy of the model as a tensor\r\n>         \"\"\"\r\n>         correct_prediction = tf.equal(tf.argmax(self.prediction, 1),\r\n>                                       tf.argmax(self.label, 1))\r\n>         return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n> \r\n> \r\n> def main():\r\n>     t_start = time.time()\r\n> \r\n>     mnist = input_data.read_data_sets(\"data/mnist/\", one_hot=True)\r\n>     batch_sz = 50\r\n>     batch = 2000\r\n> \r\n>     inputs = tf.placeholder(shape=[batch_sz, 28, 28, 1], dtype=tf.float32)\r\n>     labels = tf.placeholder(shape=[batch_sz, 10], dtype=tf.float32)\r\n> \r\n>     model = Model(inputs, labels)\r\n> \r\n>     session_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\r\n>     sess = tf.Session(config=session_config)\r\n> \r\n>     # sess = tf.Session()\r\n> \r\n>     sess.run(tf.global_variables_initializer())\r\n>     for i in range(batch):\r\n>         next_image, next_label = mnist.train.next_batch(batch_sz)\r\n>         next_image = next_image.reshape((batch_sz, 28, 28, 1))\r\n>         sess.run(model.optimize, feed_dict={inputs: next_image, labels: next_label})\r\n> \r\n>     acc, test_images, test_labels = 0, mnist.test.images, mnist.test.labels\r\n>     test_batch = math.ceil(len(test_images) / batch_sz)\r\n>     for i in range(test_batch):\r\n>         batch_images = test_images[i * batch_sz: (i + 1) * batch_sz]\r\n>         batch_images = batch_images.reshape((batch_sz, 28, 28, 1))\r\n>         batch_labes = test_labels[i * batch_sz: (i + 1) * batch_sz]\r\n>         acc += sess.run(model.accuracy, feed_dict={inputs: batch_images, labels: batch_labes})\r\n>     acc /= test_batch\r\n>     print(acc)\r\n> \r\n>     print(time.time() - t_start, 'seconds')\r\n> \r\n>     return\r\n> \r\n> \r\n> if __name__ == '__main__':\r\n>     main()\r\n> ```\r\n\r\n**This worked for me.\r\nRTX 2060\r\nubuntu 18.04\r\npython 3.6**\r\n\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = InteractiveSession(config=config)\r\nwith sess.as_default():\r\n       process ...\r\n\r\n```", "Hello @bm777 \r\n\r\nfollowing my investigation from a few month ago I summarize how I understand the problem \r\n \r\n> GPU model and memory: RTX 2070 8GB\r\n> ... which shouldn't be the case as I have 32GB of RAM and 64GB of\r\n\r\nThe problem is not the system memory, the problem is the GPU memory! \r\n\r\n>os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n\r\nworks because it does not use the GPU!\r\n\r\nA few explanations:\r\n\r\nTF has two modes of operation:\r\n\r\n1. `allow memory growth = false`:  In this case TF preallocates some memory for the system libraries using a rough guess of  \r\n     how much memory is needed. AS you can read here https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-633953715 TF uses the formula `max(300MB, GPU-MEM * fac)` for this guess. For TF2.1 `fac = 0.05` for TF2.2 and if I \r\n    remember right it is `fac=0.07`. So now you have 8GB which gives 400MB for GPU pre-allocated  memory under TF2.1\r\n    and 560MB under TF2.2. \r\n   \r\n    I have experimentally evaluated the necessary pre-allocated memory for a few GPUs and TF21 here: https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-637715002 and  here   https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-637715002\r\n    \r\n   Turns out for Conv2D operations I needed 520MB there, you would have less than that under TF21 but more under TF22. Unfortunately you don't mention your TF version but I assume you use TF2.1. If you use TF2.2 and it still fails this might be because you use a different  GPU. Anyway fact is it fails. See below\r\n\r\n2) `allow memory growth = true`: TF does not use any pre-allocated memory and loads the libraries as they come. In the TF documentation  this is declared as problematic due to potential memory fragmentation and is therefore `off` by default.  \r\n\r\nMy take:\r\n\r\nGiven the large range of required memory for the libraries that depends on the operations you perform as well on the GPU you have it seems very difficult to get mode `allow memory growth = false` right (see https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-637950411). The current solution: to increase the size of the pre-allocated memory, which was done  for TF2.2, is problematic if your GPU is rather small. This blocks memory from use assuming you will need all available libraries (blas, Conv, FFT and I don't know whether there are others). In the case where you don't use all of these, this will result in wasting pre-allocated memory, in turn reducing the modelsize you may load for your application. On the other hand I believe that the memory fragmentation problem can be prevented when you create models early forcing system libraries to load before starting the training. This seems what is happening in most cases anyway and it seems therefore beneficial, especially for GPUs with small memory and especially for training a single model, to not pre-allocate but to use `allow memory growth = true`. \r\n\r\nPersonally I use  GPUs  with memory ranging from 4GB to 11GB and following the argument above I have set TF_FORCE_GPU_ALLOW_GROWTH=true for all of them. For the moment I did not have any problems with that.\r\n\r\n\r\n", "Hello @roebel \r\n\r\nMe too, I was thinking about the issues of error of allocation of memory. This is clearly for me now.\r\nNow it looks good GPU memory\r\n\r\nIn the past, I tested many options to pre-allocate memory \ud83d\ude22:\r\n\r\n\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntry:\r\n    tf.config.experimental.set_virtual_device_configuration(gpus[0], \r\n                 tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5044)])\r\n    \"\"\"process....\"\"\"\r\nexcept Exception as e:\r\n    raise e\r\n```\r\n\r\nPersonally I use GPU with 6GB of memory.\r\nAnd thank you @roebel, for this new arrow `TF_FORCE_GPU_ALLOW_GROWTH=true` to force my GPU for allocation \ud83d\ude0a.", "I had this same issue. I can say with certainty that the problem only occurs on my 2070 RTX, and NOT on a Titan RTX, running exactly the same code.\r\n\r\nhttps://github.com/DeepLabCut/DeepLabCut/issues/837", "Just upgrade to Tensorflow 2.3 with CUDA 11 and cudnn 8.0. It magically solved all my problems and I don't even need the workaround with `config.gpu_options.allow_growth = True` now.", "unfortunately, I need to run code that only supports tensorflow 1.X", "> Just upgrade to Tensorflow 2.3 with CUDA 11 and cudnn 8.0. It magically solved all my problems and I don't even need the workaround with `config.gpu_options.allow_growth = True` now.\r\n\r\nUpgrading from 2.2 to 2.3 even with explicit `TF_FORCE_GPU_ALLOW_GROWTH=false` solved this for me as well (at least for now I am able to run [delf demo code](https://github.com/tensorflow/models/blob/master/research/delf/delf/python/delg/DELG_INSTRUCTIONS.md); have not tested with anything else). \r\n\r\nI am still on CUDA 10.1, Cudnn 7.6.5.", "Is there a fix for this issue with tensorflow 2 and python3 ???\r\n\r\nI have a:\r\nRTX 2080\r\n\r\n\r\nI am getting this message:\r\n\r\n```\r\n\r\n2020-08-20 12:38:27.172496: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-08-20 12:38:27.177708: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"/home/anantha/Desktop/RaspiCar/car.py\", line 85, in <module>\r\n    tnet.train(x, y)\r\n  File \"/home/anantha/Desktop/RaspiCar/car.py\", line 65, in train\r\n    self.model.fit(x, y, epochs=epochs)\r\n  File \"/home/anantha/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/home/anantha/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/home/anantha/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/anantha/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 644, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/anantha/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/anantha/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\r\n    return self._call_flat(\r\n  File \"/home/anantha/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/home/anantha/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\r\n    outputs = execute.execute(\r\n  File \"/home/anantha/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node sequential/conv2d/Conv2D (defined at /Desktop/RaspiCar/car.py:65) ]] [Op:__inference_train_function_951]\r\n\r\nFunction call stack:\r\ntrain_function\r\n\r\n```", "In case your problem has the same origin as the problems that are treated in the present issue (which I cannot know from your report) then there are  a few solutions that you can easily find by means of  reading the last 10-20 posts in this thread. ", "I Fixed it with this:\r\n```\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.compat.v1.Session(config=config)\r\nsess.as_default()\r\n```", "I had this same issue with RTX 2080. Then following code worked for me.\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n```\r\n\r\nThanks everyone", "I think we can stop posting the `allow_growth` fix now :)", "RTX 2070 here. Was getting this error, but now running with `TF_FORCE_GPU_ALLOW_GROWTH=true` (as other commenters have pointed out, fixes it for them) changes the error message to an out of memory error (even though I've got plenty of memory): \r\n```\r\n2020-10-17 16:35:11.717658: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.87G (4159818752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n```\r\nBut my GPU has 8GB and only about 250MB were in use before I started the process. So I don't understand, why can't it allocate 3.87GB?  (lowering batch size had no effect; the weights hdf5 file is less than 200MB)", "TF_FORCE_GPU_ALLOW_GROWTH=true worked for me.\r\ntf.config.experimental.set_memory_growth(gpu, True) worked too.\r\n\r\nHere is my configuration:\r\n  GPU          GTX 1650\r\n  cuda-10-1  10.1.243-1\r\n  libcudnn7   7.6.5.32-1+cuda10.1\r\n  Ubuntu 18.04.5 LTS\r\n\r\nWhoever cannot set the environment variable, could try this as suggested in https://www.tensorflow.org/guide/gpu:\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)", "Typing the command mentioned on the terminal just worked for me.\r\n\r\nhttps://github.com/tensorflow/tfjs/issues/671#issuecomment-494832790", "> Just upgrade to Tensorflow 2.3 with CUDA 11 and cudnn 8.0. It magically solved all my problems and I don't even need the workaround with `config.gpu_options.allow_growth = True` now.\r\n\r\n\r\nIt seems that the issue is noticed and solved in tensorflow 2.3.0.\r\n\r\n- CUDA 10.1\r\n- GPU: Quadro RTX 6000\r\n- Tensorflow 2.2.0\r\n- cudnn 7.6.5\r\n\r\nSame problem:\r\n` tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR`\r\n`tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.`\r\n\r\nAnd the workaround `allow_growth = True` does not help. \r\n\r\nAfter I upgrade tensorflow to 2.3.0, the problem disappeared, even without adding the line `allow_growth = True` .\r\n\r\n", "> ok, made it work in tf-nightly-gpu-2.0-preview and ipython notebook adding this to my code:\r\n> \r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n\r\nit works in my case", "> ok, made it work in tf-nightly-gpu-2.0-preview and ipython notebook adding this to my code:\r\n> \r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n\r\nIt works, paste to start python file you execute. Ubuntu 20.04, docker Nvidia, tensorflow 1.15, GTX 1060", "Hi,\r\n\r\nThe `config.gpu_options.allow_growth = True` option also works well with Keras. One can initialize a session and specify it to Keras, just something as follows:\r\n```\r\nfrom tensorflow.keras import backend as K\r\nimport tensorflow as tf\r\n\r\nsession_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\r\nsess = tf.Session(config=session_config)\r\nK.set_session(sess)\r\n```\r\nHope it helps.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24496\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24496\">No</a>\n"]}, {"number": 24495, "title": "Lite audio_microfrontend num_frame compute not same to prepare ", "body": "[num_frames compute](https://github.com/tensorflow/tensorflow/blob/d4ddccd3cca4fc837c66ae1dfa190739420ad122/tensorflow/lite/experimental/microfrontend/audio_microfrontend.cc#L128 ) not equal to [Prepare](https://github.com/tensorflow/tensorflow/blob/d4ddccd3cca4fc837c66ae1dfa190739420ad122/tensorflow/lite/experimental/microfrontend/audio_microfrontend.cc#L107)\r\n", "comments": []}, {"number": 24494, "title": "from tensorflow.python.data import Dataset  ModuleNotFoundError: No module named 'tensorflow.python.data'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (windows 10):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (cpu 1.2.1):\r\n- Python version: 3.6.2\r\n\r\n--------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-77-256f8ffc6dfe> in <module>()\r\n----> 1 from tensorflow.python.data import Dataset\r\n\r\nModuleNotFoundError: No module named 'tensorflow.python.data'\r\n", "comments": ["@Auroratan \r\nPlease provide part of the code which is throwing error. \r\nPlease try import tensorflow.data.Dataset(). Let me know what you get. thanks", "@jvishnuvardhan \r\nThank you for your help\uff01 I know my mistake, because the version of tensorflow is 1.2, it is a version problem, I used conda install tensorflow=1.12.0 to solve my problem.", "@Auroratan Thanks. I will close this."]}, {"number": 24493, "title": "Fix python 3.7 build error due to astor", "body": "This upgrade is needed for tensorflow to be properly built with Python 3.7, since astor 0.6.2 is python 3.7 incompatible due to usage of 'async'.", "comments": ["Yup Fixed!", "Thanks for the contribution!", "Is there still hope for Tensorflow on Python-3.7 ? ", "Sure! tf-nightly builds of 1.13 version are already python 3.7 compatible: [tf-nightly](https://pypi.org/project/tf-nightly/). But unfortunately, not for Windows. But building for windows from source with Python 3.7 (with GPU support) is possible now with this PR.", "@gunan Can these 2 failures related to Mac be ignored to proceed to \"Ready to Pull\" ?", "I just noticed. THis is a PR to the release branch.\r\nWe cannot accept this. Please recreate this to the master branch."]}, {"number": 24492, "title": "Update operation_semantics.md", "body": "documentation issue", "comments": []}, {"number": 24491, "title": "Cannot convert custom .pb file to .tflite file ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  macOS Mojave, version 10.14.2\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.12.0\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nI have a custom model pre-trained in Pytorch framework. I have converted it to ONNX model, and then to TensorFlow model, so that it can be further converted to TensorFlow Lite model for deployment on mobile devices. \r\n\r\nI used the tflite_convert command for the conversion,\r\n\r\ntflite_convert\\\r\n --graph_def_file=model.pb \\\r\n --output_file=model.lite \\\r\n --input_format=TENSORFLOW_GRAPH \\\r\n --output_format=TFLITE \\\r\n --input_arrays=0 \\\r\n --output_arrays=add_51 \\\r\n\r\nYet I got the following error message:\r\n\r\nTraceback (most recent call last):\r\n\u00a0 File \"/miniconda3/bin/tflite_convert\", line 11, in <module>\r\n\u00a0 \u00a0 sys.exit(main())\r\n\u00a0 File \"/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main app.run(main=run_main, argv=sys.argv[:1])\r\n\u00a0 File \"/miniconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n\u00a0 \u00a0 _sys.exit(main(argv))\r\n\u00a0 File \"/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main _convert_model(tflite_flags)\r\n\u00a0 File \"/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 162, in _convert_model output_data = converter.convert()\r\n\u00a0 File \"/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 453, in convert **converter_kwargs)\r\n\u00a0 File \"/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert.py\", line 342, in toco_convert_impl input_data.SerializeToString())\r\n\u00a0 File \"/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos(stdout, stderr))\r\n\r\nRuntimeError: TOCO failed see console for info.\r\nb\"2018-12-20 17:44:31.127219: F tensorflow/contrib/lite/toco/import_tensorflow.cc:2137] Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'\\n\"\r\nNone\r\n\r\n\r\n\r\nThe custom pre-trained model is adapted from a Pytorch ResNet model. Therefore I also evaluated converting the ResNet model from the torchvision package to TensorFlow model through the ONNX framework, and further to TensorFlow Lite model. And I got the following error message:\r\n\r\nConverting unsupported operation: PyFunc\\n2018-12-20 18:09:03.357810: F tensorflow/contrib/lite/toco/import_tensorflow.cc:112] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)\\n'\r\nNone\r\n\r\n\r\n\r\nI wonder if the errors above were due to some unsupported operators from the model, probably caused from converting Pytorch model to TensorFlow model through the ONNX framework. As I can convert the pre-trained Mobilenet model (https://www.tensorflow.org/lite/models) to tflite model without error on my platform. Thanks.  \r\n\r\n\r\n", "comments": ["Is there any proposed solution to this issue yet @miaout17? I am also having the same problem. My model is custom but, not converted between libraries except only Tensorflow. I am using tf-nightly-gpu and my error is also same. Hope to get your attention as soon as possible :)\r\n\r\n `2019-01-11 17:18:59.516379: F tensorflow/lite/toco/import_tensorflow.cc:2505] Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'`", "Okay, I guess, I found the reason. If we use Tensorflow GPU it uses NCHW but, tflite requires NHWC format. Thus, switching from Tensorflow GPU to CPU solved the problem for me. I used tf-nightly CPU for the conversion. Thanks :)", "How did you do this? I created a new conda environment and installed with pip the tf-nightly CPU, but the error is the same: \"Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'\r\nFatal Python error: Aborted\". Did you call tflite_convert in some other way?", "I am getting the same error even when switching from Tensorflow GPU to CPU.", "> I am getting the same error even when switching from Tensorflow GPU to CPU.\r\n\r\nCan you fix it?", "> Can you fix it?\r\n\r\nOne fix for dimension mismatch is to have a model like this:\r\n\r\n```\r\nclass Model(nn.Module):\r\n    def __init__(self, model):\r\n        super().__init__()\r\n        self.model = model\r\n    def forward(self,x):\r\n        return self.model(x.permute(0,3,1,2))\r\n```\r\n\r\nThis time I am still having :\r\n\r\n```\r\nConverterError: TOCO failed. See console for info.\r\n2019-04-03 14:06:45.559722: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: PyFunc\r\n2019-04-03 14:06:45.571306: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)\r\n```", "Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'\\n\"\r\n\r\ni am using tensorflow 1.12.0 ( ive tried nightly )\r\ni am using cuda* as device ( which is cpu)\r\nand still get this i cant seem to find out how to solve this where do i state that i want nhwc", "I;ve created a tensorflow mode of my own with custom operator and when I'm trying to convert it to lite I get something similar like it doesn't find my custom operation (MyAdd)\r\n\r\nI've tried both command line utilities tflite_convert & toco (with same error): \r\n\r\ntflite_convert --output_file lite_version_model.tflite --saved_model_dir ./SavedModel --output_format TFLITE --allow_custom_ops\r\n\r\ntoco --output_file lite_model_version.tflite --saved_model_dir ./SavedModel --output_format=TFLITE\r\n", " I am trying to convert .pb to .tflite using ssd mobilenet v1 pets config.\r\nCreate .pb file using this command\r\nsudo python export_inference_graph.py --input_type image_tensor --pipeline_config_path /home/ubuntu/training/data/ssd_mobilenet_v1_pets.config --trained_checkpoint_prefix /home/ubuntu/training/data/model.ckpt-78386 --output_directory /home/ubuntu/training/ --input_shape 1,300,300,3\r\nthen summarize the frozen_inference_graph.pb\r\n\r\nits return this output\r\nFound 1 possible inputs: (name=image_tensor, type=uint8(4), shape=[1,300,300,3])\r\nNo variables spotted.\r\nFound 4 possible outputs: (name=detection_boxes, op=Identity) (name=detection_scores, op=Identity) (name=detection_classes, op=Identity) (name=num_detections, op=Identity)\r\nFound 6130346 (6.13M) const parameters, 0 (0) variable parameters, and 187 control_edges\r\nOp types used: 1781 Const, 255 GatherV2, 230 Identity, 224 Reshape, 207 Minimum, 164 Maximum, 116 Slice, 113 Cast, 103 Mul, 99 Sub, 95 ConcatV2, 88 Greater, 82 Where, 82 Split, 72 Add, 63 Pack, 63 StridedSlice, 50 Shape, 50 Unpack, 45 ExpandDims, 43 Squeeze, 41 ZerosLike, 41 NonMaxSuppressionV3, 39 Fill, 37 Tile, 35 Relu6, 35 FusedBatchNorm, 34 Conv2D, 33 RealDiv, 21 Switch, 16 Range, 13 DepthwiseConv2dNative, 12 BiasAdd, 6 Merge, 6 Sqrt, 3 Assert, 3 Equal, 3 Transpose, 2 Exp, 1 All, 1 TopKV2, 1 Size, 1 Sigmoid, 1 Placeholder\r\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=/home/jayasri/Downloads/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18/original/frozen_inference_graph.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=1,300,300,3 --output_layer=detection_boxes,detection_scores,detection_classes,num_detections\r\n\r\nThen trying to convert tflite using bazel-bin/tensorflow/lite/toco/toco\r\n\r\nBut I got this error\r\ntensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-05-29 15:46:02.589145: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2780 operators, 4997 arrays (0 quantized)\r\n2019-05-29 15:46:02.794908: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 2741 operators, 4915 arrays (0 quantized)\r\n2019-05-29 15:46:03.070306: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2741 operators, 4915 arrays (0 quantized)\r\n2019-05-29 15:46:03.162066: F tensorflow/lite/toco/graph_transformations/resolve_constant_slice.cc:59] Check failed: dim_size >= 1 (0 vs. 1)\r\n\r\nStill i can't find the solution", " I also meet this problem: \r\n\r\n`F tensorflow/lite/toco/import_tensorflow.cc:1994] Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'.`\r\n\r\n I have tried tf-cpu(1.10.0), tf-gpu(1,13.1), tf-nightly-gpu(1.15.0-dev20190704), but I failed. Is there a solution? Thanks.", "sadly this forum is closed so expect no answers. ive had to put the small project i was working on on hold.", "I have a question Can't The people who are working on tensorflow see these errors.\r\nI have never seen any tensorflow team member commenting on these issues.\r\nObviously they have better knowledge about these issues", "> How did you do this? I created a new conda environment and installed with pip the tf-nightly CPU, but the error is the same: \"Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'\r\n> Fatal Python error: Aborted\". Did you call tflite_convert in some other way?\r\n\r\nExcuse me,Do you solve it?\r\n", "Hi,\nI am sorry no i didnt. i was just working on this a few months ago as a\npersonal project. So there wasnt any reason to actually get it working.\nGood luck & Best wishes.\n\n\nOn Mon, Dec 21, 2020 at 1:38 AM YIQIHYM <notifications@github.com> wrote:\n\n> How did you do this? I created a new conda environment and installed with\n> pip the tf-nightly CPU, but the error is the same: \"Check failed:\n> status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'\n> Fatal Python error: Aborted\". Did you call tflite_convert in some other\n> way?\n>\n> Excuse me,Do you solve it?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24491#issuecomment-748770065>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AEVTVVNWFS7NZUNNW2NH7RTSV3NOXANCNFSM4GLVBRMQ>\n> .\n>\n", "I also meet the same problem,and i try all the ways in this page,but it's useless.\r\nfinally,i try this way and then solved the problem.\r\n[https://github.com/paulbauriegel/tensorflow-tools/blob/master/convert-model-to-NHWC.py](url)\r\nThe main solution is use tf.transpose change the NCHW to NHWC.", "The link is not working.\r\n\r\n> I also meet the same problem,and i try all the ways in this page,but it's useless.\r\n> finally,i try this way and then solved the problem.\r\n> [https://github.com/paulbauriegel/tensorflow-tools/blob/master/convert-model-to-NHWC.py](url)\r\n> The main solution is use tf.transpose change the NCHW to NHWC."]}, {"number": 24490, "title": "quantization aware training in InceptionV3", "body": "Hi, \r\n\r\nIn the Figure 7 of whitepaper (Quantizing deep convolutional networks for efficient inference 2018), you mentioned that after the \"concat\" op, the \"fQ\" op will be inserted. \r\n\r\nhowever, on the evaluation/training graph of quantized InceptionV3, the \"fQ\" op is not appear after the \"concat\" op. \r\n\r\ncould you help me to explain this problem?\r\n\r\nthe version of our Tensorflow is based on v1.12\r\n![insertquant](https://user-images.githubusercontent.com/21340280/50321399-b6226f80-050b-11e9-8913-19c4b935d7a6.png)\r\n\r\nThanks\r\n\r\n", "comments": ["The rewriter may not be able to handle all possible model architectures. Please take a look at post-training integer quantization if you want to quantize InceptionV3:\r\nhttps://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba"]}, {"number": 24489, "title": "Misc. build fixes and fixes to toco converter.", "body": "", "comments": []}, {"number": 24488, "title": "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory when use screen command", "body": "I try use the **screen** command, but have met this problem.\r\n\r\nwhen I use current screen:\r\n```\r\n~$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n\r\n\r\n~$ pip list\r\nPackage         Version  \r\n--------------- ---------\r\ntensorboard     1.10.0   \r\ntensorflow-gpu  1.10.1   \r\ntermcolor       1.1.0    \r\n\r\n\r\n~$python3\r\nPython 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 18:21:58) \r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> \r\n\r\n```\r\nObviously, I can use tensorflow.\r\n\r\nNow,  \r\n```\r\n$-->screen -list\r\nThere is a screen on:\r\n\t18536.copy\t(12/20/2018 09:49:48 PM)\t(Detached)\r\n```\r\nwhen I use tensorflow in **copy** screen:\r\n\r\n```\r\n~$ screen -r copy \r\n\r\n~$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n\r\n~$ pip list\r\nPackage         Version  \r\n--------------- ---------\r\ntensorboard     1.10.0   \r\ntensorflow-gpu  1.10.1   \r\ntermcolor       1.1.0    \r\n\r\n\r\n~$ python3\r\nPython 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 18:21:58)\r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/wangqianlong/miniconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```\r\n\r\n**current and copy screen is on the same server machine**\r\nCould you give me a suggestion ?\r\nThanks very much.", "comments": ["Apologies for the delay in response. Is this still an issue?\r\nCan you please provide following information?\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24487, "title": "Enable MKL in TensorFlow for c++, could not create a primitive descriptor iterator , mkl_relu_op.cc:871", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:1.8, 1.9, 1.11.0, 1.2.0\r\n- Python version: 2.7.12\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):0.17.1, 0.18.0\r\n- GCC/Compiler version (if compiling from source): gcc version 5.4.0 \r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n- MKL version: 0.172, 0.13\r\n\r\n\r\n**Describe the problem**\r\n\r\nRun my network test code, and get this error:\r\n 2018-12-21 09:03:42.470347: W tensorflow/core/framework/op_kernel.cc:1407] OP_REQUIRES failed at mkl_relu_op.cc:874 : Aborted: Operation received an exception:Status: 5, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl_relu_op.cc:871\r\n2018-12-21 09:03:42.470495: F /home/yhj/faster_rcnn_tf/src/faster_rcnn/networks/src/network.cpp:313] Non-OK-status: session.Run(feed_map, {Shape(scope, input)}, &outputs) status: Aborted: Operation received an exception:Status: 5, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl_relu_op.cc:871\r\n\t [[{{node fc6}}]] = _MklRelu[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Add, DMT/_42)]]\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n./configure\r\nbazel build --config=mkl --copt=\"-DEIGEN_USE_VML\" -c opt //tensorflow:libtensorflow_cc.so\r\n**bazel build --config=mkl --config=opt //tensorflow:libtensorflow_cc.so\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi,\r\nCould you share the test code to us, so that we could try to reproduce the problem?\r\nThank you.", "This is probably because tensorflow::ops::Relu() doesn't work properly.", "Hi faciler \r\nit would be great to have more logs about this mkl_relu_ops exception.\r\nWould you mind to get logs again with more mkldnn debug information by following below steps?\r\n\r\n1. $export MKLDNN_VERBOSE=2\r\n2. run your test code again in the same console, and catch the debug log in the console output\r\n\r\nthanks\r\n", "I built TF sample code (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image) with TF 1.12 dynamic library to try to reproduce the error, but it worked well. The TF dynamic library was built by bazel build --config=mkl --copt=\"-DEIGEN_USE_VML\" -c opt //tensorflow:libtensorflow_cc.so.\r\n\r\nPlease try the latest TF code or share your test code to us.\r\nThank you.", "closing due to inactivity. "]}, {"number": 24486, "title": "Cherrypicks qp3 ro", "body": "Fixes the Raspberry pi builds", "comments": []}, {"number": 24485, "title": "Adding a 'README' file for RISC-V", "body": "Adding a simple README file for riscv32_mcu directory.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 24484, "title": "Make tensorforest deterministic by pre generate split features", "body": "Address https://github.com/tensorflow/tensorflow/issues/24040\r\n\r\nRight now even  there is a lock for the random number generator which is being used to find a feature to split on.  And there is an option to partition the data into different threads based on leaf_id.\r\nthere is still a possibility of non-deterministic by the order of consuming random numbers is not guaranteed. Which comes from the multi-thread. And it is hard to reproduce, if we only train a small tree, (set the max_leaf as a small number). since the multi-thread macro is smart enough not to use multi-thread.\r\n\r\nThis pr pre-generate random numbers as requested, sequentially single threaded and save them and partition them into different threads to ensure the deterministic.\r\n\r\n ", "comments": ["Is it ready for review? It seems it is not synced", "@yupbank Can you please solve the merge conflicts? Thanks!", "yeah, i think it's ready to review and i just rebased the branch", "Can you fix test failures? For example https://source.cloud.google.com/results/invocations/7a2d3181-e236-47c1-9e1f-5764c2167c4b/targets/%2F%2Ftensorflow%2Fcontrib%2Feager%2Fpython%2Fexamples%2Fspinn:spinn_test/log", "@yupbank gentle ping", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 24483, "title": "[TensorFlow Java] Generated factory methods for building operations do not take input list", "body": "**System information**\r\n- TensorFlow version: 1.12\r\n\r\n**Describe the current behavior**\r\nThere are automatically generated wrapper classes for Graph Operations that provide a neat interface and consistency in practice. \r\n\r\nHowever, we found that for certain Operations that required multiple inputs (i.e. Merge, Concat) the generated class was incorrect as it took a single input instead of an inputList. \r\n\r\nFor example in Merge.java:\r\n```  \r\npublic static <T> Merge<T> create(Scope scope, Operand<T> inputs) { // inputs should be a list\r\n    OperationBuilder opBuilder = scope.graph().opBuilder(\"Merge\", scope.makeOpName(\"Merge\"));\r\n    opBuilder.addInput(inputs.asOutput()); //Should be .addInputList() not .addInput()\r\n    return new Merge<T>(opBuilder.build());\r\n}\r\n```\r\n\r\n**Describe the expected behavior**\r\nRegarding the code snippet above, we need `inputs` to be a list rather than a single Operand and `.addInput()` should be `.addInputList()`. \r\n\r\n**Code to reproduce the issue**\r\n\r\n1. This code below compiles but errors during the run with the error below.\r\n```\r\n    Constant constant1 = Constant.create(scope, 1);\r\n    Enter enter = Enter.create(scope, constant1, \"Loop\");\r\n    Merge.create(scope, enter);\r\n```\r\n```\r\n[error] Exception in thread \"main\" java.lang.IllegalArgumentException: Single tensor passed to 'inputs', expected list while building NodeDef 'Merge' using Op<name=Merge; signature=inputs:N*T -> output:T, value_index:int32; attr=T:type; attr=N:int,min=1>\r\n```\r\n\r\n2. On the other hand the below code does not compile:\r\n```\r\n    Constant constant1 = Constant.create(scope, 1);\r\n    Enter enter = Enter.create(scope, constant1, \"Loop\");\r\n    Operand[] inputList = {enter};\r\n    Merge.create(scope, inputList);\r\n```\r\n```\r\n[info] Compiling 1 Java source to /Users/irenedea/linreg/target/scala-2.12/classes ...\r\n[error] /Users/irenedea/linreg/src/main/java/GraphBuilder.java:33:1: method create in class org.tensorflow.op.core.Merge<T> cannot be applied to given types;\r\n[error]   required: org.tensorflow.op.Scope,org.tensorflow.Operand<T>\r\n[error]   found: org.tensorflow.op.Scope,org.tensorflow.Operand[]\r\n[error]   reason: cannot infer type-variable(s) T\r\n[error]     (argument mismatch; org.tensorflow.Operand[] cannot be converted to org.tensorflow.Operand<T>)\r\n[error]     Merge.create(scope, inputList);\r\n```\r\n\r\n**Other info / logs**\r\nCreated a hack around this issue by building the merge from scratch with the opBuilder, but would be best to have the generated code to be correct as it provides a neat interface :)\r\n```\r\n  public <T> Operation merge(Scope scope, Output<T>[] inputs) {\r\n    return graph.opBuilder(\"Merge\", scope.makeOpName(\"Merge\"))\r\n            .addInputList(inputs).build();\r\n  }\r\n```\r\n```\r\n    Constant constant1 = Constant.create(scope, 1);\r\n    Enter enter = Enter.create(scope, constant1, \"Loop\");\r\n    Output[] inputList = {enter.asOutput()};\r\n    merge(scope, inputList);\r\n```\r\n", "comments": ["We found the issue in tensorflow/java/src/gen/cc/op_specs.cc in the TypeOf function. \r\n\r\nThe fix is a super simple change, moving an if statement block from the beginning to the end of the function. See the fix here: \r\nhttps://github.com/irenedea/tensorflow/commit/264648c4b30479a4f110bd6ce66830c2963ee4ae\r\n\"Move initial iterable check in TypeOf function from the beginning to the end of the function. (The check checks to see if the Op has an attribute N, which would signify the need for a list rather than a single input.) Previously, the iterable bool was overwritten by a later call to TypesOf in the TypeOf function. This caused iterable to be incorrectly false for Ops with attributes T: type and N: int, such as Merge and Concat.\"\r\n", "PR here: https://github.com/tensorflow/tensorflow/pull/24813\r\n\r\n@samdow @melissagrueter", "Thanks for the report and even more for the fix! Will take a look at the PR.\r\nFYI @karllessard ", "Many thanks @irenedea !", "For the record, fixed by #24813\r\nThanks @samdow @irenedea @melissagrueter "]}, {"number": 24482, "title": "[INTEL MKL] Add LRU caching to upper-bound the total number of MKL primitives in memory", "body": "New Feature: \r\n\r\n    Add LRUCache to bound the number of MKL primitives that are cached, in order to prevent memory growth when input dimensions change.  \r\n\r\n   If not cached yet, a primitive is created and then added to the cache. When cache is full and a new primitive needs to be added, the oldest primitive (based on creation/usage time) will be purged first. ", "comments": ["Nagging Reviewer @yifeif: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@gzmkl gentle ping to check the review comments", "Thanks for the comments, which I will address!", "@gzmkl please resolve branch conflicts.", "To address merge conflicts, I rebased mkl_util.h with the \"master\" branch. Thanks!", "Hi Penporn and Tatiana,\r\nI have done code change per code review suggestions from you, and addressed the\r\nmerge conflict issues. Please let me know if further change is needed. \r\nThanks\r\nGZ"]}, {"number": 24481, "title": "TFTRT: Convert between str and unicode in py2", "body": "We need to convert string to binary before storing a value in protobufs. We already had this encoding for python3 but not for python2. This PR adds that.\r\n\r\nThis PR also makes sure we only do the conversion if the input type is right (avoid encoding a binary or decoding a string).", "comments": []}, {"number": 24480, "title": "GPU detected on 1.11 not 1.12", "body": "### System information\r\nFollowed tensorflow and nvidia install instructions.\r\nconda env python=3.6\r\n\r\nWindows 10\r\nGPU = Quadro P1000 compute 6.1\r\nCuda = 9.0 with patches 2,3,4. Patch 1 doesn't seem to go through. \r\ncuDNN = 7.4.2.24\r\n\r\ntensorflow = 1.12\r\n\r\nAdded cuda, cudnn, and cupti to path.\r\n\r\n### Describe the problem\r\nInstallation goes smoothly for tensorflow-gpu for both 1.11 and 1.12. However when using the command:\r\n\r\n```\r\nfrom tensorflow.python.client import device_lib\r\ndevice_lib.list_local_devices()\r\n```\r\n\r\nNo GPU device is listed for v1.12. GPU is detected on v.1.11\r\n\r\n", "comments": []}, {"number": 24479, "title": "How to define a loss function that needs to input numpy array(not tensor) when build a tensorflow graph?", "body": "hello,\r\nI want to add a constraint option in my loss function. The definition of this constraint option needs numpy array type as input. So, I can not define it as a tensor type as a graph node in tensorflow. How can I define this part in graph so as to join in the network optimization?\r\n\r\n", "comments": ["Please post support related questions in [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow). We encourage users to submit an issue/feature request here. "]}, {"number": 24478, "title": "Constant Folding: Only apply size limit if size of tensor is increasing", "body": "Many kernels have sizes larger than the constfolding limit of 10mb. Many networks use identity ops or pointwise scaling ops on these kernels. The size limit prevents constant folding in these cases even though the output of the folding would not have increased the size of the protobuf, and in many cases would actually decrease it by removing the ops.\r\n\r\nIn TFTRT, TRT requires weights to be known statically so we rely heavily on constant folding. If a weight is not constant folded, we are unable to convert that layer into TRT.\r\n\r\nThis fixes https://github.com/tensorflow/tensorflow/issues/24083", "comments": ["FYI @rmlarsen @ezhulenev ", "Sorry, I think I wasn't using the correct input size originally.\r\n\r\nThere is still a problem where sometimes it thinks the input size is 0 (occuring with Square op in https://github.com/tensorflow/tensorflow/issues/24083)", "Ah I keep reviewing as you're commenting/making changes :) Are you planning to address the Square op issue in this PR? If so, please let me know when it's ready to review.", "> Ah I keep reviewing as you're commenting/making changes :) Are you planning to address the Square op issue in this PR? If so, please let me know when it's ready to review.\r\n\r\nSorry about that, I discovered the problems shortly after creating the pr. I will let you know after I have investigated the square issue. Thanks for reviewing!", "@skye I have fixed the problem now.", "Thanks for the fix."]}, {"number": 24477, "title": "FusedBatchNorm with is_training = True is folded into unsupported ops", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip binary for 1.12\r\n- TensorFlow version (or github SHA if from source):  ('v1.12.0-0-ga6d8ffae09', '1.12.0')\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nRuntimeError: TOCO failed see console for info.\r\n2018-12-20 11:50:10.890031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Size\r\n2018-12-20 11:50:10.890062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2\r\n2018-12-20 11:50:10.890069: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2\r\n2018-12-20 11:50:10.890075: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2\r\n2018-12-20 11:50:10.890241: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 28 operators, 42 arrays (0 quantized)\r\n2018-12-20 11:50:10.890357: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 28 operators, 42 arrays (0 quantized)\r\n2018-12-20 11:50:10.890666: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 20 operators, 31 arrays (1 quantized)\r\n2018-12-20 11:50:10.890848: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 20 operators, 31 arrays (1 quantized)\r\n2018-12-20 11:50:10.890954: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 19 operators, 30 arrays (1 quantized)\r\n2018-12-20 11:50:10.891050: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 19 operators, 30 arrays (1 quantized)\r\n2018-12-20 11:50:10.891093: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 19 operators, 30 arrays (1 quantized)\r\n2018-12-20 11:50:10.891172: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 19 operators, 30 arrays (1 quantized)\r\n2018-12-20 11:50:10.891194: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:100] Constant array filter_0 lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.\r\n2018-12-20 11:50:10.891217: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:474] Unimplemented: this graph contains an operator of type (Unsupported TensorFlow op: Size) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nAborted (core dumped)\r\n\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nWhen quantizing a FusedBatchNorm op with attribute is_training = True, [this method in fold_batch_norms.py ](https://github.com/tensorflow/tensorflow/blob/1fc046c3a8eb62690cd78a6da1b62463e9133f6d/tensorflow/contrib/quantize/python/fold_batch_norms.py#L267) performs a Bessel correction which adds currently unsupported Size and Cast operators to the graph.\r\n\r\n```python\r\n\"\"\" Example of issue with TFLite support caused by FoldBatchNorms\r\n\r\n    By applying a bessel correction to the graph using unsupported ops, the\r\n    resulting graph cannot be exported as a UINT8, quantized, TFLite model\r\n\"\"\"\r\n\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.python.tools import freeze_graph\r\nfrom tensorflow.contrib.lite.python import lite_constants as constants\r\nfrom foldbatchnorms import FoldBatchNorms\r\nfrom tensorflow.contrib.quantize.python import quantize\r\n\r\n# inits\r\nCHECKPOINT_PATH = os.path.join('checkpoint', 'chkp')\r\nQUANT_EVAL_GRAPH = 'model_quant_eval.pb'\r\nFREEZE_GRAPH = 'model_quant_eval_freeze.pb'\r\nTFLITE_PATH = 'model_quant_eval_freeze.tflite'\r\n\r\nOUTPUT_NODE = 'out'\r\nINPUT_NODE = 'input'\r\nINPUT_STAT = {'input' : (0., 1.)}\r\n\r\n# define a simple graph which includes a FusedBatchNorm operator\r\ninp = tf.placeholder(shape=[None,299,299,3], dtype=tf.float32, name='input')\r\nscale = tf.ones(shape=[32])\r\noffset = tf.zeros(shape=[32])\r\n_filter = tf.get_variable(\"filter\", shape=[3,3,3,32],\r\n                          initializer=tf.ones_initializer())\r\n\r\nconv = tf.nn.conv2d(input=inp,\r\n                    filter=_filter,\r\n                    data_format='NHWC',\r\n                    dilations=[1, 1, 1, 1],\r\n                    strides=[1, 2, 2, 1],\r\n                    padding='VALID')\r\n\r\ny, _, _ = tf.nn.fused_batch_norm(x=conv,\r\n                                 scale=scale,\r\n                                 offset=offset,\r\n                                 epsilon=0.0010000000474974513,\r\n                                 is_training=True,\r\n                                 data_format='NHWC')\r\ntf.nn.relu(y, name='out')\r\n\r\n\r\nwith tf.Session() as sess:\r\n\r\n    # quantize graph in-place using FakeQuant* ops (calls fold_batch_norms.py)\r\n    tf.contrib.quantize.create_eval_graph()\r\n\r\n    # init vars\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    # save metagraph .pb for converting to TFLite\r\n    with open(QUANT_EVAL_GRAPH, 'w') as f:\r\n        f.write(str(sess.graph.as_graph_def()))\r\n\r\n    saver = tf.train.Saver()\r\n    saver.save(sess, CHECKPOINT_PATH)\r\n\r\n    # freeze the quantized (FakeQuant*) graph\r\n    freeze_graph.freeze_graph(input_graph=QUANT_EVAL_GRAPH,\r\n                              input_saver='',\r\n                              input_binary=False,\r\n                              input_checkpoint=CHECKPOINT_PATH,\r\n                              output_node_names=OUTPUT_NODE,\r\n                              restore_op_name=\"save/restore_all\",\r\n                              filename_tensor_name=\"save/Const:0\",\r\n                              output_graph=FREEZE_GRAPH,\r\n                              clear_devices=False,\r\n                              initializer_nodes='')\r\n\r\n# convert frozen, quantized, eval graph to UINT8-quant TFLite model\r\nconverter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(\r\n                                        graph_def_file=FREEZE_GRAPH,\r\n                                        input_arrays=[INPUT_NODE],\r\n                                        output_arrays=[OUTPUT_NODE])\r\n\r\nconverter.inference_type = constants.QUANTIZED_UINT8\r\nconverter.inference_input_type = constants.QUANTIZED_UINT8\r\nconverter.quantized_input_stats = INPUT_STAT\r\nconverter.default_ranges_stats = (0,128) # <- bypass Conv2D missing Min/Max data\r\ntflite_model = converter.convert() # <- this will fail\r\n```\r\n\r\nPre-quantization graph containing FusedBatchNorm, Conv2D and Relu ops:\r\n![pre_quant_graph](https://user-images.githubusercontent.com/3189865/50296689-0e446d80-0449-11e9-9d84-2ce60e8d88f2.png)\r\n\r\nPost-quantization graph containing unsupported ops:\r\n![post_quant_graph](https://user-images.githubusercontent.com/3189865/50296694-100e3100-0449-11e9-96de-18a7f6906d48.png)\r\n", "comments": ["The TFLite converter expects an eval graph as input. This means a graph where all calls to batch norm are created with is_training=False.\r\n\r\nSo, you traing a float model with is_training=True and create_training_graph. Save the checkpoint.\r\nThen you build the same model with is_training=False and create_eval_graph. This is the graph that the Tfliteconverter expects.", "> The TFLite converter expects an eval graph as input. This means a graph where all calls to batch norm are created with is_training=False.\r\n> \r\n> So, you traing a float model with is_training=True and create_training_graph. Save the checkpoint.\r\n> Then you build the same model with is_training=False and create_eval_graph. This is the graph that the Tfliteconverter expects.\r\n\r\nhi, I meet a problem during these days. I want get a tflite model using inference by uint8. my network include the batchnormalization layer. when I change the pb model to the tflite. there is always return a error message\r\n \"first_bn/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array kws_model/KWS_Model/tower_0/CNN_V1/Relu, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\\nAborted (core dumped)\"\r\n\r\nif I add the \"converter.default_ranges_stats=(0, 6)\" or remove the bn layer  can fix the error , but I know its not the best method. please give me some suggestions. thanks.", "Hi, I met the same problem @PaulMcInnis, did you solved it ?", "> Hi, I met the same problem @PaulMcInnis, did you solved it ?\r\n\r\nThe model you are using for inference should not have any batch normalization layers with `is_training=True` You need to make changes to your graph to set `is_training=False` prior to TFLite conversion. ", "Hi, Thanks for your reply  @PaulMcInnis, I think my batch normalization layers are with is_trainging=False in the inference. Here is what I have done:\r\nstep 1. I train the model with quantized aware training, following the instruction in [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize). \r\nstep 2. I use an evaluation script to reload the model and freeze it. The code looks like this:\r\n````\r\n    with tf.Session(config=config) as sess:\r\n        loader = tf.train.Saver()#net.restorable_variables())\r\n        loader.restore(sess, pretrain_path)\r\n        frozen_graph_def = graph_util.convert_variables_to_constants(\r\n            sess, sess.graph_def, ['Openpose/concat_stage7'])\r\n        eval_graph_file = base_dir + '/graph.pb'\r\n        tf.train.write_graph(\r\n            frozen_graph_def,\r\n            os.path.dirname(eval_graph_file),\r\n            os.path.basename(eval_graph_file),\r\n            as_text=False)\r\n        tf.logging.info('Saved frozen graph to %s', eval_graph_file)\r\n\r\n````\r\n\r\nstep3. I use freeze_graph.py to freeze my code.\r\nstep4. tf.lite.TFLiteConverter.from_frozen_graph is utilized to convert the model to tflite file\r\n\r\nstep1-3 is working well. But I get the same error in step 4: unsupported op type cast. Here is the graph that related to the cast operation. Can you give me some clues? I checked the Internet, in the official Google example of a quantized graph, there is no fuse batch norm. I don't know how to achieve that. Any suggestion.\r\n![WeChat Screenshot_20190613111611](https://user-images.githubusercontent.com/33747685/59415738-a3bc2b00-8df6-11e9-9e55-507bfe8870a2.png)\r\n", "@caiya55 \r\nhave you solved this issue?\r\n\r\n"]}]