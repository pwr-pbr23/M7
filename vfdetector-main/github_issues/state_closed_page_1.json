[{"number": 55593, "title": "replace learning_rate_schedule with tensorflow.keras.optimizers.schedules.", "body": "Addressing #55510\r\n", "comments": ["nice work\uff0clook forward to merge", "It looks like your PR relates to the Keras component. Please submit it to the github.com/keras-team/keras repository instead. Thankyou.\r\n@fchollet, @qlzh727\r\n"]}, {"number": 55591, "title": "ImportError: cannot import name 'deepmac_meta_arch' from 'object_detection.meta_architectures'", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Google colab\r\n- TensorFlow installed from (source or binary): Google colab (default)\r\n- TensorFlow version: 1.15.2\r\n- Python version: 3.7.13\r\n- Installed using virtualenv? pip? conda?: Google Colab (no env)\r\n- Bazel version (if compiling from source): Google colab (default)\r\n- GCC/Compiler version (if compiling from source): Google colab (default)\r\n- CUDA/cuDNN version: Google colab (default)\r\n- GPU model and memory: Google colab (default)\r\n\r\n\r\n\r\n**Describe the problem**\r\nTrying to train custom object detector referring blog [Custom object detection in the browser using TensorFlow.js](https://blog.tensorflow.org/2021/01/custom-object-detection-in-browser.html)\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nRan these blocks one after other inside Google colab\r\n```\r\nfrom google.colab import drive\r\ndrive.mount('/content/drive/')\r\n```\r\n\r\n```\r\n%cd /content/drive/MyDrive/Tensorflow/models/research/\r\n!protoc object_detection/protos/*.proto --python_out=.\r\n# Install TensorFlow Object Detection API.\r\n!cp object_detection/packages/tf2/setup.py .\r\n```\r\n\r\n```\r\n!pip install tensorflow-object-detection-api\r\n```\r\n\r\n```\r\n%tensorflow_version 1.x\r\n```\r\n\r\n```\r\n%cd /content/drive/MyDrive/Tensorflow/models/research/slim/\r\n```\r\n\r\n```\r\n%%bash\r\nexport PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\r\n\r\npython setup.py build\r\npython setup.py install\r\n```\r\n\r\n```\r\n!python /content/drive/MyDrive/Tensorflow/models/research/object_detection/builders/model_builder_tf2_test.py\r\n```\r\n\r\nError in this block\r\n```\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nTraceback (most recent call last):\r\n  File \"/content/drive/MyDrive/Tensorflow/models/research/object_detection/builders/model_builder_tf2_test.py\", line 27, in <module>\r\n    from object_detection.meta_architectures import deepmac_meta_arch\r\nImportError: cannot import name 'deepmac_meta_arch' from 'object_detection.meta_architectures' (/usr/local/lib/python3.7/dist-packages/object_detection/meta_architectures/__init__.py)\r\n```\r\n\r\n", "comments": ["Since you are using `TF1.x`, \r\n\r\n1. Can you try to install `TF1.x` Object Detection API instead of `TF2.x` Object Detection API as shown below\r\n\r\n`cp object_detection/packages/tf1/setup.py \r\n`\r\n\r\n2. Can you try to change `model_builder_tf2_test.py` to `model_builder_tf1_test.py` as shown below\r\n\r\n`!python /content/drive/MyDrive/Tensorflow/models/research/object_detection/builders/model_builder_tf1_test.py\r\n`\r\n\r\nOr simple workaround is try to use `%tensorflow_version 2.x` instead of `%tensorflow_version 1.x`. \r\n\r\nFor more details please refer [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1.md). Thanks!", "> Since you are using `TF1.x`,\r\n> \r\n> 1. Can you try to install `TF1.x` Object Detection API instead of `TF2.x` Object Detection API as shown below\r\n> \r\n> `cp object_detection/packages/tf1/setup.py `\r\n> \r\n> 2. Can you try to change `model_builder_tf2_test.py` to `model_builder_tf1_test.py` as shown below\r\n> \r\n> `!python /content/drive/MyDrive/Tensorflow/models/research/object_detection/builders/model_builder_tf1_test.py `\r\n> \r\n> Or simple workaround is try to use `%tensorflow_version 2.x` instead of `%tensorflow_version 1.x`.\r\n> \r\n> For more details please refer [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1.md). Thanks!\r\n\r\nThank you it started working ", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 55589, "title": "lite: Update tflite_runtime builds", "body": "Switch to Makefile method which is easy to maintain and less dependent to TensorFlow scripts.\r\n\r\nPiperOrigin-RevId: 427943932\r\nChange-Id: I0aff1caaf34a890566ad8a745e8a1d417828c20a", "comments": []}, {"number": 55588, "title": "ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 219566, 6), found shape=(None, 6)", "body": "Guys, do you have any idea how to solve this problem (LSTM, Keras, Tensorflow)? \r\n---------------------------------------------------------------------------\r\nI looked through the similar topics but suggested solutions do not work for me. The code is pretty simple:\r\n\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport sklearn\r\nimport keras_tuner as kt\r\n\r\nfrom tensorflow import keras\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom sklearn.model_selection import train_test_split\r\nfrom tensorflow.keras import layers\r\n\r\ndf_training_data = pd.read_csv (r'G:\\Research\\Oils\\REQUESTS\\US_data\\ProdAbo_AllHeaders_csv.csv')\r\n\r\nx_total = df_training_data.loc[:,['Lat_X1','Lon_X2','Month_X3',\r\n                                  'Well_length_X5','Density_X7','Month_from_start_X8']]\r\n\r\ny_total = df_training_data.loc[:,'Liquids_Y1']\r\n\r\nscaler = MinMaxScaler()\r\nx_total = scaler.fit_transform(x_total)\r\n\r\nx_total = np.array(x_total).astype(np.float32)\r\ny_total = np.array(y_total).astype(np.float32)\r\n\r\nx_train, x_test, y_train, y_test = train_test_split(x_total, y_total, test_size = 0.3, random_state = 0)\r\n\r\nnum_rows, num_cols = x_train.shape\r\n\r\nx_train = tf.reshape(x_train, [num_rows, num_cols])\r\ny_train = tf.reshape(y_train, [num_rows, 1])\r\n\r\nbatch_size = 100\r\nnum_epochs = 5\r\n\r\nmodel = tf.keras.models.Sequential([tf.keras.layers.LSTM(num_cols, \r\n                                                         input_shape=(num_rows,num_cols), \r\n                                                         activation = tf.nn.tanh,\r\n                                                         return_sequences=True),        \r\n                                    tf.keras.layers.LSTM(10, activation = tf.nn.tanh),\r\n                                    tf.keras.layers.Dense(1, activation = tf.nn.relu)]) \r\n\r\nmodel.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\r\n\r\nmodel.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(x_test, y_test))\r\n\r\n\r\nAnd here we are:\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-18-37ddda0f53d6> in <module>\r\n      3 num_epochs = 5\r\n      4 \r\n----> 5 model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(x_test, y_test))\r\n      6 \r\n      7 #model.fit(train_data, epochs=num_epochs, batch_size=batch_size, validation_data=test_data)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py in error_handler(*args, **kwargs)\r\n     65     except Exception as e:  # pylint: disable=broad-except\r\n     66       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n---> 67       raise e.with_traceback(filtered_tb) from None\r\n     68     finally:\r\n     69       del filtered_tb\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in autograph_handler(*args, **kwargs)\r\n   1145           except Exception as e:  # pylint:disable=broad-except\r\n   1146             if hasattr(e, \"ag_error_metadata\"):\r\n-> 1147               raise e.ag_error_metadata.to_exception(e)\r\n   1148             else:\r\n   1149               raise\r\n\r\nValueError: in user code:\r\n\r\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\r\n        return step_function(self, iterator)\r\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\r\n        outputs = model.train_step(data)\r\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\r\n        y_pred = self(x, training=True)\r\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\r\n        raise e.with_traceback(filtered_tb) from None\r\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\r\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\r\n\r\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 219566, 6), found shape=(None, 6)", "comments": ["@donskoyaleksander,\r\n\r\nCan you let us know the shape of `x_train `and `y_train`?  If possible please can you share `ProdAbo_AllHeaders_csv` dataset to replicate your issue.  \r\n\r\nPlease can you refer [issue1](https://stackoverflow.com/a/67047944/14290681), [issue2](https://stackoverflow.com/a/70904517/14290681) which also discuss about the similar issue and let us know if it helps? Thanks! ", "@chunduriv \r\n\r\nData shape is as follows:\r\nx_train (219566, 6)\r\ny_train (219566, 1)\r\n\r\nI looked through the provided issues but the suggested solutions do not work.\r\nIt would be difficult to share the underlying data unfortunately.\r\nThank you!\r\n", "Problem resolved:\r\n\r\n**x_train = np.reshape(x_train, (x_train.shape[0],1,x_train.shape[1]))\r\nx_test = np.reshape(x_test, (x_test.shape[0],1,x_test.shape[1]))**\r\n\r\nmodel = tf.keras.models.Sequential([tf.keras.layers.LSTM(num_cols, \r\n                                                         **input_shape=(1,num_cols),** \r\n                                                         activation = tf.nn.tanh,\r\n                                                         return_sequences=True),        \r\n                                    tf.keras.layers.LSTM(10, activation = tf.nn.tanh), \r\n                                    tf.keras.layers.Dense(1, activation = tf.nn.relu)]) \r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55588\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55588\">No</a>\n"]}, {"number": 55587, "title": "Fix wrong source file name when building Tensorflow Lite C using CMake", "body": "fix wrong source file name", "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/55587/checks?check_run_id=5991663834).", "Hi @HedgeHao  This PR is duplicate of [PR#54566](https://github.com/tensorflow/tensorflow/pull/54566). Hence closing this PR. Thank you for your contribution. "]}, {"number": 55586, "title": "how to define representative_data_gen function when model has two inputs?", "body": "https://www.tensorflow.org/lite/performance/post_training_integer_quant:\r\n\r\ndef representative_data_gen():\r\n  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\r\n    # Model has only one input so each data point has one element.\r\n    yield [input_value]\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\ntflite_model_quant = converter.convert()\r\n\r\nMy question is, how to modify the code listed above, when model has two or more inputs?", "comments": ["Hi @njuhang ! You have to add the other input in the yield line of the representative Dataset. Please use the [input signature](https://www.tensorflow.org/lite/guide/signatures) to keep the inputs in order . Attaching relevant[ thread](https://github.com/tensorflow/tensorflow/issues/46732) for reference. Thanks!", "yes, I solve the problem", "@njuhang ! Thanks for confirmation . Moving this issue to closed status then.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55586\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55586\">No</a>\n"]}, {"number": 55584, "title": "Android- No implementation found.", "body": "Hello everyone. I'm using tensorflow for background removal of an object. Every thing is working great but I'm keep getting this exception on the first time of model load. once the model is loaded it did not occurred.\r\n\r\n**Stack Trace:**\r\nNo implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\nI/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\r\nI/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)\r\n\r\n**Code:**\r\n\r\n   private var sTFInterface: TensorFlowInferenceInterface? = null\r\n   sTFInterface = TensorFlowInferenceInterface(context.assets, MODEL_FILE)\r\n\r\n**Dependencies:**\r\n    implementation 'org.tensorflow:tensorflow-lite:2.8.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.8.0'", "comments": ["Hi @ahmadbajwa8282 ! Contrib has depreciated in 2.x version and not supported any more. Please select the suitable lite version in build.gradle according to your Tensorflow version. Thanks!", "@mohantym  but according to the repo the latest version is 2.8.0.", "@ahmadbajwa8282 ! I traced this \"TensorFlowInferenceInterface\" keyword in stack trace to [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/android) which says that it is depreciated and  use TFlite instead. Could you share the link to that repo though? ", "@mohantym Ok thanks for your help.", "@mohantym Can you explain or share an exmaple how to use TFlite?", "@ahmadbajwa8282 ! You can get a TFlite model from your TF model using TFliteConverter . Attaching [sample notebook](https://colab.research.google.com/github/frogermcs/TFLite-Tester/blob/master/notebooks/Testing_TFLite_model.ipynb) and [guide ](https://www.tensorflow.org/lite/convert)for reference. Thanks!", "@mohantym thanks for the help. I have implemented Interpreter for reading model lets see the result how it works. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55584\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55584\">No</a>\n"]}, {"number": 55582, "title": "Cannot install tensorflow-gpu==1.15.0", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 20.04\r\n- TensorFlow version: tensorflow-gpu==1.15.0\r\n- Python version: 3.17.13\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: \r\n\r\nDescribe the problem\r\nI get the following error when trying to install tensorflow-gpu in version  1.15.0:\r\n```\r\nUsing pip 22.0.4 from /home/dmitriy/.local/lib/python3.8/site-packages/pip (python 3.8)\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.15.0 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.8.0rc0, 2.8.0rc1, 2.8.0)\r\nERROR: No matching distribution found for tensorflow-gpu==1.15.0\r\n```\r\nAfter I try change this line in file `requirements.txt`.\r\n```\r\ntensorflow-gpu==1.15.0\r\n```\r\nto \r\n```\r\ntensorflow==2.8.0\r\n```\r\nAnd I've got this error:\r\n```\r\nRunning command python setup.py egg_info\r\n  fatal: not a git repository (or any of the parent directories): .git\r\n  Traceback (most recent call last):\r\n    File \"<string>\", line 2, in <module>\r\n    File \"<pip-setuptools-caller>\", line 34, in <module>\r\n    File \"/tmp/pip-install-4b7bdwir/onnx_617da0f137a840d89f00ab87f0ff4f40/setup.py\", line 72, in <module>\r\n      assert CMAKE, 'Could not find \"cmake\" executable!'\r\n  AssertionError: Could not find \"cmake\" executable!\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py egg_info did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> See above for output.\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  full command: /usr/bin/python3 -c '\r\n  exec(compile('\"'\"''\"'\"''\"'\"'\r\n  # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\r\n  #\r\n  # - It imports setuptools before invoking setup.py, to enable projects that directly\r\n  #   import from `distutils.core` to work with newer packaging standards.\r\n  # - It provides a clear error message when setuptools is not installed.\r\n  # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so\r\n  #   setuptools doesn'\"'\"'t think the script is `-c`. This avoids the following warning:\r\n  #     manifest_maker: standard file '\"'\"'-c'\"'\"' not found\".\r\n  # - It generates a shim setup.py, for handling setup.cfg-only projects.\r\n  import os, sys, tokenize\r\n  \r\n  try:\r\n      import setuptools\r\n  except ImportError as error:\r\n      print(\r\n          \"ERROR: Can not execute `setup.py` since setuptools is not available in \"\r\n          \"the build environment.\",\r\n          file=sys.stderr,\r\n      )\r\n      sys.exit(1)\r\n  \r\n  __file__ = %r\r\n  sys.argv[0] = __file__\r\n  \r\n  if os.path.exists(__file__):\r\n      filename = __file__\r\n      with tokenize.open(__file__) as f:\r\n          setup_py_code = f.read()\r\n  else:\r\n      filename = \"<auto-generated setuptools caller>\"\r\n      setup_py_code = \"from setuptools import setup; setup()\"\r\n  \r\n  exec(compile(setup_py_code, filename, \"exec\"))\r\n  '\"'\"''\"'\"''\"'\"' % ('\"'\"'/tmp/pip-install-4b7bdwir/onnx_617da0f137a840d89f00ab87f0ff4f40/setup.py'\"'\"',), \"<pip-setuptools-caller>\", \"exec\"))' egg_info --egg-base /tmp/pip-pip-egg-info-zivjykrk\r\n  cwd: /tmp/pip-install-4b7bdwir/onnx_617da0f137a840d89f00ab87f0ff4f40/\r\n  Preparing metadata (setup.py) ... error\r\nerror: metadata-generation-failed\r\n\r\n\u00d7 Encountered error while generating package metadata.\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n\r\n```\r\n\r\n\r\nAfter that I install `cmake` by `sudo apt install cmake`. And I've got this error:\r\n```\r\nThe conflict is caused by:\r\n    The user requested keras==2.3.1\r\n    tensorflow 2.8.0 depends on keras<2.9 and >=2.8.0rc0\r\n\r\nTo fix this you could try to:\r\n1. loosen the range of package versions you've specified\r\n2. remove package versions to allow pip attempt to solve the dependency conflict\r\n```\r\nI changed keras version in file `requirements.txt`.\r\n```\r\nkeras==2.3.1\r\n```\r\nto\r\n```\r\nkeras==2.8.0\r\n```\r\nAnd then I 've got error: \r\n```\r\nThe conflict is caused by:\r\n    The user requested numpy==1.19.1\r\n    tensorflow 2.8.0 depends on numpy>=1.20\r\n\r\nTo fix this you could try to:\r\n1. loosen the range of package versions you've specified\r\n2. remove package versions to allow pip attempt to solve the dependency conflict\r\n```  \r\nI changed numpy version in file `requirements.txt`.\r\n```\r\nnumpy==1.19.1\r\n```\r\nto\r\n```\r\nnumpy==1.22.0\r\n```\r\nthen i have got error:\r\n```\r\n -- Configuring incomplete, errors occurred!\r\n  See also \"/tmp/pip-install-cxnneqyj/onnx_1ec1fa5d6a8d47ca9cbff428da18c916/.setuptools-cmake-build/CMakeFiles/CMakeOutput.log\".\r\n  See also \"/tmp/pip-install-cxnneqyj/onnx_1ec1fa5d6a8d47ca9cbff428da18c916/.setuptools-cmake-build/CMakeFiles/CMakeError.log\".\r\n  Traceback (most recent call last):\r\n    File \"<string>\", line 2, in <module>\r\n    File \"<pip-setuptools-caller>\", line 34, in <module>\r\n    File \"/tmp/pip-install-cxnneqyj/onnx_1ec1fa5d6a8d47ca9cbff428da18c916/setup.py\", line 315, in <module>\r\n      setuptools.setup(\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/__init__.py\", line 87, in setup\r\n      return distutils.core.setup(**attrs)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/core.py\", line 148, in setup\r\n      return run_commands(dist)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/core.py\", line 163, in run_commands\r\n      dist.run_commands()\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 967, in run_commands\r\n      self.run_command(cmd)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/dist.py\", line 1214, in run_command\r\n      super().run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 986, in run_command\r\n      cmd_obj.run()\r\n    File \"/usr/lib/python3/dist-packages/wheel/bdist_wheel.py\", line 223, in run\r\n      self.run_command('build')\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/cmd.py\", line 313, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/dist.py\", line 1214, in run_command\r\n      super().run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 986, in run_command\r\n      cmd_obj.run()\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/command/build.py\", line 136, in run\r\n      self.run_command(cmd_name)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/cmd.py\", line 313, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/dist.py\", line 1214, in run_command\r\n      super().run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 986, in run_command\r\n      cmd_obj.run()\r\n    File \"/tmp/pip-install-cxnneqyj/onnx_1ec1fa5d6a8d47ca9cbff428da18c916/setup.py\", line 209, in run\r\n      self.run_command('cmake_build')\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/cmd.py\", line 313, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/dist.py\", line 1214, in run_command\r\n      super().run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 986, in run_command\r\n      cmd_obj.run()\r\n    File \"/tmp/pip-install-cxnneqyj/onnx_1ec1fa5d6a8d47ca9cbff428da18c916/setup.py\", line 195, in run\r\n      subprocess.check_call(cmake_args)\r\n    File \"/usr/lib/python3.8/subprocess.py\", line 364, in check_call\r\n      raise CalledProcessError(retcode, cmd)\r\n  subprocess.CalledProcessError: Command '['/usr/bin/cmake', '-DPYTHON_INCLUDE_DIR=/usr/include/python3.8', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DBUILD_ONNX_PYTHON=ON', '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON', '-DONNX_NAMESPACE=onnx', '-DPY_EXT_SUFFIX=.cpython-38-x86_64-linux-gnu.so', '-DCMAKE_BUILD_TYPE=Release', '-DONNX_ML=1', '/tmp/pip-install-cxnneqyj/onnx_1ec1fa5d6a8d47ca9cbff428da18c916']' returned non-zero exit status 1.\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> See above for output.\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  full command: /usr/bin/python3 -u -c '\r\n  exec(compile('\"'\"''\"'\"''\"'\"'\r\n  # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\r\n  #\r\n  # - It imports setuptools before invoking setup.py, to enable projects that directly\r\n  #   import from `distutils.core` to work with newer packaging standards.\r\n  # - It provides a clear error message when setuptools is not installed.\r\n  # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so\r\n  #   setuptools doesn'\"'\"'t think the script is `-c`. This avoids the following warning:\r\n  #     manifest_maker: standard file '\"'\"'-c'\"'\"' not found\".\r\n  # - It generates a shim setup.py, for handling setup.cfg-only projects.\r\n  import os, sys, tokenize\r\n  \r\n  try:\r\n      import setuptools\r\n  except ImportError as error:\r\n      print(\r\n          \"ERROR: Can not execute `setup.py` since setuptools is not available in \"\r\n          \"the build environment.\",\r\n          file=sys.stderr,\r\n      )\r\n      sys.exit(1)\r\n  \r\n  __file__ = %r\r\n  sys.argv[0] = __file__\r\n  \r\n  if os.path.exists(__file__):\r\n      filename = __file__\r\n      with tokenize.open(__file__) as f:\r\n          setup_py_code = f.read()\r\n  else:\r\n      filename = \"<auto-generated setuptools caller>\"\r\n      setup_py_code = \"from setuptools import setup; setup()\"\r\n  \r\n  exec(compile(setup_py_code, filename, \"exec\"))\r\n  '\"'\"''\"'\"''\"'\"' % ('\"'\"'/tmp/pip-install-cxnneqyj/onnx_1ec1fa5d6a8d47ca9cbff428da18c916/setup.py'\"'\"',), \"<pip-setuptools-caller>\", \"exec\"))' bdist_wheel -d /tmp/pip-wheel-i5wyi68e\r\n  cwd: /tmp/pip-install-cxnneqyj/onnx_1ec1fa5d6a8d47ca9cbff428da18c916/\r\n  Building wheel for onnx (setup.py) ... error\r\n  ERROR: Failed building wheel for onnx\r\n  Running setup.py clean for onnx\r\n  Running command python setup.py clean\r\n```\r\nand second error\r\n```\r\n-- Configuring incomplete, errors occurred!\r\n  See also \"/tmp/pip-install-uou042w3/onnx_f2684108a6604e46b4d58ca7bca088b7/.setuptools-cmake-build/CMakeFiles/CMakeOutput.log\".\r\n  See also \"/tmp/pip-install-uou042w3/onnx_f2684108a6604e46b4d58ca7bca088b7/.setuptools-cmake-build/CMakeFiles/CMakeError.log\".\r\n  Traceback (most recent call last):\r\n    File \"<string>\", line 2, in <module>\r\n    File \"<pip-setuptools-caller>\", line 34, in <module>\r\n    File \"/tmp/pip-install-uou042w3/onnx_f2684108a6604e46b4d58ca7bca088b7/setup.py\", line 315, in <module>\r\n      setuptools.setup(\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/__init__.py\", line 87, in setup\r\n      return distutils.core.setup(**attrs)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/core.py\", line 148, in setup\r\n      return run_commands(dist)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/core.py\", line 163, in run_commands\r\n      dist.run_commands()\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 967, in run_commands\r\n      self.run_command(cmd)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/dist.py\", line 1214, in run_command\r\n      super().run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 986, in run_command\r\n      cmd_obj.run()\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/command/install.py\", line 68, in run\r\n      return orig.install.run(self)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/command/install.py\", line 670, in run\r\n      self.run_command('build')\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/cmd.py\", line 313, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/dist.py\", line 1214, in run_command\r\n      super().run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 986, in run_command\r\n      cmd_obj.run()\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/command/build.py\", line 136, in run\r\n      self.run_command(cmd_name)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/cmd.py\", line 313, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/dist.py\", line 1214, in run_command\r\n      super().run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 986, in run_command\r\n      cmd_obj.run()\r\n    File \"/tmp/pip-install-uou042w3/onnx_f2684108a6604e46b4d58ca7bca088b7/setup.py\", line 209, in run\r\n      self.run_command('cmake_build')\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/cmd.py\", line 313, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/dist.py\", line 1214, in run_command\r\n      super().run_command(command)\r\n    File \"/home/dmitriy/.local/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 986, in run_command\r\n      cmd_obj.run()\r\n    File \"/tmp/pip-install-uou042w3/onnx_f2684108a6604e46b4d58ca7bca088b7/setup.py\", line 195, in run\r\n      subprocess.check_call(cmake_args)\r\n    File \"/usr/lib/python3.8/subprocess.py\", line 364, in check_call\r\n      raise CalledProcessError(retcode, cmd)\r\n  subprocess.CalledProcessError: Command '['/usr/bin/cmake', '-DPYTHON_INCLUDE_DIR=/usr/include/python3.8', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DBUILD_ONNX_PYTHON=ON', '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON', '-DONNX_NAMESPACE=onnx', '-DPY_EXT_SUFFIX=.cpython-38-x86_64-linux-gnu.so', '-DCMAKE_BUILD_TYPE=Release', '-DONNX_ML=1', '/tmp/pip-install-uou042w3/onnx_f2684108a6604e46b4d58ca7bca088b7']' returned non-zero exit status 1.\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Running setup.py install for onnx did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> See above for output.\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  full command: /usr/bin/python3 -u -c '\r\n  exec(compile('\"'\"''\"'\"''\"'\"'\r\n  # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\r\n  #\r\n  # - It imports setuptools before invoking setup.py, to enable projects that directly\r\n  #   import from `distutils.core` to work with newer packaging standards.\r\n  # - It provides a clear error message when setuptools is not installed.\r\n  # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so\r\n  #   setuptools doesn'\"'\"'t think the script is `-c`. This avoids the following warning:\r\n  #     manifest_maker: standard file '\"'\"'-c'\"'\"' not found\".\r\n  # - It generates a shim setup.py, for handling setup.cfg-only projects.\r\n  import os, sys, tokenize\r\n  \r\n  try:\r\n      import setuptools\r\n  except ImportError as error:\r\n      print(\r\n          \"ERROR: Can not execute `setup.py` since setuptools is not available in \"\r\n          \"the build environment.\",\r\n          file=sys.stderr,\r\n      )\r\n      sys.exit(1)\r\n  \r\n  __file__ = %r\r\n  sys.argv[0] = __file__\r\n  \r\n  if os.path.exists(__file__):\r\n      filename = __file__\r\n      with tokenize.open(__file__) as f:\r\n          setup_py_code = f.read()\r\n  else:\r\n      filename = \"<auto-generated setuptools caller>\"\r\n      setup_py_code = \"from setuptools import setup; setup()\"\r\n  \r\n  exec(compile(setup_py_code, filename, \"exec\"))\r\n  '\"'\"''\"'\"''\"'\"' % ('\"'\"'/tmp/pip-install-446qbri3/onnx_5a2c95655ead4090aa493849a53eb885/setup.py'\"'\"',), \"<pip-setuptools-caller>\", \"exec\"))' install --record /tmp/pip-record-6gso7uyj/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/dmitriy/.local/include/python3.8/onnx\r\n  cwd: /tmp/pip-install-446qbri3/onnx_5a2c95655ead4090aa493849a53eb885/\r\n  Running setup.py install for onnx ... error\r\nerror: legacy-install-failure\r\n\r\n\u00d7 Encountered error while trying to install package.\r\n\u2570\u2500> onnx\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for output from the failure.\r\n```\r\nFor improve fail I run this command:  `sudo apt-get install protobuf-compiler libprotoc-dev` and eventually \r\n`Installation Succeeded`", "comments": ["TF 1.15 is too old. When it was released there was no python 3.8.\r\n\r\nThis is not a TF issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55582\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55582\">No</a>\n"]}, {"number": 55580, "title": "tf.keras.callbacks.ModelCheckpoint ignores the montior parameter and always use loss", "body": "I am running tf.keras.callbacks.ModelCheckpoint with the accuracy metric but loss is used to save the best checkpoints. I have tested this in different places (my computer and collab) and two different code and faced the same issue. Here is an example code and the results:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport os\r\nimport shutil\r\n\r\ndef get_uncompiled_model():\r\n    inputs = keras.Input(shape=(784,), name=\"digits\")\r\n    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\r\n    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\r\n    outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\r\n    model = keras.Model(inputs=inputs, outputs=outputs)\r\n    return model\r\n\r\ndef get_compiled_model():\r\n    model = get_uncompiled_model()\r\n    model.compile(\r\n        optimizer=\"rmsprop\",\r\n        loss=\"sparse_categorical_crossentropy\",\r\n        metrics=[\"accuracy\"],\r\n    )\r\n    return model\r\n\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n\r\n# Preprocess the data (these are NumPy arrays)\r\nx_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\r\nx_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\r\n\r\ny_train = y_train.astype(\"float32\")\r\ny_test = y_test.astype(\"float32\")\r\n\r\n# Reserve 10,000 samples for validation\r\nx_val = x_train[-10000:]\r\ny_val = y_train[-10000:]\r\nx_train = x_train[:-10000]\r\ny_train = y_train[:-10000]\r\n\r\n\r\nckpt_folder = os.path.join(os.getcwd(), 'ckpt')\r\nif os.path.exists(ckpt_folder):\r\n    shutil.rmtree(ckpt_folder)\r\n\r\nckpt_path = os.path.join(r'D:\\deep_learning\\tf_keras\\semantic_segmentation\\logs', 'mymodel_{epoch}')\r\n\r\n\r\ncallbacks = [\r\n    tf.keras.callbacks.ModelCheckpoint(\r\n        # Path where to save the model\r\n        # The two parameters below mean that we will overwrite\r\n        # the current checkpoint if and only if\r\n        # the `val_loss` score has improved.\r\n        # The saved model name will include the current epoch.\r\n        filepath=ckpt_path,\r\n        montior=\"val_accuracy\",\r\n        # save the model weights with best validation accuracy\r\n        mode='max',\r\n        save_best_only=True,  # only save the best weights\r\n        save_weights_only=False,\r\n        # only save model weights (not whole model)\r\n        verbose=1\r\n    )\r\n]\r\n\r\nmodel = get_compiled_model()\r\n\r\n\r\nmodel.fit(\r\n    x_train, y_train, epochs=3, batch_size=1, callbacks=callbacks, validation_split=0.2, steps_per_epoch=1\r\n)\r\n```\r\n\r\n\r\n1/1 [==============================] - ETA: 0s - loss: 2.6475 - accuracy: 0.0000e+00\r\nEpoch 1: val_loss improved from -inf to 2.32311, saving model to D:\\deep_learning\\tf_keras\\semantic_segmentation\\logs\\mymodel_1\r\n1/1 [==============================] - 6s 6s/step - loss: 2.6475 - accuracy: 0.0000e+00 - val_loss: 2.3231 - val_accuracy: 0.1142\r\nEpoch 2/3\r\n1/1 [==============================] - ETA: 0s - loss: 1.9612 - accuracy: 1.0000\r\nEpoch 2: val_loss improved from 2.32311 to 2.34286, saving model to D:\\deep_learning\\tf_keras\\semantic_segmentation\\logs\\mymodel_2\r\n1/1 [==============================] - 5s 5s/step - loss: 1.9612 - accuracy: 1.0000 - val_loss: 2.3429 - val_accuracy: 0.1187\r\nEpoch 3/3\r\n1/1 [==============================] - ETA: 0s - loss: 2.8378 - accuracy: 0.0000e+00\r\nEpoch 3: val_loss did not improve from 2.34286\r\n1/1 [==============================] - 5s 5s/step - loss: 2.8378 - accuracy: 0.0000e+00 - val_loss: 2.2943 - val_accuracy: 0.1346", "comments": ["I found the issue. I was using montior instead of monitor!"]}, {"number": 55578, "title": "Bump up cudnn frontend to v0.6.1", "body": "This PR bumps up the cudnn frontend version to v0.6.1 which includes some new features and an important fix for the potential crash with  CUDNN_HEUR_MODE_B in multiple threads and on Ampere GPUs.\r\n\r\nRelnote: https://github.com/NVIDIA/cudnn-frontend/releases/tag/v0.6.1\r\n\r\ncc. @nluehr ", "comments": []}, {"number": 55577, "title": "Fix for adapting to the moved FuncOp.", "body": "/cc @chsigg @hanbinyoon \r\n\r\nFor some reason, this got missed in my original PR.\r\n", "comments": []}, {"number": 55573, "title": "training input shape changed after v2.7.0", "body": "https://github.com/tensorflow/tensorflow/blob/92a6bb06549e74a8bd8cdb8e28552496e5520007/tensorflow/python/keras/engine/training.py#L798\r\n\r\nThere is a expand operation for 1d input data, but this op is missed in v2.7.0. I'd like to know why and how can i keep compatibility", "comments": ["@wiyr \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "This is keras issue. I will close it and opend in keras repo. Thanks"]}, {"number": 55571, "title": "building tensorflow with docker image fails", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 20.04(in docker)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version: master branch\r\n- Python version:3.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):5.1.0\r\n- GCC/Compiler version (if compiling from source):gcc9.4.0\r\n- CUDA/cuDNN version:cuda11.2 cudnn8\r\n- GPU model and memory:RTX3090\r\n\r\n\r\n\r\n**Describe the problem**\r\ni follow the instruction in the tensorflow document to build the source with docker.I pulled the devel-gpu image and and try to build source in the container. i found using the command in the document i went to the directory  \"tensorflow\",but it cant pull as it is not a git repo, go to the directory \u201ctensorflow_src\" it can work but i can not checkout branch like r2.7,so i build the master branch with default configuration and i got the error.Really confused me. \r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/nsync/BUILD:467:11: Compiling platform/c++11/src/nsync_panic.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-11.0/targets/x86_64-linux/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.2/lib64 \\\r\n    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.0-linux-x86_64/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=8.6 \\\r\n    TF_CUDA_VERSION=11.2 \\\r\n    TF_CUDNN_VERSION=8 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/nsync_panic.d '-frandom-seed=bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/nsync_panic.o' -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-std=c++14' -x c++ '-std=c++11' -I./external/nsync//platform/c++11.futex -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -c external/nsync/platform/c++11/src/nsync_panic.cc -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/nsync_panic.o)\r\n# Configuration: c750dc19fc82070b294558f14293082aaaae479c4ce153dc94857ec7a3f60c5d\r\n# Execution platform: @local_execution_config_platform//:platform\r\nIn file included from /usr/include/c++/9/mutex:44,\r\n                 from ./external/nsync//platform/c++11.futex/../c++11/platform.h:29,\r\n                 from ./external/nsync//platform/c++11.futex/platform.h:22,\r\n                 from ./external/nsync//internal/headers.h:19,\r\n                 from external/nsync/platform/c++11/src/nsync_panic.cc:15:\r\n/usr/include/c++/9/bits/unique_lock.h: In instantiation of \u2018void std::unique_lock<_Mutex>::lock() [with _Mutex = std::mutex]\u2019:\r\n./external/nsync//platform/c++11.futex/../c++11/platform.h:99:14:   required from here\r\n/usr/include/c++/9/bits/unique_lock.h:136:35: internal compiler error: Segmentation fault\r\n  136 |    __throw_system_error(int(errc::operation_not_permitted));\r\n", "comments": ["Hello @N1uY ,\r\nEvery TensorFlow release is compatible with a certain version, for more information requesting you to please take a look at the tested build [configurations](https://www.tensorflow.org/install/source#gpu)(GCC 7.3.1 ,CUDA 11.2 and CuDNN 8.1) after following one of these commands `\"bazel clean --expunge\" or \"bazel clean --expunge_async\"`.\r\n\r\nAlso I request you please check this relevant thread [1](https://github.com/tensorflow/tensorflow/issues/54498) and [2](https://github.com/tensorflow/tensorflow/issues/53118) for reference. Thanks!", "@tilakrayal ,thanks for your response! I have checked the relevant thread you mentioned, it seems that uncompatible cuda or cudnn version can be the cause. After running the bazel clean command, I rebuild the TensorFlow with(cuda 11.2 and Cudnn8.1 and GCC7.5.0 (so difficult to install 7.3.1 on ubuntu 20)).Unfortunately, it failed again.I guess other cuda installed on my machine cause this and i will try to uninstall them.", "@tilakrayal it still fails....can you give some advice?\r\n`/home/ubuntu/.cache/bazel/_bazel_ubuntu/13d17b8c3a469e3b21b32031a169192f/external/llvm-project/llvm/BUILD.bazel:1881:16: C++ compilation of rule '@llvm-project//llvm:NVPTXCodeGen' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/13d17b8c3a469e3b21b32031a169192f/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib64: \\\r\n    PATH=/home/ubuntu/bl/software/anaconda3/envs/buildTF/bin:/home/ubuntu/.local/bin:/home/ubuntu/bin:/home/GaoJH/login_network/node-v14.15.4-linux-x64/bin:/home/ubuntu/bl/software/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bl/software/java/jdk1.7.0_80/bin:/home/ubuntu/bl/software/java/jdk1.7.0_80/jre/bin:/home/ubuntu/bl/software/java/neo4j-community-2.1.8/bin:/home/ubuntu/bl/software/java/jdk1.7.0_80/bin:/bin:/sbin:/bin:/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bl/software/java/jdk1.7.0_80/bin:/home/ubuntu/bl/software/java/jdk1.7.0_80/jre/bin:/home/ubuntu/bl/software/java/neo4j-community-2.1.8/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/ubuntu/bl/software/anaconda3/envs/buildTF/bin/python3 \\\r\n    PYTHON_LIB_PATH=/home/ubuntu/bl/software/anaconda3/envs/buildTF/lib/python3.9/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=8.6 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/NVPTXCodeGen/NVPTXAsmPrinter.d '-frandom-seed=bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/NVPTXCodeGen/NVPTXAsmPrinter.o' '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT=\".so\"' '-DLLVM_PLUGIN_EXT=\".so\"' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' '-DHAVE_STRERROR_R=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_PTHREAD_GETSPECIFIC=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_DEREGISTER_FRAME=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_LSEEK64=1' '-DHAVE_MALLINFO=1' '-DHAVE_POSIX_FALLOCATE=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=\"X86\"' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' '-DLLVM_HOST_TRIPLE=\"x86_64-unknown-linux-gnu\"' '-DLLVM_DEFAULT_TARGET_TRIPLE=\"x86_64-unknown-linux-gnu\"' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -iquote external/llvm-project -iquote bazel-out/k8-opt/bin/external/llvm-project -iquote external/llvm_terminfo -iquote bazel-out/k8-opt/bin/external/llvm_terminfo -iquote external/llvm_zlib -iquote bazel-out/k8-opt/bin/external/llvm_zlib -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXCodeGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/InstCombineTableGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXCommonTableGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXInfo -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXUtilsAndDesc -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/llvm-project/llvm/lib/Target/NVPTX -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/lib/Target/NVPTX -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -c external/llvm-project/llvm/lib/Target/NVPTX/NVPTXAsmPrinter.cpp -o bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/NVPTXCodeGen/NVPTXAsmPrinter.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from /usr/include/c++/7/memory:65:0,\r\n                 from external/llvm-project/llvm/include/llvm/ADT/Optional.h:24,\r\n                 from external/llvm-project/llvm/include/llvm/ADT/STLExtras.h:19,\r\n                 from external/llvm-project/llvm/include/llvm/IR/PassManager.h:41,\r\n                 from external/llvm-project/llvm/lib/Target/NVPTX/NVPTX.h:17,\r\n                 from external/llvm-project/llvm/lib/Target/NVPTX/NVPTXAsmPrinter.h:17,\r\n                 from external/llvm-project/llvm/lib/Target/NVPTX/NVPTXAsmPrinter.cpp:14:\r\n/usr/include/c++/7/bits/stl_uninitialized.h: In instantiation of \u2018_ForwardIterator std::__uninitialized_move_if_noexcept_a(_InputIterator, _InputIterator, _ForwardIterator, _Allocator&) [with _InputIterator = std::pair<const void*, llvm::Pass*>*; _ForwardIterator = std::pair<const void*, llvm::Pass*>*; _Allocator = std::allocator<std::pair<const void*, llvm::Pass*> >]\u2019:\r\n/usr/include/c++/7/bits/vector.tcc:426:6:   required from \u2018void std::vector<_Tp, _Alloc>::_M_realloc_insert(std::vector<_Tp, _Alloc>::iterator, _Args&& ...) [with _Args = {const std::pair<const void*, llvm::Pass*>&}; _Tp = std::pair<const void*, llvm::Pass*>; _Alloc = std::allocator<std::pair<const void*, llvm::Pass*> >; std::vector<_Tp, _Alloc>::iterator = __gnu_cxx::__normal_iterator<std::pair<const void*, llvm::Pass*>*, std::vector<std::pair<const void*, llvm::Pass*> > >; typename std::_Vector_base<_Tp, _Alloc>::pointer = std::pair<const void*, llvm::Pass*>*]\u2019\r\n/usr/include/c++/7/bits/stl_vector.h:948:21:   required from \u2018void std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = std::pair<const void*, llvm::Pass*>; _Alloc = std::allocator<std::pair<const void*, llvm::Pass*> >; std::vector<_Tp, _Alloc>::value_type = std::pair<const void*, llvm::Pass*>]\u2019\r\nexternal/llvm-project/llvm/include/llvm/PassAnalysisSupport.h:183:32:   required from here\r\n/usr/include/c++/7/bits/stl_uninitialized.h:313:5: internal compiler error: Segmentation fault\r\n     }\r\n     ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n`", "Hello @N1uY,\r\nRequired `CUDA version 11.2+`, from your config it looks like its 11.0. So, please see if you can upgrade your local CUDA version to 11.x. 11.2 builds should be compatible with anything 11.x. If you have multiple CUDA versions, I suggest you to delete rest of them. Thanks!", "@gadagashwini Sorry to reply so late.I have uninstalled all cuda and reinstall 11.2 and cudnn ,there is still something wrong.I really thank you all for your help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55571\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55571\">No</a>\n"]}, {"number": 55568, "title": "Bug in tf.saved_model.load(): 'NormalizeUTF8' not in self._op_def_cache", "body": "I load my model with just these two lines of code:\r\n```\r\nimport tensorflow as tf\r\nreloaded = tf.saved_model.load('my_model')\r\n```\r\n\r\nBut I get the error below. HOWEVER! When I also import tensorflow_text, everything works:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_text\r\nreloaded = tf.saved_model.load('my_model')\r\n```\r\n\r\nI don't actually call anything from tensorflow_text. I believe when tensorflow_text is imported, it updates self._op_def_cache in the Graph class in `tensorflow/python/framework/ops.py`.\r\n\r\nI tested this on both Mac and Windows, in jupyter notebook and an IDE. Behavior is the same.\r\n\r\n\r\nThe error:\r\n```\r\n--------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in _get_op_def(self, type)\r\n   4176     try:\r\n-> 4177       return self._op_def_cache[type]\r\n   4178     except KeyError:\r\n\r\nKeyError: 'NormalizeUTF8'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py in load_internal(export_dir, tags, options, loader_cls, filters)\r\n    974         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\r\n--> 975                             ckpt_options, options, filters)\r\n    976       except errors.NotFoundError as err:\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py in __init__(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\r\n    151             saved_object_graph=self._proto,\r\n--> 152             wrapper_function=_WrapperFunction))\r\n    153     # Store a set of all concrete functions that have been set up with\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py in load_function_def_library(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\r\n    408           structured_input_signature=structured_input_signature,\r\n--> 409           structured_outputs=structured_outputs)\r\n    410     # Restores gradients for function-call ops (not the same as ops that use\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function_def_to_graph.py in function_def_to_graph(fdef, structured_input_signature, structured_outputs, input_shapes)\r\n     70   graph_def, nested_to_flat_tensor_name = function_def_to_graph_def(\r\n---> 71       fdef, input_shapes)\r\n     72 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function_def_to_graph.py in function_def_to_graph_def(fdef, input_shapes)\r\n    238     else:\r\n--> 239       op_def = default_graph._get_op_def(node_def.op)  # pylint: disable=protected-access\r\n    240 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in _get_op_def(self, type)\r\n   4181         pywrap_tf_session.TF_GraphGetOpDef(self._c_graph, compat.as_bytes(type),\r\n-> 4182                                            buf)\r\n   4183         # pylint: enable=protected-access\r\n\r\nNotFoundError: Op type not registered 'NormalizeUTF8' in binary running on DESKTOP-CTLPA1S. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-2-20584f519f7a> in <module>\r\n----> 1 reloaded = tf.saved_model.load('translator')\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py in load(export_dir, tags, options)\r\n    934     ValueError: If `tags` don't match a MetaGraph in the SavedModel.\r\n    935   \"\"\"\r\n--> 936   result = load_internal(export_dir, tags, options)[\"root\"]\r\n    937   return result\r\n    938 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py in load_internal(export_dir, tags, options, loader_cls, filters)\r\n    976       except errors.NotFoundError as err:\r\n    977         raise FileNotFoundError(\r\n--> 978             str(err) + \"\\n You may be trying to load on a different device \"\r\n    979             \"from the computational device. Consider setting the \"\r\n    980             \"`experimental_io_device` option in `tf.saved_model.LoadOptions` \"\r\n\r\nFileNotFoundError: Op type not registered 'NormalizeUTF8' in binary running on DESKTOP-#####. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.\r\n```", "comments": ["TensorFlow uses [custom ops](https://www.tensorflow.org/guide/create_op) that can be implemented by other package, as it is not ideal to have TF repo be a monorepo of all possible implementation of ML models and the C++ kernels they need (for reasons including for example the question of who would maintain the code after it gets submitted, or for the reason of limiting the amount of dependencies that need to be present when compiling or users would need to install to use TF).\r\n\r\nInstead, if you need to use features implemented in custom ops, you have to import the corresponding package. In most cases this is done via the side effect of an import. Even if you don't use the imported name anywhere else in your code, the import also results in loading shared objects in the C++ runtime, some of which objects are just implementations of these kernels. This way, users that don't care about these custom ops (because their code doesn't use that) are not impacted and all other users can use these ops by just adding one single import or a minimal amount of code changes.\r\n\r\nThis is similar to how we now have the cloud filesystem support: you would import tensorflow_io and get access to reading files from these filesystems, with minimal changes to your code.", "@mihaimaruseac makes sense, thank you! Hope this will help some lost soul out there."]}, {"number": 55567, "title": "Fix issue with tf.ragged.constant when empty pylist is provided", "body": "`tf.ragged.constant` when provided empty `pylist` is provided as an input , all RAM is consumed, causing the notebook to crash. Below is the reference issue:\r\n\r\nFixes https://github.com/tensorflow/tensorflow/issues/55199", "comments": ["Please use a proper commit message / PR title, not `Update <file>`. See https://cbea.ms/git-commit/", "> Please use a proper commit message / PR title, not `Update <file>`. See https://cbea.ms/git-commit/\r\n\r\nSorry , updated description.", "Still see the same title. That would be the first line of the description after this gets merged.", "> Still see the same title. That would be the first line of the description after this gets merged.\r\n\r\ndone, thank you", "Can you check the build failures?", "Build is still failing. I don't think the fix is in Python, since the argument can be a list or a tensor or anything that can be converted to a tensor.", "> Build is still failing. I don't think the fix is in Python, since the argument can be a list or a tensor or anything that can be converted to a tensor.\r\n\r\nI agree ,any idea how we can fix this issue ?", "There needs to be a fix in C++ code where we check the number of elements in list/tensor and return a `Status` back to the user if it's 0.", "Apparently the fix was still in Python, but lower after the length of the list has been determined. See https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e", "Thank you Mihai "]}, {"number": 55565, "title": "tf.keras.metrics.MeanIoU outcome is not improving", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.95\r\n\r\n**Describe the current behavior**\r\nI am using tf.keras.metrics.MeanIoU as a metric in a semantic segmentation problem. All of the other metrics (accuracy, precision, recall) are changing during training but MeanIoU does not change. I am using tf.keras.losses.CategoricalCrossentropy(from_logits=False) for my loss function.\r\n\r\nThis issue has been reported in the following two StackOverFlow but no solution provided yet.\r\n\r\nhttps://stackoverflow.com/questions/71729853/tf-keras-metrics-meaniou-outcome-is-not-improving/71813168#71813168\r\n\r\nhttps://stackoverflow.com/questions/70848143/mean-iou-in-tensorflow-not-updating-resulting-in-correct-value/71813135#71813135\r\n\r\nHere is an example of metrics during training:\r\n\r\nEpoch 1/30\r\n\r\nEpoch 1: val_loss improved from inf to 2.39962, saving model to /content/drive/MyDrive/Colab/test/semantic_segmentation/models_trained/training-min-val_loss.hdf5\r\n269/269 - 899s - loss: 0.2277 - accuracy: 0.9108 - IoU: 0.2500 - val_loss: 2.3996 - val_accuracy: 0.9286 - val_IoU: 0.8372 - lr: 0.0010 - 899s/epoch - 3s/step\r\nEpoch 2/30\r\n\r\nEpoch 2: val_loss did not improve from 2.39962\r\n269/269 - 853s - loss: 0.1805 - accuracy: 0.9320 - IoU: 0.2500 -  val_loss: 4.5132 - val_accuracy: 0.9244 - val_IoU: 0.8303 -  lr: 0.0010 - 853s/epoch - 3s/step\r\nEpoch 3/30\r\n\r\nEpoch 3: val_loss did not improve from 2.39962\r\n269/269 - 817s - loss: 0.1672 - accuracy: 0.9380 - IoU: 0.2500 -  val_loss: 3.2384 - val_accuracy: 0.9198 - val_IoU: 0.8092 - lr: 0.0010 - 817s/epoch - 3s/step\r\nEpoch 4/30\r\n\r\nEpoch 4: val_loss improved from 2.39962 to 0.36882, saving model to /content/drive/MyDrive/Colab/test/semantic_segmentation/models_trained/training-min-val_loss.hdf5\r\n269/269 - 855s - loss: 0.1643 - accuracy: 0.9390 - IoU: 0.2500 - val_loss: 0.3688 - val_accuracy: 0.9337 - val_IoU: 0.2600 -  lr: 0.0010 - 855s/epoch - 3s/step\r\n\r\nIf I change my one hot encoded labels to a one band maks, use SparseCategoricalAccuracy and the following modified MeanIoU....it works all fine.\r\n\r\nclass SparseMeanIoU(tf.keras.metrics.MeanIoU):\r\n  def __init__(self,\r\n               y_true=None,\r\n               y_pred=None,\r\n               num_classes=None,\r\n               name=None,\r\n               dtype=None):\r\n    super(SparseMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype)\r\n\r\n  def update_state(self, y_true, y_pred, sample_weight=None):\r\n    y_pred = tf.math.argmax(y_pred, axis=-1)\r\n    return super().update_state(y_true, y_pred, sample_weight)\r\n\r\n**Describe the expected behavior**\r\nMeanIoU should change during training.\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n", "comments": ["I found the solution ... for tf.keras.losses.CategoricalCrossentropy, I should use tf.keras.metrics.OneHotMeanIoU", "@mehran66 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here if this is still an issue? \r\nPlease move this issue to closed status if it is resolved for you?Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55565\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55565\">No</a>\n"]}, {"number": 55563, "title": "Build failed: undeclared inclusion(s) in rule '@llvm-project//mlir:GPUTransforms'", "body": "**System information**\r\n\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Ubuntu 20.04.4 LTS x86_64\r\n- TensorFlow version: Latest commit [`55645ca`](https://github.com/tensorflow/tensorflow/commit/55645ca964508507890529a71591f51a344a6356)\r\n- Python version: 3.10.4\r\n- Bazel version (if compiling from source): 5.1.1\r\n- GCC/Compiler version (if compiling from source): 9.4.0\r\n- CUDA/cuDNN version: No CUDA\r\n- GPU model and memory: No GPU\r\n\r\nI am trying to build Tensorflow with TPU support on a Cloud TPU VM.\r\n\r\nI have successfully built [v2.8.0](https://github.com/tensorflow/tensorflow/tree/v2.8.0) before, but now it shows an error:\r\n\r\n```\r\n$ bazel build --config=tpu //tensorflow/tools/pip_package:build_pip_package\r\n...\r\nERROR: /home/ayaka/.cache/bazel/_bazel_ayaka/daa247f909e330d435159750c8dca57c/external/llvm-project/mlir/BUILD.bazel:3385:11: Compiling mlir/lib/Dialect/GPU/Transforms/SerializeToCubin.cpp failed: undeclared inclusion(s) in rule '@llvm-project//mlir:GPUTransforms':\r\nthis rule is missing dependency declarations for the following files included by 'mlir/lib/Dialect/GPU/Transforms/SerializeToCubin.cpp':\r\n  'bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUBaseIncGen/mlir/Dialect/GPU/GPUOpsDialect.h.inc'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 269.070s, Critical Path: 131.50s\r\nINFO: 3174 processes: 247 internal, 2927 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nI am willing to provide more information on this issue.", "comments": ["I am experiencing a similar problem.\r\n\r\n```\r\nERROR: /home/leite/.cache/bazel/_bazel_leite/cc29ca840c1f2e86437824f86564e745/external/llvm-project/mlir/BUILD.bazel:2992:11: Compiling mlir/lib/Support/ToolUtilities.cpp failed: undeclared inclusion(s) in rule '@llvm-project//mlir:Support':\r\nthis rule is missing dependency declarations for the following files included by 'mlir/lib/Support/ToolUtilities.cpp':\r\n  'bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/llvm/config.cppmap'\r\n  'bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/llvm/Demangle.cppmap'\r\n  'bazel-out/k8-opt-exec-50AE0418/bin/external/llvm_terminfo/terminfo.cppmap'\r\n  'bazel-out/k8-opt-exec-50AE0418/bin/external/llvm_zlib/zlib.cppmap'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n**System information**\r\n\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Arch Linux x86_64 (rolling release)\r\n- TensorFlow version: master branch @ [c44d14f2194](https://github.com/tensorflow/tensorflow/tree/c44d14f2194cf4c4b4060fd4141194ee62792ca8)\r\n- Python version: 3.10.4\r\n- Bazel version (if compiling from source): 5.1.1\r\n- GCC/Compiler version (if compiling from source): Clang 13.0.1\r\n- Compiling without GPU support", "I am seeing same, compiling with GPU support.", "@ayaka14732, Looks like Bazel version, can you try with Bazel 5.0.0 Version. Thanks!", "> @ayaka14732, Looks like Bazel version, can you try with Bazel 5.0.0 Version. Thanks!\r\n\r\nI manually changed `.bazelversion` to 5.0.0 and built with Bazel 5.0.0, but there are even more build errors.", "I met the same error, but it is automatically fixed after I clean up the bazel cache.", "@ZYHowell Thanks! This also works for me.\r\n\r\n@gadagashwini Is this a Bazel bug or a Tensorflow bug?", "@ayaka14732,\r\nCan you try to clear the bazel cache before building Tensorflow\r\n`bazel clean --expunge`", "@gadagashwini As I replied before, I tried to clean the cache and it works for me.", "@ayaka14732,\r\nGlad that its resolved. Can we close this issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55563\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55563\">No</a>\n"]}, {"number": 55562, "title": "[Pfor] Improve the fallback cause", "body": "This will try to improve the feedback to the user when we need fallback in a `while_loop`.\r\n\r\nFixes https://github.com/tensorflow/tensorflow/issues/55425", "comments": ["/cc @wangpengmit", "Build infra is broken", "`feedback/copybara` is failing again.", "@wangpengmit if we can expose the argument to silence warnings in pmap this would be great for KerasCV:\r\nhttps://github.com/keras-team/keras-cv/issues/264", "@LukeWood too late, It was already merged.  If @wangpengmit is ok with this additional API change I could eventually open a new one.", "I can review the change here in GitHub. It'll be reviewed again internally by TF API Owners.", "@wangpengmit Do you have any specific requirement/suggestion to propagate the `warn` boolean in the public API?", "@wangpengmit \r\n\r\n> I can review the change here in GitHub. It'll be reviewed again internally by TF API Owners.\r\n\r\nIf it is easier to do this from the internal side can you send me an email?  Happy to do so.  Couldn\u2019t find your internal username. "]}, {"number": 55559, "title": "[oneDNN] Windows CI Set python path env vars if not already set", "body": "resubmit of https://github.com/tensorflow/tensorflow/pull/55523", "comments": ["Tagging @learning-to-play and @mihaimaruseac for this PR."]}, {"number": 55555, "title": "Revert \"[oneDNN] Windows CI Set python path env vars if not already set\"", "body": "Reverts tensorflow/tensorflow#55523\r\n\r\nIt was accidentally merged and now breaks everything in the sync between internal and external", "comments": []}, {"number": 55553, "title": "Typo fix", "body": "Changed from `explict` to `explicit`", "comments": ["Please fix more than one typo per file. There are several hours of CI that run due this one letter addition."]}, {"number": 55552, "title": "[PluggableDevice] Fix tf.config.set_visible_device for PluggableDevice", "body": "tf.config.set_visible_device() is not work for PluggableDevice but can work normally through tf.ConfigProto()\r\nwork example:\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.visible_device_list = \"1\"\r\nwith tf.Session(config) as sess:\r\n```\r\nnot work example:\r\n```\r\ndevs = tf.config.list_physical_devices('XPU')\r\ntf.config.set_visible_devices(devs[1], 'XPU')\r\n```\r\n The root cause is currently python implementation in v2 API will filter out non-gpu devices in GPUOptions, this PR will enable it if the device is GPU or PluggableDevice.", "comments": ["@penpornk we find that tf.config.set_visible_device is not working for pluggable device when we do multi-cards training with horovod. We find that it was due to some 'GPU' hardcode in /tensorflow/eager/context.py, we changed the device check as 'GPU' + pluggable_device, can you help to review it? Thanks.\r\n\r\nBy the way, I saw there was a pylint error but not related with this PR. do I need to take care of it? Thanks\r\n```\r\npylint error\r\ntensorflow/python/eager/context.py:81:0: C0301: Line too long (83/80) (line-too-long)\r\n```\r\n```\r\ncode:\r\n  79 # This flag and the associated environment var are transient and will eventually\r\n  80 # be removed, once this experiment is enabled by default.\r\n  81 _RUN_EAGER_OP_AS_FUNCTION_ENABLED = os.getenv(\"TF_RUN_EAGER_OP_AS_FUNCTION\", False)\r\n```", "@penpornk ", "@penpornk Is there any update on this PR? I saw there several checks failed(AMD Rocm, code check_), do I need to take care of these. I am not sure whether it is related with this PR. Thanks "]}, {"number": 55547, "title": "Fix typos in core/data/service", "body": "Fix typos in `tensorflow/core/data/service` directory.", "comments": []}, {"number": 55543, "title": "Cherrypick coreml memory issue", "body": "Cherrypick integrate of fix coreml delegate tmp peak memory issue https://github.com/tensorflow/tensorflow/commit/9f2e9d1e58f85a2b603baa1e682c0987fa49b203\r\nFixes https://github.com/tensorflow/tensorflow/issues/55503", "comments": ["Do we really need to patch this in TF 2.8? Is there a bug for this?\r\n\r\nCan you cherrypick using the GitHub action so it's easier to also get the internal CL number?", "Closing the issue, since this fix will be available in Tensorflow 2.9."]}, {"number": 55541, "title": "r2.9 cherry-pick: 85a6eab2899 \"Use tf_kernel_library instead of cc_library to pass the correct copts. Add necessary compatible_with tags to DTensor files.\"", "body": "Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/85a6eab2899b6c6ab2a2f812f13bbd9639443369", "comments": []}, {"number": 55540, "title": "[TF:TRT] Make default dynamic shape profile strategy to implicit batch compatible", "body": "In implicit batch mode, a TRT engine can handle a range of batch sizes. If the model was built with shape [N, C, H, W], then any batch size between 1..N will be compatible with the engine. In contrast, in dynamic shape mode we use optimization profiles to define the expected input shapes for the engine.\r\n\r\nThe goal of this PR is to change the default profile generation strategy (which is enabled when the `dynamic_shape_profile_strategy` converter arg is `None`) to be `'ImplicitBatchModeCompatible'`.\r\n\r\nThis way, the shapes that a TRT engine accepts in dynamic shape mode will be the same as in implicit batch mode, unless the user explicitly specifies a different profile strategy.", "comments": ["Tagging @bixia1 and @DEKHTIARJonathan for review.", "@bixia1 Can you please review this PR ? Thank you!"]}, {"number": 55539, "title": "r2.9 cherry-pick: 296cdc612c7 \"Fix pylint violations in dtensor_device.py.\"", "body": "Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/296cdc612c7652dee05e2e31bd67c465f73cdda2", "comments": []}, {"number": 55538, "title": "imdeu.htmi", "body": "### 1. System information\r\n\r\ngit adt- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installation (pip package or built from source):\r\n- TensorFlow library (version, if pip package or github gtr SHA, if built comdlim t):\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n#### Option A: Reference colab notebooks\r\n\r\n1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.\r\n2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).\r\n\r\n```\r\n(You can paste links or attach files by dragging & dropping them below)\r\n- Provide links to your updated versions of the above two colab notebooks.\r\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\r\n```\r\n\r\n#### Option B: Paste your code here or provide a link to a custom end-to-end colab\r\n\r\n```\r\n(You can paste links or attach files by dragging & dropping them below)\r\n- Include code to invoke the TFLite Converter Python API and the errors.\r\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\r\n```\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n- Model produces wrong results and/or has lesser accuracy.\r\n- Model produces correct results, but it is slower than expected.\r\n\r\n### 4. (optional) RNN conversion support\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n### 5. (optional) Any other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi @mdmilm ! Closing this issue as it is duplicated to #55537 . Thanks!"]}, {"number": 55537, "title": "Alom", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installation (pip package or built from source):\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n#### Option A: Reference colab notebooks\r\n\r\n1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.\r\n2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).\r\n\r\n```\r\n(You can paste links or attach files by dragging & dropping them below)\r\n- Provide links to your updated versions of the above two colab notebooks.\r\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\r\n```\r\n\r\n#### Option B: Paste your code here or provide a link to a custom end-to-end colab\r\n\r\n```\r\n(You can paste links or attach files by dragging & dropping them below)\r\n- Include code to invoke the TFLite Converter Python API and the errors.\r\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\r\n```\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n- Model produces wrong results and/or has lesser accuracy.\r\n- Model produces correct results, but it is slower than expected.\r\n\r\n### 4. (optional) RNN conversion support\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n### 5. (optional) Any other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi @mdmilm ! I see the TFLite converter notebook  attached here got 97% accuracy during evaluation. Could you point out the exact issue? Thanks!", "Template not filled."]}, {"number": 55535, "title": "r2.9 cherry-pick: b5948699e20 \"Update documentation for DTensor's `pack`, `unpack` and `fetch_layout` functions.\"", "body": "Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/b5948699e2057c71171b0acc65e56e8071888f20", "comments": []}]