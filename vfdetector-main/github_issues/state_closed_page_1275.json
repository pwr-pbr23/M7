[{"number": 14882, "title": "What if we don't install tensorflow under a new environment?", "body": "How come we need to install tensorflow as a [separate](https://user-images.githubusercontent.com/33768560/33234120-05bba17a-d1ef-11e7-9f8a-0390d005aa6a.png) environment?\r\n\r\nIf we do it this way, many common libraries are not available when tensorflow is activated. \r\n![image](https://i.stack.imgur.com/zOslW.png)\r\n\r\nMost of the common libraries such as matplotlib, panda, etc. are not within tensorflow environment. So we have to install again to use them.\r\n![image](https://user-images.githubusercontent.com/33768560/33233620-c2242610-d1e6-11e7-9a23-44a20991daff.png)\r\n\r\nSo why not just install under root so we don't have to re-install all those libraries under the new environment?\r\n\r\nThanks.\r\n\r\n", "comments": ["I did a system-wide install on Ubuntu (Linux Kernel) after going through similar troubles you mention. \r\nI am guessing the rational behind the preferred approach is to preserve the state of mission-critical machines by not altering the code base of dependencies. For recreational users like us, the **native pip install** looks to be better in flexibility.", "I think new environment is optional choice. \r\nSince tensorflow has many dependencies, for example, numpy, pandas and so on, they might be upgraded when installing tensorflow. So it is safe to create a separate environment. If you don't care the case, feel free to install tensorflow in default environment.", "How come your tensorflow came with those dependencies?  Mine didn't. I had to reinstall pandas, matplotlib, pandas_datareader again under tensorflow environment. ", "Hi, @yomoko. Since the issue is neither bug report or feature request, I think it could be better to post the question on [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow)."]}, {"number": 14881, "title": "Format AUTHORS file", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14880, "title": "Restore CONTRIBUTORS file", "body": "This file got lost in 2ecd0e448b13964c542a8db23a82e666b519e2f8 and was since not restored.\r\nAlso, I populated it using [`graphs/contributors`](https://github.com/tensorflow/tensorflow/graphs/contributors)", "comments": ["Can one of the admins verify this patch?", "We never had this file in TensorFlow, only in skflow -- I don't think it's particularly valuable to repeat this information, it's contained in the git repo itself, and github does a fine job showing the list.\r\n\r\n(in particular, it's another thing to maintain explicitly, and it's hard to maintain properly. Even the file in this commit seems wrong -- it has only 69 lines)"]}, {"number": 14879, "title": "Extend estimators.md to cover Keras", "body": "Can you extend the estimators.md tutorial to cover `tf.keras.estimator.model_to_estimator` use case?", "comments": ["I think that could be useful to have a full example with estimator, tf.keras and dataset API now that all these API are in core. See also https://github.com/tensorflow/tensorflow/issues/8787#issuecomment-337297281 /cc @fchollet ", "A nice example to add to documentation is something like [this one](https://www.dlology.com/blog/an-easy-guide-to-build-new-tensorflow-datasets-and-estimator-with-keras-model/).", "/cc @Tony607", "Seems reasonable to me, can you comment on this @fchollet?  Thanks!", "Yes, I agree that covering the use case \"how can I define a custom Estimator using the Keras API\" would be great to have in the main tutorial. How should we proceed?", "I think that @Tony607's Notebook I linked it is quite complete. But I don't know if he want to contribuite it.\r\nProbably also a 2nd flavour with custom loss, explicit model_,fn, Tensorboard summaries could be interesting to have more control for power users that still want to rely on the high level API. There was  https://github.com/mjlaali/keras-estimator/ but still on the contrib interface.", "@fchollet Also what it s needed to document where the additonal summaries for Tebsorboard need to be inserted. Cause with this API call we don't have an explicit model_fn to write, so what is the best practice for placing addtional summaries? ", "Also just to point other missing documentation we can specify how to mix [tf metrics in tf.keras](https://stackoverflow.com/questions/45947351/how-to-use-tensorflow-metrics-in-keras) cause was requested in some Keras ticket but still not officially documented. We have some TF metrics that are not directly covered by tf.keras metrics counterpart so could be useful to document its use now the we are in the same tf root namespace.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied."]}, {"number": 14878, "title": "Some PATH typo : no `train_dir` in tutorial", "body": "This file uses `train_dir`. But in code file, there is no `train_dir`  anymore, it should be replaced with `log_dir` `input_data_dir` and `checkpoint_file` respectively", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I have signed it just now", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 14877, "title": "[CMake] Extract list of python modules", "body": "Progressing #10296\r\n@drpngx FYI", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "@mrry Exactly my point.\r\nChanges in `contrib` are often not reflected properly.\r\nE.g. quite a few modules have the `python` directory missing.\r\nCouldn't they be just implied if present?", "@Androbin That would be nice! If there's a reliable automatic way to get CMake to infer the set of Python modules that matches the Bazel behavior (and doesn't introduce a dependency on Bazel), I'd be in favor of that. In the meantime, once this change is submitted, adding a Bazel presubmit test that keeps the two in sync still seems like an improvement.", "Also, seems like the `.proto` files don't match up.\r\n\r\nIn filetree but not script:\r\n`cloud/kernels`\r\n`gdr`\r\n`lite/toco`\r\n`mpi`\r\n`mpi_collectives`\r\n`verbs`\r\n\r\nNot sure, if the wildcards match sub-directories as well.\r\nIf they don't, the paths have to be expanded respectively.\r\nNeeding some clarification by @mrry .\r\n\r\nSo `tensorboard` should be expanded to:\r\n`tensorboard/graph_explorer/proto`\r\n`tensorboard/plugins/projector`\r\n`tensorboard/plugins/trace`\r\n\r\nAnd `training` should be expanded to:\r\n`training/python/training`", "Expanding it out to the union of modules in the filetree and the current list seems fine. I assume there are some modules in the filetree that aren't explicitly imported by one of their parents or tested by the tests in `tf_tests.cmake`, and so we've been getting away with having fewer modules in the CMake files.", "@mrry I added the .proto files that I listed above.\r\nFor the python modules, we probably wanna list all sub-directories with a `__init__.py` file?", "It's more than that, because Bazel will add an empty `__init__.py` file to any directories containing a `py_library` when it builds the PIP package. ", "@mrry I did some more diffing. Shall I patch these discrepancies?\r\n\r\nExists in filetree but not in list:\r\n`batching/kernels`\r\n`batching/python`\r\n`batching/python/ops`\r\n`boosted_trees/kernels`\r\n`data/kernels`\r\n`fused_conv/kernels`\r\n`fused_conv/python`\r\n`fused_conv/python/ops`\r\n`hooks/python`\r\n`image/kernels`\r\n`input_pipeline/kernels`\r\n`lite/kernels`\r\n`lite/python`\r\n`lite/toco/python`\r\n`receptive_field/python`\r\n`stat_summarizer/python`\r\n`stateless/python`\r\n`tensor_forest/kernels`\r\n\r\nExists in list but not in filetree:\r\n`keras/python`\r\n`metrics/kernels`", "@Androbin I don't think we need to include the `kernel_tests` directories, since those should be purely test code that doesn't need to be included in the PIP package. (Indeed, we probably include too much test code in the current wildcards, and it would be nice to prune it if possible.)", "I think `tensorflow/contrib/testing` is needed because it gets loaded when you use `tf.contrib`, but `tensorflow/python/kernel_tests` can be omitted.", "Thanks for the updates! Still LGTM. \r\n\r\n@tensorflow-jenkins test this please.", "`//bazel_pip/tensorflow/contrib/data/python/kernel_tests:prefetch_dataset_op_test FAILED`\r\n@mrry I think `tensorflow/python/kernel_tests` is required here?", "Hmm, looks like the PIP tests can depend on code in `tensorflow/python/kernel_tests` :(. Yes, the easiest move would be to add `tensorflow/python/kernel_tests` back to the list of modules.\r\n\r\nHowever, AFAICT that was a Bazel test that failed, and you didn't change the Bazel rules, so I'm confused as to why that test did not pass.", "AFAICT, the test is initiated via `//tensorflow/python/kernel_tests/BUILD` which depends on the PIP package and the `contrib` part is delegated to `CMake`.", "@mrry Failing test `//tensorflow/contrib/data/python/kernel_tests:prefetch_dataset_op_test` was introduced in commit d73e8b36d1332723f5819d07f8c44e88c49c7cec", "Thanks for digging into it. I'm guessing it's broken for all PRs, and perhaps not part of our internal presubmit matrix?\r\n\r\n@saxenasaurabh Can you take a look? I think the problem is that libraries in `tensorflow/python/kernel_tests/...` (in particular, `dataset_serialization_test_base.py` don't get included in the PIP package, so PIP tests that depend on them don't work. However, I'm not sure why some tests pass and some don't.", "@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please.", "I fixed most of the broken tests on head last night so kicking off a new building. Let's see how that goes. ", "I'm going to merge this for now as the pip issue is known and can be addressed in another PR", "@mrry Which of the other `cmake` scripts you think would benefit the most from extraction?"]}, {"number": 14876, "title": "How to get the preivous batch after running Iteration.get_next() in tensorflow?", "body": "This problem is in the stackoverflow.\r\nhttps://stackoverflow.com/questions/47485023/how-to-get-the-preivous-batch-after-running-iteration-get-next-in-tensorflow\r\nI hope someone to help me to solve it.\r\nThanks.", "comments": ["You can close it if don't meet acquirment .", "The problem has been solved."]}, {"number": 14875, "title": "tf.data cannot be loaded with r1.4", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8 (jessie)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: b'v1.4.0-14-gb5df90f' 1.4.1\r\n- **Python version**: Python 3.6.3 |Anaconda, Inc.| (default, Nov 20 2017, 20:41:42)\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Debian 4.9.2-10) 4.9.2\r\n- **CUDA/cuDNN version**: CUDA 8, cnDNN 5.1\r\n- **GPU model and memory**: TITAN X (Pascal), 12G\r\n- **Exact command to reproduce**: `from tensorflow.data import TFRecordDataset`\r\n\r\n### Describe the problem\r\nAfter checking out r1.4 and compiling TF, tf.data cannot be loaded. Training works fine. But 'data' is listed when executing `print(dir(tf))`.\r\n\r\n### Source code / logs\r\n(tensorflow140) [13:08 user@server ~] > `python`\r\nPython 3.6.3 |Anaconda, Inc.| (default, Nov 20 2017, 20:41:42)\r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n\\>\\>\\> `import tensorflow as tf`\r\n\\>\\>\\> `tf.__version__`\r\n'1.4.1'\r\n\\>\\>\\> `from tensorflow.data import TFRecordDataset`\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow.data'\r\n\\>\\>\\> `print(dir(tf))`\r\n['AUTO_REUSE', 'AggregationMethod', 'Assert', 'AttrValue', 'COMPILER_VERSION', 'ConditionalAccumulator', 'ConditionalAccumulatorBase', 'ConfigProto', 'DType', 'DeviceSpec', 'Dimension', 'Event', 'FIFOQueue', 'FixedLenFeature', 'FixedLenSequenceFeature', 'FixedLengthRecordReader', 'GIT_VERSION', 'GPUOptions', 'GRAPH_DEF_VERSION', 'GRAPH_DEF_VERSION_MIN_CONSUMER', 'GRAPH_DEF_VERSION_MIN_PRODUCER', 'Graph', 'GraphDef', 'GraphKeys', 'GraphOptions', 'HistogramProto', 'IdentityReader', 'IndexedSlices', 'InteractiveSession', 'LMDBReader', 'LogMessage', 'MetaGraphDef', 'NameAttrList', 'NoGradient', 'NodeDef', 'NotDifferentiable', 'OpError', 'Operation', 'OptimizerOptions', 'PaddingFIFOQueue', 'Print', 'PriorityQueue', 'QUANTIZED_DTYPES', 'QueueBase', 'RandomShuffleQueue', 'ReaderBase', 'RegisterGradient', 'RunMetadata', 'RunOptions', 'Session', 'SessionLog', 'SparseConditionalAccumulator', 'SparseFeature', 'SparseTensor', 'SparseTensorValue', 'Summary', 'SummaryMetadata', 'TFRecordReader', 'Tensor', 'TensorArray', 'TensorInfo', 'TensorShape', 'TextLineReader', 'VERSION', 'VarLenFeature', 'Variable', 'VariableScope', 'WholeFileReader', '\\_\\_builtins\\_\\_', '\\_\\_cached\\_\\_', '\\_\\_compiler_version\\_\\_', '\\_\\_doc\\_\\_', '\\_\\_file\\_\\_', '\\_\\_git_version\\_\\_', '\\_\\_loader\\_\\_', '\\_\\_name\\_\\_', '\\_\\_package\\_\\_', '\\_\\_path\\_\\_', '\\_\\_spec\\_\\_', '\\_\\_version\\_\\_', 'abs', 'accumulate_n', 'acos', 'acosh', 'add', 'add_check_numerics_ops', 'add_n', 'add_to_collection', 'all_variables', 'angle', 'app', 'arg_max', 'arg_min', 'argmax', 'argmin', 'as_dtype', 'as_string', 'asin', 'asinh', 'assert_equal', 'assert_greater', 'assert_greater_equal', 'assert_integer', 'assert_less', 'assert_less_equal', 'assert_negative', 'assert_non_negative', 'assert_non_positive', 'assert_none_equal', 'assert_positive', 'assert_proper_iterable', 'assert_rank', 'assert_rank_at_least', 'assert_rank_in', 'assert_same_float_dtype', 'assert_scalar', 'assert_type', 'assert_variables_initialized', 'assign', 'assign_add', 'assign_sub', 'atan', 'atan2', 'atanh', 'batch_to_space', 'batch_to_space_nd', 'betainc', 'bfloat16', 'bincount', 'bitcast', 'bitwise', 'bool', 'boolean_mask', 'broadcast_dynamic_shape', 'broadcast_static_shape', 'case', 'cast', 'ceil', 'check_numerics', 'cholesky', 'cholesky_solve', 'clip_by_average_norm', 'clip_by_global_norm', 'clip_by_norm', 'clip_by_value', 'colocate_with', 'compat', 'complex', 'complex128', 'complex64', 'concat', 'cond', 'confusion_matrix', 'conj', 'constant', 'constant_initializer', 'container', 'contrib', 'control_dependencies', 'convert_to_tensor', 'convert_to_tensor_or_indexed_slices', 'convert_to_tensor_or_sparse_tensor', 'cos', 'cosh', 'count_nonzero', 'count_up_to', 'create_partitioned_variables', 'cross', 'cumprod', 'cumsum', '**data**', 'decode_base64', 'decode_csv', 'decode_json_example', 'decode_raw', 'delete_session_tensor', 'depth_to_space', 'dequantize', 'deserialize_many_sparse', 'device', 'diag', 'diag_part', 'digamma', 'distributions', 'div', 'divide', 'double', 'dynamic_partition', 'dynamic_stitch', 'edit_distance', 'einsum', 'encode_base64', 'equal', 'erf', 'erfc', 'errors', 'estimator', 'exp', 'expand_dims', 'expm1', 'extract_image_patches', 'eye', 'fake_quant_with_min_max_args', 'fake_quant_with_min_max_args_gradient', 'fake_quant_with_min_max_vars', 'fake_quant_with_min_max_vars_gradient', 'fake_quant_with_min_max_vars_per_channel', 'fake_quant_with_min_max_vars_per_channel_gradient', 'feature_column', 'fft', 'fft2d', 'fft3d', 'fill', 'fixed_size_partitioner', 'flags', 'float16', 'float32', 'float64', 'floor', 'floor_div', 'floordiv', 'floormod', 'foldl', 'foldr', 'gather', 'gather_nd', 'get_collection', 'get_collection_ref', 'get_default_graph', 'get_default_session', 'get_local_variable', 'get_seed', 'get_session_handle', 'get_session_tensor', 'get_variable', 'get_variable_scope', 'gfile', 'global_norm', 'global_variables', 'global_variables_initializer', 'glorot_normal_initializer', 'glorot_uniform_initializer', 'gradients', 'graph_util', 'greater', 'greater_equal', 'group', 'half', 'hessians', 'histogram_fixed_width', 'identity', 'identity_n', 'ifft', 'ifft2d', 'ifft3d', 'igamma', 'igammac', 'imag', 'image', 'import_graph_def', 'initialize_all_tables', 'initialize_all_variables', 'initialize_local_variables', 'initialize_variables', 'initializers', 'int16', 'int32', 'int64', 'int8', 'invert_permutation', 'is_finite', 'is_inf', 'is_nan', 'is_non_decreasing', 'is_numeric_tensor', 'is_strictly_increasing', 'is_variable_initialized', 'keras', 'layers', 'lbeta', 'less', 'less_equal', 'lgamma', 'lin_space', 'linalg', 'linspace', 'load_file_system_library', 'load_op_library', 'local_variables', 'local_variables_initializer', 'log', 'log1p', 'log_sigmoid', 'logging', 'logical_and', 'logical_not', 'logical_or', 'logical_xor', 'losses', 'make_ndarray', 'make_template', 'make_tensor_proto', 'map_fn', 'matching_files', 'matmul', 'matrix_band_part', 'matrix_determinant', 'matrix_diag', 'matrix_diag_part', 'matrix_inverse', 'matrix_set_diag', 'matrix_solve', 'matrix_solve_ls', 'matrix_transpose', 'matrix_triangular_solve', 'maximum', 'meshgrid', 'metrics', 'min_max_variable_partitioner', 'minimum', 'mod', 'model_variables', 'moving_average_variables', 'multinomial', 'multiply', 'name_scope', 'negative', 'newaxis', 'nn', 'no_op', 'no_regularizer', 'norm', 'not_equal', 'one_hot', 'ones', 'ones_initializer', 'ones_like', 'op_scope', 'orthogonal_initializer', 'pad', 'parallel_stack', 'parse_example', 'parse_single_example', 'parse_single_sequence_example', 'parse_tensor', 'placeholder', 'placeholder_with_default', 'polygamma', 'pow', 'profiler', 'py_func', 'python_io', 'pywrap_tensorflow', 'qint16', 'qint32', 'qint8', 'qr', 'quantize_v2', 'quantized_concat', 'quint16', 'quint8', 'random_crop', 'random_gamma', 'random_normal', 'random_normal_initializer', 'random_poisson', 'random_shuffle', 'random_uniform', 'random_uniform_initializer', 'range', 'rank', 'read_file', 'real', 'realdiv', 'reciprocal', 'reduce_all', 'reduce_any', 'reduce_join', 'reduce_logsumexp', 'reduce_max', 'reduce_mean', 'reduce_min', 'reduce_prod', 'reduce_sum', 'register_tensor_conversion_function', 'report_uninitialized_variables', 'required_space_to_batch_paddings', 'reset_default_graph', 'reshape', 'resource', 'resource_loader', 'reverse', 'reverse_sequence', 'reverse_v2', 'rint', 'round', 'rsqrt', 'saturate_cast', 'saved_model', 'scalar_mul', 'scan', 'scatter_add', 'scatter_div', 'scatter_mul', 'scatter_nd', 'scatter_nd_add', 'scatter_nd_sub', 'scatter_nd_update', 'scatter_sub', 'scatter_update', 'segment_max', 'segment_mean', 'segment_min', 'segment_prod', 'segment_sum', 'self_adjoint_eig', 'self_adjoint_eigvals', 'sequence_mask', 'serialize_many_sparse', 'serialize_sparse', 'serialize_tensor', 'set_random_seed', 'setdiff1d', 'sets', 'shape', 'shape_n', 'sigmoid', 'sign', 'sin', 'sinh', 'size', 'slice', 'space_to_batch', 'space_to_batch_nd', 'space_to_depth', 'sparse_add', 'sparse_concat', 'sparse_fill_empty_rows', 'sparse_mask', 'sparse_matmul', 'sparse_maximum', 'sparse_merge', 'sparse_minimum', 'sparse_placeholder', 'sparse_reduce_max', 'sparse_reduce_max_sparse', 'sparse_reduce_sum', 'sparse_reduce_sum_sparse', 'sparse_reorder', 'sparse_reset_shape', 'sparse_reshape', 'sparse_retain', 'sparse_segment_mean', 'sparse_segment_sqrt_n', 'sparse_segment_sum', 'sparse_slice', 'sparse_softmax', 'sparse_split', 'sparse_tensor_dense_matmul', 'sparse_tensor_to_dense', 'sparse_to_dense', 'sparse_to_indicator', 'sparse_transpose', 'spectral', 'split', 'sqrt', 'square', 'squared_difference', 'squeeze', 'stack', 'stop_gradient', 'strided_slice', 'string', 'string_join', 'string_split', 'string_to_hash_bucket', 'string_to_hash_bucket_fast', 'string_to_hash_bucket_strong', 'string_to_number', 'substr', 'subtract', 'summary', 'svd', 'sysconfig', 'tables_initializer', 'tan', 'tanh', 'tensordot', 'test', 'tile', 'to_bfloat16', 'to_double', 'to_float', 'to_int32', 'to_int64', 'trace', 'train', 'trainable_variables', 'transpose', 'truediv', 'truncated_normal', 'truncated_normal_initializer', 'truncatediv', 'truncatemod', 'tuple', 'uint16', 'uint8', 'uniform_unit_scaling_initializer', 'unique', 'unique_with_counts', 'unsorted_segment_max', 'unsorted_segment_sum', 'unstack', 'user_ops', 'variable_axis_size_partitioner', 'variable_op_scope', 'variable_scope', 'variables_initializer', 'variance_scaling_initializer', 'variant', 'verify_tensor_all_finite', 'where', 'while_loop', 'write_file', 'zeros', 'zeros_initializer', 'zeros_like', 'zeta']", "comments": ["@ccqu Does the following work?  I.e. you've imported `tensorflow as tf`.\r\n```\r\nfrom tf.data import TFRecordDataset\r\n```\r\nOr how about using `tf.data.TFRecordDataset` directly?", "@tatatodd Thanks, directly using `tf.data.TFRecordDataset` works!\r\n\\>\\>\\> `from tf.data import TFRecordDataset`\r\nTraceback (most recent call last):\r\n  File \"\\<stdin\\>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tf'\r\n\\>\\>\\> `tf.data.TFRecordDataset('filename.tfrecord')`\r\n<TFRecordDataset shapes: (), types: tf.string>\r\n\r\nWhat could be the cause of it? I have a Windows PC with TF r1.4 installed with pip giving exactly the same error...", "I don't know Python that well, so bear with me.  Perhaps @jart can confirm or correct any mistakes I make.\r\n\r\nFirst off, here's some documentation on Python modules and packages:\r\nhttps://docs.python.org/3/tutorial/modules.html#packages\r\n\r\nI'm guessing the problem is due to the way TensorFlow python code and `__init__.py` files are structured.  The file layout looks like this:\r\n```\r\n/DIR/tensorflow\r\n/DIR/tensorflow/__init__.py\r\n/DIR/tensorflow/python\r\n/DIR/tensorflow/python/__init__.py\r\n/DIR/tensorflow/python/data\r\n/DIR/tensorflow/python/data/__init__.py\r\n```\r\nHere are links to some of the interesting parts of the `__init__.py` files:\r\nhttps://github.com/tensorflow/tensorflow/blob/f567ddf87d8d85e6cefa441283d20d52aac9f7fb/tensorflow/__init__.py#L24\r\nhttps://github.com/tensorflow/tensorflow/blob/f567ddf87d8d85e6cefa441283d20d52aac9f7fb/tensorflow/python/__init__.py#L80\r\nhttps://github.com/tensorflow/tensorflow/blob/f567ddf87d8d85e6cefa441283d20d52aac9f7fb/tensorflow/python/data/__init__.py#L35\r\n\r\nWhen you say:\r\n```\r\nfrom tensorflow.data import TFRecordDataset  # FAILS\r\n```\r\nYou're telling python to import the class `TFRecordDataset` from the package `tensorflow.data`.  Python will resolve the package `tensorflow` to the path `/DIR/tensorflow`, and expects to find the package `tensorflow.data` under the path `/DIR/tensorflow/data`.  But that package doesn't exist; the package actually lives under the path `/DIR/tensorflow/python/data`.\r\n\r\nWhen you say:\r\n```\r\nimport tensorflow as tf\r\ntf.data.TFRecordDataset\r\n```\r\nThis succeeds.  The import in the first line has bound the symbol `tf` to the module under the path `/DIR/tensorflow`.  And then the `tf.data.TFRecordDataset` is resolved correctly since the `data` and `TFRecordDataset` names have been imported into the namespace by the sequence of `__init__.py` files above.\r\n\r\nIf all of this analysis is correct, what it means is that you can't simply can't say `from tensorflow.xxx import yyy`.  But note that if you're just trying to have shorter names in your code, you can do something like this:\r\n```\r\nimport tensorflow as tf\r\nTFRecordDataset = tf.data.TFRecordDataset\r\n```", "Rather than this:\r\n\r\n```py\r\nfrom tensorflow.data import TFRecordDataset\r\n```\r\n\r\nPlease do this:\r\n\r\n```py\r\n# Option 1\r\nimport tensorflow as tf\r\ntf.data.TFRecordDataset(...)\r\n\r\n# Option 2\r\nimport tensorflow as tf\r\nTFRecordDataset = tf.data.TFRecordDataset\r\nTFRecordDataset(...)\r\n\r\n# Option 3 (not subject to API stability promise)\r\nfrom tensorflow.python.data.ops import readers\r\nreaders.TFRecordDataset(...)\r\n```"]}, {"number": 14874, "title": "tf.Print converts Variable to mutable Tensor", "body": "The PR is proposed to resolve #14788.\r\n\r\ntf.Print converts Variable to mutable Tensor, instead of constant.\r\n\r\n```python\r\nv = tf.Variable([99])\r\n# <tf.Variable 'Variable:0' shape=(1,) dtype=int32_ref>\r\n\r\np = tf.Print(v, [v])\r\n# Tensor(\"PrintRef:0\", shape=(1,), dtype=int32_ref)\r\n```\r\n\r\n### How to test\r\n\r\n+ [x] add test case\r\n+ [ ] pass all tests", "comments": ["Can one of the admins verify this patch?", "Handing this off to @alextp as variables czar. I think this approach won't play at all well with `ResourceVariable`, so it might need some more work....", "Can't you achieve the same result by doing the following in the definition of Print:\r\n\r\n```\r\n if v.dtype._is_ref:\r\n  with ops.control_dependencies([Print(constant_op.constant(1), data)]):\r\n    return gen_array_ops._ref_identity(v._ref)\r\n```\r\n?", "Though I do discourage you from writing such code. Why do you want this? Forwarding mutable references to variables is weird and not quite future-proof with resource variables.", "Thanks for feedback, @mrry , @alextp . \r\nYes, I also think it weird to convert variable to mutable tensor. @alextp 's solution is better and straight. I didn't realize that `ops.control_dependencies` can work out the problem.\r\n\r\nSo I have two questions:\r\n1. Do we expect to support variable for `input_` of tf.Print?\r\n2. If yes, how about returning the identical type always, I mean, tensor for tensor, mutable tensor for mutable tensor, and variable for variable ?\r\n"]}, {"number": 14873, "title": "[Feature Request] Support for Flutter", "body": "I recently moved from native development to Flutter seeing Google backing its development.\r\nSince both TensorFlow and Flutter is by Google will there be a support for Flutter in Future?", "comments": ["@petewarden do you have any comments on this?", "What kind of support were you thinking of?\r\n\r\nAt this time, there are no concrete plans for any Dart APIs for TensorFlow to be shipped as a flutter package. You may want to use something like https://flutter.io/platform-channels/ to integrate with TensorFlow/TensorFlowLite in your mobile application.", "@asimshankar if we were to use platform channels, we need to code in both swift and kotlin. @nikhil-minz  is referring to plugin support like Flutterfire for firebase", "Looks like someone built a package: https://pub.dartlang.org/packages/tensorflow_lite", "Hi, \r\nI created a Flutter plugin for TensorFlow Lite. It works on both iOS and Android. \r\nhttps://pub.dartlang.org/packages/tflite", "@asimshankar, any reconsideration for an official Dart implementation?\r\n\r\nTwo years after this issue was closed, Dart lang has had a real revolution and we can increasingly see applications developed on it with Flutter, AngularDart and server side. I think it would be great if we also had a Tensoflow implementation for Dart.", "Check https://github.com/dart-lang/tflite_native/pull/22", "For Firebase instead https://github.com/FirebaseExtended/flutterfire/issues/788", "TensorFlow Lite Flutter Plugin is now available [am15h/tflite_flutter_plugin](https://github.com/am15h/tflite_flutter_plugin/). It provides a Dart API similar to the TFLite Java API.\r\n\r\nIt is built on the top of the bindings from [dart-lang/tflite_native](https://github.com/dart-lang/tflite_native) under the guidance of TensorFlow Lite team.", "@am15h Is https://github.com/dart-lang/tflite_native/issues/42 going to impact your GSOC proejct?", "@bhack It won't,   [am15h/tflite_flutter_plugin](https://github.com/am15h/tflite_flutter_plugin/) is fully functional without any use of `dart:cli`. ", "any news update?"]}, {"number": 14872, "title": "Add DT_HALF support for SpaceToDepth on GPU", "body": "This fix tries to address the issue raised in #14871 where there were no DT_HALF support for SpaceToDepth on GPU.\r\n\r\nThis fix adds DT_HALF support on GPU and adds aditional test cases.\r\n\r\nThis fix fixes #14871.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Does it make sense to update DepthToSpace as well in the same PR?", "@ygoncharov Thanks. The PR has been updated with changes in DepthToSpace as well.", "@josh11b could you take a look?", "Thanks @josh11b for the review. The PR has been updated with suggested changes."]}, {"number": 14871, "title": "SpaceToDepth for DT_HALF is not supported on GPU", "body": "space_to_depth can be placed on GPU for float32, but there is no a GPU kernel for float16", "comments": ["Added a PR #14872 for DT_HALF support on GPU."]}, {"number": 14870, "title": "fix overpadding in MixtureSameFamily", "body": "This fixes pad_mix_dims when mixture distribution does not have scalar batch size -- previous version would add too many dimensions", "comments": ["Can one of the admins verify this patch?", "@mwytock can you add this to the accompanying unit-test?\r\n\r\n    def testSampleAndLogProbBatch(self):\r\n      with self.test_session():\r\n        gm = mixture_same_family_lib.MixtureSameFamily(\r\n            mixture_distribution=categorical_lib.Categorical(probs=[[0.3, 0.7]]),\r\n            components_distribution=normal_lib.Normal(\r\n                loc=[[-1., 1]], scale=[[0.1, 0.5]]))\r\n        x = gm.sample([4, 5], seed=42)\r\n        log_prob_x = gm.log_prob(x)\r\n        self.assertEqual([4, 5, 1], x.shape)\r\n        self.assertEqual([4, 5, 1], log_prob_x.shape)", "Jenkins, test this please."]}, {"number": 14869, "title": "fix overpadding in MixtureSameFamily", "body": "This fixes pad_mix_dims when mixture distribution does not have scalar batch size -- previous version would add too many dimensions", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "u need to pass cla ~ i was there, now i passed , just follow me:\r\n#14867"]}, {"number": 14868, "title": "consuming Dataset becomes slower and slower, if make_one_shot_iterator each epoch ", "body": "### Problem\r\n\r\nI run make_one_shot_iterator() each epoch because  I want re-shuffle the dataset each epoch. I Know that the dataset.shuffle().repeat().batch() pipeline can do almost the same thing, but when data_num can not be divided exactly by batch_size, the pipeline merges two epochs at their boundary to construct a complete batch,  which I HATE.\r\n\r\nSo, I choose to run dataset.shuffle().batch() and make_one_shot_iterator() before each epoch. But I find that the speed of consuming dataset becomes slower and slower, significantly. I tried different settings to find out that it is make_one_shot_iterator() which makes consuming slow. \r\n\r\nBy the way, I build tensorflow 1.4 from source on OS X with support of GPU, some hacky workaround. \r\n\r\n### Code\r\n\r\n```\r\nnum_data = 1000\r\nnum_epoch = 50\r\nbatch_size = 32\r\ndataset = tf.data.Dataset.range(num_data)\r\n\r\nmode=3 \r\n# model = 1,2,or 3\r\n# 1: re-shuffle, re-batch and re-make-iterator each epoch\r\n# 2: re-batch and re-make-iterator\r\n# 3: only re-make-iterator\r\n\r\nwith tf.Session() as sess:\r\n    for epoch in xrange(num_epoch):\r\n        t1 = time.time()\r\n        if mode==1: \r\n            _dataset = dataset.shuffle(num_data).batch(batch_size)\r\n            iterator = _dataset.make_one_shot_iterator()\r\n        elif mode==2:\r\n            _dataset = dataset.batch(batch_size)\r\n            iterator = _dataset.make_one_shot_iterator()\r\n        elif mode==3: \r\n            iterator = dataset.make_one_shot_iterator()\r\n        t2 = time.time()\r\n        for i in xrange(num_data/batch_size):\r\n            a = sess.run(iterator.get_next())\r\n        t3 =time.time()\r\n        print 'epoch %d make_iterator_time %.4f comsuming_time %.4f'%(epoch,t2-t1,t3-t2)\r\n```\r\n\r\nand the outputs:\r\n\r\n```\r\nepoch 0 make_iterator_time 0.0181 comsuming_time 0.1366\r\nepoch 1 make_iterator_time 0.0036 comsuming_time 0.1444\r\nepoch 2 make_iterator_time 0.0040 comsuming_time 0.1559\r\nepoch 3 make_iterator_time 0.0036 comsuming_time 0.1695\r\nepoch 4 make_iterator_time 0.0036 comsuming_time 0.1899\r\nepoch 5 make_iterator_time 0.0036 comsuming_time 0.1955\r\nepoch 6 make_iterator_time 0.0036 comsuming_time 0.2082\r\nepoch 7 make_iterator_time 0.0037 comsuming_time 0.2191\r\nepoch 8 make_iterator_time 0.0036 comsuming_time 0.2334\r\nepoch 9 make_iterator_time 0.0040 comsuming_time 0.2461\r\nepoch 10 make_iterator_time 0.0036 comsuming_time 0.2621\r\nepoch 11 make_iterator_time 0.0036 comsuming_time 0.2720\r\nepoch 12 make_iterator_time 0.0036 comsuming_time 0.2886\r\nepoch 13 make_iterator_time 0.0036 comsuming_time 0.3006\r\nepoch 14 make_iterator_time 0.0037 comsuming_time 0.3134\r\nepoch 15 make_iterator_time 0.0039 comsuming_time 0.3260\r\nepoch 16 make_iterator_time 0.0445 comsuming_time 0.3438\r\nepoch 17 make_iterator_time 0.0037 comsuming_time 0.3576\r\nepoch 18 make_iterator_time 0.0037 comsuming_time 0.3678\r\nepoch 19 make_iterator_time 0.0040 comsuming_time 0.3827\r\nepoch 20 make_iterator_time 0.0037 comsuming_time 0.3937\r\nepoch 21 make_iterator_time 0.0038 comsuming_time 0.4172\r\nepoch 22 make_iterator_time 0.0036 comsuming_time 0.4222\r\n...\r\n ```\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.12.5\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: clang-802.0.42\r\n- **CUDA/cuDNN version**: 9.0/7\r\n- **GPU model and memory**: GTX1080 8G\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"", "comments": ["Predifining get_next_op = iterator.get_next() outside the loop and run sess.run(get_next_op) will reduce the consuming time, but it is still getting slower and slower, as long as I make_one_shot_iterator() each epoch.", "@jsimsa I believe you were looking into a similar bug internally, could you please look into this.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Creating an iterator in a loop (every epoch) will keep adding nodes to the graph, so the slowdown you are seeing is expected. I am not aware of any use case where creating an iterator in a loop would be the right thing to do.\r\n\r\nWhat is the reason you do not want to create a batch using data from different epochs? In any case, you can always do dataset.shuffle().batch().repeat() instead of dataset.shuffle().repeat().batch() in case you would prefer to have the last batch of each epoch to be possibly smaller (in the case the batch size does not divide the size of the epoch, which seems to be your case).", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I see a similar slow down though I dont add new nodes to the graph,\r\n\r\n```python\r\n    dataset = tf.data.TFRecordDataset(data_files, num_parallel_reads=79)\r\n\r\nif is_training:\r\n    dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(1743, -1, 42))\r\n\r\ndataset = dataset.apply(tf.contrib.data.map_and_batch(decode, 128,\r\n                                                        num_parallel_batches=58,\r\n                                                        drop_remainder=False)\r\n                        )\r\ndataset = dataset.prefetch(1064)\r\n```\r\nand fit a tf.keras model like so,\r\n```python\r\nmodel.fit(\r\n    dataset.make_one_shot_iterator(),\r\n    steps_per_epoch=model_meta.train_records // 128,\r\n    epochs=2,\r\n    validation_data=test_dataset.make_one_shot_iterator(),\r\n    validation_steps=test_records // 128,\r\n    verbose=0\r\n)\r\n```\r\nI'm using for a 1 GPU 7GB RAM training. What am I missing?\r\n", "@Nithanaroy can you please create a separate issue with minimal reproducible example and tag me? Thanks."]}, {"number": 14867, "title": "change bazel-mirror to mirror.bazel", "body": "hi i'm back , using only one email.", "comments": ["Can one of the admins verify this patch?", "cla signe issue seems solved ", "here's the path for passing cla:\r\n1.use google account, say wobushiliu2@gmail.com to register github, \r\n   and set a git name, say eigen2017.\r\n\r\n2.goto https://cla.developers.google.com/clas to sign a agreement as \"only self\".\r\n   set \"GitHub Account Name\" to be wobushiliu2@gmail.com\r\n\r\n3.jus fork a copy of tensor project and clone it to local:\r\n\u279c  tensorflow git:(master) git remote -v\r\norigin\thttps://github.com/eigen2017/tensorflow (fetch)\r\norigin\thttps://github.com/eigen2017/tensorflow (push)\r\n\r\n4.set name and email:\r\ngit config user.name \"eigen2017\"\r\ngit config user.email \"wobushiliu2@gmail.com\"\r\nbe care not to add \"global\" option, so the git configuration is only for ur fork project , not for other git project on your pc.\r\n\r\n5.change the source code\r\n\r\n6. git add *\r\n    git commit -m \"...\"\r\n    git push origin master\r\n\r\n7. grab a pull req from https://github.com/tensorflow/tensorflow/pulls\r\n    select:  \"compare across forks\"\r\n    select: your fork \r\n\r\n8. post the pull req\r\n", "@jart  i was jakiechris, u can call me eigan now  :)", "Welcome back! Thank you for the contribution. Mr. Jenkins: test this please.", "@eigen2017 can you look into the merge conflict", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@eigen2017 I might have made a mistake in resolving commits via the web UI. Can you just fix it and force push. Sorry about that.", "I'm closing in favor of https://github.com/tensorflow/tensorflow/pull/15007", "Actually I'm re-opening. I didn't realize that the second person who committed to this branch was the TF sync on-call. My mistake.", "Can one of the admins verify this patch?", "Let's merge #15007 as the email address used in the commit is some private email created by github due to my email being hidden.", "Sorry about the confusion. ", "Sounds good. In that case, I'm sorry this change didn't work out @eigen2017. Sometimes sync causes headaches.", "ok, next time i'll synchronize the codes before changing it.  "]}, {"number": 14866, "title": "If I import cv2, \" tf.global_variables_initializer() \" will be very slow.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04.3\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary.\r\n- **TensorFlow version (use command below)**:\r\n1.3.0\r\n- **Python version**: \r\n2.7.12\r\n- **CUDA/cuDNN version**:\r\nCUDA Version 8.0.61\r\n- **GPU model and memory**:\r\nNVIDIA GTX 1080Ti  12G\r\n\r\n### Describe the problem\r\nIf I import cv2, \" tf.global_variables_initializer() \" will be very slow, about 143s. You can run my test code below, when \" import cv2 \" is commented out, the time is about 5s. The version of opencv is 2.4.13.4.\r\n\r\n### Source code / logs\r\n```Python\r\nimport tensorflow as tf\r\nimport time\r\nimport cv2\r\n\r\nweight = tf.Variable(tf.truncated_normal([5,5,1,32], stddev=0.1))\r\n\r\not = time.time()\r\ninit_op = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    sess.run(init_op)\r\nnt = time.time()\r\nprint('time: {:.3f}'.format(nt-ot))\r\n```", "comments": ["#1197 #7755", "Thanks \uff01", "Closing as a duplicate."]}, {"number": 14865, "title": "Fix bug: divide by zero in embedding_lookup_sparse", "body": "Fix #14851\r\n\r\n\r\n### How to test\r\n\r\n+ [x] add test case\r\n+ [ ] pass all tests.", "comments": ["Can one of the admins verify this patch?", "Hi, @sb2nov . All python tests are passed on my working machine (Linux, CPU, Python2). I cannot check those failed logs because now Details links to a web which seems private for google.", "There was some infra error, restarting.", "Thanks, @drpngx ! Ubuntu CC failed with \"infrastructure error\", I cannot figure out what it means?", "Yeah, we are having issues with the infra.", "Hi, @drpngx . I merged the latest master branch, which might be helpful to pass all tests.", "Jenkins, test this please\n\nOn Sun, Dec 24, 2017, 10:07 PM Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> Hi, @drpngx <https://github.com/drpngx> . I merged the latest master\n> branch, which might be helpful to pass all tests.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/14865#issuecomment-353834705>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbS-9J02CEMvv8eiL-1JO5HQSLst0ks5tDzu9gaJpZM4Qp0bU>\n> .\n>\n", "Could you check the test errors?", "Thank you, @drpngx . Could you paste the failure log? It looks like a timeout case.\r\n```bash\r\n//bazel_pip/tensorflow/contrib/timeseries/python/timeseries:input_pipeline_test FAILED in 14.7s\r\n```", "It's more serious than than. Could you run again with asan?\r\n\r\n```\r\nINFO: From Testing //bazel_pip/tensorflow/contrib/timeseries/python/timeseries:input_pipeline_test:\r\n==================== Test output for //bazel_pip/tensorflow/contrib/timeseries/python/timeseries:input_pipeline_test:\r\n2017-12-25 18:40:07.339780: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n.......................\r\n----------------------------------------------------------------------\r\nRan 23 tests in 10.055s\r\n\r\nOK\r\nSegmentation fault (core dumped)\r\n```", "@drpngx Strange that the same test pass on my machine: \r\n+ Python 3.6\r\n+ CentOS Linux release 7.3.1611 (Core)\r\n+ Linux hxxx.xxx.xxx.xxx.com 3.10.0-514.10.2.el7.x86_64 #1 SMP Fri Mar 3 00:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n\r\n```bash\r\ncc1plus: warning: unrecognized command line option \"-Wno-self-assign\" [enabled by default]\r\nTarget //tensorflow/contrib/timeseries/python/timeseries:input_pipeline_test up-to-date:\r\n  bazel-bin/tensorflow/contrib/timeseries/python/timeseries/input_pipeline_test\r\nINFO: Elapsed time: 690.946s, Critical Path: 156.82s\r\n//tensorflow/contrib/timeseries/python/timeseries:input_pipeline_test    PASSED in 12.6s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\nThere were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\r\n```", "Did you try with address and memory sanitizers?", "I don't know what is address and memory sanitizers. And the test is executed with default configuration:\r\n\r\n```bash\r\n(py36) [facai@h1077922 tensorflow]$ cat .tf_configure.bazelrc\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/home/facai/workshop/anaconda2/envs/py36/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/home/facai/workshop/anaconda2/envs/py36/lib/python3.6/site-packages\"\r\nbuild --force_python=py3\r\nbuild --host_force_python=py3\r\nbuild --python_path=\"/usr/home/facai/workshop/anaconda2/envs/py36/bin/python\"\r\nbuild --define with_jemalloc=true\r\nbuild --define with_gcp_support=true\r\nbuild --define with_hdfs_support=true\r\nbuild --define with_s3_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild:gdr --define with_gdr_support=true\r\nbuild:verbs --define with_verbs_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"0\"\r\nbuild --define grpc_no_ares=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild:mkl --define using_mkl=true\r\nbuild:mkl -c opt\r\nbuild:monolithic --define framework_shared_object=false\r\nbuild --define framework_shared_object=true\r\nbuild:android --crosstool_top=//external:android/crosstool\r\nbuild:android --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nbuild:android_arm --config=android\r\nbuild:android_arm --cpu=armeabi-v7a\r\nbuild:android_arm64 --config=android\r\nbuild:android_arm64 --cpu=arm64-v8a\r\n```", "You should be able to build with `bazel --config=asan -c dbg` instead of your regular command.", "Thank you, @drpngx . I'd like to pend the PR for the discussion of safe_div op #15706 .", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@facaiy what do you mean \"pend\" the discussion?", "@drpngx I think he meant \"pending the discussion of safe_div op #15706\", i.e. wait for the discussion there to be resolved.", "Ahh! Thanks for the clarification :)\r\n\r\n@facaiy please ping when ready.", "@drpngx OK, I'll let you know when ready. \r\nThank you, @rmlarsen .", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 59 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 14864, "title": "change bazel-mirror to mirror.bazel", "body": "this time i added email\r\n", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "ok, i found that always confusion in two emails , so then i now register to git hub again with gmail wobushiliu2@gmail.com. hope this confusion will go away~ thank u guys , i will always support tensor and google products .~"]}, {"number": 14863, "title": "change bazel-mirror to mirror.bazel", "body": "thank to @jart ,  for letting me know how to do pull req.~", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "my fault again~"]}, {"number": 14862, "title": "C++ gradient for Select", "body": "Fix #14845 \r\n\r\nmigrate python implementation to c++ side, source: https://github.com/tensorflow/tensorflow/blob/27767d8e9c1325979cf32ff5b81c10df9006fd57/tensorflow/python/ops/math_grad.py#L919\r\n\r\n### How to test\r\n\r\n+ [x] add test case\r\n+ [ ] pass all tests", "comments": ["Can one of the admins verify this patch?", "@suharshs Hi, since `Select` seems renamed to `Where3`, I'm a little confused about which op the gradient should be registered for, `Select` or `Where3`? Could you give an advice? Thank you.", "Sorry for the delay, was on vacation :) \r\n\r\nI believe at the kernel level, the op is still \"Select\" so you should register it for that.", "Welcome back, @suharshs . I have removed grad_scope, x_shape, y_shape as requested.", "@tensorflow-jenkins test this please", "Correction, this is the cause of the flakes. reverting this PR instead."]}, {"number": 14861, "title": "Cant install tensorflow on my laptop", "body": "pip3 install --upgrade tensorflow-gpu\r\n\r\nCollecting tensorflow-gpu\r\n\r\nCould not find a version that satisfies the requirement tensorflow-gpu (from versions: )\r\nNo matching distribution found for tensorflow-gpu\r\n\r\nthis is what it shows please help", "comments": ["#8251 \r\nthis might help \r\n:)", "Make sure you're installing the corresponding binary for you system and Python installation. For instance a binary for Python 3.5 won't work on a Python 3.6 or 2.7 installation.", "Are you able to install now, or still facing any issues.", "@Carmezim yes I am using python 3.6 . Is CUDA & cuDDn installation necessary to install tensorflow on windows??", "@shivaniag I am still facing issues", "No it's not necessary to have CUDA unless you are using tensorflow CPU ", "Correction : tensorflow GPU", "@golu-golu I tried to install tensorflow cpu but still the same error popped up\r\nbut as I have a graphic card it would be nice to use it for tensorflow-gpu ", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@tatatodd I am using Windows 10 x64 bit version and I am trying to install the tensorflow version available on the tensorflow website. I am using \"pip3 install --upgrade tensorflow-gpu\" command for installing tensorflow gpu version,I am using python version \"Python 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 17:26:49) [MSC v.1900 32 bit (Intel)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\"\r\n I installed CUDA 9.0 from NVDIA and also added cuDNN's dll to the respectivefolders as said in the installation guide. I also have numpy, dev,pip and wheel installed .After this when I try to run the command \"pip3 install --upgrade tensorflow-gpu\" the error\r\n\r\npip3 install --upgrade tensorflow-gpu\r\nCollecting tensorflow-gpu\r\n  Could not find a version that satisfies the requirement tensorflow-gpu (from versions: )\r\nNo matching distribution found for tensorflow-gpu\r\n comes up. please help", "@kartikeyasaxena1012 You need the 64bit version of Python 3.6, as described here:\r\nhttps://www.tensorflow.org/install/install_windows#installing_with_native_pip", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing this out, since I believe the problem was resolved.", "I am having the same problem too \u2026 I tried Python 3.6, 3.7, Upgrade Pip, Running in the command line, running in anaconda \u2026 major problems\r\n", "Hi, I have the same problem I cannot install TensorFlow in pycharm or anaconda.  i need to get 1.5 pythons...anyone can guide me in this issue? "]}, {"number": 14860, "title": "min_quantize lib and command line", "body": "a quantize/obfuscate lib\r\n1. without Quantize/DeQuantize ops\r\n2. could obfuscate node names\r\n3. use KMeans instead of simple average slice", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "u need to pass cla ~ i was there, now i passed , just follow me:\r\nhttps://github.com/tensorflow/tensorflow/pull/14867", "I signed it!", "I signed it!", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "change repository"]}, {"number": 14859, "title": "Accuracy drop down after freezing", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 1.3.0\r\nPython version: 2.7.12\r\nBazel version (if compiling from source): 0.5.4\r\nCUDA/cuDNN version: 8.0.61\r\nGPU model and memory: NVIDIA Corporation Device 1b06\r\nExact command to reproduce:\r\n\r\nEvaluate the accuracy of ckeckpoint\r\n```\r\npython eval_image_classifier.py \\\r\n    --batch_size=100 \\\r\n    --checkpoint_path=.../model.ckpt-100000 \\\r\n    --dataset_dir=.../coco_tfrecord \\\r\n    --dataset_name=flowers \\\r\n    --dataset_split_name=validation \\\r\n    --model_name=mobilenet_v1 \\\r\n    --eval_dir=/tmp/eval\r\n```\r\n\r\nExport slim inference graph\r\n```\r\npython export_inference_graph.py \\\r\n--alsologtostderr \\\r\n--model_name=mobilenet_v1 \\\r\n--dataset_name=flowers \\\r\n--is_training=False \\\r\n--batch_size=1 \\\r\n--image_size=224 \\\r\n--output_file=.../mobilenet_v1.pb\r\n```\r\n\r\nFreeze checkpoint using slim inference graph\r\n```\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n--input_checkpoint=.../model.ckpt-100000 \\\r\n--input_graph=.../mobilenet_v1.pb \\\r\n--input_binary=true \\\r\n--output_graph=.../frozen_graph.pb \\\r\n--output_node_names=MobilenetV1/Predictions/Reshape_1\r\n```\r\n\r\n### Describe the problem\r\nI am doing some fine-tune of Mobilenet using the pretrained model.\r\n\r\nAnd I follow the tutorial of slim to freeze and evaluate the performence.\r\n\r\nI separate the validation set from train set , evaluate it using checkpoint , and do it again after freezing model , however the evaluation of  the accuracy of frozen model is far away from ckeckpoint's .\r\n\r\nHere is some familiar issues , the situations are a little different , but I think it's referable.\r\n#9724 \r\n#12450\r\n\r\n@MyHumbleSelf said \" some docs should probably be adjusted \" , means slim tutorial ? Do these problems remain ? \r\nWhere might be wrong ?\r\n\r\nThank you ! ", "comments": ["I use the script to manually freeze model , since there is dropout and Batch normalization in Mobilenet , I set is_training = False as the suggestions ( it seems like doing the same thing as slim tutorial ) . And MovingAverage doesn't apply in model.\r\nBut the accuracy is still terrible.", "@petewarden might have some ideas here.", "@tatatodd Thanks for your participation !\r\nI also notice that Mobilenet model exported directly by retrain.py is working well , but their freezing methods share the same process ( both are via \"graph_util.convert_variables_to_constants\" )\r\nIs it possible that these problems result from the graph definition of pretrained model inputted ( which is exported by export_inference_graph.py in slim )?", "I solved this by removing all Batch Normalization layers in network , followed the tutorial process and froze model with official script , achieve the same accuracy between checkpoint and freeze_graph.pb .\r\n\r\nAnd I noticed that the more different inference images comparing to training images , the loss of accuracy gets more . Does this problem result from insufficient number and lack of variety of training images ? Since batch norm normalize and shift images , and get these parameter during training process .\r\n\r\nIs there any other insights can be provided ?", "@petewarden \r\nSorry for disturbing , is there any progress on this ?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 14858, "title": "off-by-one bug in graph_editor get_forward_walk_ops", "body": "This [line](https://github.com/tensorflow/tensorflow/blob/c9de294d0a0980b1636f76757c175afbf4f58ea8/tensorflow/contrib/graph_editor/select.py#L414) converts Tensor to its corresponding op by replacing it with its consuming op\r\n\r\n`seed_ops = util.get_consuming_ops(ts)\r\n`\r\n\r\nInstead it should replace it with its producing op\r\n\r\n`seed_ops = [t.op for t in ts]\r\n`\r\n\r\nThis would makes `get_forward_walk_ops(tensor` give same result as `get_forward_walk_ops(tensor.op` which was probably the intention of this function\r\n\r\nI know `contrib` isn't really supported, but wanted to file this bug in order to reference it in work-arounds in my code", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "cc @purpledog ", "Hi,\r\n\r\nConsumer ops are not the same as t.op. Simply speaking, `t.op` is the op producing the tensor and consumer ops are the ops taking `t` as input.\r\n\r\nSo I think this is working as expected (correct me if I am wrong!).", "Indeed they are not the same, and I think it should be `t.op` instead of consumer ops.\r\nConsider how it works now\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.graph_editor as ge\r\n\r\nA0 = tf.ones((), name='A0')\r\nA1 = tf.tanh(A0, name='A1')\r\nprint(ge.get_forward_walk_ops(A0))     # => [A1]\r\nprint(ge.get_forward_walk_ops(A0.op))  # => [A0, A1]\r\n\r\n```", "Actually after thinking a bit, I can see your logic -- using `A.op` instead of `A` may forward walks to include chains of computation that don't include `A` which would also be counter-intuitive.\r\n\r\nI was too used to thinking of TensorFlow not having a concept of tensor (since the graph only contains ops, and you can't compute any tensor individually without computing all of the other tensors on the op). ", "The current counter-intuitive behavior is that there's no way to include the endpoints of your walks if you use a tensor\r\n\r\nIE the following will not include `A0` in the forward walks returned\r\n\r\n`print(ge.get_forward_walk_ops(A0, inclusive=True))`", "Yes, agreed. I think it's not possible to use the graph_editor without using both tensor and ops, which is indeed at odd with the rest of TensorFlow. I'd still call it a \"feature\" and not a bug but I do see where you are coming from.", "@purpledog BTW, I hope you will graduate graph_editor out of contrib at some point, it's useful for running larger models in GPU budget. For instance, you can do imagenet resnet with 4x memory reduction at 1.2 compute cost by checkpointing intermediate activations and using graph editor to rewire the graph.\r\n\r\nHere's a recent neat application -- do backprop on a chain of length n while using constant memory \r\nhttps://github.com/yaroslavvb/chain_constant_memory/blob/master/chain_constant_memory_test.py"]}, {"number": 14857, "title": "how can I ues Dataset to shuffle a large  whole dataset?", "body": "I know we can ues dataset.shuffle(buffer=10000) to shuffle dataset.\r\nBut I have a large image dataset with 2,325,000 images, if I use the follwing code with 'dataset = dataset.shuffle(buffer_size=2325000) ' ,the cost of time to load images is too long for me.\r\n\r\nIs there any way to shuffle the whole dataset in Dataset  API??\r\n\r\n```\r\nfrom tensorflow.contrib import data\r\ndef input_pipeline(filenames, batch_size):\r\n    # Define a `tf.contrib.data.Dataset` for iterating over one epoch of the data.\r\n    dataset = data.TextLineDataset(filenames)\r\n    dataset = dataset.map(decode_func)\r\n    dataset = dataset.shuffle(buffer_size=2325000)  # Equivalent to min_after_dequeue=10000.\r\n    dataset = dataset.batch(batch_size)\r\n\r\n    # Return an *initializable* iterator over the dataset, which will allow us to\r\n    # re-initialize it at the beginning of each epoch.\r\n    return dataset.make_initializable_iterator() \r\n```", "comments": ["I believe you don't have to create so large buffer, perhaps buffer=10000 is enough. The cost is new dataset samples from every 10000 images, instead of total images. However it should be accepted in practice, at least for me.", "The `Dataset.shuffle()` implementation is designed for data that could be shuffled in memory; we're considering whether to add support for external-memory shuffles, but this is in the early stages. In case it works for you, here's the usual approach we use when the data are too large to fit in memory:\r\n\r\n1. Randomly shuffle the entire data once using a MapReduce/Spark/Beam/etc. job to create a set of roughly equal-sized files (\"shards\").\r\n2. In each epoch:\r\n     1. Randomly shuffle the list of shard filenames, using `Dataset.list_files(...).shuffle(num_shards)`.\r\n     2. Use `dataset.interleave(lambda filename: tf.data.TextLineDataset(filename), cycle_length=N)` to mix together records from `N` different shards.\r\n     3. Use `dataset.shuffle(B)` to shuffle the resulting dataset. Setting `B` might require some experimentation, but you will probably want to set it to some value larger than the number of records in a single shard.", "Thanks for the update @mrry!\r\n\r\n@reyne-d does the suggestion of pre-shuffling your data into shards, and then shuffling the shards work for you?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Maybe you can exchange the order. And shuffle the filenames instead of the images.", "@xtudbxk             \r\n\r\nI understand shuffling filenames could handle this problem.\r\nHowever, Tensorflow recommends \"TFRecord\" and \"Dataset AP\" to build an input pipeline.\r\nTherefore, if I construct as below (TFRecord--tf.data), how can I shuffle whole dataset if a TFRecord file is too large to fit on own device memory?\r\nI think I observe repeating loss pattern on Tensorboard due to only partially random dataset. \r\n\r\nSummary: I want to shuffle whole 'tf.data.TFRecordDataset' every epoch.\r\n\r\n`dataset = tf.data.TFRecordDataset(filenames)`\r\n`dataset = dataset.map(parser, self.nMapThread)`\r\n`dataset = dataset.shuffle(buffer_size=10000)`\r\n`dataset = dataset.batch(batchsize)`\r\n`iterator = dataset.make_initializable_iterator()`\r\n`hr_image, lr_bicubic_image = iterator.get_next()`\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@mrry I am trying to implement the your approach. I have a beam job that creates 25 shards of roughly equal size and I am trying to determine the best way to implement. Let's consider `filenames = gs://my-bucket/train-data-*.csv`. I am trying to implement this as a train_input_fn for a tf.estimator like the sample [here](https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/customestimator/trainer/model.py#L321):  \r\n\r\nHere is what I think you are suggesting, but I still feel like I am missing something...\r\n\r\n```\r\ndef input_fn(filenames, num_shards, num_epochs):\r\n    dataset = Dataset.list_files(filenames).shuffle(num_shards) # num_shards = 25\r\n    dataset = dataset.interleave(lambda filename: \r\n        tf.data.TextLineDataset(filename).skip(1).map(parse_csv), cycle_length=num_shards) \r\n    dataset = dataset.shuffle(buffer_size = B) # this should be some value > rows in single shard\r\n    dataset = dataset.repeat(num_epochs)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    features = iterator.get_next()\r\n  return features, features.pop(LABEL_COLUMN)\r\n```\r\nDo you see any issues with this approach? Some questions I have on your write up are:\r\n\r\n1. Am I using the `.skip()` and `.map()` in the proper location?\r\n\r\n1. You mention that you do this each epoch -- I believe that dataset.repeat(num_epochs) takes care of repeating this shuffling for each epoch, correct?\r\n\r\n1. Was there a reason your response had `num_shards` in the filename shuffle and then `cycle_length=N` in the interleave? My understanding from your description is that you would want these both to be the number of shards so that we are sampling from each shard evenly?\r\n\r\n1. Is there any way to get the number of shards dynamically? This should basically be the length of the `list_files` correct?\r\n\r\n1. Will the buffer_size have significant effects on how fast/slow the model takes? Hypothetically, If the shards are still rather large (let's say 500 MB), then I imagine that we would use up quite a lot of memory because the buffer would essentially load 500MB into memory (since buffer_size is basically the same as the size of one shard)", "@jcomfort4 Looks mostly good. In answer to your questions:\r\n\r\n1. Yes. You might want to move the `.map()` after the second `.shuffle()`, but that depends on whether the in-memory representation for parsed or unparsed data is more compact.\r\n\r\n2. Yes, this looks fine.\r\n\r\n3. The two values are somewhat independent. When the number of shards is small (like 25) then setting the `cycle_length` to that number seems like a good idea. If it's large (e.g. 1000) then you might consider setting `cycle_length` to a number less than 1000.\r\n\r\n4. With a slight modification:\r\n\r\n   ```python\r\n   files = tf.matching_files(\"gs://my-bucket/train-data-*.csv\")\r\n   dataset = tf.data.Dataset.from_tensor_slices(files).shuffle(tf.shape(files)[0])\r\n   ```\r\n\r\n5. It'll affect startup time (because the buffer needs to fill up before it returns the first element) but if the buffer fits in RAM, it shouldn't affect training throughput.", "Thanks! So how come I need to use `.from_tensor_slices(files)` -- would this still work with `list_files(\"gs://my-bucket/train-data-*.csv\").shuffle(tf.shape(files)[0])`? I guess I don't fully follow what from_tensor_slices is doing so I'm not completely following why that is necessary.\r\n\r\nI am running into some separate issues when upgrading from the old way of doing things so I'm not sure if I have done something incorrectly. Do either of these errors seem expected to you?\r\n\r\nI have a GCMLE experiment and I am trying to upgrade my `input_fn` to use the new `tf.data` functionality. I have created the following input_fn based off of this [sample][1]\r\n\r\n    def input_fn(...):\r\n        dataset = tf.data.Dataset.list_files(filenames).shuffle(num_shards) # shuffle up the list of input files\r\n    \tdataset = dataset.interleave(lambda filename: # mix together records from cycle_length number of shards\r\n    \t\t\t\ttf.data.TextLineDataset(filename).skip(1).map(lambda row: parse_csv(row, hparams)), cycle_length=5) \r\n        if shuffle:\r\n        dataset = dataset.shuffle(buffer_size = 10000)\r\n        dataset = dataset.repeat(num_epochs)\r\n    \tdataset = dataset.batch(batch_size)\r\n    \titerator = dataset.make_one_shot_iterator()\r\n        features = iterator.get_next()\r\n\r\n        labels = features.pop(LABEL_COLUMN)\r\n        \r\n        return features, labels\r\n\r\nmy `parse_csv` is the same as what I used previously, but it is not currently working. I can fix some of the issues, but I don't fully understand **why** I am having these issues. Here is my parse_csv() function\r\n\r\n    def parse_csv(..):\r\n        columns = tf.decode_csv(rows, record_defaults=CSV_COLUMN_DEFAULTS)\r\n\t\traw_features = dict(zip(FIELDNAMES, columns))\r\n\r\n        words = tf.string_split(raw_features['sentences']) # splitting words\r\n        vocab_table = tf.contrib.lookup.index_table_from_file(vocabulary_file = hparams.vocab_file,\r\n\t\t\t\t\tdefault_value = 0)\r\n\r\n    ....\r\n\r\n1. Right away this `tf.string_split()` stops working and the error is `ValueError: Shape must be rank 1 but is rank 0 for 'csv_preprocessing/input_sequence_generation/StringSplit' (op: 'StringSplit') with input shapes: [], [].` -- this is easily solved by packing `raw_features['sentences']` into a tensor via `[raw_features['sentences']]` but I do not understand why this is needed with the this `dataset` approach? How come in the old version this worked fine? For the shapes to match up with the rest of my model, I end up needing to remove this extra dimension at the end via `words = tf.squeeze(words, 0)` because I add this \"unecessary\" dimension to the tensor. \r\n\r\n2. For whatever reason, I am also getting an error that the table is not initialized `tensorflow.python.framework.errors_impl.FailedPreconditionError: Table not initialized.` however, this code works completely fine with my old `input_fn()` (see below) so I don't know why I would now need to initialize the tables? I have not figured out a solution to this part. Is there anything that I am missing to be able to use `tf.contrib.lookup.index_table_from_file` within my parse_csv function?\r\n\r\nFor reference, this is my old input_fn() that still does work:\r\n\r\n    def input_fn(...):\r\n        filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(filenames), \r\n    \t\t\t\tnum_epochs=num_epochs, shuffle=shuffle, capacity=32)\r\n    \treader = tf.TextLineReader(skip_header_lines=skip_header_lines)\r\n\r\n\t\t_, rows = reader.read_up_to(filename_queue, num_records=batch_size)\r\n\r\n\t\tfeatures = parse_csv(rows, hparams)\r\n\r\n\t\t\t\r\n\t\t\tif shuffle:\r\n\t\t\t\tfeatures = tf.train.shuffle_batch(\r\n\t\t\t\t\tfeatures,\r\n\t\t\t\t\tbatch_size,\r\n\t\t\t\t\tmin_after_dequeue=2 * batch_size + 1,\r\n\t\t\t\t\tcapacity=batch_size * 10,\r\n\t\t\t\t\tnum_threads=multiprocessing.cpu_count(), \r\n\t\t\t\t\tenqueue_many=True,\r\n\t\t\t\t\tallow_smaller_final_batch=True\r\n\t\t\t\t)\r\n\t\t\telse:\r\n\t\t\t\tfeatures = tf.train.batch(\r\n\t\t\t\t\tfeatures,\r\n\t\t\t\t\tbatch_size,\r\n\t\t\t\t\tcapacity=batch_size * 10,\r\n\t\t\t\t\tnum_threads=multiprocessing.cpu_count(),\r\n\t\t\t\t\tenqueue_many=True,\r\n\t\t\t\t\tallow_smaller_final_batch=True\r\n\t\t\t\t)\r\n\r\n\tlabels = features.pop(LABEL_COLUMN)\r\n\r\n\treturn features, labels\r\n\r\n  [1]: https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/customestimator/trainer/model.py#L321", "In the event this type of question is better served on stack overflow, I have added it there as well:\r\n\r\nhttps://stackoverflow.com/questions/48779293/upgrade-to-tf-dataset-not-working-properly-when-parsing-csv", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing since the discussion seems to have successfully moved to stackoverflow.", "@jcomfort4 cool answer!thanks.", "Sorry, I'm really confused. I thought the title is about a large dataset of images but why are we working with CSV data? Are the filenames stored as CSV? If that's the case, is this the recommended pipeline to feed large datasets? Also, what is the difference between the \"dataset = data.TextLineDataset(filenames)\" @reyne-d 's example and \"Dataset.list_files(...).shuffle(num_shards)\" @mrry 's example to shuffle the file names? Some clarification would be much appreciated. Thanks!"]}, {"number": 14856, "title": "Enable GCS filesystem for Windows", "body": "", "comments": ["Can one of the admins verify this patch?", "@snnn any luck with this? you'd also need to pull rebase.", "Hi @drpngx\r\n\r\nThanks for reminder. I'll update this PR by EOD. ", "Thanks! No rush.", "Hi @mrry and @saeta , please review the updated change", "@tensorflow-jenkins test this please.", "Is there anything I can do?", "Looks like an unrelated failure...\r\n\r\n@tensorflow-jenkins test this please.", "note that in #11280, there might be some packaging issue with tensorboard."]}, {"number": 14855, "title": "tensorboard ImportError: cannot import name 'run_main'", "body": "I install the tensorflow on Mac from source.\r\nbut when I run the tensorboard,and got a error like this\r\n\r\n> Pro-2:Desktop xxh$ tensorboard\r\nTraceback (most recent call last):\r\n  File \"/Users/xxh/anaconda3/bin/tensorboard\", line 7, in <module>\r\n    from tensorboard.main import run_main\r\nImportError: cannot import name 'run_main'`\r\n\r\nHow to fix it?", "comments": ["(1) You can't import it. Please go to [here](https://stackoverflow.com/) for support since it's not a bug.\r\n(2) Please fill [ISSUE_TEMPLATE](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md) before raising an issue.", "It could be helpful to run a script directly, e.g. \"python /usr/local/lib/python2.7/dist-packages/tensorboard/main.py --logdir=~~~\"", "Please file TensorBoard issues under https://github.com/tensorflow/tensorboard/issues", "@XiaXuehai Did you ever figure out a fix? After building TF from source on a Mac running High Sierra, and running Tensorboard, I also get that same exact error.", "I ran into this after building tensorflow from source. Looks to me like you do not get tensorboard automatically when you build tensorflow yourself.\r\n\r\nIt worked for me after I built tensorboard separately from source: https://github.com/tensorflow/tensorboard.", "I fixed this by changing the shebang at the top of the python file /usr/local/bin/tensorboard to #!/usr/bin/python3 (was just #!/usr/bin/python). "]}, {"number": 14854, "title": "Add batch support for various image_ops", "body": "Working on #8926\r\nI used #7369 as a guide for my work here.\r\n\r\nI have added batch support for:\r\n\r\n- `flip_left_right`\r\n- `flip_up_down`\r\n- `random_flip_left_right`\r\n- `random_flip_up_down`\r\n- `transpose_image`\r\n- `rot90`\r\n\r\nI have corrected existing tests in `image_ops_test.py` and introduced a number of new tests based on existing tests for 3D inputs. \r\n\r\nThis is my first contribution to this repository and I have tried to follow the [`contributing`](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md) guidelines. However, running `pylint` on `image_ops_impl.py` and `image_ops_test.py` revealed a number of pre-existing style violations. I've tried to fix the ones relevant to my work but may have missed some.", "comments": ["Can one of the admins verify this patch?", "Hmm, I can successfully run the sanity checks locally via:\r\n`tensorflow/tools/ci_build/ci_build.sh CPU tensorflow/tools/ci_build/ci_sanity.sh`\r\n\r\nDoes anyone happen to know how I can get more information on why they're failing on CI? The relevant output says:\r\n\r\n```\r\nStep 3 : COPY install/*.sh /install/\r\ntensorflow/tools/ci_build/ci_build.sh: line 130: 26052 Terminated              docker build -t ${DOCKER_IMG_NAME} -f \"${DOCKERFILE_PATH}\" \"${DOCKER_CONTEXT_PATH}\"\r\nERROR: docker build failed. Dockerfile is at /var/lib/jenkins/workspace/tensorflow-pull-requests-sanity/tensorflow/tools/ci_build/Dockerfile.cpu\r\nBuild was aborted\r\nAborted by unknown\r\nUnable to get pull request builder trigger!!\r\nSetting status of abcb89bce4ce4b68cf714e1cccbc13d4eb1309b7 to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/11268/ and message: 'FAILURE\r\n '\r\nUsing context: Sanity Checks\r\nFinished: ABORTED\r\n```\r\n\r\n", "Thanks for taking a look guys. @martinwicke Let me know if you spot any other opportunities for cleanup/refactoring.", "Hey @JoshVarty, thanks for the contribution. Unfortunately, we have to revert this change because it turns out to be a bottleneck in our image models. There wasn't a reasonable way to fix the performance, because it's caused by both the conversion to 4-D and switching to tf.where. We'd want the 4-D version to perform reasonably well before we leave it in.\r\n\r\nI added benchmarks here:\r\nhttps://github.com/tensorflow/tensorflow/pull/15348\r\n\r\nYou can run it with `bazel run -c opt image_ops_test -- --benchmarks=FlipImageBenchmark`", "Thanks for the heads up guys. Presumably the other places where 3-D tensors are expanded to 4-D tensors would also be slow, correct? (Perhaps they're not used in your image models so it wasn't noticed until now)\r\n\r\nIt sounds like two fixes are needed:\r\n1. We shouldn't always expand 3-D tensors to 4-D.\r\n2. We need an alternative to `tf.where` when operating on 4-D tensors.\r\n\r\nThe first issue I think I know how to fix.\r\n\r\nDo you guys have any hunches on how performance for 4-D batches might be fixed?  Is there a performant alternative to `tf.where` I should experiment with? Would a new op for 4-D tensors make sense here?\r\n\r\n", "Yeah, possibly other 3-D to 4-D changes are slower too, but this change was really noticeable in our internal benchmarks.\r\n\r\nThe `tf.where` issue is because we're flipping all images. With `tf.cond` on a 3-D Tensor, we only flip images as needed.", "Would you guys be comfortable with a solution that used `tf.cond` when operating on 3-D tensors, but `tf.where` when operating on 4-D tensors? Or is it best to avoid `tf.where` altogether?", "I'm comfortable with anything where the recently added benchmarks perform about the same before and after, including with 4-D tensors.\r\n\r\nThe issue is that we'd discourage users from using 4-D if it's significantly slower than 3-D on a per-image benchmark.", "(Note the benchmark didn't include 4-D tests, but it's trivial to add)."]}, {"number": 14853, "title": "Automate download and unzip of the model file", "body": "TESTING\r\n\r\nUsed Android Studio 3.1.3, NDK r17b and Pixel XL API 24 emulator.\r\n\r\nBlocked from testing the built app due to this issue: #18658\r\n\r\n1 - Did a ./gradlew clean.  Deleted both intermediate download and unzipped versions of the model:\r\n```\r\n$ rm app/build/intermediates/mobilenet_v1_224_android_quant_2017_11_08.zip\r\n$ rm app/src/main/assets/mobilenet_quant_v1_224.tflite\r\n```\r\nBuilt the app and confirmed the model got both downloaded and unzipped:\r\n```\r\n$ ./gradlew assemble\r\n<snip>\r\n:app:downloadModel\r\nDownloading https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip\r\n:app:unzipModel\r\nUnzipping build/intermediates/mobilenet_v1_224_android_quant_2017_11_08.zip\r\n:app:preBuild\r\n<snip>\r\n```\r\n\r\n2 - Deleted the model file from the assets folder and checked it gets unzipped again from the intermediate storage location:\r\n```\r\n$ ./gradlew assemble\r\n<snip>\r\n:app:downloadModel UP-TO-DATE\r\n:app:unzipModel\r\nUnzipping build/intermediates/mobilenet_v1_224_android_quant_2017_11_08.zip\r\n:app:preBuild\r\n<snip>\r\n```\r\n\r\n3 - Built it again and check it doesn't get downloaded or unzipped again:\r\n```\r\n$ ./gradlew assemble\r\n<snip>\r\n:app:downloadModel UP-TO-DATE\r\n:app:unzipModel UP-TO-DATE\r\n<snip>\r\n```", "comments": ["Can one of the admins verify this patch?", "@daj can you resolve the merge conflict", "I've updated the pull request and rebased onto the latest master.", "Jenkins, test this please.", "@aselle WDYT?", "@aselle reminder, WDYT?", "Nagging Reviewer @aselle: It has been 41 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "@aselle please can you review?  I can update the PR to resolve the conflict, but I only want to do that if this will get reviewed.  Thanks!", "Nagging Reviewer @aselle: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @aselle: It has been 37 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "@daj could you pull rebase and push again?\r\n\r\n@aselle WDYT?", "I opened this pull request 6 months ago, and I already rebased it once during that time.  I'd like to hear from a reviewer that they intend to review and merge it before I spend any time doing more rebasing.  Thanks!", "@shashishekhar \r\n - I've updated the README (the section I modified has moved) and rebased.\r\n - I was able to test the model download code (see updated PR description), however https://github.com/tensorflow/tensorflow/issues/18658 is blocking me from running the demo app.\r\n - Let me know if you want to remove or update [the instructions for using Inception-v3](https://github.com/daj/tensorflow/blob/066ab3c76fd3f6ad5cf14137dfe7a7fbd89d276b/tensorflow/docs_src/mobile/tflite/demo_android.md#using-other-models).  I suggest either removing them (now that the Mobilenet model is automatically downloaded), or updating the docs to explain why somebody would want to use Inception-v3 instead of Mobilenet.", "@shashishekhar I rebased again and resolved another conflict.  Hopefully we can get it merged before things change again. :-)", "All tests pass. Woohoo!"]}]