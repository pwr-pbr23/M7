[{"number": 53767, "title": "TFLite Converter segfaults when trying to convert per-channel quantized transposed convolutions", "body": "When converting transposed convolutions using per-channel weight quantization the converter segfaults and crashes the Python process. Per-channel quantization is supported by TFLite Transposed convolutions:\r\nhttps://github.com/tensorflow/tensorflow/blob/f87be6c7de847017c48520649e3d771e5d6b81b6/tensorflow/lite/kernels/transpose_conv.cc#L371-L380\r\nso the converter shouldn't segfault when trying to convert such a model.\r\n\r\nIt looks like this issue has been introduced in TensorFlow 2.6 since the same model code produced a valid TFLite file in TensorFlow 2.5. This issue might also be related to #53766, but in any case the converter should never segfault.\r\n\r\n### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS / Ubuntu\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6, 2.7, 2.8rc0 and 2.9.0-dev20220114\r\n\r\n### 2. Code\r\n\r\nA minimal reproduction of the issue and a workaround is available in [this notebook](https://colab.research.google.com/drive/1IXri5HeDc9qTAtDOp-LqZyQTL8CcemGq?usp=sharing).\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass QuantConv2DTransposed(tf.keras.layers.Layer):\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight(\"kernel\", [3, 3, input_shape[-1], 24])\r\n\r\n    def call(self, inputs):\r\n        filters = tf.quantization.fake_quant_with_min_max_vars_per_channel(\r\n            self.kernel, -3.0 * tf.ones([24]), 3.0 * tf.ones([24]), narrow_range=True\r\n        )\r\n        filters = tf.transpose(filters, (0, 1, 3, 2))\r\n        return tf.nn.conv2d_transpose(inputs, filters, [*inputs.shape[:-1], 24], 1)\r\n\r\n\r\ninp = tf.keras.Input(shape=(6, 8, 48), batch_size=1)\r\nx = tf.quantization.fake_quant_with_min_max_vars(inp, -3.0, 3.0, narrow_range=True)\r\nx = QuantConv2DTransposed()(x)\r\nx = tf.quantization.fake_quant_with_min_max_vars(x, -3.0, 3.0, narrow_range=True)\r\n\r\nmodel = tf.keras.Model(inp, x)\r\n\r\nmodel.save(\"/tmp/testing\")\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"/tmp/testing\")\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n# terminated by signal SIGSEGV (Address boundary error)\r\ntflite_model = converter.convert()\r\n```", "comments": ["@Saduf2019  Was able to replicate the issue on colab using  TF v[2.6.0](https://colab.research.google.com/gist/sushreebarsa/3325c1d57489120e4a7af70701f4bae0/quant-conv-transposed-pre-channel.ipynb#scrollTo=Uwk4VEay4RRr), [2.7.0 ](https://colab.research.google.com/gist/sushreebarsa/d160e5366f330488d391b19ecdd2ba1e/53767.ipynb)and[ tf-nightly](https://colab.research.google.com/gist/sushreebarsa/8b314dfdc9b269823840e789b69eb756/quant-conv-transposed-pre-channel.ipynb#scrollTo=jNfg0zbs4N-g)(2.9.0-dev20220114),please find the attached gists.Thank you!", "> Was able to replicate the issue on colab using TF v2.6.0, 2.7.0 and tf-nightly(2.9.0-dev20220114),please find the attached gists\r\n\r\n@sushreebarsa  Thanks for confirming. Just for reference, your reproduction on TF 2.6.0 actually now fails due to an unrelated Keras version conflict. Changing the dependency from `v2.6.0` to `v2.6.2` will fix this and allow you to correctly reproduce the segfault mentioned in this issue.", "This is still an issue in `2.9.0-dev20220318`. Are there any updates on this?\r\nBeing able to trigger a converter segfault seems to be quite problematic."]}, {"number": 53766, "title": "Constant folding fails when converting int8 transposed convolutions", "body": "When converting a model that used int8 quantization aware training conversion of transposed convolutions fails.\r\n\r\nThe converter isn't able to correctly constant fold the fake quantized weights and keeps an unnecessary `tfl.transpose` operation in the graph which leads to problems when executing the TFLite model. Note that this issue is independent of TensorFlow Model Optimization and can be reproduced using plain TensorFlow as well (see linked notebook).\r\n\r\n### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS / Ubuntu\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5, 2.6, 2.7, 2.8rc0 and 2.9.0-dev20220114\r\n\r\n### 2. Code\r\n\r\nA minimal reproduction of the issue is available in [this notebook](https://colab.research.google.com/drive/1bfI_Dx3zyoHkabJlHpNFnbDA9r9G5ffE?usp=sharing). Re-run the notebook to show `netron` visualisations showing the conversion problem.", "comments": ["@lgeiger ,\r\nCan you please confirm this issue is duplicate of issue #53767.if yes, Can you please close this issue, since it is already being tracked there? Thanks!", "> Can you please confirm this issue is duplicate of issue #53767.\r\n\r\n@tilakrayal I cannot confirm, this issue is not a duplicate of #53767.\r\n\r\n#53767 talks about **per-channel quantization** which makes the converter crash/segfault (maybe this is even a security vulnerability?). Whereas this issue refers to **per-tensor quantization** which merely fails to properly constant fold the graph and therefore results in wrong output but does not crash the converter.\r\n\r\nThe problems might be related, but fundamentally they manifest themselves in two different issues.", "@Saduf2019  ,\r\nI was able to reproduce the issue in tf [v2.5](https://colab.research.google.com/gist/tilakrayal/039895e64be9e908165c62087c9986c0/quant_transposed_conv_2_5_0.ipynb),[v2.7](https://colab.research.google.com/gist/tilakrayal/21cd6a527e22819de2f26cc904b60034/quant_transposed_conv-2-7-0.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/17a4ebeb7b4d5aa1ec543e57bbc88530/quant_transposed_conv_nightly.ipynb).Please find the gist."]}, {"number": 53763, "title": "[TFLite] Accumulator and bias types coherence for int16x8 FC operator", "body": "Hello,\r\n\r\nWe noticed that int32 accumulator and int32 bias support was recently added for the int16x8 FULLY_CONNECTED operator (along with CONV_2D and TRANSPOSE_CONV_2D operators) through the `_experimental_full_integer_quantization_bias_type` option of the TFLiteConverter :\r\n* [Add option to store bias as 32 bit in 16x8 Quant.](https://github.com/tensorflow/tensorflow/commit/ea33c1e7a25d8025e8ee405ad8ab7be261798d76)\r\n* [Update TFLite kernel to use Ruy 16x8 Gemm instead of reference kernel.](https://github.com/tensorflow/tensorflow/commit/5de18207d9568ee25bdd57db8eaf6268fe0b913d) \r\n\r\nIt seems though that these changes are leading to some incoherences with potential unexpected overflow and also created a bug. Before an int16x8 FC would always use an int64 accumulator to avoid any overflow and an int64 bias. The current status now seems to be:\r\n* converter._experimental_full_integer_quantization_bias_type = tf.int64 or None:\r\n    * OpResolverType.BUILTIN_REF\r\n        * use_bias = True\r\n            * int64 accumulator and int64 bias\r\n        * use_bias = False\r\n            * int64 accumulator\r\n    * OpResolverType.BUILTIN\r\n        * use_bias = True\r\n            * int64 accumulator and int64 bias\r\n        * use_bias = False\r\n            * int32 accumulator (would have expected an int64 accumulator)\r\n * converter._experimental_full_integer_quantization_bias_type = tf.int32:\r\n    * OpResolverType.BUILTIN_REF\r\n        * use_bias = True\r\n            * int64 accumulator and int64 bias (read an int32 bias tensor as an int64 tensor => bug, need to be an int32 bias and would expect an int32 accumulator)\r\n        * use_bias = False\r\n            * int64 accumulator (would have expected an int32 accumulator)\r\n    * OpResolverType.BUILTIN\r\n        * use_bias = True\r\n            * int32 accumulator and int32 bias\r\n        * use_bias = False\r\n            * int32 accumulator\r\n\r\nIdeally we would have expected that an int64 accumulator (no matter if the layer has no bias) and int64 bias would be used when `converter._experimental_full_integer_quantization_bias_type = tf.int64 or None` (both with the reference and optimized kernels) and an int32 accumulator (no matter if the layer has no bias) and int32 bias when `converter._experimental_full_integer_quantization_bias_type = tf.int32` (both with the reference and optimized kernels).\r\n\r\nThese incoherences also affect the CONV_2D and TRANPOSE_CONV_2D operators and we were wondering if it was intentional (and if so the relation between the bias type and the accumulator type) or just an oversight?\r\n\r\nCode to illustrate the bug with `converter._experimental_full_integer_quantization_bias_type = tf.int32`, `OpResolverType.BUILTIN_REF` and `use_bias=True`. It's due to the https://github.com/tensorflow/tensorflow/blob/ea33c1e7a25d8025e8ee405ad8ab7be261798d76/tensorflow/lite/kernels/fully_connected.cc#L835 condition that will be true if `kernel_type == kReference` no matter the type of the bias while `FullyConnectedInt16` will always assume an int64 bias and thus read the int32 bias tensor as an int64 tensor. It fails to provide the expected results with tf_nightly-2.9.0.dev20220114.\r\n\r\n```Python\r\nimport tensorflow as tf\r\n\r\ninput = tf.keras.Input(shape=[1], batch_size=1)\r\noutput = tf.keras.layers.Dense(\r\n    10,\r\n    kernel_initializer=tf.keras.initializers.Constant(value=0),\r\n    bias_initializer=tf.keras.initializers.Constant(value=1),\r\n)(input)\r\nmodel = tf.keras.Model(inputs=[input], outputs=output)\r\nmodel.summary()\r\n\r\n\r\n\r\ndef representative_dataset():\r\n    yield [tf.constant([1.0])]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\r\n]\r\nconverter._experimental_full_integer_quantization_bias_type = tf.int32 # Will work if commented\r\n\r\ntflite_model = converter.convert()\r\nwith open(\"test.tflite\", \"wb\") as fp:\r\n    fp.write(tflite_model)\r\n\r\n\r\n\r\ninterpreter = tf.lite.Interpreter(\r\n    model_path=\"test.tflite\",\r\n    experimental_op_resolver_type=tf.lite.experimental.OpResolverType.BUILTIN_REF, # Will work if commented\r\n)\r\ninterpreter.allocate_tensors()\r\n\r\ninput = [tf.constant([1.0])]\r\ninterpreter.set_tensor(interpreter.get_input_details()[0]['index'], input)\r\ninterpreter.invoke()\r\noutput = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\r\n\r\n# [1.]\r\nprint(input)\r\n# Expected: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\r\nprint(output)\r\n```\r\n\r\nFYI @dayeongl", "comments": ["Hi @Tessil! Thanks for the reporting the issue in 2.9 . Please switch to stable version 2.7 for a while. Thank you!", "thanks for reporting! I will work on it ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53763\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53763\">No</a>\n", "@mohantym Would it be possible to re-open the issue? Thanks!", "Ok @Tessil ! Re-opening as it is still replicating in [2.9](https://colab.sandbox.google.com/drive/1bl87zJXGeWRShJVqTMx-zk7AVy7m8R66?resourcekey=0-qP8tlN_1-i1fbmuYZaMfyg#scrollTo=Gm5V3_HCiAQD) . Thank you!", "@dayeongl Thank you for solving the bug with commit d088d70cc655a16360bd7926db3fe2842414255b With `use_bias = True` things are coherent now, an int32 accumulator is used with an int32 bias and an int64 accumulator with an int64 bias.\r\n\r\nBut if `use_bias = False` is used, an int32 accumulator will be used in all cases, is it intentional? I would expect that setting `converter._experimental_full_integer_quantization_bias_type` to `tf.int64` or `None` would enable the usage of an int64 accumulator in all cases as even if an int16x8 layer has no bias, there's still a high risk of overflow with an int32 accumulator.\r\n\r\nIt also changes the default numerical behaviour of the reference kernel, before an int16x8 FC layer without a bias would have an int64 accumulator but now an int32 one is used even if `converter._experimental_full_integer_quantization_bias_type` is not set to `tf.int32`.\r\n\r\n"]}, {"number": 53752, "title": "Update image_ops_impl.py", "body": null, "comments": ["Thanks for the fix. PR looks good. \r\nCould you sign the Google Contributor License Agreement so I can approve it: https://opensource.google/documentation/reference/cla#external_contributors?", "@vertify-swdev1 Can you please sign CLA. Thank you!", "Also, please don't use default \"update <file>\" message. Make it more descriptive. See https://cbea.ms/git-commit/"]}, {"number": 53750, "title": "#undef some macros with generic names to prevent -Wmacro-redefined from being triggered", "body": "As tensorflow and openfst originate from Google internal codebase, they are using the same macros to implement logging.\r\n\r\nThis causes `-Wmacro-redefined` when both `fst/log.h` and `tensorflow/core/platform/logging.h` are included.\r\n\r\nThis PR backports the #undef series from protobuf (the number of macros and their names differ).\r\nIt could be better to rename these macros instead (i. e. TF_LOG / FST_LOG / PB_LOG), but I am quite unsure wrhether such rename would cause any troubles during sync.", "comments": ["protobuf series of #undefs in [here](https://github.com/protocolbuffers/protobuf/blob/master/src/google/protobuf/stubs/logging.h#L117)", "I would also like to backport this patch to openfst's fst/log.h.\r\nUnfortunately, openfst is not being developed on GitHub and it the ways to contribute are unclear to be.\r\n\r\nCould you, please, help me with reaching out openfst developers?"]}, {"number": 53749, "title": "Convert tflite model with customerized QuantizeConfig for different layers.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source):pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tf 2.5\r\n\r\n### 2. Code\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow_model_optimization as tfmot\r\nimport numpy as np\r\nimport tempfile\r\nfrom tensorflow.keras import layers\r\n\r\n\r\n###################################################\r\nLastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\r\nMovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\r\n\r\n\r\nclass DefaultConv2DQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\r\n    # Configure how to quantize weights.\r\n    def get_weights_and_quantizers(self, layer):\r\n        return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\r\n\r\n    # Configure how to quantize activations.\r\n    def get_activations_and_quantizers(self, layer):\r\n        return [(layer.activation, MovingAverageQuantizer(num_bits=16, symmetric=False, narrow_range=False, per_axis=False))]\r\n\r\n    def set_quantize_weights(self, layer, quantize_weights):\r\n        # Add this line for each item returned in `get_weights_and_quantizers`\r\n        # , in the same order\r\n        layer.kernel = quantize_weights[0]\r\n\r\n    def set_quantize_activations(self, layer, quantize_activations):\r\n        # Add this line for each item returned in `get_activations_and_quantizers`\r\n        # , in the same order.\r\n        layer.activation = quantize_activations[0]\r\n\r\n    # Configure how to quantize outputs (may be equivalent to activations).\r\n    def get_output_quantizers(self, layer):\r\n        return []\r\n\r\n    def get_config(self):\r\n        return {}\r\n\r\ndef apply_quantization_to_conv2d(layer):\r\n    if isinstance(layer, tf.keras.layers.Conv2D):\r\n        return   tfmot.quantization.keras.quantize_annotate_layer(layer, DefaultConv2DQuantizeConfig())\r\n    return layer\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\r\n    quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\r\n    quantize_scope = tfmot.quantization.keras.quantize_scope\r\n\r\n    input = keras.Input(shape=(28,28,1))\r\n    x1 = keras.layers.Conv2D(filters=12, kernel_size=(3,3), activation='relu')(input)\r\n    x2 = keras.layers.Conv2D(filters=12, kernel_size=(3,3), activation='relu')(x1)\r\n    x3 = keras.layers.Conv2D(filters=12, kernel_size=(3,3), activation='relu')(x2)\r\n\r\n    model = keras.Model(input, x3)\r\n    model = tf.keras.models.clone_model(model, clone_function=apply_quantization_to_conv2d)\r\n    with tfmot.quantization.keras.quantize_scope({'DefaultConv2DQuantizeConfig': DefaultConv2DQuantizeConfig}):\r\n        model = tfmot.quantization.keras.quantize_apply(model)\r\n\r\n\r\n    q_aware_model = model\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n    quantized_tflite_model = converter.convert()\r\n\r\n    #save tflite file\r\n    with open(\"testtf25.tflite\", 'wb') as f:\r\n        f.write(quantized_tflite_model)\r\n\r\n    interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\r\n    interpreter.allocate_tensors()\r\n\r\n\r\n\r\n### 3. Failure after conversion\r\n\r\nWe're trying to quantization-aware train a model. But we're not sure how to set the QuantizeConfig with activation 16bits, weights 8bits for different layers. We try to quantize the convoluational layers as the above code. The above code can produce a tflite model, but it outputs an RuntimeError. \r\n\r\n\"\r\n  interpreter.allocate_tensors()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py\", line 259, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\nRuntimeError: tensorflow/lite/kernels/conv.cc:353 bias->type != kTfLiteInt64 (INT32 != INT64)Node number 1 (CONV_2D) failed to prepare.\r\n\"\r\n\r\nWe're not sure how to set the QuantizeConfig of different keras layers. Could you give us some reference or examples to quantization-aware training a model with different kinds of layers: convoluational layer, MaxPooling2D() layer, etc. \r\n\r\n", "comments": ["@JuanLei2019 Could you please try to use latest TF v2.7.0 and let us know the outcome?Thanks!", "@sushreebarsa Thanks for your response. We have tried the TF v2.7.0 and it outputs the same error. \r\nBy the way, we change the  class \" InputLayerQuantize\" of  \"default_8bit_transforms.py\"  to make the output of \"InputLayer\" to int16 type. But we think it's not the reason which cause the above runtime error. ", "> @JuanLei2019 Could you please try to use latest TF v2.7.0 and let us know the outcome?Thanks!\r\n\r\n@sushreebarsa Thanks for your response. We have tried the TF v2.7.0 and it outputs the same error.\r\nBy the way, we change the class \" InputLayerQuantize\" of \"default_8bit_transforms.py\" to make the output of \"InputLayer\" to int16 type. But we think it's not the reason which cause the above runtime error.\r\n\r\n", "@JuanLei2019 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "> @JuanLei2019 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!  \r\n\r\nRunning the below code will output the error. Thanks.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow_model_optimization as tfmot\r\nimport numpy as np\r\nimport tempfile\r\nfrom tensorflow.keras import layers\r\n\r\n\r\nLastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\r\nMovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\r\n\r\nclass DefaultConv2DQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\r\n    def get_weights_and_quantizers(self, layer):\r\n        return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\r\n    def get_activations_and_quantizers(self, layer):\r\n        return [(layer.activation, MovingAverageQuantizer(num_bits=16, symmetric=False, narrow_range=False, per_axis=False))]\r\n    def set_quantize_weights(self, layer, quantize_weights):\r\n        layer.kernel = quantize_weights[0]\r\n    def set_quantize_activations(self, layer, quantize_activations):\r\n        layer.activation = quantize_activations[0]\r\n    def get_output_quantizers(self, layer):\r\n        return []\r\n    def get_config(self):\r\n        return {}\r\n\r\ndef apply_quantization_to_conv2d(layer):\r\n    if isinstance(layer, tf.keras.layers.Conv2D):\r\n        return   tfmot.quantization.keras.quantize_annotate_layer(layer, DefaultConv2DQuantizeConfig())\r\n    return layer\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\r\n    quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\r\n    quantize_scope = tfmot.quantization.keras.quantize_scope\r\n\r\n    input = keras.Input(shape=(28,28,1))\r\n    x1 = keras.layers.Conv2D(filters=12, kernel_size=(3,3), activation='relu')(input)\r\n    x2 = keras.layers.Conv2D(filters=12, kernel_size=(3,3), activation='relu')(x1)\r\n    x3 = keras.layers.Conv2D(filters=12, kernel_size=(3,3), activation='relu')(x2)\r\n\r\n    model = keras.Model(input, x3)\r\n\r\n    model = tf.keras.models.clone_model(model, clone_function=apply_quantization_to_conv2d)\r\n    with tfmot.quantization.keras.quantize_scope({'DefaultConv2DQuantizeConfig': DefaultConv2DQuantizeConfig}):\r\n        model = tfmot.quantization.keras.quantize_apply(model)\r\n\r\n    q_aware_model = model\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    quantized_tflite_model = converter.convert()\r\n\r\n    #save tflite file\r\n    with open(\"testtf25.tflite\", 'wb') as f:\r\n        f.write(quantized_tflite_model)\r\n\r\n    interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\r\n    interpreter.allocate_tensors()\r\n\r\n```", "@sachinprasadhs Was able to replicate the issue on colab using TF [v2.7.0](https://colab.research.google.com/gist/sushreebarsa/994675facdb1d4d555a645168d112e5c/53749.ipynb#scrollTo=utwlQejrw3Qq) and [tf-nightly](https://colab.research.google.com/gist/sushreebarsa/b21d3cb2b642848974d675632fad238c/53749-nightly.ipynb),please find the attached gists for reference.Thanks!", "> @sachinprasadhs Was able to replicate the issue on colab using TF [v2.7.0](https://colab.research.google.com/gist/sushreebarsa/994675facdb1d4d555a645168d112e5c/53749.ipynb#scrollTo=utwlQejrw3Qq) and [tf-nightly](https://colab.research.google.com/gist/sushreebarsa/b21d3cb2b642848974d675632fad238c/53749-nightly.ipynb),please find the attached gists for reference.Thanks!\r\n\r\n@sushreebarsa\uff0c@sachinprasadhs\uff0c hello, sorry to interrupt\uff0cI do not know how to check the \"attached gists for reference\". I can not find any attached files. Could you paste the reference here? or could you give me some advice how to solve the errors? Thanks.", "The gist for v2.7.0 is https://colab.research.google.com/gist/sushreebarsa/994675facdb1d4d555a645168d112e5c/53749.ipynb#scrollTo=utwlQejrw3Qq and for tf-nightly is https://colab.research.google.com/gist/sushreebarsa/b21d3cb2b642848974d675632fad238c/53749-nightly.ipynb, could you please verify the error message reproduced. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> The gist for v2.7.0 is https://colab.research.google.com/gist/sushreebarsa/994675facdb1d4d555a645168d112e5c/53749.ipynb#scrollTo=utwlQejrw3Qq and for tf-nightly is https://colab.research.google.com/gist/sushreebarsa/b21d3cb2b642848974d675632fad238c/53749-nightly.ipynb, could you please verify the error message reproduced. Thanks!\r\n\r\nThanks for the reply. I checked the error message. The output error message is a little different. We change the class \" InputLayerQuantize\" of \"default_8bit_transforms.py\" to make the output of \"InputLayer\" to int16 type. The output error message is  \"interpreter.allocate_tensors() \r\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py\", line 259, in allocate_tensors\r\nreturn self._interpreter.AllocateTensors()\r\nRuntimeError: tensorflow/lite/kernels/conv.cc:353 bias->type != kTfLiteInt64 (INT32 != INT64)Node number 1 (CONV_2D) failed to prepare.\" And the link error message is \"RuntimeError: tensorflow/lite/kernels/conv.cc:357 output->type != input_type (INT16 != INT8)Node number 1 (CONV_2D) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0.\".  But they are occured at the same \"conv.cc\" file. ", "> The gist for v2.7.0 is https://colab.research.google.com/gist/sushreebarsa/994675facdb1d4d555a645168d112e5c/53749.ipynb#scrollTo=utwlQejrw3Qq and for tf-nightly is https://colab.research.google.com/gist/sushreebarsa/b21d3cb2b642848974d675632fad238c/53749-nightly.ipynb, could you please verify the error message reproduced. Thanks!\r\n\r\nHi, sorry to disturb you,  we have checked the error messages. And  could you give us some advice how to solve the errors?", "@ethkim Triaging to MOT team"]}, {"number": 53746, "title": "Problems with xla and dynamic LinearOperatorBlockDiag. Issues developing custom XLA op", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution:  Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): r2.7\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source):  3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.3.1\r\n\r\n**Describe the current behavior**\r\nI have a 2D sparse square matrix, where the elements are organized in blocks around the diagonal, similar to [LinearOperatorBlockDiag](https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorBlockDiag). \r\nAt compilation time I know the total matrix size, but the size of each of its blocks its not known until execution time.\r\nI need to multiply that matrix by a 1D vector to generate a 1D vector, and similar to [matvec](https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorBlockDiag#matvec) method I would like to only compute the dense part of the matrix, i.e. block_0 x sub_vector_0, block_1 x sub_vector_1 ... block_n x sub_vector_n.\r\nAnd I would like to have it working with XLA.\r\n\r\nI have tried XLA with LinearOperatorBlockDiag, however it is not XLA compatible\r\nI would like to create a custom XLA op, which could deal with the computation and offsets internally: \r\nit can receive as input a (NxN) matrix, a (N) vector and an array of M offsets (the size of each block), and generate the final (N) vector.\r\nI cannot express it with the available [XLA ops](https://www.tensorflow.org/xla/operation_semantics): tried with xla::DynamicSlice + xla::Dot ,  but I do not know the size of the submatrices/subvectors, so I cannot pass the sizes to DynamicSlice.\r\n\r\nI have created my own [Custom Call](https://www.tensorflow.org/xla/custom_call) but I am not sure how to expose it to TensorFlow. The documentation does not include how to compile the Custom Call, integrate, and how to expose it.\r\nIn any case I have register a XLA op within `tensorflow/compiler/tf2xla/kernels/mat_vec_op.cc`, compiled, but I cannot call my CustomCall (sitting in `tensorflow/compiler/xla/service/cpu/mycustomcall.cc`), it seems it was not exposed correctly, and it does not find it.\r\n\r\n\r\nI would like to ask here:\r\n1. Is it possible to implement that kind of op on TF-XLA?\r\n2. If yes, can you please provide some guidance on how?\r\nif it is with a CustomCall, can you please provide help on how to compile, expose and use? \r\n \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nPython example of what I am trying to do:\r\n\r\n\r\n`import tensorflow as tf`\r\n`n_blocks = 4`\r\n\r\n`@tf.function(jit_compile=True)`\r\n`def myfunc(segment_ids, vector):`\r\n`    value = tf.ones_like(segment_ids)`\r\n`    size_blocks = tf.math.unsorted_segment_sum(value, segment_ids, num_segments=n_blocks)`\r\n`    offsets = tf.concat([[0], tf.math.cumsum(size_blocks)], axis=-1)`\r\n`    blocks_list = []`\r\n`    for i in range(n_blocks):`\r\n`        block = vector[offsets[i]:offsets[i+1]]`\r\n`        block = tf.einsum('i,j -> ij', block, tf.transpose(block))`\r\n`        blocks_list.append(tf.linalg.LinearOperatorFullMatrix(block))`\r\n`    matrix = tf.linalg.LinearOperatorBlockDiag(blocks_list)`\r\n`    return matrix.matvec(vector)`\r\n\r\n\r\n`for i in range(3):`\r\n`    segment_ids = tf.constant([0, 0, 1, 1, 2, 3])`\r\n`    vector = tf.ones([6], dtype=tf.float64)`\r\n`    print(myfunc(segment_ids, vector))`\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@pemoi1982jpm \r\nIn order to expedite the trouble-shooting process, please provide a code snippet in colab gist ,notebook link to reproduce the issue reported here.Thanks!", "Here you can find the gist:\r\n[https://gist.github.com/pemoi1982jpm/382ef8c23622dcfd4c878cc403fe6c74](https://gist.github.com/pemoi1982jpm/382ef8c23622dcfd4c878cc403fe6c74)\r\n\r\nThanks", "@pemoi1982jpm I tried to replicate the issue on colab using TF v2.7.0 , tf-nightly ,and faced a different error .Could you please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/6d01dae2795d6afe02d24bff53ab0c3f/53746.ipynb) and confirm the same?Could you please refer to the [ tutorial](https://www.tensorflow.org/xla/tutorials/jit_compile) and similar [issue1](https://github.com/tensorflow/tensorflow/issues/14506), [issue2 ](https://github.com/tensorflow/tensorflow/issues/43687),let us know if it helps?Please refer to the [tested build configurations](https://www.tensorflow.org/install/source#tested_build_configurations) to see version compatibility.Thanks!", "That error you faced it is the same I got, and the reason why I said: \r\n\"I have tried XLA with LinearOperatorBlockDiag, however it is not XLA compatible\"\r\n\r\nThose issues you pointed do not seem to apply here. It is not a performance issue, and I am running everything on CPU.\r\n\r\nThe dynamic shapes of the blocks seem to be the issue.\r\n\r\nI would like to ask here:\r\n    Is it possible to implement that kind of op on TF-XLA?\r\n    If yes, can you please provide some guidance on how?\r\n    if it is with a CustomCall, can you please provide help on how to compile, expose and use?\r\n\r\n\r\nThank you\r\n", "Any update on this?\r\nThank you"]}, {"number": 53743, "title": " Python API of `saved_model.load` to support `tf.saved_model.experimental.VariablePolicy`", "body": "**System information**\r\n- TensorFlow version (you are using): TF2.7\r\n- CPU memory: 16Gb\r\n- GPU memory: 10Gb\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSome models need to store large weights/tables on CPU and smaller ones on GPU. tf.saved_model.experimental.VariablePolicy is able to store device placement but not fully supported by tf.saved_model.load. Every variable is loaded to GPU if available. There is support on C++ level, but missing in python. A repo.py is as follows. Run as python repo.py --size 11, A OOM is observed. Set the number 11 to be any number b/w the memory size of your CPU and GPU in GB.\r\n\r\n```\r\nimport argparse\r\nimport tensorflow as tf\r\nclass LN(tf.keras.layers.Layer):\r\n    def __init__(self, rows, cols,trainable=True):\r\n        super(LN, self).__init__(dtype=tf.float32)\r\n        self.rows = rows\r\n        self.cols = cols\r\n        self.mat = None\r\n        self.trainable=trainable\r\n\r\n    def build(self, input_shape):\r\n        self.mat = self.add_weight(\"mat\",shape=[self.rows, self.cols],\r\n                                       dtype=tf.float32, trainable=self.trainable)\r\n\r\n    def call(self, indices):\r\n        return tf.gather(params=self.mat, indices=indices)\r\n\r\nclass TestModel(tf.keras.Model):\r\n    def __init__(self, rows, cols):\r\n        super().__init__()\r\n        self.ln = LN(rows=rows, cols=cols)\r\n\r\n    @tf.function\r\n    def call(self, x):\r\n        with tf.device('/cpu:0'):\r\n            x = self.ln(x)\r\n        with tf.device('/gpu:0'):\r\n            x = tf.math.reduce_sum(x, axis=1)\r\n        return x\r\n\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--size', type=int, default=100, help='Table size in GiB')\r\n    parser.add_argument('--load_cpu', type=bool, default=False, help='Load variables to cpu')\r\n    args = parser.parse_args()\r\n    ncols = 128\r\n    nrows = args.size * 2**30 // 512\r\n    path = '/tmp/saved_model_test'\r\n    model = TestModel(rows=nrows, cols=ncols)\r\n    indices = tf.zeros(shape=(65536, 1), dtype=tf.int32)\r\n    outputs = model(indices)\r\n    print(outputs)\r\n    model.summary()\r\n    print('Saving the model')\r\n    tf.saved_model.save(\r\n            model, path,\r\n            options = tf.saved_model.SaveOptions(\r\n                  experimental_variable_policy=tf.saved_model.experimental.VariablePolicy.SAVE_VARIABLE_DEVICES))\r\n    print('Saved successfully')\r\n    if args.load_cpu:\r\n        with tf.device('/cpu:0'):\r\n            loaded = tf.saved_model.load(path) # Good\r\n    else:\r\n        loaded = tf.saved_model.load(path) # OOM\r\n    print('Load successfully')\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```", "comments": ["@wenscarl \r\nPlease feel free to create a pr or let us know where the change has to be made.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Hi @Saduf2019 . PR-[54319 ](https://github.com/tensorflow/tensorflow/pull/54319) is created to address this issue.  Please review and reopen it at your convenience. ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 53742, "title": "TensorFlow cublas code is not honoring NVIDIA_TF32_OVERRIDE=0 ", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6\r\n- Python version: 3.8.12\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: A100\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nIrrespective of NVIDIA_TF32_OVERRIDE=0/1 setting, the following is observed coming from cublas. \r\n\r\n> I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n\r\n\r\nBy using CUBLASLT_LOG_LEVEL=5 , only see the following kernels with both NVIDIA_TF32_OVERRIDE=0/1\r\n\r\n[cublasLtCreate]\r\n[cublasLtCtxInit]\r\n[cublasLtSSSMatmulAlgoGetHeuristic]\r\n[cublasLtSSSMatmul]\r\n\r\nBoth seem to use the following\r\n\r\n2022-01-12 03:29:14][cublasLt][1420][Api][cublasLtSSSMatmulAlgoGetHeuristic] Adesc=[type=R_32F rows=200 cols=200 ld=200] Bdesc=[type=R_32F rows=200 cols=128 ld=200] Cdesc=[type=R_32F rows=200 cols=128 ld=200] preference=[maxWavesCount=0.0 gaussianModeMask=3M_MODE_DISALLOWED pointerModeMask=0 maxWorkspaceSizeinBytes=4194304 minBytesAlignmentA=16 minBytesAlignmentB=16 minBytesAlignmentC=16 minBytesAlignmentD=16 smCountTarget=108] computeDesc=[computeType=COMPUTE_32F_FAST_TF32 scaleType=R_32F]\r\n\r\n**Describe the expected behavior**\r\n\r\nShould use FP32 instead of TF32.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nAny kernel that uses cublas gemm. \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nIrrespective of NVIDIA_TF32_OVERRIDE=0/1 setting, the following is observed coming from cublas. \r\n\r\n> I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n\r\n", "comments": ["@whatdhack ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@tilakrayal ,  it is very easy to create that -  just as I mentioned do a matmul . ", "@whatdhack ,\r\nWithout the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a minimal code snippet  you are using", "@tilakrayal , an example follows.\r\n\r\n```\r\n> NVIDIA_TF32_OVERRIDE=0 python\r\n>>>  import tensorflow as tf\r\n>>> a = tf.constant(0.2, shape=[1000, 1000])\r\n>>> b = tf.constant(0.2, shape=[1000, 1000])\r\n>>> c = tf.matmul(a, b)\r\n2022-01-26 00:29:04.187721: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n>>>\r\n```\r\n", "@whatdhack ,\r\nWhile trying to execute the provided code i haven't faced any issue.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/c428751ec19835d5844d68708fd6835b/untitled199.ipynb) and let us know if anything is missing.Thanks", "Still see it  in TF 2.6  Do not have TF 2.7 yet.  Will try again after upgrade. \r\n\r\n```\r\n>>> b = tf.constant(0.2, shape=[1000, 1000])\r\n>>> c = tf.matmul(a, b)\r\n2022-01-27 18:40:10.497687: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n>>> tf.__version__\r\n'2.6.0'\r\n>>> c = tf.matmul(a, b)\r\n>>>\r\n```\r\n\r\nHowever, that part of the code does not look like have changed in TF source ( https://github.com/tensorflow/tensorflow/blob/c242bc0901c3fa5966a16e41a8d3da33e4e4b690/tensorflow/stream_executor/cuda/cuda_blas.cc#L1774  ).    \r\nAlso, the log is  displayed only once, the first time cublas is called.  Do not know how colab works - could it have  been already been run  previously ? \r\n\r\n\r\nThis question came up because I am trying to run in only FP32, do not want pollution from TF32  inadvertently.  Is there another way to ascertain that ? Changing model code is not an option however. \r\n", "Tried with TF 2.8 .  Same output.  So, looks like either it is not possible to disable TF32 or that message is erroneous.  If the former, which I think it is , then performance results for fp32 will  be misleading, \r\n```\r\nPython 3.9.7 (default, Sep 16 2021, 13:09:58)\r\n[GCC 7.5.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> NVIDIA_TF32_OVERRIDE=0\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.8.0'\r\n>>> a = tf.constant(0.2, shape=[1000, 1000])\r\n2022-02-03 00:14:02.629882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-03 00:14:02.643073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-03 00:14:02.644380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-03 00:14:02.645940: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-02-03 00:14:02.649275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-03 00:14:02.650593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-03 00:14:02.651803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-03 00:14:03.016682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-03 00:14:03.017961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-03 00:14:03.019177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-03 00:14:03.020402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:43:00.0, compute capability: 8.0\r\n>>> b = tf.constant(0.2, shape=[1000, 1000])\r\n>>> c = tf.matmul(a, b)\r\n2022-02-03 00:14:19.247907: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n>>>\r\n```", "Note that the API to enable or disable TF32 support in Tensorflow is: https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_tensor_float_32_execution\r\n\r\nUsing NVIDIA_TF32_OVERRIDE will not work because TF explicitly tells cublas to use TF32. Can you try using the enable_tensor_float_32_execution API?\r\n\r\n", "@jurahul , thanks for the pointer. "]}, {"number": 53741, "title": "Add float16 support for tf.sparse.softmax", "body": "This PR tries to address the issue raised in #53657 where float16 was not supported for tf.sparse.softmax, while the counterpart of tf.nn.softmax has the float16 support.\r\nThis PR adds float16 support.\r\n\r\nThis PR fixes #53657.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 53726, "title": "Func with custom gradient using `tf.numpy_function`/`tf.py_function` incompatible with `tf.vectorized_map`", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): installed via `pip`\r\n- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using `tf.vectorized_map` on a function that provides a custom gradient and the function uses either `tf.numpy_function` or `tf.py_function`, then unexpected behaviour arises:\r\n\r\n* `tf.numpy_function`: the gradient is a vector of zeros;\r\n* `tf.py_function`: `UnknownError:  KeyError: b'pyfunc_12'` error arises.\r\n\r\n**Describe the expected behavior**\r\nThe gradient is computed without issues.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n@tf.function\r\n@tf.custom_gradient\r\ndef sin(x):\r\n\r\n    # Note: change the following line to use tf.numpy_function to\r\n    # reproduce the other issue mentioned\r\n    res = tf.py_function(func=np.sin, inp=(x,), Tout=tf.float64)\r\n\r\n    def grad_fn(dy):\r\n        j = tf.cos(x)\r\n        return dy * j\r\n\r\n    return res, grad_fn\r\n\r\ninputs = tf.Variable(tf.ones((10,), dtype=tf.float64))\r\n\r\nwith tf.GradientTape() as tape:\r\n    loss = tf.reduce_sum([sin(x) for x in inputs])\r\n\r\nprint(\"loss:\", loss)\r\nprint(\"gradient:\", tape.gradient(loss, inputs))\r\n\r\nwith tf.GradientTape() as tape:\r\n    loss = tf.reduce_sum(tf.vectorized_map(sin, inputs))\r\n\r\nprint(\"Vectorized loss:\", loss)\r\nprint(\"Vectorized gradient:\", tape.gradient(loss, inputs))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nOutput when using \r\n```\r\nloss: tf.Tensor(8.414709848078965, shape=(), dtype=float64)\r\ngradient: tf.Tensor(\r\n[0.54030231 0.54030231 0.54030231 0.54030231 0.54030231 0.54030231\r\n 0.54030231 0.54030231 0.54030231 0.54030231], shape=(10,), dtype=float64)\r\nVectorized loss: tf.Tensor(8.414709848078965, shape=(), dtype=float64)\r\nVectorized gradient: tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(10,), dtype=float64)\r\n```\r\n```\r\nloss: tf.Tensor(8.414709848078965, shape=(), dtype=float64)\r\ngradient: tf.Tensor(\r\n[0.54030231 0.54030231 0.54030231 0.54030231 0.54030231 0.54030231\r\n 0.54030231 0.54030231 0.54030231 0.54030231], shape=(10,), dtype=float64)\r\nVectorized loss: tf.Tensor(8.414709848078965, shape=(), dtype=float64)\r\n```\r\nThen getting `UnknownError:  KeyError: b'pyfunc_12'`.\r\n\r\nFor both, a `WARNING:tensorflow:Using a while_loop for converting EagerPyFunc` is also emitted.", "comments": ["Hi @Saduf2019 ! Could you please look at this issue ? Attaching Gist in [2.6](https://colab.sandbox.google.com/gist/mohantym/5c4a3315aeb2b08ae60442e03e1f914b/github_53726.ipynb#scrollTo=i3S2CnqfArGN) and [2.7](https://colab.sandbox.google.com/gist/mohantym/93b6d0e59806cca6bd701175c9512da0/github_53726.ipynb#scrollTo=z0Of3Js-Am9U) for reference. Thanks!", "Hi @antalszava , thanks for reporting this! This does look like a bug.\r\n\r\nThe fix might take a while. Would removing e.g. `tf.py_function` unblock you for now?", "Hi @JW1992, thank you! Unfortunately not, using `tf.py_function` in the example code would be required for the use case in mind."]}, {"number": 53725, "title": "Inference time jumps for varying batch size", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes [Link below]\r\n- OS Platform and Distribution (Ubuntu 18.04):\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: Tested on 2.6, 2.4\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: Tested on 1) CUDA 10.1+cudnn 7, 2) CUDA 11.1 + cudnn 8\r\n- GPU model and memory: Tested on 1) Tesla P100, 16GB , 2) Quadro P4000, 8GB\r\n\r\n**Current behavior**\r\n\r\nWhen running classifier inference on GPU, if an input batch size is seen for the first time, the inference time is more than expected. In subsequent runs for the same input batch size, the inference time reduces. When the inference time jump is observed, the load shifts to CPU (GPU usage drops in nvidia-smi) while on subsequent inferences the load is on GPU.\r\n\r\nExample 1: \r\n\r\n![figure1_tf_expt_1](https://user-images.githubusercontent.com/3059405/148972390-3c2f86bc-bc50-44f7-ab77-a2ce78012ed2.png)\r\n\r\nFor a random batch size, the inference time on `run 2` reduces because it is seen in `run 1`. In `run 3`, all the batch sizes are seen in `run 2` and their inference time reduces.\r\n\r\n\r\nExample 2: \r\n\r\n![figure2_tf_expt_2](https://user-images.githubusercontent.com/3059405/148972415-0c4a74a5-9084-46c7-8b27-b757a257869b.png)\r\n\r\nFor 10 random batch sizes, the inference time in `run 2` reduces because all these batches are seen in `run 1`. In `run 3`, all the batch sizes are seen in `run 2` and their inference time reduces.\r\n\r\n\r\n**Why is this relevant?**\r\n\r\nSuppose we have a video sequence with varying number of objects every few frames (i.e., the batch size=number of objects varies every few frames). Every time there are total number of objects in a frame that have not been seen before, there is a jump in inference time. For example, if there are 10 objects in the first 30 frames and then there are 8 objects in the next frame, an inference time jump is observed for this frame. In a product with real time expectations, this can have system level implementations on other modules.\r\n\r\n\r\n**Expected behavior**\r\n\r\nInference time jumps should not be observed with varying batch sizes as it can have system level implications. This behavior is not observed with PyTorch.\r\nAn easy solution is to run classifier inference with all batch sizes on dummy images during initialization. For example, we can run inference for batch sizes from 1 to 64 if maximum expected objects are 64. However, that's more of a hack. I am interested in understanding the reason behind this issue. Looks like it has something to do with memory allocation - but why is it dependent on batch size? Is there a better Tensorflow configuration or inference function or memory allocation that can help resolve it?\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nJupyter notebook in Colab:\r\nhttps://colab.research.google.com/drive/1fHy3HcrYBskMLy-nNn12bbwBI-QIxg1c?usp=sharing\r\n\r\nThis notebook uses model() for inference. Using model.predict(), gives similar results.\r\n\r\n**To enable GPU in Colab, select GPU**\r\n\r\nRuntime -> Change Runtime Type -> Hardware Accelerator -> Select GPU from drop down \r\n", "comments": ["@parneetk I tried to replicate this issue on colab using TF v2.7.0 , tf-nightly(2.9.0-dev20220111) and faced different error in \r\n tf-nightly, please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/13f317728b0d420611d8b4d8531b5201/expt_variable_batch_size.ipynb#scrollTo=trURuyQLMyNO) for reference. Please confirm the same.Thanks!", "@sushreebarsa Thank you for looking into this! \r\nI did not install tf-nightly and used default TF of colab environment (v2.7.0). Not sure why colab shows an error for you and not me. I verified again and it seems to run without any errors."]}, {"number": 53721, "title": "Can not convert a tensorflow model to tflite", "body": ">>> tf.__version__\r\n'2.9.0-dev20220110'\r\n\r\nI have a model that I have converted to onnx and tensorflow from original pytorch. Both conversion works OK. Now I want to convert this model to tflite. But the conversion script is giving me error. \r\n\r\n'tf.BiasAdd' op requires channel dimension and feature dimension to match; found 1 and 1024, respectively\r\n\r\nThe model has a lstm layer\r\n\r\nThe model is attached and conversion script used is below:\r\n\r\n[pretrained.zip](https://github.com/tensorflow/tensorflow/files/7845281/pretrained.zip)\r\n[model.pb.zip](https://github.com/tensorflow/tensorflow/files/7845282/model.pb.zip)\r\n\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/home/model.pb')\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\n\r\n\r\n", "comments": ["@kafan1986 ,\r\nPlease take a look at link [1](https://www.tensorflow.org/lite/convert) and [2](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter) for more information regarding tflite converter.It helps.Thanks!", "> @kafan1986 , Please take a look at link [1](https://www.tensorflow.org/lite/convert) and [2](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter) for more information regarding tflite converter.It helps.Thanks!\r\n\r\nTried this does not work.", "@sachinprasadhs ,\r\nI was able to reproduce the issue in tf v2.5, v2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/bc89197c187ca25ea1b0f059399e6ec8/53721.ipynb)."]}, {"number": 53714, "title": "Add transpose => reshape simplifier", "body": "* Simplify transpose to reshape when they are equivalent", "comments": ["@qingyunqu Can you please resolve conflicts? Thanks!", "> @qingyunqu Can you please resolve conflicts? Thanks!\r\n\r\nDone."]}, {"number": 53713, "title": "TFLite_Detection_PostProcess produces invalid bounding box coordinates", "body": "**System information**\r\n- Have I written custom code: yes\r\n```python\r\nimport tensorflow as tf                                                                                                                                                           \r\nimport numpy as np                                                                                                                                                                \r\nimport cv2                                                                                                                                                                        \r\n                                                                                                                                                                                  \r\ninterpreter = tf.lite.Interpreter(model_path=\"ssd_mobilenet_v1_1_default_1.tflite\")                                                                                               \r\ninterpreter.allocate_tensors()                                                                                                                                                    \r\n                                                                                                                                                                                  \r\ninput_details = interpreter.get_input_details()                                                                                                                                   \r\noutput_details = interpreter.get_output_details()                                                                                                                                 \r\nprint(input_details)                                                                                                                                                              \r\nprint(output_details)                                                                                                                                                             \r\n                                                                                                                                                                                  \r\ninput_shape = input_details[0]['shape']                                                                                                                                           \r\nim = cv2.imread(\"buggy_image_2.jpg\")  # 1920x1080                                                                                                                                 \r\n                                                                                                                                                                                  \r\n# RGB conversion                                                                                                                                                                  \r\nim_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)                                                                                                                                      \r\n                                                                                                                                                                                  \r\n# Crop center                                                                                                                                                                     \r\ncrop_im = im_rgb[0:1080, 420:1500]  # 1080x1080                                                                                                                                   \r\n                                                                                                                                                                                  \r\n# Resize                                                                                                                                                                          \r\nim_rgb = cv2.resize(im_rgb, (input_shape[1], input_shape[2]))  # 300x300                                                                                                          \r\n                                                                                                                                                                                  \r\n# Set dimension                                                                                                                                                                   \r\ninput_data = np.expand_dims(im_rgb, axis=0)                                                                                                                                       \r\nprint(input_data.shape)                                                                                                                                                           \r\ninterpreter.set_tensor(input_details[0]['index'], input_data)                                                                                                                     \r\n                                                                                                                                                                                  \r\n# Inference                                                                                                                                                                       \r\ninterpreter.invoke()                                                                                                                                                              \r\n                                                                                                                                                                                  \r\ndetection_boxes = interpreter.get_tensor(output_details[0]['index'])                                                                                                              \r\ndetection_classes = interpreter.get_tensor(output_details[1]['index'])                                                                                                            \r\ndetection_scores = interpreter.get_tensor(output_details[2]['index'])                                                                                                             \r\nnum_boxes = interpreter.get_tensor(output_details[3]['index'])                                                                                                                    \r\n                                                                                                                                                                                  \r\nprint(\"==detection_boxes==\")                                                                                                                                                      \r\nprint(detection_boxes)                                                                                                                                                            \r\nprint()                                                                                                                                                                           \r\nprint(\"==detection_classes==\")                                                                                                                                                    \r\nprint(detection_classes)                                                                                                                                                          \r\nprint()                                                                                                                                                                           \r\nprint(\"==detection_scores==\")                                                                                                                                                     \r\nprint(detection_scores)                                                                                                                                                           \r\nprint()                                                                                                                                                                           \r\nprint(\"==num_boxes==\")                                                                                                                                                            \r\nprint(num_boxes)                                                                                                                                                                  \r\nprint()\r\n```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n`Mac Monterey 12.0.1`\r\n\r\n- TensorFlow version (use command below):\r\n```bash\r\npython3 -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\nunknown 2.7.0\r\n```\r\n- Python version: \r\n`3.9.9`\r\n\r\n\r\n**Describe the current behavior**\r\nModel: I got the tflite model from https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/default/1\r\nImage: https://github.com/heeh/tflite-test/blob/main/buggy_image_2.jpg\r\nBounding box produces negative coordinates as well as coordinates bigger than one.\r\n```\r\n  [ 0.8324257   0.52732474  0.9950099   1.0011996 ]\r\n  [ 0.83758724  0.5655538   1.0012565   0.8586073 ]\r\n  [ 0.8129529   0.55081666  1.0012082   0.66460264]\r\n  [-0.00595534  0.5822872   0.6086124   0.9962516 ]\r\n  [ 0.06498116  0.5597983   0.8853406   1.0037088 ]\r\n  [ 0.870738    0.7267949   1.0040077   0.99758047]\r\n```\r\n**Describe the expected behavior**\r\nThe bounding box should produce the output within `0.0` and `1.0`\r\nAccording to https://www.tensorflow.org/lite/examples/object_detection/overview\r\n```Multidimensional array of [N][4] floating point values between 0 and 1, the inner arrays representing bounding boxes in the form [top, left, bottom, right]```\r\n![image](https://user-images.githubusercontent.com/5158322/148822427-04e924c1-71e4-40d0-9912-4a4474972db7.png)\r\n\r\n\r\n\r\n- Briefly describe your candidate solution(if contributing):\r\nThe `TFLite_Detection_PostProcess` operator should enforce the output between zero and one.\r\nI believe that this has something to do with the following code.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1Ouyz_BUGSvvKG2Ib_fVMye6IjAmWuuRf?usp=sharing\r\n\r\n\r\n**Other info / logs** \r\n```\r\n[{'name': 'normalized_input_image_tensor', 'index': 175, 'shape': array([  1, 300, 300,   3], dtype=int32), 'shape_signature': array([  1, 300, 300,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0078125, 128), 'quantization_parameters': {'scales': array([0.0078125], dtype=float32), 'zero_points': array([128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n[{'name': 'TFLite_Detection_PostProcess', 'index': 167, 'shape': array([ 1, 10,  4], dtype=int32), 'shape_signature': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 168, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 169, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 170, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n(1, 300, 300, 3)\r\n==detection_boxes==\r\n[[[ 0.39625263  0.47477102  0.9839653   0.6032783 ]\r\n  [ 0.44166473  0.74635595  0.95957065  0.9920321 ]\r\n  [ 0.83758724  0.5655538   1.0012565   0.8586073 ]\r\n  [ 0.8129529   0.55081666  1.0012082   0.66460264]\r\n  [ 0.8254188   0.56289214  0.9910882   0.63345426]\r\n  [ 0.8324257   0.52732474  0.9950099   1.0011996 ]\r\n  [ 0.3868151   0.4032717   0.99340284  0.58236426]\r\n  [-0.00595534  0.5822872   0.6086124   0.9962516 ]\r\n  [ 0.06498116  0.5597983   0.8853406   1.0037088 ]\r\n  [ 0.870738    0.7267949   1.0040077   0.99758047]]]\r\n\r\n==detection_classes==\r\n[[89. 61. 61. 61. 61. 61. 89. 61. 61. 61.]]\r\n\r\n==detection_scores==\r\n[[0.5234375  0.4375     0.4140625  0.3671875  0.35546875 0.34375\r\n  0.33203125 0.30078125 0.30078125 0.28125   ]]\r\n\r\n==num_boxes==\r\n[10.]\r\n```\r\n", "comments": ["Hi @sachinprasadhs ! Could you please look at this issue? Attaching gist in [2.6](https://colab.sandbox.google.com/gist/mohantym/eb0acec5654db3ad1745712a18d8eb08/untitled0.ipynb#scrollTo=RGX60XGhyIhD), [2.7](https://colab.sandbox.google.com/gist/mohantym/0d45f2b20e7ae54a8b1e4e70bc0c185c/untitled0.ipynb#scrollTo=3m6myyEefo4W) and [nightly](https://colab.sandbox.google.com/gist/mohantym/452677eda3021557b1fc02af2c99aa8d/untitled0.ipynb#scrollTo=RGX60XGhyIhD) for reference.", "@mohantym @sachinprasadhs \r\nI am sure you are very busy, but I haven't heard back from you for a while. Any updates on this issue?", "Hello @sachinprasadhs \r\nWhat is the status of this issue? Let me know if there is anything that I can help you with.\r\ncc @mohantym ", "Hi @sachinprasadhs \r\nFor TF models, the bbox are clipped to [0,1] in the [postprocess operation](https://github.com/tensorflow/models/blob/de44470ffa22d588f3fc4ce6c1f4a789ca636eab/research/object_detection/meta_architectures/ssd_meta_arch.py#L486) where a clip_window is calculated for every image based on its true shape, if true shape is not specified the default is [0,0,1,1]. This clip_window is passed into nms op where bbox are clipped. \r\n\r\nComparing the detections:\r\n**TF model: detected bbox -> [0.  , 0.04112084, 0.44440162, 0.39392054]**\r\n![image](https://user-images.githubusercontent.com/25738889/150572693-3eaa331e-4cfd-4d25-b343-d090c94855d7.png)\r\n\r\n**Equivalent TFlite model: detected bbox -> [-0.00404607,  0.04112093,  0.4444015 ,  0.39392054]**\r\n![image](https://user-images.githubusercontent.com/25738889/150573003-602bf5e8-c3b8-443e-bef0-9fae23887403.png)\r\n\r\nIn the [detection_preprocess operation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc) of tflite, I could not find this clipping operation happening. Could this be a possible reason for the negative bbox?\r\n\r\ncc @mohantym ", "@sachinprasadhs @mohantym \r\nIt has been 23 days since I reported this bug. Could you at least share the status?", "@heeh , apologies for the delayed response, I could not find much insight on this issue, @nutsiepully , could you please look at this issue. Thanks!", "@nutsiepully @sachinprasadhs @mohantym \r\nCould you update the status of this issue?", "Hi @nutsiepully \r\nIt has been 26 days since this issue was assigned to you and I understand that this issue might not be your priority.\r\nHowever, I need to get your response for moving forward. \r\nI don't mind even if this issue is not going to be fixed anytime soon.", "Hi @heeh,\r\n\r\nSorry for the trouble. Unfortunately, we haven't had a chance to look into this. Given this is a minor issue affecting some example use cases, it's unlikely we'll be able to look into it soon.\r\n"]}, {"number": 53708, "title": "Select TF Ops build output a large library file.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.7\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 4.2.2\r\n- GCC/Compiler version (if compiling from source): 10.1.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nTry to add Select TF Ops to the libtensorflowlite.so and it outputs a large binary file about 180MB in linux gcc and 90+ MB in cross-compilation for android. Is there a way to reduce the size of library ?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI follow the guide of [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md](url) to get a BUILD file and a shared library with Select TF Ops. But it seems the size of the library is almost the same as \r\n\r\n> Add the TensorFlow ops delegate library dependency to the build dependencies: tensorflow/lite/delegates/flex:delegate.\r\nIt's to large for mobile deploy and is there any ways to reduce it?\r\n\r\nThe BUILD file is:\r\n```\r\nload(       \r\n    \"//tensorflow/lite:build_def.bzl\",\r\n    \"tflite_custom_cc_library\",\r\n    \"tflite_cc_shared_object\",\r\n)        \r\nload(       \r\n    \"@org_tensorflow//tensorflow/lite/delegates/flex:build_def.bzl\",\r\n        \"tflite_flex_shared_library\"\r\n)        \r\n         \r\ntflite_flex_shared_library(\r\n  name = \"tensorflowlite_flex\",\r\n  models = [\r\n      \":model.tflite\",\r\n  ],        \r\n)        \r\n         \r\ntflite_custom_cc_library(\r\n    name = \"selectively_built_cc_lib\",\r\n    models = [ \r\n        \":model.tflite\",\r\n    ],   \r\n)        \r\n         \r\n# Shared lib target for convenience, pulls in the core runtime and builtin ops.\r\n# Note: This target is not yet finalized, and the exact set of exported (C/C++)\r\n# APIs is subject to change. The output library name is platform dependent:\r\n#   - Linux/Android: `libtensorflowlite.so`\r\n#   - Mac: `libtensorflowlite.dylib`\r\n#   - Windows: `tensorflowlite.dll`\r\ntflite_cc_shared_object(\r\n    name = \"tensorflowlite\",\r\n    # Until we have more granular symbol export for the C++ API on Windows,\r\n    # export all symbols.\r\n    features = [\"windows_export_all_symbols\"],\r\n    linkopts = select({\r\n        \"//tensorflow:macos\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow/lite:tflite_exported_symbols.lds)\",\r\n        ],\r\n        \"//tensorflow:windows\": [], \r\n        \"//conditions:default\": [\r\n            \"-Wl,-z,defs\",\r\n            \"-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)\",\r\n        ],\r\n    }),  \r\n    per_os_targets = True,\r\n    deps = [\r\n        \":selectively_built_cc_lib\",\r\n        \"//tensorflow/lite:tflite_exported_symbols.lds\",                                                                                          \r\n        \"//tensorflow/lite:tflite_version_script.lds\",\r\n    ],   \r\n)\r\n```\r\nThe build command:\r\n`bazel build -c opt --cxxopt=--std=c++14 --config=monolithic --host_crosstool_top=@bazel_tools//tools/cpp:toolchain //tensorflow/lite/tmp:libtensorflowlite_flex.so`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["@SefaZeng ,\r\nCan you please provide the error log to debug the issue.It helps to analyse the issue.Thanks!", "> @SefaZeng , Can you please provide the error log to debug the issue.It helps to analyse the issue.Thanks!\r\n\r\nHi @tilakrayal , there is no such error log. I get a tflite shared library with select tf ops. And my model can load by this library. But I want to know is there any ways to reduce the library size as it's to large for mobile deployment. I followed the \"reduce the size\" guide but it did not reduce library size."]}, {"number": 53702, "title": "Native support for StridedSlice in 6D and Transpose in 7D", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): 804ef7223ef08fd14c274b4a4044cc4aeee68863\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2022-01-08 12:47:32.812069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/openvino_2021/data_processing/dl_streamer/lib:/opt/intel/openvino_2021/data_processing/gstreamer/lib:/opt/intel/openvino_2021/opencv/lib:/opt/intel/openvino_2021/deployment_tools/ngraph/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/tbb/lib::/opt/intel/openvino_2021/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/omp/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/lib/intel64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2022-01-08 12:47:32.812092: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\r\n2022-01-08 12:47:32.812107: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\r\n2022-01-08 12:47:32.812367: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\r\n2022-01-08 12:47:34.806218: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2022-01-08 12:47:34.806383: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\r\n2022-01-08 12:47:34.860258: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: function_optimizer did nothing. time = 0.008ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\n\r\n2022-01-08 12:47:37.790651: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\r\n2022-01-08 12:47:37.790685: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\r\n2022-01-08 12:47:38.104138: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1892] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\r\nFlex ops: FlexStridedSlice, FlexTranspose\r\nDetails:\r\n\ttf.StridedSlice(tensor<1x120x160x3x1x32xf32>, tensor<6xi32>, tensor<6xi32>, tensor<6xi32>) -> (tensor<1x120x160x1x32xf32>) : {begin_mask = 55 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 55 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 8 : i64}\r\n\ttf.Transpose(tensor<1x30x4x40x4x1x4xf32>, tensor<7xi32>) -> (tensor<1x30x40x1x4x4x4xf32>) : {device = \"\"}\r\nSee instructions: https://www.tensorflow.org/lite/guide/ops_select\r\n2022-01-08 12:47:38.104385: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 53.877 G  ops, equivalently 26.938 G  MACs\r\n\r\nEstimated count of arithmetic ops: 53.877 G  ops, equivalently 26.938 G  MACs\r\nWARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nTFLite Native support for `StridedSlice` for 6D, and `Transpose` for 7D would be very beneficial. It would be nice if we could avoid Flex operations if possible.\r\n- **`saved_model`** and converted **`tflite`** files \r\n[saved_model_and_float32tflite.zip](https://github.com/tensorflow/tensorflow/files/7833211/saved_model_and_float32tflite.zip)\r\n\r\n- Conversion Script\r\n```python\r\nimport tensorflow as tf\r\n\r\ninput_shapes = [[1,120,160,6]]\r\n\r\nmodel = tf.saved_model.load(\r\n    'flyingthings_finalpass_xl/saved_model_120x160'\r\n)\r\nconcrete_func = model.signatures[\r\n    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY\r\n]\r\nconcrete_func_input_tensors = [\r\n    tensor for tensor in concrete_func.inputs \\\r\n        if tensor.dtype != tf.resource and not 'unknown' in tensor.name\r\n]\r\nfor conc_input, def_input in zip(concrete_func_input_tensors, input_shapes):\r\n    conc_input.set_shape(def_input)\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,\r\n    tf.lite.OpsSet.SELECT_TF_OPS,\r\n]\r\ntflite_model = converter.convert()\r\nwith open('model_float32.tflite', 'wb') as w:\r\n    w.write(tflite_model)\r\n```\r\n\r\n**Any other info / logs**\r\n- Model Citation Repository - HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching\r\nhttps://github.com/google-research/google-research/tree/master/hitnet\r\n![148323208-7db28584-ce78-4398-94fa-4ce93c9f8d4d](https://user-images.githubusercontent.com/33194443/148644951-ed032ea7-f4d4-43a1-bdfb-bf368d12cb4e.gif)\r\n\r\n- FlexStridedSlice (6D)\r\n![Screenshot 2022-01-08 21:52:11](https://user-images.githubusercontent.com/33194443/148644890-d6400a0a-e8b7-423d-bfc8-e78e56135647.png)\r\n\r\n- FlexTranspose (7D)\r\n![Screenshot 2022-01-08 21:52:40](https://user-images.githubusercontent.com/33194443/148644896-b1481efb-4cd0-470a-b277-b523cfce4798.png)", "comments": ["z\u8fd9\u662f\u4e2a\u4ec0\u4e48\u73a9\u610f\uff0c\u6211\u53ea\u662f\u8bc4\u8bba\u4e00\u4e0b\uff0c\u91cd\u5408\u8089\u4e1d\u5c31\u662f\u5c31", "@PINTO0309 ,\r\nPlease take a look at this SO [link](https://stackoverflow.com/questions/41890549/tensorflow-cannot-open-libcuda-so-1) and [issue](https://github.com/tensorflow/tensorflow/issues/4078) with the similar error.It helps.Thanks!", "@tilakrayal Thank you.\r\n\r\nCUDA warnings have nothing to do with this issue. I have transcribed all the logs without omitting them, but you can ignore the warnings. Also, this is not a bug. The .tflite has been generated successfully.\r\n\r\nThis issue is an OP request for TensorFlow Lite. A **`feature request`** has been made to implement **`FlexStridedSlice`** and **`FlexTranspose`** to replace the standard operators. `StridedSlice`, `Transpose`\r\n\r\nI used the following issue template to create the issue.\r\nhttps://github.com/tensorflow/tensorflow/issues/new/choose\r\n![Screenshot 2022-01-10 19:49:45](https://user-images.githubusercontent.com/33194443/148753926-2a6940d2-4da4-421d-8f46-4c350009a307.png)\r\n\r\n"]}, {"number": 53697, "title": "[TF:TRT] Tensorflow Docker 2.7.0-gpu has incompatible TRT version 8.0.0", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 20.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.7.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: NVIDIA T4, 16GB\r\n\r\n**Steps to reproduce the issue**\r\nSetup tensorflow/tensorflow:2.7.0-gpu container\r\n```\r\n$ sudo docker run -it --name tf270 --gpus all tensorflow/tensorflow:2.7.0-gpu\r\n```\r\nTest TF/TRT\r\n```\r\n$ apt list --installed | grep infer\r\n\r\nlibnvinfer-plugin8/unknown,now 8.0.0-1+cuda11.0 amd64 [installed,upgradable to: 8.2.0-1+cuda11.4]\r\nlibnvinfer8/unknown,now 8.0.0-1+cuda11.0 amd64 [installed,upgradable to: 8.2.0-1+cuda11.4]\r\n\r\n$ python3\r\nimport tensorflow as tf\r\nimport tensorflow.compiler as tf_cc\r\ntf_cc.tf2tensorrt._pywrap_py_utils.get_linked_tensorrt_version()\r\n\r\n(7, 2, 2)\r\n```\r\nAs you can see the installed TRT version is 8.0.0 but Tensorflow was built and linked with TRT 7.2.2.\r\n\r\n@DEKHTIARJonathan  @bixia1 @sanjoy\r\n\r\n", "comments": ["Hi @Saduf2019 ! Could you look at this issue?", "@apivovarov \r\nThis is already tracked in your issue [link](https://github.com/tensorflow/tensorflow/issues/53529#issuecomment-1006301417).\r\nCan we have this in one issue so we do not duplicate it.", "Issue [53529](https://github.com/tensorflow/tensorflow/issues/53529) says \"TRT Converter not working\". My (this) issue is more general - TF/TRT is not working in TF 2.7.0 Docker image in General. \r\n\r\nQuestion. Where is Docker file for TF 2.7.0 Docker image? I can open PR to fix it.", "@apivovarov \r\nPlease feel free to open a PR for the fix.", "@Saduf2019 Do you know which Dockerfile is used to build TF 2.7.0 Docker image?"]}, {"number": 53691, "title": "TensorFlow Lite Evaluation Tools CMake Compilation", "body": "This PR introduces into CMake configuration of TF Lite the option to automatically (cross-)compile TF Lite evaluation tools (specifically [ImageNet Image Classification](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_eval_imagenet_image_classification), [COCO Object Detection](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection) and  [Indifference Diff](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff)).\r\n\r\nSimilarly to [TF Lite kernel tests cross-compilation](https://github.com/tensorflow/tensorflow/pull/52110) requiring **host** `flatc` compiler, cross-compilation of evaluation tools requires **host** `protoc` compiler. Therefore, the CMake configuration for obtaining `flatc` has been extended and abstracted into the **native_tools** CMake module. \r\n\r\nFor details please see the updated `build_cmake.md` file.", "comments": ["@terryheo Could you help to review CMake related changes?", "@terryheo thanks for the review, I applied the necessary fixes and squashed the commits.", "@terryheo one outstanding conflict has just been resolved - any other update required? Thanks!", "@terryheo  Can you please review this PR ? Thank you!"]}, {"number": 53680, "title": "error executing command crosstool_wrapper_driver_is_not_gcc", "body": "**System information**\r\n- OS Platform and Distribution: Slackware64 Linux (current), kernel 5.15.12\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 11.2.0\r\n- CUDA/cuDNN version: 11.5.50, 8.3.1.22\r\n- GPU model and memory: Nvidia Quadro RTX 4000, 8GB\r\n\r\n**the problem**\r\n\r\n     # bazel build --workspace_status_command=\"bash native_client/bazel_workspace_status_cmd.sh\" --config=monolithic -c opt --copt=-O3 --copt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --copt=-fvisibility=hidden --define=no_tensorflow_py_deps=true --define=no_nccl_support=true //native_client:libdeepspeech.so --config=cuda\r\n\r\ngives \r\n\r\n```\r\nERROR: ~/DeepSpeech/tensorflow/tensorflow/stream_executor/cuda/BUILD:469:11: C++ compilation of rule '//tensorflow/stream_executor/cuda:cupti_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/_objs/cupti_stub/cupti_stub.pic.d ... (remaining 141 argument(s) skipped)\r\nIn file included from ./tensorflow/core/platform/status.h:28,\r\n                 from ./tensorflow/core/platform/errors.h:22,\r\n                 from ./tensorflow/core/platform/env.h:27,\r\n                 from ./tensorflow/stream_executor/lib/env.h:20,\r\n                 from tensorflow/stream_executor/cuda/cupti_stub.cc:19:\r\nbazel-out/k8-opt/bin/tensorflow/core/protobuf/error_codes.pb.h:17:2: error: #error This file was generated by an older version of protoc which is\r\n   17 | #error This file was generated by an older version of protoc which is\r\n      |  ^~~~~\r\nbazel-out/k8-opt/bin/tensorflow/core/protobuf/error_codes.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please\r\n   18 | #error incompatible with your Protocol Buffer headers. Please\r\n      |  ^~~~~\r\nbazel-out/k8-opt/bin/tensorflow/core/protobuf/error_codes.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.\r\n   19 | #error regenerate this file with a newer version of protoc.\r\n      |  ^~~~~\r\nIn file included from ./tensorflow/core/platform/status.h:28,\r\n                 from ./tensorflow/core/platform/errors.h:22,\r\n                 from ./tensorflow/core/platform/env.h:27,\r\n                 from ./tensorflow/stream_executor/lib/env.h:20,\r\n                 from tensorflow/stream_executor/cuda/cupti_stub.cc:19:\r\nbazel-out/k8-opt/bin/tensorflow/core/protobuf/error_codes.pb.h:47:51: error: 'AuxillaryParseTableField' in namespace 'google::protobuf::internal' does not name a type; did you mean 'AuxiliaryParseTableField'?\r\n   47 |   static const ::PROTOBUF_NAMESPACE_ID::internal::AuxillaryParseTableField aux[]\r\n      |                                                   ^~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                   AuxiliaryParseTableField\r\nTarget //native_client:libdeepspeech.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 10.015s, Critical Path: 8.56s\r\nINFO: 123 processes: 98 internal, 25 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["This seems due to protobuf issue \"[mac Protobuf 3.12 AuxillaryParseTableField wrong spelling #7616](https://github.com/protocolbuffers/protobuf/issues/7616)\".", "Hi @Saduf2019 ! Could you please look at this issue?", "@Geremia \r\nCould you please upgrade to the latest tf version and let us know if you still face this issue, also please verify the [test build configurations](https://www.tensorflow.org/install/source#tested_build_configurations) for compatibility", "@Saduf2019 Unfortunately, I have to build 2.4.0 for compatibility with [DeepSpeech](https://deepspeech.readthedocs.io/en/r0.9/BUILDING.html)."]}, {"number": 53671, "title": "Using tf.device when creating Keras layers does not move layer computation to that device", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Cloud AI Platform (TensorFlow Enterprise 2.7)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Python version: Python 3.7.12\r\n- CUDA/cuDNN version: 11.3, V11.3.109\r\n- GPU model and memory: NVIDIA Tesla K80 (x4)\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen creating a Keras model using the `tf.device` context manager, the resources used by that layer are placed on the requested device, but the TensorFlow ops involved in that layer do not execute on that device.\r\n\r\n**Describe the expected behavior**\r\n\r\nUsing the `tf.device` context manager when constructing a Keras layer should perform that layer's computation on the specified device.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.debugging.set_log_device_placement(True)\r\n        \r\n_input = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\r\nx = _input\r\nwith tf.device(\"/GPU:1\"):\r\n    x = tf.keras.layers.Dense(10, name=\"should_be_on_gpu\")(x)\r\n    x = tf.keras.layers.Dense(10, name=\"should_be_on_gpu_2\")(x)\r\nmodel = tf.keras.models.Model(inputs=[_input], outputs=[x])\r\nmodel.compile('adam', 'mse')\r\nmodel.summary()\r\nmodel.fit([2], [4])\r\n```\r\n\r\nHundreds of log lines are printed showing the placement of each op on device, but crucially, the `/GPU:1` device stores the `Dense` layers (i.e.: `ReadVariableOp`) but is _not_ where the computation (`MatMul`, in this case) happens:\r\n\r\n```\r\nmodel_3/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/device:GPU:0\r\nmodel_3/Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0\r\nmodel_3/should_be_on_gpu/MatMul/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:1\r\nmodel_3/should_be_on_gpu/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nmodel_3/should_be_on_gpu/BiasAdd/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:1\r\nmodel_3/should_be_on_gpu/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\r\nmodel_3/should_be_on_gpu_2/MatMul/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:1\r\nmodel_3/should_be_on_gpu_2/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nmodel_3/should_be_on_gpu_2/BiasAdd/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:1\r\nmodel_3/should_be_on_gpu_2/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\r\n```", "comments": ["@Saduf2019 ,\r\nI was able to reproduce the issue in tf v2.5, v2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/9ed469b6662a039d4d1bd9647b17fb0e/53671.ipynb).", "@psobot \r\nCould you please refer to the night execution of the code shared, i do not see the error reported, the output is same as is in the cpu without [error](https://colab.research.google.com/gist/Saduf2019/a8b4a74dcb0b4ca8445613d63e29dc60/untitled654.ipynb).", "Hi @Saduf2019! The Colab notebook shared above was run with a single GPU, but the code provided moves layers onto `/GPU:1`, which would require two different GPUs to verify.\r\n\r\nI've just modified the Colab notebook to add the following code to create two virtual GPUs, allowing you to see the issue in that environment:\r\n\r\n```python\r\ntf.debugging.set_log_device_placement(True)\r\ngpus = tf.config.list_physical_devices('GPU')\r\nif not gpus:\r\n    raise ValueError(\"At least one GPU required for this test!\")\r\nif len(gpus) == 1:\r\n    # Create two virtual GPUs for this test:\r\n    tf.config.set_logical_device_configuration(\r\n        gpus[0],\r\n        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\r\n          tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\r\n    logical_gpus = tf.config.list_logical_devices('GPU')\r\n    print(f\"{len(gpus)} physical GPUs, split into {len(logical_gpus)} logical GPUs\")\r\n    print(logical_gpus)\r\n```", "I tried the code snippet above but couldn't reproduce your log. Could you provide a Colab with the log as a reproduce?\r\n\r\nMy trial is similar to @tilakrayal's gist above. There are no \"MatMul\" anywhere in the prints.", "Hi @wangpengmit! Please see [this Colab which combines the two code snippets above](https://colab.research.google.com/gist/psobot/a7439cc1761daf71db4c5a146d2d40fa/untitled654.ipynb#scrollTo=fnXa1KdRADiP). The logs I pasted in above seem to be written by TensorFlow directly to the C++ stderr stream, which doesn't show up inline when executing the code in Colab - I've included instructions on how to find those logs in the Colab.", "Thanks for the reproduce! The two lines\r\n```\r\nx = tf.keras.layers.Dense(10, name=\"should_be_on_gpu\")(x)\r\nx = tf.keras.layers.Dense(10, name=\"should_be_on_gpu_2\")(x)\r\n```\r\nonly determine where the variables are placed. The four lines (or some subset of them) afterwards\r\n```\r\nmodel = tf.keras.models.Model(inputs=[_input], outputs=[x])\r\nmodel.compile('adam', 'mse')\r\nmodel.summary()\r\nmodel.fit([2], [4])\r\n```\r\ndetermine where real computations like `MatMul` and `BiasAdd` are placed. So the current behavior is working-as-intended. If you want to place `MatMul` and `BiasAdd` on `GPU:1`, you should indent the last four lines:\r\n```\r\nwith tf.device(\"/GPU:1\"):\r\n    x = tf.keras.layers.Dense(10, name=\"should_be_on_second_gpu\")(x)\r\n    x = tf.keras.layers.Dense(10, name=\"should_also_be_on_second_gpu\")(x)\r\n    model = tf.keras.models.Model(inputs=[_input], outputs=[x])\r\n    model.compile('adam', 'mse')\r\n    model.summary()\r\n    model.fit([2], [4])\r\n```", "Thanks @wangpengmit - unfortunately, I don't think that's a useful solution here, especially in more complex models that spread their variables (and/or layers) across multiple GPUs. Consider an example like this:\r\n\r\n```python\r\n_input = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\r\n\r\nwith tf.device(\"/GPU:0\"):\r\n    x = _input\r\n    x = tf.keras.layers.Dense(10, name=\"should_be_on_first_gpu\")(x)\r\n    x = tf.keras.layers.Dense(10, name=\"should_also_be_on_first_gpu\")(x)\r\n    gpu0 = x\r\nwith tf.device(\"/GPU:1\"):\r\n    x = _input\r\n    x = tf.keras.layers.Dense(10, name=\"should_be_on_second_gpu\")(x)\r\n    x = tf.keras.layers.Dense(10, name=\"should_also_be_on_second_gpu\")(x)\r\n    gpu1 = x\r\n\r\nmodel = tf.keras.models.Model(inputs=[_input], outputs=[gpu0, gpu1])\r\nmodel.compile('adam', 'mse')\r\nmodel.summary()\r\nmodel.fit([2], [4])\r\n```\r\n\r\nIn this case, all of the computation in this model will occur on GPU0, and the current API has no ability to move that computation onto each GPU independently. (This could be possible by manually writing a training loop, manually calling the required layers in a `@tf.function`, and manually managing the gradients to ensure that both the backward and forward pass are computed on the appropriate GPUs - but that's a huge change and results in much less flexible code.) This is obviously not a huge problem for these toy examples, but as models grow beyond the memory bounds of a single GPU, this becomes a blocker.\r\n\r\nIs there any way with the current TensorFlow and Keras APIs to force computations to be colocated on the GPU where their inputs are?", "This touches on an unfortunate TF design flaw: TF op's placement is statically determined by `tf.device` scopes at graph-construction time, instead of \"following the inputs\", thus we can't build either a \"placement-polymorphic\" graph or a graph whose subparts go to different devices depending on the inputs.\r\n\r\nIn your case, you can use custom layers, something like\r\n```\r\nclass GPU0Layer(tf.keras.layers.Layer):\r\n  def call(self, x):\r\n     with tf.device(\"GPU:0\"):\r\n      ...\r\n\r\nclass GPU1Layer(tf.keras.layers.Layer):\r\n  def call(self, x):\r\n     with tf.device(\"GPU:1\"):\r\n      ...\r\n\r\nmodel = tf.keras.models.Model(inputs=[_input], outputs=[GPU0Layer(_input), GPU1Layer(_input)])\r\n```\r\nBut  I understand this is less ideal than a \"follow-inputs\" placement system.\r\n\r\nWe are investigating supporting \"follow-inputs\" placement semantics, but it'll take a while.", "That's great detail, thanks!\r\n\r\nUsing custom layers like that will indeed work just fine for computing the forward pass, but unfortunately not the backward pass. I can't find anything in the documentation that allows layers to customize how/where to run their backward passes in TF2, and the best solution I've been able to come up with involves creating a `@tf.function` that uses a different `GradientTape` instance for each GPU."]}, {"number": 53659, "title": "[TF:TRT] Add Grappler optimizer that executes rewrites for TF-TRT conversion preperation ", "body": "This change adds a grappler pass prior to the TRTOptimizer. The new pass runs rewrites on the GraphDef via fixed point iteration. Some of the code is borrowed from the arithmetic optimizer. The TF-TRT segmentation and conversion process may desire to rewrite certain operation patterns in order to facilitate conversion or the performance of the converted model. Currently, we only perform one rewrite on the GraphDef before converting (FP32 cast rewrite). This ad-hoc rewrite is reformulated into a formal rewrite stage of the grappler pass. Additionally, this changes adds a rewriter to recognize when all required shape information is present for Conv2DBackpropInput and adds the information required for TRT conversion into a special attribute.", "comments": ["More info on the Conv2DBackpropInput rewrite for explanation:\r\n\r\nThis resolves an existing issue for `Conv2DBackpropInput` converter when the input has dynamic batch size. \r\n\r\nThe issue is that TF's Conv2dBackpropInput op requires a parameter `input_sizes` which is actually the full shape of the output of the Conv2DBackpropInput op instance, including batch dimension and channel dimensions. Operations dealing with a dynamic batch dimension cannot be const-folded by Grappler.\r\n\r\nUser's code typically looks like this:\r\n\r\n```\r\nbatch_size = tf.shape(input_tensor)[0] # this StridedSlice cannot be const-folded\r\nnon_batch_dims = tf.shape(input_tensor)[1:]   # This can and will be const-folded by Grappler's default const-folding optimizer\r\noutput_shape = tf.stack([batch_size, non_batch_dims]) # Thus, this Stack/Pack op cannot be const-folded\r\nupsampled_tensor = tf.conv2d_backprop_input(input_tensor, input_sizes=output_shape, ...)\r\n```\r\n\r\nThe net result is that \"output_shape\" is a non-constant tensor.\r\n\r\nHowever, for TF-TRT conversion, we don't care/use about the non-spatial dimensions of the `input_sizes` input tensor, so to us this looks like we should be able to const-fold the graph if we ignore the batch dimension.\r\n\r\nThe commit adds a custom rewriter to recognize when Conv2DBackpropInput has the situation shown above, including that we only have dynamic batch dimension above, and all other dimensions are static. \r\nSince we need to preserve originally graph def for native execution, we find the relevant shape constants and assign as a custom attribute to the`Conv2DBackpropInput` `NodeDef` instance under consideration. During conversion time, we first check for this attribute. If it exists, then we ignore the \"input_sizes/output_shape\" op input entirely, and use the attribute parameters to achieve conversion.\r\n\r\n", "I see this PR in draft state.  We have discussed this at the meeting and agreed on this approach.\r\nAre you able to split this into a sequence of a few PRs? It looks to me most individual commits here can be a separated PRs.", "First commit was split out into [this PR](https://github.com/tensorflow/tensorflow/pull/53652), which has all comments addressed.", "@bixia1 The second commit is now pulled out as #54301", "@christopherbate This PR is in draft, any update on this? Please. Thanks!", "I've been pulling individual commits out as separate PRs.  The last one, which is still awaiting merge, is #54301 ", "@bixia1 I've combined the remaining changes into a single commit, and this is ready for review.\r\n\r\nThere are additional rewrites for cast that I've removed and will introduce in a follow-on PR.", "Rebased and addressed comments. As discussed this week, I removed parts specific to the Conv2DBackprop rewrite, which will follow after this is merged.", "@bixia1  any progress on review?", "> Rebased and addressed comments. As discussed this week, I removed parts specific to the Conv2DBackprop rewrite, which will follow after this is merged.\r\n\r\nWould you please fix the PR description to reflect this change?", "Hi @christopherbate  Can you please check @bixia1's comments and keep us posted ? Thank you!"]}, {"number": 53658, "title": "tf.sparse.softmax randomly outputs wrong results!", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nlogits = tf.random.uniform([248, 248, 1, 16], dtype=tf.float32)\r\nr1 = tf.nn.softmax(logits,axis=-1)\r\nlogits_sp = tf.sparse.from_dense(logits)\r\nr2 = tf.sparse.softmax(logits_sp)\r\nr3 = tf.sparse.to_dense(r2)\r\nassert tf.math.reduce_all(tf.math.equal(r1, r3))\r\n```\r\nNormally the assertion would pass because `tf.sparse.softmax` and `tf.nn.softmax` should have the same output for the same input value. However, if I run the above code for 100 times then sometimes the assertion fails! Please see the [gist](https://colab.research.google.com/gist/ArrowIntoTheSky/fed8a0babf38a699ce40ecf59f70dbb6/untitled0.ipynb?authuser=2#scrollTo=PgfFQVFeCyLF) here for reference.\r\n\r\n\r\n", "comments": ["@Saduf2019 Was able to reproduce this issue on colab using TF version [2.7.0](https://colab.research.google.com/gist/sushreebarsa/69d2462eec2c137c257395327d2d96fa/untitled0.ipynb?authuser=2) and [tf-nightly](https://colab.research.google.com/gist/sushreebarsa/d531db2835f9e06d5a56e67eacab7995/untitled0.ipynb?authuser=2#scrollTo=3dMF6jtdC2zz)(2.9.0-dev20220108),please find the attached gists for reference.Thanks!"]}, {"number": 53657, "title": "tf.sparse.softmax lack support for float16", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nlogits = tf.random.uniform([16, 1, 10], dtype=tf.float16)\r\nr1 = tf.nn.softmax(logits,axis=-1) # pass\r\nlogits_sp = tf.sparse.from_dense(logits)\r\nr2 = tf.sparse.softmax(logits_sp) # InvalidArgumentError\r\n```\r\n\r\n**Describe the current behavior**\r\n`tf.sparse.softmax` cannot accept a tensor of type `float16`. However, `tf.nn.softmax` do support `half`. \r\nFor the above code snippet, the error message is:\r\n```\r\nInvalidArgumentError: Value for attr 'T' of half is not in the list of allowed values: float, double\r\n\t; NodeDef: {{node SparseSoftmax}}; Op<name=SparseSoftmax; signature=sp_indices:int64, sp_values:T, sp_shape:int64 -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]> [Op:SparseSoftmax]\r\n```\r\n\r\n**Describe the expected behavior**\r\nAccording to the document for `tf.sparse.softmax`, it is equivalent to `tf.nn.softmax` (but for sparse tensors), so `tf.sparse.softmax` should also support `float16` inputs.\r\n", "comments": ["This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ArrowIntoTheSky ,\r\nThe issue will move to closed status once the PR is merged."]}, {"number": 53655, "title": "tf.sparse.segment_sum doesn't support complex dtypes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ndata = tf.complex(tf.random.uniform([3, 4], dtype=tf.float64),tf.random.uniform([3, 4], dtype=tf.float64))\r\nsegment_ids = [0,0,1]\r\nres = tf.math.segment_sum(data=data,segment_ids=segment_ids) # pass\r\nres_sp = tf.sparse.segment_sum(data=data,indices=tf.constant([0, 1, 2]),segment_ids=segment_ids) # InvalidArgumentError\r\n```\r\n\r\n**Describe the current behavior**\r\n`tf.sparse.segment_sum` cannot accept a tensor of type `complex128`. However, `tf.math.segment_sum` do support it. \r\nFor the above code snippet, the error message is:\r\n```\r\nInvalidArgumentError: Value for attr 'T' of complex128 is not in the list of allowed values: float, double, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64\r\n\t; NodeDef: {{node SparseSegmentSum}}; Op<name=SparseSegmentSum; signature=data:T, indices:Tidx, segment_ids:Tsegmentids -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]> [Op:SparseSegmentSum]\r\n```\r\n\r\n**Describe the expected behavior**\r\n`tf.sparse.segment_sum` should also support complex dtypes. Actually I would expect the valid types of `data` of `tf.sparse.segment_sum`  to be the same as `tf.math.segment_sum`, since the document of `tf.sparse.segment_sum` does not have this information.\r\n", "comments": ["Hi @Saduf2019! Could you please look at this issue. Attaching Gist in [2.6](https://colab.sandbox.google.com/gist/mohantym/82bdcdd01364e838e17ba09bdc46fe35/github_53655.ipynb#scrollTo=bP0HkuagQpt8) ,[2.7](https://colab.sandbox.google.com/gist/mohantym/f118d8aac7af27e0a9a5e5d61f3a25c7/github_53655.ipynb#scrollTo=2-mrSko-tzOy)  and [nightly ](https://colab.sandbox.google.com/gist/mohantym/f118d8aac7af27e0a9a5e5d61f3a25c7/github_53655.ipynb#scrollTo=2-mrSko-tzOy)for reference. Thank you!", "Added PR #54357 for complex support of tf.sparse.segment_sum."]}, {"number": 53651, "title": "tf.bincount outputs wrong results", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nv = tf.constant([[0, 1, 2],\r\n       [3, 4, 5]], dtype=tf.int64)\r\nres1 = tf.math.bincount(v,axis=-1,maxlength=0, binary_output=True)\r\nres2 = tf.sparse.bincount(v,axis=-1,maxlength=0, binary_output=True)\r\nprint(res1)\r\nprint(res2)\r\n```\r\n\r\n**Current output**\r\n```\r\ntf.Tensor([], shape=(2, 0), dtype=int32)\r\nSparseTensor(indices=tf.Tensor(\r\n[[0 0]\r\n [0 1]\r\n [0 2]\r\n [1 3]\r\n [1 4]\r\n [1 5]], shape=(6, 2), dtype=int64), values=tf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int64), dense_shape=tf.Tensor([2 6], shape=(2,), dtype=int64))\r\n```\r\n\r\n**Expected output**\r\nFor `tf.sparse.bincount`, the output should have length at most `maxlength`, in the above code `maxlength` is `0`, so the output should be an empty tensor.\r\n(`tf.math.bincount` is just to show that the output for `tf.sparse.bincount` should be have _same length_ as `tf.math.bincount`. )", "comments": ["@ArrowIntoTheSky ,\r\ntf.math.bincount:-Counts the number of occurrences of each value in an integer array.\r\ntf.sparse.bincount:-Count the number of times an integer value appears in a tensor.\r\nPlease take a look at this link [1](https://www.tensorflow.org/api_docs/python/tf/math/bincount) and [2](https://www.tensorflow.org/api_docs/python/tf/sparse/bincount) which provides more information.It helps.Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53651\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53651\">No</a>\n", "Hi @tilakrayal ! Thanks for the comment and sorry about the confusion. I have edited the issue description.\r\n\r\nFor `tf.sparse.bincount`, the output should have **length at most `maxlength`**, in the above code `maxlength` is `0`, so the output should be an **empty tensor**.\r\n(`tf.math.bincount` is just to show that the output for `tf.sparse.bincount` should be have **same length**(not same value) as `tf.math.bincount`. )", "@chunduriv ,\r\nI was able to reproduce the issue in tf v2.7, v2.8 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/e0d0eea36762e1b9fbf3ef8a295b2b22/53651.ipynb)."]}, {"number": 53647, "title": "Switch build system", "body": "**System information**\r\n- TensorFlow version (you are using): 2.8\r\n- Are you willing to contribute it (Yes/No): Only with a system change\r\n\r\n**Describe the feature and the current behavior/state.**\r\nBuilding TensorFlow on Windows is a nightmare. A big problem is that either TensorFlow's Bazel scripts or Bazel itself is quite buggy. Examples:\r\n - Visual Studio 2022 does not work\r\n - LLVM does not always work under Windows\r\n - Admin rights are needed to build\r\n\r\nWith Bazel, building TF is a gamble. For example, if you install VS2022 next to VS2019 it doesn't work anymore without workarounds. Another problem with Bazel is that the files are always stored in C:\\Users\\username\\ _bazel\\_username. You can't easily change the directory to e.g. move the build to a fast SSD or when C is running out of space.\r\n\r\nTF has used CMake in the past. A switch back to CMake would be desirable.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who builds TensorFlow on Windows.\r\n", "comments": ["Agreed. But since Keras requires to bazel for its build tensorflow had to follow the same. Sadly everything is tested on Linux OSS. So I don't see this being solved in near future. ", "Google uses an equivalent of Bazel internally. We cannot maintain a CMake build, the existing one was always getting out of date and was more broken than actually working.\r\n\r\nIf an external contributor were to provide CMake support and maintain it, we can take that, but otherwise, sadly we cannot.", "Bazel is just a single disaster. I have now tried to build TF on three computers and it only worked on one. Most users use Windows, so it makes no sense to build everything on Linux. In the long run, a Bazel alternative would be very desirable.", "windows build is almost impossible these days,  I've built tensorflow with cuda so many times before(2016~2021), but now I gave up", "Hi\r\n\r\nI'll also add that it would be __significantly__ easier to package and maintain tensorflow for [nixpkgs](https://github.com/NixOS/nixpkgs) without Bazel. Running Bazel in isolation is very hard and the maintenance burden is unreasonably high because of the way Bazel handles dependencies and platforms. CMake's `find_package`/Meson's `dependency` implement a sort of a dependency injection mechanism and are much easier to hand dependencies to\r\n\r\n> If an external contributor were to provide CMake support and maintain it, we can take that, but otherwise, sadly we cannot\r\n\r\nI.e. you'd be OK merging it?", "@perfinion is working on unbundling TF's dependencies so you could use those installed on the system. He's doing this for Gentoo, maybe this can also work for nix?\r\n\r\n> you'd be OK merging it?\r\n\r\nYes, either here or on [tensorflow/build](https://github.com/tensorflow/build). The second is slightly better as we're trying to separate files that are used both internally and externally from files which are only community owned."]}, {"number": 53639, "title": "[tflite] Add ruy header for profile in gather", "body": "This will include missing ruy instrumentation header for ruy profile.", "comments": ["There seems some network problem... I'll just wait till someone restarts the CI...", "@gbaned PTAL", "@miaout17 Can you please review this PR ? Thank you!"]}, {"number": 53629, "title": "Building with Bazel failed during fetching of repository \"go sdk\"", "body": "Happy New Year 2022!\r\n\r\nI've been trying to build TF 2.6 from source using Bazel on my Ubuntu machine. Unfortunately, I got stuck at the very beginning step of installing dependencies for TF. The error says that bazel failed to fetch (and also extract?) GO SDK package (see log below). My system details and building commands are also provided.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 21.04 // 16 cores AMD Ryzen 7 1700 processors with 16 GB of RAM.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6\r\n- Python version: 3.9.9\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.3\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI had cloned TF from my repo forked from the official TF repo, checkouted `r2.6`, and then configured the build (`./configure`) and left answers empty (answered No to all questions).\r\n\r\n```\r\nbazel build --jobs=8 --local_ram_resources=\"HOST_RAM*.50\" \\\r\n  --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" -c opt \\\r\n  --config=noaws --config=nogcp --config=nohdfs --config=nonccl --config=v2 \\\r\n  //tensorflow:libtensorflow.so \\\r\n  //tensorflow:libtensorflow_cc.so  \\\r\n  //tensorflow:libtensorflow_framework.so  \\\r\n  //tensorflow:install_headers\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nWARNING: Output base '/home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nWARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=83\r\nINFO: Reading rc options for 'build' from /home/rketka/github/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/rketka/github/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /home/rketka/github/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/rketka/miniconda3/envs/tfcc/bin/python3 --action_env PYTHON_LIB_PATH=/home/rketka/miniconda3/envs/tfcc/lib/python3.9/site-packages --python_path=/home/rketka/miniconda3/envs/tfcc/bin/python3\r\nINFO: Found applicable config definition build:short_logs in file /home/rketka/github/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/rketka/github/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:noaws in file /home/rketka/github/tensorflow/.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nogcp in file /home/rketka/github/tensorflow/.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:nohdfs in file /home/rketka/github/tensorflow/.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:nonccl in file /home/rketka/github/tensorflow/.bazelrc: --define=no_nccl_support=true\r\nINFO: Found applicable config definition build:v2 in file /home/rketka/github/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /home/rketka/github/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/rketka/github/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/b570a1921c9e55ac53c8972bd2bfd37cd0eb510d.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /home/rketka/github/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /home/rketka/github/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Repository go_sdk instantiated at:\r\n  /home/rketka/github/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /home/rketka/github/tensorflow/tensorflow/workspace0.bzl:120:20: in workspace\r\n  /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps\r\n  /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains\r\n  /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk\r\nRepository rule _go_download_sdk defined at:\r\n  /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'go_sdk':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/io_bazel_rules_go/go/private/sdk.bzl\", line 51, column 16, in _go_download_sdk_impl\r\n\t\t_remote_sdk(ctx, [url.format(filename) for url in ctx.attr.urls], ctx.attr.strip_prefix, sha256)\r\n\tFile \"/home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/io_bazel_rules_go/go/private/sdk.bzl\", line 128, column 17, in _remote_sdk\r\n\t\tfail(\"error extracting Go SDK:\\n\" + res.stdout + res.stderr)\r\nError in fail: error extracting Go SDK:\r\nTimed out\r\nERROR: Analysis of target '//tensorflow:libtensorflow_framework.so' failed; build aborted: error extracting Go SDK:\r\nTimed out\r\nINFO: Elapsed time: 1891.172s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (23 packages loaded, 32 targets configured)\r\n```\r\n\r\nAny ideas to tackle this issue? Hope that the info above is sufficient, but let me know if you need further info for investigation. \r\n\r\nThank you very much.\r\nRangsiman", "comments": ["@rangsimanketkaew Could you please let us know if this issue still persists in  latest stable TF v2.7.0 ? Thanks!", "@sushreebarsa\r\n\r\n> @rangsimanketkaew Could you please let us know if this issue still persists in latest stable TF v2.7.0 ? Thanks!\r\n\r\nYes, I also faced the same issue in v2.7."]}, {"number": 53620, "title": "py_function is slower in TF2.7 (and 2.6) compared to TF2.5", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): docker\r\n- TensorFlow version (use command below): 2.7\r\n- Python version: 3.8.10\r\n\r\n**Describe the current behavior**\r\n\r\nIn my training code I'm using `tf.data.Dataset.map` with a `tf.py_function` within it. I know that using `tf.py_function` is slower than regular tensorflow, but there's a part of my dataset processing that requires the usage of plain Python code.   \r\n\r\nHowever, when I tried to upgrade TF from 2.5 to 2.6, I noticed that my trainings were slower in 2.6 compared to 2.5. I postponed the upgrade at the moment. When 2.7 came out I tried again but the slower data processing was still present.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would have expected this kind of code to be as efficient as it was in 2.5\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nYou can find a reproducible example in [this colab](https://colab.research.google.com/drive/1AmOnuNosr7nq5p4m25jJwG-I2R9r7PTf?usp=sharing).\r\n\r\nTo reproduce my issue, I wrapped the opening of the image on disk within a `tf.py_function` and opened it using Pillow and numpy. I know that this can be done in a more efficient way as I explained directly in the colab.\r\n\r\nHere are the results I got when executing the `%%timeit` cell with 2 different versions of TF:\r\n- tf.__version__ = \"2.7.0\" -> 3 loops, best of 7: 1.81 s per loop (would have been the same in TF2.6)\r\n- tf.__version__ = \"2.5.2\" -> 3 loops, best of 7: 1.44 s per loop\r\n", "comments": ["Hi @chunduriv ! Could you please look at this issue? Attaching Gist in[ 2.7](https://colab.sandbox.google.com/gist/mohantym/0a49116bc0c7ea34af5b6fa563df2ffb/slow_pyfunction_tf27.ipynb#scrollTo=oQ0SYAzqIOqS) and [2.6](https://colab.sandbox.google.com/gist/mohantym/e63c8daa9c9680caf97c6678553de514/slow_pyfunction_tf27.ipynb#scrollTo=LGz8iNzS6a4K) for reference .Thank you!", "Thanks for raising this issue. We are looking into it."]}]