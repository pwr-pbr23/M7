[{"number": 34453, "title": "Avoid hard coding in getMajorVersionNumber", "body": "fix #34256\r\n\r\nCurrently, `NativeLibrary.getMajorVersionNumber` return \"1\" by hard coding. It is very ugly and not friendly for tf-2.x. we can pack version info to jar, and load the version at runtime.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34453) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34453) for more info**.\n\n<!-- ok -->", "cc @allenlavoie @sjamesr", "@sharkdtu thank you , do you mind adding some test cases ?", "@rthadur  I think existing tests can cover this commit. If there is any problem, existing tests will fail.", "@rthadur can anyone else review this pr ?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34453) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34453) for more info**.\n\n<!-- ok -->", "@sharkdtu can you please check the sanity errors here https://source.cloud.google.com/results/invocations/6a091413-533e-4a45-baf8-1f93e336733d/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_sanity_out/log", "Can you please rebase on master?", "@mihaimaruseac  done", "@mihaimaruseac \r\ni have no idea about \"import/copybara error\", so i rebased on master again, and squashed to 1 commit", "copybara errors are usually solved by a rebase on master: copybara sees files as clean/dirty so if a file touched in the PR is also changed in master copybara will fail to import.\r\n\r\nHowever, if that is not the case, then the error might be a conflict with an internal version of the code, and we will have to import the PR manually. Please ping me again if copybara still fails.", "@mihaimaruseac \r\ncopybara still failed, can you please check it"]}, {"number": 34452, "title": "python3.7.3 Anaconda tensorflow   No module named 'joblib'", "body": "I have successfully installed the sklearn library, but it shows the code running result display.\r\nNo module named 'joblib'\r\nI am very confused, I hope to get your answer, thanks \u2018\r\nHere is my code\r\n\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-25-638e12154cef> in <module>\r\n----> 1 from sklearn.datasets import load_wine\r\n      2 win_dataset=load_wine()\r\n      3 print('\\n\\n\\n')\r\n      4 print(\"the result is\")\r\n      5 print(\"\u7ea2\u9152\u4e2d\u7684\u6570\u636e\uff1a\\n{}\".format(wine_dataset.keys()))\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\__init__.py in <module>\r\n     74 else:\r\n     75     from . import __check_build\r\n---> 76     from .base import clone\r\n     77     from .utils._show_versions import show_versions\r\n     78 \r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\base.py in <module>\r\n     14 \r\n     15 from . import __version__\r\n---> 16 from .utils import _IS_32BIT\r\n     17 \r\n     18 _DEFAULT_TAGS = {\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\__init__.py in <module>\r\n     15 from .murmurhash import murmurhash3_32\r\n     16 from .class_weight import compute_class_weight, compute_sample_weight\r\n---> 17 from . import _joblib\r\n     18 from ..exceptions import DataConversionWarning\r\n     19 from .deprecation import deprecated\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\_joblib.py in <module>\r\n      6     # joblib imports may raise DeprecationWarning on certain Python\r\n      7     # versions\r\n----> 8     import joblib\r\n      9     from joblib import logger\r\n     10     from joblib import dump, load\r\n\r\nModuleNotFoundError: No module named 'joblib'\r\n\r\n\r\n", "comments": ["Install joblib package using this command\r\n`conda install -c anaconda joblib`\r\nand this should solve your issue.\r\n\r\nIf you are still running into the sane issue, you can go through the following questions [#1](https://stackoverflow.com/questions/35103338/python-no-module-named-how-to-use-pip) and [#2](https://forum.knime.com/t/modulenotfounderror-no-module-named-joblib/15275).", "I have successfully solved the problem, thank you very much for your answer."]}, {"number": 34451, "title": "'accuracy' and tf.metrics.get('accuracy') produce different results", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OpenSUSE\r\n- TensorFlow installed from (source or binary): pip binary within pyenv\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7.5\r\n\r\n**Describe the current behavior**\r\nThe same model behaves differently whether one uses `'accuracy'` or `tf.keras.metrics.get('accuracy')` (see below).\r\n\r\n**Describe the expected behavior**\r\nThey should behave identically.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n\"\"\"Bug.\"\"\"\r\n# import keras\r\nimport numpy as np\r\nimport tensorflow.keras as keras\r\n\r\nX = np.empty([10, 224, 224, 3])\r\nY = np.empty([10, 2])\r\n\r\nMODEL = keras.applications.vgg16.VGG16(weights=None, classes=2)\r\n\r\nMODEL.compile(optimizer=keras.optimizers.Adam(),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nMODEL.fit(X, Y, epochs=10)\r\n\r\nMODEL.compile(optimizer=keras.optimizers.Adam(),\r\n              loss='categorical_crossentropy',\r\n              metrics=[keras.metrics.get('accuracy')])\r\nMODEL.fit(X, Y, epochs=10)\r\n```\r\nExample output:\r\n```\r\nTrain on 10 samples\r\nEpoch 1/10\r\n\r\n10/10 [==============================] - 4s 389ms/sample - loss: inf - accuracy: 0.9000\r\nEpoch 2/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 3/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 4/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 5/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 6/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 7/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 8/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 9/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 10/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nTrain on 10 samples\r\nEpoch 1/10\r\n\r\n10/10 [==============================] - 1s 131ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 2/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 3/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 4/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 5/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 6/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 7/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 8/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 9/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 10/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\n```\r\n\r\n**Other info / logs**\r\nClosely related to #34088", "comments": ["@bersbersbers This has been fixed in the latest version of tf-nightly. Please find my gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/caca35bfc6f28afa2c3744951bd826b1/untitled247.ipynb). ", "I believe this is not fixed. Your gist clearly shows this output, with different accuracy values reported:\r\n\r\n```\r\n...\r\nEpoch 10/10\r\n10/10 [==============================] - 19s 2s/sample - loss: 0.0000e+00 - accuracy: 1.0000\r\nTrain on 10 samples\r\nEpoch 1/10\r\n10/10 [==============================] - 20s 2s/sample - loss: 0.0000e+00 - accuracy: 0.0000e+00\r\nEpoch 2/10\r\n...\r\n```", "@gowthamkpr this has not been fixed, please see my answer above.", "@bersbersbers Thank you for the issue. There are 3 metric accuracy functions:\r\n\r\n1. Accuracy: This just checks the equality of `y_true` and `y_pred`.\r\n2. Binary accuracy: This converts `y_pred` to 0 or 1 based on a threshold and then compares with `y_true`.\r\n3. Categorical_accuracy: This checks the equality of class index of the max `y_pred` with the class index of the max `y_true` value\r\n\r\nWhen you say `keras.metrics.get('accuracy')` you get the `accuracy` function. But if you pass the string `accuracy`, we infer between `binary_accuracy`/`categorical_accuracy` functions based on the shape of the model output.\r\n\r\nPlease let me know if you have any questions or concerns with this.\r\n\r\nThank you!", "> When you say `keras.metrics.get('accuracy')` you get the `accuracy` function. But if you pass the string `accuracy`, we infer between `binary_accuracy`/`categorical_accuracy` functions based on the shape of the model output.\r\n> \r\n> Please let me know if you have any questions or concerns with this.\r\n\r\nI believe it cannot be expected from a user to know all this, given that neither the behavior of `\"accuracy\"` (*why* is `\"accuracy\"` different from the `accuracy()` function?) nor that of `keras.metrics.get` (what is it supposed to be doing?) is documented. From the code, the latter simply calls `deserialize(str(identifier))`, and I have never before seen a (de)serializer (designed to) modify the behavior of an object.", "I agree that this needs more documentation. `get` does not have the behavior described above, `compile` API does. `get` will just return the deserialized version. Hence, when you give `keras.metrics.get('accuracy')` it will just return the `accuracy` function.", "I have updated the compile API docs to address this, will be in the next nightly. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34451\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34451\">No</a>\n", "wow, this change from \"accuracy\" to \"binary_accuracy\" made me think that my model trained in tf 2.1 was performing really poorly in tf 2.2. as i looked closer i realized the performance was the same, but \"accuracy\" was computed differently. it might have been better to have a warning when using \"accuracy\" in tf 2.2 saying that the definition will change, then in tf 2.3 actually changing it... or anything else to help introduce the change to folks."]}, {"number": 34450, "title": "TFLite UnityPlugin: Add more interpreter functions on Unity Plugin", "body": "Added functions calling `c_api.h` from Unity plugin", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34450) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34450) for more info**.\n\n<!-- ok -->", "Adding @jdduke since he wrote these Unity files.", "@jdduke Thanks for the reviewing. Changed it.\r\n\r\nAnd can I change the unity project version from `2017.4.6f1` to `2018.4.12f1` which is the current LTS(long term support) version.", "@jdduke Thank for your review. fixed it!", "@asus4 Could you please address the jdduke's comments. Thanks!", "Sorry for the late reply, I missed it.\r\nFixed it and confirmed it works on Unity 2017.4.  Could you check it again @jdduke thanks", "And Unity2017 will be out of support soon. May I upgrade Unity version to 2018LTS  in another PR?\r\nhttps://blogs.unity3d.com/2018/04/09/new-plans-for-unity-releases-introducing-the-tech-and-long-term-support-lts-streams/", "> And Unity2017 will be out of support soon. May I upgrade Unity version to 2018LTS in another PR?\r\n> https://blogs.unity3d.com/2018/04/09/new-plans-for-unity-releases-introducing-the-tech-and-long-term-support-lts-streams/\r\n\r\nThanks for flagging, let's wait another quarter and then proceed with the upgrade? And thanks again for the contribution! This is a really nice improvement.", "@asus4 can you please rebase to head once and lets try to pull it in again , thank you ", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34450) for more info**.\n\n<!-- need_author_consent -->", "@rthadur rebased on the current head, would you check it?", "@asus4 please rebase your branch and then apply your changes , you have checked out lot changes while rebasing  ", "@asus4 Can you please check @rthadur's comments and sign CLA ? Thanks!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34450) for more info**.\n\n<!-- ok -->", "@rthadur @gbaned  I rebased onto master branch / pushed with --force option. My local tree looks clean.\r\n![Screen Shot 2020-07-17 at 15 28 45](https://user-images.githubusercontent.com/357497/87791422-7edc8780-c842-11ea-99d3-5738f08fe67d.png)\r\n\r\n", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 34449, "title": "TensorBoard callback should not close and reopen writers to enable reuse", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OpenSUSE\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7.5\r\n\r\n**Describe the current behavior**\r\nCurrently, the `tf.keras.callbacks.TensorBoard.set_model` function closes the `train` writer, only to reopen it two lines later:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/7eef97bba7a9018d6b0630418d0810f94c1d2222/tensorflow/python/keras/callbacks.py#L1535-L1537\r\n\r\nThis complicates sharing the writer with other callbacks, e.g., in my code below: this code throws a `RuntimeError: SummaryWriter is already closed`.\r\n\r\n**Describe the expected behavior**\r\n`tf.keras.callbacks.TensorBoard.set_model` should only close those writers that will not be needed any longer.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n# import keras\r\nimport numpy as np\r\nimport tensorboard.plugins.hparams.api as hp\r\nimport tensorflow.keras as keras\r\n\r\ntb_callback = keras.callbacks.TensorBoard(log_dir='/tmp')\r\ntb_train_writer = tb_callback._get_writer(tb_callback._train_run_name)  # pylint:disable=protected-access\r\nhp_callback = hp.KerasCallback(tb_train_writer, {'param': 42})\r\nmodel = keras.applications.VGG16()\r\nmodel.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy')\r\nmodel.fit(np.empty((1, 224, 224, 3)), np.empty((1, 1000)), callbacks=[tb_callback, hp_callback])\r\n```\r\n", "comments": ["Issue replicating for version TF-2.0. Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/f64e85cedff9e96dfe12b36ef3fba472/34449.ipynb) of colab.Thanks!", "This can be closed: TF 2.2 does not even create writers any more until `set_model`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34449\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34449\">No</a>\n", "What is the accepted API for using a shared file_writer with tensorboard callback?"]}, {"number": 34448, "title": "model.compile() should check for unknown kwargs", "body": "**System information**\r\n- TensorFlow version (you are using): TF2\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThis code works:\r\n```\r\n# import keras\r\nimport tensorflow.keras as keras\r\n\r\nmodel = keras.applications.VGG16()\r\nmodel.compile(loss='mse', optimizer=keras.optimizers.Adam(),\r\n              why_does_this_not_fail=42.0)\r\nprint(\"Success\")\r\n```\r\n\r\nI believe this code should throw an exception stating that `why_does_this_not_fail` is not a known argument.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who inadvertently adds the `callbacks` argument to the `model.compile` method (instead of `model.fit`) and asks themselves why their callbacks are not working.\r\n", "comments": ["Please find the github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/a0d97fa218c98e386f353f6a351a75da/untitled248.ipynb)", "I believe this is fixed in 2.1.0rc0:\r\n\r\n```\r\n>>> model.compile(loss='mse', optimizer=keras.optimizers.Adam(),\r\n...               why_does_this_not_fail=42.0)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/home/bersbers/.pyenv/versions/3.7.5/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/bersbers/.pyenv/versions/3.7.5/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 312, in compile\r\n    'Invalid keyword argument(s) in `compile`: %s' % (unknown_kwargs,))\r\nTypeError: Invalid keyword argument(s) in `compile`: {'why_does_this_not_fail'}\r\n```", "@bersbersbers thanks for checking with 2.1.0-rc0. \r\nPlease let us know if anything else remains, else will close this issue in the next 2 days."]}, {"number": 34447, "title": "Tensorflow C/C++ only?", "body": "- OS Platform and Distribution Linux Ubuntu 18.04\r\n- Virtual guest on windows hyperV\r\n\r\nTrying your installation steps and fails again and again:\r\n\r\npip install -U pip six numpy wheel setuptools mock 'future>=0\r\n\r\nIt's trying to connect without success, although the machine does not have any network issue.\r\n\r\nIs there a way to use TensorFlow w/o python , only C/C++?", "comments": ["@sergeay Can you please explain clearly, what you are trying to do?", "If you want c++ only check https://jacobgil.github.io/deeplearning/tensorflow-cpp", "@sergeay, Install Tensorflow C using the instructions mentioned [here](https://www.tensorflow.org/install/lang_c).  Please find the tensorflow C++ operation in this [link](https://www.tensorflow.org/api_docs/cc). Thanks!", "bhack and gadagashwini , thanks I'll try those links.\r\n\r\nabhiverma1924, I am trying to make tensorflow work on my ubuntu 18.04 machine, I am not interested in any python and it seems the main page of tensorflow requires all kind of bazel, pyton this and python that, each installation step is failing one after the other and requires tons of time to look at the conflicting packages and environment and resolve the issues.", "@sergeay, Any update!", "I finally used the python interface and it works after some massages...\r\nThanks you for your help.", "Closing as it seems fixed."]}, {"number": 34446, "title": "TFLite: Missing activation in built-in Mul operation", "body": "ReLU activation went missing despite appearing in the graph as an activation function in the Mul operation.\r\nHope this fixes a TODO marked by @ekaterinaignasheva", "comments": ["@guydavid Can you please resolve conflicts? Thanks!", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR.\r\n @chanshah thank you for importing this PR internally."]}, {"number": 34445, "title": "Adapter to allow sparse matrix as target labels in Keras models", "body": "Tensorflow version : 2.0\r\nOS : Debian 9\r\n\r\nI am training a sequential model as shown below : \r\n\r\n    model = Sequential()\r\n    model.add(Embedding(input_dim=170000, output_dim=100, input_length=10))\r\n    model.add(GlobalAveragePooling1D())\r\n    model.add(Dense(num_target=3811, activation='softmax'))\r\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\r\n\r\n    model.fit(X_train,\r\n              Y_train\r\n              batch_size=16384,\r\n              epochs=200,\r\n              verbose=1)\r\n\r\nX_train.shape = (12528566, 10)\r\nY_train.shape = (12528566, 3811) \r\n\r\nX_train type : numpy.ndarray\r\nY_train type : scipy.sparse.csr.csr_matrix\r\n\r\nEarlier in tensorflow 1.13 the training worked fine, but in Tensorflow 2.0 it is throwing error :\r\n` Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, <class 'scipy.sparse.csr.csr_matrix'>`\r\n\r\nI can convert Y_train dense type, by command :\r\n`Y_train.toarray()`\r\n\r\nbut it throws error saying, \r\n`MemoryError: Unable to allocate array with shape (12528566, 3811) and data type int64`\r\n\r\nThe size of Y_train when dense is, 12528566 * 3811 * 8 = ~381GB\r\n\r\nDense Y_train works fine, but only for smaller data.\r\n\r\n\r\nSo in order to make above thing work, I changed my loss to 'sparse_categorical_crossentropy' & Y_train to `np.asarray(Y_train.tocoo().col)` which is basically the index of my label vector which maps to the corresponding output label, and model gets trained on Tensorflow 2.0 and predicts as expected.\r\n\r\n    model = Sequential()\r\n    model.add(Embedding(input_dim=170000, output_dim=100, input_length=10))\r\n    model.add(GlobalAveragePooling1D())\r\n    model.add(Dense(num_target=3811, activation='softmax'))\r\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\r\n\r\n\r\nNow I want to train a model with 'binary_crossentropy' loss model with 'sigmoid' activation for which I need to make Y_train as dense matrix, since there is no 'sparse_binary_crossentropy' loss type. If I don't use dense Y_train I get an error : \r\n\r\n`ValueError: A target array with shape (12528566, 1) was passed for an output of shape (None, 3811) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.`\r\n\r\n\r\n\r\n1. Why is the adapter to handle sparse matrix removed from Tensorflow 2.0?\r\n2. Why is there no 'sparse_binary_crossentropy' loss type?\r\n\r\n\r\n", "comments": ["EDIT : You can't use Sparse tensors either in mode.fit()\r\n\r\nFailed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"DeserializeSparse:0\", shape=(None, 2), dtype=int64), values=Tensor(\"DeserializeSparse:1\", shape=(None,), dtype=int64), dense_shape=Tensor(\"DeserializeSparse:2\", shape=(2,), dtype=int64)). Consider casting elements to a supported type.", "@yashmanuda \r\n\r\nLooks like code is incomplete. Can you help us with minimal stand alone code and supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "```\r\nimport os\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '2'\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\r\n\r\nimport tensorflow\r\nimport numpy as np\r\nimport string\r\nimport random\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nfrom sklearn.preprocessing import LabelBinarizer\r\nfrom tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\ntraining_size = 12528566  # if you change the size to 1000 it will work if to_use_sparse is False or is_convert is False\r\n\r\nto_use_sparse = True  # if set True and to_convert is False, throws error that adapter is not found\r\n\r\nX_train = np.random.randint(low=0, high=169999, size=(training_size, 10), dtype='int32')\r\nlabels = []\r\nfor i in range(3811):\r\n    labels.append(''.join(random.choices(string.ascii_uppercase + string.digits, k=6)))\r\nY = [random.choice(labels) for i in range(training_size)]\r\n\r\nif to_use_sparse:\r\n    Y_train = LabelBinarizer(sparse_output=True).fit(labels).transform(Y)  # no adapter is found\r\nelse:\r\n    Y_train = LabelBinarizer(sparse_output=True).fit(labels).transform(Y).toarray()  # this thing works but only if training_size is small, say 1000\r\n\r\n    \r\nto_convert = True # converting the labels to integers of their index and using sparse_categorical_crossentropy\r\n\r\nif to_convert and to_use_sparse: # if true, it works as expected\r\n    model = Sequential()\r\n    model.add(Embedding(input_dim=170000, output_dim=100, input_length=10))\r\n    model.add(GlobalAveragePooling1D())\r\n    model.add(Dense(3811, 'softmax'))\r\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\r\n    model.fit(X_train,\r\n              np.asarray(Y_train.tocoo().col),\r\n              batch_size=16384,\r\n              epochs=5,\r\n              verbose=1)\r\nelse:\r\n    model = Sequential()\r\n    model.add(Embedding(input_dim=170000, output_dim=100, input_length=10))\r\n    model.add(GlobalAveragePooling1D())\r\n    model.add(Dense(3811, 'softmax'))\r\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\r\n    model.fit(X_train,\r\n              Y_train,\r\n              batch_size=16384,\r\n              epochs=5,\r\n              verbose=1)\r\n```\r\n    ", "Please let me know if you don't understand anything.", "I have tried on colab with TF version 2.0  with `to_use_sparse is True and is_convert is True` and i am not seeing any issue. However with `to_use_sparse is True and is_convert is False` i am seeing error as `Failed to find data adapter`. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/1dd62dcfbb45014cc34558bf92118fa3/untitled409.ipynb). Is this the expected behavior?. Thanks!", "Yes, it is the expected output of the above code. Is there any way to fix this?\r\nAlso, if I set `to_use_sparse to False` and `training_size to 12528566` with `is_convert to False`, I get memory errors. This is because Y_train is not sparse anymore and it consumes lot of memory. ", "Is there any update on this?", "This is the default behaviour of Tensorflow 2.0 @yashmanuda \r\n\r\nAlso as this question is not related to bug/performance, build/install, docs related issues, please post it in stackoverflow where there is a bigger community to respond. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34445\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34445\">No</a>\n"]}, {"number": 34444, "title": "tf.reduce_mean crashes TensorFlow Lite", "body": "**System information**\r\n- Ubuntu 18.04 4.15.0-66-generic x86_64 \r\n- TensorFlow installed from (source or binary): official wheel\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nThe program SIGABRTs (see the code below).\r\n\r\n**Describe the expected behavior**\r\n\r\nThe program does not crash.\r\n\r\n**Code to reproduce the issue**\r\n\r\n<details>\r\n<summary>Code</summary>\r\n\r\n```python\r\nfrom time import time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.lite.python.interpreter import load_delegate\r\n\r\n\r\ndef measure(size, rep, tpu):\r\n    @tf.function(input_signature=[tf.TensorSpec([size] * 2, tf.float32)] * 2)\r\n    def bench_func(a, b):\r\n        x = tf.linalg.matmul(a, b)\r\n        return tf.reduce_mean(x)\r\n    \r\n    def gen_input_samples():\r\n        i = np.identity(size, np.float32)\r\n        yield [-i, i]\r\n        yield [i, i]\r\n\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([bench_func.get_concrete_function()])\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = gen_input_samples\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    tflite_model = converter.convert()\r\n    delegates = [load_delegate(\"libedgetpu.so.1.0\")] if tpu else []\r\n    interpreter = tf.lite.Interpreter(model_content=tflite_model, experimental_delegates=delegates)\r\n    interpreter.allocate_tensors()\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n    tensor_in1 = interpreter.tensor(input_details[0][\"index\"])\r\n    tensor_in2 = interpreter.tensor(input_details[1][\"index\"])\r\n    tensor_out = interpreter.tensor(output_details[0][\"index\"])\r\n    inp1 = np.random.rand(size, size) - 0.5\r\n    inp2 = np.random.rand(size, size) - 0.5\r\n    tensor_in1()[:] = inp1\r\n    tensor_in2()[:] = inp2\r\n    now = time()\r\n    for _ in range(rep):\r\n        interpreter.invoke()\r\n    return time() - now\r\n \r\nprint(measure(2048, 1, tpu=False))\r\n```\r\n</details>\r\n\r\n**Other info / logs**\r\n<details>\r\n<summary>Log</summary>\r\n\r\n```\r\n2019-11-20 10:43:18.538007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-11-20 10:43:18.546197: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2019-11-20 10:43:18.546227: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vadim-XPS-13-9380): /proc/driver/nvidia/version does not exist\r\n2019-11-20 10:43:18.546569: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-20 10:43:18.572569: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1992000000 Hz\r\n2019-11-20 10:43:18.573251: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x48fd5a0 executing computations on platform Host. Devices:\r\n2019-11-20 10:43:18.573290: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-20 10:43:18.671265: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-11-20 10:43:18.671336: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-11-20 10:43:18.690246: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-11-20 10:43:18.690274: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.005ms.\r\n2019-11-20 10:43:18.690277: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-11-20 10:43:18.700228: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-11-20 10:43:18.700397: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-11-20 10:43:18.720772: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-11-20 10:43:18.720800: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 0.895ms.\r\n2019-11-20 10:43:18.720822: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 0.072ms.\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Initialized TensorFlow Lite runtime.\r\nAborted (core dumped)\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>gdb bt</summary>\r\n\r\n```\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n#1  0x00007ffff7a24801 in __GI_abort () at abort.c:79\r\n#2  0x00007fff3c18b616 in TfLiteStatus tflite::ops::builtin::reduce::EvalMean<(tflite::ops::builtin::reduce::KernelType)0>(TfLiteContext*, TfLiteNode*) ()\r\n   from /usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#3  0x00007fff3c1f0577 in tflite::Subgraph::Invoke() ()\r\n   from /usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#4  0x00007fff3c1f222a in tflite::Interpreter::Invoke() ()\r\n   from /usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#5  0x00007fff3c0bff58 in tflite::interpreter_wrapper::InterpreterWrapper::Invoke() ()\r\n   from /usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#6  0x00007fff3c0be054 in _wrap_InterpreterWrapper_Invoke ()\r\n   from /usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#7  0x00000000005b0462 in _PyMethodDef_RawFastCallKeywords () at ../Objects/call.c:694\r\n#8  0x00000000005cc860 in _PyCFunction_FastCallKeywords (kwnames=<optimized out>, nargs=<optimized out>, args=0x7fff6c1216c0, func=\r\n    <built-in method InterpreterWrapper_Invoke of module object at remote 0x7fff6c11ed18>) at ../Objects/call.c:730\r\n#9  call_function.lto_priv () at ../Python/ceval.c:4568\r\n#10 0x000000000051add9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3093\r\n#11 0x00000000005b0eac in PyEval_EvalFrameEx (throwflag=0, f=\r\n    Frame 0x7fff6c121540, for file /usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py, line 109, in Invoke (self=<InterpreterWrapper(this=<SwigPyObject at remote 0x7fff74aca030>) at remote 0x7fff741f1c88>)) at ../Python/ceval.c:547\r\n#12 function_code_fastcall (globals=<optimized out>, nargs=<optimized out>, args=<optimized out>, co=<optimized out>) at ../Objects/call.c:283\r\n#13 _PyFunction_FastCallKeywords () at ../Objects/call.c:408\r\n#14 0x00000000005cc6ea in call_function.lto_priv () at ../Python/ceval.c:4616\r\n#15 0x000000000051add9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3093\r\n#16 0x00000000005b0eac in PyEval_EvalFrameEx (throwflag=0, \r\n    f=Frame 0x7fff6c125048, for file /usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/python/interpreter.py, line 453, in invoke (self=<Interpreter(_model_content=b' \\x00\\x00\\x00TFL3\\x00\\x00\\x00\\x00\\x00\\x00\\x12\\x00\\x1c\\x00\\x04\\x00\\x08\\x00\\x0c\\x00\\x10\\x00\\x14\\x00\\x00\\x00\\x18\\x00\\x12\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00&\\x00\\x00\\xf0 \\x00\\x00\\xd8 \\x00\\x00<\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x0c\\x00\\x00\\x00\\x08\\x00\\x0c\\x00\\x04\\x00\\x08\\x00\\x08\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\t\\x00\\x00\\x00\\x13\\x00\\x00\\x00min_runtime_version\\x00\\n\\x00\\x00\\x00\\x90 \\x00\\x00\\x80\\x00\\x00\\x00p\\x00\\x00\\x00d\\x00\\x00\\x00L\\x00\\x00\\x004\\x00\\x00\\x00,\\x00\\x00\\x00$\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xaa\\xff\\xff\\xff\\x04\\x00\\x00\\x00\\x05\\x00\\x00\\x001.6.0\\x00\\x00\\x00\\xa4\\xda\\xff\\xff\\xa8\\xda\\xff\\xff\\xac\\xda\\xff\\xff\\xca\\xff\\xff\\xff\\x04\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xde\\xff\\xff\\xff\\x04\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xd8\\xda\\xff\\xff\\x04\\x00\\x06\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x...(truncated)) at ../Python/ceval.c:547\r\n#17 function_code_fastcall (globals=<optimized out>, nargs=<optimized out>, args=<optimized out>, co=<optimized out>) at ../Objects/call.c:283\r\n#18 _PyFunction_FastCallKeywords () at ../Objects/call.c:408\r\n#19 0x0000000000516d6a in call_function (kwnames=0x0, oparg=<optimized out>, pp_stack=0x7fffffffd010) at ../Python/ceval.c:4616\r\n#20 _PyEval_EvalFrameDefault () at ../Python/ceval.c:3110\r\n#21 0x00000000005cd202 in PyEval_EvalFrameEx (throwflag=0, \r\n    f=Frame 0xf20238, for file bench.py, line 40, in measure (rep=1, tpu=False, bench_func=<Function(_lock=<_thread.lock at remote 0x7fffe26e7b98>, _python_function=<function at remote 0x7fffe05bf9d8>, _function_spec=<FunctionSpec(_fullargspec=<FullArgSpec at remote 0x7fff74b27c88>, _is_method=False, _default_values=None, _args_to_indices={'a': 0, 'b': 1}, arg_names=['a', 'b'], vararg_name=None, _arg_indices_to_default_values={}, _input_signature=(<TensorSpec at remote 0x7fff74ab6dc8>, <...>), _flat_input_signature=(<...>, <...>)) at remote 0x7ffff6614710>, _autograph=True, _experimental_autograph_options=None, experimental_relax_shapes=False, _created_variables=[], _stateful_fn=<Function(_python_function=<function at remote 0x7fff74ac2158>, _function_spec=<FunctionSpec(_fullargspec=<FullArgSpec at remote 0x7fff74b27cf8>, _is_method=False, _default_values=None, _args_to_indices={'a': 0, 'b': 1}, arg_names=['a', 'b'], vararg_name=None, _arg_indices_to_default_values={}, _input_signature=(...), _flat_input_signature=...(truncated)) at ../Python/ceval.c:547\r\n```\r\n</details>\r\n\r\n`tf.reduce_sum()` does not crash.", "comments": ["@vmarkovtsev, Please paste the code snippet to reproduce the reported issue here. Thanks! ", "@gadagashwini But it's already here.\r\n\r\n![image](https://user-images.githubusercontent.com/2793551/69313816-e3ebc900-0c32-11ea-9730-464a974ed199.png)\r\n", "> @gadagashwini But it's already here.\r\n> \r\n> ![image](https://user-images.githubusercontent.com/2793551/69313816-e3ebc900-0c32-11ea-9730-464a974ed199.png)\r\n\r\nSorry I didn't check. My bad. Thanks!", "Could you please try this on the nightly which you can install with pip install --upgrade tf-nightly (after uninstalling the regular version). It works for me on the nightly but not on the release version.\r\n", "@vmarkovtsev This was resolved in `TF2.2`. Can you please verify once and close the issue. I ran it in `TF2.2` and don't see any error. [Here](https://colab.research.google.com/gist/jvishnuvardhan/5af626d6d5e36951af15f3eb0ee2caac/34444_tfliteconv_with_concretefun.ipynb) is the gist for your reference. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34444\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34444\">No</a>\n"]}, {"number": 34443, "title": "Metrics not added when tf.keras.Model is compiled without loss", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \u2705 \r\n- OS Platform and Distribution : Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0\r\n```\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\nv2.0.0-rc2-26-g64c3d38 2.0.0\r\n```\r\n\r\n- Python version:\r\n```\r\nPython 3.6.8 (default, Jan 14 2019, 11:02:34)\r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\r\n```\r\n\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen compiling a `tf.keras` model without adding a loss, the `metrics` are not added.\r\n\r\n**Describe the expected behavior**\r\n\r\n`metrics` should be computed in any case to give insights about the training as it is based on model output and true value given by the data loader/sequence.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nfrom tensorflow.python.keras.layers import Conv2D, GlobalAveragePooling2D, Input, Dense\r\nfrom tensorflow.python.keras.models import Sequential\r\nfrom tensorflow.python.keras.regularizers import l2\r\n\r\nmodel = Sequential([\r\n    Input((224, 224, 3)),\r\n    Conv2D(256, (3, 3), kernel_regularizer=l2()),\r\n    GlobalAveragePooling2D(),\r\n    Dense(10, activation='sigmoid'),\r\n])\r\n\r\n# Metrics are added\r\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\nprint(model.metrics)\r\n\r\n# Metric are empty\r\nmodel.compile(optimizer='Adam', metrics=['accuracy'])\r\nprint(model.metrics)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have tried on colab with TF version 2.0 , 2.1.0-dev20191120 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/a2743456a47bd50e730f05bc6dea0261/untitled385.ipynb). Thanks!", "@ClementWalter Heres the reason. If the loss function is not defined, then the out put of this function `_prepare_skip_target_masks` will be skipped during total loss calculation and feed targets\r\n    preparation as shown [here](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/engine/training.py#L1655).\r\n\r\nAnd the model loss and weighted metric sub-graphs are compiled together and this is the reason why you don't see metrics as shown [here](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/engine/training.py#L1639) and this should not a hurdle. But we will look into this in future.", "@ClementWalter Closing this issue as this is an expected behavior. Please add additional comments and we can open this issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34443\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34443\">No</a>\n"]}, {"number": 34442, "title": "OOM (out of memory) Without any reason on Tensorflow", "body": "Everytime I try to run deepfacelab cuda 9.2 (the most updated version from a week ago) receive the following:\r\n\r\nUsing TensorFlow backend.\r\nLoading: 100%|######################################################################| 654/654 [00:03<00:00, 196.88it/s]\r\nLoading: 100%|####################################################################| 1491/1491 [00:06<00:00, 238.51it/s]\r\n============= Model Summary =============\r\n==                                     ==\r\n==        Model name: H64              ==\r\n==                                     ==\r\n== Current iteration: 0                ==\r\n==                                     ==\r\n==----------- Model Options -----------==\r\n==                                     ==\r\n==       sort_by_yaw: False            ==\r\n==       random_flip: False            ==\r\n==        lighter_ae: False            ==\r\n==        pixel_loss: False            ==\r\n==        batch_size: 2                ==\r\n==                                     ==\r\n==------------ Running On -------------==\r\n==                                     ==\r\n==      Device index: 0                ==\r\n==              Name: GeForce GTX 1050 ==\r\n==              VRAM: 4.00GB           ==\r\n==                                     ==\r\n\r\nStarting. Press \"Enter\" to stop training and save model.\r\n[10:34:55][#000001][9096ms][3.6420][2.6351]\r\nError: OOM when allocating tensor with shape[2048,1024,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node training/Adam/gradients/model_1/conv2d_5/convolution_grad/Conv2DBackpropFilter}} = Conv2DBackpropFilter[T=DT_FLOAT, _class=[\"loc:@train...kpropInput\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/model_1/conv2d_5/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, ConstantFolding/training/Adam/gradients/model_1/conv2d_5/convolution_grad/ShapeN-matshapes-1, training/Adam/gradients/AddN_23)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Jorge\\Downloads\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\mainscripts\\Trainer.py\", line 109, in trainerThread\r\n    iter, iter_time = model.train_one_iter()\r\n  File \"C:\\Users\\Jorge\\Downloads\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\models\\ModelBase.py\", line 525, in train_one_iter\r\n    losses = self.onTrainOneIter(sample, self.generator_list)\r\n  File \"C:\\Users\\Jorge\\Downloads\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\models\\Model_H64\\Model.py\", line 89, in onTrainOneIter\r\n    total, loss_src_bgr, loss_src_mask, loss_dst_bgr, loss_dst_mask = self.ae.train_on_batch( [warped_src, target_src_full_mask, warped_dst, target_dst_full_mask], [target_src, target_src_full_mask, target_dst, target_dst_full_mask] )\r\n  File \"C:\\Users\\Jorge\\Downloads\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\engine\\training.py\", line 1217, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n  File \"C:\\Users\\Jorge\\Downloads\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2715, in __call__\r\n    return self._call(inputs)\r\n  File \"C:\\Users\\Jorge\\Downloads\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2675, in _call\r\n    fetched = self._callable_fn(*array_vals)\r\n  File \"C:\\Users\\Jorge\\Downloads\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1439, in __call__\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\Jorge\\Downloads\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[2048,1024,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node training/Adam/gradients/model_1/conv2d_5/convolution_grad/Conv2DBackpropFilter}} = Conv2DBackpropFilter[T=DT_FLOAT, _class=[\"loc:@train...kpropInput\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/model_1/conv2d_5/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, ConstantFolding/training/Adam/gradients/model_1/conv2d_5/convolution_grad/ShapeN-matshapes-1, training/Adam/gradients/AddN_23)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\nDone.\r\n\r\n\r\nI have an i5 8300 and a GTX 1050, in this case I was running the H64 on the example videos (the just do it one and the iron man as source) and doesnt make sense that with 4gb of vram it goes OOM on like the (10 - 30) first iterations\r\n\r\nPLease help!\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nRequest you to provide minimal stand alone code to reproduce the issue in our environment. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Also, please use markdown formatting around code/error logs so that they can be read.", "@rabbabot \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 34441, "title": "Updating the guide for building TensorFlow Lite with select ops for iOS", "body": "Hi!\r\n\r\nWe followed the [guide](https://www.tensorflow.org/lite/guide/ops_select#ios), which seems bit outdated as `tensorflow/contrib` has been removed. We couldn't get TFLite model with select ops working on iOS, while the same model is working well on Android. It seems that bazel could also be used for building the library together with using private CocoaPods. We tested this approach but were also unsuccessful. Could the documentation be improved with this regard for having clearer guidelines?\r\n\r\nIdeas and explanations of how to get it working are also welcome under this issue. Do you have an estimation when could we expect the CocoaPods (with-select-ops version) for iOS/Swift?\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/lite/guide/ops_select#ios\r\n\r\n## Description of issue (what needs changing):\r\nThe current guide could be improved, especially because `tensorflow/contrib` is removed in the latest version of TensorFlow. Therefore, `tensorflow/contrib/makefile/build_all_ios_with_tflite.sh` is no longer available for building TensorFlow Lite with select ops support.\r\n\r\n### Clear description\r\nCouldn't get a TFLite model with select ops working in iOS by following the [guide](https://www.tensorflow.org/lite/guide/ops_select#ios). The same model was successfully working on Android with nightly builds. Would be nice to have a guide for how to build it with bazel.\r\n\r\n### Correct links\r\nN/A\r\n\r\n### Parameters defined\r\nN/A\r\n\r\n### Returns defined\r\nN/A\r\n\r\n### Raises listed and defined\r\nN/A\r\n\r\n### Usage example\r\nWould be good if it would be explained in more detail what has to be done. Do we only have to compile and link libraries as explained in the documentation or do we need to modify the code also, e.g. add Flex delegate as an option for Interpreter?\r\n\r\n### Request visuals, if applicable\r\nNo need for visuals\r\n\r\n### Submit a pull request?\r\nN/A", "comments": ["Will update the official document soon to reflect the changes in building TFLite with select ops for iOS.\r\n\r\nIn the meantime, you can try building the following bazel target:\r\n```sh\r\nbazel build --config=ios --ios_multi_cpus=armv7,arm64,x86_64 -c opt \\\r\n  //tensorflow/lite/experimental/ios:TensorFlowLiteCWithSelectTfOps_framework\r\n```\r\n\r\nThe resulting framework can then be used in place of the `TensorFlowLiteC` framework described in this guide: https://www.tensorflow.org/lite/guide/build_ios\r\n\r\nThere's one more step to follow when building an app with this framework. In your Xcode project, go to `Build Settings` tab of your app target configuration, and add `-force_load <path/to/TensorFlowLiteCWithSelectTfOps.framework/TensorFlowLiteCWithSelectTfOps>` to the `Other Linker Flags` setting.\r\n\r\nLet me know if you have any further questions. Will keep this bug open until the official doc is updated.", "Thank you very much! We got it working :)", "@yyoon Hi, Yoon, thanks for your indication, great explaination! And you might want to change the cmdline a little for there is a typo, which causes the build process encounter bazel's \"no such target\" errors, remove the __C__ between TensorFlowLite and SelectTfOps_framework.\r\n\r\n```\r\nbazel build -c opt --config=ios --ios_multi_cpus=armv7,arm64,x86_64 //tensorflow/lite/experimental/ios:TensorFlowLiteSelectTfOps_framework\r\n```", "@tpeet Hi, tpeet, is there any documentation on how to use the big binary library with swift or obj-c? I import it to the project, but do not know where and how to start use it? Many thanks.", "@yyoon Hi, Yoon, could you be more specific about the step at the stage of \"in place of the TensorFlowLiteC framework\"? Do I need to modify the .podsepc file only? or do I need to rewrite the TensorflowLiteSwift's source code, such as change the __import__ of __TensorflowLiteC__ to __TensorflowLiteSelectTfOps__ ?", "Just a note that the [documentation](https://www.tensorflow.org/lite/guide/ops_select) is still not  updated, maybe this issue should be reopened to track this?", "The select TF ops documentation for iOS has been updated (4654b512e70aa11958c0830f5c1fd07dc996745a). Feel free to file a new bug if you encounter any issues."]}, {"number": 34440, "title": "Error while building API docs", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/flags/tf_decorator/tf_stack/FileAndLine\r\n\r\n## Description of issue (what needs changing):\r\nGet a `ValueError` while setting up to view TensorFlow-style HTML locally. (According to https://www.tensorflow.org/community/contribute/docs)\r\n\r\n### Clear description\r\n```\r\nTraceback (most recent call last):\r\n  File \"generate2.py\", line 284, in <module>\r\n    app.run(main)\r\n  File \"C:\\Users\\ASPIRE\\.conda\\envs\\tensorflow2\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\ASPIRE\\.conda\\envs\\tensorflow2\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"generate2.py\", line 280, in main\r\n    search_hints=FLAGS.search_hints)\r\n  File \"generate2.py\", line 273, in build_docs\r\n    doc_generator.build(output_dir)\r\n  File \"C:\\Users\\ASPIRE\\.conda\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_docs\\api_generator\\generate_lib.py\", line 839, in build\r\n    site_path=self._site_path)\r\n  File \"C:\\Users\\ASPIRE\\.conda\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_docs\\api_generator\\generate_lib.py\", line 507, in write_docs\r\n    'Failed to generate docs for symbol: `{}`'.format(full_name))\r\nValueError: Failed to generate docs for symbol: `tf.compat.v1.flags.tf_decorator.tf_stack.FileAndLine`\r\n```", "comments": ["@nikochiko ,\r\nCan you provide code snippet to reproduce the error reported here?Thanks!", "@oanush , this code on the TensorFlow documentation: https://www.tensorflow.org/community/contribute/docs#python_reference\r\n", "@nikochiko Can you please attach a colab gist. I am not running into any error on colab. Thanks!", "I think this is a local error. This seems to work fine: https://colab.research.google.com/gist/nikochiko/2a6d87131461d9bc959f6c4e18138494/test-build-api-docs.ipynb", "@nikochiko i am closing this issue as this issue has been resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34440\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34440\">No</a>\n"]}, {"number": 34439, "title": "Build failure: undefined reference to protobuf symbols #34117 closed issue still occurring on r2.1", "body": "Re: [https://github.com/tensorflow/tensorflow/issues/34117](url)\r\nThis issue is still occurring when I build r2.1 branch with bazel 1.1.0.\r\nIt has been fixed on head.  Would it be possible to implement the fix on build r2.1\r\nAlso the maximum bazel version needs to be increased to 1.1.0 in configure.py once the fix is implemented (in both head and r2.1).", "comments": ["We do not intend to change the Bazel version on r2.1 branch. \r\n@mihaimaruseac as well to confirm. ", "We should only cherry-pick the fix from head but we are too late in the release process to do another bazel version switch.", "@dbonner,\r\nHope the above comments answered your query. Can we close this issue if you have no further queries? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34439\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34439\">No</a>\n"]}, {"number": 34438, "title": "Build libtensorflow-lite.a with android-ndk-r14b and some errors has occurred", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:master\r\n- Python version:3.5.2\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 1.0.1\r\n- GCC/Compiler version (if compiling from source): 4.9.X\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory: I7 + 16G\r\n\r\n\r\n\r\n**Describe the problem**\r\n/tensorflow/lite/kernels/internal/reference/reference_ops.h:1100:36: error: 'round' is not a member of 'std'\r\n               static_cast<int32_t>(std::round(input_ptr[j] * scale + bias)) +\r\n\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:4097:38: error: 'rint' is not a member of 'std'\r\n       const int32_t prob_quantized = std::rint(log_prob) + params.zero_point;\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI had created two files:\r\n ./tensorflow/lite/tools/make/build_android_ndk_lib.sh\r\nThe contents are as follows:\r\n\r\n#############################START#############################\r\n#!/bin/bash\r\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\r\nset -x\r\nset -e\r\n\r\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\r\nTENSORFLOW_DIR=\"${SCRIPT_DIR}/../../../..\"\r\n\r\nmake -j 4 TARGET=android_ndk -C \"${TENSORFLOW_DIR}\" -f tensorflow/lite/tools/make/Makefile\r\n#############################END#############################\r\n\r\n ./tensorflow/lite/tools/make/targets/android_ndk_makefile.inc\r\nThe contents are as follows:\r\n\r\n#############################START#############################\r\n# Settings for generic aarch64 boards such as Odroid C2 or Pine64.\r\nifeq ($(TARGET),android_ndk)\r\n  # The aarch64 architecture covers all 64-bit ARM chips. This arch mandates\r\n  # NEON, so FPU flags are not needed below.\r\n  TARGET_ARCH := armv7-a\r\n  TARGET_TOOLCHAIN_PREFIX := /opt/build_tools/android-ndk-r14b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-\r\n\r\n  CXXFLAGS += \\\r\n    -march=armv7-a \\\r\n    -mfpu=neon \\\r\n    -funsafe-math-optimizations \\\r\n    -ftree-vectorize \\\r\n    -Wall \\\r\n    -std=c++11 \\\r\n    -fPIC\r\n\r\n  CFLAGS += \\\r\n    -march=armv7-a \\\r\n    -mfpu=neon \\\r\n    -funsafe-math-optimizations \\\r\n    -ftree-vectorize \\\r\n    -Wall \\\r\n    -fPIC\r\n\r\n  LDFLAGS := \\\r\n    -Wl,--no-export-dynamic \\\r\n    -Wl,--exclude-libs,ALL \\\r\n    -Wl,--gc-sections \\\r\n    -Wl,--as-needed \\\r\n    -lrt\r\n       \r\n  LIBS := \\\r\n    -lstdc++ \\\r\n    -lpthread \\\r\n    -lm \\\r\n    -ldl\r\n    \r\n  INCLUDES += \\\r\n    -I/opt/build_tools/android-ndk-r14b/platforms/android-21/arch-arm/usr/include \\\r\n    -I/opt/build_tools/android-ndk-r14b/platforms/android-21/arch-arm/usr/include/sys \\\r\n    -I/opt/build_tools/android-ndk-r14b/sources/cxx-stl/gnu-libstdc++/4.9/include \\\r\n    -I/opt/build_tools/android-ndk-r14b/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include  \\\r\n    -I/opt/build_tools/android-ndk-r14b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits \r\n       \r\nendif\r\n##############################END#####################################\r\n\r\nand run  ./tensorflow/lite/tools/make/build_android_ndk_lib.sh \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["r14b has a number of issues, please upgrade to NDK r17c.", "I try to use NDK r17c\uff0c there is still the same err\u3002Do you have any suggestions or experiences to share\uff1f", "We generally discourage use of Makefiles for Android, instead preferring Bazel. Looks like the key difference in your makefile is that we use llvm-libc++ by default, rather than gnu-libstdc++, so I think you'll need to update your LIBS and INCLUDES accordingly. One good way to compare with bazel builds is to configure your Android build (run `configure` in your root TensorFlow checkout), then run `bazel build -s --config=android_arm //tensorflow/lite:libtensorflowlite.so` to see how the compile/link commands compare to what you're doing in the Makefile.", "Thank you very much!  I will finish these comparisons as soon as possible"]}, {"number": 34437, "title": "[Grappler] Fix comparison between node name and input in UpdateConsumers", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nThe input name is always \"node:port\", it could not compare to the node name directly.  @ezhulenev ", "comments": ["@ezhulenev Could you help take a look? Thanks!", "@xinan-jiang Could you please address Ubuntu Sanity errors? Thanks!", "I can not see the log @gbaned\r\nI think this PR will not break sanity case @ezhulenev "]}, {"number": 34436, "title": "Add GPU delegate support to the TFLite experimental c api", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nthe lite experimental c api (used by the unity plugin) currently only supports running the network on the CPU.\r\nIt would be very useful to be able to use the GPU delegate for apps made directly in unity.\r\n\r\n\r\n**Will this change the current api? How?**\r\nIt extends the lite c api to be more in line with the c++ api. \r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to use Tensorflow in unity on modern phones.\r\n", "comments": ["@stephen-systemfriend \r\n\r\nWe made sure that `//tensorflow/lite/delegates/gpu/delegate.h` stays compatible with C.  I need a little bit of elaboration.  Are you requesting an API beyond the TFLite delegate?", "I want to access the gpu delegate from some C# code. I'm currently using the Unity plugin, and was hoping to extend it to use the gpu delegate as well, but it didn't seem to be supported. I'll attempt to use the delegate as is and get back to you if it doesn't work.", "Ah, I always forget Unity is in C#.\r\n\r\n@jdduke \r\n\r\nDo we have an official strategy on C# usage?", "We do support C# and Unity for CPU inference. We'd have to do some more investigation about GPU delegate work, though. Unity's render thread and context are likely unavailable on the sim thread, so we'd need to do some testing first to see how well this would work.", "with the changes in https://github.com/stephen-systemfriend/tensorflow/commit/5c8659b0b61b2d6789119e18c231c8d6cb5c5d47 it seems to run fine on android (though the SSD net I wanted to use has some custom ops that currently get in the way). Can I make a pull request/what kind of tests should I run first?", "I would apply the delegate, try running the example model in the sample Unity app (I believe the operator should be supported by the GPU), check the adb logs and make sure you see something about GPU delegate initialization.", "@stephen-systemfriend,\r\nSorry for the delayed response. It looks like [TensorFlow Lite GPU delegate](https://www.tensorflow.org/lite/performance/gpu) has been implemented. Can you please check and confirm the same? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34435, "title": "Significantly slow training with TFRecords when the labels are one-hot encoded", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: K80\r\n\r\n**Describe the current behavior**\r\n\r\nI am incorporating TFRecords along with all the `tf.data` legacy functions in my projects. I primarily work with images. I am trying to develop a data input pipeline for an image classification project with the **Flowers-17** dataset. My aim is to first serialize the entire dataset in form TFRecords with multiple shards. I have been able to do that. I am then reading those TFRecords, preprocessing them, augmenting them and finally, I am feeding them to a model. \r\n\r\nNow, I ran two versions of the above experiment:\r\n- One where the image labels were simply encoded using indexing\r\n- Another one where the image labels were one-hot encoded\r\n\r\nWhile compiling the model for the above two scenarios I supplied the loss functions accordingly as well. It now seems that the first experiment where the model **was not** supplied one-hot encoded class labels is training _much faster_ than the other one. \r\n\r\n**Describe the expected behavior**\r\n\r\nModel training time should be closely equal in both the cases I mentioned. \r\n\r\n**Code to reproduce the issue**\r\n- [Colab notebook](https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/tf.data%20exploration/TFRecords_with_tf_data_Advanced.ipynb) for where one-hot encoding wasn't done. \r\n- [Colab notebook](https://colab.research.google.com/drive/1CuOZJNlWsXdGlhCCfZ14PvSEU2Gtcrdf) with one-hot encoding. ", "comments": ["@sayakpaul,\r\nHi,when tried running both the colab gist's,model training time were almost same. Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/6e9acb4750eb1c1227d4b500015d55a8/copy-of-tfrecords_with_tf_data_advanced-one-hot.ipynb) of colab for one-hot encoding. Thanks!"]}, {"number": 34434, "title": "tensorflow is not using GPU", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:None\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version:tensorflow-gpu==1.14\r\n- Python version:3.6.8\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):none\r\n- GCC/Compiler version (if compiling from source):none\r\n- CUDA/cuDNN version:10.1/7.6.4\r\n- GPU model and memory:GeForce GTX 1060 6GB and 6144 MB\r\n\r\n\r\n\r\n**Describe the problem** my problem is that my gpu is not being used during training \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI have run the following command for training my model\r\npython -u \"DeepSpeech.py\"\\\r\n    --train_files \"/home/sehar/urdu/cv-valid-train.csv\"\\\r\n    --dev_files \"/home/sehar/urdu/cv-valid-dev.csv\" \\\r\n    --test_files \"/home/sehar/urdu/cv-valid-test.csv\" \\\r\n    --alphabet_config_path \"/home/sehar/urdu-models/alphabet.txt\" \\\r\n    --lm_binary_path \"/home/sehar/urdu-models/lm.binary\" \\\r\n    --lm_trie_path \"/home/sehar/urdu-models/trie\" \\\r\n    --learning_rate 0.000025 \\\r\n    --dropout_rate 0\\\r\n    --log_level 1 \\\r\n    --noearly_stop \\\r\n    --epochs 100 \\\r\n    --max_to_keep 1 \\\r\n    --checkpoint_dir \"/home/sehar/DeepSpeech/check\" \\\r\n    --export_dir \"/home/sehar/urdu-models\"\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please switch to cuda 10.0 for gpu support\r\nSee [software requirements](https://www.tensorflow.org/install/gpu#software_requirements)", "@sehargul-123 \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 34433, "title": "r2.0/2.1 Python 3.8 AutoGraph could not transform <bound method LinearRegressionTF.fit of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7a5ccc1fa0>> and will run it as-is", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (attached file code_warning_py38.py.txt -> rename it as .py and execute it)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): This problem occurs with both r2.0 (built from head) and r2.1rc0\r\n- Python version: 3.8 virtual environment (The problem does not occur with Python 3.7.3)\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: CUDA 10 / cuDNN 7.6.5\r\n- GPU model and memory: Nvidia Geforce RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nexport AUTOGRAPH_VERBOSITY=10\r\nRun the attached python script (renamed as .py)\r\npython code_warning_py38.py &> output.txt\r\nRead the output in the attached output.txt\r\n\r\n**Describe the expected behavior**\r\nThere is no warning message when the script is run with Python 3.7.3\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nSee both files attached.\r\n[code_warning_py38.py.txt](https://github.com/tensorflow/tensorflow/files/3867410/code_warning_py38.py.txt)\r\n[output.txt](https://github.com/tensorflow/tensorflow/files/3867412/output.txt)\r\n", "comments": ["@dbonner, Thanks for reporting the issue. Please provide the input data file to reproduce the reported issue. Thanks!", "@gadagashwini \r\n[cal_house.json.gz](https://github.com/tensorflow/tensorflow/files/3873037/cal_house.json.gz)\r\nSorry, I forgot to do this.  Here it is.\r\nRegards,\r\nDan", "Works without warning or error with Tf 2.0 and Python 3.6.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/850e1f79b50a8847c2fff9e42aa2b4bf/untitled274.ipynb). Thanks!", "@gadagashwini and @jvishnuvardhan \r\nI expected Python 3.6 with TF2.0 to work without warning or error.\r\nI believe this problem only occurs with Python 3.8 with TF2.0 built from source using Python 3.8.\r\nAll the best,\r\nDaniel", "I'm looking into this, but am a bit swamped today. If you have the chance, could you check the version of `gast` you have installed?", "Hi Dan,\ngast==0.2.2\nCheers\n\nOn Thu, 5 Dec 2019 at 01:59, Dan Moldovan <notifications@github.com> wrote:\n\n> I'm looking into this, but am a bit swamped today. If you have the chance,\n> could you check the version of gast you have installed?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34433?email_source=notifications&email_token=AAB26QXHEPMOUDV47WAHBJDQW7AUFA5CNFSM4JPNL272YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEF5JVBI#issuecomment-561683077>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAB26QSSYU74LWBQPDBBHC3QW7AUFANCNFSM4JPNL27Q>\n> .\n>\n", "Thanks, that saved me a few roundtrips.\r\n\r\nI ran a build with 3.8, and it looks like the AST transformations need to be updated to work with Python 3.8, I suspect that's because of the grammar changes. I'll make the necessary fixes, hopefully it won't take more than a week or two to finish them.", "Update - after https://github.com/tensorflow/tensorflow/commit/a396eb4a3a7dc08b859df42919fcc7bdf8236a01, 3.8 syntax should be fully supported. I'm re-checking the script to see if anything else remains.", "Looks like astor is not compatible with 3.8, either, so we'll need to replace that dependency. Fortunately it's fairly straightforward to swap in something else like astunparse.", "Progress update - upgrade to astunparse is done, awaiting new release of gast which fixes a bug with FunctionDef nodes in 3.8 (see https://github.com/serge-sans-paille/gast/pull/43).", "This should now be fixed at head. With the pip package from source, running code_warning_py38.py should no longer print a warning.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34433\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34433\">No</a>\n", "I can't test the resolution until I build tensorflow for python 3.8.0.  Unfortunately my build is failing (see issue #36170 )", "I got the build to work.  Yes, this is fixed."]}, {"number": 34432, "title": "Model mismatch between create_low_latency_conv_model and cnn-one-fstride4", "body": "https://github.com/tensorflow/tensorflow/blob/ba63d9082a2265da91ec4daefecfa4cd47fcf07f/tensorflow/examples/speech_commands/models.py#L337-L339\r\n\r\nBut the stride is actually **one** instead of **four**:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ba63d9082a2265da91ec4daefecfa4cd47fcf07f/tensorflow/examples/speech_commands/models.py#L388-L389\r\n\r\nwhich results in a **huge** increase of the number of parameters of the subsequent FC layer.", "comments": ["@csukuangfj \r\n\r\nCan you please let us know which TensorFlow version you are using.Will it be possible to share related code, it helps us in localizing the issue faster. Thanks!", "@ravikyram \r\nthe links in the above issue comment come from the latest master branch.", "@csukuangfj\r\nIs this still an issue.", "@Saduf2019 \r\n\r\nSorry, I am no longer working on it and I am not able to set up an environment to verify it.", "@csukuangfj\r\nIn that case can you please move this to close status."]}, {"number": 34431, "title": "AttributeError: module 'tensorflow' has no attribute 'app'", "body": "Hi, I'm new in the field of machine learning, and I started watching a video about how to classify images\r\nVideo: https://www.youtube.com/watch?v=EKe05rMG-Ww\r\n\r\n\r\nbut in a part when I run: python csv_a_tf.py --csv_input=CSV/test.csv --output_path=TFRecords/test.record --images=images\r\n\r\n\r\nthis gives me back:\r\n----------------------------------------\r\nTraceback (most recent call last):\r\n  File \"csv_a_tf.py\", line 24, in <module>\r\n    flags = tf.app.flags\r\nAttributeError: module 'tensorflow' has no attribute 'app'\r\n----------------------------------------\r\nany ideas?\r\n\r\n", "comments": ["Are you using tensorflow version 1.x or tensorflow version 2.x, in this tensorflow 2 guide, https://www.tensorflow.org/guide/effective_tf2, it says `tf.app` is removed. Therefore I think the code in `csv_a_tf.py` is using `tf 1.x` version and the tensorflow in your environment in 2.x. I hope this helps!\r\n\r\nTo resolve, either uninstall tensorflow 2.x and then install 1.x, or modify the code so that it uses 2.x API :)", "Thanks it has helped me a lot :D", "Can you tell me how solved this AttributeError:module 'tensorflow' has no attribute 'app'.", "Like what @shun-lin said, 'app' is removed from tensorflow 2.x. So, check your tensorflow version if it is 2.x then you either have to install a tensorflow 1.x version or modify your code to support the 2.x version.", "> Are you using tensorflow version 1.x or tensorflow version 2.x, in this tensorflow 2 guide, https://www.tensorflow.org/guide/effective_tf2, it says `tf.app` is removed. Therefore I think the code in `csv_a_tf.py` is using `tf 1.x` version and the tensorflow in your environment in 2.x. I hope this helps!\r\n> \r\n> To resolve, either uninstall tensorflow 2.x and then install 1.x, or modify the code so that it uses 2.x API :)\r\n\r\napt solution ", "Try to use: `import tensorflow.compat.v1 as tf\r\n`\r\n", "To resolve this issue I uninstall tensorflow : pip uninstall tensorflow     (if install tf 2 version)\r\n                                       and\r\nInstall tensorflow :  pip install tensorflow==1.15 ", "Use abseil instead: See https://abseil.io/docs/python/quickstart", "Replace tf.app with tf.compat.v1.app", "If I do this - Replace tf.app with tf.compat.v1.app it says AttributeError: module 'tensorflow_core._api.v2.compat.v1.app' has no attribute 'DEFINE_string'\r\n\r\n", "> Replace tf.app with tf.compat.v1.app\r\n\r\nIt worked for me but later on I got tf.contrib error. Even I tried on Google Colab, tf.contrib error is same.\r\n\r\nSo finally on my local machine, I downgraded the tensorflow to 1.15.x using\r\npip uninstall tensorflow\r\npip install tensorflow==1.15.2\r\n\r\n\r\nFor Google Colab, I used \r\n\r\n%tensorflow_version 1.x\r\nimport tensorflow as tf\r\n", "If I don't want to uninstall tensorflow==2.1 then how should I progress further.\r\nThere is any other option.", "For anyone reading this who doesn't want to ditch Tensorflow 2, the Effective Tensorflow 2 API changes are listed [here](https://www.tensorflow.org/guide/effective_tf2).\r\n\r\nThat page has this sentence:\r\n\r\n> Some of the major changes include removing tf.app, tf.flags, and tf.logging in favor of the now open-source absl-py\r\n\r\nand links you to [this project](https://github.com/abseil/abseil-py)\r\n\r\nwhich says you can install with this `pip install absl-py`.\r\n\r\nThe README for absl-py also links to [an example application](https://github.com/abseil/abseil-py/blob/master/smoke_tests/sample_app.py) which shows what needs to change in your code.\r\n\r\nAlternatively, you can just run `tf_upgrade_v2` against your code as detailed [here](https://www.tensorflow.org/guide/upgrade).", "\u4eb2\u6d4b\uff0c1.14\u7248\u672c\u7684\u4e0d\u884c", "If you're using TensorFlow 2 there's a very easy solution.\r\n\r\nFirst make sure you have abseil installed https://github.com/abseil/abseil-py\r\n\r\nIn your code, `import absl`\r\n\r\nMany resources are identical between absl and tf.app.\r\n\r\nSo if, for example, you replace `tf.app.flags` for `absl.flags`, it still runs.\r\n\r\nRead more here: https://abseil.io/docs/python/guides/app\r\n", "So the solution is to \"use a third party library because we removed ours?\"\r\nThis is nowhere close to a solution, what if -for instance- anyone has to use an existing generate_tfRecord.py? Perhaps into a cloab notebook that pulls it from github\r\n", "If anyone has to use an existing generate_tfrecord.py that is incompatible\nwith the TensorFlow version they are using, but are also impeded from\ndowngrading their Tensorflow version as other comments have suggested here,\nbut also refuse to use a third party open-source library to patch that case\nof incompatibility, then my final suggestion would be to look in other\nthreads for a different existing generate_tfrecord script, that is\ncompatible with their TensorFlow version.\n\nOn Wed, May 19, 2021 at 1:10 PM TheManjia ***@***.***> wrote:\n\n> So the solution is to \"use a thirdy part library because we removed ours?\"\n> This is nowhere close to a solution, what if anyone has to use an existing\n> generate_tfRecord.py?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34431#issuecomment-844251512>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQDEDJHNWYVSHKAGQMR24UTTOPPGHANCNFSM4JPMIM7Q>\n> .\n>\n", "Indeed to go look somewhere else for a solution is not a valid solution either\r\n\r\nWhen a feature that worked before stops working it is called a [software regression](https://en.wikipedia.org/wiki/Software_regression), and its something that should be avoided in software development, please keep this in mind for the future", "> Try to use: `import tensorflow.compat.v1 as tf `\r\n\r\nThank you!\r\nThat was the solution!", "> Replace tf.app with tf.compat.v1.app\r\n\r\nit does not work", "> For anyone reading this who doesn't want to ditch Tensorflow 2, the Effective Tensorflow 2 API changes are listed [here](https://www.tensorflow.org/guide/effective_tf2).\r\n> \r\n> That page has this sentence:\r\n> \r\n> > Some of the major changes include removing tf.app, tf.flags, and tf.logging in favor of the now open-source absl-py\r\n> \r\n> and links you to [this project](https://github.com/abseil/abseil-py)\r\n> \r\n> which says you can install with this `pip install absl-py`.\r\n> \r\n> The README for absl-py also links to [an example application](https://github.com/abseil/abseil-py/blob/master/smoke_tests/sample_app.py) which shows what needs to change in your code.\r\n> \r\n> Alternatively, you can just run `tf_upgrade_v2` against your code as detailed [here](https://www.tensorflow.org/guide/upgrade).\r\n\r\nThank you so much for your comments! I finally solve the problem by using the abseil package. Specifically, I use the following:\r\n\r\n```\r\nfrom absl import app\r\nfrom absl import flags\r\n\r\nFLAGS = flags.FLAGS\r\n```\r\n", "> > Replace tf.app with tf.compat.v1.app\r\n> \r\n> It worked for me but later on I got tf.contrib error. Even I tried on Google Colab, tf.contrib error is same.\r\n> \r\n> So finally on my local machine, I downgraded the tensorflow to 1.15.x using pip uninstall tensorflow pip install tensorflow==1.15.2\r\n> \r\n> For Google Colab, I used\r\n> \r\n> %tensorflow_version 1.x import tensorflow as tf\r\n\r\nthanks somuch"]}, {"number": 34430, "title": "Speech_command validation accuracy is very low,just 8.3%,Why?", "body": "Hi ALL , I download from this repo. Run the example of speech_command.However, I find the validation accuracy just is 8.3%. I run the model by pycharm and the edition of tensorflow is 2.0.0b.\r\n\r\n", "comments": ["\u4e0d\u8981\u6c89\uff0c\u6211\u81ea\u5df1\u9876\u4e00\u4e0b\uff0c\u9876\uff0c\u9876", "Hi!\r\n\r\nMay you update the tensorflow version to 2.0.0 in your environment and check the accuracy again? If the problem persists let me know! Happy to help.", "> Hi!\r\n> \r\n> May you update the tensorflow version to 2.0.0 in your environment and check the accuracy again? If the problem persists let me know! Happy to help.\r\n\r\nI run the model on two computers ,one the tensorflow is 1.14.0 , another is 2.0.0b.The result of two computers is same.Validation accuracy = 8.3% (**by the model of conv**). the confusion matrix is:\r\nINFO:tensorflow:Confusion Matrix:\r\n [[371   0   0   0   0   0   0   0   0   0   0   0]\r\n [371   0   0   0   0   0   0   0   0   0   0   0]\r\n [397   0   0   0   0   0   0   0   0   0   0   0]\r\n [406   0   0   0   0   0   0   0   0   0   0   0]\r\n [350   0   0   0   0   0   0   0   0   0   0   0]\r\n [377   0   0   0   0   0   0   0   0   0   0   0]\r\n [352   0   0   0   0   0   0   0   0   0   0   0]\r\n [363   0   0   0   0   0   0   0   0   0   0   0]\r\n [363   0   0   0   0   0   0   0   0   0   0   0]\r\n [373   0   0   0   0   0   0   0   0   0   0   0]\r\n [350   0   0   0   0   0   0   0   0   0   0   0]\r\n [372   0   0   0   0   0   0   0   0   0   0   0]]\r\nI1120 12:58:16.983035 139977653700352 train.py:288] Confusion Matrix:\r\n [[371   0   0   0   0   0   0   0   0   0   0   0]\r\n [371   0   0   0   0   0   0   0   0   0   0   0]\r\n [397   0   0   0   0   0   0   0   0   0   0   0]\r\n [406   0   0   0   0   0   0   0   0   0   0   0]\r\n [350   0   0   0   0   0   0   0   0   0   0   0]\r\n [377   0   0   0   0   0   0   0   0   0   0   0]\r\n [352   0   0   0   0   0   0   0   0   0   0   0]\r\n [363   0   0   0   0   0   0   0   0   0   0   0]\r\n [363   0   0   0   0   0   0   0   0   0   0   0]\r\n [373   0   0   0   0   0   0   0   0   0   0   0]\r\n [350   0   0   0   0   0   0   0   0   0   0   0]\r\n [372   0   0   0   0   0   0   0   0   0   0   0]]\r\nINFO:tensorflow:Step 2400: Validation accuracy = 8.3% (N=4445)\r\n\r\nHowever, **by the model of single_fc** \uff0cthe confusion matrix is able to present a good validation result.\r\n", "Hi, \r\n\r\nSo the confusion matrix above indicates that every data points is been classified as the same class (that's why you see a column with numbers and all others are zeros). There may be many cause to this but one cause may be `layer` in the model the weights all become 0 and thus the output for rest of the models all become 0 as well (and as you can see from the confusion matrix that all points are predicted class 0). It also looks like you have training it for many steps (2400). May you provide me with which command you put into your terminal and if you have modify the code?", "> Hi,\r\n> \r\n> So the confusion matrix above indicates that every data points is been classified as the same class (that's why you see a column with numbers and all others are zeros). There may be many cause to this but one cause may be `layer` in the model the weights all become 0 and thus the output for rest of the models all become 0 as well (and as you can see from the confusion matrix that all points are predicted class 0). It also looks like you have training it for many steps (2400). May you provide me with which command you put into your terminal and if you have modify the code?\r\n\r\nI just download from the repo of tensorflow by way of the zip file. I don't change any code.Just try to run it", "> Hi,\r\n> \r\n> So the confusion matrix above indicates that every data points is been classified as the same class (that's why you see a column with numbers and all others are zeros). There may be many cause to this but one cause may be `layer` in the model the weights all become 0 and thus the output for rest of the models all become 0 as well (and as you can see from the confusion matrix that all points are predicted class 0). It also looks like you have training it for many steps (2400). May you provide me with which command you put into your terminal and if you have modify the code?\r\n\r\nWhen I debug the code, I find that  the all value of valid_fingerprint is not zero. ", "> Hi,\r\n> \r\n> So the confusion matrix above indicates that every data points is been classified as the same class (that's why you see a column with numbers and all others are zeros). There may be many cause to this but one cause may be `layer` in the model the weights all become 0 and thus the output for rest of the models all become 0 as well (and as you can see from the confusion matrix that all points are predicted class 0). It also looks like you have training it for many steps (2400). May you provide me with which command you put into your terminal and if you have modify the code?\r\n\r\nI run the code by the 'run' buttom of  pycharm.", "I have encountered the same issue...", "I get ```Step 18000: Validation accuracy = 88.1%``` in TF 1.15.0\r\nDid you change training steps when executing `train.py`?", "Closing this issue for now. Feel free to reopen if have any questions. Thanks!"]}, {"number": 34429, "title": "build failed with cuda 10.2", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7.5 x64\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 1.1.0 x64\r\n- GCC/Compiler version (if compiling from source):  7.4.0\r\n- CUDA/cuDNN version: 10.2 / 7.6.5\r\n- GPU model and memory: GTX1080Ti GDDR5X 11GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n```\r\nERROR: /home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/external/nccl_archive/BUILD.bazel:53:1: fatbinary external/nccl_archive/device_dlink_hdrs.fatbin failed (Exit 1)\r\nfatbinary fatal   : Unknown option '-bin2c-path'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/vai/repo/tensorflow/tensorflow/python/tools/BUILD:81:1 fatbinary external/nccl_archive/device_dlink_hdrs.fatbin failed (Exit 1)\r\n```", "comments": ["after set nccl version manually to 2.5.6, above error doen't appear, instead, another error as below came\r\n\r\n```\r\nERROR: /home/vai/repo/tensorflow/tensorflow/python/keras/api/BUILD:115:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\n2019-11-20 13:15:41.004621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\r\nTraceback (most recent call last):\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 771, in <module>\r\n    main()\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 767, in main\r\n    lazy_loading, args.use_relative_imports)\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 625, in create_api_files\r\n    api_version, compat_api_versions, lazy_loading, use_relative_imports)\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 502, in get_api_init_text\r\n    _, attr = tf_decorator.unwrap(attr)\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 219, in unwrap\r\n    elif _has_tf_decorator_attr(cur):\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 124, in _has_tf_decorator_attr\r\n    hasattr(obj, '_tf_decorator') and\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/home/vai/anaconda3/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 28, in <module>\r\n    _wrap_py_utils = swig_import_helper()\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)\r\n  File \"/home/vai/anaconda3/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/vai/anaconda3/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 696, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN10tensorflow3Env7DefaultEv\r\n----------------\r\nNote: The failure of target //tensorflow/python/keras/api:create_tensorflow.python_api_1_keras_python_api_gen_compat_v1 (with exit code 1) may have been caused by the fact that it is a Python 2 program that was built in the host configuration, which uses Python 3. You can change the host configuration (for the entire build) to instead use Python 2 by setting --host_force_python=PY2.\r\n\r\nIf this error started occurring in Bazel 0.27 and later, it may be because the Python toolchain now enforces that targets analyzed as PY2 and PY3 run under a Python 2 and Python 3 interpreter, respectively. See https://github.com/bazelbuild/bazel/issues/7899 for more information.\r\n----------------\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/vai/repo/tensorflow/tensorflow/python/tools/BUILD:141:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\n```", "Can you downgrade to cuda 10.0 and see how it progress.Thanks!", "@ravikyram Cuda 10.0 and 10.1 are supported and have been for a fair while now, this looks new.\r\n\r\n@yifeif have you tried cuda 10.2 yet?", "also failed build of version 1.15.0 with cuda-10.2\r\n\r\nfatbinary fatal   : Unknown option '-bin2c-path'\r\n\r\nhere is a comparison of \"fatbinary --help\" output from cuda-10.0 and cuda-10.2\r\n\r\nhttps://pastebin.com/E1SXpuqh\r\n\r\nseems that there is no '-bin2c-path' option anymore\r\n\r\n", "I was able to successfully compile on v2.0.0 with cuda-10.2.\r\n\r\nI had the same error. My workaround was to remove the line `\"--bin2c-path=%s\" % bin2c.dirname,` from the file `third_party/nccl/build_defs.bzl.tpl`\r\n\r\nseems the '-bin2c-path' option is not required anymore.", "\r\n\r\n\r\n\r\n> I was able to successfully compile on v2.0.0 with cuda-10.2.\r\n> \r\n> I had the same error. My workaround was to remove the line `\"--bin2c-path=%s\" % bin2c.dirname,` from the file `third_party/nccl/build_defs.bzl.tpl`\r\n> \r\n> seems the '-bin2c-path' option is not required anymore.\r\n\r\nI've removed this, but now am getting a failure with it unable to find cupti.h.  I'm on the r2.0 branch though, as the last time I tried master I had other non-CUDA issues that failed the build.\r\n\r\nWhich branch did you use for this (r2.0 or the v2.0.0 tag?), and did you see this error at all?\r\n\r\nThanks!", "@themikem \r\nuse master, not r2.0", "Here are the changes that were necessary to get r2.0 branch to build w/ my setup:\r\n\r\nhttps://github.com/themikem/tensorflow/commit/cc5645f28312388d884a88b85b65e24f68488fb7", "Here are today's list\r\n\r\npython 3.8\r\ncuda 10.2\r\ngast 0.3.2\r\nbazel 1.1\r\n\r\n\r\nwill 2.1.0-rc.1 be okay all of those?", "@alanpurple 2.1.0-rc1 is not yet released. With 2.1-rc0? No.\r\n\r\nFor Python 3.8 support, there also needs to be gprcio 3.8 wheels available, https://github.com/grpc/grpc/issues/20831 (especially affecting Windows where grpcio builds are very complicated).\r\n\r\ncuda 10.2 - there's a recent work done on cuda 10.1: https://github.com/tensorflow/tensorflow/tree/4f11eb8e5389ffffa5d0e0ee9e1071b752564df4/third_party/toolchains/preconfig/ubuntu16.04/cuda10.1-cudnn7 that might land soon. I've also noticed a source saying tensorflow might eventually skip cuda 10.1 and move straight to 10.2/whatever is next, but I can no longer find the reference.\r\n\r\nbazel - master works with bazel 1.1.0, but not with bazel 1.2.1. Indeed, rc0 still requires 0.29.1 or lower. I'd just use whatever is required, there is no reason to stress over getting it upgraded with apt.\r\n```bash\r\napt-get install -y --no-install-recommends \\\r\n   bash-completion \\\r\n   g++ \\\r\n   zlib1g-dev \\\r\n && curl -LO \"https://github.com/bazelbuild/bazel/releases/download/0.29.1/bazel_0.29.1-linux-x86_64.deb\" \\\r\n && dpkg -i bazel_*.deb\r\n```\r\nThere's also a single static fat binary version of bazel without the need for apt.\r\n\r\nFrom a pragmatic perspective, the python 3.8 issue is the only \"real\" problem that affects the daily use and productivity. But yes, having cuda 10.2 would be very nice as it brings quite a few performance improvements over 10.0.", "@ahtik \r\nthank you very much", "PhilippJKratzer's comment pointed me into the right direction. Just removing the line containing `--bin2c-path` didn't help. Without any deeper understanding of the problem I modified the original line \r\n ` \"--link --bin2c-path $$(dirname $(location %s)) \" % bin2c +`\r\nin tensorflow release 1.13.2 to  \r\n  ``\"--link `cat %s >/dev/null` \" % bin2c +``\r\nto get rid of the extra command line argument. That obviously did the trick to compile tensorflow for cuda 10.2 and libcudnn7_7.6.5.32 on ubuntu 18.04.3. (I had also to fix some symbolic links to cuda libraries; the resulting tensorflow package just completed the MRPC task on Google BERT as a first test)\r\n", "@ahtik \r\n\r\nnow grpc support python 3.8 in windows 10!!  good news\r\n\r\nhttps://pypi.org/project/grpcio/1.26.0rc1/#files", "@alanpurple \r\n> @ahtik\r\n> now grpc support python 3.8 in windows 10!! good news\r\n> https://pypi.org/project/grpcio/1.26.0rc1/#files\r\n\r\nYes! Windows 10, grpcio==1.26.0rc1, Python 3.8 and Cuda 10.1 compiles and runs perfectly fine now without any tweaks with tensorflow v2.1.0-rc0.\r\n\r\nIt is worth mentioning that although tensorflow works now fine with Python 3.8, there are closely related projects like Hugging Face Transformers with it's own upstream and downstream dependencies waiting for the wheels.\r\n\r\nThe scope of this issue was cuda 10.2, but for now I'd still stay on cuda 10.0 or 10.1. Looking at the source and recent changes, simply removing bin2c could have it's own less visible consequences not worth the risk.", "still failed with tensorrt,  no problem without tensorrt", "> still failed with tensorrt, no problem without tensorrt\r\n\r\nI was able to get rid of tensorrt issue by installing 6.0.1 \r\n\r\nhttps://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.html\r\n\r\ni am downgrading to cuda 10.1 for v2.1 , but surprisingly i was using v2.0-gpu with cuda 10.2 till now ", "\r\n@alanpurple \r\n\r\nExactly the same **ERROR** from me:\r\n\r\n\r\n```console\r\nERROR: ~/....../tensorflow/python/keras/api/BUILD:130:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 776, in <module>\r\n    main()\r\n  File \"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 772, in main\r\n    lazy_loading, args.use_relative_imports)\r\n  File \"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 629, in create_api_files\r\n    compat_api_versions, lazy_loading, use_relative_imports)\r\n  File \"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 503, in get_api_init_text\r\n    _, attr = tf_decorator.unwrap(attr)\r\n  File \"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 219, in unwrap\r\n    elif _has_tf_decorator_attr(cur):\r\n  File \"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 124, in _has_tf_decorator_attr\r\n    hasattr(obj, '_tf_decorator') and\r\n  File \"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 28, in <module>\r\n    _wrap_py_utils = swig_import_helper()\r\n  File \"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 684, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: ~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: ~/....../tensorflow/python/tools/BUILD:81:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)\r\nINFO: Elapsed time: 128.286s, Critical Path: 26.37s\r\nINFO: 306 processes: 306 local.\r\nFAILED: Build did NOT complete successfully\r\n\u279c  tensorflow git:(master) \u2717 \r\n```\r\n\r\n", "Just piping in with my own experience since there seem to be several people reporting errors even with the removal of the line `\"--bin2c-path=%s\" % bin2c.dirname,`.\r\nFor me, removing this line addressed the issue. I was able to complete the tensorflow build (version 2.1.0) with no other issues. So I suspect the other issues being reported are likely unrelated.", "This problem also occurs in Windows 10 when build tensorflow 2.0 or tensorflow 2.1 whether the branch is master or r2.1.\r\n\r\n- Windows 10 Pro x64 18362\r\n- Visual Studio 2017\r\n- cuda 10.2, cudnn 7.6\r\n- Python 2.7 or 3.5~3.8", "\r\n@vistart \r\nWhat is the solution tho? ", "> @vistart\r\n> What is the solution tho?\r\n\r\nSorry, I don't have any good way for now, I can only wait for the official update.", "It works for me with the master branch of TF.", "this issue has been resolved at some point, closing", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34429\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34429\">No</a>\n", "I used this \"fix\" with tensorflow 1.14 and cuda 10.2. It compiled and installed ok, but I cannot run previously working models. I get an out of memory error, or a failed to load CUDNN error. Seems somewhat random which one will occur.\r\n\r\nthe crashing only happened once, now it seems as solid as the 10.1 version.", "For the record, it was fixed in this commit:\r\n\r\ncommit 67edc16326d6328e7ef096e1b06f81dae1bfb816\r\nAuthor: Sami <skama@nvidia.com>\r\nDate:   Fri Dec 6 08:49:20 2019\r\n\r\n    Make nccl bindings compilable with cuda 10.2\r\n", "> I used this \"fix\" with tensorflow 1.14 and cuda 10.2. It compiled and installed ok, but I cannot run previously working models. I get an out of memory error, or a failed to load CUDNN error. Seems somewhat random which one will occur.\r\n> \r\n> the crashing only happened once, now it seems as solid as the 10.1 version.\r\n\r\nSomehow, with new cuda, you need to set TF_FORCE_GPU_ALLOW_GROWTH=true to solve the \"failed to load CuDNN issue\".", "> For the record, it was fixed in this commit:\r\n> \r\n> commit [67edc16](https://github.com/tensorflow/tensorflow/commit/67edc16326d6328e7ef096e1b06f81dae1bfb816)\r\n> Author: Sami [skama@nvidia.com](mailto:skama@nvidia.com)\r\n> Date: Fri Dec 6 08:49:20 2019\r\n> \r\n> ```\r\n> Make nccl bindings compilable with cuda 10.2\r\n> ```\r\n\r\nThe fix could be cherry-picked / backported into the 1.15 line. Will TF 1.15 continue to get updates like this to keep it supported as time goes on as the last version of TF1?", "Not really. Release branches only get security patches and build fixes on the same toolchain. This seems to change cuda toolchain, which I don't think is low risk enough to warrant cherry-picking"]}, {"number": 34428, "title": "Multiple public header files are not usable. Dependencies are missing.", "body": "Hi,\r\n\r\nThe following files import header files from path which are not existing:\r\n\r\n```bash\r\n$ cd /usr/local/lib/python3.6/dist-packages/tensorflow/include/\r\n$ grep -rnw '.' -e 'third_party/gpus/cuda'\r\n\r\n./tensorflow/stream_executor/gpu/gpu_types.h:31:#include \"third_party/gpus/cuda/include/cuComplex.h\"\r\n./tensorflow/stream_executor/gpu/gpu_types.h:32:#include \"third_party/gpus/cuda/include/cuda.h\"\r\n./tensorflow/core/util/gpu_kernel_helper.h:22:#include \"third_party/gpus/cuda/include/cuda_fp16.h\"\r\n./tensorflow/core/util/gpu_device_functions.h:34:#include \"third_party/gpus/cuda/include/cuComplex.h\"\r\n./tensorflow/core/util/gpu_device_functions.h:35:#include \"third_party/gpus/cuda/include/cuda.h\"\r\n```\r\n\r\nThis is problematic because the headers can't be imported as the path requested does not exist:\r\n```bash\r\n$ ls /usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/gpus/cuda/include/\r\nls: cannot access '/usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/gpus/cuda/include/': No such file or directory\r\n```\r\n\r\nProblem was caused by this commit: https://github.com/tensorflow/tensorflow/commit/922386b9fcc23596877da3500787a045a861cb52#diff-3146a2ef48234a027b42c52f71bcb177\r\n\r\nThis issue lead to multiple issues as soon as CUDA GPUs are used with the C++ API.\r\n\r\n**Workaround:**\r\nAs a temporary solution, one may create a symbolic link from the CUDA path, nonetheless modifying system should not be a permanent solution.\r\n```bash\r\nmkdir -p /usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/gpus/cuda/\r\nln -s /usr/local/cuda/include /usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/gpus/cuda/\r\nls /usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/gpus/cuda/include\r\n```\r\n\r\n**Solution:**\r\nInstead of:\r\n```cpp\r\n#include \"third_party/gpus/cuda/include/cuComplex.h\"\r\n#include \"third_party/gpus/cuda/include/cuda.h\"\r\n```\r\n\r\nPlease do:\r\n```cpp\r\n#include \"cuda/include/cuComplex.h\"\r\n#include \"cuda/include/cuda.h\"\r\n```", "comments": ["@DEKHTIARJonathan can you share how you're building and/or using TensorFlow?  Are you building a TF pip package and using that?", "Do I understand it correctly that you are attempting to use TF headers that are included along with a *python* package? I'm not sure if that's supposed to work in principle. Tensorflow in general requires  rather particular build setup in order to work. I would not expect any C++ bits to work/compile outside of that build environment. I could be wrong, though. \r\n@sanjoy -- does TF provide precompiled C++ libraries to end-users? Or do we only expose python API packages? I see that there's a separate C library: https://www.tensorflow.org/install/lang_c but it does not expose any C++ headers.", "There is a custom-op template repo which implements the CUDA kernel using TF Python package; take look at [this](https://github.com/tensorflow/custom-op/blob/master/tensorflow_time_two/cc/kernels/time_two_kernels.cu.cc#L21).\r\n\r\nIf you are using Bazel, my guess is that the headers are linked via [this line](https://github.com/tensorflow/custom-op/blob/master/tensorflow_time_two/BUILD#L34). Alternatively, if you are using Makefile, take a look at [this](https://github.com/tensorflow/custom-op/blob/master/Makefile#L34).\r\n\r\nAlso ping @yifeif; she is the author of the custom-op template repo.", "The custom op most likely requires building it in the same environment where TF itself was built -- the headers rely on all sorts of macros that may be set differently depending on TF build configuration. \r\nCustom op build instructions here https://github.com/tensorflow/custom-op  use docker containers provided by TF for that purpose. \r\n\r\nFrom the FAQ on that page:\r\n\r\n> Q: Do I need both the toolchain and the docker image? \r\n> A: Yes, you will need both to get the same setup we use to build TensorFlow's official pip package.\r\n> \r\n\r\n\r\n", "> The custom op most likely requires building it in the same environment where TF itself was built -- the headers rely on all sorts of macros that may be set differently depending on TF build configuration.\r\n> Custom op build instructions here https://github.com/tensorflow/custom-op use docker containers provided by TF for that purpose.\r\n> \r\n> From the FAQ on that page:\r\n> \r\n> > Q: Do I need both the toolchain and the docker image?\r\n> > A: Yes, you will need both to get the same setup we use to build TensorFlow's official pip package.\r\n\r\nI guess that is so, until Op registration is implemented in C API.", "Closing since IIUC there is nothing for us to do here as per comments above..  Please reopen if you disagree.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34428\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34428\">No</a>\n"]}, {"number": 34427, "title": "Segfaults and SIGABRTs on nightlies since 2019-11-19", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ('v1.12.1-18736-g229a46c', '2.1.0-dev20191119')\r\n- Python version: Python 2.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nTensorFlow version `tf-nightly==2.1.0.dev20191119` experiences core\r\ndumps on some simple summary operations. I can\u2019t reproduce this on my\r\nnormal workstation, but I have a consistent repro on a Cloud VM (which\r\ndoes not have Cuda installed; maybe relevant?).\r\n\r\nRepro steps:\r\n\r\nCreate a new virtual machine running Ubuntu 14.04:\r\n<https://console.cloud.google.com/compute/instancesDeploySolution?solution=ubuntu-os-cloud:ubuntu-trusty>\r\n\r\nPrepare the system:\r\n\r\n```\r\nsudo apt install python-pip python-virtualenv python3.5\r\n```\r\n\r\nCreate a virtualenv:\r\n\r\n```\r\nvirtualenv -p python2.7 ./ve &&\r\n. ./ve/bin/activate &&\r\npip install -U pip setuptools &&\r\npip install tf-nightly==2.1.0.dev20191119 &&\r\n:\r\n```\r\n\r\nAttempt to reproduce the failure:\r\n\r\n```\r\npython -c '__import__(\"tensorboard\").summary.scalar(\"test\", 1)'\r\n```\r\n\r\nAbout half the time, the last command will SIGABRT. On Python 2.7, the message is:\r\n\r\n```\r\n(ve)wchargin@trusty-travis-debug-37:~$ python -c '__import__(\"tensorboard\").summary.scalar(\"test\", 1)'\r\n2019-11-19 21:08:01.525309: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2019-11-19 21:08:01.525350: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-11-19 21:08:01.525377: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (trusty-travis-debug-37): /proc/driver/nvidia/version does not exist\r\n2019-11-19 21:08:01.525707: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-11-19 21:08:01.534370: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000170000 Hz\r\n2019-11-19 21:08:01.535487: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x526b010 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-19 21:08:01.535515: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n*** Error in `python': free(): invalid pointer: 0x0000000001ecaad0 ***\r\nAborted (core dumped)\r\n```\r\n\r\nOn Python 3.5, the log is:\r\n\r\n```\r\n(ve3.5a)wchargin@trusty-travis-debug-37:~$ python -c '__import__(\"tensorboard\").summary.scalar(\"test\", 1)'\r\n2019-11-19 21:06:03.550539: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2019-11-19 21:06:03.550578: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-11-19 21:06:03.550601: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (trusty-travis-debug-37): /proc/driver/nvidia/version does not exist\r\n2019-11-19 21:06:03.550931: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-11-19 21:06:03.559645: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000170000 Hz\r\n2019-11-19 21:06:03.560529: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a31d20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-19 21:06:03.560558: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n*** Error in `python': corrupted size vs. prev_size: 0x0000000001572680 ***\r\nAborted (core dumped)\r\n```\r\n\r\nNote that though this code is invoked through TensorBoard APIs, the\r\nerror is actually in TensorFlow (TensorBoard does not have any C++ code\r\nof its own), and the TensorBoard nightlies did not change substantially\r\nbetween the last good nightly and the current broken nightly.\r\nDowngrading TensorBoard to nightly `tb-nightly==2.1.0a20191118` or to\r\nstable `tensorboard==2.0.1` does not fix the problem.\r\n\r\nIf you instead install the following dependencies\u2014\r\n\r\n<details>\r\n<summary>`pip freeze` output; install with `pip install -r requirements.txt`</summary>\r\n\r\n```\r\nabsl-py==0.8.1\r\nastor==0.8.0\r\nattrs==17.3.0\r\naws-xray-sdk==0.95\r\nbackports.ssl-match-hostname==3.7.0.1\r\nbackports.tempfile==1.0\r\nbackports.weakref==1.0.post1\r\nboto==2.49.0\r\nboto3==1.9.86\r\nbotocore==1.12.253\r\ncachetools==3.1.1\r\ncertifi==2019.9.11\r\ncffi==1.13.2\r\nchardet==3.0.4\r\nconfigparser==4.0.2\r\ncookies==2.2.1\r\ncryptography==2.8\r\ndocker==4.1.0\r\ndocutils==0.15.2\r\necdsa==0.14.1\r\nentrypoints==0.3\r\nenum34==1.1.6\r\nflake8==3.7.8\r\nfuncsigs==1.0.2\r\nfunctools32==3.2.3.post2\r\nfuture==0.18.2\r\nfutures==3.3.0\r\ngast==0.2.2\r\ngoogle-auth==1.7.1\r\ngoogle-auth-oauthlib==0.4.1\r\ngoogle-pasta==0.1.8\r\ngrpcio==1.25.0\r\ngrpcio-testing==1.24.3\r\nh5py==2.10.0\r\nidna==2.8\r\nipaddress==1.0.23\r\nJinja2==2.10.3\r\njmespath==0.9.4\r\njsondiff==1.1.1\r\njsonpickle==1.2\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nMarkdown==3.1.1\r\nMarkupSafe==1.1.1\r\nmccabe==0.6.1\r\nmock==3.0.5\r\nmoto==1.3.7\r\nnose==1.3.7\r\nnumpy==1.16.5\r\noauthlib==3.1.0\r\nopt-einsum==2.3.2\r\npathspec==0.6.0\r\npbr==3.1.1\r\npluggy==0.6.0\r\nprotobuf==3.10.0\r\npy==1.5.2\r\npyaml==19.4.1\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.7\r\npycodestyle==2.5.0\r\npycparser==2.19\r\npycryptodome==3.9.4\r\npyflakes==2.1.1\r\npytest==3.3.0\r\npython-dateutil==2.8.1\r\npython-jose==2.0.2\r\npytz==2019.3\r\nPyYAML==5.1.2\r\nrequests==2.22.0\r\nrequests-oauthlib==1.3.0\r\nresponses==0.10.6\r\nrsa==4.0\r\ns3transfer==0.1.13\r\nsix==1.13.0\r\ntb-nightly==2.1.0a20191119\r\ntermcolor==1.1.0\r\ntf-estimator-nightly==2.0.0.dev2019111909\r\ntf-nightly==2.1.0.dev20191119\r\ntyping==3.7.4.1\r\nurllib3==1.25.7\r\nwebsocket-client==0.56.0\r\nWerkzeug==0.16.0\r\nwrapt==1.11.2\r\nxmltodict==0.12.0\r\nyamllint==1.17.0\r\n```\r\n\r\n</details>\r\n\r\n\u2014then the following script consistently segfaults after completing each\r\ncommand (except rarely when it SIGABRTs instead):\r\n\r\n\r\n```\r\n(ve2.7)wchargin@trusty-travis-debug-37:~$ cat scriptzz.py\r\nimport tensorboard as tb\r\ntb.summary.v1.scalar_pb('test', 42)\r\ntb.summary.scalar('test v2', 1337)\r\nfrom tensorboard.plugins.beholder import Beholder, BeholderHook\r\nprint(\"DONE\")\r\n(ve2.7)wchargin@trusty-travis-debug-37:~$ python scriptzz.py\r\n2019-11-19 21:26:36.188506: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2019-11-19 21:26:36.188546: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-11-19 21:26:36.188579: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (trusty-travis-debug-37): /proc/driver/nvidia/version does not exist\r\n2019-11-19 21:26:36.188899: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-11-19 21:26:36.197595: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000170000 Hz\r\n2019-11-19 21:26:36.198580: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5df0030 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-19 21:26:36.198617: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nDONE\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nExample double-free/invalid-free in failed build log:\r\n<https://travis-ci.com/tensorflow/tensorboard/jobs/258256205>\r\n\r\nExample segfault in failed build log:\r\n<https://travis-ci.com/tensorflow/tensorboard/jobs/258256206>\r\n\r\n**Describe the expected behavior**\r\n\r\nRunning TensorFlow code should never SIGSEGV or SIGABRT.\r\n\r\n**Code to reproduce the issue**\r\n\r\nSee above.\r\n\r\n**Other info / logs**\r\n\r\nThis previously occurred with TensorFlow\u2019s 20191024 nightlies:\r\n\r\n```\r\npip: tb-nightly==2.1.0a20191024\r\npip: tf-nightly==2.1.0.dev20191024\r\npython: tb.__version__ == 2.1.0a20191024\r\npython: tf.__git_version__ == v1.12.1-16634-g0e90b26\r\npython: tf.__version__ == 2.1.0-dev20191024\r\n```\r\n\r\nSee <https://github.com/tensorflow/tensorboard/pull/2834> for context.\r\n\r\nThis is blocking TensorBoard CI. We\u2019ll probably just pin yesterday\u2019s\r\nnightlies in the short term, but last time we did that (see above PR)\r\nthe underyling issue was evidently never actually resolved.\r\n", "comments": ["cc @mihaimaruseac ", "FWIW, I\u2019m continuing to see these errors on 2019-12-02 nightlies:\r\n\r\n```\r\n(ve)wchargin@trusty-travis-debug-39:~$ python -c '__import__(\"tensorboard\").summary.scalar(\"test\", 1)'\r\n2019-12-02 22:43:19.710483: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2019-12-02 22:43:19.710528: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-12-02 22:43:19.710552: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (trusty-travis-debug-39): /proc/driver/nvidia/version does not exist\r\n2019-12-02 22:43:19.710889: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-12-02 22:43:19.720134: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2000165000 Hz\r\n2019-12-02 22:43:19.721200: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5716340 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-12-02 22:43:19.721227: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n*** Error in `python': free(): invalid pointer: 0x0000000002349ad0 ***\r\nAborted (core dumped)\r\n(ve)wchargin@trusty-travis-debug-39:~$ python -c 'print(__import__(\"tensorflow\").__version__)'\r\n2.1.0-dev20191202\r\n(ve)wchargin@trusty-travis-debug-39:~$ pip freeze --all | grep -e tensor -e tf- -e tb-\r\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\r\ntb-nightly==2.1.0a20191202\r\ntf-estimator-nightly==2.0.0.dev2019120209\r\ntf-nightly==2.1.0.dev20191202\r\n```\r\n", "I didn't yet get a chance to look into these unfortunately. I think the earliest I can look into them is next week.", "Acknowledged; thanks! It\u2019s certainly quite weird that I can reproduce\r\nthis on a fresh Ubuntu machine but not my normal environment, which is\r\nDebian-based. I\u2019ve hacked around this by disabling the one integration\r\ntest in our CI that happens to hit this, but that integration test is\r\nactually pretty important for us, so it would be awesome if we could get\r\nto the bottom of this. Thanks so much!\r\n", "@mihaimaruseac Any updates on the investigation of this issue? We continue to see it in the latest nightly (20191222). ", "Can you try upgrading `scipy` to `1.4.1`? There have been similar sigabrts last week which were caused by `scipy` (scipy/scipy#11237)\r\n\r\nThere was another segfault I fixed recently, I need to do some post-fixing work today and then I can investigate this more", "@mihaimaruseac  @wchargin The most recent tensorboard build log that shows this error already shows scipy==1.4.1 is being used. See log here: https://api.travis-ci.org/v3/job/628578238/log.txt. So I don't think scipy is the direct cause.", "Thanks. One more segfault to investigate, then. I'm on it.", "@mihaimaruseac Thanks! Let me know if I can be of any help.", "Oh, I just noticed you're using Ubuntu 14 VMs. Can you try with Ubuntu 16 ones? Our toolchain is tested on ubuntu 16 and we expect issues with ubuntu 14.", "Should we also close this one, now that we identified the issue to come from the toolchain?", "Closing, since issue stems from toolchain and we fixed that"]}, {"number": 34426, "title": "Error on distributed training \" Variable is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\"", "body": "I have this issue when I try to run distributed training with my own custom training loop. \r\n\r\nThere is something going wrong when calling apply_gradient. \r\n\r\n**System information**\r\nTest on Google Colab with GPU TF 2.0\r\n\r\n**Code to reproduce the issue**\r\n```\r\ndef model_builder():\r\n    resnet = ResNet50(include_top=False, weights='imagenet')\r\n    model = Sequential()\r\n    model.add(resnet)\r\n    \r\n    model.add(Dense(100, activation=\"softmax\"))\r\n\r\n    return model\r\n\r\ndef experiment_dist(model_builder, x_train, y_train, x_test, y_test):  \r\n    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(64)\r\n    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\r\n    \r\n    mirrored_strategy = tf.distribute.MirroredStrategy()\r\n    train_ds = mirrored_strategy.experimental_distribute_dataset(train_ds)\r\n    test_ds = mirrored_strategy.experimental_distribute_dataset(test_ds)\r\n   \r\n    \r\n\r\n    with mirrored_strategy.scope():\r\n\r\n        test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n\r\n        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n            name='train_accuracy')\r\n        test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n            name='test_accuracy')\r\n\r\n        model = model_builder()\r\n        # model = ResNet50(include_top=False, weights='imagenet')\r\n\r\n        optimizer = tf.keras.optimizers.SGD()\r\n        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n            reduction=tf.keras.losses.Reduction.NONE)\r\n        def train_step(inputs):\r\n            x, y = inputs\r\n            x = tf.dtypes.cast(x, tf.float32)\r\n            with tf.GradientTape() as tape:\r\n                predictions = model(x)\r\n                loss = loss_object(y, predictions)\r\n            gradients = tape.gradient(loss, model.trainable_variables)\r\n            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n            \r\n            train_loss(loss)\r\n            train_accuracy(y, predictions)\r\n\r\n        def test_step(inputs) :\r\n            x, y = inputs\r\n            x = tf.dtypes.cast(x, tf.float32)\r\n\r\n            predictions = model(x)\r\n            t_loss = loss_object(y, predictions)\r\n\r\n            test_loss(t_loss)\r\n            test_accuracy(y, predictions)\r\n\r\n        @tf.function\r\n        def distributed_train_step(inputs):\r\n            per_replica_losses = strategy.experimental_run_v2(train_step,\r\n                                                            args=(inputs, ))\r\n            return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n                                axis=None)\r\n        \r\n        @tf.function\r\n        def distributed_test_step(inputs):\r\n            return strategy.experimental_run_v2(test_step, args=(inputs, ))            \r\n    iter_count = 0\r\n    MAX_ITER = 95000\r\n\r\n    while True:\r\n        print(\"start\")\r\n        for inputs in train_ds:\r\n            print(\"train_loop\")\r\n            train_step(inputs)\r\n            iter_count += 1\r\n            if iter_count > MAX_ITER:\r\n                break\r\n\r\n\r\n        for test_images, test_labels in test_ds:\r\n            images = tf.dtypes.cast(images, tf.float32)\r\n            test_step(test_images, test_labels)\r\n\r\n        template = 'Iteration {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n        print(template.format(iter_count,\r\n                                train_loss.result(),\r\n                                train_accuracy.result()*100,\r\n                                test_loss.result(),\r\n                                test_accuracy.result()*100))\r\n\r\n        # Reset the metrics for the next epoch\r\n        train_loss.reset_states()\r\n        train_accuracy.reset_states()\r\n        test_loss.reset_states()\r\n        test_accuracy.reset_states()\r\n\r\n        if iter_count > MAX_ITER:\r\n            break\r\n    \r\nexperiment_dist(model_builder, x_train, y_train, x_test, y_test)\r\n```\r\n\r\n\r\n**Error Track**\r\n```\r\nWARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nstart\r\ntrain_loop\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-29-d2da1a38aa3a> in <module>()\r\n----> 1 experiment_dist(model_builder, x_train, y_train, x_test, y_test)\r\n\r\n14 frames\r\n<ipython-input-28-357ae31bb355> in experiment_dist(model_builder, x_train, y_train, x_test, y_test)\r\n     72         for inputs in train_ds:\r\n     73             print(\"train_loop\")\r\n---> 74             train_step(inputs)\r\n     75             iter_count += 1\r\n     76             if iter_count > MAX_ITER:\r\n\r\n<ipython-input-28-357ae31bb355> in train_step(inputs)\r\n     40                 loss = loss_object(y, predictions)\r\n     41             gradients = tape.gradient(loss, model.trainable_variables)\r\n---> 42             optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n     43 \r\n     44             train_loss(loss)\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name)\r\n    439           functools.partial(self._distributed_apply, apply_state=apply_state),\r\n    440           args=(grads_and_vars,),\r\n--> 441           kwargs={\"name\": name})\r\n    442 \r\n    443   def _distributed_apply(self, distribution, grads_and_vars, name, apply_state):\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py in merge_call(self, merge_fn, args, kwargs)\r\n   1915     if kwargs is None:\r\n   1916       kwargs = {}\r\n-> 1917     return self._merge_call(merge_fn, args, kwargs)\r\n   1918 \r\n   1919   def _merge_call(self, merge_fn, args, kwargs):\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py in _merge_call(self, merge_fn, args, kwargs)\r\n   1922         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\r\n   1923     try:\r\n-> 1924       return merge_fn(self._strategy, *args, **kwargs)\r\n   1925     finally:\r\n   1926       _pop_per_thread_mode()\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in _distributed_apply(self, distribution, grads_and_vars, name, apply_state)\r\n    483           update_ops.extend(\r\n    484               distribution.extended.update(\r\n--> 485                   var, apply_grad_to_update_var, args=(grad,), group=False))\r\n    486 \r\n    487       any_symbolic = any(isinstance(i, ops.Operation) or\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py in update(self, var, fn, args, kwargs, group)\r\n   1528       kwargs = {}\r\n   1529     with self._container_strategy().scope():\r\n-> 1530       return self._update(var, fn, args, kwargs, group)\r\n   1531 \r\n   1532   def _update(self, var, fn, args, kwargs, group):\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py in _update(self, var, fn, args, kwargs, group)\r\n   2140     # The implementations of _update() and _update_non_slot() are identical\r\n   2141     # except _update() passes `var` as the first argument to `fn()`.\r\n-> 2142     return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\r\n   2143 \r\n   2144   def _update_non_slot(self, colocate_with, fn, args, kwargs, should_group):\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py in _update_non_slot(self, colocate_with, fn, args, kwargs, should_group)\r\n   2146     # once that value is used for something.\r\n   2147     with UpdateContext(colocate_with):\r\n-> 2148       result = fn(*args, **kwargs)\r\n   2149       if should_group:\r\n   2150         return result\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in apply_grad_to_update_var(var, grad)\r\n    465       if \"apply_state\" in self._dense_apply_args:\r\n    466         apply_kwargs[\"apply_state\"] = apply_state\r\n--> 467       update_op = self._resource_apply_dense(grad, var, **apply_kwargs)\r\n    468       if var.constraint is not None:\r\n    469         with ops.control_dependencies([update_op]):\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/optimizer_v2/gradient_descent.py in _resource_apply_dense(self, grad, var, apply_state)\r\n    106 \r\n    107   def _resource_apply_dense(self, grad, var, apply_state=None):\r\n--> 108     var_device, var_dtype = var.device, var.dtype.base_dtype\r\n    109     coefficients = ((apply_state or {}).get((var_device, var_dtype))\r\n    110                     or self._fallback_apply_state(var_device, var_dtype))\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/values.py in device(self)\r\n    735   @property\r\n    736   def device(self):\r\n--> 737     return self._get_closest().device\r\n    738 \r\n    739   @property\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/values.py in _get_closest(self)\r\n    663     if device is None:\r\n    664       device = device_util.canonicalize(device_util.current())\r\n--> 665     replica_id = self._device_map.replica_for_device(device)\r\n    666     if replica_id is None:\r\n    667       return self.primary\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/values.py in replica_for_device(self, device)\r\n    210 \r\n    211   def replica_for_device(self, device):\r\n--> 212     return self._device_to_replica.get(device)\r\n    213 \r\n    214   def select_for_device(self, values, device):\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/ops/variables.py in __hash__(self)\r\n   1084   def __hash__(self):\r\n   1085     if ops.Tensor._USE_EQUALITY and ops.executing_eagerly_outside_functions():  # pylint: disable=protected-access\r\n-> 1086       raise TypeError(\"Variable is unhashable if Tensor equality is enabled. \"\r\n   1087                       \"Instead, use tensor.experimental_ref() as the key.\")\r\n   1088     else:\r\n\r\nTypeError: Variable is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\r\n```", "comments": ["@burnmg ,\r\nHi, When tried executing the given code `NameError: name 'x_train' is not defined` was faced, Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/32a572dae4f19281f8c53c277eb09499/34426.ipynb) of colab. Can you please provide complete code to reproduce the error mentioned here.Thanks!", "@burnmg ,\r\nAny update on the issue ?Thanks!", "Hi, \r\nThanks for the message\r\nI have reimplemented my code by using another way. \r\n\r\nLet me try to recover my previous code if possible. ", "`x_train` here is a CIFAR-10 dataset", "@burnmg ,\r\nThank you for responding, can you provide the actual code being used to replicate the issue from our side with input dataset as `CIFAR-10 `?", "@burnmg Can you please explain how you were able to resolve this issue so that it will be useful to the community. Thanks!", "@burnmg friendly ping. Let us know if you were able to resolve this issue. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I am closing this issue as it has been resolved. Please add additional comments and we can open this issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34426\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34426\">No</a>\n"]}, {"number": 34425, "title": "bazel build issue for tensorflow 2.0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04 \r\n- TensorFlow installed from source\r\n- TensorFlow version 2.0.0\r\n- Python version: 2.7.15+ and python 3.6.8\r\n- Installed using pip\r\n- Bazel version 1.1.0\r\n- GCC/Compiler version : gcc 7.4.0\r\n- CUDA/cuDNN version: CUDA Version 10.1.243,  cuDNN version 7.6.5\r\n- CPU model and memory: Intel\u00ae Core\u2122 i7-7700HQ CPU @ 2.80GHz \u00d7 8, 15.4GB\r\n- GPU model and memory: GeForce GTX 1050 Ti/PCIe/SSE2, 4GB\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nI try to do bazel build for tensorflow\r\nwhen running this command $bazel build //tensorflow/tools/pip_package:build_pip_package\r\nit give me these error:\r\n\r\n```\r\nERROR: /home/linxiang/tensorflow/tensorflow/stream_executor/cuda/BUILD:198:1: C++ compilation of rule '//tensorflow/stream_executor/cuda:cublas_stub' failed (Exit 1)\r\nIn file included from tensorflow/stream_executor/cuda/cublas_stub.cc:66:0:\r\n./tensorflow/stream_executor/cuda/cublas_10_1.inc:122:25: error: 'cublasLogCallback' was not declared in this scope\r\n cublasSetLoggerCallback(cublasLogCallback userCallback) {\r\n                         ^~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublas_10_1.inc:122:25: note: suggested alternative: 'cublasSetLoggerCallback'\r\n cublasSetLoggerCallback(cublasLogCallback userCallback) {\r\n                         ^~~~~~~~~~~~~~~~~\r\n                         cublasSetLoggerCallback\r\n./tensorflow/stream_executor/cuda/cublas_10_1.inc:130:25: error: 'cublasLogCallback' was not declared in this scope\r\n cublasGetLoggerCallback(cublasLogCallback *userCallback) {\r\n                         ^~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublas_10_1.inc:130:25: note: suggested alternative: 'cublasGetLoggerCallback'\r\n cublasGetLoggerCallback(cublasLogCallback *userCallback) {\r\n                         ^~~~~~~~~~~~~~~~~\r\n                         cublasGetLoggerCallback\r\n./tensorflow/stream_executor/cuda/cublas_10_1.inc:130:44: error: 'userCallback' was not declared in this scope\r\n cublasGetLoggerCallback(cublasLogCallback *userCallback) {\r\n                                            ^~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublas_10_1.inc:130:44: note: suggested alternative: 'CUstreamCallback'\r\n cublasGetLoggerCallback(cublasLogCallback *userCallback) {\r\n                                            ^~~~~~~~~~~~\r\n                                            CUstreamCallback\r\n\r\n```", "comments": ["in configure.py, I only put yes for Do you wish to build TensorFlow with CUDA support? [y/N]: Y.\r\n", "@xiang123, In the configure.py, did you set the path for CUDA. Thanks!", "@gadagashwini \r\nit found the path by itself\r\nthe configure shows:\r\n\r\nFound CUDA 10.1 in:\r\n/usr/local/cuda/lib64\r\n/usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n/usr/local/cuda/lib64\r\n/usr/local/cuda/include\r\n", "after I switch CUDA back to 10.0 problem solved", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34425\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34425\">No</a>\n", "What if I want to build tensorflow using cuda 10.2.\r\nIs there a solution other than downgrading cuda to 10.0?"]}, {"number": 34424, "title": "Extremely slow pre-processing of images using InceptionV3", "body": "**System information**\r\n- OS Platform and Distribution: Google AI Platform\r\n- TensorFlow version: v2.0\r\n- Python version: v3.*\r\n- Machine type: 4 vCPUs, 15 GB RAM\r\n- GPU: 1x NVIDIA Tesla K80\r\n\r\nI am training a model to caption images and have mostly used this tutorial: https://www.tensorflow.org/tutorials/text/image_captioning#download_and_prepare_the_ms-coco_dataset\r\n\r\nThe issue is, I am trying to pre-process each image with InceptionV3 and cache the output to disk, however, each iteration (code snippet below) takes about 22s without GPUs and 18s with NVIDIA Tesla K80. This is way too slow for 30,000+ images for example. \r\n\r\nMy image file size ranges between 60-150kb each.\r\n\r\n```\r\n# Get unique images\r\nencode_train = sorted(set(img_name_vector))\r\n\r\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\r\nimage_dataset = image_dataset.map(\r\n  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\r\n\r\nfor img, path in image_dataset:\r\n  batch_features = image_features_extract_model(img)\r\n  batch_features = tf.reshape(batch_features,\r\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\r\n\r\n  for bf, p in zip(batch_features, path):\r\n    np.save(tf.io.gfile.GFile(IMG_PATH, 'w'), bf.numpy())\r\n```\r\n\r\nThe tensorflow tutorial claims that caching will take about 10 minutes to run in Colab with a GPU (for 30,000 images).\r\n\r\nI wonder if I am doing something wrong, or if there is a more efficient way to achieve this?\r\n\r\nAny help is appreciated!\r\n", "comments": ["@anthonyatp,\r\nCan you try reducing the Batch Size and check if it helps. Thanks! ", "@rmothukuru \r\nI reduced the batch size from 16 to 8 and it takes around 11s per iteration but it has double the amount of iterations so would actually take longer than if batch size was 16?\r\n\r\nAs mentioned, the tutorial claims this takes 10mins to run in colab with GPU (for 30k images and batch size 16) - currently that would take me over 9 hours. Obviously I'm using a different dataset but the image sizes are comparable to the tutorial.\r\n\r\nThe other notable difference is that I am using a dataset stored in GCS and not local - could this be the issue?\r\n\r\nOtherwise, it's pretty unrealistic for me to train this model using a relatively small dataset", "I think data read here is a bottle neck as I tried running the code thats given [here](https://www.tensorflow.org/tutorials/text/image_captioning#download_and_prepare_the_ms-coco_dataset) and didn't face any issues.\r\n\r\nMore over you can try converting your data into TFRecords asTFRecords and corresponding TF infrastructure for them (queues/runners/readers) are really optimized for fast reading.By using tf.data API and especially Dataset.prefetch(), you can gain a substantial speedup.\r\n\r\nYou can also go though the 2nd answer given [here](https://stackoverflow.com/questions/55260190/how-to-speed-up-my-keras-cnn-with-pre-trained-inceptionv3). If you have any more questions, I would recommend posting them on stackoverflow. Thanks!"]}]