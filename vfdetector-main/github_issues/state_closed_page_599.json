[{"number": 35702, "title": "Tensorflow 2.1.0 failed to list GPU devices and detect GPU devices automatically", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): pip3 install tensorflow --upgrade\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: GTX2080Ti 11GB\r\n\r\n**Describe the current behavior**\r\nI try to implement the official distributed training example by using `strategy = tf.distribute.MirroredStrategy()` to detect the GPU devices. It is supposed to detect all gpu devices automatically. However, when I use this API under my environment, it can only find the CPU devices. \r\n**Describe the expected behavior**\r\n\r\nIt should have the same output like the Tensorflow 2.0.0b1.\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf \r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\r\n```\r\n\r\n**Other info / logs**\r\nI also find that the API `tf.config.experimental.list_physical_devices('GPU')` cannot list the GPU devices. After I check, my gpu device type is XLA_GPU. But I can use the same API to list all my GPU devices.\r\n", "comments": ["@crownk1997 \r\nCan you please go through the [link ](https://www.tensorflow.org/guide/gpu)and see if it helps you.Thanks!", "@ravikyram I have checked this link. Actually, there is a toy example in this tutorial which uses `gpus = tf.config.experimental.list_physical_devices('GPU')`this API to check if there is any avaliable gpu device. However, in my case, I have 9 GTX2080Ti and I always get an empty list from the same API. Thank you !", "Try specifying 'XLA_GPU'.  Or just tf.config.list_physical_devices() and see what it returns.\r\n\r\nOn my machines with TF 2.1.0  tf.config.list_physical_devices shows XLA_GPU instead of GPU.  In my TF 2.0.0 based Docker image both device type GPU and XLA_GPU are being reported.\r\n\r\nI think the problem is, as has been mentioned elsewhere, a matter of not having CUDA 10.1 installed in  my 2.1.0 Docker image.", "The problem was in fact that CUDA 10.2 was installed in the Docker image and not 10.1.  Once that was done both device type GPU and XLA_GPU show up.  So specifying GPU instead of XLA_GPU works.  I'm not sure what a remote function is but I suspect that this is due to using mirrored distribution strategy.\r\n\r\nSo to the OP: Make sure that CUDA 10.1 is installed in your environment.", "@sfleisch Thank you for your instruction and I will have a try soon.", "Hi!\r\n@crownk1997 Have you resolved this issue?\r\nI installed tensorflow-gpu 2.1 with cuda 10.1 but my GPU (GTX 1060) is not detected by tensorflow as well and I don't know why because I followed the exact instructions and got all the versions right.", "@crownk1997 \r\n\r\nAny update on this issue please. Thanks!", "@trangnm58 I am so sorry about that. Because I do not have the permission to change the CUDA version on our server.", "@crownk1997 If you are using a Docker container you need CUDA 10.1 in the container image not the host OS level. You just need a compatible driver and if CUDA 10.2 is installed on the host then you, in fact, do.", "Setting cuda version to 10.1 with TF 2.1 should provide gpu support. Closing this issue now. Thanks!", "Either there has been a regression or there's another problem here\r\n\r\n![image](https://user-images.githubusercontent.com/62040901/134939109-28a09264-9eb9-453a-90b0-eb1675f4febc.png)\r\n"]}, {"number": 35701, "title": "[TF2.0] Build from sources to support CUDA9", "body": "As we all know \uff0cthe tf2-cuda9.0 haven't the pkg by official.\r\nBut in some server\uff0c we can't update the GPU driver to support CUDA10. \r\nSo we need to bazel the pkg by ourself.\r\nI bazeled the  tensorflow2.0 pkgs that support cuda9.0 for py3.5 3.6 3.7 . \r\nI hope it could help more people.\r\n\r\nBut \uff0csometimes \uff0cit will have this error.\r\n\r\n```shell\r\nW tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.9.0';.....undefined symbol: GOMP_critical_end;\r\n\r\n```\r\n\r\nand I find if you add the two lines of code, it will work.\r\n\r\n```python\r\nimport ctypes\r\nctypes.CDLL(\"libgomp.so.1\", mode=ctypes.RTLD_GLOBAL)\r\n```\r\n\r\nI'm not sure if these two lines of code have some side effects.\r\nSo I need the official to explain how to avoid this error\uff0cand the  side effects will  bring by the two lines of code.\r\n\r\nAnd the pkg and installation method in this link.\r\nhttps://github.com/SmileTM/Tensorflow2.X-GPU-CUDA9.0", "comments": ["@SmileTM \r\n\r\nTF 2.0-GPU supports cuda 10.0. I see that you are using cuda 9.0 can you please switch to cuda 10.0 and let us know how it progresses.Please, follow the instructions from [TensorFlow website](https://www.tensorflow.org/install/source) .Thanks!", "> @SmileTM\r\n> \r\n> TF 2.0-GPU supports cuda 10.0. I see that you are using cuda 9.0 can you please switch to cuda 10.0 and let us know how it progresses.Please, follow the instructions from [TensorFlow website](https://www.tensorflow.org/install/source) .Thanks!\r\n\r\n\r\nI can't update my gpu-drive to cuda10.\r\n\r\n So I Bazeled these pkgs from source code to help some people like me.\r\n\r\nI just want you to verify the viability of my pkgs and my method. \r\n\r\nThank you\uff01\r\n\r\n\r\n\r\n\r\n\r\n\r\nthis is my   `.tf_configure.bazelrc` configures.\r\n\r\n```shell\r\nbuild --action_env PYTHON_BIN_PATH=\"/disk1/lx/conda/envs/mk/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/disk1/lx/conda/envs/mk/lib/python3.7/site-packages\"\r\nbuild --python_path=\"/disk1/lx/conda/envs/mk/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/disk1/lx/cuda\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"5.2,5.2\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/disk1/lx/cuda/lib64:/disk1/lx/cuda/lib64:/disk1/lx/cuda/lib64:/disk1/lx/cuda/lib64:/disk1/lx/cuda:/disk1/lx/cuda/extras/CUPTI/lib64\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n\r\n\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35701\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35701\">No</a>\n"]}, {"number": 35700, "title": "tensorflow2.1 can not initialize colab TPU ", "body": "    if '2.1' in tf.__version__:\r\n        if 'COLAB_TPU_ADDR' in os.environ:\r\n            resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n            tf.config.experimental_connect_to_cluster(resolver)\r\n            tf.tpu.experimental.initialize_tpu_system(resolver)\r\n            strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n            print('Running on TPU ', resolver.cluster_spec().as_dict())\r\n\r\n\r\n\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-1-53f26737ac5e> in <module>()\r\n    160             resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n    161             tf.config.experimental_connect_to_cluster(resolver)\r\n--> 162             tf.tpu.experimental.initialize_tpu_system(resolver)\r\n    163             strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n    164             print('Running on TPU ', resolver.cluster_spec().as_dict())\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: '__inference__tpu_init_fn_4' is neither a type of a primitive operation nor a name of a function registered in binary running on n-3ea5ef93-w-0. Make sure the operation or function is registered in the binary running in this process.", "comments": ["tensorflow-gpu-2.1 has this problem ,and tensorflow-2.1 is fine.", "**Update:**\r\nEven when using `tensorflow==2.1` and `tf.test.is_built_with_gpu_support()` returning `False` I still get the same error.\r\n\r\nI tried installing using `pip install tensorflow==2.1`, but I still get nearly the same error:\r\n```\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-5-0a40fe7e3383> in <module>\r\n      8     resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\r\n      9     tf.config.experimental_connect_to_cluster(resolver)\r\n---> 10     tf.tpu.experimental.initialize_tpu_system(resolver)\r\n     11     strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\n~/.local/lib/python3.5/site-packages/tensorflow_core/python/tpu/tpu_strategy_util.py in initialize_tpu_system(cluster_resolver)\r\n    101     context.context()._clear_caches()  # pylint: disable=protected-access\r\n    102 \r\n--> 103     serialized_topology = output.numpy()\r\n    104 \r\n    105     # TODO(b/134094971): Remove this when lazy tensor copy in multi-device\r\n\r\n~/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py in numpy(self)\r\n    940     \"\"\"\r\n    941     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n--> 942     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n    943     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n    944 \r\n\r\n~/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py in _numpy(self)\r\n    908       return self._numpy_internal()\r\n    909     except core._NotOkStatusException as e:\r\n--> 910       six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n    911 \r\n    912   @property\r\n\r\n/usr/local/lib/python3.5/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: '__inference__tpu_init_fn_4' is neither a type of a primitive operation nor a name of a function registered in binary running on n-9a4d14b8-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n```\r\n\r\nStrangely though, when I test for GPU support using `tf.test.is_built_with_gpu_support()` it returns `True`, so perhaps that's the issue. When I check `tf.__version__` and `tf.version.GIT_VERSION` I get 2.1.0 and v2.1.0-rc2-17-ge5bf8de respectively.", "@ofpppppppdbfjs ,\r\nCan you share code to reproduce the issue?Thanks!", "> **Update:**\r\n> Even when using `tensorflow==2.1` and `tf.test.is_built_with_gpu_support()` returning `False` I still get the same error.\r\n> \r\n> I tried installing using `pip install tensorflow==2.1`, but I still get nearly the same error:\r\n> \r\n> ```\r\n> NotFoundError                             Traceback (most recent call last)\r\n> <ipython-input-5-0a40fe7e3383> in <module>\r\n>       8     resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\r\n>       9     tf.config.experimental_connect_to_cluster(resolver)\r\n> ---> 10     tf.tpu.experimental.initialize_tpu_system(resolver)\r\n>      11     strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n> \r\n> ~/.local/lib/python3.5/site-packages/tensorflow_core/python/tpu/tpu_strategy_util.py in initialize_tpu_system(cluster_resolver)\r\n>     101     context.context()._clear_caches()  # pylint: disable=protected-access\r\n>     102 \r\n> --> 103     serialized_topology = output.numpy()\r\n>     104 \r\n>     105     # TODO(b/134094971): Remove this when lazy tensor copy in multi-device\r\n> \r\n> ~/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py in numpy(self)\r\n>     940     \"\"\"\r\n>     941     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n> --> 942     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n>     943     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n>     944 \r\n> \r\n> ~/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py in _numpy(self)\r\n>     908       return self._numpy_internal()\r\n>     909     except core._NotOkStatusException as e:\r\n> --> 910       six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n>     911 \r\n>     912   @property\r\n> \r\n> /usr/local/lib/python3.5/dist-packages/six.py in raise_from(value, from_value)\r\n> \r\n> NotFoundError: '__inference__tpu_init_fn_4' is neither a type of a primitive operation nor a name of a function registered in binary running on n-9a4d14b8-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n> ```\r\n> \r\n> Strangely though, when I test for GPU support using `tf.test.is_built_with_gpu_support()` it returns `True`, so perhaps that's the issue. When I check `tf.__version__` and `tf.version.GIT_VERSION` I get 2.1.0 and v2.1.0-rc2-17-ge5bf8de respectively.\r\n\r\nuse\r\n\r\n%tensorflow_version 2.x", "@ofpppppppdbfjs thanks, I had figured out how to specify the correct TensorFlow version, but I still get the same error.\r\n\r\n@oanush \r\nI'm using Colab connected to Google Cloud CPU & TPU instances running:\r\nPython:\r\n3.5.3 (default, Sep 27 2018, 17:25:39) \r\n[GCC 6.3.0 20170516]\r\nTensorFlow:\r\n2.1.0\r\nv2.1.0-rc2-17-ge5bf8de\r\nGPU Support: False\r\n\r\nI still get the same error:\r\n```\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-6-c84946626dc0> in <module>\r\n     11     except tf.errors.UnimplementedError as uie:\r\n     12         print(uie, \"This appears to be caused by the TPU already being connected. Ignoring.\", sep='\\n')\r\n---> 13     tf.tpu.experimental.initialize_tpu_system(resolver)\r\n     14     tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\n3 frames\r\n/usr/local/lib/python3.5/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: '__inference__tpu_init_fn_4' is neither a type of a primitive operation nor a name of a function registered in binary running on n-80f5d4ef-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n```\r\n\r\nHere is the code I can use to reproduce:\r\n```\r\nimport tensorflow as tf\r\n\r\nTPU_ADDRESS = \"grpc://\" + \"10.0.0.2:8470\"\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n```\r\n\r\nWhich is exactly what the documentation says to do here: https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/TPUStrategy?version=stable", "I can confirm this on my Google Colab instance. For pip install tensorflow==2.1 as well es the current tf-nightly. It's very unfortunate because you can't use TPU. For me the only solution so far has been to use tensorflow 1.15.  \r\nI use this code:\r\n\r\n```\r\n# Try to run on TPU\r\n# Detect hardware, return appropriate distribution strategy\r\ntry:\r\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\r\n    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\r\nexcept ValueError:\r\n    tpu = None\r\n\r\nif tpu:\r\n    tf.config.experimental_connect_to_cluster(tpu)\r\n    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\nelse:\r\n    strategy = tf.distribute.get_strategy()\r\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\r\n```\r\n\r\nIt returns this:\r\n```\r\nRunning on TPU  ['10.8.85.34:8470']\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.8.85.34:8470\r\n\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.8.85.34:8470\r\n\r\nINFO:tensorflow:Clearing out eager caches\r\n\r\nINFO:tensorflow:Clearing out eager caches\r\n\r\n---------------------------------------------------------------------------\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n\r\n<ipython-input-2-5c79288551ed> in <module>()\r\n      7 if tpu:\r\n      8     tf.config.experimental_connect_to_cluster(tpu)\r\n----> 9     tf.tpu.experimental.initialize_tpu_system(tpu)\r\n     10     strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n     11 else:\r\n\r\n3 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tpu/tpu_strategy_util.py in initialize_tpu_system(cluster_resolver)\r\n    101     context.context()._clear_caches()  # pylint: disable=protected-access\r\n    102 \r\n--> 103     serialized_topology = output.numpy()\r\n    104 \r\n    105     # TODO(b/134094971): Remove this when lazy tensor copy in multi-device\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in numpy(self)\r\n    968     \"\"\"\r\n    969     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n--> 970     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n    971     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n    972 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _numpy(self)\r\n    936       return self._numpy_internal()\r\n    937     except core._NotOkStatusException as e:\r\n--> 938       six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n    939 \r\n    940   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: '__inference__tpu_init_fn_4' is neither a type of a primitive operation nor a name of a function registered in binary running on n-0bbd054a-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n\r\n```", "Hello! This seems to be a Colab-specific issue (Colab might not have fully rolled out the TensorFlow TPU 2.1 backend yet). Can you follow the instructions at https://github.com/googlecolab/colabtools#contacting-us and file an issue/send feedback to the Colab team?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35700\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35700\">No</a>\n"]}, {"number": 35699, "title": "Dangling pointer  through IntArrayFromInitializer causing Segfault in Tests", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): source\r\n\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source): 9.2.1\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\nThe IntArrayFromInitializer routine maintains a reference to the initializer_list's data.\r\nThe initializer_list is destroyed at the end of the function's scope as it is not a reference, and there is no guarantee that the initializer_list will exist beyond the function's scope.\r\nThis causes Segfaults and Corruption of the memory being referenced as revealed from testing \"tensorflow/lite/micro/kernels:elementwise_test\". This is illustrated in the screenshot attached below.\r\n\r\n**Describe the expected behavior**\r\nThere should be an established certainty that the data being pointed to will be valid beyond the function scope. This can be achieved by using C++ constructs; concrete types instead of the error-prone C-style pointer constructs.\r\n\r\n**Code to reproduce the issue**\r\ntest cases available under:\r\ntensorflow/lite/micro/kernels/elementwise_test.cc\r\n\r\n**Other info / logs**\r\nSee screenshots attached below:\r\n\r\nThe culprit (tensorflow/lite/micro/testing/test_utils.h):\r\n![Screenshot from 2020-01-09 11-06-37](https://user-images.githubusercontent.com/26050398/72058282-24310480-32d0-11ea-940e-a1faf1fbc17e.png)\r\n\r\nThe test log (tensorflow/lite/micro/kernels/elementwise_test.cc):\r\n![Screenshot from 2020-01-09 11-02-22](https://user-images.githubusercontent.com/26050398/72058451-66f2dc80-32d0-11ea-9059-d85ae50b6fa5.png)", "comments": ["Hello, \r\n\r\nThis is currently being fixed internally. However, as there are many references of the code (tensorflow/lite/micro/testing/test_utils.h) throughout the tests, we cannot provide an ETA on the fix. \r\n\r\nLet us know if this is a blocker for you, otherwise feel free to close the issue.", "Alright, Will do!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35699\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35699\">No</a>\n"]}, {"number": 35698, "title": "Add calls to mli lib functions for ARC platform", "body": "", "comments": ["@petewarden thank you for merging this internally."]}, {"number": 35697, "title": "tensorflow.python.framework.errors_impl.CancelledError: Cancelled, When do distributed training with parameter-server", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 14.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.13.2\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source): 4.8.4\r\n- CUDA/cuDNN version: no CUDA\r\n- GPU model and memory: no GPU\r\n\r\n**Describe the current behavior**\r\nWhen do **distribute training** using **parameter-server strategy**, there are some workers failed with the information `tensorflow.python.framework.errors_impl.CancelledError: Cancelled` **occasionally**.\r\n\r\nThere is no more information, so I have no idea how to debug the error. Is there anyone has seen this error or how to debug?  appreciate your kind help.\r\n\r\n**Code to reproduce the issue**\r\nIt's an occasional problem, so it's not easy to reproduce.\r\n\r\n**Other info / logs**\r\n```\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 676, in run \r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1171, in run \r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1270, in run \r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1255, in run \r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1327, in run \r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1091, in run \r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 929, in run \r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.CancelledError: Cancelled\r\n```", "comments": ["Hmmm....I haven't seen this error before. It's also not easy to debug given the context", "Hi @nolanliou, are you still facing this problem? Can you provide any more info on when you face this issue or the code that occasionally causes the problem?", "It's not easy to reproduce...", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35697\">No</a>\n"]}, {"number": 35696, "title": "About symbols' visibility in shared library", "body": "**System information**\r\n- TensorFlow version (you are using):  v2.0.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAs we can see from [tf_version_script.lds in v2.0.0](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/tf_version_script.lds), none of `Eager*` symbols have been set to `global visibility`. But in newest master codes, this has been [updated](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tf_version_script.lds#L7).\r\n\r\nIn our usage scenario, we need transfer `EagerTensor_Handle` in our python apis to read or update the tensors. It's important that no copy happens in Tensor transfering. However, `EagerTensor*` like symbols depend on `_pywrap_tensorflow_internal.so` are `Local` visibility in current stable release.\r\n\r\n**Will this change the current api? How?**\r\nNo. Only version script file: tf_version_script.lds \r\n\r\n**Who will benefit with this feature?**\r\nThe one who need read or write Tensors outside tensorflow.\r\n\r\n**And more**\r\nMaybe it's not good to update symbols' stripping rule in `v2.0.0` release (or other tags). But this have changed in master branch. When will you release this feature in next edition? Or is this a good way to interchange data between outerside and tf (**with zero copy**)?", "comments": ["It is an API change as new symbols get exposed.\r\n\r\nFor 2.0 and 2.1 it is unlikely that these new symbols will be exposed. However, you can get them from `tf-nightly` and from the TF2.2 release in a few months.", "@mihaimaruseac Thanks for response. Looking forward to the new feature and TF2.2 release."]}, {"number": 35695, "title": "2020-01-09 12:25:17.491189: F tensorflow/lite/toco/graph_transformations/quantize.cc:611] Check failed: is_rnn_state_array ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version:1.14\r\n- Python version:3.7.4\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):7.4\r\n- CUDA/cuDNN version:10.2\r\n- GPU model and memory:GeForce GTX 960M/PCIe/SSE2, 16GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n2020-01-09 12:25:17.491189: F tensorflow/lite/toco/graph_transformations/quantize.cc:611] Check failed: is_rnn_state_array \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI am converting a quantized graph def (.pb ) to a quantized tflite (.tflite) using the dummy quantization and encounter error as follows\r\n```\r\n(tf_gpu_clone) ridlr@ridlr107:~/TensorFlow/exported_model_12k_quantized$ tflite_convert --output_file tflite_graph.tflite --graph_def_file tflite_graph.pb --input_arrays image_tensor --output_arrays TFLite_Detection_PostProcess --input_shapes 1,576,720,3 --allow_custom_ops --inference_type QUANTIZED_UINT8 --std_dev_values 127 --mean_values 128 --default_ranges_min 0 --default_ranges_max 6\r\n2020-01-09 12:25:15.452049: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-09 12:25:15.474575: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-01-09 12:25:15.475004: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561bb6736540 executing computations on platform Host. Devices:\r\n2020-01-09 12:25:15.475031: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"/home/ridlr/anaconda3/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 503, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 499, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 193, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 904, in convert\r\n    **converter_kwargs)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 373, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 172, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2020-01-09 12:25:16.861669: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2020-01-09 12:25:16.957738: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1537 operators, 2264 arrays (0 quantized)\r\n2020-01-09 12:25:17.017901: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1537 operators, 2264 arrays (0 quantized)\r\n2020-01-09 12:25:17.482076: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 181 operators, 341 arrays (0 quantized)\r\n2020-01-09 12:25:17.485583: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 181 operators, 341 arrays (0 quantized)\r\n2020-01-09 12:25:17.486877: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 99 operators, 259 arrays (0 quantized)\r\n2020-01-09 12:25:17.488034: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 99 operators, 259 arrays (0 quantized)\r\n2020-01-09 12:25:17.489088: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 99 operators, 259 arrays (0 quantized)\r\n2020-01-09 12:25:17.489972: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 99 operators, 259 arrays (0 quantized)\r\n2020-01-09 12:25:17.491160: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 99 operators, 259 arrays (0 quantized)\r\n2020-01-09 12:25:17.491189: F tensorflow/lite/toco/graph_transformations/quantize.cc:611] Check failed: is_rnn_state_array \r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007fb839eed740 (most recent call first):\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 299 in run\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/home/ridlr/anaconda3/bin/toco_from_protos\", line 10 in <module>\r\nAborted (core dumped)\r\n\r\n```\r\n\r\nHowever if I do not include the following specifiers a *.tflite is created.\r\n`--inference_type QUANTIZED_UINT8 --std_dev_values 127 --mean_values 128 --default_ranges_min 0 --default_ranges_max 6 `\r\n\r\nThis *.tflite file when used to convert to *_edgetpu.tflite (this model is used to run inference on Google coral) gives the following error\r\n```\r\n(tf_gpu_clone) ridlr@ridlr107:~/TensorFlow/exported_model_12k_quantized$ edgetpu_compiler tflite_graph.tflite \r\nEdge TPU Compiler version 2.0.267685300\r\nInvalid model: tflite_graph.tflite\r\nModel not quantized\r\n```\r\n\r\nHence it is necessary to include the specifiers for quantization.\r\n\r\n", "comments": ["Additional Information\r\n\r\nI have created tflite_graph.pb from export_tflite_ssd_graph.py, quantized checkpoint and config files. Using the following command\r\n\r\n`python object_detection/export_tflite_ssd_graph.py --pipeline_config_path /home/ridlr/TensorFlow/exported_model_12k_quantized/pipeline.config  --trained_checkpoint_prefix /home/ridlr/TensorFlow/exported_model_12k_quantized/model.ckpt --output_directory /home/ridlr/TensorFlow/exported_model_12k_quantized/`\r\n", "Hi,\r\n\r\nAny information about this issue.", "I have tried to convert a base model ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03. I have converted this model to a frozen graph as follows successfully\r\n\r\n`python object_detection/export_tflite_ssd_graph.py --pipeline_config_path /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/pipeline.config --trained_checkpoint_prefix /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt --output_directory /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph_export_tflite/ --add_postprocessing_op=true`\r\n\r\nNow I am converting this frozen to graph to tflite with the command as follows\r\n```\r\n(tf_gpu_clone) ridlr@ridlr107:~/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph_export_tflite$ tflite_convert --output_file tflite_graph.tflite --graph_def_file tflite_graph.pb --input_arrays normalized_input_image_tensor --output_arrays TFLite_Detection_PostProcess --input_shapes 1,300,300,3 --inference_type QUANTIZED_UINT8 --std_dev_values 127 --mean_values 128 --default_ranges_min 0 --default_ranges_max 6\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-01-16 16:24:24.250854: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2020-01-16 16:24:24.259891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:24:24.260244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\n2020-01-16 16:24:24.260418: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 16:24:24.261524: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2020-01-16 16:24:24.262887: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\r\n2020-01-16 16:24:24.263114: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\r\n2020-01-16 16:24:24.264526: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-16 16:24:24.265295: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-16 16:24:24.268320: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-16 16:24:24.268471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:24:24.268943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:24:24.269179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2020-01-16 16:24:24.269493: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-01-16 16:24:24.294484: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-01-16 16:24:24.295098: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e96396fac0 executing computations on platform Host. Devices:\r\n2020-01-16 16:24:24.295120: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-01-16 16:24:24.295298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:24:24.295575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\n2020-01-16 16:24:24.295631: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 16:24:24.295681: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2020-01-16 16:24:24.295700: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\r\n2020-01-16 16:24:24.295730: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\r\n2020-01-16 16:24:24.295760: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-16 16:24:24.295777: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-16 16:24:24.295796: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-16 16:24:24.295853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:24:24.296084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:24:24.296279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2020-01-16 16:24:24.296313: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 16:24:24.327472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-16 16:24:24.327529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2020-01-16 16:24:24.327538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2020-01-16 16:24:24.327722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:24:24.328031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:24:24.328235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:24:24.328418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 764 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-01-16 16:24:24.329712: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e966f209c0 executing computations on platform CUDA. Devices:\r\n2020-01-16 16:24:24.329726: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0\r\nTraceback (most recent call last):\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 503, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ridlr/.local/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ridlr/.local/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 499, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 193, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 904, in convert\r\n    **converter_kwargs)\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 373, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 172, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-01-16 16:24:25.878202: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2020-01-16 16:24:25.980693: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1537 operators, 2263 arrays (0 quantized)\r\n2020-01-16 16:24:26.040496: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1537 operators, 2263 arrays (0 quantized)\r\n2020-01-16 16:24:26.511304: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 182 operators, 341 arrays (1 quantized)\r\n2020-01-16 16:24:26.514996: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 182 operators, 341 arrays (1 quantized)\r\n2020-01-16 16:24:26.516329: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 100 operators, 259 arrays (1 quantized)\r\n2020-01-16 16:24:26.517532: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 100 operators, 259 arrays (1 quantized)\r\n2020-01-16 16:24:26.518674: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 100 operators, 259 arrays (1 quantized)\r\n2020-01-16 16:24:26.519589: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 100 operators, 259 arrays (1 quantized)\r\n2020-01-16 16:24:26.520845: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 100 operators, 259 arrays (1 quantized)\r\n2020-01-16 16:24:26.586776: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 1: 106 operators, 265 arrays (236 quantized)\r\n2020-01-16 16:24:26.592195: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 2: 106 operators, 265 arrays (240 quantized)\r\n2020-01-16 16:24:26.596841: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 3: 101 operators, 260 arrays (242 quantized)\r\n2020-01-16 16:24:26.601577: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 4: 101 operators, 260 arrays (243 quantized)\r\n2020-01-16 16:24:26.605346: W tensorflow/lite/toco/graph_transformations/quantize.cc:132] Constant array anchors lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.\r\n2020-01-16 16:24:26.605481: W tensorflow/lite/toco/graph_transformations/quantize.cc:622] (Unsupported TensorFlow op: TFLite_Detection_PostProcess) is a quantized op but it has a model flag that sets the output arrays to float.\r\n2020-01-16 16:24:26.605488: W tensorflow/lite/toco/graph_transformations/quantize.cc:622] (Unsupported TensorFlow op: TFLite_Detection_PostProcess) is a quantized op but it has a model flag that sets the output arrays to float.\r\n2020-01-16 16:24:26.606381: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 5: 99 operators, 258 arrays (244 quantized)\r\n2020-01-16 16:24:26.606729: W tensorflow/lite/toco/graph_transformations/quantize.cc:622] (Unsupported TensorFlow op: TFLite_Detection_PostProcess) is a quantized op but it has a model flag that sets the output arrays to float.\r\n2020-01-16 16:24:26.610982: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before shuffling of FC weights: 99 operators, 258 arrays (244 quantized)\r\n2020-01-16 16:24:26.614144: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 3060032 bytes, theoretical optimal value: 2700032 bytes.\r\n2020-01-16 16:24:26.614533: I tensorflow/lite/toco/toco_tooling.cc:433] Estimated count of arithmetic ops: 1.56954 billion (note that a multiply-add is counted as 2 ops).\r\n2020-01-16 16:24:26.614956: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\nTraceback (most recent call last):\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ridlr/.local/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ridlr/.local/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\n\r\n```\r\n\r\nWhen I change the --input_arrays and  --input_shapes the output of the command is as follows and results in error Check failed:is_rnn_state_array.\r\n\r\n```\r\n(tf_gpu_clone) ridlr@ridlr107:~/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph_export_tflite$ tflite_convert --output_file tflite_graph.tflite --graph_def_file tflite_graph.pb --input_arrays image_tensor --output_arrays TFLite_Detection_PostProcess --input_shapes 1,576,720,3 --inference_type QUANTIZED_UINT8 --std_dev_values 127 --mean_values 128 --default_ranges_min 0 --default_ranges_max 6\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-01-16 16:29:49.244407: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2020-01-16 16:29:49.256539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:29:49.256795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\n2020-01-16 16:29:49.256980: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 16:29:49.258169: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2020-01-16 16:29:49.259466: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\r\n2020-01-16 16:29:49.259715: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\r\n2020-01-16 16:29:49.261109: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-16 16:29:49.261899: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-16 16:29:49.265052: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-16 16:29:49.265205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:29:49.265495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:29:49.265709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2020-01-16 16:29:49.266019: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-01-16 16:29:49.290476: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-01-16 16:29:49.291137: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55766d722a80 executing computations on platform Host. Devices:\r\n2020-01-16 16:29:49.291160: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-01-16 16:29:49.291357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:29:49.291576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\n2020-01-16 16:29:49.291608: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 16:29:49.291619: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2020-01-16 16:29:49.291660: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\r\n2020-01-16 16:29:49.291669: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\r\n2020-01-16 16:29:49.291680: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-16 16:29:49.291690: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-16 16:29:49.291700: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-16 16:29:49.291745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:29:49.291956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:29:49.292135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2020-01-16 16:29:49.292158: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 16:29:49.327820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-16 16:29:49.327863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2020-01-16 16:29:49.327870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2020-01-16 16:29:49.328012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:29:49.328261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:29:49.328479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 16:29:49.328679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 758 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-01-16 16:29:49.329922: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557670cd3560 executing computations on platform CUDA. Devices:\r\n2020-01-16 16:29:49.329941: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0\r\nTraceback (most recent call last):\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 503, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ridlr/.local/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ridlr/.local/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 499, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 193, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 904, in convert\r\n    **converter_kwargs)\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 373, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 172, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-01-16 16:29:50.875328: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2020-01-16 16:29:50.977286: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1537 operators, 2264 arrays (0 quantized)\r\n2020-01-16 16:29:51.036947: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1537 operators, 2264 arrays (0 quantized)\r\n2020-01-16 16:29:51.503252: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 181 operators, 341 arrays (0 quantized)\r\n2020-01-16 16:29:51.506779: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 181 operators, 341 arrays (0 quantized)\r\n2020-01-16 16:29:51.508019: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 99 operators, 259 arrays (0 quantized)\r\n2020-01-16 16:29:51.509210: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 99 operators, 259 arrays (0 quantized)\r\n2020-01-16 16:29:51.510286: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 99 operators, 259 arrays (0 quantized)\r\n2020-01-16 16:29:51.511177: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 99 operators, 259 arrays (0 quantized)\r\n2020-01-16 16:29:51.512363: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 99 operators, 259 arrays (0 quantized)\r\n2020-01-16 16:29:51.512393: F tensorflow/lite/toco/graph_transformations/quantize.cc:611] Check failed: is_rnn_state_array \r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f44ec596740 (most recent call first):\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/home/ridlr/.local/lib/python3.6/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/home/ridlr/.local/lib/python3.6/site-packages/absl/app.py\", line 299 in run\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/home/ridlr/anaconda3/envs/tf_gpu_clone/bin/toco_from_protos\", line 11 in <module>\r\nAborted (core dumped)\r\n\r\n\r\n```", "The input and output arrays value is given by \r\n\r\n```\r\n(tf_gpu_clone) ridlr@ridlr107:~/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph_export_tflite$ /home/ridlr/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=/home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph_export_tflite/tflite_graph.pb --print_structure=true\r\nFound 1 possible inputs: (name=normalized_input_image_tensor, type=float(1), shape=[1,300,300,3]) \r\nNo variables spotted.\r\nFound 1 possible outputs: (name=TFLite_Detection_PostProcess, op=TFLite_Detection_PostProcess) \r\nFound 6112114 (6.11M) const parameters, 0 (0) variable parameters, and 0 control_edges\r\nOp types used: 725 Const, 719 Identity, 180 Mul, 154 FakeQuantWithMinMaxVars, 130 Add, 60 Sub, 60 Rsqrt, 55 Conv2D, 43 Relu6, 29 Reshape, 17 DepthwiseConv2dNative, 12 BiasAdd, 2 ConcatV2, 1 TFLite_Detection_PostProcess, 1 Squeeze, 1 Sigmoid, 1 RealDiv, 1 Placeholder\r\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=/home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph_export_tflite/tflite_graph.pb --show_flops --input_layer=normalized_input_image_tensor --input_layer_type=float --input_layer_shape=1,300,300,3 --output_layer=TFLite_Detection_PostProcess\r\n```\r\n\r\n", "There is Docker provided by Coral which has all the necessary environment set.\r\nDocker Link :\r\nhttps://coral.ai/docs/edgetpu/retrain-detection/#set-up-the-docker-container\r\n\r\nHowever the conversion is successfull only when using the standard parameters while tflite_convert.\r\n\r\nWhen I try to convert my custom model by changing input_arrays, input_shapes I get the same error\r\n\r\n```\r\n\r\nroot@1ae3dc55a8ca:~/base_model/exported_model_12k_quantized# tflite_convert   --output_file=tflite_graph_576_720.tflite   --graph_def_file=tflite_graph.pb   --inference_type=QUANTIZED_UINT8   --input_arrays=image_tensor   --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --default_ranges_min=0 --default_ranges_max=6   --mean_values=128   --std_dev_values=128   --input_shapes=1,576,720,3   --change_concat_input_ranges=false   --allow_nudging_weights_to_use_fast_gemm_kernel=true   --allow_custom_ops\r\n2020-01-22 05:12:46.931163: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 162, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 311, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\n2020-01-22 05:12:48.742828: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2020-01-22 05:12:48.845288: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1537 operators, 2267 arrays (0 quantized)\r\n2020-01-22 05:12:48.915931: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1537 operators, 2267 arrays (0 quantized)\r\n2020-01-22 05:12:49.491249: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 181 operators, 344 arrays (0 quantized)\r\n2020-01-22 05:12:49.494982: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 181 operators, 344 arrays (0 quantized)\r\n2020-01-22 05:12:49.496398: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 99 operators, 262 arrays (0 quantized)\r\n2020-01-22 05:12:49.497525: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 99 operators, 262 arrays (0 quantized)\r\n2020-01-22 05:12:49.498378: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 99 operators, 262 arrays (0 quantized)\r\n2020-01-22 05:12:49.499539: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 99 operators, 262 arrays (0 quantized)\r\n2020-01-22 05:12:49.499571: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:589] Check failed: is_rnn_state_array\r\nAborted (core dumped)\r\n\r\nNone\r\n```\r\n\r\nHow can we convert the model to accept custom input parameters?", "I got the same error as yours. I tried the simple model as follows:\r\n```\r\ninput1 = keras.layers.Input(shape=(input_length,))\r\ninput2 = keras.layers.Input(shape=(input_length,))\r\nx1     = keras.layers.Add()([input1, input2])\r\nmodel  = tf.keras.models.Model(inputs=[input1, input2], outputs=x1)\r\n```\r\nWhen I use toco to try to convert to .tflite, I got:\r\n```\r\nF tensorflow/lite/toco/graph_transformations/quantize.cc:606] Check failed: is_rnn_state_array\r\n```\r\n\r\nThis is weird since I notice that ADD operation should be supported by coral/m2 edgetpu, but the converter cannot convert it successfully? (from this [thread](https://github.com/tensorflow/tensorflow/issues/29117#issuecomment-498103605))", "Another simple model I tried works:\r\n```\r\n input1 = keras.layers.Input(shape=(input_length*2, ))\r\n split  = Lambda(lambda x: tf.split(x, num_or_size_splits=2, axis=1))(input1)\r\n x1     = keras.layers.Add()(split)\r\n model  = tf.keras.models.Model(inputs=[input1], outputs=x1)\r\n```\r\nIt gives the following :\r\n```\r\nOperator                       Count      Status\r\n\r\nADD                            1          Mapped to Edge TPU\r\nSPLIT                          1          Mapped to Edge TPU\r\n```\r\n\r\nIt doesn't make sense.\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35695\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35695\">No</a>\n"]}, {"number": 35694, "title": "MultiWorkerMirroredStrategy stuck", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow 1.15-2.1\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nMultiWorkerMirroredStrategy stucks at start server when I run cluster on different computers.\r\n**Describe the expected behavior**\r\nWorks same as on a computer\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\nimport json\r\nimport os\r\nimport tensorflow_datasets as tfds\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 4\r\n\r\ndef make_datasets_unbatched():\r\n  # Scaling MNIST data from (0, 255] to (0., 1.]\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n\r\n  datasets, info = tfds.load(name='mnist',\r\n                            with_info=True,\r\n                            as_supervised=True)\r\n\r\n  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\r\n\r\n\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  model.compile(\r\n      loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n      metrics=['accuracy'])\r\n  return model\r\n\r\ndef train_task(index):\r\n    os.environ['TF_CONFIG'] = json.dumps({\r\n        'cluster': {\r\n            'worker': [\"ip1:9901\",\"ip2:9902\"],\r\n        },\r\n        'task': {'type': 'worker', 'index': index},\r\n    })\r\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    # Here the batch size scales up by number of workers since\r\n    # `tf.data.Dataset.batch` expects the global batch size. Previously we used 64,\r\n    # and now this becomes 128.\r\n    GLOBAL_BATCH_SIZE = 12\r\n    train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\r\n    with strategy.scope():\r\n        multi_worker_model = build_and_compile_cnn_model()\r\n    multi_worker_model.fit(x=train_datasets, epochs=3)\r\n\r\n# runs on ip1\r\ntrain_task(0)\r\n# runs on ip2\r\n# train_task(1)\r\n```\r\n**Other info / logs**\r\nAbove code works on a single computer.\r\nPorts are accessible between computers.\r\nI have searched on the internet and read serveal books, but I still can't find solution.", "comments": ["@fsx950223 Can you move \r\n`train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)` within the strategy scope? So your code above would look like:\r\n\r\n```\r\nwith strategy.scope():\r\n        train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\r\n        multi_worker_model = build_and_compile_cnn_model()\r\n```\r\n", "> @fsx950223 Can you move\r\n> `train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)` within the strategy scope? So you code above would look like:\r\n> \r\n> ```\r\n> with strategy.scope():\r\n>         train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\r\n>         multi_worker_model = build_and_compile_cnn_model()\r\n> ```\r\n\r\nThe problem is solved. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35694\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35694\">No</a>\n", "Can `strategy.experimental_distribute_dataset(dataset)` also be used to fix the stuck?"]}, {"number": 35693, "title": "Correction in calculation of error while Naive Forecasting", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c03_moving_average.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIt should be 'mean absolute error' instead of squared error while Naive Forecasting\r\n\r\n![Screenshot from 2020-01-09 12-10-09](https://user-images.githubusercontent.com/29497701/72044240-4bd89a80-32d9-11ea-937f-a189784b83b0.png)\r\n\r\n### Submit a pull request?\r\n\r\nYes, I'll be submitting one shortly", "comments": ["@ManishAradwad \r\ncan we please move this issue to closed as there is a pr [#140] to monitor this"]}, {"number": 35692, "title": "tflite_convert failed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.14\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n(tf_gpu_clone) ridlr@ridlr107:~/TensorFlow/exported_model_12k_quantized$ tflite_convert --output_file tflite_graph.tflite --graph_def_file tflite_graph.pb --input_arrays image_tensor --output_arrays TFLite_Detection_PostProcess --input_shapes 1,576,720,3\r\n2020-01-09 12:10:44.239300: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-09 12:10:44.262441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-01-09 12:10:44.262923: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558c8fa667e0 executing computations on platform Host. Devices:\r\n2020-01-09 12:10:44.262939: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"/home/ridlr/anaconda3/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 503, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 499, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 193, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 904, in convert\r\n    **converter_kwargs)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 373, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 172, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2020-01-09 12:10:45.667362: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2020-01-09 12:10:45.763812: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1537 operators, 2264 arrays (0 quantized)\r\n2020-01-09 12:10:45.824420: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1537 operators, 2264 arrays (0 quantized)\r\n2020-01-09 12:10:46.292215: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 181 operators, 341 arrays (0 quantized)\r\n2020-01-09 12:10:46.295908: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 181 operators, 341 arrays (0 quantized)\r\n2020-01-09 12:10:46.298914: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 181 operators, 341 arrays (0 quantized)\r\n2020-01-09 12:10:46.304648: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 20160000 bytes, theoretical optimal value: 17280000 bytes.\r\n2020-01-09 12:10:46.305189: I tensorflow/lite/toco/toco_tooling.cc:433] Estimated count of arithmetic ops: 1.29335 billion (note that a multiply-add is counted as 2 ops).\r\n2020-01-09 12:10:46.305598: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/Conv/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305607: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305629: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305633: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_1/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305636: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305641: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_1/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305645: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_2/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305650: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305654: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_2/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305658: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_2/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305662: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_3/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305665: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305669: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_3/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305674: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_4/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305678: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305681: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_4/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305684: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_4/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305688: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_5/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305692: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305696: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_5/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305700: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_5/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305703: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_6/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305706: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305709: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_6/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305713: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_7/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305717: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305721: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_7/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305725: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_7/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305729: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_8/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305733: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305737: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_8/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305741: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_8/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305745: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_9/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305749: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305753: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_9/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305758: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_9/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305762: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_10/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305766: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305770: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_10/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305774: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_11/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305778: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305782: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_11/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305786: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_11/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305790: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_12/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305793: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305796: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_12/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305800: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_12/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305803: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_13/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305807: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305811: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_13/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305815: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_14/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305819: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305823: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_14/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305827: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_14/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305831: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_15/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305835: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305839: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_15/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305843: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_15/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305847: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_16/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305851: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305855: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_16/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305859: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/Conv_1/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305863: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305867: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305871: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305875: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305879: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305883: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305887: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305891: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305896: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_0/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305900: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_0/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305904: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_1/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305908: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_1/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305912: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_2/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305916: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_2/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305920: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_3/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305924: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_3/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305928: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_4/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305932: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_4/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305936: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_5/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305940: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_5/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\r\n2020-01-09 12:10:46.305998: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FAKE_QUANT, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\nTraceback (most recent call last):\r\n  File \"/home/ridlr/anaconda3/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FAKE_QUANT, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n", "comments": ["Adding --allow_custom_ops in the command resolved the issue and I was able to generate a .tflite file.\r\n\r\nHowever this file is not quantized tflite and not relevant for me even though it is converted from a quantized graph def file. This issue is reported in https://github.com/tensorflow/tensorflow/issues/35690\r\n", "@batulrangwala Can you please share a standalone code to reproduce the issue? Thanks!", "Kindly explain what do expect when you ask for standalone code.\r\n\r\n tflite_graph.pb is generated using the following command \r\n`\r\npython object_detection/export_tflite_ssd_graph.py --pipeline_config_path /home/ridlr/TensorFlow/exported_model_12k_quantized/pipeline.config --trained_checkpoint_prefix /home/ridlr/TensorFlow/exported_model_12k_quantized/model.ckpt --output_directory /home/ridlr/TensorFlow/exported_model_12k_quantized/`", "System information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos 10.12\r\nTensorFlow installed from (source or binary): pip\r\nTensorFlow version (or github SHA if from source): 1.15.0\r\n\r\nI also have similar problem\uff1aIf I set `converter.post_training_quantize = True` and get the quantized tflite model\uff0cit can't be loaded, it says as follow\r\n```\r\n----> 7 interpreter = tf.lite.Interpreter(model_path=\"/Users/zhuxinyu/Documents/dkt/models/infer_models/uncompressed_models/wordtest180w_var_fix_time/model1.tflite\")\r\n      8 interpreter.allocate_tensors()\r\n      9 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow_core/lite/python/interpreter.py in __init__(self, model_path, model_content, experimental_delegates)\r\n    204       self._interpreter = (\r\n    205           _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n--> 206               model_path))\r\n    207       if not self._interpreter:\r\n    208         raise ValueError('Failed to open {}'.format(model_path))\r\n\r\nValueError: Input array not provided for operation 'reshape'.\r\n```\r\n", "There is documentation available [here](https://github.com/rockchip-linux/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#convert-a-tensorflow-graphdef-to-tensorflow-lite-for-quantized-inference-) which shows an example of converting an SSD model. An example command might look like the following if you are using the Python installation:\r\n\r\n```\r\ntflite_convert\r\n  --input_file=/tmp/some_quantized_graph.pb \\\r\n  --output_file=/tmp/foo.tflite \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_shape=1,128,128,3 \\\r\n  --input_array=input \\\r\n  --output_array=MobilenetV1/Predictions/Reshape_1 \\\r\n  --mean_value=128 \\\r\n  --std_value=127\r\n```\r\n\r\nThe details of each flag are available [here](https://github.com/rockchip-linux/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_reference.md).", "> There is documentation available [here](https://github.com/rockchip-linux/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#convert-a-tensorflow-graphdef-to-tensorflow-lite-for-quantized-inference-) which shows an example of converting an SSD model. An example command might look like the following if you are using the Python installation:\r\n> \r\n> ```\r\n> tflite_convert\r\n>   --input_file=/tmp/some_quantized_graph.pb \\\r\n>   --output_file=/tmp/foo.tflite \\\r\n>   --inference_type=QUANTIZED_UINT8 \\\r\n>   --input_shape=1,128,128,3 \\\r\n>   --input_array=input \\\r\n>   --output_array=MobilenetV1/Predictions/Reshape_1 \\\r\n>   --mean_value=128 \\\r\n>   --std_value=127\r\n> ```\r\n> \r\n> The details of each flag are available [here](https://github.com/rockchip-linux/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_reference.md).\r\n\r\n@gargn Thank you for your answer, I know how to convert the model, but my problem is if the model has reshape operation in its graph, it can't be loaded because using the QUANTIZED_UINT8 inference_type makes the infer model not able to support the reshape op\uff0csomeone says tf1.15 doesn't support reshape in quantized model, I found that if I do not quantize the model and just convert it to tflite, it works correctly, so if I can make the quanitized model support reshape op or if I can  make the reshape op not happen in dense layer? ", "@jianlijianli Do you happen to have any insight on this quantization related issue?", "@gargn I found the solution in issue #35841 .Thanks!", "Using a docker resolved this issue. The Docker is provided by Coral and has all the necessary environment set.\r\nDocker Link :\r\nhttps://coral.ai/docs/edgetpu/retrain-detection/#set-up-the-docker-container"]}, {"number": 35691, "title": "A model results inconsistent output values depending on input batch size", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): python:3.7.5-slim docker image, macOS Catalina 10.15.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip 19.3.1\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.5(docker), 3.7.4(macOs)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nThe `predict` method of `tf.keras.applications.mobilenet.MobileNet`, gives different output values when the batch sizes are different even though all input value are the same.\r\nThis problem does not occur with Tensorflow 2.0.0 and occurs with 2.1.0\r\n\r\n**Describe the expected behavior**\r\nThe output value of an input does not change as it is in another batch with different size.\r\n\r\n**Code to reproduce the issue**\r\n``` python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef test_mobile_net():\r\n    four_black_images = np.zeros((4, 224, 224, 3), dtype='uint8')\r\n    one_black_image = np.zeros((1, 224, 224, 3), dtype='uint8')\r\n\r\n    four_input = tf.keras.applications.mobilenet.preprocess_input(four_black_images)\r\n    one_input = tf.keras.applications.mobilenet.preprocess_input(one_black_image)\r\n\r\n    # below assert statement passes, meaning all input batches have the same input values\r\n    for i in range(4):\r\n        assert (four_input[i] == one_input[0]).all()\r\n\r\n    model = tf.keras.applications.mobilenet.MobileNet(input_shape=(224, 224, 3), include_top=False, pooling='avg')\r\n\r\n    one_image_result = model.predict(one_input)\r\n    four_images_result = model.predict(four_input)\r\n\r\n    # bellow assert statement passes\r\n    # There is no inter-differences between  output values within the same batch\r\n    for i in range(4):\r\n        for j in range(4):\r\n            assert (four_images_result[i] == four_images_result[j]).all()\r\n\r\n    # Bellow assert statement fails with Tensorflow 2.1.0 but passes with 2.0.0\r\n    assert (four_images_result[0] == one_image_result[0]).all()\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI also have tested with batch size with 4 and 7, wondering if the output result is only different when the batch has size 1, but it also gave inconsistent results. \r\nSo the output value changes as batch size changes with version 2.1.0.", "comments": ["I have tried on colab with TF version 2.0 and i am not seeing any issue. However with TF version 2.1 I am seeing `AssertionError: `.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c67db102bcfbfbeaeda51e28c3620efc/untitled541.ipynb). Thanks!", "@donguklim I agree. `AssertionError: ` is reproducible with `tf-nightly` but not with `TF2.0`. However, the inconsistency is in 5th decimal when I applied `tf.reduce_sum` for the two data. So I would say the inconsistency could be due to precision related. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/4009ed5d1e5f99b942c4f175e1439809/untitled741.ipynb).\r\n\r\nOutput of your code with `tf.reduce_sum` is as follows. You can check the results differ only in the 5th decimal. Thanks\r\n\r\n```\r\nFour tf.Tensor(76.64975, shape=(), dtype=float32)\r\nSingle: tf.Tensor(76.64976, shape=(), dtype=float32)\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-8-646cdf461c0a> in <module>()\r\n     29     print('Single:',tf.reduce_sum(one_image_result[0]))\r\n     30     assert (four_images_result[0] == one_image_result[0]).all()\r\n---> 31 test_mobile_net()\r\n\r\n<ipython-input-8-646cdf461c0a> in test_mobile_net()\r\n     28     print(\"Four\",tf.reduce_sum(four_images_result[0]))\r\n     29     print('Single:',tf.reduce_sum(one_image_result[0]))\r\n---> 30     assert (four_images_result[0] == one_image_result[0]).all()\r\n     31 test_mobile_net()\r\n\r\nAssertionError: \r\n```", "I agree with  @jvishnuvardhan that the numerical difference is precision related and very insignificant. Rather than use \"==\", you can use np.allclose() which will return True.\r\n\r\nClosing this bug since there is nothing need to be addressed here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35691\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35691\">No</a>\n"]}, {"number": 35690, "title": "Determine input_arrays and output_arrays values for tflite_convert", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version:1.14\r\n- Python version:3.7.4\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):7.4\r\n- CUDA/cuDNN version:10.2\r\n- GPU model and memory:GeForce GTX 960M/PCIe/SSE2, 16GB\r\n\r\n\r\n\r\n**Describe the problem**\r\ntflite_convert : need to know the values for inout_arrays and --output_arrays\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI have created a tflite_graph.pb from export_tflite_ssd_graph.py, quantized checkpoint and config files succesfully.\r\n\r\nMy task is to generate a .tflite file using the generated graph_def_file using the tflite_convert command. And then use this to generate a edgetpu.tflite file to run on Google coral.\r\nfollowing is the log of the command\r\n\r\n```\r\n(tf_gpu_clone) ridlr@ridlr107:~/TensorFlow/exported_model_12k_quantized$ tflite_convert --output_file tflite_graph.tflite --graph_def_file tflite_graph.pb --input_arrays image_tensor --output_arrays detection_boxes --input_shapes 1,576,720,3\r\n2020-01-09 11:05:56.913487: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-09 11:05:56.934582: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-01-09 11:05:56.935469: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5569f40357d0 executing computations on platform Host. Devices:\r\n2020-01-09 11:05:56.935514: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"/home/ridlr/anaconda3/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 503, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 499, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 193, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 904, in convert\r\n    **converter_kwargs)\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 373, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 172, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2020-01-09 11:05:58.375534: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2020-01-09 11:05:58.456167: F tensorflow/lite/toco/tooling_util.cc:918] Check failed: GetOpWithOutput(model, output_array) Specified output array \"detection_boxes\" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f347d2e6740 (most recent call first):\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 299 in run\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/home/ridlr/anaconda3/bin/toco_from_protos\", line 10 in <module>\r\nAborted (core dumped)\r\n```\r\n\r\nHow do I determine the correct value of --output_array.\r\n\r\n\r\n**Any other info / logs**\r\nIf I use the specifier --inference_type=QUANTIZED_UINT8\r\nHow do I determine the values of following specifiers?\r\n--std_dev_values \r\n--mean_values \r\n--default_ranges_min \r\n--default_ranges_max\r\n", "comments": ["@batulrangwala Did you try using [`netron`](https://github.com/lutzroeder/netron) or tensorboard to visualize input/outputs of the model .pb file? You can check couple of resources [1](https://medium.com/@daj/how-to-inspect-a-pre-trained-tensorflow-model-5fd2ee79ced0) or [a colab gist here](https://colab.sandbox.google.com/drive/1A9X34s4LkhtkcoFqB6PYCCaGCLAcJRbp#scrollTo=5d7KEDIr6a_t). Thanks!", "I have used tensorflow/tools/graph_transforms/summarize_graph_main.cc from tensorflow (github : https://github.com/tensorflow/tensorflow) to determine the inputs/outputs of the model (.pb) file\r\n\r\nhttps://colab.sandbox.google.com/drive/1A9X34s4LkhtkcoFqB6PYCCaGCLAcJRbp#scrollTo=5d7KEDIr6a_t\r\nI have requested access now for this. have to wait!\r\n\r\nhttps://medium.com/@daj/how-to-inspect-a-pre-trained-tensorflow-model-5fd2ee79ced0\r\nI am unable to view this link currently. \r\n\r\nHow can I use netron to determine the input/output of the model (.pb) file. \r\nCurrently I am using following vales (determined from tensorflow tool summarize_graph).\r\n--input_arrays image_tensor --output_arrays TFLite_Detection_PostProcess --input_shapes 1,576,720,3\r\n\r\nBut to convert to a quantized model (.tflite) I have to add the following parameters\r\n--inference_type=QUANTIZED_UINT8\r\n--std_dev_values\r\n--mean_values\r\n--default_ranges_min\r\n--default_ranges_max\r\n\r\nHow do I determine the values of following specifiers? will these be available from netron?\r\n--std_dev_values\r\n--mean_values\r\n--default_ranges_min\r\n--default_ranges_max\r\n\r\nWhen I use dummy values it leads me to  2020-01-09 12:25:17.491189: F tensorflow/lite/toco/graph_transformations/quantize.cc:611] Check failed: is_rnn_state_array #35695\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Using a docker resolved this issue. The Docker is provided by Coral and has all the necessary environment set.\r\nDocker Link :\r\nhttps://coral.ai/docs/edgetpu/retrain-detection/#set-up-the-docker-container"]}, {"number": 35688, "title": "RuntimeError: Encountered unresolved custom op: Enter.Node number 8 (Enter) failed to prepare.", "body": "I have converted tf.keras model to tf.lite successfully. However, when I use it for inference, I get an error. Is there anyone who can resolve it? Thanks!\r\n\r\ncode:\r\n`interpreter = tf.lite.Interpreter(model_path=\"E:/object_detection/EfficientDet-region_anchor_opt_mbconv-head-ckpts/tflites/ckpts_B0_image-size-768/mbconv-se-head_1e-5_unfreeze-backbone_freeze-bn/csv_04_0.6736_0.7484_opts.tflite\")\r\n\r\ninterpreter.allocate_tensors()`\r\n\r\nerror:\r\n`RuntimeError                              Traceback (most recent call last)\r\n<ipython-input-12-ca8eb7ec6089> in <module>()\r\n      1 # interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n----> 2 interpreter.allocate_tensors()\r\n      3 # help(tf.lite.Interpreter)\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\lite\\python\\interpreter.py in allocate_tensors(self)\r\n    242   def allocate_tensors(self):\r\n    243     self._ensure_safe()\r\n--> 244     return self._interpreter.AllocateTensors()\r\n    245 \r\n    246   def _safe_to_run(self):\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\lite\\python\\interpreter_wrapper\\tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)\r\n    104 \r\n    105     def AllocateTensors(self):\r\n--> 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n    107 \r\n    108     def Invoke(self):\r\n\r\nRuntimeError: Encountered unresolved custom op: Enter.Node number 8 (Enter) failed to prepare.`", "comments": ["@2696120622, Can you provide the standalone code to reproduce the issue and also provide tensorflow version. Thanks", "> @2696120622, Can you provide the standalone code to reproduce the issue and also provide tensorflow version. Thanks\r\n\r\nOK, Thanks!\r\nMy tensorflow version is 1.15.0-rc0.\r\nMy code as follows.\r\n`\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nimport layers as layers_new\r\nimport initializers\r\nimport losses\r\nimport efficientnet\r\n\r\ncustom_objects = {\r\n    'BatchNormalization': layers_new.BatchNormalization,\r\n    'swish'            : efficientnet.get_swish(backend=keras.backend,layers=keras.layers,models=keras.models,utils=keras.utils),\r\n    'FixedDropout'     : efficientnet.get_dropout(backend=keras.backend,layers=keras.layers,models=keras.models,utils=keras.utils),\r\n    'wBiFPNAdd'        : layers_new.wBiFPNAdd,\r\n    'PriorProbability' : initializers.PriorProbability,\r\n    'RegressBoxes'     : layers_new.RegressBoxes,\r\n    'FilterDetections' : layers_new.FilterDetections,\r\n    'ClipBoxes'        : layers_new.ClipBoxes,\r\n    '_smooth_l1'       : losses.smooth_l1(),\r\n    '_focal'           : losses.focal(),\r\n}\r\ninput_shapes = {'input_1':[None,512,512,3],'input_4':[None,49104,4]}\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file('E:/object_detection/Efficient-region-checkpoints/inference_checkpoints/inference_checkpoints_B0/freeze-backbone-false_1e-5/csv_05_0.3995_0.5901.h5',\r\n                                                          custom_objects=custom_objects,\r\n                                                          input_shapes=input_shapes,\r\n                                                          )\r\nconverter.allow_custom_ops=True\r\ntflite_model = converter.convert()\r\nopen(\"E:/object_detection/Efficient-region-checkpoints/tflites/inference_checkpoints_B0/freeze-backbone-false_1e-5/csv_05_0.3995_0.5901.tflite\", \"wb\").write(tflite_model)\r\ninterpreter = tf.lite.Interpreter(model_path=\"E:/object_detection/Efficient-region-checkpoints/tflites/inference_checkpoints_B0/freeze-backbone-false_1e-5/csv_05_0.3995_0.5901.tflite\")\r\ninterpreter.allocate_tensors()\r\n`\r\n\r\nThen I get the following error.\r\n\r\n`RuntimeError                              Traceback (most recent call last)\r\n<ipython-input-13-e8f284b41149> in <module>()\r\n----> 1 interpreter.allocate_tensors()\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\lite\\python\\interpreter.py in allocate_tensors(self)\r\n    242   def allocate_tensors(self):\r\n    243     self._ensure_safe()\r\n--> 244     return self._interpreter.AllocateTensors()\r\n    245 \r\n    246   def _safe_to_run(self):\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\lite\\python\\interpreter_wrapper\\tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)\r\n    104 \r\n    105     def AllocateTensors(self):\r\n--> 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n    107 \r\n    108     def Invoke(self):\r\n\r\nRuntimeError: Encountered unresolved custom op: Enter.Node number 8 (Enter) failed to prepare.`\r\n\r\nDoes retinanet or efficientdet require control flow? Could you help me?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35688\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35688\">No</a>\n"]}, {"number": 35687, "title": "multi_gpu_model is slower than single gpu model.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: Python 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 2*Titan XP - 12GB memory per card\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThe performance of a multi_gpu_model is slower than using a single GPU. When the model is running on a single GPU, the compute performance of the GPU tops at 91% to 95%, but when model is running on a multiple GPUs, the performance varies from 9% to 21%.\r\n\r\n**Describe the expected behavior**\r\n\r\nExpect the model to train faster on multiple GPUs compared to single GPU.\r\n\r\n**Code to reproduce the issue**\r\n\r\nAll the Convolution networks I've tried.\r\n\r\n", "comments": ["@CleanPegasus, Can you provide the standalone code to replicate the reported issue. Thanks!", "@CleanPegasus, Any update!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35686, "title": "Fix Validation error on GenerateBoxProposals op", "body": "This PR fixes a argument validation error introduced during the review process due to inversion of logic between CHECK* and OP_REQUIRES macros. I suggest cherry-picking this to other branches whenever possible.", "comments": ["@sanjoy FYI"]}, {"number": 35684, "title": "Making a copy of the the input in from_config", "body": "Proposed fix for #35683", "comments": ["please make changes against master so that it can be cherry picked for next release.We don't accept PRs to release branch.\r\ncc @mihaimaruseac ", "@rthadur Understood, thank you. Please see #35753"]}, {"number": 35683, "title": "Wrapper.from_config mutates its input", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): platform-independent\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.keras.layers.Wrapper.from_config` modifies its `config` parameter, which can cause unexpected side effects in calling code.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/layers/wrappers.py#L83-L87\r\n\r\nSpecifically, `config.pop` in line 86 above mutates the `config` dict in a way that persists outside the `from_config` function call.\r\n\r\nElsewhere (e.g., in `tf.keras.layers.Bidirectional.from_config`) this is avoided by copying the `config` dict:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/layers/wrappers.py#L743-L745\r\n\r\n**Describe the expected behavior**\r\n\r\nBeing able to call `tf.keras.layers.Wrapper.from_config(config)` without `config` changing.\r\n\r\nI have a use case where I am subclassing the `Wrapper` class and relying on its `from_config` method. My workaround is to call `from_config(config.copy())`, but I don't think this should be required.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass MyWrapper(tf.keras.layers.Wrapper):\r\n    def call(self, inputs, *args, **kwargs):\r\n        return self.layer(inputs, *args, **kwargs)\r\n\r\n\r\nwrapper = MyWrapper(tf.keras.layers.Dense(1))\r\nconfig = wrapper.get_config()\r\nconfig_copy = config.copy()\r\nassert config == config_copy\r\n\r\nwrapper_from_config = MyWrapper.from_config(config)\r\nnew_config = wrapper.get_config()\r\nassert new_config == config_copy\r\nassert config == config_copy  # Fails! The 'layer' key has been popped from config\r\n```", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35683\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35683\">No</a>\n"]}, {"number": 35682, "title": "tf.data.Dataset.map repeats random numbers in each epoch  of a Keras training loop when a graph-level seed is set", "body": "Thanks for staying with me after this long title. There are many issues on non-determinism of `Dataset.map` - this one is contrary. With random data augmentation, I would expect `Dataset.map` to behave differently between epochs, but it does not if a graph-level seed is set.\r\n\r\n**System information**\r\n- Have I written custom code: yes, below.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 & Linux\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1.0rc2, also 2.0.0 and tf-nightly\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: 10.1/7.x\r\n- GPU model and memory: various\r\n\r\n**Describe the current behavior**\r\nThe code below outputs the same random numbers in each epoch:\r\n```\r\nTrain for 3 steps\r\nEpoch 1/3\r\nWARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.\r\n0.926393032\r\n0.0866344\r\n0.783794165\r\n3/3 [==============================] - 1s 186ms/step - loss: 0.0000e+00\r\nEpoch 2/3\r\n0.926393032\r\n0.0866344\r\n0.783794165\r\n3/3 [==============================] - 0s 7ms/step - loss: 0.0000e+00\r\nEpoch 3/3\r\n0.926393032\r\n0.0866344\r\n0.783794165\r\n3/3 [==============================] - 0s 6ms/step - loss: 0.0000e+00\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt should output different random numbers in each epoch.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.random.set_seed(0)\r\n\r\n\r\ndef do_something_random(*args):\r\n    \"\"\"It does the same things in each epoch. That's not random!\"\"\"\r\n    tf.print(tf.random.uniform((), 0, 1))\r\n    return args\r\n\r\n\r\ndata = [[1], [2], [3]]\r\ndata = tf.data.Dataset.from_tensor_slices((data, data)).batch(1)\r\ndata = data.map(do_something_random)\r\n\r\nlayer = tf.keras.layers.Input(shape=(1,))\r\nmodel = tf.keras.models.Model(inputs=layer, outputs=layer)\r\nmodel.compile(optimizer=\"adam\", loss=\"mse\")\r\nmodel.fit(x=data, epochs=3)\r\n```\r\n\r\n**Other info / logs**\r\nInitially, I thought, well, the `Dataset.map` command is maybe compiled once and run identically in each epoch. But\r\n- I am using `tf.random.uniform` explicitly,\r\n- the problem disappears when commenting out `tf.random.set_seed(0)`, implying that the `Dataset.map` command *is* able to yield different results in each epoch:\r\n```\r\nTrain for 3 steps\r\nEpoch 1/3\r\nWARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.\r\n0.632945657\r\n0.70087862\r\n0.662360072\r\n3/3 [==============================] - 1s 186ms/step - loss: 0.0000e+00\r\nEpoch 2/3\r\n0.536124825\r\n0.0930280685\r\n0.26403141\r\n3/3 [==============================] - 0s 7ms/step - loss: 0.0000e+00\r\nEpoch 3/3\r\n0.323968768\r\n0.376766324\r\n0.693181396\r\n3/3 [==============================] - 0s 6ms/step - loss: 0.0000e+00\r\n```\r\n\r\nThe issue is why the graph-level seed has an influence on this effect. Are the `Dataset.map` calls in each epoch maybe run in separate graphs, each of which receives the same graph-level seed?", "comments": ["Issue is replicating on colab with tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/f5f0f14eae16deceb24d70aa08a6e2ec/untitled339.ipynb?authuser=1).Thanks!", "Even worse: `tf.data.Dataset.shuffle` suffers from a similar problem with `disable_eager_execution()`: shuffles are identical from one epoch to the next.\r\n\r\n(This is based on my assumption that `Dataset.shuffle` should reshuffle between epochs. If that is *not* the case, then the issue is why it *does* reshuffle when eager execution is on.)\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\ntf.random.set_seed(0)\r\n\r\n\r\ndef show_args(*args):\r\n    \"\"\"It's the same order of samples in each epoch. That's not shuffling!\"\"\"\r\n    tf.print(args)\r\n    return args\r\n\r\n\r\ndata = [[1], [2], [3], [4], [5]]\r\ndata = tf.data.Dataset.from_tensor_slices((data, data)).batch(1)\r\ndata = data.shuffle(buffer_size=1024).map(show_args)\r\n\r\nlayer = tf.keras.layers.Input(shape=(1,))\r\nmodel = tf.keras.models.Model(inputs=layer, outputs=layer)\r\nmodel.compile(optimizer=\"adam\", loss=\"mse\")\r\nmodel.fit(x=data, epochs=3, verbose=0)\r\n```\r\noutputs\r\n```\r\n[[5]] [[5]]\r\n[[1]] [[1]]\r\n[[2]] [[2]]\r\n[[4]] [[4]]\r\n[[3]] [[3]]\r\n[[5]] [[5]]\r\n[[1]] [[1]]\r\n[[2]] [[2]]\r\n[[4]] [[4]]\r\n[[3]] [[3]]\r\n[[5]] [[5]]\r\n[[1]] [[1]]\r\n[[2]] [[2]]\r\n[[4]] [[4]]\r\n[[3]] [[3]]\r\n```\r\n(note 5-1-2-4-3 in each epoch).", "The `dataset.map()` will execute one function on every element of the `Dataset` separately and returns one transformed element and also as you set the random seed, this is the reason why you see same random numbers in each epoch.", "@gowthamkpr assuming that your explanation is correct, why do I see *different* random numbers in `map()` in each epoch without a graph-level seed? And why are `shuffle()`s *different* across epochs, even with a graph-level seed set?\r\n\r\nWhatever the expected output is, it is inconsistent that the behavior of\r\n- `dataset.map()` is the same across epochs with a seed, but different without;\r\n- `dataset.shuffle()` is the same across epochs with `disable_eager_execution`, but different without.", "Also, @jsimsa explains `tf.data` quite differently in https://github.com/tensorflow/tensorflow/issues/22000#issuecomment-444679792:\r\n```\r\ndataset = tf.data.Dataset.range(100)\r\ndataset = dataset.shuffle(buffer_size=100)\r\nfeature = dataset.map(lambda x: x)\r\nlabel = dataset.map(lambda x: x)\r\n```\r\n\r\n> that is not how tf.data works. The definition in your example will in fact create *two independent instances of the range and shuffle datasets*. Informally, you can think of the dataset construction as having \"by value\" semantics.\r\n\r\nThis makes the `feature` and `label` datasets `shuffle()` differently in the above example. I would expect this principle to hold for `map()` as well, and also for re-use of `shuffle()`d datasets in different epochs of a Keras training loop.", "@bersbersbers the TLDR is that legacy graph-mode (i.e. `tf.compat.v1.disable_eager_execution()`) does not support controlling shuffling behavior across different iterations of the same dataset.\r\n\r\nIf you would like to control the shuffle order and run in graph-mode with TF2, you should rely on `tf.function` (as oppposed to the legacy graph-mode). The following snippet illustrates idiomatic mechanism for controlling shuffling behavior in TF 2:\r\n\r\n```\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow.compat.v2 as tf\r\n\r\ntf.enable_v2_behavior()\r\n\r\ndef eager_execution(reshuffle_each_iteration):\r\n  ds = tf.data.Dataset.range(5).shuffle(5, reshuffle_each_iteration=reshuffle_each_iteration).batch(5)\r\n\r\n  def print_dataset(dataset):\r\n    for elem in dataset:\r\n      tf.print(elem)\r\n\r\n  print_dataset(ds)\r\n  print_dataset(ds)\r\n\r\nprint(\"Eager mode with reshuffling:\")\r\neager_execution(True)\r\nprint(\"Eager mode without reshuffling:\")\r\neager_execution(False)\r\n\r\n@tf.function\r\ndef graph_execution(reshuffle_each_iteration):\r\n  return eager_execution(reshuffle_each_iteration)\r\n\r\nprint(\"Graph mode with reshuffling:\")\r\ngraph_execution(True)\r\n\r\nprint(\"Graph mode without reshuffling:\")\r\ngraph_execution(False)\r\n```\r\n\r\nproduces:\r\n\r\n```\r\nEager mode with reshuffling:\r\n[4 1 0 2 3]\r\n[2 1 3 4 0]\r\nEager mode without reshuffling:\r\n[2 0 3 1 4]\r\n[2 0 3 1 4]\r\nGraph mode with reshuffling:\r\n[2 3 4 0 1]\r\n[2 4 1 3 0]\r\nGraph mode without reshuffling:\r\n[4 2 3 0 1]\r\n[4 2 3 0 1]\r\n```", "Thanks for the explanation, @jsimsa. I must admit that I was not aware that `reshuffle_each_iteration` existed, but seeing that it defaults to `True` means that at least my intuition was correct. It is unfortunate that `disable_eager_execution()` breaks this control, but your confirmation is valuable and I'll try and see if I can make the workaround work with my example of the Keras training loop (which needs to run non-eagerly since I rely on the `fetches` session kwarg.) I'll open a separate issue about this if I encounter other issues [I finished this, confirming indeed `shuffle()` indeed works in my application in eager mode], so let's ignore `shuffle()` in this issue for now and re-focus on the behavior of `map()` again.\r\n\r\nSo back the the main issue: Should I expect `dataset.map()` to run once in each Keras epoch (different random numbers), or only once (identical random numbers)?\r\n- If so, why are random numbers different across epochs without a seed?\r\n- If not, why are random numbers identical across epochs with a seed?\r\n\r\nI must admit due to the order of `shuffle().map()`, since shuffling works, in each epoch, the same operations in `map()` are applied to different data, so in terms of maximum data augmentation it's not a disaster. Still, I am worried that at least some non-randomness each epoch may impact learning.", "> The `dataset.map()` will execute one function on every element of the `Dataset` separately and returns one transformed element and also as you set the random seed, this is the reason why you see same random numbers in each epoch.\r\n\r\nHaving thought about this for a while, I don't think this is a valid explanation. The fact that the `map_fun` is called separately for each element together with the random see would explain, if anything, why the random operation returned the same random numbers for each *sample*, not only for each *epoch of samples*. But this is not the case. It does not explain why some samples receive different random numbers until a new epoch starts.\r\n\r\nAlso, if caching would be involved, this would raise the question what the purpose of the `Dataset.cache()` function should be.", "I am not sure I understand your question regarding `map` because `map` does not reorder elements. It simply applies the user-defined transformation to each element. Are you talking about the situation when there is a randomized op inside of the user-defined function? The answer to how randomness behaves in that case depends on the implementation of the op and is orthogonal to `map`.\r\n\r\nIn other words, if your input pipeline does not contain `shuffle` and your `map`'s user-defined function does not contain random ops, then your input pipeline will produce the same sequence every time it is executed.", "> I am not sure I understand your question regarding `map` because `map` does not reorder elements. [...] Are you talking about the situation when there is a randomized op inside of the user-defined function? \r\n\r\nYes, exactly! \r\n\r\n> The answer to how randomness behaves in that case depends on the implementation of the op and is orthogonal to map.\r\n\r\nThis is what I am interested in: my examples in the very first post DO contain calls to `tf.random.uniform`, which I would have expected will be re-applied with new random numbers every time I call the sequence. And this *is* the case in my example when disabling the graph-level seed; it is *not* the case when a graph-level seed is applied. So how can I make this \"depend on the implementation\" - what should I do differently if I want TF to behave deterministically (I need to set a graph-level seed) but want different epochs to behave differently?", "My recommendation would be to avoid reliance on graph-level / default seeds and switch to using `tf.random.stateless_uniform` which takes the `seed` argument so that you can control the sequence of seeds.\r\n\r\nHere is a fully reproducible example that deterministically reshuffles between epochs:\r\n\r\n```\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow.compat.v2 as tf\r\n\r\ntf.enable_v2_behavior()\r\n\r\ndef map_fn(seed):\r\n  return tf.random.stateless_uniform([], [seed, seed])\r\n\r\nds = tf.data.Dataset.range(5)\r\nds = ds.map(map_fn).shuffle(5, seed=42, reshuffle_each_iteration=True).batch(5)\r\n\r\nfor elem in ds:\r\n  print(elem.numpy())\r\nfor elem in ds:\r\n  print(elem.numpy())\r\n```\r\n\r\nThe above (consistently) produces the following output:\r\n```\r\n[0.21101546 0.61040807 0.09827709 0.7736759  0.9589814 ]\r\n[0.7736759  0.61040807 0.09827709 0.21101546 0.9589814 ]\r\n```", "@jsimsa sorry for introducing the side-issue of shuffling, consider this solved. Regarding mapping, I fear we're still talking about different aims.\r\n\r\nSo regarding mapping using a random function, you are proposing to skip the graph-level seed and use the element itself for seeding a stateless RNG, did I get this correct? That is, assuming a deterministic order of fixed input elements, this will yield a deterministic sequence of random numbers for each epoch - I get as much. But this is not what I need (and frankly, not what I expect TF to produce).\r\n\r\nI'm rather looking for *different* random numbers across epochs, and I want these to be *identical* across runs. And my finding a solution to this issue is hampered by the fact that I don't get none of the the two solutions below provides that. I'll try to summarize my lack of understanding in one piece of code:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef testing():\r\n    rand = lambda elem: tf.random.uniform([])\r\n    data = tf.data.Dataset.range(5).map(rand).batch(5)\r\n    print(\"epoch 1: \", end=\"\")\r\n    [print(elem.numpy()) for elem in data]\r\n    print(\"epoch 2: \", end=\"\")\r\n    [print(elem.numpy()) for elem in data]\r\n\r\nprint(\"w/o graph-level seed: different random numbers across epochs and runs\")\r\ntesting()\r\nprint(\"w/ graph-level seed: same random numbers across epochs and runs\")\r\ntf.random.set_seed(0)\r\ntesting()\r\n```\r\n\r\nrun 1:\r\n```\r\nw/o graph-level seed: different random numbers across epochs and runs\r\nepoch 1: [0.52723086 0.07255614 0.9757483  0.8239068  0.90070856]\r\nepoch 2: [0.6677898  0.4100479  0.21574163 0.144176   0.04567683]\r\nw/ graph-level seed: same random numbers across epochs and runs\r\nepoch 1: [0.2773143  0.92967796 0.11471713 0.51984704 0.6447146 ]\r\nepoch 2: [0.2773143  0.92967796 0.11471713 0.51984704 0.6447146 ]\r\n```\r\n\r\nrun 2 (with annotations):\r\n```\r\nw/o graph-level seed: different random numbers across epochs and runs\r\nepoch 1: [0.5421027 0.3198986 0.8723525 0.5778849 0.7087102] <-- diff. from run 1. expected!\r\nepoch 2: [0.8138256  0.2707013  0.32449722 0.34031594 0.30259633] <-- diff. from epoch 0 (?)\r\nw/ graph-level seed: same random numbers across epochs and runs\r\nepoch 1: [0.2773143  0.92967796 0.11471713 0.51984704 0.6447146 ] <-- same as run 1. expected!\r\nepoch 2: [0.2773143  0.92967796 0.11471713 0.51984704 0.6447146 ] <-- same as epoch 0 (?)\r\n```\r\n\r\nI focus on the two lines with `(?)`. Is this (both!) intended behavior? If so, *why* is the first number of epoch 2 the same as in epoch 1 (`0.2773143`) when a graph-level seed is set, but not without the graph-level seed (`0.8138256 != 0.5421027` in run 2 and `0.6677898 != 0.5421027` in run 1), if the whole `rand` mapping function is only called once? This simply does not make sense to me.", "tf.data does not maintain any state across iterations (with the exception of `shuffle` and `cache`). In other words, if your input pipeline is deterministic, does not contain `shuffle` and unseeded random operations, and does not depend on any external state, it will always produce the same sequence of numbers.\r\n\r\nIf you would like your input pipeline to produce deterministic behavior, that at the same time is different between different epochs, then you either need to introduce external state (e.g. an epoch counter that is used to seed the randomness) or piggyback on `shuffle`.\r\n\r\nHere is an example of how you can create an input pipeline for which different iterations produce different but deterministic sequence of numbers:\r\n\r\n```\r\nseed = tf.Variable(0, dtype=tf.int64)\r\n\r\ndef get_seed(_):\r\n  seed.assign_add(1)\r\n  return seed\r\n\r\nseeds = tf.data.Dataset.range(1).map(get_seed)\r\nseeds = seeds.flat_map(lambda seed: tf.data.experimental.RandomDataset(seed=seed))\r\n\r\nds = tf.data.Dataset.zip((seeds.batch(2), tf.data.Dataset.range(5)))\r\nds = ds.map(lambda seed, _: tf.random.stateless_uniform([], seed=seed)).batch(5)\r\n\r\nprint(\"epoch 1:\")\r\nfor elem in ds:\r\n  print(elem.numpy())\r\nprint(\"epoch 2:\")\r\nfor elem in ds:\r\n  print(elem.numpy())  \r\n```\r\n\r\nIn my colab, the above determinstically produces:\r\n```\r\nepoch 1:\r\n[0.17277443 0.01138496 0.5387242  0.14688337 0.98976684]\r\nepoch 2:\r\n[0.12150574 0.7640343  0.28948808 0.09558952 0.08135116]\r\n```\r\n\r\nLast but not least, to answer your question about `(?)`: both is intended behavior. For the case when the graph-level seed is not specified, the random ops in the input pipeline graph will be unseeded, which means non-deterministic behavior. For the case when the graph-level seed is specified, then the input pipeline will be deterministic and since it does not depend on any state outside of the input pipeline graph, each execution of the input pipeline will be identical.", "Thanks for the explanations!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35682\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35682\">No</a>\n"]}, {"number": 35681, "title": "OOM error when running ops on large tensors in TF2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0.130 / 7.6.0\r\n- GPU model and memory: Nvidia GeForce GTX 1060\r\n\r\n**Describe the current behavior**\r\n\r\nI want to allocate a large tensor (in the 100Ms of elements) and perform an operation on it such as `tf.exp`. However, when I apply an operation to my tensor it appears that TF is reallocating the entire thing, causing an OOM. This behavior is not limited to `tf.exp`, I believe it is common across all ops. Is there any way to force TF not to do such a massive reallocation?\r\n\r\n**Describe the expected behavior**\r\n\r\nTF should return the result of `tf.exp` without error.\r\n\r\n**Code to reproduce the issue**\r\n\r\nRunning:\r\n```python\r\nx = tf.ones([1, 25088, 25088])\r\ntf.exp(x)\r\n```\r\nGives me the following error:\r\n```\r\n2020-01-08 15:06:32.805387: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cwise_ops_common.h:259 : Resource exhausted: OOM when allocating tensor with shape[1,25088,25088] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/evan/opt/miniconda3/envs/tf-gpu-2.0.0/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 3970, in exp\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,25088,25088] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Exp]\r\n```\r\n\r\nIn PyTorch, I could run the following identical code without error:\r\n```python\r\nx = torch.ones((1,25088,25088))\r\ntorch.exp(x)\r\n```\r\n", "comments": ["Hi,\r\n\r\nYou are computing the exponential (or any value-wise transformation) of N values, without re-assigning the results to their initial container, so obviously N new values have to be created and allocated space for. In your case, N is very large; the created array should take around 2.5 GB of memory space (with default float32 dtype). Note that if you reassign to the inital tensor (`x = tf.exp(x)`), values will be overwritten, but there can still be some extra memory used during the operation (as transformed values have to be computed somewhere before being moved back to their initial location), but it will be minor as compared to assigning results to a distinct tensor.\r\n\r\nNow, the fact that the OOM occurs in TensorFlow and not in PyTorch could be due to the fact that in TF the computation occurs on the GPU, which in most cases has a more limited dedicated memory than when using system RAM for CPU computations. My knowledge of PyTorch is limited, but my guess is that the computation is happening on CPU there, hence the fact that it fits. Note that depending on the material you are using, actual memory limits will vary.\r\n\r\nTo force a computation to occur on CPU in TF, you can use a `tf.device` context manager:\r\n```python\r\nwith tf.device('cpu:0'):\r\n    x = tf.ones([1, 25088, 25088])\r\n    tf.exp(x)\r\n```\r\n\r\nI hope this helps.", "@evancasey ,\r\nWhen tried executing the code for TF-2.0, worked fine. Also the solution provided by @pandrey-fr  works well too. Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/f47c3278a1b478b3f5e78f0ba278ea64/35681.ipynb) of colab for your reference. Kindly close the issue if its resolved.Thanks!", "@evancasey ,\r\nAny update on the issue?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35680, "title": "Redundant functions between `Tensorflow Features Columns` API and `TF Keras Preprocessing Layers`", "body": "In working with TF 2.0, I noticed that the TF Feature Columns API seems to overlap with the Keras Preprocessing layers (or Keras Utils). For example, you can create an `tf.feature_column.indicator_column()` which creates a bunch of dummy variables or a one-hot encoded matrix based upon a categorical variable. With Keras preprocessing or Utils, you can use the `tf.keras.backend.one_hot()` function to perform the same operation. I think there are similar overlaps between TF Feature Columns like the embedding columns and similar Keras functions for embedding columns.\r\n\r\nI was just wondering if the steering committees for Tensorflow have any direction on whether they plan to promote one set of functions versus the other? Are there any plans to deprecate one set versus the other. For me, it is just a question of where to invest time and planning for code that might have to change in the future. Seems like maintaining the redundancy in the package will potentially lead to performance differences between similar functions, or confusion in setting up the code, etc. \r\n\r\n**NB** \r\nOh yes, I actually asked this question in the Tensorflow Discussion forum, but no one answered it. @dynamicwebpaige even forwarded the message to @karmel and Mark Omernick, but no one responded. Hence, I posted here. \r\n", "comments": ["Hey Folks, any additional thoughts on this questions?", "@fchollet I was wondering if you had any additional thoughts on this question? Seems like there should be a direction on what TF will go in terms of Features API versus the Keras preprocessing functions. Or does Google and the Community want to keep both sets of functions, even though they overlap. ", "@fchollet I thought I would just follow up on this question. Is there any direction on whether users should use the Feature Columns API, or the TF Keras Preprocessing features? Both seem to do very similar things, so I image that Google would not want to maintain redundant code. Or is there a direction to use something like Tensorflow Transform for all of these preprocessing operations--though I believe TFT is meant for much bigger datasets. \r\nThanks. ", "According to this [Accepted RFC](https://github.com/tensorflow/community/blob/0650b34a755e51050ae4f0e91a4e773b2515b46f/rfcs/20191212-keras-categorical-inputs.md), the Keras Preprocessing Layers are introduced to replace feature columns and DenseFeatures. So, I guess once those layers are ready and exported, the plan is to deprecate feature columns. As mentioned in RFC, it is difficult to use feature columns in Keras functional API.\r\n\r\nPreprocessing layers are better than feature columns in lot ways. Using layers like Normalization, TextVectorization avoids building preprocessing pipeline during prediction time, since they are built right into the model.\r\n\r\nBut I'm concerned that preprocessing layers like [RandomCrop](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomCrop), [RandomRotation](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomRotation), which are required only at training time are redundant. Since they are required only at training time and not at prediction time, it makes more sense to apply those preprocessing steps in tf.data.Dataset instead of in the model.\r\n\r\n```\r\ndataset = ... # Load images\r\ndataset.map(lambda x: tf.image.random_crop(x, (size))\r\ndataset.map(tf.image.random_flip_up_down)\r\n\r\nmodel = ... #Build model\r\n\r\nmodel.fit(dataset)\r\n\r\n# Prediction\r\nmodel.prediction(..)\r\n```\r\nSo in my view, the criteria to define Preprocessing layers vs Dataset operations can be:\r\n- If the step is required at both training and prediction time -> Preprocessing layer\r\n- If the step is required only during training -> No Preprocessing layer, use Dataset operation\r\n\r\nThis can speed up the inference by eliminating unnecessary layer.", "@ilango100 Thanks for the info. Yeah, I saw some mentions of the Keras Preprocessing layers but I think only a few of them are available under experimental right now. I agree that the feature layers are a bit confusing, but I have finally become used to them. That is good though, now I know what to keep my eye on for the future. ", "@00krishna  Keras Preprocesing layers are released in TF 2.3.0 release. Check the [release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.3.0) for details. \r\nIn addition you can also checkout the FC to Preprocessing Layers [migration guide](https://github.com/tanzhenyu/community/blob/5507ba06c8f43830f4a77ea85bc8d345f5b60765/rfcs/20191212-keras-categorical-inputs.md)\r\n\r\n\"Feature columns will continue to work for canned Estimators, but we believe Keras Preprocessing Layers better reflect the TensorFlow v2 programming models, and they integrate better with Keras, which is the preferred high level API in TF2. Thus, we encourage you to use Keras Preprocessing Layers for your preprocessing needs, and feature columns will be deprecated at some point in the future when the use cases can be fully replaced by Preprocessing Layers + Keras models.\"", "@00krishna am closing this based on my previous comment, if you have any usecase that is not satisfied with the Keras Preprocessing Layers, please feel free to open a new issue. "]}, {"number": 35679, "title": "Develop upstream sync 200108", "body": "", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35679) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 35678, "title": "TensorFlowLite_LSTM_Keras_Tutorial.ipynb update ", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/TensorFlowLite_LSTM_Keras_Tutorial.ipynb\r\n\r\n## Description of issue (what needs changing):\r\nThe example script on how to make lstm layers ready for tf lite is outdated and not working anymore, because the requested tf-nightly package causes issues. \r\n\r\n\r\nI would like to get an updated tutorial or a better alternative. To use the TFLite converter with the experimental_flag set to True works with lstm layers, but does not allow post training quantization. As a general question I would like to know, if this would be possible with the the model that is build in the example script?\r\n\r\n", "comments": ["@AlexVaith Are you looking for the tutorial with the new converter? [Here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb) is the tutorial with the new converter. Couple of days back there was an update, so if you plan to use `tf-nightly`, then you don't need to specify `converter.experimental_new_converter = True` which is default in the latest `tf-nightly`. Thanks!", "> @AlexVaith Are you looking for the tutorial with the new converter? [Here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb) is the tutorial with the new converter. Couple of days back there was an update, so if you plan to use `tf-nightly`, then you don't need to specify `converter.experimental_new_converter = True` which is default in the latest `tf-nightly`. Thanks!\r\n\r\nThanks for the quick reply  @jvishnuvardhan . I did not know that this tutorial existed, but I found the same solution on stackoverflow. I just thpught that the official documentation should be up to date, which it is apparently. \r\nHowever, I thought that with the old implementation via the TFLiteLSTMCell and tf.lite.experimental.nn.dynamic_rnn, post training quantization could be possible, which is not with the converter.experimental_new_converter = True. Is this something that is added to tf lite in the future or are lstm or rnn structures in general not compatible with the requirements needed for post training quantization?", "@AlexVaith Can you please check this page on [Post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) and also use the experimental flag `converter.experimental_new_quantizer = True` and let us know what you think? Thanks!", "@jvishnuvardhan I tried this link with an lstm model looking like the following:\r\n\r\n```\r\ndef build_model_tf(lr, window_size, features, classes):\r\n    # input layer\r\n    inp = tf.keras.layers.Input(shape=[window_size, features], batch_size=42)\r\n    # batch norm layer to normalize data\r\n    x = tf.keras.layers.BatchNormalization()(inp)\r\n\r\n    # two convolutional layers\r\n    x = tf.keras.layers.Conv1D(filters=16, kernel_size=3)(x)\r\n    x = tf.keras.layers.Activation('relu')(x)\r\n    x = tf.keras.layers.BatchNormalization()(x)\r\n\r\n    x = tf.keras.layers.Conv1D(filters=32, kernel_size=6)(x)\r\n    x = tf.keras.layers.Activation('relu')(x)\r\n    x = tf.keras.layers.BatchNormalization()(x)\r\n\r\n    # 3 lstm layers\r\n    x = tf.keras.layers.LSTM(32, return_sequences=True, recurrent_regularizer=tf.keras.regularizers.L1L2(0.5, 0.1))(x)\r\n    x = tf.keras.layers.LSTM(24, return_sequences=True, recurrent_regularizer=tf.keras.regularizers.L1L2(0.5, 0.1))(x)\r\n    x = tf.keras.layers.LSTM(16, return_sequences=False, recurrent_regularizer=tf.keras.regularizers.L1L2(0.5, 0.1))(x)\r\n\r\n    # dense layer\r\n    x = tf.keras.layers.Dense(8)(x)\r\n    x = tf.keras.layers.Activation('relu')(x)\r\n    x = tf.keras.layers.Dropout(0.35)(x)\r\n    #\r\n    # output layer\r\n    out = tf.keras.layers.Dense(classes, activation='softmax')(x)\r\n\r\n    # create functional model object\r\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\r\n\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr, decay=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\n    return model\r\n```\r\n\r\nit was than converted like this:\r\n```\r\n# convert keras model to concrete functions\r\ntf.saved_model.save(tf_keras_model, join(ROOT, 'trained_models', model_name + '_conv'))\r\nmodel = tf.saved_model.load(join(ROOT, 'trained_models', model_name + '_conv'))\r\nconcrete_func = model.signatures[\r\n  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n# initialize the tf_light converter\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n# add experimental flag, because rnn structures are not covered by the default converter\r\nconverter.experimental_new_converter = True\r\n# convert model to tf_light\r\ntf_lite_model = converter.convert()\r\n```\r\nWithout the following line it works:\r\n```\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n```\r\nBut with this line the same issue like in the following post occurs:\r\nhttps://github.com/tensorflow/tensorflow/issues/35194\r\n\r\nI am using tensorflow 2.0.0 and tf nightly.", "@AlexVaith Can you please create a standalone code (a colab or jupyter notebook) and share it. That will be fastest way to resolve issues. Thanks!", "@AlexVaith The line in your code `converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]`, will only trigger weight-only (hybrid) quantization (hybrid quantization). TFLite doesn't have hybrid LSTM kernels. Please switch to `tf.lite.Optimize.DEFAULT` and specify the sample data for a fully quantization. Thanks!", "I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!"]}, {"number": 35676, "title": "Micro: Fix compile error for Arm Mbed OS.", "body": "This fixes a compile error when building for Arm Mbed OS related to the cmsis-nn pooling implementation.", "comments": ["The \"MacOS CPU Python3\" and \"MacOS Python2 and CC\" show \"Page not found\" when attempting to view them.\r\n\r\nThe \"Windows Bazel\" and \"Windows Bazel GPU\" tests are failing due to a 404 error when trying to download something llvm-related, which is unrelated to this patch.", "@lintian06 gentle ping for review.", "cc @petewarden "]}, {"number": 35675, "title": "tflite crash with segmentfault when I use set_tensor to set input tensor.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu16.04 windows and Raspbian 10\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\nI test it both on x64 and armv7l (Raspberry Pi4 which is running 32bit OS)\r\n\r\n- TensorFlow installed from (source or binary):\r\nfrom binary\r\n\r\n- TensorFlow version (use command below):\r\nI test with 2.0 1.12\r\n\r\n- Python version:\r\n3.7\r\n\r\n**Describe the current behavior**\r\nAs I use tf1.12 produce a saved_model.pb, I use tf2.0 to convert it to tflite file. But when I try to inference with tflite. it alway crash with segment fault when I try to set_tensor. I tried it on windows and linux and raspberry pi, all doesn't work.\r\n\r\nI have test the saved_model.pb as it work well, so I think the it's not the model's problem.\r\n\r\n**Code to reproduce the issue**\r\nI use the following code to convert the model to tflite:\r\n`import tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('./')\r\n\r\ntflite_model = converter.convert() \r\n\r\nopen('model_network.tflite', 'wb').write(tflite_model)`\r\n\r\nand following code to inference:\r\n`\r\ninterpreter = tf.lite.Interpreter(model_path=model_path)\r\n\r\ninput_details = interpreter.get_input_details()\r\n\r\noutput_details = interpreter.get_output_details()\r\n\r\ninput_shape = input_details[0]['shape']\r\n\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\nresult = interpreter.get_tensor(output_details[0]['index'])\r\n`\r\nit crash when interpreter.set_tensor\r\nI attach my model files here.\r\nhttps://drive.google.com/file/d/1NflTBZ2iB4hptDozODdaSke4o7pzPBNW/view?usp=sharing\r\nThe original code is to long, if needed I can upload it later.", "comments": ["Call `interpreter.allocate_tensors()` before running set_tensor()", "I also submitted a fix (above) 5521416 so  this warns you that allocate needs to be called first.", "Thanks, it worked out."]}, {"number": 35674, "title": "SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'input_1:0' shape=(None, 30, 78) dtype=float32>]", "body": "------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Version 1909\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow version (use command below)**: 2.0.0\r\n- **Python version**: 3.6.9\r\n- **CUDA/cuDNN version**:10.0.130\r\n- **GPU model and memory**:GTX 1660 Ti 6GB\r\n\r\n\r\n[Jazz improvisation with LSTM.zip](https://github.com/tensorflow/tensorflow/files/4035452/Jazz.improvisation.with.LSTM.zip)\r\n\r\nI uploaded a zip file in which the .ipynb file when runs gets that error. Please you can run and check what's the problem. How to fix this issuse?", "comments": ["@Krishnarohith10 ,\r\nOne workaround for the issue can be trying to disable eager execution and run the .ipynb file\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n```\r\nYou can also refer [solution ](https://stackoverflow.com/questions/57704771/inputs-to-eager-execution-function-cannot-be-keras-symbolic-tensors)of this similar issue.\r\nKindly post the question in Stack-overflow if above solution is not helpful.Thanks!\r\n", "Thank you for your Solution. Sadly it gone worse. I don't know what is it even. Here have a look:\r\n![1](https://user-images.githubusercontent.com/44919399/72087388-61c47a80-332e-11ea-8a5e-41920f2dc5f9.jpg)\r\n", "This is error message I get:\r\n![1](https://user-images.githubusercontent.com/44919399/72214532-137bcb00-352a-11ea-9fd1-d36d93260676.jpg)\r\n", "@Krishnarohith10 Sorry for the late response. Is this still an issue for you? Thanks!", "Not anymore. So I'm closing this issue. Sorry that I didn't notice this issue is still open. Thank You"]}, {"number": 35673, "title": "ValueError: too many values to unpack (expected 2)", "body": "while performing language translation it is rising an issue value error.\r\ni followed the code from the tensorflow guide\r\n\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\n\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.ticker as ticker\r\n#from sklearn.model_selection import train_test_split\r\n\r\nimport unicodedata\r\nimport re\r\nimport numpy as np\r\nimport os\r\nimport io\r\nimport time\r\n# Download the file\r\n\r\npath_to_file = \"spanish.txt\"\r\n# Converts the unicode file to ascii\r\ndef unicode_to_ascii(s):\r\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\r\n                   if unicodedata.category(c) != 'Mn')\r\n\r\n\r\ndef preprocess_sentence(w):\r\n    w = unicode_to_ascii(w.lower().strip())\r\n    # creating a space between a word and the punctuation following it\r\n    # eg: \"he is a boy.\" => \"he is a boy .\"\r\n    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\r\n    w = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", w)\r\n    w = re.sub(r'[\" \"]+', \" \", w)\r\n\r\n    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\r\n    w = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", w)\r\n    w = w.rstrip().strip()\r\n    # adding a start and an end token to the sentence\r\n    # so that the model know when to start and stop predicting.\r\n    w = '<start> ' + w + ' <end>'\r\n    return w\r\nen_sentence = u\"Wow!\"\r\nsp_sentence = u\"\u00a1\u00d3rale!\"\r\nprint(preprocess_sentence(en_sentence))\r\nprint(preprocess_sentence(sp_sentence).encode('utf-8'))\r\n\r\n# 1. Remove the accents\r\n# 2. Clean the sentences\r\n# 3. Return word pairs in the format: [ENGLISH, SPANISH]\r\ndef create_dataset(path, num_examples):\r\n    lines = io.open(path, encoding='UTF-8').read().strip().split(\"\\t\")\r\n    word_pairs = [[preprocess_sentence(w) for w in l.split(\"\\t\")] for l in lines[:num_examples]]\r\n    return word_pairs\r\nen, sp = create_dataset(path_to_file, None)\r\nprint(en[-1])\r\nprint(sp[-1])\r\n\r\n\r\nRaising an error in line : en, sp = create_dataset(path_to_file, None)", "comments": ["Hi, try printing out word pairs before it is returned and see the size of the array", "> Hi, try printing out word pairs before it is returned and see the size of the array\r\n\r\nIt is printing the output with start and end tags separated by ','\r\n\r\nbut it is again raising the same error\r\n", "What is the size of the array?\r\n", "The word_pairs is a single array, and cannot be split up into 2 values, try zipping the array by\r\nzip(*word_pairs) as mentioned in the documentation.\r\n[https://www.tensorflow.org/tutorials/text/nmt_with_attention](url)", "> The word_pairs is a single array, and cannot be split up into 2 values, try zipping the array by\r\n> zip(*word_pairs) as mentioned in the documentation.\r\n> [https://www.tensorflow.org/tutorials/text/nmt_with_attention](url)\r\n\r\nThq it works for me", "Hi, \r\nInstead of translating a single sentence always how can we take inputs from the user and translate it.\r\ncan we integrate this translation model in android.please help me to do that.", "@nagapoornima22, Take input from user using \r\n`input_sentence = raw_input(\"write a sentence\") `or \r\n`input_sentence = input(\"Write a sentence\")`\r\nand please find the [guide](https://www.tensorflow.org/lite/guide/android) for deploying the translation model in android using Tensorflow. Thanks", "> @nagapoornima22, Take input from user using\r\n> `input_sentence = raw_input(\"write a sentence\") `or\r\n> `input_sentence = input(\"Write a sentence\")`\r\n> and please find the [guide](https://www.tensorflow.org/lite/guide/android) for deploying the translation model in android using Tensorflow. Thanks\r\n\r\nThankyou", "hi, i tried with the zip but the same error", "> hi, i tried with the zip but the same error\r\n\r\ndef create_dataset(path, num_examples):\r\n    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\r\n    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\r\n    return zip(*word_pairs)\r\n\r\n\r\nThis works!", "please i want to save my architecture and save the model to my google drive, i am using google colab ,to be able to design the application."]}, {"number": 35672, "title": "tf.keras.estimator.model_to_estimator InvalidArgumentError: while setting up XLA_GPU_JIT device number 2", "body": "I get an error similar to the issue described in https://github.com/tensorflow/tensorflow/issues/31451\r\nWhen using tf.keras.estimator.model_to_estimator in TF 1.14 and 1.15 I get the InvalidArgumentError described below. The problem doesn't occur in TF 1.13.2. \r\n\r\nI use Linux Mint with Python 3.6 with 2 GPUs (Nvidia GTX1080Ti). To be complete, there are 3 NVidia video cards in the machine (that seems relevant if I understand the error correctly).\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-6-3ce5a1d7f637> in <module>\r\n      2     keras_model=model_f,\r\n      3     custom_objects={'Merge': Merge},\r\n----> 4     model_dir='./data/estimator')\r\n\r\n~/.../python3.6/site-packages/tensorflow_core/python/keras/estimator/__init__.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config, checkpoint_format)\r\n    105       config=config,\r\n    106       checkpoint_format=checkpoint_format,\r\n--> 107       use_v2_estimator=False)\r\n    108 \r\n    109 \r\n\r\n~/.../python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config, checkpoint_format, use_v2_estimator)\r\n    574   if keras_model._is_graph_network:\r\n    575     warm_start_path = _save_first_checkpoint(keras_model, custom_objects,\r\n--> 576                                              config, save_object_ckpt)\r\n    577   elif keras_model.built:\r\n    578     logging.warning('You are creating an Estimator from a Keras model manually '\r\n\r\n~/.../python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in _save_first_checkpoint(keras_model, custom_objects, config, save_object_ckpt)\r\n    390 \r\n    391       # save to checkpoint\r\n--> 392       with session.Session(config=config.session_config) as sess:\r\n    393         if keras_weights:\r\n    394           model.set_weights(keras_weights)\r\n\r\n~/.../python3.6/site-packages/tensorflow_core/python/client/session.py in __init__(self, target, graph, config)\r\n   1583           protocol buffer with configuration options for the session.\r\n   1584     \"\"\"\r\n-> 1585     super(Session, self).__init__(target, graph, config=config)\r\n   1586     # NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\r\n   1587     self._default_graph_context_manager = None\r\n\r\n~/.../python3.6/site-packages/tensorflow_core/python/client/session.py in __init__(self, target, graph, config)\r\n    697     try:\r\n    698       # pylint: disable=protected-access\r\n--> 699       self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\r\n    700       # pylint: enable=protected-access\r\n    701     finally:\r\n\r\nInvalidArgumentError: Invalid device ordinal value (2). Valid range is [0, 1].\r\n\twhile setting up XLA_GPU_JIT device number 2\r\n```", "comments": ["@coert, Thanks for reporting the issue. Can you provide the reproducible code snippet to analyze the issue. Thanks!", "@gadagashwini sorry for the late reply!\r\n\r\nThis is the minimal code example:\r\n```\r\nimport os\r\nimport sys\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.models import load_model\r\n\r\nmodule_path = os.path.abspath(os.path.join('./src'))\r\nif module_path not in sys.path:\r\n    sys.path.append(module_path)\r\n\r\nfrom src.config import CACHE_PATH\r\n\r\nTRAIN_RESULTS_DIR = os.path.join(CACHE_PATH, 'models')\r\nmodel_file = os.path.join(TRAIN_RESULTS_DIR, 'model.h5')\r\nmodel = load_model(model_file)\r\n\r\nESTIMATOR_DIR = os.path.join(CACHE_PATH, 'estimator')\r\nestimator = tf.keras.estimator.model_to_estimator(\r\n    keras_model=model,\r\n    model_dir=ESTIMATOR_DIR)\r\n```\r\n\r\nTF log output:\r\n> [I 15:08:58.427 NotebookApp] Replaying 5 buffered messages\r\n> 2020-01-21 15:09:05.038867: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n> 2020-01-21 15:09:05.098955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\n> name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\n> pciBusID: 0000:17:00.0\r\n> 2020-01-21 15:09:05.099452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\n> name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\n> pciBusID: 0000:65:00.0\r\n> 2020-01-21 15:09:05.100057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: \r\n> name: GeForce GT 710 major: 3 minor: 5 memoryClockRate(GHz): 0.954\r\n> pciBusID: 0000:b3:00.0\r\n> 2020-01-21 15:09:05.100289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n> 2020-01-21 15:09:05.101249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n> 2020-01-21 15:09:05.102169: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n> 2020-01-21 15:09:05.102434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n> 2020-01-21 15:09:05.103553: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n> 2020-01-21 15:09:05.104468: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n> 2020-01-21 15:09:05.106899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-01-21 15:09:05.109872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1731] Ignoring visible gpu device (device: 2, name: GeForce GT 710, pci bus id: 0000:b3:00.0, compute capability: 3.5) with core count: 1. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\r\n> 2020-01-21 15:09:05.109883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n> 2020-01-21 15:09:05.110185: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n> 2020-01-21 15:09:05.135240: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz\r\n> 2020-01-21 15:09:05.136730: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x702be70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-01-21 15:09:05.136766: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n> 2020-01-21 15:09:05.335023: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x70ceaf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-01-21 15:09:05.335076: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n> 2020-01-21 15:09:05.335098: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n> 2020-01-21 15:09:05.339283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\n> name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\n> pciBusID: 0000:17:00.0\r\n> 2020-01-21 15:09:05.340643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\n> name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\n> pciBusID: 0000:65:00.0\r\n> 2020-01-21 15:09:05.340741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n> 2020-01-21 15:09:05.340784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n> 2020-01-21 15:09:05.340821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n> 2020-01-21 15:09:05.340860: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n> 2020-01-21 15:09:05.340904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n> 2020-01-21 15:09:05.340942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n> 2020-01-21 15:09:05.340982: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-01-21 15:09:05.344734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n> 2020-01-21 15:09:05.344786: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n> 2020-01-21 15:09:05.347038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-01-21 15:09:05.347056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 \r\n> 2020-01-21 15:09:05.347067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y \r\n> 2020-01-21 15:09:05.347075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N \r\n> 2020-01-21 15:09:05.351812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10479 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n> 2020-01-21 15:09:05.352967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10003 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n\r\nError message:\r\n```\r\n21-01-2020:15:16:28,886 INFO  [estimator.py:1800] Using default config.\r\n21-01-2020:15:16:28,888 INFO  [keras.py:536] Using the Keras model provided.\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-4e5a90fa8adc> in <module>\r\n      2 estimator = tf.keras.estimator.model_to_estimator(\r\n      3     keras_model=model,\r\n----> 4     model_dir=ESTIMATOR_DIR)\r\n\r\n~/.../lib/python3.7/site-packages/tensorflow_core/python/keras/estimator/__init__.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config, checkpoint_format)\r\n    105       config=config,\r\n    106       checkpoint_format=checkpoint_format,\r\n--> 107       use_v2_estimator=False)\r\n    108 \r\n    109 \r\n\r\n~/.../lib/python3.7/site-packages/tensorflow_estimator/python/estimator/keras.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config, checkpoint_format, use_v2_estimator)\r\n    574   if keras_model._is_graph_network:\r\n    575     warm_start_path = _save_first_checkpoint(keras_model, custom_objects,\r\n--> 576                                              config, save_object_ckpt)\r\n    577   elif keras_model.built:\r\n    578     logging.warning('You are creating an Estimator from a Keras model manually '\r\n\r\n~/.../lib/python3.7/site-packages/tensorflow_estimator/python/estimator/keras.py in _save_first_checkpoint(keras_model, custom_objects, config, save_object_ckpt)\r\n    390 \r\n    391       # save to checkpoint\r\n--> 392       with session.Session(config=config.session_config) as sess:\r\n    393         if keras_weights:\r\n    394           model.set_weights(keras_weights)\r\n\r\n~/.../lib/python3.7/site-packages/tensorflow_core/python/client/session.py in __init__(self, target, graph, config)\r\n   1583           protocol buffer with configuration options for the session.\r\n   1584     \"\"\"\r\n-> 1585     super(Session, self).__init__(target, graph, config=config)\r\n   1586     # NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\r\n   1587     self._default_graph_context_manager = None\r\n\r\n~/.../lib/python3.7/site-packages/tensorflow_core/python/client/session.py in __init__(self, target, graph, config)\r\n    697     try:\r\n    698       # pylint: disable=protected-access\r\n--> 699       self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\r\n    700       # pylint: enable=protected-access\r\n    701     finally:\r\n\r\nInvalidArgumentError: Invalid device ordinal value (2). Valid range is [0, 1].\r\n\twhile setting up XLA_GPU_JIT device number 2\r\n```", "@coert, Looks like code is incomplete to replicate the reported issue.\r\nPlease provide more information on `from src.config import CACHE_PATH`  to reproduce the issue. Thanks!", "I did some more digging. I believe the error only occurs when you use load_model in combination with model_to_estimator. So it seems something is not loaded properly. Here's the example:\r\n\r\n```\r\nimport os\r\nimport sys\r\nimport tensorflow as tf\r\nfrom shutil import rmtree\r\nfrom pathlib import Path\r\n\r\ntf.get_logger().setLevel(tf.logging.ERROR)\r\n\r\nfrom tensorflow.keras.models import load_model\r\nfrom tensorflow.keras import backend as K\r\n\r\nlayers = tf.keras.layers\r\ninputs = layers.Input(shape=(8,8,3), name=\"input\")\r\noutputs = tf.keras.layers.Conv2D(filters=1, kernel_size=(1,1),\r\n                                 name=\"output\")(inputs)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"model\")\r\nmodel.compile(optimizer=tf.keras.optimizers.Adagrad(lr=3e-4), \r\n              loss='binary_crossentropy', metrics=['accuracy'])\r\nmodel.summary()\r\n\r\n## Uncommenting the next 4 lines will cause model_to_estimator to fail! ##\r\n# model_filename = './model.h5'\r\n# model.save(model_filename)\r\n# K.clear_session()\r\n# model = load_model(model_filename)\r\n\r\n# careful! we're removing the whole estimator path\r\n# (if it exists, it will not be recreated)\r\nestimator_path = Path('./data/estimator')\r\nif estimator_path.exists():\r\n    rmtree(str(estimator_path))\r\n    \r\nestimator = tf.keras.estimator.model_to_estimator(\r\n    keras_model=model,\r\n    model_dir=str(estimator_path))\r\n```", "@coert, I tried reproducing it with Tf-gpu 1.15 but no error. Please take a look at [gist](https://colab.research.google.com/gist/gadagashwini/5eb5578f670744f3efb51bfef6290efd/untitled369.ipynb) and confirm the issue still exists or not. Thanks", "@gadagashwini how can I check that the gist is using multiple GPU devices? I think the problem is in the combination of reloading a model while have multiple usable GPUs in your system. From the original log above:\r\n```\r\n2020-01-21 15:09:05.347056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]    0 1 \r\n2020-01-21 15:09:05.347067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0: N Y \r\n2020-01-21 15:09:05.347075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1: Y N\r\n```\r\n\r\nLong story short, the gist works fine, but it isn't representative of the problem.", "Can you please check if this still happens on TF-nightly?  We have made some improvements in this area.", "Looks good on tf-nightly 2.2.0.dev20200403. Thanks!", "Unable to check tf-nightly 1.15.x, I think that's due to: \r\nhttps://github.com/tensorflow/tensorflow/issues/37317\r\nhttps://github.com/pypa/pypi-support/issues/224", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35672\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35672\">No</a>\n"]}, {"number": 35671, "title": "Where is self.activity_regularizer used in SimpleRNN, GRU and LSTM? Is it a dangling orphan attribute?", "body": "In the `__init__()` of keras' `SimpleRNN`, `GRU` and `LSTM` the input parameter `activity_regularizer` is stored\r\n\r\n```\r\n...\r\nself.activity_regularizer = regularizers.get(activity_regularizer)\r\n...\r\n```\r\n\r\nHowever, it seems that `self.activity_regularizer` never used anywhere in `SimpleRNN`, `GRU` and `LSTM` afterwards. ", "comments": ["It is used in the base layer. See https://github.com/tensorflow/tensorflow/blob/52a9ff02c5d630494b8d5ce5a42004b5a7247478/tensorflow/python/keras/engine/base_layer.py#L942, and search for all the usage of self._ activity_regularizer.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35671\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35671\">No</a>\n"]}, {"number": 35670, "title": "Add PrefetchedInputStream and BufferedRandomAccessFile.", "body": "Hi, guys!\r\n\r\nHere are the prefetch-enabled input stream and yet another random access file with buffer we talked about earlier in #33023. Here come's the code, benchmark numbers are on the way.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35670) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35670) for more info**.\n\n<!-- ok -->", "I'll review this over the weekend. Apologies for the delay", "Here comes the benchmark numbers.\r\n\r\nThe test is performed via downloading a large file (~450 MB) from alicloud OSS service.\r\nHere we have prefetching thread number zero as the baseline which fallback to the original buffered input stream, so the `buffer_per_thread` argument actually has no effect.\r\nA range of `prefetching_threads` and `buffer_per_thread` arguments are tested, see the following table and chart.\r\n\r\n\r\n| **prefetching_threads\\buffer_per_thread** | **1 MB** | **4 MB** | **8 MB** | **16 MB** |\r\n| ----------------------------- | -------- | -------- | -------- | --------- |\r\n| **0**                         | 7.85     | 7.85     | 7.85     | 7.85      |\r\n| **1**                         | 3.97     | 4.34     | 6.83     | 6.85      |\r\n| **2**                         | 4.61     | 6.44     | 12.87    | 13.45     |\r\n| **4**                         | 5.67     | 12.68    | 24.58    | 23.99     |\r\n| **8**                         | 6.17     | 24.32    | 46.09    | 45.10     |\r\n| **16**                        | 12.57    | 44.45    | 79.80    | 80.94     |\r\n| **32**                        | 22.03    | 63.49    | 98.40    | 111.68    |\r\n| **64**                        | 29.09    | 60.06    | 82.23    | 80.03     |\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/261796/72517673-c59af600-388e-11ea-825e-c7d73d0550f5.png)\r\n", "Also, can you please resolve conflict?", "Also needs API review", "There is a buffer_size argument with RandomAccessFile, and tf.data has parallel_interleave/prefetch to retrieve data from multiple files. What is the motivation re-implementing them in the I/O library? ", "Hi, @byronyi !\r\n\r\n> There is a buffer_size argument with RandomAccessFile ...\r\n\r\nDo you mean [RandomAccessFile class in file_system.h ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.h#L238)? I see no `buffer_size` on it. Maybe I miss your point.\r\n\r\n> What is the motivation re-implementing them in the I/O library.\r\n\r\nIn this PR, we provide parallel prefetching in file-level for a single file. When working in file-level, all happens outside a tf session, which's different from dataset. This could be useful for users to simply and quickly get a file to local file system. The other difference is we work on single file, users don't have to split a large file into multiple ones to get parallel-enabled.", "@qiuxiafei Sorry, I mean one could use `RandomAccessFile::Read` to directly read as much as one wants with buffer supplied. For example, there is a `buffer_size` argument with `TFRecordDataset` which directly calls `RandomAccessFile::Read` under the hood: https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/data/ops/readers.py#L205", "> In this PR, we provide parallel prefetching in file-level for a single file. When working in file-level, all happens outside a tf session, which's different from dataset. This could be useful for users to simply and quickly get a file to local file system. The other difference is we work on single file, users don't have to split a large file into multiple ones to get parallel-enabled.\r\n\r\nThere is no session in TF 2.x, and users could iterate a parallel_interleaved/prefetched dataset in a single for loop. But I agree it is not always the best choice for users to split their inputs into multiple files. However, I do strongly suggest you to consider about how we could use the APIs in this PR with tf.data, as tf.data is still the most common and recommeded API TF users use to load data from external storage.", "Hi, @byronyi , well, we found it quite common to do some buffering on RandomAccessFile when implementing several our internal file systems. To make the design orthogonal, we use this `BufferedRandomAccessFile` as a util class to decorator raw RandomAccessFile. This provides buffer utilities while keeping the original RandomAccessFile interface.\r\n\r\nIMO, `tf.io.gfile.GFile` and `tf.data.Dataset` work on different layers of TF API and users are always encouraged the use high-level dataset API. So there seems no need to use API in this PR with dataset. This is just an extension for `GFile`. For ones who are using `GFile`, it can be prefetched in parallel now while for ones who're using `Dataset` they still work with parallel_interleaved and prefetched. It just like we have file and dataset simultaneousely.\r\n\r\nBTW, file and dataset are quite different. Dataset is like a collection of structured and usually unordered records. But files are continuous bytes, we preserve the order while prefetching in parallel.\r\n\r\n\r\n", "Hey guys, any updates? Looking forward to your advice.", "@qiuxiafei Can you please resolve conflicts? Thanks!", "@gbaned updated.", "Hi, @mihaimaruseac , I made some change according to your comments.", "Thanks for your feedback, @jsimsa !\r\n\r\nThe motivation to implement these two utility IO class come from our practice bridging multiple internal storage system into TF. Buffering and I/O parallelization are quite common requirements when bridging these system, instead of implementing these functionalities time and time again, `PrefetchedInputStream` and `BufferedRandomAccessFile` are abstracted. They are actually low-level IO utilities which help bridging storage systems.\r\n\r\nAs for \"number of threads\" and \"buffer sizes\" in `GFile`, we found that some of our user (especially those using old versions of TF) use `GFile` quite frequently. Exposing these ability to `GFile` helps to speed up their code. But, yes, it also exposes complexity to end user. How about keeping the C++ utilities while reverting the Python API changes?", "Can you bridge internal storage systems via developing tf.data.Dataset sources and re-use the tf.data building blocks (such as `interleave` and `prefetch` for I/O parallelization and prefetching)? That would be the consistent with how data is ingested in most TensorFlow jobs.", "So may I think that it's not recommended to do such enhancements at file system level ?", "The filesystem story is already quite complex, so it would be better if we can simplify it instead of adding more features to it.\r\n\r\nHigher level libraries, like tf.data seem more suited for this", "@qiuxiafei Can you please check mihaimaruseac's comments and keep us posted. Thanks!", "According to @mihaimaruseac 's advice, it seems to be not encouraged to add buffering/prefetching into file system layer.  It's a different story to do buffering/prefetching in tf.data, maybe we can consider it in an other PR and close this one."]}]