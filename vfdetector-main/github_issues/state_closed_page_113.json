[{"number": 51649, "title": "What is the format setting used in the repository?", "body": "I am trying to create a pr but I am having problems with the auto-formatting code.\r\n\r\nWhat is the configuration used ? Right now I use yapf with :\r\n```\r\nyapf    \"--style\",   \"{based_on_style: google, indent_width: 2}\"\r\n```", "comments": ["@DachuanZhao ,\r\n\r\nCan you please refer this links for information regarding PR contribution.[Link1](https://www.tensorflow.org/datasets/contribute),[Link2](https://www.tensorflow.org/community/contribute/code).It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @DachuanZhao ,\r\n> \r\n> Can you please refer this links for information regarding PR contribution.[Link1](https://www.tensorflow.org/datasets/contribute),[Link2](https://www.tensorflow.org/community/contribute/code).It helps.Thanks!\r\n\r\nI have read them . I wonder why ```tensorflow``` use a linter ```pylint``` instead of a formatter ```yapf``` ? When I use ```yapf``` to format a python file , many parts of the file have been changed . Is this OK when pushing a PR?\r\n\r\n", "@DachuanZhao ,\r\nPlease take a look at the comment provided mentioned.It helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51648, "title": "\u3010JavaAPI\u3011IllegalStateException happended while running a model loading from SavedModel and the graph instance cant close itself ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, i will attach below.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 &&  Window 10 1909\r\n- TensorFlow installed from (source or binary): Java Maven\r\n```xml\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>tensorflow</artifactId>\r\n            <version>1.15.0</version>\r\n        </dependency>\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>libtensorflow</artifactId>\r\n            <version>1.15.0</version>\r\n        </dependency>\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>libtensorflow_jni_gpu</artifactId>\r\n            <version>1.15.0</version>\r\n        </dependency>\r\n```\r\n- TensorFlow version (use command below): 1.15.0\r\n\r\n**Describe the current behavior**\r\nI try to load a Saved Model from keras. It all works well till I try to run the session\u2014\u2014\r\nStrange thing occurs: the code just can't continue and throw no exception. \r\nWhen i use 'try catch finally' style instead of 'try with resource' style, i finally got such error message below:\r\n```\r\njava.lang.IllegalStateException: Error while reading resource variable dense_2/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/dense_2/kernel)\r\n\t [[{{node dense_2/MatMul/ReadVariableOp}}]]\r\n```\r\nAnd what's more, though I got the error message, the graph instance can't close itself,\r\nwhen I paused the test code in idea, i found the code stop at the Object.wait() method\uff0c\r\nwhich means that the graph.refcount kept 1 value all the time. \r\nThe code couldn't escape from graph.close() method.\r\nTo prove the correctness of saved model, I try to load model in python code like below:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nexport_path = \"./test/\";\r\n\r\ninput = np.random.random((1, 30));\r\n\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n    loaded = tf.saved_model.loader.load(sess, [\"serve\"], export_path)\r\n    graph = tf.get_default_graph()\r\n    # print(graph.get_operations())\r\n    x = sess.graph.get_tensor_by_name('rp_input:0')\r\n    y = sess.graph.get_tensor_by_name('dense_2/Sigmoid:0')\r\n    scores = sess.run(y,\r\n                      feed_dict={x: input});\r\n    print(\"predict: %d\" % (np.argmax(scores, 1)));\r\n```\r\nIt works well and print predict result, in that case, I think the problem may not lie in the model. (maybe?) \r\nI tried hard to find solution or workaround on stackoverflow and issues here,\r\nI saw several similar problems to mine, but they all occurs in python, such as :\r\nhttps://github.com/tensorflow/tensorflow/issues/28287\r\nand \r\nhttps://github.com/tensorflow/tensorflow/issues/22362\r\nthe second issues seems most alike, but the model export method is different.\r\n**Standalone code to reproduce the issue**\r\nHere is my model:\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/7036923/model.zip)\r\nHere is the test code, because it fails all over the time, i ommit the code to close the resources.\r\n```java\r\n    public void test_09_justTestAPI() {\r\n        float[] a = new float[]{1.53672f, 2.047399f, 1.42194f, 1.494959f, -0.69123f, -0.39482f, 0.236573f, 0.733827f, -0.531855f, -0.973978f, 1.704854f, 2.085134f, 1.615931f, 1.723842f, 0.102458f, -0.017833f, 0.693043f, 1.263669f, -0.217664f, -1.058611f, 1.300499f, 2.260938f, 1.156857f, 1.291565f, -0.42401f, -0.069758f, 0.252202f, 0.808431f, -0.189161f, -0.490556f};\r\n        long[] shape = new long[]{1, 30};\r\n        try {\r\n            SavedModelBundle savedModelBundle = SavedModelBundle.load(\".\", \"serve\");\r\n            Graph graph = savedModelBundle.graph();\r\n            Tensor<Float> data = Tensor.create(shape, FloatBuffer.wrap(a));\r\n            Session session = new Session(graph);\r\n            Session.Runner runner = session.runner()\r\n                    .feed(\"rp_input\", data)\r\n                    .fetch(\"dense_2/Sigmoid\");\r\n            float[][] res = new float[1][1];\r\n            Tensor<?> out = runner.run().get(0);\r\n            out.copyTo(res); // <artifactId>commons-io</artifactId>\r\n            BigDecimal pro = BigDecimal.valueOf(res[0][0]);\r\n        } catch (Exception e) {\r\n            throw e;\r\n        }\r\n    }\r\n```\r\n\r\n**Other info / logs** \r\nThe model is produced by webank federal learining program,\r\nIn their code, the model is build by code using keras api:\r\n```py\r\ndef _load_model(nn_struct_json):\r\n    return tf.keras.models.model_from_json(nn_struct_json, custom_objects={})\r\n```\r\nThe json content is definded like:\r\n```json\r\n      \"nn_define\": {\r\n        \"class_name\": \"Sequential\",\r\n        \"config\": {\r\n          \"name\": \"sequential\",\r\n          \"layers\": [\r\n            {\r\n              \"class_name\": \"RepeatVector\",\r\n              \"config\": {\r\n                \"name\":\"rp\",\r\n                \"n\":1\r\n              }\r\n            },\r\n            {\r\n              \"class_name\": \"LSTM\",\r\n              \"config\": {\r\n                \"name\":\"lstm\",\r\n                \"units\":32\r\n              }\r\n            },\r\n            {\r\n              \"class_name\": \"Dense\",\r\n              \"config\": {\r\n                \"name\": \"dense\",\r\n                \"trainable\": true,\r\n                \"dtype\": \"float32\",\r\n                \"units\": 64,\r\n                \"activation\": \"relu\"\r\n              }\r\n            },\r\n            {\r\n              \"class_name\": \"Dense\",\r\n              \"config\": {\r\n                \"name\": \"dense_2\",\r\n                \"trainable\": true,\r\n                \"dtype\": \"float32\",\r\n                \"units\": 1,\r\n                \"activation\": \"sigmoid\"\r\n              }\r\n            }\r\n          ]\r\n        },\r\n        \"keras_version\": \"2.2.4-tf\",\r\n        \"backend\": \"tensorflow\"\r\n      }\r\n```\r\nthe model is saved by code below:\r\n```py\r\n    def export_model(self):\r\n        with tempfile.TemporaryDirectory() as tmp_path:\r\n            # try:\r\n            #     tf.keras.models.save_model(self._model, filepath=tmp_path, save_format=\"tf\")\r\n            # except NotImplementedError:\r\n            #     import warnings\r\n            #     warnings.warn('Saving the model as SavedModel is still in experimental stages. '\r\n            #                   'trying tf.keras.experimental.export_saved_model...')\r\n            tf.keras.experimental.export_saved_model(self._model, saved_model_path=tmp_path)\r\n\r\n            model_bytes = _zip_dir_as_bytes(tmp_path)\r\n\r\n        return model_bytes\r\n``` \r\nYou can check the code in this link : \r\nhttps://github.com/FederatedAI/FATE/blob/master/python/federatedml/nn/backend/tf_keras/nn_model.py\r\nIn case, here is the log of my test code:\r\n```c\r\n2021-08-24 15:11:00.368680: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: .\r\n2021-08-24 15:11:00.377471: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2021-08-24 15:11:00.382175: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2021-08-24 15:11:00.390552: I tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\r\n2021-08-24 15:11:00.409095: I tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: .\r\n2021-08-24 15:11:00.416363: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 47658 microseconds.\r\n```\r\nI really stuck on this problem.\r\nI would appreciate it if someone can help me out, many thanks!", "comments": ["Hi @SennriSyunnga !we see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "> Hi @SennriSyunnga !we see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!\r\n\r\nThank you for your help! \r\nI keep the java maven tensorflow version equal to that of FATE project,\r\nAnd I didnt realize that maven repository have higher version artifacts than 1.15.0 until you told me.\r\nI read https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java first, the maven 1.15.0 link in the 'Quickstart' part really misled me.\r\nI will be appreciate it if someone can update the information in that page.\r\nI 'll have a try to the 2.0+ version artifact and give you the result as soon as I can.", "> > Hi @SennriSyunnga !we see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!\r\n> \r\n> Thank you for your help!\r\n> I keep the java maven tensorflow version equal to that of FATE project,\r\n> And I didnt realize that maven repository have higher version artifacts than 1.15.0 until you told me.\r\n> I read https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java first, the maven 1.15.0 link in the 'Quickstart' part really misled me.\r\n> I will be appreciate it if someone can update the information in that page.\r\n> I 'll have a try to the 2.0+ version artifact and give you the result as soon as I can.\r\n\r\nI fixed this problem by learning from this issue[https://github.com/tensorflow/java/issues/365] after using a higher version api:\r\n```xml\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>tensorflow-core-api</artifactId>\r\n            <version>0.3.1</version>\r\n        </dependency>\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>tensorflow-core-api</artifactId>\r\n            <version>0.3.1</version>\r\n            <classifier>linux-x86_64</classifier>\r\n        </dependency>\r\n```\r\nIn this version of artifact, I finnaly can use session.init in Java, like other ones who solved the problem did in python\r\nThen I correct my code like this: (*WATCH OUT! PLEASE SEE THE UPDATE PART BEBLOW!*\uff09\r\n```java\r\npublic void test_10_justTestAPI() {\r\n        float[] a = new float[]{1.53672f, 2.047399f, 1.42194f, 1.494959f, -0.69123f, -0.39482f, 0.236573f, 0.733827f, -0.531855f, -0.973978f, 1.704854f, 2.085134f, 1.615931f, 1.723842f, 0.102458f, -0.017833f, 0.693043f, 1.263669f, -0.217664f, -1.058611f, 1.300499f, 2.260938f, 1.156857f, 1.291565f, -0.42401f, -0.069758f, 0.252202f, 0.808431f, -0.189161f, -0.490556f};\r\n        try (SavedModelBundle savedModelBundle = SavedModelBundle.load(\".\", \"serve\")) {\r\n            FloatNdArray m = StdArrays.ndCopyOf(new float[][]{a});\r\n            try (TFloat32 data = TFloat32.tensorOf(m)) {\r\n                try (Session session = savedModelBundle.session()) {\r\n                    SignatureDef modelInfo = savedModelBundle.metaGraphDef().getSignatureDefMap().get(\"serving_default\");\r\n                    Map<String, TensorInfo> inputs = modelInfo.getInputsMap();\r\n                    String inputName = null;\r\n                    for (Map.Entry<String, TensorInfo> input : inputs.entrySet()) {\r\n                        TensorInfo ti = input.getValue();\r\n                        inputName = ti.getName();\r\n                        break;\r\n                    }\r\n                    String outputName = null;\r\n                    Map<String, TensorInfo> outputs = modelInfo.getOutputsMap();\r\n                    for (Map.Entry<String, TensorInfo> output : outputs.entrySet()) {\r\n                        outputName = output.getValue().getName();\r\n                        break;\r\n                    }\r\n                    session.run(\"init\");\r\n                    Session.Runner runner = session.runner();\r\n                    runner.feed(inputName, data)\r\n                            .fetch(outputName);\r\n                    try (TFloat32 out = (TFloat32) runner.run().get(0)) {\r\n                        FloatNdArray matrix = StdArrays.ndCopyOf(new float[1][1]);\r\n                        out.copyTo(matrix);\r\n                        FloatDataBuffer floatDataBuffer = DataBuffers.ofFloats(1);\r\n                        matrix.read(floatDataBuffer);\r\n                        float[] res = new float[1];\r\n                        floatDataBuffer.read(res);\r\n                        BigDecimal pro = BigDecimal.valueOf(res[0]);\r\n                    }\r\n                }\r\n            }\r\n        }catch (Exception e) {\r\n            logger.error(e);\r\n            throw e;\r\n        }\r\n    }\r\n```\r\nSoon after that, I found out the reason for why graph instance failed to close:\r\n**I use the graph of savedModelBundle to construct a new session**\r\n```java\r\n            SavedModelBundle savedModelBundle = SavedModelBundle.load(\".\", \"serve\");\r\n            Graph graph = savedModelBundle.graph();\r\n            Tensor<Float> data = Tensor.create(shape, FloatBuffer.wrap(a));\r\n            Session session = new Session(graph);// \u2190 this line\r\n```\r\nIn the correct way, I should use savedModelBundlesession() directly.\r\nI dont know whether this behavior is normal or not, but hope my experience can help others.\r\n\r\n***Update***\r\nI find 'session.run(\"init\")' is unnecessary and may be harmful.\r\nWhen you run(\"init\") once , you willl get a totally differernt predict result\u2026\u2026\r\nSo, it not a good practice. Don't follow it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51648\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51648\">No</a>\n"]}, {"number": 51647, "title": "[PluggableDevice] Add pluggable profiler C Interface support", "body": "Add TensorFlow profiler support for PluggableDevice.", "comments": ["@penpornk @yisitu @jbaiocchi This PR is the implementation of the RFC: [Add pluggable device profiler support ](https://github.com/tensorflow/community/pull/389), please help to review. Thanks very much", "@jbaiocchi Thanks for the comments, I have addressed all the comments, please help review it. thanks very much", "@jbaiocchi   I have addressed all the comments, please help to review it again. Thanks!", "@penpornk Thanks for the comments, I have addressed all the comments, please help to review. And I will submit an follow-up PR to add mock test for pluggable profiler. Thanks.", "@penpornk I saw some internal checks failed, if there is some thing need to fix, please let me know, thanks very much.", "@jzhoulon Thanks for checking! I've taken care of it. Right now it's waiting for reviews from API owners. :)"]}, {"number": 51646, "title": "Autograph could not transform <function Model.make_test_function.<locals>.test_function at 0x14ec9d430> and will run it as-is.", "body": "**System information**\r\n- Have I written custom code:\r\n- OS Platform and Distribution : MacOS 11.5.2\r\n- TensorFlow installed from TensorFlow_MacOS\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.8.10\r\n- GPU model and memory: Using apple \u0010M1 CPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Occasionally will return during training: \r\n```\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x14c680d30> and will run it as-is.\r\nPlease report this to the TensorFlow team.\r\n```\r\n**\r\n\r\n**Describe the expected behavior: no error message**\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nTraining fashion-MNIST:\r\n\r\n```from functools import partial\r\n\r\nDefaultConv2D = partial(keras.layers.Conv2D,\r\n                        kernel_size=3, activation='relu', padding=\"SAME\")\r\n\r\nmodel = keras.models.Sequential([\r\n    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\r\n    keras.layers.MaxPooling2D(pool_size=2),\r\n    DefaultConv2D(filters=128),\r\n    DefaultConv2D(filters=128),\r\n    keras.layers.MaxPooling2D(pool_size=2),\r\n    DefaultConv2D(filters=256),\r\n    DefaultConv2D(filters=256),\r\n    keras.layers.MaxPooling2D(pool_size=2),\r\n    keras.layers.Flatten(),\r\n    keras.layers.Dense(units=128, activation='relu'),\r\n    keras.layers.Dropout(0.5),\r\n    keras.layers.Dense(units=64, activation='relu'),\r\n    keras.layers.Dropout(0.5),\r\n    keras.layers.Dense(units=10, activation='softmax'),\r\n])\r\nmodel.summary()\r\n\r\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\r\nhistory = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\r\nscore = model.evaluate(X_test, y_test)\r\nX_new = X_test[:10] # pretend we have new images\r\ny_pred = model.predict(X_new)\r\n```\r\nSorry if I have not included the information correctly. I have never submitted a bug before.", "comments": ["@Loganpi In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.Thanks!", "Here is my full code:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\r\nX_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\r\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\r\n\r\nX_mean = X_train.mean(axis=0, keepdims=True)\r\nX_std = X_train.std(axis=0, keepdims=True) + 1e-7\r\nX_train = (X_train - X_mean) / X_std\r\nX_valid = (X_valid - X_mean) / X_std\r\nX_test = (X_test - X_mean) / X_std\r\n\r\nX_train = X_train[..., np.newaxis]\r\nX_valid = X_valid[..., np.newaxis]\r\nX_test = X_test[..., np.newaxis]\r\n\r\nfrom functools import partial\r\n\r\nDefaultConv2D = partial(keras.layers.Conv2D,\r\n                        kernel_size=3, activation='relu', padding=\"SAME\")\r\n\r\nmodel = keras.models.Sequential([\r\n    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\r\n    keras.layers.MaxPooling2D(pool_size=2),\r\n    DefaultConv2D(filters=128),\r\n    DefaultConv2D(filters=128),\r\n    keras.layers.MaxPooling2D(pool_size=2),\r\n    DefaultConv2D(filters=256),\r\n    DefaultConv2D(filters=256),\r\n    keras.layers.MaxPooling2D(pool_size=2),\r\n    keras.layers.Flatten(),\r\n    keras.layers.Dense(units=128, activation='relu'),\r\n    keras.layers.Dropout(0.5),\r\n    keras.layers.Dense(units=64, activation='relu'),\r\n    keras.layers.Dropout(0.5),\r\n    keras.layers.Dense(units=10, activation='softmax'),\r\n])\r\nmodel.summary()\r\n\r\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\r\nhistory = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\r\nscore = model.evaluate(X_test, y_test)\r\nX_new = X_test[:10] # pretend we have new images\r\ny_pred = model.predict(X_new)\r\n```\r\nIs this what you wanted?\r\nThanks, Logan", "@Loganpi I tried to run your code on colab using  latest stable version of TF 2.6.0 and didn't face the error reported.Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/9ae64262f1c1f121cbbefba20628b4cf/untitled404.ipynb) for reference .Please try to execute your code using TF v2.6 and let us know if the issue still persists ?Thank you! ", "Hey thanks, I actually can't get a newer nervation of TF because I'm on an M1 Mac. I guess ill just use collab until my pc comes.\r\nThanks, Logan", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51646\">No</a>\n"]}, {"number": 51645, "title": "model.layers returns empty list", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom Code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6\r\n- Python version: `3.8.2`\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n`model.layers` returns an empty list `[]`\r\n\r\n\r\nMinimum reproducible example:\r\n\r\n```\r\n# Third Party\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Concatenate, Dense\r\nfrom tensorflow.python.keras.models import Model\r\n\r\nclass MyModel(Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.con = Concatenate()\r\n        self.dense = Dense(10, activation=\"relu\")\r\n\r\n    def call(self, x):\r\n        x = self.con([x, x])\r\n        return self.dense(x)\r\n\r\nif __name__ == \"__main__\":\r\n   model = MyModel()\r\n   print(model.layers)\r\n```\r\n\r\nWhen I attach a pdb and step through the code, I find that there is a change in the boolean logic here:\r\nhttps://github.com/tensorflow/tensorflow/blob/002e5f47fa2ea99aa910ba83e66240c0cda8ff8a/tensorflow/python/keras/engine/base_layer.py#L2853\r\n\r\nI suspect that is part of the breaking changes described [here](https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0)\r\n\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nIf the `model.layers` API is being deprecated, what is correct API to use? If not, what is the correct class to use to create layers.", "comments": ["@NihalHarish ,\r\n\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n", "The issue is present only with the `tf.keras` APIs and not `keras.` APIs.\r\nIs there a plan to support both APIs for TF `2.6.x` while we transition to `2.7.x`\r\n\r\nThe code snippet shared above works fine if I change the import statements to:\r\n\r\n```\r\nfrom keras.layers import Concatenate, Dense\r\nfrom keras.models import Model\r\n```", "@NihalHarish ,\r\nAs mentioned above comment,all the keras related issues can be tracked in keras repo.Please feel free to post in keras repo.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51645\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51645\">No</a>\n"]}, {"number": 51644, "title": "tensorflow-gpu==2.6 don't run on cuda by default ", "body": "dear all dev,\r\n\r\ntensorflow-gpu==2.6 by default use oneAPI/oneDNN library \r\n\r\nput on by default cuda lib like previous version 2.5.\r\n\r\nproblem:\r\n\r\nprevious version , when import tensorflow as tf then 2nd message gave Successfully opened dynamic library cudart64_110.dll\r\nbut now 2.6 version don't saw that message and by default use intel integrated gpu (oneAPI/oneDNN library). \r\n\r\n", "comments": ["Hi! @sparrow84001  , \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue.", "[\r\n\r\nhttps://user-images.githubusercontent.com/37022413/130611931-760cbb11-c6f2-4797-8b91-a5d2a3c2107d.mp4\r\n\r\n](url)", "Hi @Saduf2019 ,Could you please look into this feature request.", "@sparrow84001 \r\nThis is not replica table, kindly use colab to run your code and let us know if you face any issues.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51641, "title": "Does a Docker container need to use the same exact libcuda version as the host's driver?", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: libcuda reported version is: 450.142.0 kernel reported version is: 450.66.0\r\n- GPU model and memory: GeForce RTX 2080 SUPER\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI'm trying to run an application that uses torch and tensorflow inside a docker container but I'm getting this error message:\r\n\r\n\"kernel version 450.66.0 does not match DSO version 450.142.0 -- cannot find working devices in this configuration\"\r\n\r\nThe cuda-compat-11-0 version inside the container is 450.142.0 but the host driver version is 450.66.0.\r\n\r\nAs I can't find cuda-compat-11-0 version 450.66.0, I tried other versions around it but I got the same error.\r\n\r\nI'm not able to change the Host driver, so my question is:\r\n\r\nDo I really have to install the exact same version on the docker container in order to make it work, or am I doing something wrong?\r\n\r\nPlease help me understand this issue. Thanks in advance!\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @ricardofunke , Could you please look into answers of following similar[ issue.](https://github.com/tensorflow/tensorflow/issues/48722)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51641\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51641\">No</a>\n"]}, {"number": 51640, "title": "Cannot load model checkpoints: Two checkpoint references resolved to different objects", "body": "I am trying to load model checkpoints saved during training. My model has custom layers and the full implementation can be found [here](https://github.com/AdityaKane2001/regnety/issues/15). \r\n\r\nMy model looks as follows:\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nPreStem (PreStem)            (None, 224, 224, 3)       0         \r\n_________________________________________________________________\r\nStem (Stem)                  (None, 111, 111, 32)      992       \r\n_________________________________________________________________\r\nStage_0 (Stage)              (None, 56, 56, 24)        4542      \r\n_________________________________________________________________\r\nStage_1 (Stage)              (None, 28, 28, 56)        12390     \r\n_________________________________________________________________\r\nStage_2 (Stage)              (None, 14, 14, 152)       277400    \r\n_________________________________________________________________\r\nStage_3 (Stage)              (None, 7, 7, 368)         2567444   \r\n_________________________________________________________________\r\nHead (Head)                  (None, 1000)              369000    \r\n=================================================================\r\nTotal params: 3,231,768\r\nTrainable params: 3,210,920\r\nNon-trainable params: 20,848\r\n_________________________________________________________________\r\n```\r\n\r\nWhen I try to load the model checkpoint after initializing the model architecture, I get the following warning message with an error as described below. \r\n\r\n```\r\nTwo checkpoint references resolved to different objects (<regnety.models.blocks.Stage object at 0x7f3c2049e110> and <regnety.models.blocks.Stage object at 0x7f3c2058ed50>).\r\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\r\n...\r\n(many such messages)\r\n...\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-0c1dca79eafe> in <module>()\r\n      1 from regnety.models import RegNetY\r\n      2 \r\n----> 3 model = RegNetY(\"200mf\", load_weights=True)\r\n      4 \r\n \r\n(long traceback)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)\r\n   1159     \"\"\"\r\n   1160     if not self.is_compatible_with(other):\r\n-> 1161       raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\n   1162 \r\n   1163   def most_specific_compatible_shape(self, other):\r\n\r\nValueError: Shapes (56,) and (24,) are incompatible\r\n```\r\nGist [here](https://colab.research.google.com/gist/AdityaKane2001/424e9f482c52b2e205d344d6cc27a8d9/groupedconvissue.ipynb?authuser=5). \r\n\r\nOther info:\r\n\r\nTF version: 2.6.0\r\nEnvironment: Colab with GPU\r\n", "comments": ["@AdityaKane2001 Could you please have a look at the [link ](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint?version=nightly) and similar [issue](https://github.com/tensorflow/models/issues/8892) .Please let us know if it helps?Thanks!", "@sushreebarsa \r\n\r\nI don't think the issue is relevant as none of them had the same issue. But I'll try with the link and get back to you.\r\n\r\nThe link suggests that we must compile and load the relevant optimizer also when loading the weights. AFAIK that isn't the case with Keras models without custom layers.", "@sanatmpa1 Was able to replicate this issue on colab using TF v[2.5](https://colab.research.google.com/gist/sushreebarsa/77833cb4ef97e25eb685ff0e6d7aceeb/groupedconvissue.ipynb#scrollTo=2ktbxlQ7klob),[2.6 ](https://colab.research.google.com/gist/sushreebarsa/8ab1cd8eb7b5cd3a37e98993cfa06ca8/groupedconvissue.ipynb#scrollTo=YmloOJA1cd4i)and [tf-nightly](https://colab.research.google.com/gist/sushreebarsa/e59d3712ab7003365176c67c714d84cf/untitled403.ipynb) ,please find the attached gists . Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51640\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51640\">No</a>\n"]}, {"number": 51639, "title": "No module named 'keras.api' after compiling tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.8.10\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\n**Describe the problem**\r\n\r\nI manually compiled tensorflow from sources. After compiling, my script is giving the error message \"No module named 'keras.api'\". \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\ni can't post my script but this is the import section...\r\n\r\nimport os\r\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nfrom pandas import read_csv, DataFrame, concat\r\nfrom numpy import array\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom tensorflow.keras.layers import LSTM, Dense, Input, Bidirectional, Attention, Conv1D, TimeDistributed\r\nimport tensorflow as tf\r\ntf.config.optimizer.set_jit(True)\r\nfrom math import fabs, floor, ceil, isnan\r\nfrom sklearn import preprocessing\r\nfrom tensorflow.keras import Input, layers, models\r\nimport itertools\r\nimport matplotlib.pyplot as plt\r\nfrom statsmodels.tsa.stattools import kpss\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.preprocessing import PolynomialFeatures\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\nfrom deap import base, creator, tools, algorithms\r\nimport random\r\nimport multiprocessing\r\nimport pickle\r\nfrom fredapi import Fred\r\nimport csv\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThere's no traceback. There's no error. The script pumps out that keras.api is missing.\r\n\r\nI think I compiled it wrong. Any help greatly appreciated.\r\n\r\n\r\nEDIT:\r\n\r\nI dug in and its the tf.keras.optimizers.Adam function. \r\n\r\nHere's the traceback.\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 973, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'keras.api'\r\n", "comments": ["I fixed it reinstalling keras using pip.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51639\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51639\">No</a>\n"]}, {"number": 51638, "title": "Failed to load the native TensorFlow runtime.", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution: Mac OS X 10.11.6\r\n- TensorFlow installed:from terminal\r\n- TensorFlow version:~\r\n- Python version:3.8\r\n- Installed using pip\r\n- Bazel version ~\r\n- GCC/Compiler version not compiled from source\r\n- CUDA/cuDNN version:No cuda graphic card\r\n- GPU model and memory:memory 10 gb GPU~\r\n\r\n\r\n\r\n**Describe the problem**\r\nSo ive ran mlagents-learn config/trainer_config.yaml --run-id=firstrun3dball --train which give error Failed to load the native TensorFlow runtime. which ive traced that is not the package problem but instead tensor flow problem\r\n\r\n\r\n**Any other info / logs**\r\nfull log:Traceback (most recent call last):\r\n  File \"/Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation\r\n  Referenced from: /Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n in /Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/phukhao/opt/anaconda3/envs/ML-agent/bin/mlagents-learn\", line 33, in <module>\r\n    sys.exit(load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')())\r\n  File \"/Users/phukhao/opt/anaconda3/envs/ML-agent/bin/mlagents-learn\", line 25, in importlib_load_entry_point\r\n    return next(matches).load()\r\n  File \"/Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/importlib/metadata.py\", line 77, in load\r\n    module = import_module(match.group('module'))\r\n  File \"/Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/Users/phukhao/Downloads/ml-agents-release_1/ml-agents/mlagents/trainers/learn.py\", line 12, in <module>\r\n    from mlagents import tf_utils\r\n  File \"/Users/phukhao/Downloads/ml-agents-release_1/ml-agents/mlagents/tf_utils/__init__.py\", line 1, in <module>\r\n    from mlagents.tf_utils.tf import tf as tf  # noqa\r\n  File \"/Users/phukhao/Downloads/ml-agents-release_1/ml-agents/mlagents/tf_utils/tf.py\", line 3, in <module>\r\n    import tensorflow as tf  # noqa I201\r\n  File \"/Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/python/eager/context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"/Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/python/pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation\r\n  Referenced from: /Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n in /Users/phukhao/opt/anaconda3/envs/ML-agent/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["ok its pillow problem,and its unsolved", "Hi @dogevspenguin ,  We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].  \r\n              \r\nCould you also please look at the [comments](https://github.com/tensorflow/tensorflow/issues/24642#issuecomment-452911775) in this issues , It suggests upgrading to macOS 10.12.6 (Sierra) or later. To use Tensorflow on old CPUs you'll need to build a binary on your own.  Also try uninstall tensorflow and install using `\"conda install tensorflow\" `command\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51638\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51638\">No</a>\n"]}, {"number": 51637, "title": "Tensor data is incorrectly loaded from FlatBuffers on big-endian targets", "body": "Currently `InitializeTfLiteTensorFromFlatbuffer` and `InitializeTfLiteEvalTensorFromFlatbuffer` refer to a data in a FlatBuffer using a direct pointer. For example in https://github.com/tensorflow/tflite-micro/blob/a9f2e03b943990bc958ce3b92ca76ffc27fcf6d6/tensorflow/lite/micro/micro_allocator.cc#L440:\r\n\r\n```\r\nresult->data.data = GetFlatbufferTensorBuffer(flatbuffer_tensor, buffers); \r\n``` \r\n\r\nBecause data in FlatBuffers is always little-endian, on big-endian machines it causes problems because any type wider than 8 bits will be read with bytes swapped.", "comments": ["This issue is related to https://github.com/tensorflow/tflite-micro, so I'll report it there. Closing this one."]}, {"number": 51636, "title": "Typo Fix: arugment -> argument", "body": "arugment -> argument", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51636) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\r\n", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac"]}, {"number": 51635, "title": "tf 2.6 break interfance of model.compile, unable to pass tf.keras.optimizers.Adam", "body": "tf 2.6\r\n      from tensorflow.python.keras.optimizer_v1 import Optimizer\r\n      from tensorflow.python.keras.optimizer_v2 import optimizer_v2\r\n      optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\n      ic(optimizer)\r\n      ic(isinstance(optimizer, (Optimizer, optimizer_v2.OptimizerV2)))\r\n\r\nic| optimizer: <keras.optimizer_v2.adam.Adam object at 0x7fc5e03f32b0>\r\nic| isinstance(optimizer, (Optimizer, optimizer_v2.OptimizerV2)): False\r\n\r\nSo we can not pass optimizer to model.compile now as in tensorflow/python/keras/optimizers.py\r\n\r\n@keras_export('keras.optimizers.get')\r\ndef get(identifier):\r\n  \"\"\"Retrieves a Keras Optimizer instance.\r\n\r\n  Args:\r\n      identifier: Optimizer identifier, one of\r\n          - String: name of an optimizer\r\n          - Dictionary: configuration dictionary. - Keras Optimizer instance (it\r\n            will be returned unchanged). - TensorFlow Optimizer instance (it\r\n            will be wrapped as a Keras Optimizer).\r\n\r\n  Returns:\r\n      A Keras Optimizer instance.\r\n\r\n  Raises:\r\n      ValueError: If `identifier` cannot be interpreted.\r\n  \"\"\"\r\n  if isinstance(identifier, (Optimizer, optimizer_v2.OptimizerV2)):\r\n    return identifier", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51635\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51635\">No</a>\n", "tested ok on 2.6rc2"]}, {"number": 51634, "title": "Document XLA core functionality", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/xla/operation_semantics\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThere are references throughout the XLA docs to various core types that aren't fully documented. These include `XlaBuilder`, `XlaOp`, `XlaComputation` and `Parameter`. There may be others. It would be really helpful if these were documented.\r\n\r\n### Clear description\r\n\r\nIdeally there would be comprehensive API docs including all public functionality, how and why to use each, but anything in this direction would be most appreciated.\r\n\r\n### Correct links\r\n\r\nWhere they exist\r\n\r\n### Parameters defined\r\n\r\nNo\r\n\r\n### Returns defined\r\n\r\nNo\r\n\r\n### Raises listed and defined\r\n\r\nNo\r\n\r\n### Usage example\r\n\r\nSome\r\n\r\n### Request visuals, if applicable\r\n\r\nI don't know. I don't think they're crucial. Don't know if they'd help.\r\n\r\n### Submit a pull request?\r\n\r\nNo", "comments": ["Hi @Saduf2019 ,Could you please look into this issue. ", "@joelberkeley \r\nPlease feel free to submit a pr with the changes expected.", "@Saduf2019 I can't because I don't know how the functionality works. I'm looking for the docs so I can learn it", "Hi, I am familiar with the functionality. If you'd like, I could go through the codebase and add missing documentation", "@hohilwik I'd find that really helpful, thank you", "I am not sure where to make the pull request though. The documentation folder seems to have been moved. \r\n@Saduf2019 If you could suggest something, and maybe assign this issue to me", "this might be it tensorflow/compiler/xla/g3doc/operation_semantics.md", "Alright, I'll work on it then", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "XLA documentation uses different markdown from github so it took me a while to figure out. Sorry about that. Will submit PR in a day or two", "Closing as stale. Please reopen if you'd like to work on this further.\n", "hi @hohilwik how did it go?"]}, {"number": 51633, "title": "Error in model.fit", "body": "Made a Tensorflow fuctional API model on top of  TFAutoModelForSequenceClassification with 3 sentence as input. \r\n\r\nTraining model directly on tokenized input raises\r\n**ValueError: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'tensorflow.python.framework.ops.EagerTensor\\'>\"})'}), (<class 'list'> containing values of types {\"<class 'int'>\"})**\r\n\r\nIf I convert it into numpy array it raises \r\n**ValueError: Data cardinality is ambiguous:**\r\n\r\n`model(X_train[0])`  prduces the desired result in both cases but on training the model it raises errors.\r\n\r\n\r\nCode can be found in this [notebook](https://colab.research.google.com/drive/1wsVVHiaqBF8joIEsP_XSMF35fnDQS19D?usp=sharing)", "comments": ["@old-school-kid Could you please have a look at the [link](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#fit), and similar[ issue1](https://stackoverflow.com/questions/64718185/efficiently-converting-for-model-fit), [issue2](https://stackoverflow.com/questions/58605279/tensorflow-value-error-in-model-fit-how-to-fix).Please let us know if it helps?Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51633\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51633\">No</a>\n"]}, {"number": 51632, "title": "add label_names to make_csv_dataset_v2", "body": "[https://github.com/tensorflow/tensorflow/issues/51571](https://github.com/tensorflow/tensorflow/issues/51571)", "comments": ["@DachuanZhao  Can you please address PyLint errors? Thanks!", "> address PyLint errors\r\n\r\nHow could I address PyLint errors in vscode ? Is there a guide or a document ?", "@DachuanZhao Can you please check @aaudiber's comments and keep us posted ? Thanks!", "> Thanks! Can you please also add unit tests in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/kernel_tests/make_csv_dataset_test.py?\r\n\r\nGet it. I will add the unit test.", "@DachuanZhao  Any update on this PR? Please. Thanks!", "@DachuanZhao Any update on this PR? Please. Thanks!", "@DachuanZhao Any update on this PR? Please. Thanks!"]}, {"number": 51631, "title": "DeprecationWarning: the imp module is deprecated in favour of importlib", "body": "- Python version: 3.8.10\r\n- Tensorflow version: 2.6.0\r\n\r\nHi team,\r\nI keep on getting the deprecation warning while using tensorflow:\r\n`DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses`\r\n\r\nI know that's just a warning, but do you plan to update this library?\r\nBest!", "comments": ["@HannaLochOlszewska ,\r\n In order to expedite the trouble-shooting process, could you please provide a minimal code snippet where you are facing the warning.Thanks!", "Hi @tilakrayal, sure. The warning is visible when I run pytest on file with loaded tensorflow.\r\ndummy_test.py:\r\n```\r\nimport pytest\r\nimport tensorflow as tf\r\n```\r\nI run:\r\n`pytest dummy_test.py`\r\nand get a warning.", "The issue will be fixed with PR #51663.", "The issue will move to closed status once the PR is merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51631\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51631\">No</a>\n"]}, {"number": 51630, "title": "Implementation of a Copy Removal Pass.", "body": "In this PR we implemented the first version of the CopyRemovalPass. This pass removes operations that implement a `CopyOpInterface` when the source `UseInterval` ends with the copy operation and the properties of dominance are not violated. In the respective `UseInterval` after the copy operation the target value is replaced with the source value.\r\n\r\nThe next iteration should also consider copy operations that copy slices.\r\n\r\nThis PR also includes cleanups of the `UserangeAnalysis` in form of an addition `UseInterval` struct.", "comments": []}, {"number": 51627, "title": "[PluggableDevice] Enable TensorList C APIs", "body": "This PR adds support for TensorList C APIs following to the Variable RFC ideas. It supports\r\n\r\n1. create or delete TensorList\r\n2. set/get TensorList from Tensor\r\n3. some TensorList's methods like get tensor, get element shape or dtype, deep copy and so on.\r\n\r\nDeveloper can use these C APIs to implement TensorList relative kernels.", "comments": ["@saxenasaurabh Can you please review this PR ? Thanks!", "@saxenasaurabh Can you please review this PR ? Thanks!"]}, {"number": 51626, "title": "Why my tf-lite model's fullyconnect and softmax options not be delegated with CoreML?", "body": "*enviorment: iPhone 12*\r\n\r\nI have a model converted by TFLiteConverter. It has some opttions like lstm\u3001add\u3001mul\u3001fullyconnect\u3001softmax. When I use the CoreML delegate like the example code in docs. I found that There are just some of the all options delegated by CoreML. I understand some option like lstm would not be delegated. But I have no idea when I found my fullyconnect and softmax are also not be delegated by profiling the app in Xcode Instrument. \r\nFor contrast, I download a DensetNet model. I found CoreML delegate almost all the options include fc and softmax.\r\n\r\nWhy is that? Any reply will be appreciated.", "comments": ["Now I found the reason why my Dense Not been delegate to CoreML is that its' dims except last dim are not 1. ", "> Now I found the reason why my Dense Not been delegate to CoreML is that its' dims except last dim are not 1.\r\n\r\nwhat do u mean ? i am having the same issue , \r\n\r\nError compiling model compiler error: Size of bias = 0 is not equal to the output channels = 7 in layer FullyConnectedOpBuilder_17\r\n   "]}, {"number": 51625, "title": "tf.keras.layers.ConvLSTM2D crashes when kernel_size contains zero", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4 and 2.6.0\r\n- Python version: 3.6\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ndata_format = \"channels_first\"\r\nreturn_sequences = False\r\nfilters = 2\r\nkernel_size = [0, 1]\r\npadding = \"valid\"\r\nlayer = tf.keras.layers.ConvLSTM2D(data_format=data_format,return_sequences=return_sequences,filters=filters,kernel_size=kernel_size,padding=padding,)\r\nx = tf.keras.Input(shape = (2, 2, 5, 5))\r\ny = layer(x)\r\nmodel = tf.keras.Model(x, y)\r\ninput = tf.random.uniform((2, 2, 2, 5, 5), dtype=tf.float32)\r\nres = model(input)\r\n```\r\n\r\n**Describe the current behavior**\r\nIt crashes in both TF 2.4 and TF 2.6.\r\n", "comments": ["Hi @lugalUrim ,\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "This issue seems to have been fixed by the latest tf-nightly as well.", "Hi @lugalUrim, But Could you point me the reason behind making kernel size = 0 . We give a kernel size  to  filter  through image while extracting certain features. I think this might be issue of this crash.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mohantym if user supplies invalid arguments then there should be an error, not a crash. An attacker will try to pass invalid arguments, so we should still handle these properly.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51625\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51625\">No</a>\n"]}, {"number": 51624, "title": "tf.keras.layers.LocallyConnected2D crashes with implementation mode = 2 and negative strides", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4 and 2.6\r\n- Python version: 3.6\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np                                                                                                                                                                         \r\nstrides = [-4,4,]\r\nlayer = tf.keras.layers.LocallyConnected2D(filters=3,kernel_size=3,padding=\"valid\",strides=strides,implementation=2)\r\ninput = tf.random.uniform((3, 6, 10, 4), dtype=tf.float32)\r\nres = layer(input)\r\n```\r\n\r\n**Describe the current behavior**\r\nI run this snippet on TensorFlow 2.4 and it outputs `Floating point exception (core dumped)` and crashes. I tried to execute it on Google Colab with TensorFlow 2.6.0, and the session also crashes.\r\n\r\n\r\n", "comments": ["@lugalUrim \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "This issue seems to have been fixed by the latest tf-nightly.", "@lugalUrim I tried to run your code on colab using TF-nightly 2.7.0-dev20210824 and didn't face the error as reported here , please find the attached gist  [here](https://colab.research.google.com/gist/sushreebarsa/bf73be4ca71e28640957401b965ba76a/untitled406.ipynb#scrollTo=9S1Di1lI9V0U) for reference .Please confirm the same.Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51624\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51624\">No</a>\n"]}, {"number": 51623, "title": "calback pyfunc_18 is not found", "body": "**tensorflow 2.5, python3.7**\r\n\r\nI'm using tf.py_function in the model, and after training, the model is saved with tf.saved_model.save().\r\n\r\nIs this the compatible problem of these two functions that causes this error?", "comments": ["@sjtusmartboy ,\r\n\r\n In order to expedite the trouble-shooting process, could you please provide a complete code and dataset  you are using.Also please take a look at this [comment](https://stackoverflow.com/questions/56227140/using-py-func-inside-tensorflow-valueerror-callback-pyfunc-0-is-not-found) with similar error.It helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51623\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51623\">No</a>\n"]}, {"number": 51621, "title": "use SPICE tf-lite model in Swift but get error \"Provided data count 64000 must match the required count 4\"", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS15.0\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): \r\n```\r\n    'TensorFlowLiteSwift', '~> 0.0.1-nightly'  \r\n    'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'\r\n```\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nFailed to invoke the interpreter with error: Provided data count 64000 must match the required count 4.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nI'm using the SPICE model [here](https://tfhub.dev/google/lite-model/spice/1) which is used to recognize the dominant pitch in sung audio. \r\n\r\nThis model's input is like below:\r\n<img width=\"482\" alt=\"\u622a\u5c4f2021-08-22 \u4e0b\u53482 05 14\" src=\"https://user-images.githubusercontent.com/5517281/130344383-fac8767c-906e-474a-8184-5291435c0037.png\">\r\n\r\n\r\nIt says this input sample rate should be 16000, so I set the sameRate to 16000,I run model in Swift like this:\r\n```swift\r\nfunc runModel(onBuffer buffer: [Int16]) -> [Dictionary<String, Any>]? { // here buffer has 16000 elements.\r\n   do {\r\n            // Copy the `[Int16]` buffer data as an array of `Float`s to the audio buffer input `Tensor`'s.\r\n            let audioBufferData = Data(copyingBufferOf: buffer.map { Float($0) / maxInt16AsFloat32 })\r\n            try interpreter.copy(audioBufferData, toInputAt: 0)\r\n\r\n            // Run inference by invoking the `Interpreter`.\r\n            let startDate = Date()\r\n            try interpreter.invoke(). <------ Here get the error:  Provided data count 64000 must match the required count 4\r\n            ...\r\n    }\r\n}\r\n```\r\n\r\nBut finally I got this error. Could anyone tell me where am I wrong?  Thanks!\r\n", "comments": ["Looks like you need to resize the input tensors before invoking the model.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51621\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51621\">No</a>\n"]}, {"number": 51620, "title": "error", "body": "Internal Server Error: /form/\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\MONZER\\Desktop\\progect\\lib\\site-packages\\django\\core\\handlers\\exception.py\", line 34, in inner\r\n    response = get_response(request)\r\n  File \"C:\\Users\\MONZER\\Desktop\\progect\\lib\\site-packages\\django\\core\\handlers\\base.py\", line 115, in _get_response\r\n    response = self.process_exception_by_middleware(e, request)\r\n  File \"C:\\Users\\MONZER\\Desktop\\progect\\lib\\site-packages\\django\\core\\handlers\\base.py\", line 113, in _get_response\r\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\r\n  File \"C:\\Users\\MONZER\\Desktop\\progect\\djangoAPI\\MyAPI\\views.py\", line 80, in cxcontact\r\n    answer=approvereject(ohevalue(df))[0]\r\n  File \"C:\\Users\\MONZER\\Desktop\\progect\\djangoAPI\\MyAPI\\views.py\", line 52, in approvereject\r\n    y_pred=mdl.predict(X)\r\n  File \"C:\\Users\\MONZER\\Desktop\\progect\\lib\\site-packages\\keras\\engine\\training.py\", line 1693, in predict\r\n    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\r\n  File \"C:\\Users\\MONZER\\Desktop\\progect\\lib\\site-packages\\keras\\engine\\training.py\", line 716, in distribute_strategy\r\n    return self._distribution_strategy or tf.distribute.get_strategy()\r\nAttributeError: 'Sequential' object has no attribute '_distribution_strategy'\r\n[21/Aug/2021 22:54:34] \"POST /form/ HTTP/1.1\" 500 88802", "comments": ["@monzer511 ,\r\n\r\n In order to expedite the trouble-shooting process, could you please provide a complete code and the TensorFlow version you are using.Thanks!", "Also please take a look at this links for information with similar error.[Link1](https://stackoverflow.com/questions/59765784/attributeerror-sequential-object-has-no-attribute-get-distribution-strategy),[Link2](https://newbedev.com/keras-and-tensorboard-attributeerror-sequential-object-has-no-attribute-get-distribution-strategy).It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51618, "title": "tf.image.extract_glimpse crashes with negative input", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.6\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nx = np.arange(9).reshape([1,3,3,1])\r\nres = tf.image.extract_glimpse(x, size=[1023, -63], offsets=[1023, 63], centered=False, normalized=False) # Crash\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nIt crashes when I execute the above code.\r\n\r\n**Describe the expected behavior**\r\nShould throw a `ValueError`.\r\n\r\n", "comments": ["Happy to add the `app.log` from Google Colab executing above snippet for the latest version `2.6.0`.\r\n\r\nSeems to me like the call of `CHECK_LT()` in `tensorflow/core/framework/tensor_shape.cc:569` defined at `tensorflow/core/platform/default/logging.h:413` causes that issue.\r\n\r\nHowever, I am not a C++ expert, so I don't get any much further than that for now.\r\n\r\n<html><body>\r\n<!--StartFragment-->\r\n\r\nTimestamp | Level | Message\r\n-- | -- | --\r\nAug 22, 2021, 12:34:44 AM | WARNING | WARNING:root:kernel 8e337399-7e5c-4782-aaa6-34f661e5c692 restarted\r\nAug 22, 2021, 12:34:44 AM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports\r\nAug 22, 2021, 12:34:44 AM | WARNING | 2021-08-21 22:34:44.441658: F tensorflow/core/framework/tensor_shape.cc:569] Check failed: size >= 0 (-63 vs. 0)\r\nAug 22, 2021, 12:34:44 AM | WARNING | 2021-08-21  22:34:44.376372: I  tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver  does not appear to be running on this host (c679a9bf8bfa):  /proc/driver/nvidia/version does not exist\r\nAug 22, 2021, 12:34:44 AM | WARNING | 2021-08-21  22:34:44.375437: E tensorflow/stream_executor/cuda/cuda_driver.cc:271]  failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is  detected\r\nAug 22, 2021, 12:34:37 AM | INFO | Adapting to protocol v5.1 for kernel 8e337399-7e5c-4782-aaa6-34f661e5c692\r\nAug 22, 2021, 12:34:35 AM | INFO | Kernel started: 8e337399-7e5c-4782-aaa6-34f661e5c692\r\nAug 22, 2021, 12:33:19 AM | INFO | Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\nAug 22, 2021, 12:33:19 AM | INFO | http://172.28.0.12:9000/\r\nAug 22, 2021, 12:33:19 AM | INFO | The Jupyter Notebook is running at:\r\nAug 22, 2021, 12:33:19 AM | INFO | Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\nAug 22, 2021, 12:33:19 AM | INFO | http://172.28.0.2:9000/\r\nAug 22, 2021, 12:33:19 AM | INFO | The Jupyter Notebook is running at:\r\nAug 22, 2021, 12:33:19 AM | INFO | 0 active kernels\r\nAug 22, 2021, 12:33:19 AM | INFO | Serving notebooks from local directory: /\r\nAug 22, 2021, 12:33:19 AM | INFO | 0 active kernels\r\nAug 22, 2021, 12:33:19 AM | INFO | Serving notebooks from local directory: /\r\nAug 22, 2021, 12:33:19 AM | INFO | google.colab serverextension initialized.\r\nAug 22, 2021, 12:33:19 AM | INFO | google.colab serverextension initialized.\r\nAug 22, 2021, 12:33:19 AM | INFO | Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\r\nAug 22, 2021, 12:33:19 AM | INFO | Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\r\n\r\n<!--EndFragment-->\r\n</body>\r\n</html>", "@lugalUrim Could you please refer to the [link](https://www.tensorflow.org/api_docs/python/tf/image/extract_glimpse) . Please try to upgrade the TF version 2.4 to 2.6.0 and refer to the above [comment](https://github.com/tensorflow/tensorflow/issues/51618#issuecomment-903185385).Please let us know if it helps ?", "Added a PR #51618 for the fix.", "@lugalUrim This issue will be closed once the PR is merged! Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51618\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51618\">No</a>\n"]}, {"number": 51616, "title": "Error while building tflite python wheel for beaglebone black ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Beaglebone black\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: Created inside a virtualenv then cross compiled using cmake v3.21.1\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Error while building custom Tflite python wheel for Beaglebone black**\r\n\r\nI have a beaglebone black version with cpuinfo of:\r\n\r\nprocessor       : 0\r\nmodel name      : ARMv7 Processor rev 2 (v7l)\r\nBogoMIPS        : 995.32\r\nFeatures        : half thumb fastmult vfp edsp thumbee neon vfpv3 tls vfpd32\r\nCPU implementer : 0x41\r\nCPU architecture: 7\r\nCPU variant     : 0x3\r\nCPU part        : 0xc08\r\nCPU revision    : 2\r\nHardware        : Generic AM33XX (Flattened Device Tree)\r\nRevision        : 0000\r\nSerial          : 2033SBI01413\r\n\r\nI want to build a custom python wheel since the prebuilt wheels aren't compatible with my board. I followed the instructions on Tensorflow lite documentation for cross compiling the py wheel for ARM devices using cmake. Link to documentation: https://www.tensorflow.org/lite/guide/build_cmake_pip\r\n\r\nThe installation is done inside a virtualenv with python version 3.7 with these packages installed inside it:\r\n- cmake         3.21.1.post1\r\n- numpy         1.21.2\r\n- pip           21.2.4\r\n- pkg_resources 0.0.0\r\n- pybind11      2.7.1\r\n- setuptools    57.4.0\r\n- wheel         0.37.0\r\n\r\nI used docker to setup a cross build environment. Then, configured custom build flags to be compatible with my target in tensorflow/lite/tools/cmake/download_toolchains.sh as instructed in the documentation. \r\n\r\nI entered this command:\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n  tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh armhf\r\n```\r\n\r\nThe Docker container environment was created but the tflite py wheel failed to compile raising this error at 96%\r\n\r\n```\r\nmake[3]: Leaving directory '/workspace/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.7/cmake_build'\r\nmake[2]: *** [CMakeFiles/tensorflow-lite.dir/all] Error 2\r\nCMakeFiles/Makefile2:1145: recipe for target 'CMakeFiles/tensorflow-lite.dir/all' failed\r\nmake[2]: Leaving directory '/workspace/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.7/cmake_build'\r\nmake[1]: *** [CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/rule] Error 2\r\nCMakeFiles/Makefile2:1253: recipe for target 'CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/rule' failed\r\nmake[1]: Leaving directory '/workspace/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.7/cmake_build'\r\nMakefile:215: recipe for target '_pywrap_tensorflow_interpreter_wrapper' failed\r\nmake: *** [_pywrap_tensorflow_interpreter_wrapper] Error 2\r\n```\r\n\r\n\r\n**Any help to solve this issue will be greatly appreciated**\r\n", "comments": ["@terryheo for the visibility", "I've just tested 2.5 branch and It works well for me.\r\n\r\nCould you share more log before the error?", "Thanks for your response. \r\n\r\nSo I just solved the issue and it works now. The problem that I was using the configuration inside bbb_makefile.inc located in tensorflow\\tensorflow\\lite\\tools\\make\\targets to target my beaglebone (This directory was deprecated last week). Apparently the Neon VFP version wasn't specified in it, so I edited ```-mfpu=neon``` to ```-mfpu=neon-vfpv3```, which compiled fine and generated the custom pywheel and confirmed that it is working. \r\n\r\nHere is the [Beaglebone Black TFlite Python Wheel](https://drive.google.com/file/d/1E9g4nV5f46PtaQBa_i1Xxfi1AernfhxB/view?usp=sharing) in case anyone needs it. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51616\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51616\">No</a>\n"]}, {"number": 51615, "title": "[WIP] Fixing issues in python package", "body": "* Fixing typo mistakes\r\n* Fixing type issues\r\n* Fixing documentation\r\n* Fixing OOP conceptual issues\r\n* Refactoring in some cases\r\n* etc", "comments": []}, {"number": 51614, "title": "Resource exhausted: EfficientNetB7 OOM", "body": "<h1>1. System information</h1>\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\nTensorFlow installation (pip package or built from source): 2.6.0\r\nTensorFlow library (version, if pip package or github SHA, if built from source): all other libraries cloned from https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\r\n\r\n<h1>2. Error Code</h1>\r\nFull Error code was saved in\r\nhttps://github.com/cjfghk5697/Dacon_Code_Review/blob/main/cifar10.ipynb\r\n\r\nError code\r\n```python\r\nepochs = 40 \r\nhist = model.fit(ds_train, epochs=epochs, validation_data=ds_test, verbose=2)\r\n````\r\nError\r\n```python\r\nResourceExhaustedError: 2 root error(s) found.\r\n  (0) Resource exhausted:  OOM when allocating tensor with shape[64,192,300,300] and type half on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node sequential_2/model_2/efficientnetb7/block2a_expand_conv/Conv2D (defined at /lib/python3.7/threading.py:926) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\r\n\r\n\t [[div_no_nan_1/ReadVariableOp/_26]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\r\n\r\n  (1) Resource exhausted:  OOM when allocating tensor with shape[64,192,300,300] and type half on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node sequential_2/model_2/efficientnetb7/block2a_expand_conv/Conv2D (defined at /lib/python3.7/threading.py:926) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\r\n\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_308507]\r\n```\r\nI use a EfficientNetB7. But EfficientNetB7 fails fit model. Because the Resource exhausted in Colab. So I wish want to know how do fix this error.\r\n\r\n<h1>3. After an Error</h1>\r\nFirst, I added operations such as batch normalization because the efficientnetb7 parameter was too large to cause oom, but it didn't help.\r\n\r\nSecond, I followed the code on https://stackoverflow.com/questions/49665757/how-to-add-report-tensor-allocations-upon-oom-to-runoptions-in-keras, but the tensorflow version was different, so I couldn't use it.\r\n\r\nThird\r\nI've tried similar code on https://stackoverflow.com/questions/64197155/tf2-add-report-tensor-allocations-upon-oom-to-runoptions, but I don't know if this is the solution. The error remained the same.\r\n\r\n\r\n", "comments": ["How about EfficientNet-B1 or B2 ? Since EfficientNet-B7 requires large GPU memory, have you tried to use TPU in colab  ? (https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/tpu.ipynb).", "@edwardyehuang Thanks for your answer.  I was tried to EfficientNet-B2. It was work well. But when i was used TPU in colab. It has a some error about labels. So i will try to more code. Once Again Thx for your answer.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51614\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51614\">No</a>\n"]}, {"number": 51604, "title": "disable ampere tf32 failures", "body": "Some tests use matmul op to compute the reference value. For TF32 those ops are less precise causing some tests to fail. This is PR is to address the issue.", "comments": ["> Some tests use matmul op to compute the reference value. For TF32 those ops are less precise causing some tests to fail.\r\n\r\nIn cases where matmul is only used to compute reference values, it is preferable to change the test to use [`test_util.matmul_without_tf32`](https://github.com/tensorflow/tensorflow/blob/9edd7b971aacc3545f7f08c628c39f4761036f01/tensorflow/python/framework/test_util.py#L2087), so that TF32 is not disabled for the op(s) being tested. But given we are trying to get the tests working with CUDA 11.4 as soon as possible, I'm also fine with sticking with `test_util.run_without_tensor_float_32` for now.\r\n\r\nAgree with Sanjoy that, if we stick with `run_without_tensor_float_32`, there should be a more descriptive description.", "> > Some tests use matmul op to compute the reference value. For TF32 those ops are less precise causing some tests to fail.\r\n> \r\n> In cases where matmul is only used to compute reference values, it is preferable to change the test to use [`test_util.matmul_without_tf32`](https://github.com/tensorflow/tensorflow/blob/9edd7b971aacc3545f7f08c628c39f4761036f01/tensorflow/python/framework/test_util.py#L2087), so that TF32 is not disabled for the op(s) being tested. But given we are trying to get the tests working with CUDA 11.4 as soon as possible, I'm also fine with sticking with `test_util.run_without_tensor_float_32` for now.\r\n> \r\n> Agree with Sanjoy that, if we stick with `run_without_tensor_float_32`, there should be a more descriptive description.\r\n\r\nI suspect as few tests still might fail which I had previously as \"no status\". I am confirming those.\r\n\r\n> > Some tests use matmul op to compute the reference value. For TF32 those ops are less precise causing some tests to fail.\r\n> \r\n> In cases where matmul is only used to compute reference values, it is preferable to change the test to use [`test_util.matmul_without_tf32`](https://github.com/tensorflow/tensorflow/blob/9edd7b971aacc3545f7f08c628c39f4761036f01/tensorflow/python/framework/test_util.py#L2087), so that TF32 is not disabled for the op(s) being tested. But given we are trying to get the tests working with CUDA 11.4 as soon as possible, I'm also fine with sticking with `test_util.run_without_tensor_float_32` for now.\r\n> \r\n> Agree with Sanjoy that, if we stick with `run_without_tensor_float_32`, there should be a more descriptive description.\r\n\r\n I agree, added a better description. Also had to refactor some tests to be more clear and also resolve some issues in the original commit due to some tests that had \"no status\" and hence hadn't raised a flag.", "> Can you please add a more descriptive description than `\"This test fails in TF32\"`?\r\n> \r\n> This is also 11.4 specific right? If yes, do you know why? Does 11.4 include different algorithms?\r\n\r\nI think it is related to matmul's reduced precision in TF32 devices. I tested the some failures with 11.3 and the issues were present. It is possible that the number of failures have increased due to CUDA's extended TF32 coverage.", "Closing this, since the PR is no longer relevant. The CUDA upgrade has been pushed out."]}]