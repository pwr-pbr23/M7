[{"number": 16018, "title": "Branch 181499300", "body": "", "comments": ["@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please.", "Why was this merged? This is breaking master:\r\nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/8012/console", "hey @av8ramit, sorry this is my fault -- should have checked more closely. when I last merged the `parallel_ops_interleave` test was timing out on Py3 so I thought this was the same problem.", "ah no worries @frankchn I believe it is fixed now."]}, {"number": 16017, "title": "Disable all failing tests to fix TF opensource tests.", "body": "PiperOrigin-RevId: 181212111", "comments": []}, {"number": 16016, "title": "Disabling the warnings test for python3.5 as well.", "body": "", "comments": []}, {"number": 16015, "title": "Modify `_parse_bazel_version` to return a tuple of ints", "body": "Bazel is updating its version to 0.10.0, and this will break the version check. Applying suggested fix in https://github.com/bazelbuild/bazel/issues/4425.", "comments": ["@tensorflow-jenkins test this please."]}, {"number": 16014, "title": "Error While Importing Tensorflow 1.5 RC0 with CUDA 9.1", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64-bit\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5 RC0\r\n- **Python version**: 3.6.3 (via Anaconda)\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.1/7.0.5\r\n- **GPU model and memory**: GTX 1050 4GB (notebook version)\r\n- **Exact command to reproduce**: import tensorflow (from a Python shell)\r\n\r\n### Describe the problem\r\nThe installation of the CUDA, cuDNN and Tensorflow went smoothly. When I import tensorflow I get the error pasted below. CUDA and CUDA_PATH_V9_1 were automatically set by the installer to `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1`.\r\n\r\nInstallation instructions used:\r\n\r\nCUDA: http://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#windows\r\n(steps 7 to 10 ignored, because it involved downloading nearly 6 GB for a test. Is it required? I haven't installed TF in Windows before.)\r\ncuDNN: http://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html\r\n(step 5 ignored as I didn't have a Visual Studio project - is there a VS project in the context of Tensorflow?)\r\nTensorflow: https://www.tensorflow.org/install/install_windows\r\n(`pip install --ignore-installed --upgrade tensorflow-gpu==1.5.0rc` inside a conda environment)\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n(tensorflow) C:\\Users\\Navneeth>python\r\nPython 3.6.3 |Anaconda, Inc.| (default, Nov  8 2017, 15:10:56) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Navneeth\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 75, in preload_check\r\n    ctypes.WinDLL(build_info.cudart_dll_name)\r\n  File \"C:\\Users\\Navneeth\\Miniconda3\\envs\\tensorflow\\lib\\ctypes\\__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Navneeth\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Navneeth\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Navneeth\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"C:\\Users\\Navneeth\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 82, in preload_check\r\n    % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\nImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit\r\n```", "comments": ["You likely have `cudart64_91.dll` instead of `cudart64_90.dll` (i.e CUDA 9.1 instead of CUDA 9.0). It seems as though tf doesn't yet have support for CUDA 9.1.", "Yes, that's true, I can see the dll for 9.1. 9.1 is the default download option at the CUDA website. Since the 1.5.0 rc page* didn't specify anything about minor versions, I assumed 9.1 was okay. \r\n\r\n`* https://github.com/tensorflow/tensorflow/releases/tag/v1.5.0-rc0`", "The `tf-nightly` build might might have support for 9.1, although I havent looked into that. Also be sure to close this issue!", "Could someone tell me when 9.1 is likely to be supported? And for future reference: where can I find the exact versions of the dependencies? Thanks.", "@navneethc @aalexsmithh @cy89  Hi could some one help me out , i have installed cuda 9.1 with cuddn 7.1 and all the dlls are present in the folder yet when i try to use tensorflow gpu it give me \"ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit\" but when i download the cuda 9.0 it again gives the same error saying cudart64_80.dll is not available . \r\n\r\nI have tired installing all the version of the tensorflow and cuda yet i get error i have checked the path variable to not sure why i am getting this error ", "@abhigoku10 Until the 01/26/2018 nighty built version, the tensorflow still does NOT support cuda 9.1, but only cuda 9.0. Just see another nighty built version 02/15/2018; not know whether that supports.\r\n\r\nIf it doesn't work, just use cuda 9.0, and you need to re-set your default cuda path to the 9.0 instead of the 9.1 in your environmental variable settings.", "> I have tired installing all the version of the tensorflow and cuda\r\n\r\n@abhigoku10 Make sure that you are using CUDA 9.0/cuDNN 7 only with TF version 1.5 or higher.", "@ybsave  for me one time it worked then, due to other reasons i re-installed it and now its not working \r\n\r\n@navneethc  i have run it with cuda 9.0 yet i was not successful i shall give one last time again ", "@navneethc @ybsave  i reinstalled with cuda 9.0 and cuddn 7 so currently its working now thanks a lot ", "The problem I got was the mismatch between the CUDA version and CUDNN version. I initially installed CUDA 9 and CUDNN v7.0.5 (for CUDA 9.1) which caused these problems. Once the cudnn version is changed (the  version for CUDA 9) it solved the problem. ", "I dont know what to do, I have tried all versions and everything found in two days on the internet. But this \r\n>>>ImportError: Could not find 'cudart64_90.dll'\r\nis not leaving me alone. \r\nI am trying to install Tensorflow-gpu\r\non windows 10 64\r\ntired: CUDA 9.0 , 9.1, 8.0\r\nTried cudNN: 7.2,7.1,7.0\r\nPlease someone help me out in it", "@hashamMunir It seems that you may not set your correct Windows environment path, leading to cannot find the dll. As discussed above, just use Cuda 9.0, cudnn 7, and make sure setting the \"PATH\" variable correctly in your system. Then it should work.", "@ybsave  Thank you for your reply. I think I did it I set the path in the system \r\n>CUDA_PATH_v9_0 : C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\r\n>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib\r\nis it ok, or I did it wrong??\r\nThanks again", "I guess the dll files are usually found in the \"PATH\", not \"CUDA_PATH_v9_0\". Whenever you cannot find a dll or an exe file during running, first search the file location, and then add the path into \"PATH\" and restart your console.\r\n"]}, {"number": 16013, "title": "Disabling the interleave_op_test for now.", "body": "", "comments": ["Looks fine to me, but does this mean that @frankchn's fix in https://github.com/tensorflow/tensorflow/commit/c17abb0aafab284f3a8470e57e3479db5008fb52 didn't solve the problem?", "@mrry I'm not sure it did. It's failing on master and his change seems to already be pulled in.\r\n\r\nhttps://source.cloud.google.com/results/invocations/b3c943f9-2262-4abb-bb78-293ccad7478f/targets/%2F%2Ftensorflow%2Fcontrib%2Fdata%2Fpython%2Fkernel_tests:interleave_dataset_op_test/tests"]}, {"number": 16012, "title": "Fix a bug in ResolveConstantConcat", "body": "Changes to fix a bug in ResolveConstantConcat whereby shared tensors are removed without checking if they are used in other operators in the graph.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!\n\nOn Wed, Jan 10, 2018 at 10:13 AM, googlebot <notifications@github.com>\nwrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address on your commit.\n>    Check your existing CLA data <https://cla.developers.google.com/clas>\n>    and verify that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If your company signed a CLA, they designated a Point of Contact who\n>    decides which employees are authorized to participate. You may need to\n>    contact the Point of Contact for your company and ask to be added to the\n>    group of authorized contributors. If you don't know who your Point of\n>    Contact is, direct the project maintainer to go/cla#troubleshoot. The email\n>    used to register you as an authorized contributor must be the email used\n>    for the Git commit.\n>    - In order to pass this check, please resolve this problem and have\n>    the pull request author add another comment and the bot will run again. If\n>    the bot doesn't comment, it means it doesn't think anything has changed.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16012#issuecomment-356687853>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADUcf3AbkmhSkFPD9SRnRlG_2aQvljf1ks5tJP3EgaJpZM4RZuwf>\n> .\n>\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "now?\n\nOn Wed, Jan 10, 2018 at 3:25 PM, googlebot <notifications@github.com> wrote:\n\n> We found a Contributor License Agreement for you (the sender of this pull\n> request), but were unable to find agreements for the commit author(s). If\n> you authored these, maybe you used a different email address in the git\n> commits than was used to sign the CLA (login here\n> <https://cla.developers.google.com/> to double check)? If these were\n> authored by someone else, then they will need to sign a CLA as well, and\n> confirm that they're okay with these being contributed to Google.\n> In order to pass this check, please resolve this problem and have the pull\n> request author add another comment and the bot will run again. If the bot\n> doesn't comment, it means it doesn't think anything has changed.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16012#issuecomment-356771346>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADUcf-XEQgRPw-089T9_uSz0lOA5nv50ks5tJUbtgaJpZM4RZuwf>\n> .\n>\n", "added irobot email to my account.", "CLAs look good, thanks!\n\n<!-- ok -->", "@mohamedadaly Please resolve conflicts.", "Conflicts resolved.", "@mohamedadaly I'm afraid a recent push introduced new conflict, please resolve again.", "@rmlarsen Resolved again.", "@mohamedadaly Thanks!", "@mohamedadaly Thanks for the contribution!", "Sure, you are welcome!\n\nOn Wed, Jan 24, 2018 at 5:26 PM, Rasmus Munk Larsen <\nnotifications@github.com> wrote:\n\n> Merged #16012 <https://github.com/tensorflow/tensorflow/pull/16012>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16012#event-1440746811>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADUcf9fxTTGSxqLT2s9qUJ3ScWmGX_z8ks5tN9g_gaJpZM4RZuwf>\n> .\n>\n"]}, {"number": 16011, "title": "Tensorboard issue with the official docker image - 1.5.0-rc0-gpu-py3", "body": "Hello everyone,\r\n\r\nI have the exact same issue as stated here: https://github.com/tensorflow/tensorflow/issues/14855\r\nAnd on the official tensorboard repository: https://github.com/tensorflow/tensorboard/issues/812\r\n\r\nThe fact that I am using the official Docker Image and I didn't build anything from scratch.\r\n\r\n`tensorflow/tensorflow   1.5.0-rc0-gpu-py3   9e770d59b136        6 days ago          2.85GB`\r\n\r\nWhat I get when I try to launch Tensorboard as usual:\r\n\r\n```\r\n# tensorboard --logdir=log_directory\r\n/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tensorboard\", line 7, in <module>\r\n    from tensorboard.main import run_main\r\nImportError: cannot import name 'run_main'\r\n```\r\n\r\n**Resolution Idea:**\r\nI noticed that by simply running the command: `pip install tensorboard` inside the container the problem is solved and I am able to normally launch Tensorboard.\r\n\r\nThanks a lot for the help,\r\n\r\nAll the best,\r\n\r\nJonathan\r\n", "comments": ["Please redirect your issue to https://github.com/tensorflow/tensorboard/issues. ", "@shivaniag this is not about tensorboard, just import tensorflow also cause same deprecated warning message from h5py\r\n\r\nand issubdtype is in\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/dtypes.py ", "This is not a deprecation issue. This is real issue ", "@DEKHTIARJonathan sorry I haven't read it carefully\r\nand this is the only issue the deprecation is referred", "I observe the same problem with plain Tensorflow, not using Docker or Tensorboard. Appears to be in h5py.", "@yazabaza this has nothing to do with the issue related here", "Not clear to me how it has nothing to do with the issue if I am observing exactly the same error.\r\n```\r\n/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n```\r\nIf you are saying it's an issue in h5py, I get that.", "The warning in h5py is unrelated with this issue.\n\nThe import error (and not the warning) is generated by TensorBoard, not h5py.\n\nSo I repeat, the deprecation warning in h5py IS NOT an issue, and it has nothing to do with the issue in TensorBoard. ", "@DEKHTIARJonathan I second, opened an new issue here: https://github.com/tensorflow/tensorboard/issues/877", "the official docker image - 1.5.0-rc1-gpu has the same problem.", "cc @jart @chihuahua ", "cc @chihuahua", "I get this error as well when I just do this from a tensorflow session and a python console:\r\n```\r\n$ python \r\nPython 3.5.3 (default, Sep 14 2017, 22:58:41) \r\n[GCC 6.3.0 20170406] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n/usr/lib/python3/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n>>> \r\n\r\n```\r\nI tried the workarounds from above, but I think it is coming from elsewhere in either python or TF.", "@daniellowell please read carefully the discussion, it's really anoying to repeat over and over the same thing: https://github.com/tensorflow/tensorflow/issues/16011#issuecomment-357527284", "@jart, could your team PTAL?", "A fix was pushed a few hours ago that works towards solving this issue:  https://github.com/tensorflow/tensorflow/commit/34a589ef3ac01b65bac3b2dfbd6a81bc10578ec3\r\n\r\n\r\nI'd like to close out this issue in favor of tensorflow/tensorboard#877. ", "Also note the workaround [reported](https://github.com/tensorflow/tensorboard/issues/877#issuecomment-358801103) by @MikalaiDrabovich:\r\n\r\n```sh\r\npython3 /usr/local/lib/python3.5/dist-packages/tensorboard/main.py --logdir=/tmp/\r\n```"]}, {"number": 16010, "title": "lib_package does not bundle MKL-DNN", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.8.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nI wan't to build the tensorflow C-API from source with MKL-DNN support in order to use it in another project. The easiest solution (if not the only convenient one) I found for building the C-API is using the lib_package tool:\r\n\r\n```bash\r\nbazel build --config=mkl -c opt //tensorflow/tools/lib_package:libtensorflow\r\n```\r\nThe build succeeds. However, the packaged library does not contain `libmklml_intel.so` and `libiomp5.so`. \r\n\r\n```bash\r\n$ ldd libtensorflow_framework.so\r\n\tlinux-vdso.so.1 =>  (0x00007ffec0f8a000)\r\n\tlibmklml_intel.so => not found\r\n\tlibiomp5.so => not found\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007feb07b2f000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007feb07826000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007feb07609000)\r\n\tlibstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007feb07287000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007feb07071000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007feb06ca7000)\r\n\r\n```\r\nIs there a way to fix the Bazel build such that it outputs all necessary libs?\r\n", "comments": ["@chrert do any of the flags/instructions at the Intel website help: https://software.intel.com/en-us/articles/build-and-install-tensorflow-on-intel-architecture\r\n\r\nIn particular, they use --copt=\"-DEIGEN_USE_VML\" for building a pip package. ", "@chrert : Note that the tarball built by the `//tensorflow/tools/lib_package:libtensorflow` bazel rule includes only files (two `.so`s and one `.h`) specific to TensorFlow. It does not package other binary dependencies (like `libc`, or CUDA libraries if building for GPUs etc.). So, in this case, it isn't packaging the MKL libraries either.\r\n\r\nSo you'd want the MKL libraries to be installed on the machine when you're unpacking the tarball.", "@cy89 Thanks for the resource, but I guess it's not directly related.\r\n\r\n@asimshankar I see your point, but I thought it would be consistent to include all libraries that were built during tensorflow compilation. After all, the build routine downloads and builds MKL-DNN by itself. Anyway, thanks for your clarification!"]}, {"number": 16009, "title": "bazel build ask for ANDROID_NDK_HOME, ANDROID_SDK_HOME -- no way to disable it", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n('v1.5.0-rc0-1-g793280a', '1.5.0-rc0')\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n```\r\nBuild label: 0.9.0\r\nBuild target: bazel-out/k8-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Dec 19 09:31:58 2017 (1513675918)\r\nBuild timestamp: 1513675918\r\nBuild timestamp as int: 1513675918\r\n```\r\n- **GCC/Compiler version (if compiling from source)**:\r\ng++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\ntoolkit_9.0 and cudnn 7.0.5_for_9.0\r\n- **GPU model and memory**:\r\ndifferent machines (irrelevant)\r\n- **Exact command to reproduce**:\r\nsee [this gist](https://gist.github.com/PatWie/aef90e72dbeaf2f79fbcaa031d74baad) which is mainly\r\n\r\n```bash\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_VERSION=\"$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \\(.*\\),.*/\\1/p')\"\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=6.1,5.2,3.5\r\nexport TF_NEED_HDFS=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_JEMALLOC=1\r\nexport TF_ENABLE_XLA=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_CUDA_CLANG=0\r\nexport TF_CUDNN_VERSION=7\r\nexport TF_NEED_MKL=0\r\nexport TF_DOWNLOAD_MKL=0\r\nexport TF_NEED_MPI=0\r\nexport TF_NEED_GDR=0\r\nexport TF_NEED_S3=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_NEED_COMPUTECPP=0\r\nexport GCC_HOST_COMPILER_PATH=$(which gcc)\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\n\r\n./configure\r\n\r\nbazel build --config=opt --copt=-mfpmath=both --copt=-msse4.2 --copt=-O3 --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=1 ....\r\n```\r\n### Describe the problem\r\nIn the past, using exactly this scripted worked. However, there are now a few issues:\r\nThe build uses `AVX2` even I haven't specified it as `--copt` (which worked in the past)\r\n\r\n### Source code / logs\r\ndepending on the machine it gives\r\n\r\n```\r\nPython 2.7.12 (default, Nov 20 2017, 18:23:56) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n2018-01-10 15:06:19.070740: F tensorflow/core/platform/cpu_feature_guard.cc:36] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine.\r\nzsh: abort      python\r\n```\r\nor\r\n```\r\nPython 2.7.12 (default, Nov 20 2017, 18:23:56) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nzsh: illegal hardware instruction  python\r\n```\r\n\r\nOn machines with AVX2 everything is fine. Further, there is no way to skip to setup ANDROID_NDK_HOME, ANDROID_SDK_HOME (I manually uncommented this in `configure.py`).\r\n\r\n*edit*\r\nI am willing to provide a pull-request for `configure.py`, adding something like `TF_NEED_ANDROID`.", "comments": ["@gunan could you see if this is something we would want to have. ", "This is because you built with `--config=opt`,  it will try to build with whatever your machine arch seems to return. So I recommend trying with omitting `--config=opt`. Please note that `-c opt` and `--config=opt` are different.\r\n\r\nFor skipping android settings, cc @angersson \r\n", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "You should be able to get around the prompt for SDK/NDK by also setting `TF_SET_ANDROID_WORKSPACE=0`.", "Closing since @gunan and @angersson have answered the questions."]}, {"number": 16008, "title": "Java/JNI , Object Detection: Not big Difference with GPU or CPU? (Insignificant difference) ~300ms with and without GPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: \r\n\r\n**binary** by instructions https://www.tensorflow.org/versions/master/install/install_java\r\n> \r\n> Install on Linux\r\n> \r\n> Take the following steps to install TensorFlow for Java on Linux or macOS:\r\n> \r\n> 1 Download libtensorflow.jar, which is the TensorFlow Java Archive (JAR).\r\n> 2 Decide whether you will run TensorFlow for Java on CPU(s) only or with the help of GPU(s). To help you decide, read the section entitled \"Determine which TensorFlow to install\" in one of the following guides:\r\n>  - Installing TensorFlow on Linux\r\n>\r\n> 3 Download and extract the appropriate Java Native Interface (JNI) file for your operating system and processor support by running the following shell commands:\r\n> \r\n>  TF_TYPE=\"gpu\"\r\n>  OS=$(uname -s | tr '[:upper:]' '[:lower:]')\r\n>  mkdir -p ./jni\r\n>  curl -L \\\r\n>    \"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-${TF_TYPE}-${OS}-x86_64-1.4.0.tar.gz\" |\r\n>    tar -xz -C ./jni\r\n\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: n/a, not used here (Java instead)\r\n- **Bazel version (if compiling from source)**: n/a, not used here\r\n- **GCC/Compiler version (if compiling from source)**: n/a, not used here\r\n- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61, cuDNN 6\r\n- **GPU model and memory**: GeForce 940MX\r\n\r\n### Source code / logs\r\n```\r\nChecking to see if TensorFlow native methods are already loaded\r\nTensorFlow native methods not found, attempting to load via tensorflow_inference\r\nSuccessfully loaded TensorFlow native methods (RunStats error may be ignored)\r\n2018-01-10 15:51:41.115224: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-01-10 15:51:41.254497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-01-10 15:51:41.255183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.2415\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 1,96GiB freeMemory: 1,51GiB\r\n2018-01-10 15:51:41.255217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nModel load took 313ms, TensorFlow version: 1.4.0\r\n```\r\n", "comments": ["actually it seems it uses GPU (allocated all available memory)i, but I didn't notice any speed-up, it works I guess the as on CPU\r\n\r\n`nvidia-smi:`\r\n```\r\n------------------------------------------------+\r\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce 940MX       Off  | 0000:01:00.0     Off |                  N/A |\r\n| N/A   55C    P0    N/A /  N/A |   1913MiB /  2002MiB |     82%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1002    G   /usr/lib/xorg/Xorg                             306MiB |\r\n|    0      1400    G   compiz                                          61MiB |\r\n|    0      2195    G   ...el-token=642304F43EA12C293DB722E17D520289   103MiB |\r\n|    0      5257  C+G   ...lib/jvm/java-1.8.0-openjdk-amd64/bin/java  1440MiB |\r\n+-----------------------------------------------------------------------------+\r\n```", "I'm running Object Detection https://github.com/tensorflow/models/tree/master/research/object_detection\r\n\r\non **ssd_inception_v2_coco** model  https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\r\n\r\nAnd it's insignificant difference when using GPU or only CPU\r\n\r\nInput Image size for feed is 416x416 and `inferenceInterface.run(outputNames, logStats);` takes about ~300 ms with GPU or without GPU acceleration ", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nIf you post to StackOverflow, you might want to share more information on how you're measuring time. A couple of things to think about:\r\n\r\n- If you're measuring the time for the very first call to `inferenceInterface.run` in your process, then that includes some overhead of GPU initialization by the TensorFlow runtime. So, you might want to discard that.\r\n- The exact numbers will depend on the specifications of the GPU. From a cursory look, the GeForce 940MX [can do 950GFLOPS](https://www.techpowerup.com/gpudb/2797/geforce-940mx), while the TitanX that was used to report the numbers in the [Object Detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) can do about 7x that ([~6700GFLOPS](https://www.techpowerup.com/gpudb/2632/geforce-gtx-titan-x)). So, you could expect almost 7x more time than reported there.\r\n\r\nLong story short, it is possible that with the GPU you're using, you won't see much of a speedup.\r\nAlso, would be good to make sure that your measurement isn't biased by the first run.\r\n\r\nFor example, I tried the following (showing just the relevant snippet) on a 416x416 image:\r\n\r\n```java\r\ntry (Tensor<UInt8> input = makeImageTensor(filename)) {\r\n  for (int i = 0; i < 10; ++i) {\r\n    final long start = System.currentTimeMillis();\r\n    List<Tensor<?>> outputs =\r\n            .session\r\n            .runner()\r\n            .feed(\"image_tensor\", input)\r\n            .fetch(\"detection_scores\")\r\n            .fetch(\"detection_classes\")\r\n            .fetch(\"detection_boxes\")\r\n            .run();\r\n    final long end = System.currentTimeMillis();\r\n    for (Tensor<?> t : outputs) {\r\n      t.close();\r\n    }\r\n    System.out.printf(\"%3d - %d ms\\n\", i, (end - start));\r\n  }\r\n}  \r\n```\r\n\r\nWhich is only measuring the time it takes to execute the graph, not any time spent in creating the input or other things. With that I see about 1/2 the time taken with GPU than CPU (50ms vs. 100ms). I'm using an Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz CPU and an NVIDIA TitanX GPU.\r\n\r\nHope that helps.", "@asimshankar  Thanks for the answer. I'll later upload sample java project on my github page which is used for tensorflow object detection with video (I ported some Android Sample Tensorflow Code https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java to work it on Ubuntu/Windows Java PC). I'll ask question on Stackoverflow with link for a github sample and write a link for stackoverflow question here\r\n\r\nI process every frame from a video, so there are many calls to run and only this call I measure:\r\n```\r\nlong startTime = System.currentTimeMillis();\r\n// call to run method\r\nlong stopTime = System.currentTimeMillis()  - startTime;\r\n```\r\n\r\nI just would like to know if **Tensorflow Object Detection** is faster than **Darknet Yolo 2 Object Detection**\r\nI see that Tensorflow is very fast with CPU than Darknet\r\nbut Darknet with GPU is much better on the same Video Card (it's much faster than ~300ms, also with 416x416 input frames)\r\n\r\nAlso I didn't take attention at Tensorflow model accuracy (mAP) for the first time. And now when I look at the table, I'm not sure..\r\n\r\n![screenshot_2](https://user-images.githubusercontent.com/8851301/34837321-1acd62b8-f703-11e7-9e0d-4b80d95074d9.png)\r\n\r\nSo its accuracy is much worst than Darknet Yolo detection's? \r\n\r\n![screenshot_3](https://user-images.githubusercontent.com/8851301/34837407-61343c0e-f703-11e7-910e-e4029a553107.png)\r\n\r\n\r\nSo I'm not sure if I should try replace Darknet Yolo 2 with Tensorflow Object Detection in my projects", "Sorry, those questions are beyond the scope of my expertise - or really beyond the scope of a TensorFlow bug/feature request :)\r\n\r\nThough, aren't you comparing different models? TensorFlow Object Detection is using mobilenet or inceptionv2 or something, while you're comparing against a different Yolo model?\r\n\r\n@jch1 @tombstone @derekjchow @jesu9 @dreamdragon may have some comments on the mAP numbers and model characteristics. But again, this would be beyond the scope of a TensorFlow bug/feature request."]}, {"number": 16007, "title": "Fix inline if/else statement in CMAKE_CACHE_ARGS", "body": "An if/else statement was given inline as an argument to CMAKE_CACHE_ARGS for some CMake external projects as discussed in #15209. This resulted in the following init cache entries on some systems:\r\n\r\n```\r\nset(CMAKE_POSITION_INDEPENDENT_CODE \"ON;if(;tensorflow_ENABLE_POSITION_INDEPENDENT_CODE;);else;(;)\"CACHE BOOL \"Initial cache\" FORCE)\r\nset(CMAKE_POSITION_INDEPENDENT_CODE \"OFF;endif;(;)\" CACHEBOOL \"Initial cache\" FORCE)\r\n```\r\nThis commit changes the inline if/else arguments to -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=${tensorflow_ENABLE_POSITION_INDEPENDENT_CODE} which is functionality equivalent.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "I will try this thanks @kaspermarstal ", "Tested, thanks @kaspermarstal  you are right, worked for me !", "Test failures are unrelated. Merging PR. Thanks @kaspermarstal "]}, {"number": 16006, "title": "Add property to get cell wrapped by DropoutWrapper ", "body": "Adding wrapped cell property as discussed in #15810.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Approved for tf-api-owners.", "@michaelkhan3 please update the goldens:\r\n\r\n```\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```\r\n\r\nNote that you need to run that under python2.", "@michaelkhan3 Please update the golden files as described in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/tests/README.txt", "Is that right?", "@michaelkhan3 thanks for the contribution!"]}, {"number": 16005, "title": "Verbs w 0 copies", "body": "## Verbs implementation to use direct tensor writes (0 copies)\r\n\r\n### Motivation:\r\n\r\nFollowing HKUST research on the use of GPU direct, and their [GDR implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/README.md), we wish to adopt the 0 copies approach and apply it to the current verbs implementation, while keeping the current implementation advantages, such as configurability and the use of RDMA for control messages.\r\n\r\n### Performance:\r\n\r\nCompared with the current GRPC, verbs and GDR implementation, the result implementation gave the best performance for every model, with any number of nodes. For VGG16 on 8 nodes with 4 P100 GPUs each, the prototype beat the second place by over 15%.\r\n\r\n### Implementation requirements:\r\n\r\n1. Tensor writes need to be done directly from the source Tensor to the destination Tensor, with no memory copies in between. This should be done for all DMAble tensors which are located either on CPU or on a RDMA compatible GPU device (GPU direct). \r\n2. Non DMAble tensors (CanMemCopy == false) will be serialized to proto on the sender side, RDMA written to a registered buffer on the receiver side, and then deserialized by the receiver.\r\n3. Tensors which are located on a non-RDMA-compatible GPU, will be RDMA written to a registered CPU proxy buffer on the receiver side, and then copied to GPU by the receiver.\r\n\r\n### Implementation constrains:\r\n\r\nFor best stability and proof of correctness, we will divide the implementation to two stages:\r\n1. At first stage we will keep changes to the current implementation to the minimum possible. The expense will be that we may have unused or unnecessary code leftovers, which may also affect performance. \r\n2. At second stage, we will re-iterate over the code and remove irrelevant code parts.\r\nThe design of the solution aims that we will achieve both stages with relative ease. \r\n\r\n### Design guidelines:\r\n\r\n1. Since we do not want to do any unnecessary memory copying, we will no longer allocate a fixed CPU buffer as the destination for the RDMA write. Instead we will do the writing directly to the result tensor, or if the result tensor is on a device which does not support RDMA, we will do the writing to a proxy CPU tensor and then copy its content to the result tensor.\r\n2. The address of the destination Tensor needs to be sent to the sender side for writing, meaning that the result/proxy tensor should be pre-allocated on the receiver side, prior to sending the tensor request. In order to do that, we need to know its meta-data, i.e. shape and data-type for DMAble tensors, and proto-size for serialized tensors. Unfortunately, this information is only available on the sender side which complicates manners. In order to avoid sending extra messages for querying the meta-data on each step, we store a local meta-data cache per tensor. Based on the assumption that the meta-data of a tensor rarely changes between steps, we expect that on most times the cache will only be updated once. When the sender receives a request for a tensor, if it is the first time this tensor is requested, or in the rare case that the meta-data did change, the sender will first send a meta-data response, on which the receiver will update the local cache, and reallocate the result/proxy tensors if required. When the receiver sends the tensor request, it will contain also the meta-data currently stored in its local cache, so the sender can compare it to see if there was a change.\r\n3. When the sender writes the tensor content to the result tensor, no additional data is being written with it. That means we need to reside on ibverbs immediate (uint32_t) to indicate which request we are responding to (in order to trigger the receive callback). The easiest and most elegant way is to key the recv callback with a unique request_index (uint32_t), instead of the current key_with_step_id (string). \r\n4. Since the sender no longer writes the tensor from/to fixed buffers, we no longer need to schedule the writes using the local/remote status. In addition we no longer rely on the RmdaTensorBuffer members as the source/destination addresses and rkey/lkey. Instead, each RdmaTensorBuffer will hold multiple \"Response\" objects (one per step-id), from which we derive destination address and rkey. The source address and lkey are always the ones of the source Tensor.\r\n5. With the addition of tensor pre-allocation, we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes. After implementing a common method for tensor pre-allocation, it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending, meta-data response callback and content response callback, all in a single \"Request\" class. The request class holds all the relevant request information, which reduces excessive parameter passing and lambda capturing. This decision is purely for elegance and code simplicity, and we decided to implement it in first stage because it makes the implementation much easier.\r\n6. At phase 2, we adopt that approach for the sender side as well, by encapsulate all the send and resend logic in the \"Response\" class (and remove the RdmaTensorBuffer class completely). This should make our design easier to understand, and also hold common notions with the rest of the distributed implementations.\r\n\r\n### New types/classes:\r\n\r\n* **enum RdmaImmDataType** - Immediate types to distinguish between different RDMA writes on the remote side. Ack writes and control-message writes have a fixed immediate value. The rest of the writes are tensor writes and the immediate value is the relevant request index.\r\n* **enum  RdmaWriteIDType**    - Types to distinguish between different RDMA write-complete events: Ack, control message, tensor DMA write and tensor proto write.\r\n* **class RdmaWriteID**        - Context for RDMA write complete events. Holds the RdmaWriteIDType and additional data.\r\n* **class RemoteAddressContext** - Remote address information (address + mr). Will be passed as write context for tensor proto writes.\r\n* **class RdmaTensorMetaData** - Meta-data for a tensor (type, shape, is_dead, proto_size).\r\n* **class RdmaMemoryMgr**      - Manages the meta-data cache, and the registered memory regions.\r\n* **class RdmaTensorRequest**    - Holds and manages information for a single tensor request throughout the entire receive cycle. API:\r\n\t* **Start()**                - Start the request sequence.\r\n\t\t* Allocate the result tensor (and proxy tensor if required).\r\n\t\t* Send RDMA_MESSAGE_TENSOR_REQUEST to the remote side.\r\n\t* **RecvTensorMetaData()**   - Receive meta-data from the remote side.\r\n\t\t* Update the local meta-data cache.\r\n\t\t* Reallocate the result tensor (and proxy tensor if required).\r\n\t\t* Re-send the request to the remote side.\r\n\t* **RecvTensorContent()**    - Receive tensor content from the remote side (RDMA write was completed).\r\n\t\t* Decode proto if required and/or move to GPU if the content was not written to it directly (GPU direct is not avaliable).\r\n\t\t* Invoke the done callback.\r\n* **class RdmaTensorResponse**   - Holds and manages information for a single tensor response throughout the entire send cycle. API:\r\n\t* **Start()**                - Start the response sequence. \r\n\t\t* Find the tensor in the local tag-match table.\r\n\t\t* Compare the tensor's meta-data to the meta-data in the message (taken from the requester's local cache). \r\n\t\t\t* If meta-data changed:\r\n\t\t\t\t* Clone the tensor to be sent later.\r\n\t\t\t\t* Send a meta-data update message and wait for re-request.\r\n\t\t\t* Else:\r\n\t\t\t\t* Send the tensor's content (using direct RDMA write).\r\n\t* **Resume()**               - Resume the response sequence after a re-request. Send the tensor's content that was cloned earlier.\r\n\t* **Destroy()**              - Destroy the response's resources and remove it form the pending list.\r\n\r\n### Protocol changes:\r\n\r\nThe protocol messages themselves will remain mostly unchanged at the first stage, but will be used differently, as described below. The current messages structures already have most of the required fields for the new implementation. The only change is the \"buffer_size\" field which is no longer used since we are no longer sending additional information with the tensor, and thus it is now always equal to the \"tensor_bytes\" field. Instead, we use that field to pass the \"request_index\".\r\n\r\n### Message structure:\r\n\r\n| type | name_size | name | step_id | request_index | remote_addr | rkey | is_dead | data_type | tensor_shape | tensor_bytes |\r\n|------|---------- |------|---------|---------------|-------------|------|---------|-----------|--------------|--------------|\r\n|  1B  |    2B     | 512  |  8B     |      8B       |         8B  |   4B |      1B |     XB    |    XB        |    8B        |\r\n\r\n* **RDMA_MESSAGE_TENSOR_REQUEST**  - (receiver ==> sender) The original tensor request. \r\n\t* type - The message type.\r\n\t* name (name_size) - Name of the requested tensor.\r\n\t* step_id - Step ID.\r\n\t* request_index - Request index.\r\n\t* remote_addr/rkey - Address/rkey of the result/proxy tensor. Irrelevant for first-time request.\r\n\t* is_dead/data_type/tensor_shape/tensor_bytes - The current meta-data as stored in the receiver local cache. The sender will use that information to know if the receiver's cache requires updating.\r\n* **RDMA_MESSAGE_BUFFER_REQUEST**  - (sender ==> receiver) The meta-data update message in case meta-data had changed (or if it is the first time the tensor is requested).\r\n\t* type - The message type.\r\n\t* request_index - Request index.\r\n\t* is_dead/data_type/tensor_shape/tensor_bytes - The up-to-date meta-data.\r\n\r\n  **Note:** At phase 2 this message is renamed to - **RDMA_MESSAGE_META_DATA_UPDATE**.\r\n* **RDMA_MESSAGE_BUFFER_RESPONSE** - (receiver ==> sender) Tensor re-requset after meta-data update and reallocation of result/proxy tensors.\r\n\t* type - The message type.\r\n\t* name (name_size) - Name of the requested tensor.\r\n\t* step_id - Step ID.\r\n\t* request_index - Request index.\r\n\t* remote_addr/rkey - Address/rkey of the reallocated result/proxy tensor.\r\n\t* is_dead/data_type/tensor_shape/tensor_bytes - The new meta-data. Will be removed in the next phase.\r\n\r\n  **Note:** At phase 2 this message is renamed to - **RDMA_MESSAGE_TENSOR_RE_REQUEST**.\r\n* **RDMA_MESSAGE_TENSOR_WRITE**    - (sender ==> receiver) No longer sent. There is only a direct write of the tensor content to the result/proxy tensor. Request index passed as the immediate value of the write.\r\n* **RDMA_MESSAGE_TENSOR_IDLE**     - (receiver ==> sender) No longer sent.\r\n\r\n### Phase 1:\r\n![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.jpg \"Phase 1 transport protocol\")\r\n\r\n### Phase 2:\r\n![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies.png \"Phase 2 transport protocol\")\r\n\r\n### Second stage optimizations:\r\n1. Remove unused code leftovers - Done.\r\n2. Remove the ACK buffer completely, since we can rely completely on its immediate value - Done.\r\n\r\n### Future optimizations:\r\n1. Map the tensor names to indexes, to significantly reduce the request message size.\r\n2. Understand the purpose of empty tensors and if we can skip remote fetching for them.\r\n3. Consider concatenating multiple requests and/or using multiple message buffers.\r\n4. Consider a no-request architecture.\r\n  \r\n ", "comments": ["Can one of the admins verify this patch?", "@yanivbl6 @shamoya @byronyi @poxvoculi @junshi15  @poxvoculi \r\nMoved PR https://github.com/tensorflow/tensorflow/pull/15927 to master. ", "Great job to remove the extra copy! \r\n\r\nI try to understand how the tensor buffers are created on the receiver for the first time. As the receiver does not know the tensor size before hand, it needs to get the info from the sender, hence the use of RDMA_MESSAGE_BUFFER_REQUEST and RDMA_MESSAGE_BUFFER_RESPONSE. You mentioned those messages in the note above, but I do not see them [here](https://github.com/Mellanox/tensorflow/blob/f15b6492deda4f15a7c1eb47895b4adbc87bd1e6/tensorflow/contrib/verbs/rdma.cc). Can you clarify how receiver gets the meta-data and how the buffer are registered and how the MR info are exchanged before the sender does RDMA-WRITE?\r\n\r\nAlso some of the comments can be cleaned up, for example, [this segment](https://github.com/Mellanox/tensorflow/blob/f15b6492deda4f15a7c1eb47895b4adbc87bd1e6/tensorflow/contrib/verbs/rdma.cc#L665-L671) is outdated.", "@junshi15 @shamoya  Hi. You are right, the messages have been renamed in phase 2 of the implementation. https://github.com/tensorflow/tensorflow/pull/16005/commits/d931cf3c66138c0f2af1a92a0baddbf55a155107. They are now called **RDMA_META_DATA_UPDATE** and **RDMA_TENSOR_RE_REQUEST**. I should probably update the PR notes. The README is already up-to-date with the full implementation. Sorry for the confusion.\r\n\r\nAlso good note about the comments. I will update them.", "The patch looks good to me.", "@eladweiss \r\n\r\nI tested it to find this error log\uff1a\r\n\r\ntensorflow/contrib/verbs/rdma_mgr.cc:304] Check failed: visitable_allocator is not visitable for instrumentationmklcpu\r\n\r\nLooks like it can not co-exist with MKL", "Thanks for reporting this, we will take a look.", "@sunnygao1980 @yanivbl6 Thanks. Indeed, the current solution does not work correctly with MKL. We have created a fix, but we still need to fully test it before creating a PR. In the meantime, it can be taken from here: https://github.com/Mellanox/tensorflow/commit/86414eccac9da3ae70e6b2296e76e5674f4a9a27", "@eladweiss  Thanks for your job.\r\nWhy not change MklCPUAllocator to VisitableAllocator \uff1f", "@sunnygao1980 @byronyi @yanivbl6 Indeed, it would be the best solution. However it will require us to extend outside of the contrib zone. @byronyi  do you think it is the best approach?", "See relavant discussion here: https://github.com/tensorflow/tensorflow/pull/11392#issuecomment-320142984, https://github.com/tensorflow/tensorflow/pull/11392#issuecomment-320273255, and https://github.com/tensorflow/tensorflow/pull/11392#issuecomment-320324981.", "@byronyi @yanivbl6 @shamoya Thanks. This is super relevant. \r\n1. I think the comment https://github.com/tensorflow/tensorflow/pull/11392#issuecomment-320273255 is wrong. The MklCpuAllocator is not visitiable. At least not on latest version.\r\n2. @poxvoculi mentioned here https://github.com/tensorflow/tensorflow/pull/11392#issuecomment-320324981 that there might be a discussion about making MklCpuAllocator and DefaultCpuAllocator visitiable. Any updates about that?", "I've started testing an internal change to convert MklCpuAllocator to a VisitableAllocator.  I don't foresee any problem with making this change, but it's possible there's a difficulty somewhere.  I'm less confident that DefaultCPUAllocator can be changed that way, since it may need to be a very simple type for use in some contexts, e.g. mobile devices.   @eladweiss : would your needs be satisfied by changing just MklCpuAllocator?", "@poxvoculi Yes, that would be great. \r\n\r\nWe'll still need to override the CPU registration for DefaultCPUAllocator, like Bairen mentioned [here](https://github.com/tensorflow/tensorflow/pull/11392#issuecomment-320142984), but other than being a little hacky, I don't see it posing a real problem."]}, {"number": 16004, "title": "cudnn", "body": "INFO:tensorflow:Starting Queues.\r\n2018-01-10 18:45:29.789178: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-01-10 18:45:29.843774: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\nINFO:tensorflow:global_step/sec: 0\r\n2018-01-10 18:45:33.552377: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-01-10 18:45:33.587139: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-01-10 18:45:33.621270: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-01-10 18:45:33.660148: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-01-10 18:45:34.072946: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-01-10 18:45:34.244066: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-01-10 18:45:37.678240: E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2018-01-10 18:45:37.678276: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2018-01-10 18:45:37.678313: F tensorflow/core/kernels/conv_ops.cc:667] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "hi have same problem ..\r\n2018-03-20 18:03:26.184957: E tensorflow/stream_executor/cuda/cuda_dnn.cc:371] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2018-03-20 18:03:26.185004: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n\r\nhow can i solve this problem\r\n", "Hello,\r\nI have the same error:\r\nMy Python crushes when it gets to the first convolutional layer, I tried making the model simpler, still crushes. \r\n\r\nThis is the output and error: \r\n\r\nI have the latest cuDNN version (7.1.2) On  tensorflow website, it seems cudnn 7 works with tensorflow-gpu 1.7..\r\n\r\nTensorFlow 1.7 may be the last time we support cuDNN versions below 6.0.\r\nStarting with TensorFlow 1.8 release, 6.0 will be the minimum supported\r\nversion. [here](url): https://github.com/tensorflow/tensorflow/releases\r\n\r\nMy purpose is to use cudnnLSTM instead of regular  LSTM (layer 2), because regular LSTM works so slow (it uses 10% of GPU or less)\r\n\r\nThe code below works fine if I use tensorflow-gpu verison 1.1 and LSTM layer instead of  using cuDNNLSTM layer. cuDNNLSTM needs tensorflow-gpu not less than 1.2, so I updated my tensorflow-gpu to higher version (1.7.0) , but now, I have the following error. Any idea what is the issue?\r\n\r\n\r\n_________________________________________________________________\r\nLayer (type)                 [Output] Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 100, 20, 20, 1)    0         \r\n_________________________________________________________________\r\ntime_distributed_1 (TimeDist (None, 100, 16, 16, 20)   520       \r\n_________________________________________________________________\r\ntime_distributed_2 (TimeDist (None, 100, 5120)         0         \r\n_________________________________________________________________\r\ntime_distributed_3 (TimeDist (None, 100, 1)            5121      \r\n=================================================================\r\nTotal params: 5,641\r\nTrainable params: 5,641\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nTrain on 30960 samples, validate on 7740 samples\r\nEpoch 1/50\r\n2018-04-10 10:40:15.085958: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-04-10 10:40:15.616355: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.64GiB\r\n2018-04-10 10:40:15.616355: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0 \r\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N \r\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6411 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2018-04-10 10:40:19.469530: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:396] Loaded runtime CuDNN library: 7101 (compatibility version 7100) but source was compiled with 7003 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2018-04-10 10:40:19.469530: F T:\\src\\github\\tensorflow\\tensorflow\\core\\kernels\\conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\n\r\n----------------------------------------------------------\r\nI have the following settings\r\nkeras                     2.1.5                    py36_0\r\ntensorflow                1.7.0                    py36_0    aaronzs\r\ntensorflow-gpu            1.7.0                    py36_0    aaronzs\r\nWindows 7\r\nCuda version 9.0\r\ncudnn version 7 (cudnn64_7)\r\nGPU NVIDIA GeForce GTX 1080 8 GB\r\n\r\nMy code:\r\n    # Functional model definition\r\n    inputs = Input(shape=(seqLength, 20, 20, 1))  # layer[0]\r\n    output_of_layer1 = TimeDistributed(Conv2D(20, (5, 5), activation=\"relu\"))(inputs)  # layer[1]\r\n    output_of_layer2 = TimeDistributed(Bidirectional(CuDNNLSTM(100, activation=\"sigmoid\", return_sequences='true', unit_forget_bias = True)))(output_of_layer1 )# layer[2]\r\n    output_of_layer3 = TimeDistributed(Flatten())(output_of_layer2)  # layer[3]\r\n    output_of_layer4 = TimeDistributed(Dense(1, activation=\"sigmoid\"))(output_of_layer3)  # layer[4]\r\n    model = Model(inputs=inputs, outputs=output_of_layer8)\r\n\r\n    model.summary()\r\n    nepochs = 50\r\n\r\n    sgd = optimizers.SGD(lr=0.01, decay=0, momentum=0.95, nesterov=False, clipvalue=20)\r\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n    checkpoint = ModelCheckpoint('trained_plaqueDetection_model_bidirectional.h5', monitor='val_acc', verbose=1, save_best_only=False, mode='max')\r\n    callbacks_list = [checkpoint]\r\n    history = model.fit(training_dataset[0], training_dataset[1], epochs = nepochs, batch_size=10, shuffle=True, verbose = 1, validation_split = 0.2, callbacks = callbacks_list)\r\nplot_losses(history);\r\n\r\nThanks!", "I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n"]}, {"number": 16003, "title": "Adding meta_graph_be.pb testdata for big endian for framework_meta_graph_test", "body": "", "comments": ["Can one of the admins verify this patch?", "@drpngx, Could you please help with this?", "How big is the metagraph? Is there a doc on how to generate it?", "@drpngx, I have created the `metrics_export_meta_graph_be.pb` from the original [metrics_export_meta_graph.pb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/testdata/metrics_export_meta_graph.pb) itself. There is only 1 change in [tensor_content](https://github.com/linux-on-ibm-z/tensorflow/blob/67bc8fc36f74ea57e0477e84e99ec024c716fce4/tensorflow/python/framework/testdata/metrics_export_meta_graph_be.pb#L1161) where data is swapped as below for int32 dtype(with size 2):\r\n```\r\noriginal: tensor_content: \"\\000\\000\\000\\000\\001\\000\\000\\000\"\r\nBE:       tensor_content: \"\\000\\000\\000\\000\\000\\000\\000\\001\"\r\n```\r\nWith this change, the test `//tensorflow/python:framework_meta_graph_test` passes on big endian, which earlier was failing with error: \r\n`ValueError: Invalid reduction dimension 16777216 for input with 2 dimensions. for 'mean/Sum' (op: 'Sum') with input shapes: [1,2], [2] and with computed input tensors: input[1] = <0 16777216>.\r\n`\r\n\r\nSince the original file contains 1559 lines, added a new file for big endian and referenced it in test. Is there some other way to handle this failure?", "I see. I think the \"right\" way is to standardize the metagraph to be small endian and do the conversion on load. We have the dtypes for that. Do you want to submit a PR for that?", "@drpngx, Thank you for your feedback. I will have a look at how to carry out conversion on load. It would be helpful if you could give me some pointers about the location where the endian conversion might be needed to achieve uniformity while reading test data.", "You can trace the load through [here](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.cc#L220), probably.", "Indeed, the stored format should be platform independent.", "I think we should close this PR and open an issue for standardizing the saved format and/or converting to big-endian on read when needed."]}, {"number": 16002, "title": "fix a_ to allocator_", "body": "```bash\r\n[07:05:58]\t[Step 1/1] In file included from tensorflow/core/common_runtime/threadpool_device.cc:32:0:\r\n[07:05:58]\t[Step 1/1] ./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::ClearStats()':\r\n[07:05:58]\t[Step 1/1] ./tensorflow/core/common_runtime/mkl_cpu_allocator.h:120:32: error: 'a_' was not declared in this scope\r\n[07:05:58]\t[Step 1/1]    void ClearStats() override { a_->ClearStats(); }\r\n[07:05:58]\t[Step 1/1]                                 ^\r\n```\r\n\r\nPlease review this PR ASAP... @yuefengz ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Closing this one as it is a duplicate of pull 15975. "]}, {"number": 16001, "title": "fail to convert resnet_v1_50 to tflite", "body": "I downloaded resnet_v1_50 model from https://github.com/tensorflow/models/tree/master/research/slim\r\n\r\nwhen I tried to convert this model to tflite, I got below error:\r\n2018-01-10 14:54:19.491608: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Mean, Squeeze.\r\n\r\nI use below command to convert:\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=./resnet_v1_50_frozen.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n  --output_file=./resnet_v1_50.tflite --inference_type=FLOAT \\\r\n  --input_type=FLOAT --input_arrays=input \\\r\n  --output_arrays=resnet_v1_50/predictions/Reshape_1  --input_shapes=1,224,224,3\r\n\r\n\r\nDoes it means that  tflite do not support op Mean and Squeeze  ?\r\n\r\n  ", "comments": ["maybe op cannot support , we can chat by qq 1208091722 ,or zyx_Cambridge@163.com", "I had a similar error today too:\r\n\r\n    2018-01-10 13:57:31.820886: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Dequantize, QuantizeV2, QuantizedBiasAdd, QuantizedConv2D, QuantizedMatMul, QuantizedMaxPool, QuantizedRelu, QuantizedReshape, RequantizationRange, Requantize, TensorFlowMax, TensorFlowMin.\r\n\r\nAlthough its with my own [custom alexnet model](https://gist.github.com/OluwoleOyetoke/30f2cac788042c495f1ae34a6b742a1d)", "@OluwoleOyetoke see https://github.com/tensorflow/tensorflow/issues/15871#issuecomment-356419505\r\n\r\n@huanyingjun Mean and Squeese are only supported if they can be completely removed during the conversion process (usually because they can be folded into constant nodes). We are working on better supporting other models, including resnet."]}, {"number": 16000, "title": "TensorFlow Installation Error on SLES 11 SP Linux ", "body": "I have been trying to install tensorflow (1.3.0/1.4/1.4.1) on SLES 11 Linux, I was getting GLIBC_2.14 not found an exception, currently, we have the **GLIBC_2.11.3 version in SLES 11 SP3T**. \r\n\r\nPlease help me to install any of **tensorflow > 0.8** versions on SLES 11 Linux and let me know the tensorflow compatible version for SLES 11. OR SLES 11 SP3 Linux is not compatible with TensorFlow any of the versions.\r\n\r\nThanks in Advance.\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks for your quick response.\r\n**Kindly find the below details.**\r\nHave I written custom code - **Haven't written any custom code.**\r\nOS Platform and Distribution - **SUSE Linux 11 SP3 and CDH 5.11.0 (Teradata Appliances)** \r\nTensorFlow installed from: **pip install tensorflow-1.4.0-cp36-cp36m-manylinux1_x86_64.whl**\r\nTensorFlow version: **Tried 1.3.0, 1.4.0 and 1.4.1**\r\nBazel version: Haven't Installed.\r\nCUDA/cuDNN version: Haven't installed.\r\nGPU model and memory: NA\r\nExact command to reproduce: **Attached error screenshot for more reference.**\r\n![tensorflow import error](https://user-images.githubusercontent.com/7345716/34870949-b1be6f14-f7c6-11e7-8338-a471fb665275.JPG)\r\n\r\nKindly help me to import tensorflow successfully.\r\nThanks.\r\n", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "So, the `GLIBC` error means that your libc version is too old. At this point I can see only three options:\r\n* Upgrade libc on your system\r\n* Rebuild tensorflow\r\n* Run tensorflow within a docker container", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15999, "title": "[Feature Request] Request for weighted sampling in tf.data.Dataset ", "body": "Are there any methods that we can sample with weights without pre-weighting the dataset as the input in Dataset API?", "comments": ["The only sampling-specific method in the API is [`tf.contrib.data.rejection_resample()`](https://www.tensorflow.org/api_docs/python/tf/contrib/data/rejection_resample). If that doesn't work for your application, can you specify what sampling operation you'd like? Like `rejection_resample()` it may be possible to build it out of existing parts, like `Dataset.filter()` and `tf.contrib.data.scan()`.", "I think explicit methods for giving weights corresponding to the label would be highly useful. e.g. Dataset.resample(label, weight) ", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "The `tf.contrib.data.rejection_resample()` transformation allows you to specify a mapping from `Dataset` elements to an integer class, and a vector of probabilities representing the target distribution (indexed by class) for the resampling. Can you be more specific about what a `Dataset.resample()` method would do in addition to this?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Is it possible to sample rows from our dataset using a custom distribution like [tf.distributions.Exponential](https://www.tensorflow.org/api_docs/python/tf/distributions/Exponential)?", "Hello all,\r\nI'm also interested if it's possible to batch elements from Dataset with distribution, so to pick last elements more often than elements in the begin of dataset.\r\n\r\nI'm talking about numeric dataset made from tensor_slices with initial shape 1000*10*3\r\n\r\nIn this case depending on distribution probability we'll take elements with index [999,:,:] more often than elements with index [1,:,:].", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Is there a workaround to create a weighted sampler?"]}, {"number": 15998, "title": "tensorflow input/output tensor reshape c++", "body": "currently , I am working on loading and testing a tensorflow model on android using c++, and the trained model is a full convolutional model, so the input need to be dynamically reshaped according to input image size.\r\n\r\nI can make this done easily using python. but when turn to c++ , I can hardly find much examples and experience on this.\r\n\r\nthe trained model is converted to *.pb file , and the input and output tensor shape has been specified before conversion in python. and now I want to reshape the input and output in c++ before using the model.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have not figured it out, but instead I find a way around.  \r\nthe problem lies on the code how to generate the .pb file .   since , here my requirement is to make the input size flexible, I need to add two more input layer for width and height. after generation of .pb file, the model will support variable input sizes"]}, {"number": 15996, "title": "CMake building fail on Linux Centos 7 ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**:  2.7.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: 8/6\r\n- **GPU model and memory**: K80\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nAfter successful config by CMake, the make failed.\r\n\r\n### Source code / logs\r\n[  1%] Performing build step for 'zlib'\r\nScanning dependencies of target zlib\r\n[  2%] Building C object CMakeFiles/zlib.dir/adler32.o\r\n[  5%] Building C object CMakeFiles/zlib.dir/compress.o\r\n[  7%] Building C object CMakeFiles/zlib.dir/crc32.o\r\n[ 10%] Building C object CMakeFiles/zlib.dir/deflate.o\r\n[ 12%] Building C object CMakeFiles/zlib.dir/gzclose.o\r\n[ 15%] Building C object CMakeFiles/zlib.dir/gzlib.o\r\n[ 17%] Building C object CMakeFiles/zlib.dir/gzread.o\r\n[ 20%] Building C object CMakeFiles/zlib.dir/gzwrite.o\r\n[ 22%] Building C object CMakeFiles/zlib.dir/inflate.o\r\n[ 25%] Building C object CMakeFiles/zlib.dir/infback.o\r\n[ 27%] Building C object CMakeFiles/zlib.dir/inftrees.o\r\n[ 30%] Building C object CMakeFiles/zlib.dir/inffast.o\r\n[ 32%] Building C object CMakeFiles/zlib.dir/trees.o\r\n[ 35%] Building C object CMakeFiles/zlib.dir/uncompr.o\r\n[ 37%] Building C object CMakeFiles/zlib.dir/zutil.o\r\n[ 40%] Linking C shared library libz.so\r\n/usr/bin/ld: CMakeFiles/zlib.dir/compress.o: relocation R_X86_64_32 against `.rodata.str1.1' can not be used when making a shared object; recompile with -fPIC\r\nCMakeFiles/zlib.dir/compress.o: could not read symbols: Bad value\r\ncollect2: error: ld returned 1 exit status\r\nmake[5]: *** [libz.so.1.2.8] Error 1\r\nmake[4]: *** [CMakeFiles/zlib.dir/all] Error 2\r\nmake[3]: *** [all] Error 2\r\nmake[2]: *** [zlib/src/zlib-stamp/zlib-build] Error 2\r\nmake[1]: *** [CMakeFiles/zlib.dir/all] Error 2\r\nmake: *** [all] Error 2\r\n\r\n\r\nDo I need revise some code of CMakeLists.txt?\r\n\r\n", "comments": ["Is this related to #15209? I saw this exact error message. I am preparing a PR.", "@kaspermarstal Thanks for replying, I solve the problem what I submitted, simply add -fPIC flag on the corresponding dependency folder's CMakeCache.txt should be OK, beside zlib, png has the same problem.\r\n\r\nso now the question is how to modify the CMakeLists to avoid the error.", "Updated my previous comment with the correct issue number (#15209). PR is here: #16007. Don't know about the second issue though :)", "@kaspermarstal Thanks, I think your PR is related to my previous problem, I will check the CMakeLists file. My second issue is due to the contrib/nccl/ project has two wrong include path, already fixed.\r\nNow struggle with the final link step... let's make the cmake build more friendly for linux.", "If close grpc OFF, the build can not pass. noted", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 125 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 15995, "title": "/home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/source_repr.py failed (Exit 1). ", "body": "\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:clone from git\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:0.5.4\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:9.0/7.0\r\n- **GPU model and memory**:GTX1070ti  8GB\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nbuild error.when I finished ./configure and run bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package, met an error.\r\n\r\n### Source code / logsExtracting Bazel installation...\r\n..............\r\nWARNING: /home/hp/Downloads/tensorflow/tensorflow/core/BUILD:1825:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/hp/Downloads/tensorflow/tensorflow/tensorflow.bzl:1152:30.\r\nWARNING: /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/grpc/WORKSPACE:1: Workspace name in /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/grpc/WORKSPACE (@com_github_grpc_grpc) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions.\r\nWARNING: /home/hp/Downloads/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/hp/Downloads/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Found 1 target...\r\nERROR: /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/source_repr.py failed (Exit 1).\r\n\r\n  ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15994, "title": "Feature request: (documentation) operation complexity / performance chart", "body": "* Have I written custom code: NA\r\n* OS Platform and Distribution: Any\r\n* TensorFlow installed from: NA\r\n* TensorFlow version: NA\r\n* Bazel version: NA\r\n* CUDA/cuDNN version: NA\r\n* GPU model and memory: NA\r\n* Exact command to reproduce: NA\r\n\r\nIt would be interesting to have a complexity/performance chart for different operations. For example, to know that `tf.reshape` is computationally cheaper than `tf.transpose`. \r\n\r\nI did see the [Performance Guide](https://www.tensorflow.org/performance/performance_guide), but that's not what I mean.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@tensorflowbutler yes", "@erickrf for that case, basically, tensorflow just manipulates contiguous blobs of data, so reshape just needs to change the metadata, whereas transpose needs to move actual data around, and possibly make a copy depending on the graph.\r\n\r\nCC @MarkDaoust for documentation.\r\nCC @tfboyd for perf.", "This isn't something we can automate, and there are a lot ops.\r\n\r\nIn many cases these are standard operations and the information is easy to find.\r\n\r\nBut If you have any specific suggestions feel free to send PRs improving the docs.\r\n", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 15993, "title": "Fix typos", "body": "This PR fixes some typos: `refered`, `ouptuts`, `from from`, `suport`, `whithin`, `posibility`, and `then then`.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15992, "title": "android demo ,How to switch the horizontal screen?", "body": "tensorflow 1.4\r\nTF-OD-API model\r\n\r\n```\r\n<activity android:name=\"org.tensorflow.demo.DetectorActivity\"\r\n                  android:screenOrientation=\"landscape\"\r\n                  android:label=\"@string/activity_name_detection\">\r\n            <intent-filter>\r\n                <action android:name=\"android.intent.action.MAIN\" />\r\n                <category android:name=\"android.intent.category.LAUNCHER\" />\r\n            </intent-filter>\r\n </activity>\r\n```\r\n\r\nI have some problems now, and I want to ask how to do the rotation.\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 15991, "title": "Hide MSVC workaround from Clang on Windows", "body": "#15990", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15990, "title": "[Tracking bug] Building Tensorflow with Clang on Windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: Python 3.6\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: VS 2017 15.5\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\nThis is tracking bug to use Clang on Windows to build Tensorflow.\r\n\r\nBenefits:\r\n- Faster build (Clang does not suffer from `__forceinline` issue in pure MSVC #10521).\r\n- Mostly compatible with MSVC (`clang-cl` understands MSVC command flags, macros and even imitates some of the MSVC's bugs).\r\n- Cross compilation.\r\n- Unlock some runtime optimizations such as [crc32c acceleration](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/hash/crc32c_accelerate.cc) that relies on `__builtin_cpu_supports` (compiler-rt is required).\r\n- Better x64 code (according to one Chromium engineer).", "comments": ["Marking as \"awaiting response\" because of the [tracking bug] in the title.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@cy89 I am using this bug to track all PRs for Clang on Windows support. Perhaps `stat:community support` label is more suitable for this issue?", "Thanks; done.", "I can see all the above linked PRs on Clang on windows are merged.\r\nClosing this issue. Thanks!"]}, {"number": 15989, "title": "Fix freeze_graph command line argument error.", "body": "Fix TypeError: main() missing 1 required positional argument: 'unused_args' when using freeze_graph command line tool (pip console script entry point)", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Dear \n\n \n\nI appreciate your guide. \n\nI have already signed a CLA. So, I have changed my primary e-mail address from this to yesmung@gmail.com <mailto:yesmung@gmail.com>  \n\nDo I have any more to do?\n\n \n\nThank you\n\nBest regards\n\n \n\nFrom: googlebot [mailto:notifications@github.com] \nSent: Wednesday, January 10, 2018 9:13 AM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Fix freeze_graph command line argument error. (#15989)\n\n \n\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n\ud83d\udcdd Please visit https://cla.developers.google.com/ to sign.\n\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\n\n  _____  \n\n*\tIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data <https://cla.developers.google.com/clas>  and verify that your email is set on your git commits <https://help.github.com/articles/setting-your-email-in-git/> .\n*\tIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n*\tIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/pull/15989#issuecomment-356456399> , or mute the thread <https://github.com/notifications/unsubscribe-auth/AA1t2XVnl78-Z7Hi0NF0Q3Xw5OCxp17Uks5tJACegaJpZM4RYoOR> .  <https://github.com/notifications/beacon/AA1t2X7NNkbte5hgbnxkp1v7RoHHbWX5ks5tJACegaJpZM4RYoOR.gif> \n\n", "I don't fully understand the fix.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/28c3c5dd38e3b397c2cf0acdaa6388dcbf0349f7/tensorflow/python/platform/app.py#L124\r\n\r\nThis should be calling main() with some form of argv.\r\n\r\nI believe all of TF scripts are written this way, so I don't think it makes sense to modify this one in isolation.  Can you provide more information for what's going wrong? Maybe some sample command line and code?\r\n", "@vrv \r\nThe code works fine when using `python -m tensorflow.python.tools.freeze_graph`, but with the \r\n new command line tool added in r1.5, it will return `TypeError: main() missing 1 required positional argument: 'unused_args'`"]}, {"number": 15988, "title": "Add internal release notes that were previously missing.", "body": "I wasn't sure about some of the Important/Other changes, so please double-check\nthat I haven't missed anything actually critical.", "comments": ["I fixed @av8ramit's suggestions and did some additional cleanup."]}]