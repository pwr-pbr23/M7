[{"number": 23273, "title": "Unable to import tensorflow", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.11\r\n- Python version: python 2.7.12\r\n- Installed using virtualenv? pip? conda?: inside of a virtualenv installed using pip\r\n- Bazel version (if compiling from source): not sure\r\n- GCC/Compiler version (if compiling from source): not sure\r\n- CUDA/cuDNN version: not sure\r\n- GPU model and memory: not sure\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have installed tensorflow (CPU only) for python 2.7 from source. However, when I try to import tensorflow I come across the following error\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /home/john/python_2_7_12/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: PyUnicodeUCS4_FromString\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\npython -c \"import tensorflow as tf; print(tf.__version__)\"\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["TensorFlow is tested and supported on Ubuntu 16.04 or later. However there are users who have installed it on [ubuntu 14.04](https://taufiqhabib.wordpress.com/2016/11/29/installing-tensorflow-on-ubuntu-16-04/).\r\nWere you able to install TF outside of virtual environment? Trying to understand if creation of virtual env is causing the problem.\r\nAlso what is your pip version?", "Hello,\n\nI fixed this problem by installing python2.14 with unicode 4 instead of\nunicode 2.\n\nThanks,\nJohn\n\nOn Thu, Nov 1, 2018 at 2:39 PM ymodak <notifications@github.com> wrote:\n\n> TensorFlow is tested and supported on Ubuntu 16.04 or later. However there\n> are users who have installed it on ubuntu 14.04\n> <https://taufiqhabib.wordpress.com/2016/11/29/installing-tensorflow-on-ubuntu-16-04/>\n> .\n> Were you able to install TF outside of virtual environment? Trying to\n> understand if creation of virtual env is causing the problem.\n> Also what is your pip version?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23273#issuecomment-435138689>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AqEAOS_hiev9N-ZPFD2QBuTUQUKpmrJaks5uqz_RgaJpZM4X7VeB>\n> .\n>\n", "Thanks for posting your solution. Closing this issue since you have found the solution."]}, {"number": 23272, "title": "Seg Fault when using tf.data.contrib.map_and_batch.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes. I am using a modified ResNet50 model architecture.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04. Specifically I am using the AWS DL AMI 16.0 Ubunutu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9/7 I believe\r\n- GPU model and memory: Nvidia Tesla K80 (AWS p2.8xlarge)\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nSegmentation Fault\r\n\r\n**Describe the expected behavior**\r\nNot a segmentation fault.\r\n\r\n**Code to reproduce the issue**\r\nI do not have a minimal repro case which is unfortunate. My model is built using the TF Estimator API and I use the MirroredStrategy for utilizing multiple GPUs. I don't have any custom lower level code (C++).\r\n\r\nI have been able to trigger this behavior in 2 ways.\r\n\r\n1. Using all the GPUs on the system. It will always occur if I use all 8 GPUs.\r\n2. Using train_and_evaluate as opposed to train. This seems to work if I use N-2 GPUs.\r\n\r\nI am happy to run with any additional configurations or rebuild TF to provide more specific information to help debug this problem.\r\n\r\n**Other info / logs**\r\n```\r\nINFO:tensorflow:loss = 1147.4103, step = 0\r\n\r\nThread 73 \"python3\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7ffeebfff700 (LWP 68942)]\r\n0x00007fff7f601bf3 in std::_Function_handler<void (long, long), Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n(gdb) bt\r\n#0  0x00007fff7f601bf3 in std::_Function_handler<void (long, long), Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#1  0x00007fff7e87f019 in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fff7e87efef in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fff7e87efef in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fff7c32693a in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fff7c325a02 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#6  0x00007fff9c62ec5c in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1520532893746/work/.build/src/gcc-7.2.0/libstdc++-v3/src/c++11/thread.cc:110\r\n#7  0x00007ffff7bc16ba in start_thread (arg=0x7ffeebfff700) at pthread_create.c:333\r\n#8  0x00007ffff78f741d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n```", "comments": ["Are you able to execute your code successfully if you run it on a single gpu?", "Yes. It works fine on a single GPU. I am yet to see it crash. I have a run with a single GPU going now to see if it ever does.", "@sseveran did you mean this works when you use train_and_evaluate with 6 GPUs, but not 8GPUs. And if you use train instead of train_and_evaluate, how many GPUs does it take for this to happen?\r\n\r\nIn general, I don't see why distribution strategy usage itself will lead to a seg fault. From the stack trace, it seems hard to tell of any connection either. Is it possible to find a way to reproduce this? ", "Ok. So I did a number of runs over the weekend. I did see it finally happen on a single GPU so clearly the issue is not in MirroredStrategy. I did 2 CPU runs and it did not segfault but I hardly think that can be taken to be conclusive.\r\n\r\nI am going to whittle down the model and see if I can cause it to reproduce on limited set of data and a toy model. Is there any mechanism for privately sharing code/data?", "I have run this down I believe. The issue is somewhere in TensorFlow.contrib.data.map_and_batch. I have run a variety of variations of my training input function to my custom estimator.\r\nThis version crashes:\r\n```python   \r\ndef training_data_input_fn_foo(self):\r\n        files = tf.data.Dataset.list_files(os.path.join(self.parameters.get('examples_path'), \"*.tfrecord.gz\"))\r\n        dataset = files.apply(tf.contrib.data.parallel_interleave(lambda x:\r\n                    tf.data.TFRecordDataset(x, compression_type='GZIP'), cycle_length=4))\r\n        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(10000, None))\r\n        dataset = dataset.apply(tf.contrib.data.map_and_batch(self.parse_example, batch_size=self.parameters.batch_size,\r\n                                                              num_parallel_calls=self.parameters.num_gpus * 4))\r\n        dataset = dataset.prefetch(self.parameters.batch_size * 4 * 100)\r\n        return dataset\r\n```\r\nThis version does not crash:\r\n\r\n```python \r\n\r\n def training_data_input_fn_safe(self):\r\n        files = tf.data.Dataset.list_files(os.path.join(self.parameters.get('examples_path'), \"*.tfrecord.gz\"))\r\n        dataset = files.apply(tf.contrib.data.parallel_interleave(lambda x:\r\n                    tf.data.TFRecordDataset(x, compression_type='GZIP'), cycle_length=4))\r\n        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(10000, None))\r\n        dataset = dataset.map(self.parse_example, num_parallel_calls=self.parameters.num_gpus * 4)\r\n        dataset = dataset.batch(self.parameters.batch_size)\r\n        dataset = dataset.prefetch(self.parameters.batch_size * 4 * 100)\r\n        return dataset\r\n```\r\n\r\nMy parsing function I call in map is quite simple:\r\n```python\r\n    @property\r\n    def features(self):\r\n        return {\r\n            'data': tf.FixedLenFeature([300, 15, 64], tf.float32),\r\n            'label': tf.FixedLenFeature([1], tf.string)\r\n        }\r\n\r\n    def parse_example(self, x):\r\n        parsed = tf.parse_single_example(x, self.features)\r\n        return {'data': parsed['data']}, parsed['label']\r\n```\r\n\r\nI am happy to provide more specific information to whoever needs it if I can. I could now create an encapsulated reproduction.\r\n\r\nSteve", "Thanks for digging into this @sseveran ! I will assign to @jsimsa to look into the map_and_batch issue. ", "@sseveran could you please provide up-to-date stack trace for the single GPU crash? Is the crash with `map_and_batch()` deterministic? If not, how many runs of the `map_and_batch()` vs. the `map().batch()` pipeline did you do and how many of them succeeded vs crashed?", "The stack trace is identical:\r\n\r\n```#0  0x00007fff7f601bf3 in std::_Function_handler<void (long, long), Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#1  0x00007fff7e87f019 in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) ()\r\n   from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fff7c32693a in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fff7c325a02 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fff9c62ec5c in std::execute_native_thread_routine_compat (__p=<optimized out>)\r\n    at /opt/conda/conda-bld/compilers_linux-64_1520532893746/work/.build/src/gcc-7.2.0/libstdc++-v3/src/c++11/thread.cc:110\r\n#5  0x00007ffff7bc16ba in start_thread (arg=0x7fff0ffff700) at pthread_create.c:333\r\n#6  0x00007ffff78f741d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n```\r\n\r\nI would not call it deterministic. I have seen it take differing lengths of time to manifest itself. The `map().batch()` has not crashed yet. The `map_and_batch()` always seem the crash  but sometimes it can take quite a while.", "Do you know how what `intra_op_parallelism` are you using? Could you try setting `intra_op_parallelism_threads` to `1` using the `SessionConfig` as described [here](https://www.tensorflow.org/guide/performance/overview) and see if that fixes the problem?", "One possibility you are seeing the segfault is because your host machine is running out of memory (and TensorFlow code base fails to check whether a dynamic allocation succeeded). This is somewhat more likely to happen with `map_and_batch()` because it, unlike `map().batch()`, uses internal buffering.\r\n\r\nCan you report what the host memory usage is? You almost certainly do not want to prefetch `self.parameters.batch_size * 4 * 100` elements since elements at that point are batched. You should be prefetching `self.parameters.num_gpus` elements. I would try that and see whether the problem goes away.", "FYI, I submitted https://github.com/tensorflow/tensorflow/commit/b4cc45a2f66fad9f2dbf6db16b28741fffa7780b to explicitly check whether certain internal memory allocations succeed. If it is possible for you to build and run from source, give it a try to see whether the issue was indeed caused by OOM.", "Nagging Assignee @jsimsa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to inactivity.", "@sseveran \r\n\r\nFYI, I submitted https://github.com/tensorflow/tensorflow/commit/323f3eb4c2f15c713e992881ea8854069f2433f4 to fix an issue with map_and_batch and SparseTensors. It does not seem that your input pipeline uses sparse tensors, but my change could be a fix your issue as well."]}, {"number": 23271, "title": "Improve shape function of tf.sparse_reduce_sum", "body": "This fix tries to address the issue raised in #23114 where the shape function of tf.sparse_reduce_sum did not infer the shape even if the input shape were known. This fix improves the shape function.\r\n\r\nThis fix fixes #23114.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Looks like there are some build issues. Will close this one for now, and reopen once fixed.", "> Looks like there are some build issues. Will close this one for now, and reopen once fixed.\r\n\r\n@yongtang  Any update please ?", "Thanks @harshini-gadige, The PR has been reopened on https://github.com/tensorflow/tensorflow/pull/23414"]}, {"number": 23270, "title": "Update documentation regarding Dockerfiles", "body": "As per my note in #23170.", "comments": []}, {"number": 23269, "title": "Variable Length Sequence Support for CUDNN LSTM", "body": "The Cudnn LSTM is supposed to make the training much more efficient compared to the dynamic RNN. However I am using variable length sequences with the dynamic RNN using padded batches and passing the sequence length argument to the dynamic RNN to correctly denote the length of each sequence. But this kind of support is not available in the Cudnn LSTM. Or am I missing something here?", "comments": ["@HansikaPH You are right in your current approach to use variable length sequences. A [PR](https://github.com/tensorflow/tensorflow/pull/22308) has been submitted to provide support for Variable Length Sequence for CUDNN LSTM and is work in progress.", "@ymodak Thanks for the notice. Seems like this PR is still not merged to the codebase.", "@ymodak  @HansikaPH actually, the PR is done but I heard from @protoget that there is something coming down the line from Nvidia that would obviate that PR.\r\n\r\nYou manually build that PR and it works fine. Let me know if you have any issues. You need to be careful with sorting and lengths.\r\n\r\nAs to long-term, maybe talk to @protoget about when the other thing will pull through and when. See discussion on that thread. \r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/22308", "Seems like cuDNN now supports variable sequence lengths in a batch, see [[1]](https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnSetRNNDataDescriptor) and [[2]](https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNForwardTrainingEx).\r\n\r\nIs there any work being done to expose that functionality in the TensorFlow API?", "Looks like this was done in https://github.com/tensorflow/tensorflow/pull/23588 / https://github.com/tensorflow/tensorflow/commit/bf46fa4c5225e853e602b0e18951884b08729163 ?\r\n\r\nShould this issue be closed?\r\n\r\nFor those who run into this issue, see also: #24812 ", "@reuben Not sure myself. Don't think anyone compared packing vs masking, Masking is space-inefficient but it might not make a big difference. Packing and unpacking add additional operations but the RNN itself might be more efficient.\r\n\r\nHaven't tried out the masked LSTM yet. Assuming you get the same results in about the same time, then this can be closed.", "Seems that the change has been made to contrib/cudnn_rnn, but the it hasn't been port to core tf.keras API. I will work on that in this week.", "@qlzh727 I am getting a `tensorflow.python.framework.errors_impl.UnknownError: CUDNN_STATUS_EXECUTION_FAILED` error on using dropout in `contrib.cudnn_rnn.CudnnLSTM` in TF 2.0.0-alpha0.\r\nNot sure if we can raise new issues for errors related to contrib in TF 2 but it might be relevant if you are porting it to tf.keras API.", "Sorry for the very late reply. The sequence_length/masking should now by supported in keras.LSTM/GRU in 2.0 with change https://github.com/tensorflow/tensorflow/commit/ac04087f7fb9d535d33b800d6e2bfb82c7df7077 and https://github.com/tensorflow/tensorflow/commit/9eec7d3f0dae15792a58444b74873c3d6d96b037", "@qlzh727 Glad to see this is coming. I noticed that the doc for the keras LSTM class still says that the cudnn implementation can't be used if masking is used---I'm guessing you'll want to remove that?", "@rightaditya, the docstring was updated by https://github.com/tensorflow/tensorflow/commit/9eec7d3f0dae15792a58444b74873c3d6d96b037, I guess it just hasn't been pickup by the latest release yet. ", "@qlzh727 Hmm, I must be missing something, but it says \"Inputs are not masked or strictly right padded.\" My reading of that is that the inputs should not be masked to use the CuDNN version.", "The cudnn kernel only support seqence_length parameter, so that it can skip the padding data on the tail. If your data has padding at the beginning of the sequence (left padded), then cudnn kernel will not be able to work correctly in this situation. The comment here is trying to point out that specific use case.", "@qlzh727 Ah, I think I see how you wrote it. If I saw the doc out of context I would parse the English text to the condition `not (masking or strictly-right-padded)`, but you intended `(not masking) or strictly-right-padded`. Is that correct? So I guess what I'm saying is that the parse of the stated condition is ambiguous as written, but that might just be me. The scope of the \"not\" is ambiguous.\r\n\r\nThanks for replying so quickly :)", "Sorry to bother you guys, but how could I go about using this feature? From this thread, I was hoping I would be able to use a Masking layer with `keras.layers.CuDNNLSTM` like I am with `keras.layers.LSTM`, but I still get the \"Masking is not supported for CuDNN RNNs\" error (tested with stable, nightly and tf 2.0.0alpha). Would really appreciate if you could point me in the right direction.", "In 2.0 alpha, the normal keras.layer.LSTM will use cudnn kernel if possible, so you don't need to use keras.layers.CuDNNLSTM layer any more.", "Thanks! It seems part of my mistake was not using the _nightly_ 2.0 alpha version. I got it to work now :) ", "I'm new to this stuff, so i understood nothing. I have installed the 2.0 alpha version of tensorflow, and gpu version 2.0 alpha, but the keras LSTM layer does not use the cudnn kernel. What is this nightly version? Where can i find it? Do i need to install it? Am i supposed to do something in particular with my code?\r\n\r\nWhat do i need to do to be able to use masking in a cudnn lstm layer?", "@CrazyLeonida To the best of my knowledge:\r\n\r\nTensorflow has stable distributions and nightly distributions of the code. The nightly builds are, as the name implies, updated more often, but less stable. Version 2.0 alpha also exists in stable and nightly versions. At the present time, it seems the LSTM lacks variable-length CuDNN support for the non-nightly build. I installed the nightly version with `pip install tf-nightly-gpu-2.0-preview`. Following that, I was able to use `tf.keras.layers.LSTM` just like normal, and it automatically placed itself on the GPU. \r\n\r\nMasking propagates between layers, so if you add a Masking layer or otherwise enable masking, all subsequent layers inherit the masking. In my case, I use something like the following code:\r\n\r\n```\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, vocab_size, embedding_dim, units):\r\n        super(MyModel, self).__init__() \r\n        self.units = units\r\n        self.lstm = tf.keras.layers.LSTM(units)\r\n\r\n        # Here, mask_zero enables masking for the value zero. I pad my sequences with zeros,\r\n        # which are thus ignored (the LSTM state propagates without being augmented for these steps)\r\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\r\n       \r\n    def call(self, x, hidden):\r\n        x = self.embedding(x)\r\n        outputs = self.lstm(x, initial_state=hidden)\r\n        return ouputs\r\n```\r\n\r\nEquivalently, instead of `mask_zero`, you could use\r\n\r\n```\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, vocab_size, embedding_dim, units):\r\n        # ...\r\n        self.mask = tf.keras.layers.Masking(mask_value = 0)\r\n        \r\n    def call(self, x, hidden):\r\n        x = self.mask(x)\r\n        # ....\r\n```\r\n ", "That places on GPU but doesn't utilize CuDNN. Only tf.contrib.cudnn_rnn.CudnnLSTM currently supports CuDNN RNNs with variable lengths.\r\n\r\n> Masking is not supported for CuDNN RNNs\r\n\r\nNot the hardest thing to fix. Just need to convert a mask back into sequence lengths and pass to `cudnn_rnn` here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/cudnn_recurrent.py#L296", "@bstriner Are there any plans to expand the support of masked inputs or variable length sequences? Looking at your linked github, we can modify existing GRU code and build locally is this correct?", "Hi @qlzh727 thank you for your contribution, \r\nAbout mask ,i have three question about biLSTM model.\r\n\r\n>In 2.0 alpha, the normal keras.layer.LSTM will use cudnn kernel if possible, so you don't need to use keras.layers.CuDNNLSTM layer any more.\r\n\r\n(1)So in tf2 I can use lstm  don't need to use keras.layers.CuDNNLSTM layer,but why is the GPU running speed different tf2 LSTM slow then tf1 CuDNNLSTM?\r\n\r\n> The cudnn kernel only support seqence_length parameter, so that it can skip the padding data on the tail. If your data has padding at the beginning of the sequence (left padded), then cudnn kernel will not be able to work correctly in this situation. The comment here is trying to point out that specific use case.\r\n\r\n (2)So if I use  tf2  layer\u2192 tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM ....),input data already padding on the tail,then cudnn kernel will be able to work correctly in this situation? But how can i prove it about masked,about model runing loss not consider padded location  ? \r\n\r\n(3)If padding data then model output result I need to drop padding location  right? "]}, {"number": 23268, "title": "[XLA:GPU/CuDNN] Add support for 1x1 window reversal", "body": "CuDNN supports both convolution (CUDNN_CONVOLUTION) and cross correlation\r\n(CUDNN_CROSS_CORRELATION). However, only the latter was hooked up, causing\r\n\r\n    Tensorflow error: Status: Hit a case for convolution that is not implemented on GPU.\r\n\r\nfor convolutions of the first kind (corresponding to convolutions with both\r\nwindow dimensions reversed at the HLO level). Reversing the dimensions (i.e.\r\ndoing convolutions rather than cross correlations) is the default behavior\r\nfor the Flux.jl ML framework, so it's easy to hit this error trying to run\r\npre-existing Flux models through the Julia:XLA->XLA:GPU compilation path.\r\n\r\nPlumb through the reversal option to CuDNN to make this pattern work. The\r\nsame HLO already works fine against the CPU and TPU backends.", "comments": ["Updated.", "Nagging Assignee @ymodak: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ymodak anything we're waiting for here?", "Sorry Keno, I misunderstood our uplift processes and lost track of this when I should have been paying attention.  I'm trying to submit it now.", "No problem. Thanks!"]}, {"number": 23267, "title": "1.12.0-rc2 cherry-pick request: Fp16 LSTMBlocKCell and LSTMBlockFusedCell", "body": "PiperOrigin-RevId: 216632480", "comments": []}, {"number": 23266, "title": "1.12.0-rc2 cherry-pick request: Upgrade setuptools before installing absl-py.", "body": "PiperOrigin-RevId: 218471042", "comments": []}, {"number": 23265, "title": "No example exporting a model from an Estimator on TensorFlow.org", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.9\r\n- Doc Link: https://www.tensorflow.org/tutorials/estimators/cnn\r\n\r\n\r\n**Describe the documentation issue**\r\nIn this CNN tutorial example, there is no exporting savedmodel example. For users that need to save the trained model for future usage, it will be better to add exporting savedmodel part in the tutorial.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Hi @karmel  I don't think we have a single modern Estimator -> saved_model example.\r\n\r\nDo you know who the right person to ask is?", "There's a section in the SavedModel guide: https://www.tensorflow.org/guide/saved_model#savedmodels_from_estimators\r\n\r\nBut maybe we should have an example in the Estimator guide too.", "@ hanmeng31 \r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23264, "title": "1.12.0-rc2 cherry-pick request: Upgrade setuptools before installing absl-py in remaining scripts.", "body": "PiperOrigin-RevId: 218730741", "comments": []}, {"number": 23263, "title": " CUDA driver version is insufficient for CUDA runtime version", "body": "Python 3.6.6 | packaged by conda-forge | (default, Oct 12 2018, 14:08:43) \r\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n2018-10-25 12:10:00.651103: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-10-25 12:10:00.738944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-10-25 12:10:00.739553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.40GiB\r\n2018-10-25 12:10:00.739580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jaimeet/.conda/envs/py-dl/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1511, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/home/jaimeet/.conda/envs/py-dl/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 634, in __init__\r\n    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\r\ntensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version\r\n\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution :Ubuntu 18.04 \r\n- TensorFlow installed from (source or binary): library\r\n- TensorFlow version: 1.11.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Cuda compilation tools, release 9.0, V9.0.176. NVIDIA-SMI 390.87 \r\n- GPU model and memory:  GTX 1070 \r\n\r\n\r\n*\r\n\r\n\r\n", "comments": ["Is this still an issue? I think installing compatible nvidia driver for your cuda 9.0 should do the trick. Please refer this [compatibility chart](https://stackoverflow.com/questions/30820513/what-is-version-of-cuda-for-nvidia-304-125/30820690#30820690) to get appropriate drivers.", "I've got the same proble with Ubuntu 18.04+GeForce 1080Ti+nvidia-390+cuda-9.[012] and corresponding cuDNNs +tensorflow-1.11.0 .\r\nMy solution is to downgrade the tensorflow and tensorflow-gpu to 1.6.0 with cuda-9.0+CuDNN 7. It worked.\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23262, "title": "TPU support for C/C++ API", "body": "Hello,\r\n\r\nThe available Tensorflow versiosn seems to have only Python APIs for (cloud) TPU devices. Is it possible to write C/C++ programs to utilize TPU? Are there corresponding C or C++ APIs?\r\n\r\nThanks for any help!", "comments": ["It's been over a year at this point. Is this still being considered?", "Nope. Although it's a nice feature that we wanted, we turned to Python APIs at the end.", "Well that's sad. I guess the only option now is to rewrite a ton of stuff in the significantly slower python.", "@yuhc  , @danegraphics Are there any native libraries in c/c++ to program the google cloud tpus ? ", "> @yuhc , @danegraphics Are there any native libraries in c/c++ to program the google cloud tpus ?\r\n\r\nNot afaik. @Aditya-11 ", "oof still nope, wanted to use it with lc0 "]}, {"number": 23261, "title": "Newer TF post-install python test does not expose CUDA/cuDNN driver problems", "body": "**System information** Windows 10\r\n- TensorFlow version: tf-nightly-gpu (Oct. 25. 2018)\r\n- Doc Link:\r\nhttps://www.tensorflow.org/install/pip\r\n\r\n\r\n**Describe the documentation issue**\r\nThe suggested post-install test:\r\n\r\n`python -c \"import tensorflow as tf; print(tf.__version__)\"\r\n`\r\npasses even when there are unresolved GPU CUDA/cuDNN driver issues.\r\n\r\nThe test previously suggested exercises the CUDA/cuDNN GPU drivers and seems to be better one to suggest:\r\n\r\n`python -c \"import tensorflow as tf; hello = tf.constant('Hello, TF'); sess = tf.Session(); print(sess.run(hello))\"\r\n`\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nyes\r\n", "comments": ["Thanks, but I'd like to avoid the `Session` API.\r\n\r\n@alextp Will `tfe.num_gpus()` highlight a bad GPU config?", "tfe.num_gpus() won't but enable_eager_execution followed by a matmul (of floats) will."]}, {"number": 23260, "title": "Undefined reference error when trying to run tensorflow lite model on android", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\nyes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nubuntu 16.04\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nSamsung A3, armeabi-v7a\r\n\r\nI cloned from master (Oct 25), last commit is 670eff0b0f60b9fde8231e927807677a80016904\r\n\r\n**Describe the current behavior**\r\n\r\nWhen compiling the app to use the tflite model I get an error:\r\n```\r\nerror: undefined reference to 'tflite::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::Interpreter, std::__ndk1::default_delete<tflite::Interpreter> >*)'\r\n```\r\nI've tried\r\n```\r\nstd::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(model_path.c_str());\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n```\r\nAs per the [docs here](https://www.tensorflow.org/lite/apis) (note the difference is because they seem to be out of date). As it wasn't working, I came across [a slightly different way](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/minimal/minimal.cc#L53), with\r\n```\r\ntflite::InterpreterBuilder builder(*model.get(), resolver);\r\nbuilder(&interpreter);\r\n```\r\nbut that also did not work. These are the relevant headers I include\r\n```\r\n#include \"tensorflow/contrib/lite/model.h\"\r\n#include \"tensorflow/contrib/lite/interpreter.h\"\r\n#include \"tensorflow/contrib/lite/kernels/register.h\"\r\n```\r\n\r\nI followed [this great guide](https://stackoverflow.com/questions/49834875/problems-with-using-tensorflow-lite-c-api-in-android-studio-project) for using tensorflow lite for android. Only thing I did different was some cmake stuff (I don't actually include the flatbuffer files inside my android project, but added them through cmake).\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nTo compile.\r\n\r\n\r\n", "comments": ["Hi Andre, do you mind to take a look or reassign?", "Forgot to mention: I'm using ndk 17 as I have a dependency that requires it (oboe, also from Google!), when running configure there was a warning saying bazel (0.18, just released) couldn't work with NDK 17 and it would lead to weird compile errors. It seems to me that can't be right though as I've found issues from 6 months ago where it seemed like building with NDK 17 should be alright?", "I've also tried using ndk16b and ndk15c, as well as setting `ANDROID_NDK_API_LEVEL=21`. Nothing worked.", "I don't think this is related to the NDK. From the error message it looks like you are not linking the TF Lite library into your binary. It could be your android studio setup, or your cmake setup.", "It was related to the ndk. It worked with NDK 18. You can find a reproducible example in my issue [here](https://github.com/tensorflow/tensorflow/issues/23629).", "Thanks @Nimitz14 ", "@Nimitz14 It.s work, Thanks!!", "@Nimitz14 Thanks for the solution.\r\nBut I meet the same problem using NDK18. I build tflite from the TensorFlow 1.12 source code. \r\nIs there any solution?", "> It was related to the ndk. It worked with NDK 18. You can find a reproducible example in my issue [here](https://github.com/tensorflow/tensorflow/issues/23629).\r\n\r\nUpdate to NDK 18 works for me. Thanks!"]}, {"number": 23259, "title": "1.12.0-rc2 cherry-pick request: Explicitly quote every command piece.", "body": "PiperOrigin-RevId: 218399942", "comments": []}, {"number": 23258, "title": "1.12.0-rc2 cherry-pick request: Don't set TF_PER_DEVICE_MEMORY_LIMIT_MB as a --test_env\u2026", "body": "\u2026if it isn't specified.\r\n\r\nPiperOrigin-RevId: 218634344", "comments": []}, {"number": 23257, "title": "Golang Tensorflow API cannot be build any more", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow version: 1.10.1, 1.11.0, 1.12.0-rc1\r\n- Only tested CPU version\r\n\r\n\r\n**Describe the problem**\r\nI tried to build my golang project which uses the tensorflow Go API. The CI build in Gitlab failed without a change (also old builds, which finished successfully before, cannot be build any more). \r\n\r\nI run the following commands:\r\n```\r\ncurl --silent -L \"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.12.0-rc1.tar.gz\" | tar -C /usr/local -xz\r\nldconfig\r\ngo get github.com/tensorflow/tensorflow/tensorflow/go\r\n```\r\n\r\nThen I receive the following error:\r\n\r\n```\r\n# github.com/tensorflow/tensorflow/tensorflow/go\r\n/tmp/go-build813894416/b001/_x003.o: In function `_cgo_691d94bb61b7_Cfunc_TF_ImportGraphDefOptionsSetDefaultDevice':\r\n/tmp/go-build/cgo-gcc-prolog:198: undefined reference to `TF_ImportGraphDefOptionsSetDefaultDevice'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\nThis did not happend before with the exact same commands. It also happens if I use other versions of the C library. Therefore I assume that the tensorflow Go API is broken in the current master. ", "comments": ["Just to give more information on this\r\n\r\nI'm having the same issue building tensorflow GO API. I first tried it on my macOS, to receive the following error \r\n\r\n```\u2718  \ue0b0 go get -v github.com/tensorflow/tensorflow/tensorflow/go\r\n\r\ngithub.com/tensorflow/tensorflow/tensorflow/go\r\n# github.com/tensorflow/tensorflow/tensorflow/go\r\nUndefined symbols for architecture x86_64:\r\n  \"_TF_ImportGraphDefOptionsSetDefaultDevice\", referenced from:\r\n      __cgo_9c811c796559_Cfunc_TF_ImportGraphDefOptionsSetDefaultDevice in _x003.o\r\n     (maybe you meant: __cgo_9c811c796559_Cfunc_TF_ImportGraphDefOptionsSetDefaultDevice)\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation) \r\n```\r\n\r\nThen, in order to isolate from issues arising due to linking issues on my personal dev env, I ran the above in a docker container and got the error \r\n\r\n``` root@1f822f8566dc:/notebooks#  go get -v github.com/tensorflow/tensorflow/tensorflow/go\r\ngithub.com/tensorflow/tensorflow (download)\r\ngithub.com/tensorflow/tensorflow/tensorflow/go\r\n# github.com/tensorflow/tensorflow/tensorflow/go\r\n/tmp/go-build042240265/b001/_x003.o: In function `_cgo_691d94bb61b7_Cfunc_TF_ImportGraphDefOptionsSetDefaultDevice':\r\n/tmp/go-build/cgo-gcc-prolog:198: undefined reference to `TF_ImportGraphDefOptionsSetDefaultDevice'\r\ncollect2: error: ld returned 1 exit status\r\n ```\r\n\r\nSame issue, AFAICT. \r\n", "Same problem here. I can not download the go package from the github. :( \r\n\r\n`package github.com/tensorflow/tensorflow/tensorflow/go: exit status 128\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/op: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/op\" in any of:`\r\n", "@andreas-eberle @Bearbite : This is because the code in the Go library for the \"master\" branch depends on the C library (`libtensorflow.so`) built from the master branch.  Since the C library being used is from the 1.10/1.11/1.12 release, you'd have to checkout the corresponding branch.\r\n\r\nThis is similar to: https://github.com/tensorflow/tensorflow/issues/14546#issuecomment-347433966, where you'd want to do something like:\r\n\r\n```sh\r\ncd $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/go\r\ngit checkout r1.11\r\n```\r\n\r\nI'll keep this issue open because this seems like something that we should at least document better in the installation instructions.", "@asimshankar: This definitely has to be documented better. When I follow the official installation instructions, I cannot build the tensorflow Go API. Maybe consider changing the installation instructions to use a dependecy management like Go Dep. With that you can easily specify the version you'd like to build.", "I also interested to see correct set of instructions. So far everything was \"trivial\" we call go get and get the build. Now it seems the orchestration should be done with git, versions, branches.", "To be clear, yes we want things to be easy.\r\nMy comments abive were meant as an explanation and a suggested workaround.\r\n\r\nI will have to look into the appropriate fix", "@asimshankar is it beneficial to mention this issue on this page for the mean time until the complete fix\r\nhttps://www.tensorflow.org/install/lang_go \r\nI was recently installing and found this issue page to be really useful", "Nagging Assignee @asimshankar: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "have the same problem installing go\r\n>go test github.com/tensorflow/tensorflow/tensorflow/go\r\ngithub.com/tensorflow/tensorflow/tensorflow/go\r\n/usr/bin/ld: $WORK/b048/_x003.o: in function `_cgo_9c811c796559_Cfunc_TF_ImportGraphDefOptionsSetDefaultDevice':\r\n/tmp/go-build/cgo-gcc-prolog:204: undefined reference to `TF_ImportGraphDefOptionsSetDefaultDevice'\r\ncollect2: error: ld returned 1 exit status\r\nFAIL\tgithub.com/tensorflow/tensorflow/tensorflow/go [build failed]\r\n", "anybody test the older version?  maybe better use https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.3.0.tar.gz ?", "test with old version:\r\n\r\n#include <stdio.h>\r\n#include <tensorflow/c/c_api.h>\r\n\r\nint main() {\r\n  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\r\n  return 0;\r\n}\r\n\r\n\r\nalso return error:  \r\n\r\nsudo gcc test.c  -o hello\r\n\r\n/usr/bin/ld: /tmp/ccJT12Vj.o: in function `main':\r\ntest.c:(.text+0xa): undefined reference to `TF_Version'\r\ncollect2: error: ld returned 1 exit status\r\n", "g++ -I/usr/local/include -L/usr/local/lib  test.c  -ltensorflow\r\n\r\nwork fine  :+1: ", "https://github.com/tensorflow/tensorflow/issues/35133\r\nnot exactly the same issue. But the same part is that installing tensorflow go failed when there is a update. There should be an integration test here."]}, {"number": 23256, "title": "TF 1.11 build issue with CUDA 8", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: TF 1.11\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: Docker\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 8.0/7\r\n- GPU model and memory: GeForce GTX 1060 3GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nAttempting to build a docker container with TF 1.11 and CUDA 8 on an GeForce 1060 3GB GPU. An error keeps occurring in the build (Log attached. \".txt\" was appended to be able to upload to github). Issue 22729 (https://github.com/tensorflow/tensorflow/issues/22729) was looked at but the work around didn't work for TF 1.11 and that's what is needed. The docker file is also attached. Any help you can provide would be greatly appreciated. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nsudo docker build --no-cache . -f Dockerfile.tf-1.11-py27-gpu.txt -t tf-1.11-py27-gpu\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[Dockerfile.tf-1.11-py27-gpu.txt](https://github.com/tensorflow/tensorflow/files/2514994/Dockerfile.tf-1.11-py27-gpu.txt)\r\n[tf11cuda8.log.txt](https://github.com/tensorflow/tensorflow/files/2514996/tf11cuda8.log.txt)\r\n\r\nThank you,\r\nKyle", "comments": ["Same issue, hope this problem could be fixed.\r\n\r\nAs for now, our code containing some TF 1.11 new features could only run on servers with CUDA 9 :(\r\n", "As far as I know, TensorFlow 1.11 and 1.12 only officially supports CUDA 9 right now (see https://www.tensorflow.org/install/gpu). Please consider upgrading your CUDA version or reverting to TF 1.10 in your container. Sorry for the inconvenience.\r\n\r\nSince TF team doesn't support older CUDA versions, I'm closing this issue. You can also ask a question on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow), where it's more likely to get community attention."]}, {"number": 23255, "title": "install tensorflow from source code, run ./configure in virtuealenvwrapper due to issues with missing site.getsitepackages()", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.11.0\r\n- Python version: 3\r\n- Installed using virtualenv? pip? conda?: virtualenvwrapper\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.2/7.1\r\n- GPU model and memory: Geforce 1060\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\ninstall tensorflow from source code, run ./configure in virtuealenvwrapper due to issues with missing site.getsitepackages()\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n(tensorflow) $python --version\r\nPython 3.5.2\r\n(tensorflow) $ which python\r\n/home/raymond/.virtualenvs/tensorflow/bin/python\r\n(tensorflow) raymond@raymond:~/tensorflow-1.11.0\u27eb ./configure \r\nWARNING: Processed legacy workspace file /home/raymond/tensorflow-1.11.0/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/home/raymond/.cache/bazel/_bazel_raymond/install/f1e11885a5cc7ba9947679cffb18bf94/_embedded_binaries/A-server.jar) to field java.lang.String.value\r\nWARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.18.0 installed.\r\nPlease specify the location of python. [Default is /home/raymond/.virtualenvs/tensorflow/bin/python]: \r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'site' has no attribute 'getsitepackages'\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@raymond-vincent-wang  -  Hi, please check if this [ issue ](https://github.com/pypa/virtualenv/issues/355) helps.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23254, "title": "[Features] Enable Variable Partitioning in ParameterServerStrategy graph mode", "body": "Hi @yuefengz ,\r\n\r\nVariable Partitioning is very important in Parameter Server architecture for loading balancing. It has been widely used in Recommendation systems for distributing the large embedding variables. \r\n\r\nIn `DistributionStrategy` architecture, the variable partitioner is ignored to all cases. I understand it will be complicated if we enable variables partitioner to all cases such as  `Eager`. It may be even involved with the `PartitionVariableScope` in TF 2.0 which will influence `tf.Variable` declaration with `tf.variable_creator_scope`.  However, it is easy and suitable to support partitioning on `ParameterServerStrategy` in graph mode. Every subclasses of  `DistributionStrategy` can override `_allow_variable_partition` method to decide whether to enable it or not. Currently, only `ParameterServerStrategy` has override it.\r\n\r\nIt would be appreciated to have a discussion  if there is another solutions to support variable partitioner.\r\n\r\nThanks.", "comments": ["@wangsiyu Hi, thank you so much for sending out this PR! Right now I am not able to import your PR. Is it because your repository is not up to date?", "Hi @yuefengz, thanks for your comments and I will check the merge compatibility and simplify the test case.    ", "Could you also try running unit tests with num_gpus=2? You don't have to have 2 GPUs to run that. Just to make sure `AggregatingVariable` works with `PartitionedVariable`. Thanks!", "@yuefengz I have simplified the unit test and add test case when num gpus > 1.  It works with `AggregatingVariable` with `VariableAggregation.SUM`.  And I have merge the code with the latest version. Are you able to import this now?  ", "@yuefengz Code have been refined. Please check again. ", "@wangsiyu Thank you for your PR! Please let me know whether `ParameterServerStrategy` works in your case and what else you need. Feel free to send me emails.", "@yuefengz Yes. Currently `ParameterServerStrategy` has been used in training models for recommendation system in one business unit of Alibaba Group. And I am also trying push `MirroredStrategy` to other users.  Actually,  I have encountered some performance optimization problems and also want to have a discussion with you about how to improve them in `DistributionStrategy` framework. I will send PRs or e-mails to you when needed. Thanks very much.", "@ymodak Could you please help merge this PR? Thank you!"]}, {"number": 23253, "title": "TOCO failed  Batch normalization resolution requires that mean, multiplier and offset arrays be constant.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.9 gpu\r\n- Python version: 27\r\n- Bazel version (if compiling from source): no use\r\n- GCC/Compiler version (if compiling from source): no use\r\n- CUDA/cuDNN version: CUDA9.1    CuDNN7.0\r\n- GPU model and memory: GTX1070 8G\r\n\r\n\r\n**Describe the current behavior**\r\n`toco --output_file=test.tflite --graph_def_file=freeze.pb --input_arrays=Placeholder --output_arrays=logits/BatchNorm/Reshape_1 --output_format=TFLITE --inference_type=FLOAT --std_dev_values=1 --mean_values=0`\r\n\r\nfailed\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/icare/.local/bin/toco\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 320, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 316, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 121, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 309, in convert\r\n    allow_custom_ops=self.allow_custom_ops)\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 225, in toco_convert\r\n    input_data.SerializeToString())\r\n  File \"/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 107, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\n2018-10-25 17:33:59.010229: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 135 operators, 224 arrays (0 quantized)\r\n2018-10-25 17:33:59.011116: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 127 operators, 212 arrays (0 quantized)\r\n2018-10-25 17:33:59.012209: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 127 operators, 212 arrays (0 quantized)\r\n2018-10-25 17:33:59.012368: F tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:42] Check failed: IsConstantParameterArray(*model, bn_op->inputs[1]) && IsConstantParameterArray(*model, bn_op->inputs[2]) && IsConstantParameterArray(*model, bn_op->inputs[3]) Batch normalization resolution requires that mean, multiplier and offset arrays be constant.\r\nAborted (core dumped)\r\n\r\nNone\r\n\r\n**Describe the expected behavior**\r\n\r\n I want to use toco to transform the pb file to the tflite file\r\n\r\n**Code to reproduce the issue**\r\n\r\nI use the code below to generate graph.pb and ckpy file\r\n`g = tf.get_default_graph()\r\n graph_def = g.as_graph_def()\r\n tf.train.write_graph(graph_def, \"./model\", 'graph.pb', as_text=False) \r\n saver = tf.train.Saver()\r\n saver.save(sess, os.path.join(FLAGS.checkpoint_dir, 'mnist-conv-slim.ckpt'))`\r\n\r\n  \r\n\r\nand then I freezed the graph.pb successfullly\r\n\r\n python freeze_graph.py --input_graph=/payh/to/graph.pb --input_checkpoint=/payh/tpo/mnist-conv-slim.ckpt --output_graph=/payh/to/mobile_face/model/freeze.pb --output_node_names=logits/BatchNorm/Reshape_1 --input_binary=True\r\n\r\n\r\n\r\n**Other info / logs**\r\nI use the code below to watch the nodes of each layer\r\n\r\n`import tensorflow as tf\r\ngf = tf.GraphDef()\r\n#gf.ParseFromString(open('/tmp/inception_v3_quantized.pb','rb').read())\r\ngf.ParseFromString(open('./model/freeze.pb','rb').read())\r\nfor n in gf.node:\r\n    print ( n.name +' ===> '+n.op )  `\r\n\r\nresult:\r\n\r\n\r\nPlaceholder ===> Placeholder\r\nPlaceholder_1 ===> Placeholder\r\nPlaceholder_3 ===> Placeholder\r\nReshape/shape ===> Const\r\nReshape ===> Reshape\r\nconv1/weights ===> Const\r\nconv1/weights/read ===> Identity\r\nconv1/Conv2D ===> Conv2D\r\nconv1/BatchNorm/Const ===> Const\r\nconv1/BatchNorm/beta ===> Const\r\nconv1/BatchNorm/beta/read ===> Identity\r\nconv1/BatchNorm/moving_mean ===> Const\r\nconv1/BatchNorm/moving_mean/read ===> Identity\r\nconv1/BatchNorm/moving_variance ===> Const\r\nconv1/BatchNorm/moving_variance/read ===> Identity\r\nconv1/BatchNorm/cond/Switch ===> Switch\r\nconv1/BatchNorm/cond/switch_t ===> Identity\r\nconv1/BatchNorm/cond/pred_id ===> Identity\r\nconv1/BatchNorm/cond/Const ===> Const\r\nconv1/BatchNorm/cond/Const_1 ===> Const\r\nconv1/BatchNorm/cond/FusedBatchNorm ===> FusedBatchNorm\r\nconv1/BatchNorm/cond/FusedBatchNorm/Switch ===> Switch\r\nconv1/BatchNorm/cond/FusedBatchNorm/Switch_1 ===> Switch\r\nconv1/BatchNorm/cond/FusedBatchNorm/Switch_2 ===> Switch\r\nconv1/BatchNorm/cond/FusedBatchNorm_1 ===> FusedBatchNorm\r\nconv1/BatchNorm/cond/FusedBatchNorm_1/Switch ===> Switch\r\nconv1/BatchNorm/cond/FusedBatchNorm_1/Switch_1 ===> Switch\r\nconv1/BatchNorm/cond/FusedBatchNorm_1/Switch_2 ===> Switch\r\nconv1/BatchNorm/cond/FusedBatchNorm_1/Switch_3 ===> Switch\r\nconv1/BatchNorm/cond/FusedBatchNorm_1/Switch_4 ===> Switch\r\nconv1/BatchNorm/cond/Merge ===> Merge\r\nconv1/CRelu/Neg ===> Neg\r\nconv1/CRelu/axis ===> Const\r\nconv1/CRelu ===> ConcatV2\r\nconv1/CRelu/Relu ===> Relu\r\npool1/MaxPool ===> MaxPool\r\nconv2/weights ===> Const\r\nconv2/weights/read ===> Identity\r\nconv2/Conv2D ===> Conv2D\r\nconv2/BatchNorm/Const ===> Const\r\nconv2/BatchNorm/beta ===> Const\r\nconv2/BatchNorm/beta/read ===> Identity\r\nconv2/BatchNorm/moving_mean ===> Const\r\nconv2/BatchNorm/moving_mean/read ===> Identity\r\nconv2/BatchNorm/moving_variance ===> Const\r\nconv2/BatchNorm/moving_variance/read ===> Identity\r\nconv2/BatchNorm/cond/Switch ===> Switch\r\nconv2/BatchNorm/cond/switch_t ===> Identity\r\nconv2/BatchNorm/cond/pred_id ===> Identity\r\nconv2/BatchNorm/cond/Const ===> Const\r\nconv2/BatchNorm/cond/Const_1 ===> Const\r\nconv2/BatchNorm/cond/FusedBatchNorm ===> FusedBatchNorm\r\nconv2/BatchNorm/cond/FusedBatchNorm/Switch ===> Switch\r\nconv2/BatchNorm/cond/FusedBatchNorm/Switch_1 ===> Switch\r\nconv2/BatchNorm/cond/FusedBatchNorm/Switch_2 ===> Switch\r\nconv2/BatchNorm/cond/FusedBatchNorm_1 ===> FusedBatchNorm\r\nconv2/BatchNorm/cond/FusedBatchNorm_1/Switch ===> Switch\r\nconv2/BatchNorm/cond/FusedBatchNorm_1/Switch_1 ===> Switch\r\nconv2/BatchNorm/cond/FusedBatchNorm_1/Switch_2 ===> Switch\r\nconv2/BatchNorm/cond/FusedBatchNorm_1/Switch_3 ===> Switch\r\nconv2/BatchNorm/cond/FusedBatchNorm_1/Switch_4 ===> Switch\r\nconv2/BatchNorm/cond/Merge ===> Merge\r\nconv2/CRelu/Neg ===> Neg\r\nconv2/CRelu/axis ===> Const\r\nconv2/CRelu ===> ConcatV2\r\nconv2/CRelu/Relu ===> Relu\r\npool2/MaxPool ===> MaxPool\r\nFlatten/flatten/Shape ===> Shape\r\nFlatten/flatten/strided_slice/stack ===> Const\r\nFlatten/flatten/strided_slice/stack_1 ===> Const\r\nFlatten/flatten/strided_slice/stack_2 ===> Const\r\nFlatten/flatten/strided_slice ===> StridedSlice\r\nFlatten/flatten/Reshape/shape/1 ===> Const\r\nFlatten/flatten/Reshape/shape ===> Pack\r\nFlatten/flatten/Reshape ===> Reshape\r\nfc1/weights ===> Const\r\nfc1/weights/read ===> Identity\r\nfc1/MatMul ===> MatMul\r\nfc1/BatchNorm/Reshape/shape ===> Const\r\nfc1/BatchNorm/Reshape ===> Reshape\r\nfc1/BatchNorm/beta ===> Const\r\nfc1/BatchNorm/beta/read ===> Identity\r\nfc1/BatchNorm/Const ===> Const\r\nfc1/BatchNorm/moving_mean ===> Const\r\nfc1/BatchNorm/moving_mean/read ===> Identity\r\nfc1/BatchNorm/moving_variance ===> Const\r\nfc1/BatchNorm/moving_variance/read ===> Identity\r\nfc1/BatchNorm/cond/Switch ===> Switch\r\nfc1/BatchNorm/cond/switch_t ===> Identity\r\nfc1/BatchNorm/cond/pred_id ===> Identity\r\nfc1/BatchNorm/cond/Const ===> Const\r\nfc1/BatchNorm/cond/Const_1 ===> Const\r\nfc1/BatchNorm/cond/FusedBatchNorm ===> FusedBatchNorm\r\nfc1/BatchNorm/cond/FusedBatchNorm/Switch ===> Switch\r\nfc1/BatchNorm/cond/FusedBatchNorm/Switch_1 ===> Switch\r\nfc1/BatchNorm/cond/FusedBatchNorm/Switch_2 ===> Switch\r\nfc1/BatchNorm/cond/FusedBatchNorm_1 ===> FusedBatchNorm\r\nfc1/BatchNorm/cond/FusedBatchNorm_1/Switch ===> Switch\r\nfc1/BatchNorm/cond/FusedBatchNorm_1/Switch_1 ===> Switch\r\nfc1/BatchNorm/cond/FusedBatchNorm_1/Switch_2 ===> Switch\r\nfc1/BatchNorm/cond/FusedBatchNorm_1/Switch_3 ===> Switch\r\nfc1/BatchNorm/cond/FusedBatchNorm_1/Switch_4 ===> Switch\r\nfc1/BatchNorm/cond/Merge ===> Merge\r\nfc1/BatchNorm/Shape ===> Shape\r\nfc1/BatchNorm/Reshape_1 ===> Reshape\r\nfc1/CRelu/Neg ===> Neg\r\nfc1/CRelu/axis ===> Const\r\nfc1/CRelu ===> ConcatV2\r\nfc1/CRelu/Relu ===> Relu\r\nDropout/sub/x ===> Const\r\nDropout/sub ===> Sub\r\nDropout/sub_1/x ===> Const\r\nDropout/sub_1 ===> Sub\r\nDropout/dropout_1/Shape ===> Shape\r\nDropout/dropout_1/random_uniform/min ===> Const\r\nDropout/dropout_1/random_uniform/max ===> Const\r\nDropout/dropout_1/random_uniform/RandomUniform ===> RandomUniform\r\nDropout/dropout_1/random_uniform/sub ===> Sub\r\nDropout/dropout_1/random_uniform/mul ===> Mul\r\nDropout/dropout_1/random_uniform ===> Add\r\nDropout/dropout_1/add ===> Add\r\nDropout/dropout_1/Floor ===> Floor\r\nDropout/dropout_1/div ===> RealDiv\r\nDropout/dropout_1/mul ===> Mul\r\nlogits/weights ===> Const\r\nlogits/weights/read ===> Identity\r\nlogits/MatMul ===> MatMul\r\nlogits/BatchNorm/Reshape/shape ===> Const\r\nlogits/BatchNorm/Reshape ===> Reshape\r\nlogits/BatchNorm/beta ===> Const\r\nlogits/BatchNorm/beta/read ===> Identity\r\nlogits/BatchNorm/Const ===> Const\r\nlogits/BatchNorm/moving_mean ===> Const\r\nlogits/BatchNorm/moving_mean/read ===> Identity\r\nlogits/BatchNorm/moving_variance ===> Const\r\nlogits/BatchNorm/moving_variance/read ===> Identity\r\nlogits/BatchNorm/cond/Switch ===> Switch\r\nlogits/BatchNorm/cond/switch_t ===> Identity\r\nlogits/BatchNorm/cond/pred_id ===> Identity\r\nlogits/BatchNorm/cond/Const ===> Const\r\nlogits/BatchNorm/cond/Const_1 ===> Const\r\nlogits/BatchNorm/cond/FusedBatchNorm ===> FusedBatchNorm\r\nlogits/BatchNorm/cond/FusedBatchNorm/Switch ===> Switch\r\nlogits/BatchNorm/cond/FusedBatchNorm/Switch_1 ===> Switch\r\nlogits/BatchNorm/cond/FusedBatchNorm/Switch_2 ===> Switch\r\nlogits/BatchNorm/cond/FusedBatchNorm_1 ===> FusedBatchNorm\r\nlogits/BatchNorm/cond/FusedBatchNorm_1/Switch ===> Switch\r\nlogits/BatchNorm/cond/FusedBatchNorm_1/Switch_1 ===> Switch\r\nlogits/BatchNorm/cond/FusedBatchNorm_1/Switch_2 ===> Switch\r\nlogits/BatchNorm/cond/FusedBatchNorm_1/Switch_3 ===> Switch\r\nlogits/BatchNorm/cond/FusedBatchNorm_1/Switch_4 ===> Switch\r\nlogits/BatchNorm/cond/Merge ===> Merge\r\nlogits/BatchNorm/Shape ===> Shape\r\nlogits/BatchNorm/Reshape_1 ===> Reshape\r\n\r\nIt seems that the BN para are already constant\r\n\r\nwhat should I do to solver the problem?\r\n\r\ngood luck to you thanks\r\n", "comments": ["same problem here (tensorflow-gpu 1.11.0)", "what? I am not understand ", "I'm also facing same problem with facenet pretrain model, tensorflow 1.10.0, here is my command:\r\n\r\n```\r\ntflite_convert --graph_def_file=optimized_facenet.pb \\\r\n  --output_file=optimized_graph_quantized.tflite \\\r\n  --output_format=TFLITE \\\r\n  --input_shape=1,160,160,3 \\\r\n  --input_array=input \\\r\n  --output_array=embeddings \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --std_dev_values=128 --mean_values=128 \\\r\n  --default_ranges_min=-6 --default_ranges_max=6 \\\r\n  --quantize_weights\r\n```", "@tung238 is your model trained with quantization? 'inference_type=QUANTIZED_UINT8' and 'quantize_weights' are not meant to be used at the same time. If your model was trained with quantization, use 'inference_type=QUANTIZED_UINT8' and remove 'quantize_weights'.", "@sxsxsx Can you attach your model here?", "I am trying to convert a trained tensorflow model to tensorflow lite\r\n\r\ni have tensorflow 1.10 with bazel 0.15.0\r\nthis is command i am using\r\nbazel-bin/tensorflow/contrib/lite/toco/toco --input_file=\"/home/jitender/lesslayerLite/Nfrozen2.pb\" --output_file=\"/home/jitender/lesslayerLite/handLite.tflite\" --input_shapes=1,160,120,3 --input_arrays=Placeholder --output_arrays='sigmoid_activation' --inference_type=FLOAT --output_format=TFLITE\r\n![tocoerror](https://user-images.githubusercontent.com/22957065/49565741-31055b00-f94e-11e8-943c-6deba2334c9c.png)\r\n", "I am facing the same problem with my conversion, I'm attaching my model below.\r\n\r\nCommand to convert - \r\ntoco --graph_def_file frozen_graph.pb --output_file converted.tflite --input_arrays input --output_arrays embeddings --input_shape 1,160,160,3\r\n\r\n[frozen_graph.zip](https://github.com/tensorflow/tensorflow/files/2657328/frozen_graph.zip)\r\n\r\n@liyunlu0618 ", "Looks like you're trying to convert a training graph. You first need to get a frozen EVAL graph and then convert it.", "@liyunlu0618 my model is frozen. ", "I have the same issue", "i have the same issue too.", "I have the same issue as well", "Same issue. Is there a solution for this?", "same problem here, I am using `tf.slim.batch_norm`. ", "I've solved my problem, my model uses a placeholder to initialize Batchnorm, so my exported diagram contains some Op nodes for training purposes.\r\nAfter I made the following modifications, the problem was solved after refreezing the diagram.\r\nsuch as:\r\nis_training = tf.placeholder (tf.bool, name = 'is_training')\r\nsess.run (net, feed_dict= {x: x, is_training: is_training})\r\n=>\r\nis_training = False\r\nsess.run (net, feed_dict= {x: x, is_training: is_training})", "I think in the process of converting pb to tflite, everything wrong with batchnorm, that means this graph is not a clean prediction graph, is that right?", "anyone has a concrete solution to this? @liyunlu0618 can you elaborate more on this?", "Marking issue as resolved due to inactivity. Feel free to re-open this if it's unresolved or file a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose)"]}, {"number": 23252, "title": "Tensorflow Docker files for 1.12 don't have XLA built binaries included.", "body": "Using `tensorflow/tensorflow:1.12.0-rc1-devel-gpu-py3` and running the [example script](https://www.tensorflow.org/performance/xla/jit) shows that the tensorflow included in the docker image isn't built with XLA.\r\n\r\nHowever, if you manually pip install tensorflow: `pip install tensorflow_gpu==1.12.0rc1` then run the example script it shows XLA support is built into the binary.\r\n\r\nPlease can someone update the Docker files, I would do but it's far from obvious where the arg would be inserted.\r\n\r\ncc @angersson \r\n\r\n", "comments": ["They would also need ptxas cherry picking from CUDA 9.2.", "The development images `-devel` are for making changes to the TensorFlow code base. The TensorFlow Python version they have installed is built when the Docker image is built to verify that the development environment works as intended. I don't think that in-image build configuration includes XLA, but use of that package isn't the point of the `-devel` images (although it may still be an error, technically; I'm not sure).\r\n\r\nIf you want to only want to use TensorFlow via Docker, try one of the non-`devel` `gpu` images. Those images install the Pip package with the same version as the Docker image tag, so it should have XLA as expected. That should be able to handle the script you linked. If you want the other utilities included by the `devel` images, you can also uninstall and then reinstall `tensorflow_gpu`.\r\n\r\nIf you want to *develop* TensorFlow via Docker, you can use the `devel` image, but you'll have to rebuild the Pip package from inside of the image.", "Hi @angersson\r\n\r\nI think this was closed prematurely, TF 1.12 explicitly mentions building with XLA.\r\n\r\nYet the docker version is not - I fully believe this to be an error.\r\n\r\nIt might well be in alpha/beta but now that TF is built with XLA enabled the docker versions should support that.\r\n\r\nI\u2019ve managed to fix it by manually installing the TF version in RC yet it still seems a hack fix when you provide the container.\r\n\r\nThanks,\r\nJoe "]}, {"number": 23251, "title": "I want to use the beamsearch in my seq2seq architecture. But I FAILED after working for a WHOLE afternoon!!!", "body": "[beam_search_prediction.py.zip](https://github.com/tensorflow/tensorflow/files/2514356/beam_search_prediction.py.zip)\r\n\r\nerror is about the attention wrapper i guess", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23250, "title": "Added implementation of an absolute_name_scope and a unit test.", "body": "This PR adds a new `name_scope` called `absolute_name_scope` which builds on `name_scope` but allows reusing `name_scope`s without creating a new `name_scope` which is normally uniquified by adding a `_N`.\r\nBased on the discussion in https://github.com/tensorflow/tensorflow/issues/9545", "comments": ["I believe this behavior is already implemented if you append \"/\" to the argument of `name_scope`: https://www.tensorflow.org/api_docs/python/tf/Graph#name_scope\r\n\r\nUnfortunately this isn't documented in `tf.name_scope`: https://www.tensorflow.org/api_docs/python/tf/name_scope\r\nPRs welcome for this :)\r\n\r\nCan you check if adding \"/\" works for your use case?", "No, this does not work right now without doing the same approach that is build into this PR.\r\n\r\nRunning the following code for example prints:\r\n**1** will actually be created as `abs_inner/FloatOutput`\r\n**2** will actually be created as `abs_inner/FloatOutput_1`\r\nExtra:\r\n**3** Removing the slash from the first `abs_outer` yields `abs_inner/FloatOutput`\r\n**4** Removing the slash from the first `abs_inner` yields `abs_outer/abs_inner/FloatOutput`\r\nBUT: The second `with` scope will create `abs_outer/abs_inner_1/FloatOutput`\r\n```python\r\ng = ops.Graph()\r\n    with g.as_default():\r\n      with g.name_scope('abs_outer/'):\r\n        with g.name_scope('abs_inner/'):\r\n          print(g.create_op(\"FloatOutput\", [], [dtypes.float32]).name) # 1\r\n      with g.name_scope('abs_outer/'):\r\n        with g.name_scope('abs_inner/'):\r\n          print(g.create_op(\"FloatOutput\", [], [dtypes.float32]).name) # 2\r\n```\r\n\r\nIn general, once you start using `/` and try to \"reuse\" name scopes it quickly becomes a mess.", "Ah I see. I think it's intended to be used more like this:\r\n```python\r\nwith tf.name_scope(\"outer\"):\r\n    with tf.name_scope(\"inner\") as inner_scope:\r\n        print(tf.constant(1))\r\nwith tf.name_scope(inner_scope):\r\n    print(tf.constant(1))\r\n```\r\nThis prints:\r\nouter/inner/Const:0\r\nouter/inner/Const_1:0\r\n\r\nI'm hesitant to expose a new API endpoint for doing the same thing, especially since you might get unintended behavior with `absolute_name_scope` if there's a regular `name_scope` somewhere in the stack. Could you save the scope and re-use it as shown above?", "I know the intention, but this forces you to carry around the `name_scope` object, which is not necessary when using `variable_scopes` which I find to be a strange inconsistency.\r\nAlternatively, add a parameter to `name_scope` which makes it behave like the suggestion I made.\r\nTrue, using it with an `as` currently does not work as \"intended\". I'll think about it.", "Regarding the inconsistency with `variable_scope`, we're deprecating `variable_scope` in TF 2.0 in favor of a more object-oriented approach (see [this RFC](https://github.com/tensorflow/community/blob/master/rfcs/20180817-variables-20.md)). So I'm still not convinced we should add this API -- it would be more convenient in some situations, but also could lead to confusion and is one more API endpoint.\r\n\r\n@alextp @asimshankar do you have an opinion?", "If the trend is more towards being object-oriented, then I agree that this might not be relevant.", "variable_scopes are not a part of tf2, yes. Name scopes are still mostly as a way of making your tensorboard prettier.", "As such I intended this and referenced the issue. I just want to know if I should put more effort into this or if it will be trashed. I find it useful to have a cleaner tensorboard and am happy to share.\r\n\r\nEdit: Just tested the `as` part and that works as well.", "Hi @sleighsoft, sorry for the delay. My opinion is that we should keep the API small and encourage the object-oriented technique, so I'm not gonna sign off on this. The \"API review\" means that someone else tasked specifically with reviewing API changes should take a look though, so we can get a second opinion. Everyone's busy with the 2.0 effort right now, so that's why it's taking a while. Thanks for your patience.", "For tf-api review:\r\n\r\nI added a comment on the associated issue, which I've copied here:\r\n\r\nThe normal TensorFlow way to handle using the same scope in two places is:\r\n\r\n```\r\n    with tf.name_scope('outer'):\r\n      with tf.name_scope('inner') as scope:\r\n        print(tf.constant(1)) # Will print outer/inner/Const:0\r\n    with tf.name_scope(scope):\r\n      print(tf.constant(1)) # Will print outer/inner/Const_1:0\r\n```", "Hey @josh11b,\r\n@skye mentioned that very same thing above. The whole point of my pull request is to allow you to open up that very same `name_scope` without having to carry around the `name_scope` object.\r\nI myself find this useful because it allows me to group operations logically in tensorboard without blowing up my code by passing around `name_scope` objects.", "Nagging Reviewer @skye: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied.", "The paradigm we are moving towards in 2.0 is carrying around the Python objects, as per @josh11b 's comment above. In this case, as a workaround, you should be able to create a name scope function in your own code that tracks global namescopes. Closing given that workaround."]}, {"number": 23249, "title": "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory", "body": " Linux Ubuntu 16.04  \r\nPython 3.5\r\nmy cuda vision is 9.2.88\r\ncudnn is 7.1.4\r\nonly one gpu : NVIDIA 1080ti\r\n I use pip to install tf\r\n\r\nwhen I just install tf use \" pip install tensorflow==1.8.0 \"  ,it's OK ,it can import tensorflow,  but can't use gpu\r\n  when I  pip install tensorflow-gpu==1.8.0 ,  these is error :  ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nuse   pip install tensorflow-gpu==1.11.0  or   pip install tensorflow==1.11.0  will get the same error.\r\n\r\n\r\n", "comments": ["1. Latest TensorFlow __supports__ CUDA 8.0-10.0. CuDNN 6-7.\r\n2. Each TensorFlow binary has to work with the version of CUDA and CuDNN it was built with. If they don't match, you have to change either the TensorFlow binary or the Nvidia softwares.\r\n3. Official latest TensorFlow binaries (the one downloaded by pip or conda) are built with CUDA 9.0, CuDNN 7.\r\n4. If you don't like to change your Nvidia software, you can:\r\n(1) Use non-official binaries built by others. e.g.: https://github.com/mind/wheels/releases, https://github.com/hadim/docker-tensorflow-builder#builds\r\n(2) Build the binaries by yourself from source  with your version of Nvidia software.", "Were you able to resolve your issue?  Did you try building TensorFlow 1.11 against CUDA 9.0?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23248, "title": "tf.estimator.train_and_evaluation stops before it reaches max_step", "body": "**System information**\r\n- TensorFlow version: 1.11.0, using docker images tensorflow/tensorflow:1.11.0-gpu-py3\r\n- Doc Link: [tf.estimator.train_and_evaluation](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate)\r\n\r\n**Describe the documentation issue**\r\n\r\nAccording to the doc, `tf.estimator.train_and_evaluation` was meant to train forever or `max_step` if specified in `train_spec`.\r\n\r\nHowever, it took me many trial and errors to realize: if the dataset returned by `input_fn` in the `train_spec` was repeated a finite number of times and that number was smaller than `max_step`, the training would stop before it reached `max_step`.\r\n\r\nFor example, I had a dataset about 120000 images. With batch size of 64, the training would stop at 1875 step (~120000/64), even though `max_step` was set at 10000 or higher.\r\n\r\nThis behavior is not written in the document. Should the document be update or was it an unexpected behavior of the function?\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nYes.", "comments": ["You should not limit repeat num, please use `repeat()`, it will repeat the input indefinitely, and will not throw `OutOfRangeError`."]}, {"number": 23247, "title": "pip install failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n-windows 10pro):\r\n- TensorFlow source (source or binary):\r\n- TensorFlow version:\r\n- Python 3.6:\r\n- Installed  pip:\r\n- GCC/Compiler version (if compiling from source):\r\ncuda 9.0\r\n-gtx760\r\n\r\n\r\n**Describe the problem**\r\n\r\nPS F:\\Downloads>  pip install tensorflow-gpu\r\nCollecting tensorflow-gpu\r\n  Using cached https://files.pythonhosted.org/packages/3d/a0/60f72b76915a7c83e336e7f9ccf3a08305c30c7262cd15fedde44e026c3f/tensorflow_gpu-1.11.0-cp36-cp36m-win_amd64.whl\r\nRequirement already satisfied: wheel>=0.26 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (0.32.2)\r\nRequirement already satisfied: six>=1.10.0 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (1.11.0)\r\nRequirement already satisfied: tensorboard<1.12.0,>=1.11.0 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (1.11.0)\r\nRequirement already satisfied: setuptools<=39.1.0 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (39.1.0)\r\nRequirement already satisfied: numpy>=1.13.3 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (1.15.3)\r\nRequirement already satisfied: keras-applications>=1.0.5 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (1.0.6)\r\nRequirement already satisfied: termcolor>=1.1.0 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\r\nRequirement already satisfied: keras-preprocessing>=1.0.3 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (1.0.5)\r\nRequirement already satisfied: gast>=0.2.0 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\r\nRequirement already satisfied: protobuf>=3.6.0 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (3.6.1)\r\nRequirement already satisfied: grpcio>=1.8.6 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (1.16.0)\r\nRequirement already satisfied: astor>=0.6.0 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (0.7.1)\r\nRequirement already satisfied: absl-py>=0.1.6 in f:\\development\\python\\lib\\site-packages (from tensorflow-gpu) (0.6.0)\r\nRequirement already satisfied: markdown>=2.6.8 in f:\\development\\python\\lib\\site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu) (3.0.1)\r\nRequirement already satisfied: werkzeug>=0.11.10 in f:\\development\\python\\lib\\site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu) (0.14.1)\r\nRequirement already satisfied: h5py in f:\\development\\python\\lib\\site-packages (from keras-applications>=1.0.5->tensorflow-gpu) (2.8.0)\r\nInstalling collected packages: tensorflow-gpu\r\nException:\r\nTraceback (most recent call last):\r\n  File \"f:\\development\\python\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 143, in main\r\n    status = self.run(options, args)\r\n  File \"f:\\development\\python\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 366, in run\r\n    use_user_site=options.use_user_site,\r\n  File \"f:\\development\\python\\lib\\site-packages\\pip\\_internal\\req\\__init__.py\", line 49, in install_given_reqs\r\n    **kwargs\r\n  File \"f:\\development\\python\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 760, in install\r\n    use_user_site=use_user_site, pycompile=pycompile,\r\n  File \"f:\\development\\python\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 382, in move_wheel_files\r\n    warn_script_location=warn_script_location,\r\n  File \"f:\\development\\python\\lib\\site-packages\\pip\\_internal\\wheel.py\", line 215, in move_wheel_files\r\n    prefix=prefix,\r\n  File \"f:\\development\\python\\lib\\site-packages\\pip\\_internal\\locations.py\", line 153, in distutils_scheme\r\n    d.parse_config_files()\r\n  File \"f:\\development\\python\\lib\\distutils\\dist.py\", line 395, in parse_config_files\r\n    parser.read(filename)\r\n  File \"f:\\development\\python\\lib\\configparser.py\", line 697, in read\r\n    self._read(fp, filename)\r\n  File \"f:\\development\\python\\lib\\configparser.py\", line 1080, in _read\r\n    raise MissingSectionHeaderError(fpname, lineno, line)\r\nconfigparser.MissingSectionHeaderError: File contains no section headers.\r\nfile: 'setup.cfg', line: 1\r\n'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\r\n", "comments": ["Use conda instead", "Is this still an issue?", "> still an issue?\r\n\r\nI turned conda. It was solved. Thank you."]}, {"number": 23246, "title": "Grpc+RDMA problem", "body": "Same as https://github.com/tensorflow/tensorflow/issues/18630\r\n\r\nOnly start ps, it will got error, when worker is on other node. And will waiting when worker is on the same node.\r\n\r\n**System information**\r\nHave I written custom code\uff1a No\r\nOS Platform\uff1a Ubuntu 16.04.3 LTS\r\nTensorFlow installed from\uff1a by source code\r\nTensorFlow version\uff1a1.8.0\r\nBazel version: bazel release 0.16.0\r\nCUDA/cuDNN version: 9.0\r\nGPU model and memory: P100-PCIE 16280MiB\r\n\r\nconfig 2Ps and 2 workers, but only start one ps and one worker.\r\nHere is the ps log. It doesn't wait for the ps/worker, and failure after  retry 5 times.\r\n2018-10-18 09:09:22.257003: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.96.135:20002, 1 -> 192.168.96.74:20003}\r\n2018-10-18 09:09:22.264672: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:20001, 1 -> 192.168.96.74:20004}\r\n2018-10-18 09:09:22.264716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.96.135:20002, 1 -> 192.168.96.74:20003}\r\n2018-10-18 09:09:22.277272: I tensorflow/contrib/verbs/rdma.cc:315] RoCE v2 is not configured for GID_INDEX 0\r\n2018-10-18 09:09:22.285727: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:20001\r\n2018-10-18 09:10:57.293232: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:0\r\n2018-10-18 09:10:57.295504: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (1/5)...\r\n2018-10-18 09:10:59.296411: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (2/5)...\r\n2018-10-18 09:11:01.297541: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (3/5)...\r\n2018-10-18 09:11:04.298136: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (4/5)...\r\n2018-10-18 09:11:06.299257: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (5/5)...\r\n2018-10-18 09:11:06.299300: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:1\r\n2018-10-18 09:11:06.302309: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:1\r\n2018-10-18 09:11:06.302436: F tensorflow/contrib/verbs/rdma_mgr.cc:142] Check failed: rc->PingPostSend() == 0 Couldn't post send  to /job:worker/replica:0/task:1 with error: Invalid argument\r\nAborted (core dumped)\r\n", "comments": ["@poxvoculi is more familiar with this code than I am, but I'm not sure if this is supported by us or the community.", "Perhaps @yanivbl6 is the best person to look at this.", "@yanivbl6 -  Hi, could you please look into this ?", "There was an issue with 1.8 when a specific initialization order was required:\r\n\r\nQuoting @boriskovalev:\r\n> 1.\tNeed to run by strict sequence when worker/PS with higher index is started first.\r\n> Example: We run 2 PS + 2 Workers.  So, launch sequence must be PS1->PS0->W1->W0.\r\n\r\nCan you try it?\r\n", "> There was an issue with 1.8 when a specific initialization order was required:\r\n> \r\n> Quoting @boriskovalev:\r\n> \r\n> > 1. Need to run by strict sequence when worker/PS with higher index is started first.\r\n> >    Example: We run 2 PS + 2 Workers.  So, launch sequence must be PS1->PS0->W1->W0.\r\n> \r\n> Can you try it?\r\n\r\nSorry, reply this too late.\r\nIn my case, I can't ensure the start sequence.\r\nAnd it alse get error with this sequence: PS1->PS0->W1->W0 when it's interval of more than 10 seconds between any two nodes.\r\nBelow is the output:\r\n2018-11-05 09:19:30.492683: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:20013\r\n2018-11-05 09:19:30.499571: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (1/5)...\r\n2018-11-05 09:19:32.501270: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (2/5)...\r\n2018-11-05 09:19:34.502214: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (3/5)...\r\n2018-11-05 09:19:36.503246: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (4/5)...\r\n2018-11-05 09:19:38.504283: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (5/5)...\r\n2018-11-05 09:19:38.504325: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:0\r\n2018-11-05 09:19:38.506257: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:1\r\n2018-11-05 09:19:38.509235: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:0\r\n2018-11-05 09:19:38.509325: F tensorflow/contrib/verbs/rdma_mgr.cc:142] Check failed: rc->PingPostSend() == 0 Couldn't post send to /job:worker/replica:0/task:0 with error: Invalid argument\r\nAborted (core dumped)", "I see.\r\nThe requirement for the strict order was eventually fixed. I believe it was a grpc problem (It also occured without verbs) , but I don't know to point over the exact patch that fixed it.\r\n\r\nI can see that the retrying window in did places the limitation of 10 seconds only. I will note to myself to increase that in the future. As a temporary fix, you can increase the amount of attempts [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma_mgr.cc#L90 ). Each attempt is 2 seconds.\r\n", "> I see.\r\n> The requirement for the strict order was eventually fixed. I believe it was a grpc problem (It also occured without verbs) , but I don't know to point over the exact patch that fixed it.\r\n> \r\n> I can see that the retrying window in did places the limitation of 10 seconds only. I will note to myself to increase that in the future. As a temporary fix, you can increase the amount of attempts [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma_mgr.cc#L90). Each attempt is 2 seconds.\r\n\r\nThank you for reply. \r\nI will try to add waiting func before the train.server start unit all nodes are ready to avoid the problem.", "@lianyunwen  Any update ? Feel free to close if it's resolved.", "In \"awaiting response\" status for more than 3 days. Hence closing this. Please post the updates here if any, we will reopen the issue. Thanks ! "]}, {"number": 23245, "title": "ModuleNotFoundError: No module named 'tensorflow'", "body": "i am using python 3.6.7 , in python 3.6.7 version not able to install tensorflow, please post the solution and guide to me.\r\n\r\nerror:         Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow  ", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation."]}, {"number": 23244, "title": "[Features]mixed precision support enhancement using decorator", "body": "This feature realize the automatic/constant loss scaling algorithm in Backprop, which can help preserve small gradient magnitudes ( ref to  https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html)\r\nMixed precision with loss scaling is already support by tf.contrib.mixed_precision.LossScaleOptimizer,  with the following usage:\r\n```python\r\nloss = loss_fn()\r\nopt = tf.AdamOptimizer(learning_rate=...)\r\n\r\n# Choose a loss scale manager which decides how to pick the right loss scale\r\n# throughout the training process.\r\nloss_scale_manger = tf.contrib.mixed_precision.FixedLossScaleManager(5000)\r\n\r\n# Wraps the original optimizer in a LossScaleOptimizer.\r\nloss_scale_optimizer = LossScaleOptimizer(opt, loss_scale_manager)\r\n\r\n# Call minimize() on the loss scale optimizer.\r\ntrain_op = loss_scale_optimizer.minimize(loss)\r\n```\r\n\r\nThe optimizer `LossScaleOptimizer` is actually a wrapper of other optimizers.  However, the usage using wrapper is too limited to support all cases.  For example,\r\n    + if the users  are using `tf.gradients()` to compute the gradients rather than `optimizer.compute_gradients()`/`optimizer.minimize()`,  the wrapper is no longer in user. \r\n    + the `LossScaleOptimizer` wrapper may have compatibility problem with other optimizer wrappers, e.g. `SyncReplicasOptimizer`\r\n    + the current implementation in `LossScaleOptimizer` did not take the colocation parameter into consideration. \r\n\r\nInstead, we implement loss scaling using a decorator on `gradients()`, which is the atom function to compute the backprop gradients. The usage is as following:\r\n```python\r\nloss = loss_fn()\r\nopt = tf.AdamOptimizer(learning_rate=...)\r\n# constant loss scaling with loss_scale=64\r\nwith tf.mixed_precision_scope(automatic_loss_scaling=False, loss_scale=64):\r\n      grad = tf.gradients()  # if using tf.gradients\r\n      var_and_grad =opt.minimize(loss) # if using optimizer\r\n\r\n# automatic loss scaling\r\nwith tf.mixed_precision_scope(automatic_loss_scaling=True):\r\n      grad = tf.gradients()  # if using tf.gradients\r\n      var_and_grad =opt.minimize(loss) # if using optimizer\r\n```\r\n\r\nThe implementation can handle all of the above problems encounter by `LossScaleOptimizer` .", "comments": ["@protoget Hi, I think LossScaleOptimizer is introduced by 4c2cb71 . Would you mind taking a look or redirecting to someone else who can? Thank you.", "This feature has been used internally and simplifies the code change to model function. Please take a look. Thanks. @protoget ", "@protoget Could you please kindly take a look at this CR or delegate it to someone who have the bandwidth? This has already been deployed and successfully used in our in-house environment and we think it should also be beneficial for the community, so we make this PR. \r\n\r\nThanks.", "@protoget Thank you for your comments.\r\nAs for the issue  \"applying zero gradients\" versus \"not applying gradient update\": the case when there is overflow appears is actually in quite low-frequency during the training procedure. In a certain training case, the frequency is approximately 4/10k, which contributes minor impact to the training process. Additionally, we conducted experiments on certain models, e.g. on Faster RCNN with ADAM optimizer, the result is mAP=69.78 (applying zero gradients) versus mAP=69.68 (not applying gradients update). The result is a little bit better. Of course we can say that it is because of random experiments noise, however it can demonstrate that the difference is ignorable.\r\n\r\nAs for the points you pointed out:\r\n+ If the users are using `tf.gradients()` to compute the gradients rather than `optimizer.compute_gradients()`/`optimizer.minimize()`, the wrapper is no longer in user.\r\nCould you give an example where `tf.gradients()` couldn't be replaced by `optimizer.compute_gradients()`?\r\nThis point is mainly for ease of use. Although, the usage of `tf.gradients()` can be replaced by `optimzier.compute_gradients()`, the users need to change more code to take use of `LossScaleOptimzier`. It may introduce more overhead for the users.\r\n\r\n+ The LossScaleOptimizer wrapper may have compatibility problem with other optimizer wrappers, e.g. SyncReplicasOptimizer\r\nWe have to check on that. Most likely we'd fix that single optimizer if that's the case. Do you have an example?\r\nWe have encountered this issue in our in-house model, when using LossScaleOptimizer and SyncReplicasOptimizer. When there is overflow, the apply_gradients will apply no_op, as a result, global_step will not be updated. SyncReplicasOptimizer is synchronized using a token related to global_step. The correlation will trigger hang, when using these two optimizer. Maybe there will be more similar cases for LossScaleOptimizer to be compatible with other optimizers. This is another reason that we choose to apply zeros gradients for overflow case in `tf.gradients()`. The decorator will make `tf.gradients` seems to have totally no difference with/without mixed precision training.\r\n\r\n+ The current implementation in LossScaleOptimizer did not take the colocation parameter into consideration.\r\nOne can use `optimizer.compute_gradients()` and `optimizer.apply_gradients()` separately. The first API allows colocation options the same way `tf.gradients()` does. LossScaleOptimizer allows split `minimize()` into `compute_gradients()` and `apply_gradients()`.\r\nIn loss scale algorithm, we can separate the gradients computation into three parts: 1. scale the loss; 2. backpropagation computation using standard `tf.gradients()` or compute_gradients(); 3. unscale the grads. What you point out that \"`optimizer.compute_gradients()`  allows colocation options the same way `tf.gradients()` does\" is actually referring the second part. What we would like to point out is the first and third step. The current `LossScaleOptimizer` did not deal with the colocation issue for the 1st and 3rd step. The colocation issue is quite important in certain cases, such as in distribution training.  We have already take it into consideration in the decorator implementation.\r\n\r\nThank you again to review our work in detail with great effort, the above is our consideration to the points you pointed out. ", "Thanks for your response.\r\n\r\n* In terms of the parity between 'zero grad' and 'no grad update' -- `tf.gradients()` is a util function and thus should faithfully return the gradient result, and make no assumptions about how client would use the result. How to use the gradient if NaN/Inf, should stay in client code. \r\n* In terms of the ease of use for `tf.gradients()` vs `opt.compute_gradient()`: maybe, but I don't find it very  concerning. I'm afraid that's not strong enough to justify to zero-out gradients in `tf.gradient()`.\r\n* For \"compatibility problem of SyncReplicasOptimzier\":\r\n  * (Sorry, but) this optimizer is depreciated.\r\n  * NV suggests just incr the global step even if you get a NaN/Inf grad, just don't update the variables. We did a series of exp and we found that it working.\r\n* For \"colocation constraints\": I think it's not fundamental limit. The `LossScaleOptimizer' can just do the extra work `gradients_with_scaling()` did.\r\n\r\nIn summary I think `tf.gradients()` should return gradients as they are, and leave proper handling to client code. We probably couldn't merge this decorator in official TF core, but you can have a library for your production environment, and override the tf implementation when you call it.", "> Thanks for your response.\r\n> \r\n> * In terms of the parity between 'zero grad' and 'no grad update' -- `tf.gradients()` is a util function and thus should faithfully return the gradient result, and make no assumptions about how client would use the result. How to use the gradient if NaN/Inf, should stay in client code.\r\n> * In terms of the ease of use for `tf.gradients()` vs `opt.compute_gradient()`: maybe, but I don't find it very  concerning. I'm afraid that's not strong enough to justify to zero-out gradients in `tf.gradient()`.\r\n> * For \"compatibility problem of SyncReplicasOptimzier\":\r\n>   \r\n>   * (Sorry, but) this optimizer is depreciated.\r\n>   * NV suggests just incr the global step even if you get a NaN/Inf grad, just don't update the variables. We did a series of exp and we found that it working.\r\n> * For \"colocation constraints\": I think it's not fundamental limit. The `LossScaleOptimizer' can just do the extra work `gradients_with_scaling()` did.\r\n> \r\n> In summary I think `tf.gradients()` should return gradients as they are, and leave proper handling to client code. We probably couldn't merge this decorator in official TF core, but you can have a library for your production environment, and override the tf implementation when you call it.\r\n\r\n@Dido0o0  Hi, could you please respond to the above comments. ", "> > Thanks for your response.\r\n> > \r\n> > * In terms of the parity between 'zero grad' and 'no grad update' -- `tf.gradients()` is a util function and thus should faithfully return the gradient result, and make no assumptions about how client would use the result. How to use the gradient if NaN/Inf, should stay in client code.\r\n> > * In terms of the ease of use for `tf.gradients()` vs `opt.compute_gradient()`: maybe, but I don't find it very  concerning. I'm afraid that's not strong enough to justify to zero-out gradients in `tf.gradient()`.\r\n> > * For \"compatibility problem of SyncReplicasOptimzier\":\r\n> >   \r\n> >   * (Sorry, but) this optimizer is depreciated.\r\n> >   * NV suggests just incr the global step even if you get a NaN/Inf grad, just don't update the variables. We did a series of exp and we found that it working.\r\n> > * For \"colocation constraints\": I think it's not fundamental limit. The `LossScaleOptimizer' can just do the extra work `gradients_with_scaling()` did.\r\n> > \r\n> > In summary I think `tf.gradients()` should return gradients as they are, and leave proper handling to client code. We probably couldn't merge this decorator in official TF core, but you can have a library for your production environment, and override the tf implementation when you call it.\r\n> \r\n> @Dido0o0 Hi, could you please respond to the above comments.\r\n\r\nHi hgadig,\r\n\r\nMengdi is on vocation due to the traditional Chinese spring festival so her response may be a little bit late.\r\n\r\nAs to the comments, actually we have had a phone-call discussion with Google colleagues several months ago and provided our explanations. We will also input our comments into this thread to ensure we share the same background.\r\n\r\nThanks.", "Hi @hgadig \r\nAs @yangjunpro mentioned , regarding to the above comments, we had online discussion with Google colleagues months ago. The discussion is mainly focused on 1) whether `tf.gradients` should return the 'processed' gradients 2) zero gradients to be applied (this PR) or none gradients  to be applied(`LossScaleOptimizer` ), which is better for the accuracy. \r\n\r\nFor 1), in our opinion, once the users turn on loss scaling strategy using `with tf.mixed_precision_scope(automatic_loss_scaling=True):`, the users have already noticed that  \r\n the gradients results will be handled by the  loss scaling manager,  thus the returned zero-gradients are reasonable and acceptable ( actually I guess the mainly concern is that the code is involved in the core gradients logic);\r\nFor 2), actually  both applying zero-gradients and no-gradients applied are approximation method when compared with the fp32 training process. We can not judge which is better. We have conducted experiments with this zero-gradients method upon multiple models, including CNN, RNN and WDL, etc, there are no significant difference to the accuracies obtained by fp32 training.\r\n\r\nThe advantage of this processing logic with respect to `LossScaleOptimizer` is already mentioned in the above replies. The mainly contribution is that this processing logic can handle the atomic operations of the backpropagation `tf.gradients`, the returned gradients seem to be the same as fp32 bp, thus all the optimizers can be used without any modification.  However `LossScaleOptimizer` is a wrapper for the optimizer (modifying the function `apply_gradients`), which may have potential conflict with other optimizer wrappers.\r\n \r\nThanks.", "Thank you for the PR, and the detailed justification you provided for putting this in tf.gradients. Currently, our plan is to use the `LossScaleOptimizer`, and not to apply loss scaling in `tf.gradients`. Unfortunately this means I am closing this PR. In addition to the points @protoget already stated, additional reasons for not putting this in tf.gradients are:\r\n\r\n* We do not want to add another context manager. Context managers tend to be fairly complicated and we'd like to avoid adding another if necessary.\r\n* In TensorFlow 2, the recommended API is tf.keras, so we are focusing on the optimizer API and not tf.gradients.\r\n\r\nAll issues with `LossScaleOptimizer`, other than that it is not applied with `tf.gradients`, can be resolved. Also note, we are in the process of moving the `LossScaleOptimizer` outside contrib. See [MixedPrecisionLossScaleOptimizer](https://github.com/tensorflow/tensorflow/blob/1518db83d96aac0efe2c85950581001cfd365821/tensorflow/python/training/experimental/loss_scale_optimizer.py#L30)\r\n\r\nThis decision is not set in stone. In the upcoming months, we will have a mixed precision design review, where we will describe the entire mixed precision API, including the LossScaleOptimizer.\r\n"]}]